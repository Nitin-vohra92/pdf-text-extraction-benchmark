=1

Faster Sequential Search with a Two-Pass Dynamic-Time-Warping Lower Bound

Introduction

Dynamic Time Warping (DTW) was initially introduced to recognize spoken words [\cite=sakoe1978dpa], but it has since been applied to a wide range of information retrieval and database problems: handwriting recognition [\cite=bahlmann2004wio] [\cite=niels2005udt], signature recognition [\cite=1219544] [\cite=chang2007mdt], image de-interlacing [\cite=almog2005], appearance matching for security purposes [\cite=Kale2004], whale vocalization classification [\cite=brown2006cvk], query by humming [\cite=4432642] [\cite=zhu2003wie], classification of motor activities [\cite=muscillo2007cma], face localization [\cite=lopez2007flc], chromosome classification [\cite=legrand2007ccu], shape retrieval [\cite=TPAMI.2005.21] [\cite=marzal2006cbs], and so on. Unlike the Euclidean distance, DTW optimally aligns or "warps" the data points of two time series (see Fig. [\ref=fig:example]).

When the distance between two time series forms a metric, such as the Euclidean distance or the Hamming distance, several indexing or search techniques have been proposed [\cite=502808] [\cite=1221301] [\cite=958948] [\cite=230528] [\cite=1221193]. However, even assuming that we have a metric, Weber et al. have shown that the performance of any indexing scheme degrades to that of a sequential scan, when there are more than a few dimensions [\cite=671192]. Otherwise--when the distance is not a metric or that the number of dimensions is too large--we use bounding techniques such as the Generic multimedia object indexing (GEMINI) [\cite=faloutsos1996smd]. We quickly discard (most) false positives by computing a lower bound.

Ratanamahatana and Keogh [\cite=ratanamahatana2005tmd] argue that their lower bound (LB_Keogh) cannot be improved upon. To make their point, they report that LB_Keogh allows them to prune out over 90% of all DTW computations on several data sets.

We are able to improve upon LB_Keogh as follows. The first step of our two-pass approach is LB_Keogh itself. If this first lower bound is sufficient to discard the candidate, then the computation terminates and the next candidate is considered. Otherwise, we process the time series a second time to increase the lower bound. If this second lower bound is large enough, the candidate is pruned, otherwise we compute the full DTW. We show experimentally that the two-pass approach can be several times faster.

The paper is organized as follows. In Section [\ref=sec:dtw], we define the DTW in a generic manner as the minimization of the lp norm ([formula]). In Section [\ref=sec:properties], we present various secondary mathematical results. Among other things, we show that if x and y are separated by a constant (x  ≥  c  ≥  y or x  ≤  c  ≤  y) then the [formula] is the l1 norm (see Proposition [\ref=prof:band1]). In Section [\ref=sec:ti], we derive a tight triangle inequality for the DTW. In Section [\ref=sec:whichmeasure], we show that [formula] is good choice for time-series classification. In Section [\ref=sec:definingUL], we compute generic lower bounds on the DTW and their approximation errors using warping envelopes. In Section [\ref=sec:envelopes], we show how to compute the warping envelopes quickly and derive some of their mathematical properties. The next two sections introduce LB_Keogh and LB_Improved respectively, whereas the last section presents an experimental comparison.

Conventions

Time series are arrays of values measured at certain times. For simplicity, we assume a regular sampling rate so that time series are generic arrays of floating-point values. A time series x has length |x|. Time series have length n and are indexed from 1 to n. The lp norm of x is [formula] for any integer 0 < p <   ∞   and ||x||∞  =   max i|xi|. The lp distance between x and y is ||x  -  y||p and it satisfies the triangle inequality ||x  -  z||p  ≤  ||x  -  y||p  +  ||y  -  z||p for 1  ≤  p  ≤    ∞  . Other conventions are summarized in Table [\ref=tab:commonnot].

Related Works

Beside DTW, several similarity metrics have been proposed including the directed and general Hausdorff distance, Pearson's correlation, nonlinear elastic matching distance [\cite=veltkamp2001sms], Edit distance with Real Penalty (ERP) [\cite=chen:mln], Needleman-Wunsch similarity [\cite=needleman1970gma], Smith-Waterman similarity [\cite=smith1981icm], and SimilB [\cite=1340465].

Dimensionality reduction, such as piecewise constant [\cite=keogh2005eid] or piecewise linear [\cite=xiao2004nst] [\cite=shou2005] [\cite=dong2006lsd] segmentation, can speed up retrieval under DTW distance. These techniques can be coupled with other optimization techniques [\cite=Sakurai2005].

The performance of lower bounds can be further improved if one uses early abandoning [\cite=1106385] to cancel the computation of the lower bound as soon as the error is too large. Boundary-based lower-bound functions sometimes outperform LB_Keogh [\cite=zhou2007bbl]. Zhu and Shasha showed that computing a warping envelope prior to applying dimensionality reduction results in a tighter lower bound [\cite=zhu2003wie]. We can also quantize [\cite=1166502] or cluster [\cite=keogh2006lse] the time series.

Dynamic Time Warping

A many-to-many matching between the data points in time series x and the data point in time series y matches every data point xi in x with at least one data point yj in y, and every data point in y with at least a data point in x. The set of matches (i,j) forms a warping path Γ. We define the DTW as the minimization of the lp norm of the differences {xi - yj}(i,j)∈Γ over all warping paths. A warping path is minimal if there is no subset Γ' of Γ forming a warping path: for simplicity we require all warping paths to be minimal.

In computing the DTW distance, we commonly require the warping to remain local. For time series x and y, we do not align values xi and yj if |i - j|  >  w for some locality constraint w  ≥  0 [\cite=sakoe1978dpa]. When w = 0, the DTW becomes the lp distance whereas when w  ≥  n, the DTW has no locality constraint. The value of the DTW diminishes monotonically as w increases.

Other than locality, DTW can be monotonic: if we align value xi with value yj, then we cannot align value xi + 1 with a value appearing before yj (yj' for j' < j).

We note the DTW distance between x and y using the lp norm as [formula] when it is monotonic and as [formula] when monotonicity is not required.

By dynamic programming, the monotonic DTW requires O(wn) time. A typical value of w is n / 10 [\cite=ratanamahatana2005tmd] so that the DTW is in O(n2). To compute the DTW, we use the following recursive formula. Given an array x, we write the suffix starting at position i, [formula]. The symbol [formula] is the exclusive or. Write [formula] so that [formula], then

[formula]

For p =   ∞  , we rewrite the preceding recursive formula with [formula], and qi,j  =   max (|xi - yj|, min (qi + 1,j,qi,j + 1,qi + 1,j + 1)) when |x(i)|  ≠  0, |y(j)|  ≠  0, and |i - j|  ≤  w.

We can compute [formula] without time constraint in O(n log n) [\cite=1275562]: if the values of the time series are already sorted, the computation is in O(n) time.

We can express the solution of the DTW problem as an alignment of the two initial time series (such as x = 0,1,1,0 and y = 0,1,0,0) where some of the values are repeated (such as x' = 0,1,1,0, and y' = 0,1,,0,0). If we allow non-monotonicity (NDTW), then values can also be inverted.

The non-monotonic DTW is no larger than the monotonic DTW which is itself no larger than the lp norm: [formula] for all 0 < p  ≤    ∞  .

Some Properties of Dynamic Time Warping

The DTW distance can be counterintuitive. As an example, if x,y,z are three time series such that x  ≤  y  ≤  z pointwise, then it does not follow that [formula]. Indeed, choose x = 7,0,1,0, y = 7,0,5,0, and z = 7,7,7,0, then [formula] and [formula]. Hence, we review some of the mathematical properties of the DTW.

The warping path aligns xi from time series x and yj from time series y if (i,j)∈Γ. The next proposition is a general constraint on warping paths.

Consider any two time series x and y. For any minimal warping path, if xi is aligned with yj, then either xi is aligned only with yj or yj is aligned only with xi.

Suppose that the result is not true. Then there is xk,xi and yl,yj such that xk and xi are aligned with yj, and yl and yj are aligned with xi. We can delete (k,j) from the warping path and still have a warping path. A contradiction.

Hence, we have that the cardinality of the warping path is no larger than 2n. Indeed, each match (i,j)∈Γ must be such that i or j only occurs in this match by the above proposition.

The next lemma shows that the DTW becomes the lp distance when either x or y is constant.

For any 0  <  p  ≤    ∞  , if y = c is a constant, then [formula].

When p =   ∞  , a stronger result is true: if y = x + c for some constant c, then [formula]. Indeed, [formula] which shows the result. This same result is not true for p <   ∞  : for x = 0,1,2 and y = 1,2,3, we have [formula] whereas [formula]. However, the DTW is translation invariant: p(x,z) = p(x + b,z + b) and p(x,z) = p(x + b,z + b) for any scalar b and 0 < p  ≤    ∞  .

The [formula] has the property that if the time series are value-separated, then the DTW is the l1 norm as the next proposition shows.

If x and y are such that either x  ≥  c  ≥  y or x  ≤  c  ≤  y for some constant c, then [formula].

Assume x  ≥  c  ≥  y, there exists x',y' such that x'  ≥  c  ≥  y' and [formula]. Since we also have [formula], the equality follows.

Proposition [\ref=prof:band1] does not hold for p > 1: [formula] whereas [formula].

In classical analysis, we have that n1 / p - 1 / q||x||q  ≥  ||x||p [\cite=folland84] for 1  ≤  p  <  q  ≤    ∞  . A similar results is true for the DTW and it allows us to conclude that [formula] and [formula] decrease monotonically as p increases.

For 1  ≤  p  <  q  ≤    ∞  , we have that [formula] where |x|  =  |y| = n. The result also holds for the non-monotonic DTW.

The argument is the same for the monotonic or non-monotonic DTW. Given x,y consider the two aligned (and extended) time series x',y' such that [formula]. As a consequence of Proposition [\ref=prop:warpingpathprop], we have |x'|  =  |y'|  ≤  2n. From classical analysis, we have |x'|1 / p - 1 / q||x' - y'||q  ≥  ||x' - y'||p, hence |2n|1 / p - 1 / q||x' - y'||q  ≥  ||x' - y'||p or [formula]. Since x',y' represent a valid warping path of x,y, then [formula] which concludes the proof.

The Triangle Inequality

The DTW is commonly used as a similarity measure: x and y are similar if [formula] is small. Similarity measures often define equivalence relations: A  ~  A for all A (reflexivity), A  ~  B  ⇒  B  ~  C (symmetry) and [formula] (transitivity).

The DTW is reflexive and symmetric, but it is not transitive. Indeed, consider the following time series:

[formula]

We have that [formula], [formula], [formula] for 1  ≤  p <   ∞   and w = m - 1. Hence, for ε small and n  ≫  1 / ε, we have that X  ~  Y and Y  ~  Z, but [formula]. This example proves the following lemma.

For 1  ≤  p  <    ∞   and w > 0, neither [formula] nor [formula] satisfies a triangle inequality of the form d(x,y) + d(y,z)  ≥  cd(x,z) where c is independent of the length of the time series and of the locality constraint.

This theoretical result is somewhat at odd with practical experience. Casacuberta et al. found no triangle inequality violation in about 15 million triplets of voice recordings [\cite=casacuberta1987mpd]. To determine whether we could expect violations of the triangle inequality in practice, we ran the following experiment. We used 3 types of 100-sample time series: white-noise times series defined by xi  =  N(0,1) where N is the normal distribution, random-walk time series defined by xi  =  xi - 1 + N(0,1) and x1 = 0, and the Cylinder-Bell-Funnel time series proposed by Saito [\cite=921732]. For each type, we generated 100 000 triples of time series x,y,z and we computed the histogram of the function

[formula]

for p = 1 and p = 2. The DTW is computed without time constraints. Over the white-noise and Cylinder-Bell-Funnel time series, we failed to find a single violation of the triangle inequality: a triple x,y,z for which C(x,y,z)  >  1. However, for the random-walk time series, we found that 20% and 15% of the triples violated the triangle inequality for [formula] and [formula].

The DTW satisfies a weak triangle inequality as the next theorem shows.

Given any 3 same-length time series x,y,z and 1  ≤  p  ≤    ∞  , we have

[formula]

where w is the locality constraint. The result also holds for the non-monotonic DTW.

Let Γ and Γ' be minimal warping paths between x and y and between y and z. Let [formula]. Iterate through the tuples (i,j,k) in Γ'' and construct the same-length time series x'',y'',z'' from xi, yj, and zk. By the locality constraint any match (i,j)∈Γ corresponds to at most min (2w + 1,n) tuples of the form (i,j,  ·  )∈Γ'', and similarly for any match (j,k)∈Γ'. Assume 1  ≤  p <   ∞  . We have that [formula] and [formula]. By the triangle inequality in lp, we have

[formula]

For p =   ∞  , [formula] and [formula], thus proving the result by the triangle inequality over l∞. The proof is the same for the non-monotonic DTW.

The constant min (2w + 1,n)1 / p is tight. Consider the example with time series X,Y,Z presented before Lemma [\ref=lemma:ti]. We have [formula] and [formula]. Therefore, we have

[formula]

A consequence of this theorem is that [formula] satisfies the traditional triangle inequality.

The triangle inequality d(x,y) + d(y,z)  ≥  d(x,z) holds for [formula] and [formula].

Hence the [formula] is a pseudometric: it is a metric over equivalence classes defined by x  ~  y if and only if [formula]. When no locality constraint is enforced, [formula] is equivalent to the discrete Fréchet distance [\cite=eitermannila94].

Which is the Best Distance Measure?

The DTW can be seen as the minimization of the lp distance under warping. Which p should we choose? Legrand et al. reported best results for chromosome classification using [formula] [\cite=legrand2007ccu] as opposed to using [formula]. However, they did not quantify the benefits of [formula]. Morse and Patel reported similar results with both [formula] and [formula] [\cite=morse2007eaa].

While they do not consider the DTW, Aggarwal et al. [\cite=656414] argue that out of the usual lp norms, only the l1 norm, and to a lesser extend the l2 norm, express a qualitatively meaningful distance when there are numerous dimensions. They even report on classification-accuracy experiments where fractional lp distances such as l0.1 and l0.5 fare better. Franois et al. [\cite=francois2007cfd] made the theoretical result more precise showing that under uniformity assumptions, lesser values of p are always better.

To compare [formula], [formula], [formula] and [formula], we considered four different synthetic time-series data sets: Cylinder-Bell-Funnel [\cite=921732], Control Charts [\cite=pham1998ccp], Waveform [\cite=breiman1998car], and Wave+Noise [\cite=gonzalez2000tsc]. The time series in each data sets have lengths 128, 60, 21, and 40. The Control Charts data set has 6 classes of time series whereas the other 3 data sets have 3 classes each. For each data set, we generated various databases having a different number of instances per class: between 1 and 9 inclusively for Cylinder-Bell-Funnel and Control Charts, and between 1 and 99 for Waveform and Wave+Noise. For a given data set and a given number of instances, 50 different databases were generated. For each database, we generated 500 new instances chosen from a random class and we found a nearest neighbor in the database using [formula] for p = 1,2,4,  ∞   and using a time constraint of w = n / 10. When the instance is of the same class as the nearest neighbor, we considered that the classification was a success.

The average classification accuracies for the 4 data sets, and for various number of instances per class is given in Fig. [\ref=fig:classaccuracy]. The average is taken over 25 000 classification tests (50  ×  500), over 50 different databases.

Only when there are one or two instances of each class is [formula] competitive. Otherwise, the accuracy of the [formula]-based classification does not improve as we add more instances of each class. For the Waveform data set, [formula] and [formula] have comparable accuracies. For the other 3 data sets, [formula] has a better nearest-neighbor classification accuracy than [formula]. Classification with [formula] has almost always a lower accuracy than either [formula] or [formula].

Based on these results, [formula] is a good choice to classify time series whereas [formula] is a close second.

Computing Lower Bounds on the DTW

Given a time series x, define U(x)i  =   max k{xk|  |k - i|  ≤  w} and L(x)i  =   min k{xk|  |k - i|  ≤  w} for [formula]. The pair U(x) and L(x) forms the warping envelope of x (see Fig. [\ref=fig:pykeogh-4]). We leave the time constraint w implicit.

The theorem of this section has an elementary proof requiring only the following technical lemma.

If b∈[a,c] then (c - a)p  ≥  (c - b)p + (b - a)p for 1  ≤  p <   ∞  .

For p = 1, (c - b)p + (b - a)p = (c - a)p. For p > 1, by deriving (c - b)p + (b - a)p with respect to b, we can show that it is minimized when b = (c + a) / 2 and maximized when b∈{a,c}. The maximal value is (c - a)p. Hence the result.

The following theorem introduces a generic result that we use to derive two lower bounds for the DTW including the original Keogh-Ratanamahatana result [\cite=keogh2005eid].

Given two equal-length time series x and y and 1  ≤  p <   ∞  , then for any time series h satisfying xi  ≥  hi  ≥  U(y)i or xi  ≤  hi  ≤  L(y)i or hi = xi for all indexes i, we have

[formula]

For p =   ∞  , a similar result is true: [formula].

Suppose that 1  ≤  p <   ∞  . Let Γ be a warping path such that [formula]. By the constraint on h and Lemma [\ref=lemma:technical], we have that |xi - yj|p  ≥  |xi - hi|p  +  |hi  -  yj|p for any (i,j)∈Γ since hi∈[min (xi,yj), max (xi,yj)]. Hence, we have that [formula]. This proves the result since [formula]. For p =   ∞  , we have that [formula], concluding the proof.

While Theorem [\ref=thm:mainthm] defines a lower bound (||x - h||p), the next proposition shows that this lower bound must be a tight approximation as long as h is close to y in the lp norm.

Given two equal-length time series x and y, and 1  ≤  p  ≤    ∞   with h as in Theorem [\ref=thm:mainthm], we have that ||x - h||p approximates both [formula] and [formula] within ||h - y||p.

By the triangle inequality over lp, we have ||x - h||p  +  ||h - y||p  ≥  ||x - y||p. Since [formula], we have [formula], and hence [formula]. This proves the result since by Theorem [\ref=thm:mainthm], we have that [formula].

This bound on the approximation error is reasonably tight. If x and y are separated by a constant, then [formula] by Proposition [\ref=prof:band1] and [formula]. Hence, the approximation error is exactly ||h - y||1 in such instances.

Warping Envelopes

The computation of the warping envelope U(x),L(x) requires O(nw) time using the naive approach of repeatedly computing the maximum and the minimum over windows. Instead, we compute the envelope using at most 3n comparisons between data-point values [\cite=lemiremaxmin] using Algorithm [\ref=algo:mystreamingalo].

Envelopes are asymmetric in the sense that if x is in the envelope of y (L(y)  ≤  x  ≤  U(x)), it does not follow that x is in the envelope of y (L(x)  ≤  y  ≤  U(x)). For example, [formula] is in the envelope of [formula] for w > 1, but the reverse is not true. However, the next lemma shows that if x is below or above the envelope of y, then y is above or below the envelope of x.

L(x)  ≥  y is equivalent to x  ≥  U(y).

Suppose xi  <  U(y)i for some i, then there is j such that |i - j|  ≤  w and xi  <  yj, therefore L(x)j <  yj. It follows that L(x)  ≥  y implies x  ≥  U(y). The reverse implication follows similarly.

We know that L(h) is less or equal to h whereas U(h) is greater or equal to h. The next lemma shows that U(L(h)) is less or equal than h whereas L(U(h)) is greater or equal than h.

We have U(L(h))  ≤  h and L(U(h))  ≥  h for any h.

By definition, we have that L(h)j  ≤  hi whenever |i - j|  ≤  w. Hence, max j||i - j|  ≤  wL(h)j  ≤  hi which proves U(L(h))  ≤  h. The second result (L(U(h))  ≥  h) follows similarly.

Whereas L(U(h)) is greater or equal than h, the next lemma shows that U(L(U(h))) is equal to U(h).

We have U(h) = U(L(U(h))) and L(h)  =  L(U(L(h))) for any h.

By Lemma [\ref=lemma:ul2], we have L(U(h))  ≥  h, hence U(L(U(h)))  ≥  U(h). Again by Lemma [\ref=lemma:ul2], we have U(L(h'))  ≤  h' for h' = U(h) or U(L(U(h)))  ≤  U(h). Hence, U(h) = U(L(U(h))). The next result (L(h)  =  L(U(L(h)))) follows similarly.

LB_Keogh

Let H(x,y) be the projection of x on y defined as

[formula]

for [formula]. We have that H(x,y) is in the envelope of y. By Theorem [\ref=thm:mainthm] and setting h = H(x,y), we have that [formula] for 1  ≤  p <   ∞  . Write [formula] (see Fig. [\ref=fig:pykeogh-2]). The following corollary follows from Theorem [\ref=thm:mainthm] and Proposition [\ref=prop:error].

Given two equal-length time series x and y and 1  ≤  p  ≤    ∞  , then

[formula] is a lower bound to the DTW:

[formula]

the accuracy of LB_Keogh is bounded by the distance to the envelope:

[formula]

for all x.

Algorithm [\ref=algo:lbkeogh] shows how LB_Keogh can be used to find a nearest neighbor in a time series database. The computation of the envelope of the query time series is done once (see line [\ref=line:mylbkeoghenvelope]). The lower bound is computed in lines [\ref=line:mylbkeoghstart] to [\ref=line:mylbkeoghend]. If the lower bound is sufficiently large, the DTW is not computed (see line [\ref=line:mylbkeoghtest]). Ignoring the computation of the full DTW, at most (2N + 3)n comparisons between data points are required to process a database containing N time series.

LB_Improved

Write [formula] for 1  ≤  p <   ∞  . By definition, we have [formula]. Intuitively, whereas [formula] measures the distance between x and the envelope of y, [formula] measures the distance between y and the envelope of the projection of x on y (see Fig. [\ref=fig:pykeogh-3]). The next corollary shows that LB_Improved is a lower bound to the DTW.

Given two equal-length time series x and y and 1  ≤  p <   ∞  , then [formula] is a lower bound to the DTW: [formula].

Recall that [formula]. First apply Theorem [\ref=thm:mainthm]: [formula]. Apply Theorem [\ref=thm:mainthm] once more: [formula]. By substitution, we get [formula] thus proving the result.

Algo. [\ref=algo:lbimproved] shows how to apply LB_Improved as a two-step process. Initially, for each candidate c, we compute the lower bound [formula] (see lines [\ref=line:lbkeoghstart] to [\ref=line:lbkeoghend]). If this lower bound is sufficiently large, the candidate is discarded (see line [\ref=line:lbkeoghdiscard]), otherwise we add [formula] to [formula], in effect computing [formula] (see lines [\ref=line:lbimprovedstart] to [\ref=line:lbimprovedend]). If this larger lower bound is sufficiently large, the candidate is finally discarded (see line [\ref=line:lbimproveddiscard]). Otherwise, we compute the full DTW. If α is the fraction of candidates pruned by LB_Keogh, at most (2N + 3)n + 5(1 - α)Nn comparisons between data points are required to process a database containing N time series.

Comparing LB_Keogh and LB_Improved

In this section, we benchmark Algorithms [\ref=algo:lbkeogh] and [\ref=algo:lbimproved]. We know that the LB_Improved approach has at least the pruning power of the LB_Keogh-based approach, but does more pruning translate into a faster nearest-neighbor retrieval under the DTW distance?

We implemented the algorithms in C++ and called the functions from Python scripts. We used the GNU GCC 4.0.2 compiler on an Apple Mac Pro, having two Intel Xeon dual-core processors running at 2.66 GHz with 2 GiB of RAM. All data was loaded in memory before the experiments, and no thrashing was observed. We measured the wall clock total time. In all experiments, we benchmark nearest-neighbor retrieval under the [formula] with the locality constraint w set at 10% (w = n / 10). To ensure reproducibility, our source code is freely available [\cite=googlelbimproved], including the script used to generate synthetic data sets. We compute the full DTW using a straight-forward O(n2)-time dynamic programming algorithm.

Synthetic data sets

We tested our algorithms using the Cylinder-Bell-Funnel [\cite=921732] and Control Charts [\cite=pham1998ccp] data sets, as well as over a database of random walks. We generated 1 000-sample random-walk time series using the formula xi  =  xi - 1 + N(0,1) and x1 = 0. Results for the Waveform and Wave+Noise data sets are similar and omitted.

For each data set, we generated a database of 10 000 time series by adding randomly chosen items. The order of the candidates is thus random. Fig. [\ref=fig:perf1], [\ref=fig:perf2] and [\ref=fig:perf3] show the average timings and pruning ratio averaged over 20 queries based on randomly chosen time series as we consider larger and large fraction of the database. LB_Improved prunes between 2 and 4 times more candidates and it is faster by a factor between 1.5 and 3.

Shape data sets

For the rest of the section, we considered a large collection of time-series derived from shapes [\cite=keogh2006lse] [\cite=keoghshapedata]. The first data set is made of heterogeneous shapes which resulted in 5 844 1 024-sample times series. The second data set is an arrow-head data set with of 15 000 251-sample time series. We shuffled randomly each data set so that candidates appear in random order. We extracted 50 time series from each data set, and we present the average nearest-neighbor retrieval times and pruning power as we consider various fractions of each database (see Fig. [\ref=fig:perf4] and [\ref=fig:perf5]). The results are similar: LB_Improved has twice the pruning power and is faster by a factor of 3.

Conclusion

We have shown that a two-pass pruning technique can improve the retrieval speed by up to three times in several time-series databases. We do not use more memory.

We expect to be able to significantly accelerate the retrieval with parallelization. Several instances of Algo. [\ref=algo:lbimproved] can run in parallel as long as they can communicate the distance between the time series and the best candidate.

Acknowledgements

The author is supported by NSERC grant 261437 and FQRNT grant 112381.