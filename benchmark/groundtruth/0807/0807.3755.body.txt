Approximating Document Frequency with Term Count Values

{mklein, mln}

Introduction and Motivation

In information retrieval (IR) research the term frequency (TF) - inverse document frequency (IDF) concept is well known and established to extract the most significant terms while dismissing the more common terms from textual content. It also has been used to generate lexical signatures (LSs) of web pages [\cite=phelps:hyperlinks] [\cite=park:ls-tois] [\cite=harrison:just-in-time] [\cite=klein:ls] [\cite=sugiyama:refinement-of-tfidf]. Such LSs can be used to (re-)discover missing web pages when fed back into search engine interfaces. The computation of TF values for a web page is straight forward since we can simply count the occurrences for each term within the page. Two values are mandatory for the IDF computation: the overall amount of documents in the corpus and the amount of documents a term appears in. We call the second value document frequency (DF). Since both values are unknown when the entire web is the corpus, accurate IDF computation for web pages is impossible and values need to be estimated.

Various corpora containing web pages, their textual content and their in- and outlinks are available and can be used to estimate IDF values since they are considered a representative sample for the Internet [\cite=soboroff:trec]. The TREC Web Track is probably the most common corpus and has, for example, been used in [\cite=sugiyama:refinement-of-tfidf] for IDF estimation. The British National Corpus (BNC) [\cite=leech:frequencies], as another example, has been used in [\cite=staddon:inference]. Google published the N-grams [\cite=google-n-grams] in 2006 and hence provides a powerful alternative source for TC values of terms extracted from web pages from the Google index. The Web as Corpus kool ynitiative (WaCky) provides the WaC corpus as an alternative with no charge for researchers. The problem with these corpora is that they do not provide DF values for the terms (or n-term tokens) they contain. We can count the total number of documents and therefore determine the DF values in case the corpus documents are provided along with the terms. Table [\ref=tab:corpora] gives an overview of selected corpora and their characteristics. The first row indicates what kind of documents the corpus is based upon. The row Number of Documents shows the total number of documents the corpus consists of (or in the case of the Google N-grams the number of documents the corpus was generated from). This row also gives information about whether the documents of the corpus are available. As mentioned above, recognizing the document boundaries within the corpus becomes necessary when computing IDF values.

The row TC indicates whether TC values of the corpus are available. The following simple example is to illustrate the difference between TC and DF. Let us consider a corpus of 5 documents D = d1...d5 where each document contains the title of a song by The Beatles: [formula] Table [\ref=tab:tc_df_example] shows the TC and DF values of all terms occurring in our small sample corpus. We can see that the values are identical for the majority of the terms (8 out of 10). The example also shows that term processing such as stemming would have an impact on these numbers since Love and Loving are here treated as different terms.

Since we are interested in computing accurate IDF values for web page content it seems reasonable to chose a corpus that is based on textual content of web pages. Even though the TREC WT10g provides the documents and the corpus size seems sufficiently large, it has been shown to be somewhat dated [\cite=chiang:wt10g].

We are interested in using the Google N-gram dataset as a corpus to generate accurate IDF values from but unfortunately Google only provides TC values. The WaC corpus in contrast provides both, TC and DF values and therefore we can:

establish a relationship between TC and DF values within the WaC

establish a relationship between WaC based TC and Google N-gram based TC

and finally infer Google N-gram DF from point [\ref=tc-df] and point [\ref=tc-tc].

This paper presents the preliminary results of the study and the results indicate that for sufficiently sized and recently generated corpora DF values can be estimated from TC values.

Related Work

Correlation between DF and TC Values

Zhu et al. [\cite=zhu:improving_trigram] used an Internet search engine to obtain estimates for DF values of n-grams. They used these values to estimate TC values and compared those to TC values from a 103 million word Broadcast News corpus which acted as their baseline. They found that the values are very similar and thus conclude that values obtained from the web are usable to estimate TC. Keller et al. [\cite=keller:using_the_web] also used Internet search engines to obtain DF values for bigrams. Like Zhu et al. they show a high correlation between values obtained from the web and values from a given (traditional) corpus (the BNC). The main application Keller et al. suggests is for bigrams that are missing in a given corpus. Nakov et al. [\cite=nakov:using_se_page_hits] show that the n-gram count from several Internet search engines differs but is not statistically significantly different. They also show that the results from one search engine are stable over time which is encouraging for researchers using this technique to obtain TC values.

All these studies have two things in common: 1) they all show a strong correlation between DF and TC values and 2) they use DF estimates from search engines and compare it to TC values from conventional corpora. This is where our approach is different since we use TC values from well established text corpora and show the correlation to measured DF values.

Generating IDF Values for Web Pages

Sugiyama et al. [\cite=sugiyama:refinement-of-tfidf] use the TREC-9 Web Track dataset [\cite=hawking:trec] to estimate IDF values for web pages. The novel part of their work was to also include the content of hyperlinked neighboring pages in the TF-IDF calculation of a centroid page. They show that augmenting the generation of TF-IDF values with content of in-linked pages increases the retrieval accuracy more than augmenting TF-IDF values with content from out-linked pages. They claim that this method represents the web page's content more accurately and hence improves the retrieval performance.

Phelps and Wilensky [\cite=phelps:hyperlinks] proposed using the TF-IDF model to generate LSs of web pages and introduced "robust hyperlinks", an URL with a LS appended. Phelps and Wilensky conjectured if the an URL would return a HTTP 404 error, the web browser could submit the appended LS to a search engine to either find a copy of the page at a different URL or a page with similar content compared to the missing page. Phelps and Wilensky did not publish details about how they determined IDF values but stated that the mandatory figures can be taken from Internet search engines. That implies the assumption that the index of a search engine is representative for all Internet resources. However, they do not publish the value they used for the estimated total number of documents on the Internet.

Experiment Setup

The WaC corpus provides what they call a frequency list, a list of all unique terms in the corpus (lemmatized and non-lemmatized) and their TC value. Since the document boundaries in the corpus are obvious, we can compute the DF values for all terms. Since we are interested in generating TF-IDF values for web pages and feeding them back into search engines we dismiss all lemmatized terms and only use the non-lemmatized terms. We rank both lists in decreasing order of their TC and DF values and honor ties with the minimum value. For example consider four terms a, b, c, d where term a has the highest value, terms b and c have the same second highest value and term d has the lowest value. The ranking here would be a=1, b=2, c=2, d=4. This kind of ranking is also known as the sports ranking. We compute the Spearman's ρ and Kendall τ to mathematically prove the correlation. Table [\ref=tab:tc_df_20] shows the top 20 terms from the WaC corpus ordered by decreasing TC and DF values. The similarity between the two rankings already becomes visible with that small example since the intersection of both top 20 rankings just holds 22 unique terms. It is not surprising that the TC values are much greater than the DF values since for DF duplicates within one document are not counted. Since Table [\ref=tab:tc_df_20] mainly holds stop words we show terms ranked between 101 and 120 and their TC and DF values in Table [\ref=tab:tc_df_120]. The correlation is obviously less strong and the number of intersecting terms went up to 33.

Experiment Results

Correlation within the WaC Corpus

Figure [\ref=fig:wac_ties_loglog] shows (in loglog scale) the correlation of ranked terms within the WaC corpus. The x-axis represents the TC ranks of terms and the y-axis the corresponding DF rank of the same term. As expected we see the majority of the points within a diagonal corridor which indicates a great similarity between the rankings.

Figures [\ref=fig:spear_kend_corr_norm] and [\ref=fig:spear_kend_corr_log] show the measured and estimated correlation between TC and DF values in the WaC dataset. The solid black line displays Spearman's ρ. The increasing size of the dataset is shown on the x-axis. The value for ρ at any size of the dataset is beyond 0.8 which indicates a very strong correlation between the rankings. The results are statistically significant with a p-value of [formula]. The blue line in both Figures shows the computed Kendall τ values for the top 1,000,000 ranks and the dotted red line represents the estimated values for the remaining set of data in the WaC corpus. Since the computed τ values are hard to read on a normal scale (Figure [\ref=fig:spear_kend_corr_norm]) we plotted the same graph in semi-log scale in Figure [\ref=fig:spear_kend_corr_log]. The computed τ values vary between 0.82 and 0.74 and the estimated values have a minimum of 0.66.

We did not compute τ for greater ranks since it is a very time consuming operation and the estimated values also indicate a strong correlation. Gilpin [\cite=gilpin_kend_spear_table] provides a table for converting τ into ρ values. We use this data to estimate our τ values. Even though the data in [\cite=gilpin_kend_spear_table] is based on τ values computed from a dataset with bivariate normal population (which we do not believe to have in the WaC dataset), it supports our measured values. For example, it shows that a τ value of 0.8 can be converted to a ρ of 0.94 which is consistent with our measured values shown in Figure [\ref=fig:spear_kend_corr_norm]. Therefore we can predict the high τ values even beyond the top 1,000,000 ranks shown in Figure [\ref=fig:spear_kend_corr_log].

Computation Time for Kendall τ

Figure [\ref=fig:kendall_time] shows the measured and predicted computation time (y-axis, in seconds) for τ of top n rankings (x-axis). The black solid line shows the measured time values for rankings up to the top 1,000,000 terms. The red dashed line represents the predicted time values for the entire corpus and (in the small plot in the left top corner) for the top 1,000,000 ranks. Figure [\ref=fig:kendall_time] shows the observed complexity of O(n2). For the entire WaC dataset (over 11 million unique terms) we estimate a computation time for Kendall τ of almost 11 million seconds or more than 126 days which is clearly beyond a reasonable computation time for a correlation value. Kendall τ was computed using an off-the-shelf correlation function as part of the R-Project, an open source environment for statistical computing. The software (version 2.6) was run on a Dell Server with a Pentium P4 2.8Ghz CPU and 1 GB of memory.

Term Count - Document Frequency Ratio in the WaC Corpus

Another interesting way to show the correlation between TC and DF values is simply looking at the ratio of the two values. Figure [\ref=fig:ratio_02] shows the distribution of TC/DF ratios with values rounded after the second decimal and Figure [\ref=fig:ratio_01] shows the ratios rounded after the first decimal. It becomes obvious that the vast majority of the ratio values are very small. The visual impression is supported by the computed mean value of 1.23 with a standard deviation of σ = 1.21 for both, Figure [\ref=fig:ratio_02] and [\ref=fig:ratio_01]. The median of ratios is 1.00 and 1.0 respectively. Figure [\ref=fig:ratio_int] shows the distribution of TC/DF ratios rounded as integer values. It is consistent with the pattern of Figures [\ref=fig:ratio_02] and [\ref=fig:ratio_01] and the mean value is equally low at 1.23 (σ = 1.22). The median here is also 1. Figure [\ref=fig:ratio] together with the computed mean and median values accounts for another solid indicator for the strong correlation between TC and DF values within the corpus.

Correlation between the WaC and the N-gram Corpus

The TC values for both corpora, WaC and N-gram, are available and therefore we investigate their correlation. Figure [\ref=fig:tc_freqs] displays (in loglog scale) the frequencies of unique TC values in both corpora. The graph shows the TC threshold of 200 Google applied while creating the N-gram. By visual observation it becomes obvious that the distribution of TC values in both corpora is very similar. Just the size of the Google N-gram corpus is responsible for the offset between the graphs.

Conclusion

We have shown a very strong correlation between the TC and DF values within the WaC corpus with Spearman's [formula] ([formula]). This result leads us to the conclusion that the two values can be used interchangeably and therefore TC values are usable for the generation of accurate IDF values. We also show (by visual observation) a high correlation between the TC values of the WaC and of the N-gram datasets. We can now claim that, despite the fact that the Google N-gram dataset does not contain DF values, the corpus and its TC values are also usable for accurate IDF computation which can lead to the generation of LSs of web pages.

Acknowledgements

We thank the Linguistic Data Consortium, University of Pennsylvania and Google, Inc. for providing the "Web 1T 5-gram Version 1" dataset. We also thank the WaCky community for providing the ukWaC dataset. Further we would like to thank Thorsten Brants from Google Inc. for promptly answering our emails and helping to clarify questions on the Google N-gram corpus.