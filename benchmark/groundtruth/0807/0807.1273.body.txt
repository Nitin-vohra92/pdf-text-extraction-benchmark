Multiple Extremal Eigenpairs of Very Large Matrices by Monte Carlo Simulation

A common problem in computational physics is computing the eigenpairs of large matrices. We will present a new Monte Carlo algorithm that allows the simultaneous determination of a few extremal eigenpairs of a very large matrix without the need to orthogonalize pairs of vectors to each other or store all the components of any vector.

The new algorithm is an extension of the power (projection) method [\cite=wilkinson], which is the traditional starting point for a Monte Carlo determination of the eigenpair associated with the eigenvalues of largest absolute value λ1. While various Monte Carlo versions of the power method often compute this dominant eigenvalue very well, computing subdominant eigenvalues [formula] has often proven much more difficult and is much less frequently attempted [\cite=hammond]. Our Monte Carlo power method computes multiple extremal eigenpairs simultaneously and straightforwardly. For the particular algorithm presented we also introduce a new sampling method, the sewing method, that does a large state space sampling as a succession of small state space samplings. Although our new algorithm is extendable to finding more than two extremal eigenpairs, here we will focus on finding just λ1 and λ2.

Our ultimate targets are matrices so large that they are unassailable deterministically because no single vector can be stored in memory. As the system size increases, finding a few extremal eigenpairs of the transfer matrix of the two-dimensional Ising model becomes such a problem. Its two extremal eigenvalues are also of significant physical interest: the logarithm of λ1 is proportional to the free energy, and the ratio λ2  /  λ1 controls long range spin correlations near the critical point [\cite=thompson] [\cite=onsager]. Onsager [\cite=onsager] derived exact expressions the eigenvalues of this transfer matrix for any finite-sized system thereby providing a nearly unprecedented opportunity for non-trivially benchmarking our algorithm for exceptionally large systems. We comment however that there is no a priori restriction of our algorithm to problems in classical statistical mechanics. It is also applicable to transfer matrices of quantum origin and to ground and low lying excited states of many Hamiltonian matrices. In addition it has application to more widely diverse problems as the nuclear critically problem [\cite=lux]. We also note that our extension of the power method is not limited to a Monte Carlo implementation [\cite=gubernatis].

The transfer matrix A(σ,σ') of an m  ×  m Ising model with periodic boundary conditions in one direction and open boundary conditions in the other is a 2m  ×  2m matrix whose elements are [\cite=thompson]

[formula]

with ν = J / kBT, J is the exchange constant, kB being Boltzmann's constant, and T equal to the temperature. The Ising spin variable μk has the value of   ±  1, μm  +  1  =  μ1, and the symbol [formula] denotes a configuration of Ising spins. (There are 2m possible configurations .) Numerically, we represented a σ by the first m bits of integers ranging from 0 to 2m - 1. We comment that all the elements of A(σ,σ') are greater than zero so the matrix is maximally dense, and because of the one open boundary, it is also asymmetric. From the Perron-Frobenius Theorem [\cite=wilf] we have that a dominant eigenvalue that is real and positive. Further, all components of the corresponding eigenstate are real and have the same sign.

The power method [\cite=wilkinson] for some real-valued M  ×  M matrix A, not necessarily symmetric, is an iterative procedure, started with some ψ, normalized in a convenient but otherwise relatively arbitrary, manner, that cycles the two steps

[formula]

until some convergence criterion is met. If (λi,ψi) are eigenpairs of A, then starting with some

[formula]

and specifying [formula], we find after n iterations that

[formula]

Accordingly, [formula] and [formula] as n  →    ∞  .

Finding more than one eigenpair by the power method requires initializing the method with more than one starting point [\cite=wilkinson] [\cite=golub]. For methods of which we are aware, these starting points need to be orthogonal, and this orthogonality needs to be maintatined, at least periodically, throughout the iteration. This is much more difficult to do in a Monte Carlo procedure than in a determinisitic one [\cite=hammond].

In developing a Monte Carlo algorithm to estimate two extremal eigenvalues, we will exploit several observations of Booth [\cite=booth1] [\cite=booth2]. He noted that for any eigenpair (λ,ψ) and for each non-zero component of the eigenvector, the eigenvalue equation Aψ  =  λψ can be rewritten as

[formula]

and that similar equations can also be written for any number of groupings of components,

[formula]

where the Ri are rules for different groupings. Here, we will exploit the fact that any two groupings, say 1 and 2, imply

[formula]

This directly follows from ([\ref=eq:groupings]).

As do standard procedures for finding for just the two extremal eigenvalues, we also use two normalized, starting points [formula] and [formula]  but they need not necessarily be orthogonal [\cite=booth1] [\cite=booth2] [\cite=gubernatis]. At each step of the power method, we apply A to them individually; however, to prevent both from projecting to the same dominant eigenfunction, we adjust at each step the relationship between their sum to direct one to the dominant state and the other to the next dominant one. We do this in the following way: we start the iteration with ψ  =  ψ' + ηψ''. If at the nth step, ψ' and ψ'' have iterated to ψ̂' and ψ̂'', then at the (n + 1)th step, we use ([\ref=eq:groupings]) and ([\ref=eq:cross_product])

[formula]

to obtain

[formula]

The algorithm thus is to apply A repeatedly to ψ' and ψ''. If two real solutions η1 and η2 of ([\ref=eq:quadratic]) exist, then update via

[formula]

otherwise via

[formula]

After the η's become real, η1 guides further iterations to (λ1,ψ1); η2, to (λ2,ψ2). The eigenvalues are estimated from

[formula]

where η1 and η2 generate the largest and next largest eigenvalue estimates. A justification of this procedure is given in [\cite=gubernatis].

The Monte Carlo method is used to estimate the result of the repeated matrix-vector multiplication. In the basis defining the matrix elements of A, we write

[formula]

and call the amplitudes ω'i and ω''i weights even though they are not necessarily all positive nor are the sums of their absolute values unity. We assume that the elements of the M  ×  M matrix A are easily generated on-the-fly as opposed to being stored. Next, we imagine we have N particles distributed over the M basis states and interpret Aij as the weight of particles arriving in state |i〉 on iteration n + 1 per unit weight of a particle in state |j〉 on iteration n and will regard the action of A on a ψ as causing a particle to jump from some |j〉 to some |i〉, carrying its current weight ωj, modified by Aij, to state |i〉. To do this, we define the total weight leaving state |j〉 as

[formula]

and the transition probability from |j〉 to |i〉 as

[formula]

Instead of always (i.e., with probability 1) moving weight Aij from state |j〉 to state |i〉, we will instead sample a |i〉 from Tij and multiply the transferred weight by Wj

For many Monte Carlo simulations, as is the case for the transfer matrix of the Ising model, the particle weights defining the eigenvector associated with the largest eigenvalue can be made all positive. The second eigenfunction however must be represented by some particles of negative weight and some particles of positive weight. For some jumps these negative and positive weights must at least partially cancel to maintain a correct estimation of the second eigenfunction. When N  ≪  M, as is typical, this cancellation does not occur often enough in a Monte Carlo simulation without proper design.

There are several ways to design the cancellation [\cite=booth1]. For our Ising simulations, we promoted this cancellation by sorting the particles into state order (a state is represented by the bits of an integer) at the end of each iteration. Particles 1 and 2 are then sampled together according to the Arnow et al. scheme [\cite=arnow], then particles 3 and 4, and so forth. An ordered list means there are (typically) many nearby states |i〉 accessible from both particles [formula] and [formula] with nontrivial transition probabilities.

As the iteration progresses, the absolute value of the weights of some particles becomes very large, and those of some others, very small. As standard for Monte Carlo methods with weighted particles, we stochastically eliminated particles with weights of small magnitude and stochastically split those with large magnitudes. To do this we used a procedure called the comb [\cite=comb1].

The steps of the algorithm are: First, we initialize the weights of two vectors. For the Ising simulation, we selected ωi' uniformly and randomly over the interval (0,1) and the ωi'' uniformly and randomly over the interval (-1,1). Then, for a fixed number of times we iterate. For each iteration we execute the jump procedure for each particle, place the particle list in state order, effect cancellations, estimate the eigenvalues from ([\ref=eq:eigenvalues]), update ψ' and ψ'', and then comb.

If M is sufficiently small so we can store all components of our vectors, sampling from the cumulative probability [formula] works well. If the number of states gets too large (e.g., m > 12), then Ci cannot be sampled directly because it cannot fit in the computer's memory. In this case, we could just randomly pick from any state |j〉 any state |i〉 with probability 1 / M instead of always picking a state |i〉 (i.e., with probability 1). The problem with this approach is that the Aij can have immense variation so that this simple sampling scheme is unlikely to work well as a Monte Carlo method. This situation is especially true for the Ising problem. A large part of such variations however can be removed by sampling the new state in stages and then sewing the stages together.

To explain our sewing procedure, we will first assume that we can write any state |i〉 in our basis as a direct product of the states in a smaller basis, [formula]. Instead of transferring weight Wj from state |j〉 to state |i〉 with probability Tij, we will use the aij that would apply to the smaller set of states and then make an appropriate weight correction. For the Ising model, aij is the transfer matrix of a smaller lattice size.

For each smaller set of states, we rewrite the analogous transition probability from state |j〉 to state |i〉 as

[formula]

and the analogous weight multiplier as

[formula]

We thus will sample |i1〉 and |i2〉 from the probability function

[formula]

The weight correction Cij, necessary to preserve the expected weight transfer from state |j〉 to |i〉, satisfies

[formula]

Thus

[formula]

The sewing method generalizes easily. For k sets of states, ([\ref=eq:transfer_mtrx]) and ([\ref=eq:weight_correction]) become

[formula]

with

[formula]

For the Ising problem, we took for state |i1〉 first m / k bits of the integer representing |i〉; for |i2〉, the second set; etc. The weight correction becomes

[formula]

where apart form a factor of ν, Di is the energy difference between calculating with the bits together and the bits separately. It is straightforward to calcualte.

Using this sewing algorithm for the sampling of states, we computed the first and second eigenvalues for m = 16 to m = 48 Ising models by sewing 6 sets of 8 bits. We note that 248  ≈  2.8  ×  1014. The results are shown in Table [\ref=table:2]. To compute averages and standard errors for each size, we executed twenty independent Monte Carlo runs, each with 1 million particles per iteration (5 million for m = 48), 500 iterations per run and used only the second half of the iterations in each run for the estimation process. ν = 0.4406867935097715 [\cite=thompson] [\cite=onsager], the value at the critical temperature. Presented are 3 systems sizes. For each we give the values of λ1 and λ2 predicted from Onsager's expression and from our enhanced power method. We see that our Monte Carlo produced eigenvalues agree very well with Onsager's predictions. For our eigenvalue estimates, R1 consisted of the states for which more than half of its m bits were 0's and R2 consisted of the states for which more than half of its m bits were 1's. More results and algorithmic detals will be given elsewhere [\cite=booth3].

We anticipate the sewing algorithm being applicable to other many-body problems defined on a lattice. Likely, these applications will require more sophisticated programming than for the Ising model. The "sewing" method for the Ising model worked well as high as m = 60, sewing together 6 sets of 10 bits. For m > 60, our computer codes would need significant modification to implement a more flexible scalable procedure for representing a state configuration requiring more than a single computer word. We do not know how large an m can be accommodated with a better computer program.

The transfer matrix of the Ising model is real, positive, asymmetric, and dense. How is our algorithm changed if a matrix lacks one or more of these properties? For simple test cases, we have successfully constructed deterministic procedures for matrices whose elements are complex valued. Also, in this context, we have had success for real asymmetric matrices whose eigenvalues are complex valued. Devising Monte Carlo algorithms for real, symmetric, sparse matrices has however received more of our attention [\cite=gubernatis2]. To find the eigenvalue of smallest size, if it not the one with the largest absolute value, one simply uses a shifted matrix, A  →  A - σI.

In closing, we believe that our new algorithm is accurate, easy to implement, and applicable to many other problems. Wider use of the algorithm will define more crisply its strengths and limitations than is possible by just the present application. The intent of the present application was benchmarking and not studying the scaling of the eigenvalues of the transfer matrix of the two-dimensional Ising model. Both deterministic and Monte Carlo power methods have been used for such studies. Deterministic methods [\cite=richards93] have computed λ1 and λ2, while Monte Carlo methods [\cite=examples], just λ1. The system sizes were considerably smaller (m  ≤  25 deterministically and m  ≤  21 stochastically) than the largest size (m = 48) presented here. This size should not be the largest accessible by our methods. All our calculations were done on a single processor.