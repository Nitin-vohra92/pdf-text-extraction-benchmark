Random Walks in Local Dynamics of Network Losses

Many systems, both natural and man-made are organized as complex networks of interconnected entities: brain cells [\cite=Arbib:01], interacting molecules in living cells [\cite=Jeong:00], multi-species food webs [\cite=Cohen:90], social networks [\cite=Liljeros:01] and the Internet [\cite=Pastor:01] are just a few examples. In addition to the classical Erdös-Rényi model for random networks [\cite=ErdRen], new overarching models of scale-free [\cite=Barabasi:99] or small-world [\cite=Watts:98] networks turn out to be describe real world examples. These and other network models have received extensive attention by physicists (see Refs. [\cite=Watts:99] [\cite=Albert:02] for reviews).

One of particularly interesting problems for a wide range of complex networks is their resiliency to breakdowns. The possibility of random or intentional breakdowns of the entire network has been considered in the context of scale-free networks where nodes were randomly or selectively removed [\cite=Albert:00] [\cite=Cohen:00] [\cite=Braunstein:03], or in the context of small-world networks where a random reduction in the sites connectivity leads to a sharp increase in the optimal distance across network which destroys its small-world nature [\cite=Braunstein:03] [\cite=Dorogovtsev:00] [\cite=Ashton:05]. In all these models, the site or bond disorder acts as an input which makes them very general and applicable to a wide variety of networks.

Network breakdowns can result not only from a physical loss of connectivity but from an operational failure of some network nodes to forward data. In the more specific class of communication networks, this could happen due to excessive loading of a single node. This could trigger cascades of failures and thus isolate large parts of the network [\cite=Moreno:03]. In describing the operational failure in a particular network node, one needs to account for distinct features of the dynamically 'random' data traffic which can be a reason for such a breakdown.

In this Letter we model data losses in a single node of a packet-switched network like the Internet. We demonstrate that such losses may have critical behavior with an abrupt transition from an exponentially small to finite loss rate as the data arrival rate increases. At the critical point the loss rate exhibits strong fluctuations which only become Gaussian in the (unrealistically) long time limit. Although we model data arrivals as a Markovian process, the loss rate at intermediate times shows long-range power-law correlations in time. When excessive data losses start, it is more probable that they persist for a while, thus impacting on network operation.

There are two distinct features which must be preserved in modelling data losses in a packet-switched network: a discrete character of data propagation and the possibility of data overflow in a single node. In such a network, data is divided into packets which are routed from source to destination via a set of interconnected nodes (routers). At each node packets are queued in a memory buffer before being served, i.e. forwarded to the next node. (There are separate buffers for incoming and outgoing packets but we neglect this for the sake of simplicity). Due to the finite capacity of memory buffers and the stochastic nature of data traffic, any buffer can become overflown which results in discarded packets.

In the model we suggest here data losses in a single memory buffer start when the average rate of random packets arrival exceeds the service rate. In such a model the transition from free flow to lossy behavior is, on average, very steep: when the arrival rate exceeds a certain threshold, the buffer becomes full and a finite fraction of arriving packets is dropped. Such a sharp onset of network congestion is familiar to everyone using the Internet and was numerically confirmed in different models [\cite=Ohira:98]. Here we stress two characteristic consequences of the model considered which would be preserved in any realistic model allowing for the discrete data propagation and finite capacity of the nodes: (i) congestion can originate from a single node and (ii) loss rate statistics turns out to be highly nontrivial. The latter makes present considerations qualitatively different from, e.g., bulk queue models which have been extensively studied before [\cite=Cohen:69] [\cite=Schwartz:87] but considered loss rate only on average. Although fluctuations in network dynamics were studied in [\cite=Menezes], this was done in a continuous limit for the data traffic. In our model there is a more close analogy with mesoscopic physics where, e.g., the electron density of states in disordered conductors is, on average, a constant, but its fluctuations are rather nontrivial, either globally or locally [\cite=AS].

We consider the model of randomly arriving packets which form a queue in the buffer and are served at regular, discrete time intervals. The length of the queue after n service intervals, [formula] serves as a dynamical variable. The random arrival is modelled as a telegraph noise described by the discrete-time Langevin equation,

[formula]

where the simplest model for noise ξn is built by assuming that all incoming packets are of the same length and arrive one by one at each service interval with probability p. We further assume that one half of a packet is served at each interval on a first-come first-served basis. Let L be the buffer capacity, i.e. the maximal number of service units (half-packets) in the queue. This gives

[formula]

with probability p, and

[formula]

with probability 1 - p. The meaning of the above conditions is that either the length of the queue increases by one service unit when one packet arrives and one service unit is served, or decreases by one when no new packet arrives. The approximate boundary conditions above correspond to discarding a newly arrived packet when buffer is full ([formula]) and an idle interval when no packet arrives at an empty buffer ([formula]). We show at the end of the Letter that more accurate boundary conditions do not change the asymptotic form of our results.

The main quantity which characterizes congestion is the packet loss rate which is defined via the number of packets discarded during a time interval N by

[formula]

The meaning of this definition is that the arriving packet is discarded when by the moment of arrival the queue was at the maximal capacity L as illustrated in Fig. [\ref=Figure_1]. Thus the continuous limit cannot be exploited for this problem which makes the loss statistics profoundly different from, e.g., the thoroughly studied statistics of first-passage time [\cite=Hughes-RandWalk:95]. We will find the average and the variance of the loss rate defined above. Although the arrival of packets defined by the Langevin dynamics of Eqs. ([\ref=L])-([\ref=2]) is a Markovian process, we will show that the loss rate dynamics turns out to be non-Markovian in the critical regime. The reason for this is that the loss rate ([\ref=loss]) is defined entirely by the process occurring at the boundary of the random walk (RW).

We will express the quantities of interest via the conditional probability of the queue being of length [formula] at time n provided that it was of length [formula] at time n0,

[formula]

where [formula] stand for the averaging over the telegraph noise of Eqs. ([\ref=L]) - ([\ref=2]). The stationary distribution of the queue length is related to G by

[formula]

On averaging the loss rate, Eq. ([\ref=loss]), we thus obtain

[formula]

while its variance is given by

[formula]

To calculate G (which is non-trivial due to the boundary conditions of Eqs. ([\ref=1]) and ([\ref=2])), we note that it is the Green's function of the Focker-Planck equation (FPE) corresponding to the Langevin equation ([\ref=L]). The FPE can be written in terms of the probability [formula] for the queue being of length [formula] at time n as

[formula]

The transition matrix ŵ with elements [formula] corresponding to Eqs. ([\ref=L])-([\ref=2]) is given by

[formula]

with the boundary conditions

[formula]

Eqs. ([\ref=P])-([\ref=w]) describe a usual biased discrete-time RW on a one-dimensional lattice [\cite=Hughes-RandWalk:95]. However, both the quantity to calculate, Eq. ([\ref=loss]), and the boundary conditions, Eq. ([\ref=w0]), make the problem under consideration profoundly different from those in [\cite=Hughes-RandWalk:95].

Eqs. ([\ref=P]) - ([\ref=w0]) are clearly non-Hermitian. This leads to different right, ψ+, and left, ψ-, eigenfunctions of the matrix ŵ (normalized by [formula]):

[formula]

where λk are the eigenvalues, labeled with a discrete 'momentum' k. Although there exists a similarity transformation which turns the problem into Hermitian (which means that all λk are real), it is convenient to keep the above representation unchanged.

The Green's function of the FPE ([\ref=P]) can immediately be expressed as [formula] which gives Diagonalizing the tri-diagonal matrix ŵ defined by Eqs. ([\ref=w]) and ([\ref=w0]), one finds the eigenvalues of Eq. ([\ref=RL])

[formula]

where k = πn / (L + 1), [formula]. The appropriate eigenfunctions are given by

[formula]

The eigenfunctions corresponding to k = 0 are given by

[formula]

and the appropriate eigenvalue, λ0 = 1 is separated by a gap from the continuous (as L  →    ∞  ) spectrum of Eq. ([\ref=eigv]), unless p = 1 / 2. The RW is biased towards [formula] (full buffer and congested traffic) for p > 1 / 2, or towards [formula] (empty buffer) for p < 1 / 2. At p = 1 / 2 when the RW is unbiased and the eigenvalue spectrum is gapless, the fluctuations are strongest. In all cases, since λ0 = 1 while λk < 1 for [formula], it is the isolated solution ([\ref=0]) which governs the stationary distribution ([\ref=st]):

[formula]

Noticing that [formula] so that G1(L,L) = p, one finds the average loss rate from Eqs. ([\ref=l-av]) and ([\ref=st2]) as

[formula]

so that the loss rate is a constant of order 1 for p > 1 / 2, a small fraction of the buffer capacity for p = 1 / 2 and an exponentially vanishing fraction for p < 1 / 2. This straightforward result could be obtained directly from the Langevin description. The matching between these three asymptotic regimes takes place in a narrow region (of width [formula]) around [formula].

The result for the variance, Eq. ([\ref=l-var]), is convenient to express in terms of the 'compressibility' defined by

[formula]

From Eqs. ([\ref=l-var]) and ([\ref=R-eig]) we find

[formula]

The behavior of χ is illustrated in Fig. [\ref=Figure_2] which shows its fast increase at the critical point, p = 1 / 2. Using Eq. ([\ref=eigv]) for λk, it is easy to simplify Eq. ([\ref=L-var]). We find that a steady-state regime (when one neglects the N-dependent term in the square brackets above) is reached for N  ≫  N0 where

[formula]

In this regime, the compressibility saturates at

[formula]

Thus, the compressibility diverges at the transition point p = 1 / 2 in the thermodynamic limit, L  →    ∞   and N / L2  →    ∞  . The variance ([\ref=var-chi]) remains finite at the transition point and in the thermodynamic limit it obeys the central limit theorem.

However, at the critical point, p = 1 / 2, the steady-state regime is reachable only at unrealistically long times [formula]. In the intermediate regime, 1  ≪  N  ≪  N0, the compressibility rapidly increases with time:

[formula]

so that the variance exceeds the average value of the loss-rate and its distribution is no longer normal. More importantly, in this regime the fluctuations of the loss rate are no longer Markovian as they exhibit long-time correlations. To show this, we consider the temporal correlation function of the loss rate defined by

We obtain an exact expression for R2(N,M) similarly to that for χN, Eq. ([\ref=L-var]), omitted for brevity. In the most relevant regime, [formula] and M > N, it reduces to

[formula]

At the critical point this reduces using Eq. ([\ref=chiN]) to

[formula]

This long-time correlation (in spite of the packet arrival being Markovian) is another clear sign of criticality.

Let us note that the boundary conditions in Eq. ([\ref=w0]) correspond to simultaneous arrival and service of packets. In this case overflown packets are only partially discarded. In more realistic models the overflown packets should be discarded completely. To reflect this, we can choose one of the standard procedures: service first or packet arrival first. This is straightforward to formulate: the transition matrix remains the same in the bulk, Eq. ([\ref=w]), while changes in 3  ×  3 blocks in the boundary corners. In solving the eigenvalue problem ([\ref=RL]) the appropriate boundary layer states can be eliminated. This reduces our problem to that described by Eqs. ([\ref=w]) and ([\ref=w0]) but with a smaller number of states and different (and dependent on eigenvalues) corner elements on the main diagonal. This can be solved in a similar way as the model of Eqs. ([\ref=P])-([\ref=w0]) and the dependence on N and M turns out to be the same in the asymptotical regime of Eqs. ([\ref=chiN])-([\ref=R2]).

In conclusion, we have demonstrated that the stochastic nature of discrete data traffic in packet-switched networks (e.g., the Internet) results in a critical behavior with an abrupt transition from free to lossy operation at the level of a single node when the arrival rate reaches a certain critical value. The critical point is characterized by strong fluctuations and long-memory effects in the loss rate. This leads to an operational failure of a single node which can contribute to cascaded failures and thus congestion of large parts of the network. We intend to use the results of the present model as building blocks for describing such a congestion within the framework that accounts also for the topological disorder [\cite=Stepanenko:05].