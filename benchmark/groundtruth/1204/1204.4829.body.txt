cm 15pt =0pt

Proposition

Improved Balas and Mazzola Linearization for Quadratic 0-1 Programs with Application in a New Cutting Plane Algorithm

Keywords:  Quadratic Program, Integer program, Linearization, Cutting plane algorithm.

Introduction

In this article, we consider the generalized quadratic 0-1 program given as follows

[formula]

where B = (bij) is an n  ×  n nonnegative matrix. Without loss of generality we assume bii = 0 since biix2i = biixi. Problem (P) is a generalization of unconstrained zero-one quadratic problems, zero-one quadratic knapsack problems, quadratic assignment problems and so on. It is a classical NP-hard problem [\cite=Garey].

Linearization strategies are to reformulate the zero-one quadratic programs as equivalent mixed-integer programming problems with additional binary variables and/or continuous variables and continuous constraints, see [\cite=Chaovalitwongse] [\cite=Elloumi] [\cite=Fortet] [\cite=Glover1] [\cite=Glover2] [\cite=Glover3] [\cite=Liberti]. Recently, Sherali and Smith [\cite=Sherali] developed small linearizations for more generalized quadratic 0-1 programs. Gueyea and Michelon [\cite=Gueyea] proposed a framework for unconstrained quadratic 0-1 programs. These linearizations are standard for employing exact algorithms such as branch and bound. Balas and Mazzola proposed a small-size linearization [\cite=Balas] and then successfully applied it to devise exact or heuristic cutting plane algorithms.

In this article, we focus on new small-size tight linearizations. We first propose a primal version of Balas and Mazzola linearization (BML). By strengthening the linearization and then considering the dual model, we obtain a new linearization which improves BML. As a direct application, a new cutting plane algorithm is proposed.

This article is organized as follows. In section 2, we discuss Balas and Mazzola linearization (BML) [\cite=Balas] and the related primal linearization. In section 3, we create a new approach to obtain a tighter linearization. It improves the primal linearization of BML in the sense that the linear programming relaxation often give tighter lower bound. In section 4, we apply this dual linearization to devise cutting plane algorithm and compare the efficacy with that of BML. Concluding remarks are made in section 5.

The Primal Model of Balas and Mazzola Linearization

In this section, we show that Balas and Mazzola Linearization has a primal model.

Define a column vector u with components

[formula]

where [formula] is any suitable relaxation of X such that the problems ([\ref=u:def]) can be solved relatively easily.

We rewrite the objective function of (P) as

[formula]

Introducing n continuous variables

[formula]

we can obtain the following mixed 0-1 linear program

[formula]

where the two series inequality constraints follow from the fact [formula] and xi  ≥  0, bij  ≥  0, respectively.

Problems (P) and (PL1) are equivalent in the sense that for each optimal solution to one problem, there exists an optimal solution to the other problem having the same optimal objective value.

The proof is found in Appendix [\ref=s1].

If we restrict (P) as the quadratic assignment problem, the proposed linearization (PL1) reduces to Kaufman-Broeckx linearization [\cite=Kaufman].

Below we apply Benders' decomposition approach to Problem (P), as in [\cite=Burkard]. Firstly, (P) can be decomposed in the following way

[formula]

where

[formula]

For fixed x, we dualize the first series constraints of the problem [formula] using Lagrangian multipliers λi (i = 1,2,...n). We obtain the subproblem

[formula]

Note that the feasible solution region F of SP(x) does not depend on the chosen vector x. Let λt be the incidence vectors of the extreme points of F (which is unit hypercube in [formula]). Introducing

[formula]

we can see that Problem ([\ref=decom]) is equivalent to

[formula]

by the fact that for any fixed x, the second-stage problem [formula] of ([\ref=decom]) is a linear programming whose dual formulation is just ([\ref=spx]) and the fact that one of the optimal solutions to the linear programming problem ([\ref=spx]) is attained at an extreme point of F. Problem ([\ref=obj1]) yields now the following mixed 0-1 linear program

[formula]

In some sense, linearization (DL1) can be regarded as the dual formulation of (PL1). Above we also obtained the equivalence between (PL1) and (DL1):

Problems (PL1) and (DL1) are equivalent in the sense that for each optimal solution to one problem, there exists an optimal solution to the other problem having the same optimal objective value.

Combining Theorem [\ref=thm:1] with Theorem [\ref=thm:2], we can see (DL1) is equivalent to (P). In literature, linearization (DL1) is known as Balas and Mazzola linearization (BML) [\cite=Balas].

New Tight Primal and Dual Linearizations

In this section, we propose a new approach to establish new tight linearizations.

Define

[formula]

Let v and l be the vectors with components vi and li respectively, [formula].

Let x∈X  ⊆  {0,1}n. For all [formula],

[formula]

Therefore, the new linearization reads

[formula]

Under the linear transformations [formula], the above linearization becomes

[formula]

As a corollary of Lemma [\ref=lem:1], we have

Problems (PL2) (or (PL2')) and (P) are equivalent in the sense that for each optimal solution to one problem, there exists an optimal solution to the other problem having the same optimal objective value.

Continuously relaxing linearizations (PL1) and (PL2), i.e., replacing X with [formula], we obtain linear programming lower bounds for (P), denoted by [formula] and [formula] respectively. (PL2) is not weaker than (PL1) in the following sense.

[formula].

It is sufficient to show any feasible solution of (PL2) is also feasible in (PL1), which follows from the fact that vi  ≤  ui, xi∈[0,1] and li  ≥  0 since B = (bij) is nonnegative.

Next, we consider the dual model of (PL2). As the formulation of (PL2') is similar to that of (PL1), we immediately have the dual model based on (DL2).

[formula]

where

[formula]

Similarly to Theorem [\ref=thm:2], we have

Problems (PL2) (or (PL2') ) and (DL2) are equivalent in the sense that for each optimal solution to one problem, there exists an optimal solution to the other problem having the same optimal objective value.

Cutting Plane Algorithms Based on Dual Linearizations

We first establish cutting plane algorithm based on (DL1). As in any decomposition approach the master problem (DL1) is not solved for all restrictions [formula], but only for a subset {t| 1  ≤  t  ≤  r} of indices. We denote the restricted master problem by (DL1r).

Getting a solution [formula] for the restricted master problem, the subproblem [formula] is solved, which yields

[formula]

λ(r + 1) is an optimal solution of the subproblem [formula] because of the definition of the constants ui and the constraints [formula].

A new cut

[formula]

is added to the current (DL1r). Thus we get (DL1r + 1).

The objection function value [formula] of SP(x) is an upper bound for (P), whereas the objective function value [formula] of the master problem (DL1r) is a lower bound. If [formula], stop and return an optimal solution.

This is the flow of cutting plane algorithm. Below we show the finite convergence. The proof is found in Appendix [\ref=s2].

Assume that [formula] is an optimal solution of (DL1r) and [formula]. For any s > r, [formula] cannot be an optimal solution of (DL1s) unless it is the optimal solution of (P).

From the above lemma, we have the convergence result proved in Appendix [\ref=s3].

The above cutting plane algorithm stops in a finite number of steps and returns the optimal solution of (P).

Cutting plane algorithm based on (DL2) can be similarly devised. To compare with (DL1) conveniently, instead of (DL2), we use the following equivalent model

[formula]

Assume li  >  0 for all i = 1,2,...,n. The restricted master program (DL2r') gives a lower bound strictly better than that of (DL1r) until the cutting plane algorithm stops.

Conclusions

In this article, we focus on the generalized quadratic 0-1 program, denoted by (P). We propose a linearization (PL1) for (P) and show that it can be regarded as a dual formulation of Balas and Mazzola linearization (BML), denoted (DL1). By applying a new approach, we establish a tight linearization (PL2) of the same size. We proved (PL2) is not weaker than (PL1) in the sense that the continuous linear programming relaxation of (PL2) gives tighter lower bound than that of (PL1). The dual linearizations of (PL1) and (PL2), (DL1) and (DL2) are successfully used in devising cutting plane algorithms, respectively. We showed that the cutting plane algorithm based on (DL2) is strictly better than that of (DL1) under some weak assumptions.

Proof ~ of ~ Theorem ~ : ~

Let x be any feasible solution to (P). It is easy to verify that (x,y) is feasible in (PL1) with the same objective value, where [formula]. As a consequence, the optimal objective value of (PL1) gives a lower bound for (P). It is sufficient to show that if (x*,y*) is an optimal solution to (PL1), x* is optimal in (P) with the same objective value. We notice that [formula]. If x*i = 1, we have [formula], otherwise, x*i = 0, y*i = 0. As a conclusion, [formula] which implies [formula]. That is, x* is a feasible solution to (P) whose objective value equals a lower bound. Therefore, x* is optimal in (P) and both the optimal objective values are equal. [formula]

Proof ~ of ~ Lemma ~ : ~

Denote the optimal objective function value of any master problem (DL1s) by [formula], which is a lower bound for (P). If [formula] is also an optimal solution of (DL1s) for some s > r, it follows that

[formula]

since (DL1s) contains the constraint ([\ref=eq:0]). The left-hand side of ([\ref=eq:1]) is a lower bound for (P) while the right-hand side of ([\ref=eq:1]) corresponds to a feasible objective function value of (P), which can be shown as follows:

[formula]

Therefore ([\ref=eq:1]) holds as equality and [formula] must be the optimal solution of (P). [formula]

Proof ~ of ~ Theorem ~ : ~

If the cutting plane algorithm based on (DL2') has not stopped at step r, the optimal solution [formula] must be different from λ(t) for any 1  ≤  t  ≤  r, i.e., there exist index it such that [formula]. Then the right-hand side of (DL2r') satisfies

[formula]

for any 1  ≤  t  ≤  r. Therefore the objective function value of (DL2r') is strictly larger than that of (DL1r). [formula]