A square bias transformation: properties and applications

Research supported by the Russian Foundation for Basic Research (projects 11-01-00515a, 11-07-00112a, 11-01-12026-ofi-m) and by the grant of the President of Russia (MK-2256.2012.1).

Key words and phrases: probability transformation, zero bias transformation, size bias transformation, square bias transformation, characteristic function, L1-metric

AMS 2010 Mathematics Subject Classification: 60E10, 60E15

Introduction

Let X be a random variable (r.v.) with the distribution function (d.f.) [formula], [formula] and the characteristic function (ch.f.)

[formula]

which is the Fourier-Stieltjes transform of the d.f. F(x). As is well known, if X is nonnegative with [formula], then

[formula]

is a ch.f., and if [formula], then

[formula]

are ch.f.'s as well (see, e.g., [\cite=Lukacs1970]). The probability transformation given by [\eqref=SizeBiasChF] is called the X-size bias transformation. By a transformation of a random variable we mean that of its distribution. The X-size bias transformation was introduced by Goldstein and Rinott [\cite=GoldsteinRinott1996] for the purpose of estimation of the accuracy of the multivariate normal approximation to nonnegative random vectors under conditions of local dependence by Stein's method. Namely, in [\cite=GoldsteinRinott1996] an almost surely nonnegative r.v. X* with [formula] is said to have the X-size biased distribution if

[formula]

It is easy to see that the distribution given by [\eqref=SizeBiasDF] has the ch.f. given by [\eqref=SizeBiasChF], hence, by virtue of the uniqueness theorem, definitions [\eqref=SizeBiasChF] and [\eqref=SizeBiasDF] are equivalent. In the same paper Goldstein and Rinott also noticed that the distribution of X* may be characterized by the relation

[formula]

which should hold for all functions G such that [formula].

As regards the second transformation, if [formula], then the distribution given by the ch.f.

[formula]

where [formula], is called the X-zero biased distribution. This definition was introduced by Goldstein and Reinert in [\cite=GoldsteinReinert1997] in an equivalent form for the purpose of generalization of the size bias transformation to r.v.'s taking both positive and negative values and was inspired by the characteristic property of the mean zero normal distribution as the unique fixed point of the zero bias transformation. Namely, in [\cite=GoldsteinReinert1997] a r.v. X(z) is said to have the X-zero biased distribution if [formula] and

[formula]

for all absolutely continuous functions G for which [formula] exists. The zero biased transformation possesses the following elementary properties (most of them are noticed/proved in [\cite=GoldsteinReinert1997]):

The zero biased distribution is absolutely continuous and unimodal about zero with the probability density function

[formula]

and the ch.f.

[formula]

[formula] if and only if X has the normal distribution with zero mean [\cite=Stein1981] [\cite=GoldsteinReinert1997].

The zero biased transformation preserves symmetry.

[formula] for [formula], in particular, [formula].

If [formula], where [formula] are independent r.v.'s with zero means and [formula] so that [formula], then X(z)  =  XI + Y(z)I, where I is a random index independent of [formula] with the distribution [formula], [formula] and [formula].

The following non-trivial estimate was proved in 2009 independently by Goldstein [\cite=Goldstein2009] and Tyurin [\cite=Tyurin2009DAN] [\cite=Tyurin2009arxiv]:

[formula]

L1(X,Y) being the L1-distance between the r.v.'s X and Y,

[formula]

As regards the third transformation, given by the characteristic function

[formula]

where [formula], it is called the X-square bias transformation. It is easy to see that a r.v. [formula] has the ch.f. [formula] if and only if

[formula]

for all functions G such that [formula]. In 2007 L. Goldstein [\cite=Goldstein2007] called the distribution of a r.v. [formula] satisfying [\eqref=ShapeBiasCharacterizationG] the X-square biased distribution. In 2011 L. Chen, L. Goldstein and Q.-M. Shao [\cite=ChenGoldsteinShao2011] proved the following relation between the distribution of the zero biased X(z) and square biased [formula] distributions of a symmetric r.v. X:

[formula]

where the r.v.'s U, [formula] are independent, U having uniform distribution on

[formula]

and is independent of [formula], if and only if [formula], where Z has the standard normal distribution.

It is easy to see that the square bias transformation possesses the following elementary properties.

A r.v. [formula] has the X-square biased distribution if and only if its d.f. [formula] satisfies

[formula]

[formula] if and only if [formula], i. e. any Bernoulli distribution with symmetric atoms is a fixed point of the square bias transformation. This can be verified by noticing that the solution of the corresponding linear homogeneous differential equation f''(t) + σ2f(t) = 0 of the second order with the initial condition f(0) = 1 has the form f(t) = peiσt + (1 - p)e- iσt, [formula], being a ch.f. if and only if p∈[0,1].

Square bias transformation preserves symmetry. Indeed, if the r.v. X has a symmetric distribution, then it's ch.f. f(t) is even, and hence, [formula], i.e. the distribution of [formula] is symmetric as well.

[formula], where (X2)* has the X2-size biased distribution.

[formula] for any constant [formula].

[formula] [formula], [formula] r > 0, in particular, [formula] [formula].

Moreover, the following estimate for the L1-distance between the distributions of X and [formula] will be proved in this paper.

If [formula] [formula] and [formula] then

[formula]

moreover, for any ε > 0 there exists a distribution of a r.v. X concentrated in two points, such that [formula] [formula] [formula] and

[formula]

The existence of the square bias transformation follows from the earlier result of [\cite=Lukacs1970] mentioned above. Moreover, in 2005, Goldstein and Reinert [\cite=GoldsteinReinert2005] proved the existence of a class of transformations of probability distributions that are characterized by equations like [\eqref=ShapeBiasCharacterizationG]. Namely, the authors described a class of measurable functions [formula] that provide the existence and uniqueness of the distribution of a random variable X(T) such that

[formula]

for all m times differentiable functions [formula] with [formula]. The authors of [\cite=GoldsteinReinert2005] also noticed that this class includes the zero- and size- bias transformations respectively with m = 1, T(x) = x and m = 0, T(x) = x+. L. Goldstein [\cite=Goldstein2007] noticed that this class also includes the square bias transformation (with m = 0, T(x) = x2). However, up till now the properties of the square biased transformation have not been studied, in particular, the characteristic function of the square biased distribution and the estimate for [formula] are established in this paper for the first time.

Motivation and applications

The zero bias transformation gives an opportunity to construct an integral estimate for the proximity of a ch.f. with zero mean to the normal one with the same variance in terms of the proximity of the corresponding zero biased distribution to the original one, which might be sharper than non-integral estimates based on the Taylor formula in the neighborhood of zero. Namely, for the sake of convenience put σ2 = 1 implying [formula] by the Lyapounov inequality. Then using the elementary relations

[formula]

and the estimate for the difference of arbitrary ch.f.'s with finite first moments due to Korolev and Shevtsova [\cite=KorolevShevtsova2010SAJ]:

[formula]

where [formula], [formula], it is not difficult to conclude that

[formula]

[formula]

Finally, applying inequality [\eqref=ZeroBiasL1] to estimate L1(X,X(z)) one obtain

[formula]

for any r.v. X with [formula], [formula], [formula]. Estimate [\eqref=ChFDiffEstimZeroBiasInt1_n=1] is exact as [formula], since, as is well known, r(t)  ~  β3|t|3 / 6, and [\eqref=ChFDiffEstimZeroBiasInt1_n=1] implies that for all [formula] such that [formula] we have

[formula]

with the least possible factor 1 / 6. However, estimate [\eqref=ChFDiffEstimZeroBiasInt1_n=1] is always sharper than the power-type estimate [formula] especially for moderate (separated from zero) values of β3|t|. Note that β3|t| can be separated from zero for large enough values of [formula] even if |t| is small. Thus, the estimates for r(t) of an integral [\eqref=ChFDiffEstimZeroBiasInt1_n=1]-type form play an important role in the construction of the least possible upper moment-type bounds of the accuracy of the normal approximation which should be uniform in some classes of distributions, especially if in these classes extremal distributions have large third absolute moments. This situation is typical, for example, for the problem of optimization of the absolute constants in the Berry-Esseen-type inequalities with an improved structure (see [\cite=KorolevShevtsova2010DAN1] [\cite=KorolevShevtsova2009] [\cite=KorolevShevtsova2010SAJ] [\cite=KorolevShevtsova2010TVP] [\cite=Shevtsova2011arxiv] where a smoothing inequality is applied with the subsequent estimation of the difference |fn(t) - e- t2 / 2|, fn(t) being the ch.f. of the normalized sum of independent random variables, in terms of the difference |f(t) - e- t2 / 2|, f(t) being the ch.f. of a single r.v.) and in its non-uniform analogues for sums of independent r.v.'s that use the Berry-Esseen inequality with an improved structure (see [\cite=Gavrilenko2011] [\cite=NefedovaShevtsova2012] [\cite=GrigorievaPopov2012]), as well as in the moment-type estimates of the rate of convergence in limit theorems for compound and mixed compound Poisson distributions (where β3  →    ∞  , see [\cite=KorolevShevtsova2010DAN2] [\cite=KorolevShevtsova2010SAJ] [\cite=NefedovaShevtsova2011DAN]) which use the Berry-Esseen inequality with an improved structure as well.

The above reasoning suggests that for the moderate values of β3|t|, estimates for r(t) in the twice-integrated form might be even sharper than estimates in the once-integrated form like [\eqref=ChFDiffEstimZeroBiasInt1_n=1]. Since the ch.f. f(t) is supposed to be differentiable at least twice, it is possible to continue [\eqref=ChFDiffInt1] as

[formula]

or as

[formula]

Note that the second estimate contains the additional term [formula], but the factor es2 / 2 does not exceed the analogous factor eu2 / 2 in the first one. However, for all [formula] this additional term satisfies

[formula]

(actually, an even sharper estimate can be obtained, if inequalities [\eqref=ChFDiffSinKSh] and [\eqref=ZeroBiasL1] are used). If [formula], [formula], then for all [formula] we have

[formula]

so that

[formula]

while the first term

[formula]

should be equivalent to β3s as [formula] in order that the final integrated estimate have the exact order β3|t|3 / 6 as [formula]. Thus, it is g1(s) that determines the behavior of the final integral estimate for small values of s, and the problem of construction of the least possible bound for g1(s) is very important. Theorem [\ref=ThL1(X] [\ref=X-shape_bias)] gives an opportunity to construct such a bound. Namely, the following corollaries hold.

Let X be a r.v. with the ch.f. f(t) and [formula], [formula], [formula]. Then for all [formula]

[formula]

Let X be a r.v. with the ch.f. f(t) and [formula], [formula], [formula]. Then for all [formula]

[formula]

[formula]

Note that, as [formula], the r.-h. sides of the inequalities presented in corollary [\ref=CorChFDiffShapeBiasInt2_n=1] are equivalent to

[formula]

provided that [formula]. Thus, the estimates including the square bias transformation which are presented in corollary [\ref=CorChFDiffShapeBiasInt2_n=1] in the twice-integrated form are sharper as [formula] that the estimates in the once-integrated form which include the zero bias transformation only. So, corollary [\ref=CorChFDiffShapeBiasInt2_n=1] plays an important role in estimation of the rate of convergence in limit theorems for sums of independent random variables mentioned above. However, particular application of corollary [\ref=CorChFDiffShapeBiasInt2_n=1] is the subject of a separate investigation and will be published elsewhere.

Proof of theorem [\ref=ThL1(X] [\ref=X-shape_bias)]

As is known (see, e.g. [\cite=Zolotarev1986]), the L1-metric can be represented in terms of the ζ1-metric as

[formula]

where F1 is the set of all real-valued functions on [formula] such that [formula]. Since for any function g∈F1 we also have ( - g)∈F1, we conclude that the modulus in the definition of ζ1(X,Y) can be omitted:

[formula]

Let X be a r.v. with the d.f. F(x) and [formula], [formula], [formula], [formula] have X-square biased distribution, i.e. the d.f. [formula] of the r.v. [formula] satisfying the relation [formula], [formula]. Then

[formula]

For g∈F1 denote

[formula]

Then

[formula]

and the statement of the theorem is equivalent to

[formula]

where the supremum sup F is taken over all d.f.'s F of the r.v. X satisfying two moment-type conditions: [formula], [formula]. As it follows from the results of [\cite=Hoeffding1955] [\cite=MulhollandRogers1958], the supremum of a linear (with respect to the d.f. F(x)) functional J(X,g) under two linear equality-type conditions [formula], [formula] is attained at the distributions concentrated in at most three points.

Before passing to checking three- and two-point distributions, recall that for L1-metric the following representation in terms of the mean metric holds as well (see, e.g. [\cite=Zolotarev1986]):

[formula]

and hence,

[formula]

[formula]

Let the r.v. X take two values and satisfy the conditions [formula], [formula]. Then its distribution should necessarily have the form

[formula]

It is easy to see that [formula], [formula]. Then

[formula]

[formula]

and hence

[formula]

by virtue of the Jensen inequality, thus, the statement of the theorem holds. Moreover, for any ε > 0

[formula]

for all 0 < p < ε / 2.

Now consider a r.v. X taking exactly three values. Note that

[formula]

where the supremums are taken over three-point distributions of the r.v. X. Let X take values x,y,z with probabilities p,q,r > 0 respectively, p + q + r = 1. Without loss of generality one can assume that [formula]. From the conditions [formula] we find that

[formula]

For all [formula] we have

[formula]

[formula]

Noticing that (px2 + qy2) / σ2 - p - q = (σ2 - rz2) / σ2 - 1 + r = r(1 - z2  /  σ2), we obtain

[formula]

Consider the function

[formula]

The statement of the theorem is equivalent to sup g(x,y,z,σ2) = 0, where the sumpremum is taken over all σ2 > 0, [formula] such that - yz < σ2 <  - xz. Note that it suffices to consider only σ2  <   max {x2,z2}, since the opposite inequality (with q > 0) implies that [formula]. So, there are only three possibilities: 1) 0 < σ2  <   min {x2,z2}, 2) [formula], 3) [formula]. Opening the modules, we notice that g(x,y,z,σ2) is a parabola with respect to σ2 on each of the intervals specified above. Consider the behavior of g(x,y,z,σ2) on each of these intervals.

0 < σ2  <   min {x2,z2}, then necessarily σ2 <  - xz and

[formula]

The coefficient - 2 / (z - x) at σ4 is negative, thus the branches of this parabola with respect to σ2 look down and the maximum value of the function g(z,y,z,σ2) within the interval 0 < σ2  <   min {x2,z2} is attained either at the vertex

[formula]

if σ2* >  - yz, or at the point σ2  →   - yz + 0, if [formula]. We have

[formula]

since [formula], with the equality attained if and only if y = 0. Thus, the supremum is attained as σ2  →   - yz + 0, which implies that [formula] and reduces the problem to checking two-point distributions considered above.

[formula], then

[formula]

by virtue of the conditions r > 0, z > 0.

[formula], then the function

[formula]

is linear and decreases monotonically in σ2, since x2(z - y) + y2(z - x) + xyz > 0. Thus, if [formula], then the supremum of g(x,y,z,σ2) is supplied by σ2  →   - yz + 0, which reduces the problem to checking two-point distributions considered above. If x2 >  - yz, then the supremum of g(x,y,z,σ2) is attained at σ2 = x2. With this value of σ2 we have

[formula]

[formula]

since x <  - z and [formula]. Thus, the supremum of g(x,y,z,x2) over all y such that [formula] and - yz < x2  =  σ2 is supplied by y  →   max {x, - x2 / z} + 0 =  - x2 / z + 0, i. e., [formula], which reduces the problem to checking two-point distributions considered above. Thus, the theorem is completely proved.