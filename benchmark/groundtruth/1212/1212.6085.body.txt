Algebraic Solutions to Multidimensional Minimax Location Problems with Chebyshev Distance

Recent Researches in Applied and Computational Mathematics: Intern. Conf. on Applied and Computational Mathematics (ICACM’11), WSEAS Press, 2011, pp. 157-162.

Lemma Corollary

Introduction

Location problems form one of the classical research domains in optimization that has its origin dating back to XVIIth century. Over many years a large body of research on this topic contributed to the development in various areas including integer programming, combinatorial and graph optimization. Among other solution approaches to location problems are models and methods of idempotent algebra [\cite=Baccelli1993Synchronization] [\cite=Cuninghame-Green1994Minimax] [\cite=Kolokoltsov97Idempotent] [\cite=Golan2003Semirings] [\cite=Heidergott2006Max-plus] [\cite=Butkovic2010Maxlinear], which find expanding applications in the analysis of actual problems in engineering, manufacturing, information technology, and other fields. Specifically, an algebraic solution to a one-dimensional location problem on a graph is proposed in [\cite=Cuninghame-Green1991Minimax] [\cite=Cuninghame-Green1994Minimax]. A constrained location problem and its representation in terms of idempotent algebra are examined in [\cite=Zimmermann2003Disjunctive] [\cite=Tharwat2010Oneclass].

In this paper, we consider a multidimensional minimax single facility location problem with Chebyshev distance, and show how the problem can be solved based on new results of the spectral theory in idempotent algebra. The aim of the paper is twofold: first, to give a new algebraic solution to the location problem, and second, to extend the area of application of idempotent algebra. The rest of the paper is as follows. We begin with an overview of preliminary algebraic definitions and results. Specifically, we present an extremal property of the eigenvalue of irreducible matrices. Furthermore, we examine an unconstrained minimax location problem and represent it in the terms of idempotent algebra. A new solution is given that reduces the problem to evaluation of the eigenvalue and eigenvectors of an irreducible matrix. Finally, the solution is extended to solve a constrained location problem.

Preliminary Results

We start with a brief overview of definitions, notation and preliminary results of idempotent algebra that underlie the solution approach developed in subsequent sections. Further details can be found in [\cite=Baccelli1993Synchronization] [\cite=Kolokoltsov97Idempotent] [\cite=Cuninghame-Green1994Minimax] [\cite=Golan2003Semirings] [\cite=Heidergott2006Max-plus] [\cite=Butkovic2010Maxlinear].

Idempotent Semifield

Let [formula] be a set with two operations, addition [formula] and multiplication [formula], and their respective neutral elements, [formula] and [formula]. We suppose that [formula] is a commutative semiring where the addition is idempotent and the multiplication is invertible. Since the nonzero elements of the semiring form a group under multiplication, the semiring is usually referred to as idempotent semifield.

The integer power is defined in the ordinary way. Let us put [formula]. For any [formula] and integer p > 0, we have [formula], [formula], x- p = (x- 1)p, and [formula].

We assume that the integer power can naturally be extended to the case of rational and real exponents.

In what follows, we drop, as is customary, the multiplication sign [formula]. The power notation is used in the sense of idempotent algebra.

The idempotent addition allows one to define a relation of partial order ≤   such that x  ≤  y if and only if [formula]. From the definition it follows that [formula] and [formula], as well as that the addition and multiplication are both isotonic. Below the relation symbols are thought of as referring to this partial order.

It is easy to verify that the binomial identity now takes the form [formula] for all [formula].

As an example, one can consider the idempotent semifield of real numbers

[formula]

In the semifield [formula], there are the null and identity elements defined as [formula] and [formula]. For each [formula], there exists its inverse x- 1 equal to the opposite number - x in conventional arithmetic. For any [formula], the power xy coincides with the arithmetic product xy.

Vectors and Matrices

Vector and matrix operations are routinely introduced based on the scalar addition and multiplication defined on [formula]. Consider the Cartesian power [formula] with its elements represented as column vectors. For any two vectors [formula] and [formula], and a scalar [formula], vector addition and multiplication by scalars follow the rules

[formula]

The set [formula] with these operations is a vector semimodule over the idempotent semifield [formula].

As it usually is, a vector [formula] is linearly dependent on vectors [formula], if there are scalars [formula] such that [formula]. In particular, the vector [formula] is collinear with [formula], if [formula].

For any column vector [formula], we define a row vector [formula] with its elements x-i = x- 1i. For all [formula], the componentwise inequality [formula] implies [formula].

For any conforming matrices A = (aij), B = (bij), and C = (cij), matrix addition and multiplication together with multiplication by a scalar [formula] are performed according to the formulas

[formula]

A matrix with all zero entries is referred to as zero matrix and denoted by [formula].

Consider the set of square matrices [formula]. The matrix that has all diagonal entries equal to [formula] and off-diagonal entries equal to [formula] is the identity matrix denoted by I.

With respect to matrix addition and multiplication, the set [formula] forms idempotent semiring with identity.

For any matrix [formula] and an integer p > 0, we have A0 = I and Ap = Ap - 1A = AAp - 1.

The trace of the matrix A = (aij) is defined as [formula].

A matrix is irreducible if and only if it cannot be put in a block triangular form by simultaneous permutations of rows and columns. Otherwise the matrix is reducible.

Eigenvalues and Eigenvectors of Matrices

A scalar λ is an eigenvalue of a square matrix [formula] if there exists a vector [formula] such that [formula]. Any vector [formula] that satisfies the equation is an eigenvector of A, corresponding to λ.

If the matrix A is irreducible, then it has only one eigenvalue given by

[formula]

The corresponding eigenvectors of A have no zero entries and are found as follows. First we evaluate the matrix

[formula]

Let [formula] and a×ii be respective column i and diagonal entry (i,i) of A×. Consider the subset of columns [formula] such that [formula], [formula]. In the subset, find those columns that are linearly independent of the others, and take them to form a matrix A+.

The set of all eigenvectors of A corresponding to λ (together with zero vector) coincides with the linear span of the columns of A×, whereas each vector takes the form

[formula]

where [formula] is a nonzero vector of appropriate size.

An Extremal Property of Eigenvalues

Suppose [formula] is an irreducible matrix with an eigenvalue λ. Consider a function [formula] defined on [formula]. It has been shown in [\cite=Krivulin2005Evaluation] [\cite=Krivulin2006Eigenvalues] that [formula] has λ as its minimum, which is attained at any eigenvector of A.

Now we improve the above result by extending the set of vectors that provide for the minimum of [formula].

Let [formula] be an irreducible matrix with an eigenvalue λ. Suppose [formula] and [formula] are eigenvectors of the respective matrices A and AT. Then it holds that

[formula]

where the minimum is attained at any vector [formula] for all α such that [formula].

It is easy to verify as in [\cite=Krivulin2006Eigenvalues] that any vector [formula] with nonzero elements satisfies the inequality [formula]. Indeed, since [formula], we have [formula].

It remains to present a vector [formula] that turns the inequality into an equality. With [formula] we immediately have [formula]. Similarly, if [formula], then [formula].

Let us now take any vector [formula] with elements xi = uαivα - 1i, where α is a real number such that [formula]. With the following calculations

[formula]

we arrive at the inequality [formula]. Since the opposite inequality is always valid, we conclude that the vector [formula] satisfies the condition [formula].

The Unconstrained Location Problem

In this section we consider a minimax single facility location problem with Chebyshev distance. For any two vectors [formula] and [formula] in [formula], the Chebyshev distance (L∞ or maximum metric) is calculated as

[formula]

Given [formula] vectors [formula] and constants [formula], [formula], the problem under consideration is to find a vector [formula] so as to provide

[formula]

It is not difficult to solve the problem by using geometric arguments (see, eg, [\cite=Sule2001Logistics] [\cite=Moradi2009Single]). Below we give a new algebraic solution that is based on representation of the problem in terms of the idempotent semifield [formula] and application of the result from the previous section. First we rewrite [\eqref=M-Chebyshev] as follows

[formula]

Denote the objective function of the problem by [formula]. With the vectors

[formula]

we can write

[formula]

and then represent problem [\eqref=P-Chebyshev] as

[formula]

Furthermore, we introduce a vector [formula] and a matrix A of order n + 1 as follows

[formula]

Since we now have [formula], problem [\eqref=P-Chebyshev1] reduces to that of the form

[formula]

Note that the vectors [formula] that solve [\eqref=P-Chebyshev2] do not always have an appropriate form to give a solutions to [\eqref=P-Chebyshev1]. Specifically, to be consistent to [\eqref=P-Chebyshev1], the vector [formula] must have the first element equal to [formula].

Algebraic Solution of the Unconstrained Problem

Consider problem [\eqref=P-Chebyshev2], and note that the matrix A is irreducible. It follows from Lemma [\ref=L-mxmAx] that the minimum in [\eqref=P-Chebyshev2] is equal to the eigenvalue λ of the matrix A. For all [formula] we have

[formula]

and therefore, [formula], [formula]. Finally, application of [\eqref=E-lambda] gives

[formula]

To find vectors that produce the minimum in [\eqref=P-Chebyshev2], we need to derive the eigenvectors of the matrices A and AT. Note that AT is obtained from A by replacement of [formula] with [formula] and [formula] with [formula]. Therefore, it will suffice to find the eigenvectors for A, and then turn them into the eigenvectors for AT by the above replacement.

To get the eigenvectors of A, we first find the matrix A×. Since for any [formula] it holds that

[formula]

we arrive at the matrix A× in the form

[formula]

It is not difficult to verify that in the matrix A×, any column that has [formula] on the diagonal is collinear with the first column. Indeed, suppose that the submatrix [formula] has a diagonal element equal to [formula], say the element in its first column (that corresponds to the second column of A×). In this case, we have [formula], whereas the matrix A× takes the form

[formula]

where the second column proves to be collinear with the first.

Let us construct a matrix A+ that includes such columns of A× that have the diagonal element equal to [formula]. Since all these columns are collinear with the first one, they can be omitted. With the matrix A+ formed from the first column of A×, we finally represent any eigenvector of A as

[formula]

By replacing [formula] with [formula] and [formula] with [formula], we get the eigenvectors of AT

[formula]

It follows from Lemma [\ref=L-mxmAx] that the solution of [\eqref=P-Chebyshev1] takes the form

[formula]

With the condition that the first element of [formula] must be equal to [formula], we have to set [formula]. Going back to problem [\eqref=P-Chebyshev1], we arrive at the following result.

The minimum in problem [\eqref=P-Chebyshev1] is given by [formula], and it is attained at any vector

[formula]

With the usual notation, we can reformulate the statement of Lemma [\ref=L-Chebyshev] as follows.

The minimum in [\eqref=P-Chebyshev] is given by [formula], and it is attained at any vector

[formula]

where [formula], [formula] for each [formula].

A Constrained Location Problem

Suppose that there is a set [formula] given to specify feasible location points. Consider a constrained location problem that is represented in terms of the semifield [formula] in the form

[formula]

To solve the problem we put it in the form of [\eqref=P-Chebyshev1] by including the area constraints into the objective function of an unconstrained problem. First we slightly transform problem [\eqref=P-Chebyshev1] to enable accommodation of the constraints in a natural way. With the notation we turn to the problem

[formula]

It follows from Lemma [\ref=L-Chebyshev] that the last problem has its minimum equal to [formula], whereas its solution set obviously coincides with that of [\eqref=P-Chebyshev1].

Suppose that there are constraints on the maximum distance from the facility location point to each given points that determine the feasible location set in the form [formula].

For each [formula], the inequality [formula] can be rewritten in an equivalent form as [formula], whereas all constraints can be replaced with one inequality

[formula]

We introduce the notation and note that [formula] when and only when the above constraints are satisfied.

Furthermore, we put

Now we can replace the original problem [\eqref=P-ChebyshevConstrained] by an unconstrained problem with the objective function [formula] that has the form of problem [\eqref=P-Chebyshev1] with the solution given by Lemma [\ref=L-Chebyshev]. It is clear that both problems give the same solution set provided that a solution exists. At the same time, the new problem allows one to get approximate solutions in the case when the original problem does not have a solution.