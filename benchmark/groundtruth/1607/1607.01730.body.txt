Rolling Horizon Coevolutionary Planning for Two-Player Video Games

Introduction

Since the dawn of computing, games have provided an excellent test bed for AI algorithms, and increasingly games have also been a major AI application area. Over the last decade progress in game AI has been significant, with Monte Carlo Tree Search dominating many areas since 2006[\cite=coulom2006efficient] [\cite=kocsis2006improved] [\cite=gelly2006modification] [\cite=chaslot2006monte] [\cite=chaslot2008monte], and more recently deep reinforcement learning has provided amazing results, both in stand-alone mode in video games, and in combination with MCTS in the shape of AlphaGo[\cite=Silver], which achieved one of the greatest steps forward in a mature and competitive area of AI ever seen. AlphaGo not only beat Lee Sedol, previous world Go champion, but also soundly defeated all leading Computer Go bots, many of which had been in development for more than a decade. Now AlphaGo is listed at the second position among current strongest human players.

With all this recent progress a distant observer could be forgiven for thinking that Game AI was starting to become a solved problem, but with smart AI as a baseline the challenges become more interesting, with ever greater opportunities to develop games that depend on AI either at the design stage or to provide compelling gameplay.

The problem addressed in this paper is the design of general algorithms for two-player video games. As a possible solution, we introduce a new algorithm: Rolling Horizon Coevolution Algorithm (RHCA). This only works for cases when the forward model of the game is known and can be used to conduct what-if simulations (roll-outs) much faster than real time. In terms of the General Video Game AI (GVGAI) competition series, this is known as the two-player planning track[\cite=perez20152014]. However, this track was not available at the time of writing this paper, so we were faced with the choice of using an existing two-player video game, or developing our own. We chose to develop our own set simple battle game, bearing some similarity to the original 1962 Spacewar game though lacking the fuel limit and the central death star with the gravity field. This approach provides direct control over all aspects of the game and enables us to optimise it for fast simulations, and also to experiment with parameters to test the generality of the results.

The two-player planning track is interesting because it offers the possibility of AI which can be instantly smart with no prior training on a game, unlike Deep Q Learning (DQN) approaches [\cite=mnih2015human] which require extensive training before good performance can be achieved. Hence the bots developed using our planning methods can be used to provide instant feedback to game designers on aspects of gameplay such as variability, challenge and skill depth.

The most obvious choice for this type of AI is Monte Carlo Tree Search (MCTS), but recent results have shown that for single-player video games, Rolling Horizon Evolutionary Algorithms (RHEA) are competitive. Rolling horizon evolution works by evolving a population of action sequences, where the length of each sequence equals the depth of simulation. This is in contrast to MCTS where by default (and for reasons of efficiency, and of having enough visits to a node to make the statistics informative) the depth of tree is usually shallower than the total depth of the rollout (from root to the final evaluated state).

The use of action-sequences rather than trees as the core unit of evaluation has strengths and weaknesses. Strengths include simplicity and efficiency, but the main weakness is that the individual sequence-based approach would by default ignore the savings possible by recognising shared prefixes, though prefix-trees can be constructed for evaluation purposes if it is more efficient to do so[\cite=perez2015open]. A further disadvantage is that the system is not directly utilising tree statistics to make more informed decisions, though given the limited simulation budget typically available in a real-time video game, the value of these statistics may be outweighed by the benefits of the rolling horizon approach.

Given the success of RHEA on single-player games, the question naturally arises of how to extend the algorithm to two-player games (or in the general case N-player games), with the follow-up question of how well such an extension works. The main contributions of this paper are the algorithm, RHCA, and the results of initial tests on our two-player battle game. The results are promising, and suggest that RHCA could be a natural addition to a game AI practitioner's toolbox.

The rest of this paper is structured as follows: Section [\ref=sec:back] provides more background to the research, Section [\ref=sec:battle] describes the battle games used for the experiments, Section [\ref=sec:cont] describes the controllers used in the experiments, Section [\ref=sec:xp] presents the results and Section [\ref=sec:conc] concludes.

Background

Open Loop control

Open loop control refers to those techniques which action decision mechanism is based on executing sequences of actions determined independently from the states visited during those sequences. Weber discusses the difference between closed and open loop in [\cite=weber2010optimization]. An open loop technique, Open Loop Expectimax Tree Search (OLETS), developed by Couëtoux, won the first GVGAI competition in 2014 [\cite=perez20152014], an algorithm inspired by Hierarchical Open Loop Optimistic Planning (HOLOP, [\cite=ICAPS124697]). Also in GVGAI, Perez et al. [\cite=perez2015open] discussed three different open loop techniques, then trained them on 28 games of the framework, and tested on 10 games.

The classic closed loop tree search is efficient when the game is deterministic, i.e., given a state s, [formula] (A(s) is the set of legal actions in s), the next state [formula] is unique. In the stochastic case, given a state s, [formula] (A(s) is the set of legal actions in s), the state [formula] is drawn with a probability distribution from the set of possible future states.

Rolling Horizon planning

Rolling Horizon planning, sometimes called Receding Horizon Control or Model Predictive Control (MPC), is commonly used in industries for making decisions in a dynamic stochastic environment. In a state s, an optimal input trajectory is made based on a forecast of the next th states, where th is the tactical horizon. Only the first input of the trajectory is applied to the problem. This procedure repeats periodically with updated observations and forecasts.

A rolling horizon version of an Evolutionary Algorithm that handles macro-actions was applied to the Physical Traveling Salesman Problem (PTSP) as introduced in [\cite=perez2013rolling]. Then, RHEA was firstly applied on general video game playing by Perez et al. in [\cite=perez2015open] and was shown to be the most efficient evolutionary technique on the GVGAI Competition framework.

Battle games

Our two-player space battle game could be viewed as derived from the original Spacewar (as mentioned above). We then experimented with a number of variations to bring out some strengths and weaknesses of the various algorithms under test. A key finding is that the rankings of the algorithms depend very much on the details of the game; had we just tested on a single version of the game the conclusions would have been less robust and may have shown RHCA in a false light. The agents are given full information about the game state and make their actions simultaneously: the games are symmetric with perfect and incomplete information. Each game commences with the agents in random symmetric positions to provide varied conditions while maintaining fairness.

First, a simple version without missiles is designed, referred as G1. Each spaceship, either owned by the first (green) or second (blue) player has the following properties:

has a maximal speed equals to 3 units distance per game tick;

slows down over time;

can make a clockwise or anticlockwise rotation, or to thrust at each game tick.

Thus, the agents are in a fair situation.

End condition

A player wins the game if it faces to the back of its opponent in a certain range before the total game ticks are used up. Figure [\ref=fig:range] illustrates how the winning range is defined. If no one wins the game when the time is elapsed, it's a draw.

Game state evaluation

Given a game state s, if a ship i is located in the predefined winning range (Fig. [\ref=fig:range]), the player gets a score DistScorei = HIGH_VALUE and the winner is set to i; otherwise, [formula] (∈(0,1]), where doti is the scalar product of the vector from the ship i's position to its opponent and the vector from its direction to its opponent's direction. The position and direction of both players are taken into account to direct the trajectory.

Perfect and incomplete information

The battle game and its variants have perfect information, because each agent knows all the events (e.g. position, direction, life, speed, etc. of all the objects on the map) that have previously occurred when making any decision; they have incomplete information because neither of agents knows the type or strategies of its opponent and moves are made simultaneously.

Controllers

In this section, we summarize the different controllers used in this work. All controllers use the same heuristic to evaluate states (Section [\ref=sec:heuristic]).

Search Heuristic

All controllers presented in this work follow the same heuristic in the experiments where a heuristic is used (Algorithm [\ref=algo:heuristic]), aiming at guiding the search and evaluating game states found during the simulations.

RHEA Controllers

In the experiments described later in Section [\ref=sec:xp], two controllers implement a distinct version of rolling horizon planning: the Rolling Horizon Genetic Algorithm (RHGA) and Rolling Horizon Coevolutionary Algorithm (RHCA) controllers. These two controllers are defined next.

Rolling Horizon Genetic Algorithm (RHGA)

In the experiments described later in Section [\ref=sec:xp], this algorithm uses truncation selection with arbitrarily chosen threshold 20, i.e., the 20% best individuals will be selected as parents. The pseudo-code of this procedure is given in Algorithm [\ref=algo:bga].

Rolling Horizon Coevolutionary Algorithm (RHCA)

RHCA (Algorithm [\ref=algo:coev]) uses a tournament-based truncation selection with threshold 20 and two populations [formula] and [formula], where each individual in [formula] represents some successive behaviours of current player and each individual in [formula] represents some successive behaviours of its opponent. The objective of [formula] is to evolve better actions to kill the opponent, whereas the objective of [formula] is to evolve stronger opponents, thus provide a worse situation to the current player. At each generation, the best 2 individuals in [formula] (respectively [formula]) are preserved as parents (elites), afterwards the rest is generated using the parents by uniform crossover and mutation. Algorithm [\ref=algo:eval] is used to evaluate game state, given two populations, then sort both populations by average fitness value. Only a subset of the second population is involved.

Macro-actions and single actions

A macro-action is the repetition of the same action for to successive time steps. Different values of to are used during the experiments in this work in order to show how this parameter affects the performance. The individuals in both algorithms have genomes with length equals to the number of future actions to be optimised, i.e., th.

In all games, different numbers of actions per macro-action are considered in RHGA and RHCA. The first results show that there is an improvement in performance, for all games, the shorter the macro-action is. The result using one action per macro-action, i.e., to = 1, will be presented.

Recommendation policy In both algorithms, the recommended trajectory is the individual with highest average fitness value in the population (Algorithm [\ref=algo:bga] line [\ref=lst:ga]; Algorithm [\ref=algo:coev] line [\ref=lst:coev]). The first action in the recommended trajectory, presented by the gene at position 1, is the recommended action in the next single time step.

Open Loop MCTS

A Monte Carlo Tree Search (MCTS) using Open Loop control (OLMCTS) is included in the experimental study. This was adapted from the OLMCTS sample controller included in the GVGAI distribution [\cite=Perez2015]. The OLMCTS controller was developed for single player games, and we adapted it for two player games by assuming a randomly acting opponent. Better performance should be expected of a proper two-player OLMCTS version using a minimax (more max-N) tree policy and immediate future work is to evaluate such an agent.

Recommendation policy The recommended action in the next single time step is the one present in the most visited root child, i.e., robust child[\cite=coulom2006efficient]. If more than one child ties as the most visited, the one with the highest average fitness value is recommended.

One-Step Lookahead

One-Step Lookahead algorithm (Algorithm [\ref=algo:osl]) is deterministic. Given a game state s at timestep t and the sets of legal actions of both players A(s) and A'(s), One-Step Lookahead algorithm evaluates the game then outputs an action at + 1 for the next single timestep using some recommendation policy.

Recommendation policy There are various choices of π, such as Wald [\cite=wald1939] and Savage [\cite=savage1951] criteria. Wald consists in optimizing the worst case scenario, which means that we choose the best solution for the worst scenarios. Thus, the recommended action for player 1 is

[formula]

Savage is an application of the Wald maximin model to the regret:

[formula]

We also include a simple policy which chooses the action with maximal average score, i.e.,

[formula]

Respectively,

[formula]

The OneStep controllers use separately (1) Wald (Equation [\ref=eq:maxmin]) on its score; (2) maximal average score (Equation [\ref=eq:maxmean]) policy; (3) Savage (Equation [\ref=eq:minmax]); (4) minimal the opponent's average score (Equation [\ref=eq:minmean]); (5) Wald on (its score - the opponent's score) and (6) maximal average (its score - the opponent's score). In the experiments described in Section [\ref=sec:xp] we only present the results obtained by using (6), which performs the best among the 6 policies.

Experiments on battle games

We compare a RHGA controller, a RHCA controller, an Open Loop MCTS controller and an One-Step Lookahead controller, a move in circle controller and a rotate-and-shoot controller, denoted as RHGA, RHCA, OLMCTS, OneStep, ROT and RAS respectively, by playing a two-player battle game of perfect and incomplete information

Parameter setting

All controllers should decide an action within 10ms. OLMCTS uses a maximum depth= 10. Both RHCA and RHGA have a population size λ = 10, ProbaMut = 0.3 and th = 10. The size of sub-population used in tournament (SubPopSize in Algorithm [\ref=algo:coev]) is set to 3. These parameters are arbitrarily chosen. Games are initialised with random positions of spaceships and opposite directions (cf. Figure [\ref=fig:initial]).

Analysis of numerical results

We measure a controller by the number of wins and how quickly it achieves a win. Controllers are compared by playing games with each other using the full round-robin league. As the game is stochastic (due to the random initial positions of ships, the random process included in recoil or hitting), several games with random initialisations are played between any pair of the controllers.

rotated controller, denoted as ROT, is also compared. For comparison, a random controller, denoted as RND, is included in all the battle games.

Table [\ref=tab:games] illustrates the experimental results.

Every entry in the table presents the number of wins of the column controller against the line controller among 100 trials of battle games. The number of wins is calculated as follows: for each trial of game, if it's a win of the column controller, the column controller accumulates 1 point, if it's a loss, the line controller accumulates 1 point; otherwise, it's a draw, both column and line controllers accumulates 0.5 point.

ROT

The rotated controller ROT, which goes in circle, is deterministic and vulnerable in simple battle games

RND

The RND controller is not outstanding in the simple battle game

OneStep

OneStep is feeble in all games against all the other controllers except one case: against ROT in G1. It's no surprise that OneStep beats ROT, a deterministic controller, in all the 100 trials of G1. Among the 100 trials of G1, OneStep is beaten separately by OLMCTS once and RND twice, the other trials finish by a draw. This explains the high standard error.

OLMCTS

OLMCTS outperforms ROT and RND, however, there is no clear advantage or disadvantage when against OneStep, RHCA or RHGA.

RHCA

In all the games, the less actions set in a macro-action, the better RHCA performs. RHCA outperforms all the controllers

RHGA

In all the games, the less actions set in a macro-action, the better RHGA performs. RHGA is the second-best in the battle game

Conclusions and further work

In this work, we design a new Rolling Horizon Coevolutionary Algorithm (RHCA) for decision making in two-player real-time video games. This algorithm is compared to a number of general algorithms on the simple battle game designed, and some more difficult variants to distinguish the strength of the compared algorithms. In all the games, more actions per macro-action lead to a worse performance of both Rolling Horizon Evolutionary Algorithms (controllers denoted as RHGA and RHCA in the experimental study). Rolling Horizon Coevolution Algorithm (RHCA) is found to perform the best or second-best in all the games.

More work on battle games with weapon is in progress. In the battle game, the sum of two players' fitness value remains 0. An interesting further work is to use a mixed strategy by computing Nash Equilibrium[\cite=osborne1994course]. Furthermore, a Several-Step Lookahead controller is used to recommend the action at next tick, taken into account actions in the next n ticks. A One-Step Lookahead controller is a special case of Several-Step Lookahead controller with n = 1. As the computational time increases exponentially as a function of n, some adversarial bandit algorithms may be included to compute an approximate Nash[\cite=liu2015portfolio] and it would be better to include some infinite armed bandit technique.

Finally, it is worth emphasizing that rolling horizon evolutionary algorithms provide an interesting alternative to MCTS that has been very much under-explored. In this paper we have taken some steps to redress this with initial developments of a rolling horizon coevolution algorithm. The algorithm described here is a first effort and while it shows significant promise, there are many obvious ways in which it can be improved, such as biasing the roll-outs [\cite=lucas2014fast]. In fact, any of the techniques that can be used to improve MCTS rollouts can be used to improve RHCA.