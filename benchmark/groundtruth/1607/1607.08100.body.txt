Automatically Reinforcing a Game AI

Introduction

Portfolios are widely used in many domains; after early papers in machine learning  [\cite=utgoff1988] [\cite=aha1992], they are now ubiquitous in Artificial Intelligence, planning, and combinatorial optimization [\cite=nudelmann2004] [\cite=xuhydra2010] [\cite=kott]. The special case of parameter tuning (close to our "variants problem" later in the present document) is widely studied [\cite=ksurvey], with applications to SAT-solving [\cite=satenstein] [\cite=portsat] or computer vision [\cite=bolme].

Recently, portfolios were also applied in games [\cite=bouzy2011hedging] [\cite=swie].

A "portfolio" here refers to a family of algorithms which are candidates for solving a given task. On the other hand, "portfolio combination" or "combination" refers to the combined algorithm. Let us introduce a simple combined algorithm. If we have algorithms [formula] in the portfolio, and if the combination is π  =  πi with probability pi where 0  ≤  pi  ≤  1 and [formula] (the random choice is made once and for all at the beginning of each game), then π is, by definition, the portfolio combination with probability distribution p. Moreover, also by definition, it is stationary. Furthermore we will consider a case in which the probability distribution is not stationary (namely, UCBT, defined in Section [\ref=ucbp]).

Another approach, common in optimization, is "chaining" [\cite=chaining], which means interrupting one program and using its internal state as a hint for another algorithm. The combination can even be "internal" [\cite=vassilevska2006], i.e. parts of a solver are used in other solvers. The most famous applications of portfolios are in SAT-solving [\cite=xu2008satzilla]; nowadays, portfolios routinely win SAT-solving competitions. In this paper, we focus on portfolios of policies in games, i.e. portfolios of GPP. Compared to optimization, portfolios of policies in games or control policies have been less widely explored, except for e.g. combinations of local controllers by Fuzzy Systems [\cite=fuzzyevo], Voronoi controllers [\cite=vorocontrol] or some case-based reasoning [\cite=cbrcontrol]. These methods are based on "internal" combinations, using the current state for choosing between several policies. We here focus on external combinations; one of the internal programs is chosen at the beginning of a game, for all games. Such combinations are sometimes termed "ensemble methods"; however, we simply consider probabilistic combinations of existing policies, the simplest case of ensemble methods. This is an extension of a preliminary work [\cite=publinashrandomseed].

To the best of our knowledge, there is not much literature on combining policies for games when only one program is available. The closest past work might be Gaudel et al.  [\cite=gaudel2011principled], which proposed a combination of opening books, using tools similar to those we propose in Section [\ref=np] for combining policies.

Main goal of the present paper

The main contribution of this paper is to propose a methodology that can generically improve the performance of policies without actually changing the policies themselves, except through the policy's options or the policy's random seed. Incidentally, we establish that the random seed can have a significant contribution to the strength of an artificial intelligence, just because random seeds can decide the answer to some critical moves as soon as the original randomized GPP has a significant probability of finding the right move. In addition, while a fixed random seed cannot be strong against an adaptive opponent, our policies are more diversified (see the Nash approach) or adaptive (see our UCBT-portfolio).

Our approach is particularly relevant when the computational power is limited, because the computational overhead is very limited. Our main goal is to answer the following question: how can we, without development and without increasing the online computational cost, significantly increase the performance of a GPP in games ?

Outline of the present paper

We study 2 different portfolio problems:

The first test case is composed of a set of random seeds for a given GPP. By considering many possible seeds, we get deterministic variants of the original stochastic GPP. We restrict our attention to combinations which are a fixed probability distribution over the portfolio: we propose a combination such that, at the beginning of the game, one of the deterministic GPPs (equivalently, one of the seeds) is randomly drawn and then blindly applied. Hence, the problem boils down to finding a probability distribution over the set of random seeds such that it provides a strong strategy. We test the obtained probability distribution on seeds versus (i) the GPP with uniformly randomly drawn seeds (i.e. the standard, original, version of the GPP) and (ii) a stronger GPP, defined later, termed "exploiter" (Section [\ref=genera]).

In the second case, we focus on different parameterizations of a same program, so that we keep the spirit of the main goal above. The goal here is to find a probability distribution over these parameterizations. We will assess the performance of the obtained probability distribution against the different options.

A combination can be constructed either offline [\cite=Kadioglu2011] or online [\cite=gaglioloSchmidhuber2006b] [\cite=armstrong2006]. In this paper, we use three different methods for combining several policies:

In the first one, termed Nash-portfolio, we compute a Nash Equilibrium (NE) over the portfolio of policies in an offline fashion. This approach computes a distribution such that it generates a robust (not exploitable) agent. Further tests show a generalization ability for this method.

In the second one, termed UCBT-portfolio, we choose an element in the portfolio, online, using a bandit approach. This portfolio learns a specialized distribution, adaptively, given a stationary opponent. This approach is very good at exploiting such opponent.

The third one, Best Arm, is the limit case of UCBT-portfolio. It somehow cheats by selecting the best option against its opponent, i.e. it uses prior knowledge. This is what UCBT will do asymptotically, if it is allowed to play enough games.

These concepts are explained in Fig. [\ref=portfolio]. There are important related works using teams of programs [\cite=team] [\cite=team2] [\cite=team3]. The specificity of the present work is to get an improvement with a portfolio of programs which are indeed obtained from a single original program - i.e. we get an improvement "for free", in terms of development.

The rest of the paper is divided as follows. Section [\ref=csmg] formalizes the problem. Section [\ref=app] describes our approach. Section [\ref=sec:set] details the experimental setup. Section [\ref=xp] presents the results. Section [\ref=robust] shows robustness elements. Section [\ref=simpler] presents simplified variants of our algorithms, performing similarly to the original ones. Section [\ref=conc] concludes.

Problem Statement

In this section, we formalize the notion of policies, adversarial portfolios, and the framework of matrix games. We also introduce the concepts of overfitting, exploitation and generalization.

Policies

We consider policies, i.e. game playing programs (GPP [\cite=team2]), and tools (portfolios) for combining/selecting them.

When a GPP is stochastic, it can be made deterministic by choosing a fixed seed at the beginning of the game. From a stochastic π, we can therefore build several GPP π1, π2, corresponding to seeds 1, 2, In the case of our portfolio, and in all experiments and algorithms in the present paper, the choice of the seed is done once and for all, when a new game starts.

Matrix games

In this paper we only consider finite constant-sum adversarial games (i.e. if one player wins the other loses, constant-sum and adversarial are synonyms) with a reward that is only available at the end of the game. To properly define our algorithms in the following sections, let us introduce the concept of constant-sum matrix game. Without loss of generality, we define the concept of 1-sum matrix game instead of an arbitrary constant.

Consider a matrix K  ×  K', with values in

[formula]

Overfitting, exploitation & generalization

Overfitting in a game sense refers to the poor performance of a GPP P when P seems to be strong according to a given criterion which was used in the design of P. For instance, a GPP built through trials and errors by accepting any modifications which increase the success rate against a GPP X might have an excellent success rate against X, but a poor winning rate against another program Y. This is a case of overfitting.

This is important when automatic tuning is applied, and in particular for portfolio methods when working on random seeds. Selecting good random seeds for Player 1, by analyzing a matrix of results for various seeds for Player 1 and Player 2, might be excellent in terms of performance against the seeds used for Player 2 in the data; but for a proper assessment of the performance against the original randomized program, we should use games played against other seeds for Player 2. The performance against the seeds used in the data is referred to as an empirical performance, whereas the performance against new seeds is referred to as the performance in generalization [\cite=VAP]. Only the performance in generalization is a proper assessment of performance; we provide such results.

In games, overfitting is related to exploitability. Exploitability is an indicator of overfitting; when we build a GPP by some machine learning method, we can check, by the exploitability measure, whether it is good more generally than just against the opponents which have been used during the learning process.

In practice, exploitability defined as above is hard to measure. Therefore, we often use simpler proxies, e.g. the worst performance against a set of opponents. We say that a program A "exploits" a program B when A has a great success rate against B, much higher than the success rate of most programs against B - and we say that a family A exploits a program B when there exists A∈A which exploits B. The existence of A which "exploits" B suggests an overfitting issue in the design of B.

Approaches

Section [\ref=np] proposes a method for combining policies offline, given a set of policies for Player 1 and a set of policies for Player 2. Section [\ref=ucbp] proposes a method for combining policies online, given a portfolio of policies for player 1 and a stationary opponent.

Offline learning: Nash-portfolios and Best Arm

Consider two players P1 and P2, playing some game (not necessarily a matrix game). P1 is Black, P2 is White. Assume that P1 has a portfolio of K policies. Assume that P2 has a portfolio of K' policies. Then, we can construct a static combination of these policies by solving (i.e. finding a Nash equilibrium of) the matrix game associated to the matrix M, with Mi,j the winning rate of the ith policy of P1 against the jth policy of P2. Solving this 1-sum matrix game provides [formula] and [formula], probabilities, and the combination consists in playing, for P1, the ith policy with probability pi and, for P2, the jth policy with probability qj. Such a combination will be termed here a Nash-portfolio. By construction,

the Nash-portfolio can play both as Black and as White (P1 and P2);

the Nash-portfolio does not change over time but is, in the general case, stochastic.

Let us define more formally the Nash-portfolio and the Best Arm portfolio.

Definition:

Best Arm can be seen as the best response to the uniform policy. In both cases, Nash-portfolio and Best Arm, there is no uniqueness.

The Nash equilibrium can be found using an exact solving, in polynomial time, by linear programming [\cite=GaleKuhnTucker]. It can also be found approximately and iteratively, in sublinear time, as shown by [\cite=grigoriadis] [\cite=auer95gambling]; the EXP3 algorithm is classical for doing so. From the properties of Nash equilibria, we deduce that the Nash-portfolio has the following properties:

It depends on a family of policies for player 1 on a family of policies for player 2. It is therefore based on a training, by offline learning.

It is optimal (for player 1) among all mixed strategies (i.e. stochastic combinations of policies in the portfolio of player 1), in terms of both

worst case among the pure strategies in the portfolio of player 2;

worst case among the mixed strategies over the portfolio of player 2.

It is not necessarily uniquely defined.

In optimization settings, it is known [\cite=samulowitz2007] that having a somehow "orthogonal" portfolio of algorithms, i.e. algorithms as different from each other as possible, is a good solution for making the combination efficient. It is however difficult, in the context of policies, to know in advance if two algorithms are orthogonal - we can however see, a posteriori, which strategies have positive probabilities in the obtained combination.

Online learning: UCBT-Portfolio

Section [\ref=np] assumed that S1 and S2, two sets of strategies, are available and that we want to define a combination of policies in S1 (resp. in S2). A different point of view consists in adapting online the probabilities pi and qi, against a fixed opponent. We propose the following algorithm. We define this approach in the case of Black, having K policies at hand. The approach is similar for White. It is directly inspired by the bandit literature [\cite=lairobbins] [\cite=Auer02], and, more precisely, by Upper-Confidence-Bounds-Tuned (UCBT) [\cite=ucbtcorr], with parameters optimized for our problem:

Define ni = 0, ri = 0, for [formula].

For each iteration [formula].

compute for each [formula] [formula] [formula] using X / 0 =  +   ∞   (even for X = 0), p = 2.1 and C = 2 (UCBT, i.e. UCB-Tuned, formula).

choose k maximizing score(k).

play a game using algorithm k in the portfolio.

if it is a win, rk←rk + 1.

nk←nk + 1.

Definition.

Settings

This section presents the settings used in our experiments. Section [\ref=ssec:set] details the notion of portfolio of random seeds for 4 different games (Go, Chess, Havannah, Batoo). Section [\ref=ssec:algo] explains the context a portfolio of parameterizations for the game of Go.

Portfolio of Random Seeds

First, let us explain the principle of GPPs that just differ by their random seeds. We first apply the portfolio approach in this case. Without loss of generality, we will focus on the case where K  =  K'. The K GPPs for Black and the K GPPs for White use random seed 1, 2, , K respectively. Let us see what our Nash-portfolio and other portfolios become in such a setting. We define Mi,j = 1 if, with random seed i, Black wins against White with random seed j. Otherwise, Mi,j = 0. Importantly, the number of games to be played for getting this matrix M, necessary for learning the Nash-portfolio is K2. This is because there is no need for playing multiple games, since fixing the random seed makes the result deterministic. Thus, we just play one game for each [formula].

Then, we compute (p,q), one of the Nash equilibria of the matrix game M. This learns simultaneously the Nash-portfolio for Black and for White. Using this matrix M, we can also apply:

the uniform portfolio, simply choosing randomly uniformly among the seeds;

the Best Arm portfolio, choosing (I,J) optimizing Eqs. [\ref=bablack] and [\ref=bawhite] and using I as a seed for Black and J as a seed for White;

the UCBT-portfolio, which is the only non-stationary portfolio in the present paper.

We use 4 different testbeds in this category (portfolio of random seeds): Go, Chess, Havannah, Batoo. These games are all deterministic (Batoo has an initial important simultaneous move, namely the choice of a base-build, i.e. some initial stones - but we do not keep the partially observable stone, see details below).

The game of Go

The first testbed is the game of Go for which the best programs are Monte-Carlo Tree Search (MCTS) with specialized Monte Carlo simulations and patterns in the tree. The Black player starts. The game of Go is an ancient oriental game, invented in China probably at least 2 ~ 500 years ago. It is still a challenge for GPP, as even though MCTS[\cite=coulom06] revolutionized the domain, the best programs are still not at the professional level. Go is known as a very deep game [\cite=depth].For the purpose of our analysis, we use a 9x9 Go board.

We use GnuGo's random seed for having several GnuGo variants. The random seed of GnuGo makes the program deterministic, by fixing the seed used in all random parts of the algorithm. We define 32 variants, using "GnuGo -level 10 -random-seed k" with [formula]. In other words, we use a MCTS with 80 000 simulations per move, as GnuGo uses, by default, 8 000 simulations per level.

Chess

The second testbed is Chess. There are 2 players, Black and White. The White player starts. Chess is a two-player strategy board game played on a chessboard, a checkered game board with 64 squares arranged in an 8-by-8 grid. As in Go, this game is deterministic and full information. For the game of Chess, the main algorithm is alpha-beta [\cite=campbell2002deep], yet here we use a vanilla MCTS. We define 100 variants for the portfolios of random seeds (giving a matrix M of size 100-by-100), using a MCTS with 1 ~ 000 simulations per move and enhanced by an evaluation function. Our implementation is roughly ELO 1600 on game servers, i.e. amateur level.

Havannah

The third testbed is the game of Havannah. There are 2 players in this game: Black and White. The Black player starts. Havannah is an abstract board game invented by Christian Freeling. It is best played on a base-10 hexagonal board, i.e. 10 hexes (cells) to a side. Havannah belongs to the family of games commonly called connection games; its relatives include Hex and TwixT. This game is also deterministic with full information. For the game of Havannah, a vanilla MCTS with rapid action value estimates [\cite=icmlmogo] provides excellent performance. We define 100 variants for the portfolio of random seeds (giving a matrix M of size 100-by-100), using a MCTS with 1 ~ 000 simulations per move.

Batoo

The fourth testbed is a simplified version of Batoo. Batoo is related to the game of Go, but contains 2 features which are not fully observable:

Each player, once per game, can put a hidden stone instead of a standard stone.

At the beginning, each player, simultaneously and privately, puts a given number of stones on the board. These stones, termed "base build", define the initial position. When the game starts, these stones are revealed to the opponent and colliding stones are removed.

We consider a simplified Batoo, without the hidden stones - but we keep the initial, simultaneous, choice of base build. As in Go, this game is deterministic. Once the initial position of the stones is chosen for both player a normal game of 9x9 Go is executed using a GnuGo level 10.

Portfolio of parameterizations: variants of GnuGo

We consider the problem of combining several variants (each variant corresponds to a set of options which are enabled) of a GPP for the game of Go.

Our matrix M is a 32  ×  32 matrix, where Mi,j is the winning rate of the ith variant of GnuGo (as black) against the jth variant of GnuGo (as white). We consider all combinations of 5 options of GnuGo, hence 32  =  25 variants. In short, the first option is 'cosmic-go', which focuses on playing at the center. The second option is the use of fuseki (global opening book). The third option is 'mirror', which consists in mirroring your opponent at the early stages of the game. The fourth option is the large scale attack, which evaluates if a large attack across several groups is possible. The fifth option is the break-in. It consists in breaking the game analysis into territories that require deeper tactical reading and are impossible to read otherwise. It revises the territory valuations. Further details on the 5 options are listed on our website [\cite=taogames]. As opposed to Section [\ref=ssec:set], we need more than one evaluation in order to get Mi,j, because the outcome of a game between 2 different GPP is not deterministic. For the purpose of this paper, we build the matrix Mi,j offline by repeating each game (i,j) ~ 289 times, leading to a standard deviation at most 0.03 per entry. For this part, experiments are performed on the convenient 7x7 framework, with MCTS having 300 simulations per move - this setting is consistent with the mobile devices setting. We refer to the ith algorithm for Black as BAIi (Black Artificial Intelligence # i), and WAIj is the jth algorithm for White.

Experiments

In this section we evaluate the performance of our approaches across different settings.

Section [\ref=xp:offline] focuses on the problem of computing a probability distribution in an offline manner for the games defined in Section [\ref=sec:set]. We evaluate the scores of the Nash-portfolio approach and of the Best ~ Arm approach. We also include the uniform portfolio. In the case of a portfolio of random seeds, the uniform portfolio is indeed the original algorithm.

Section [\ref=xp:online] focuses on the problem of learning a probability distribution in an online manner to play against a specific opponent for the games defined in Section [\ref=sec:set]. We evaluate the learning ability of our UCBT-portfolio.

Learning Offline

In this section we present an analysis of the different offline portfolios across the testbeds. Table [\ref=tb:ana] shows the performance of the portfolios. The column V presents the value of the matrix game M. The following columns are self-explanatory where 1 indicates the player with the initiative and 2 indicates the player without.

We briefly describe the results in the four portfolios of random seeds as follows:

For the game of Go, the number of seeds with positive probability in the Nash-portfolio is 11 for Black and 9 for White, i.e. roughly [formula] of the random seeds. Nash-portfolio outperforms Best ~ Arm, which in turn wins against Uniform.

In Chess, the number of seeds with positive probability in the Nash equilibrium is 34 for White and 37 for Black, i.e. roughly [formula] of the random seeds. The best arm strategy is easily beaten by the Nash portfolio.

For the game of Havannah, the number of seeds with positive probability in the Nash-portfolio is 36 for White and 34 for Black, i.e. roughly [formula] of the random seeds are selected. The best arm strategy is outperformed by the Nash portfolio.

For the game of Batoo, the number of seeds with positive probability in the Nash-portfolio is 11 for Black and 14 for White. The uniform strategy is seemingly quite easily beaten by the Nash-portfolio or the Best Arm-portfolio.

These descriptive statistics are extracted in the learning step, i.e. on the training data, namely the matrix M. They provide insights, in particular around the fact that no seed dominates, but a clean validation requires a test in generalization, as discussed in Section [\ref=over]. Performances in generalization are discussed in Section [\ref=genera].

We now consider the case of "Variants", which refers to the case in which we do not work on random seeds, but on variants of GnuGo, as explained in Section [\ref=gnugovariants]; the goal is to "combine" optimally the variants, among randomized choices between variants. For Variants, in the NE, the number of selected options (i.e. options with positive probability) is 4 for Black and also 4 for White, i.e. [formula] of the variants are in the Nash. This means that no option could dominate all others. The uniform strategy is quite easily beaten by the Nash as well as the best arm strategy. The Best Arm portfolio is beaten by the Nash portfolio. This last point is interesting: it shows that selecting the variant which is the best for the average winning rate against other variants (this is what Best Arm does), leads to a combined variant which is weaker than the Nash combination.

Generalization ability of offline Portfolio

We now switch to the performance in generalization of the Nash and Best ~ Arm approach. In other words, we test whether it is possible to use a distribution computed over a portfolio of policies (learned against a given set of opponent policies) against new opponent policies that are not part of the initial matrix. The idea is to select a submatrix of size K (learning set), compute our probability distribution for this submatrix using either Nash or Best ~ Arm and make it play against the remainder of the seeds (validation set). We restrict our analysis to the setting presented in Section [\ref=ssec:set]. We focus on the 4 portfolios with random seeds. We test policies (Nash-portfolio, uniform portfolio, Best Arm) against an opponent which is not in the training set in order to evaluate whether our approach is robust.

The x-axis represents the number of policies K considered for each player (hence a matrix M of type K  ×  K). The y-axis shows the win rate of the different approaches

against an opponent that uses the uniform strategy (this is tested with independently drawn random seeds, not used in the matrix used for learning);

against an "exploiter"; by exploiter, we mean an algorithm which selects, among the N > K considered seeds which are not used in the learning set, the best performing one. Obviously, this opponent is somehow cheating; he knows which probability distribution you have, and uses it for choosing his seed among the M = N - K seeds which are considered in the experiment but not used in the learning. This is a proxy for the robustness; some algorithms are better than other for resisting to such opponents who use some knowledge about you.

Figure [\ref=fig:go] summarizes the results for the game of Go. Figure [\ref=fig:gene] presents the results of 2 different approaches (Nash and Best Arm) versus the uniform baseline. All experiments are reproduced 10 ~ 000 times (5 ~ 000 times for the Black player and 5 ~ 000 times for the White player) and standard deviations are smaller than 0.007.

Figure [\ref=fig:exploit] shows the difference between a Nash approach and a Best Arm approach in terms of exploitability.

From Figure [\ref=fig:gene] we can observe that there is a clear advantage to use either the Nash or the Best Arm approach when facing a new set of policies. Moreover, as expected, as the size of the initial matrix grows, the winning rates of both Nash and Best Arm increase when compared to the baseline. It is interesting to note that there is a sharp increase when the submatrix size is relatively small (between 3 and 7). Afterwards, the size of the submatrix has a moderate impact on the performance until most options are included in the matrix.

It does not come as a surprise that the approach Best Arm performs slightly better than the Nash against a uniformly random opponent. The Best Arm approach is particularly well suited to play against such an opponent. However, the Best Arm approach is easily exploitable. This behavior is shown in Figure [\ref=fig:exploit].

From Figure [\ref=fig:exploit] it clearly appears that Best Arm is a strategy very easy to exploit. Thus, even if Figure [\ref=fig:gene] shows that the use of the Best Arm approach outperforms Nash versus the uniform baseline, Nash is a much more resilient strategy.

Chess: Figure [\ref=fig:chess] summarizes the results for the game of Chess. Figure [\ref=fig:chess:geneAll] presents the results of 2 different approaches versus the uniform baseline. All experiments are reproduced 10 ~ 000 times (5 ~ 000 times for the Black player and 5 ~ 000 times for the White player) and standard deviations are smaller than 10- 2. Figure [\ref=fig:chess:exploit] shows the difference between a Nash approach and a Best Arm approach in terms of exploitability.

From Figure [\ref=fig:chess:geneAll] we can observe, as it was the case in the game of Go, that there is a clear advantage to use either the Nash or the Best Arm approach when facing a new set of policies. As the size of the initial matrix grows, the winning rates of both Nash and Best Arm increase, in generalization, when compared to the baseline. Also, we observe that the shape of the curve for the Nash approach is quite similar to the one seen in the game of Go. However, the Best Arm approach keeps increasing almost linearly throughout the entire x-axis.

From Figure [\ref=fig:chess:exploit] it clearly appears that Best Arm is a strategy very easy to exploit. Thus, while Figure [\ref=fig:chess:geneAll] shows that the use of the Best Arm approach outperforms Nash versus the uniform baseline, Nash is a much more resilient strategy.

Havannah: Figure [\ref=fig:havanah] summarizes the results for the game of Havannah. Figure [\ref=fig:havanah:geneAll] presents the results of 2 offline portfolio algorithms (namely Nash and Best Arm) versus the uniform baseline. Same setting as for chess (number of experiments and same bound on the standard deviation).Figure [\ref=fig:havanah:exploit] shows the difference between a Nash approach and a Best Arm approach in terms of exploitability.

From Figure [\ref=fig:havanah:geneAll] we can observe, as it was the case in the game of Go, that there is a clear advantage to use either the Nash or the Best Arm approach when facing a new set of policies. As the size of the initial matrix grows, the winning rates of both Nash and Best Arm increase when compared to the baseline.

From Figure [\ref=fig:havanah:exploit] it clearly appears that Best Arm is a strategy very easy to exploit. Thus, even if Figure [\ref=fig:havanah:geneAll] shows that the use of the Best Arm approach outperforms Nash versus the uniform baseline, Nash is a much more resilient strategy.

The performance of Nash and more especially Best Arm increase significantly as the size of the submatrix grows. This is in sharp contrast with the 2 previous games. In the case of Havannah, the sharpest gain is towards the end of the x-axis, which suggests that further gains would be possible with bigger matrix.

Batoo: Figure [\ref=fig:batoo] summarizes the results for the game of Batoo. Figure [\ref=fig:batoo:geneAll] presents the results of 2 different approaches versus the uniform baseline. Same setting as for Chess and Havannah.Figure [\ref=fig:batoo:exploit] shows the difference between a Nash approach and a Best Arm approach in terms of exploitability. The x-axis represents the number of policies considered. The y-axis shows the loss rates. All experiments are reproduced 100 times.

From Figure [\ref=fig:batoo:geneAll] we can observe, as it was the case in the game of Go, that there is a clear advantage to use either the Nash or the Best Arm approach when facing a new set of policies. As the size of the initial matrix grows, the winning rates (in generalization) of both Nash and Best Arm increase when compared to the baseline.

From Figure [\ref=fig:batoo:exploit] it clearly appears that Best Arm is a strategy very easy to exploit. Thus, though Figure [\ref=fig:batoo:geneAll] shows that the use of the Best Arm approach outperforms Nash versus the uniform baseline, Nash is a much more resilient strategy.

Conclusion: The performance of Nash and Best Arm increase steadily as the size K of the submatrix grows. Also, we observe a behavior similar to the game of Go. The simultaneous action nature of the first move does not seem to impact the general efficiency of our approach.

Learning Online

The purpose of this section is twofold:

Propose an adaptive algorithm, built automatically buy the random seed trick as in the case of Nash-Portfolio.

Show the resilience of our offline-learning algorithms, namely Nash-Portfolio and Best Arm, against this adaptive algorithm - in particular, this shows a weakness of Best Arm in terms of exploitability/overfitting.

Here we present the losing rate of UCBT (see Section [\ref=ucbp]) against 3 baselines. The purpose is to evaluate whether learning a strategy online against a specific unknown opponent (baselines) can be efficiently done.

The first baseline is the Nash equilibrium (label Nash and previously defined in Section [\ref=app]. The second baseline is the uniform player (label Unif) which consists in playing uniformly each option of the bandit. The third baseline consists in playing a single deterministic strategy (only one random seed) regardless of the opponent.

Go: Figure [\ref=fig:learnRow] (and Figure [\ref=fig:learnCol]) shows the learning of UCBT for the Black player (and White respectively) for the game of Go.

First and foremost, as the number of iterations grows, there is a clear learning against both Nash and Unif baselines. We see that (i) UCBT eventually reaches, against Nash-portfolio, approximately the value of the game for each player, (ii) the Nash-portfolio is among the most difficult opponents (the curve decreases slowly only). We can also observe from Figures [\ref=fig:learnRow] and [\ref=fig:learnCol] that against the Unif baseline UCBT learns a strategy that outperforms this opponent.

When it plays as the Black player, it takes less than 27 (128) games to learn the correct strategy and win with a 100 % ratio against every single deterministic variant. As the White player, it is even faster with only 25 games required to always win. Also, it is without surprise that the losing rate is lower when UCBT is the first player.

Chess: Figure [\ref=fig:chess:learnRow] (and Figure [\ref=fig:chess:learnCol]) shows the learning of UCBT for the Black player (and White respectively) for the game of Chess.

Again, as the number of iterations grows, there is a clear learning against both Nash and Unif baselines. UCBT eventually reaches, against Nash-portfolio, almost the value of the game for each player. Moreover, by looking at the slope of the curves, we see that the Nash-portfolio is among the most difficult opponents. We can also observe from Figures [\ref=fig:chess:learnRow] and [\ref=fig:chess:learnCol] that against the Unif baseline UCBT learns a strategy that outperforms this opponent. This is consistent with the theory behind UCBT.

When it plays as the Black player, it takes less than 27 games to learn the correct strategy and win with a 100 % ratio against every single deterministic variant. As the White player, it is even faster with only 26 games required to always win. In Section [\ref=xp:offline] we observe that the uniform strategy for the game of Chess is much more difficult to play against than the uniform strategy for the game of Go. Figures [\ref=fig:chess:learnRow] and [\ref=fig:chess:learnRow] corroborate this results as the slope of learning against the uniform strategy is less pronounced in Chess than in Go.

Havannah: Figure [\ref=fig:havanah:learnRow] (resp. Figure [\ref=fig:havanah:learnCol]) shows the learning of UCBT for the Black player (resp. White player) for the game of Havannah.

Once more, as the number of iterations grows, there is a clear learning against both Nash and Unif baselines. Moreover, by looking at the slope of the curves, we see that the Nash-portfolio is harder to exploit than other opponents, and in particular than the original algorithm, i.e. the uniform random seed. We can also observe from Figures [\ref=fig:havanah:learnRow] and [\ref=fig:havanah:learnCol] that against the Unif baseline UCBT learns a strategy that outperforms this opponent. However, it takes about 26 iterations before the learning really kicks in.

When it plays as the Black player, it takes less than 25 games to learn the correct strategy and win with a 100 % ratio against every single deterministic variant. As the White player, it is even faster with only 25 games required to always win.

Batoo: Figure [\ref=fig:batoo:learnRow] and Figure [\ref=fig:batoo:learnCol] show the learning of UCBT for the Black and White players respectively for the game of simplified Batoo.

Even though this game contains a critical simultaneous action at the beginning, the results are quite similar to the previous games. As the number of iterations grows, there is a clear learning against both Nash and Unif baselines. Moreover, by looking at the slope of the curves, we see that the Nash-portfolio is among the most difficult opponents - it is harder to exploit than the original algorithm with uniform seed. We can also observe from Figure [\ref=havanah:learning99go] that against the Unif baseline UCBT learns a strategy that outperforms this opponent.

When it plays as the Black player, it takes less than 27 games to learn the correct strategy and win with a 100 % ratio against every single deterministic variant. As the White player, it is even faster with only 27 games required to always win.

We now switch to UCBT applied to the Variants problem. The losing rates of the recommended variant are presented in Fig. [\ref=32:learning99go].

First and foremost, as the number of iterations grows, there is a clear learning against both Nash and Unif baselines. We see that (i) UCBT eventually reaches, against Nash-portfolio, approximately the value of the game for each player (ii) the Nash-portfolio is among the most difficult opponents (the curve decreases slowly only). We can also observe from Figure [\ref=32:learning99go] that against the Unif baseline UCBT learns a strategy that outperforms his opponent.

Conclusions

UCBT can learn very efficiently against a fixed deterministic opponent; this confirms its ability for eTeaching - a human opponent can learned her weaknesses by playing against a UCBT program. UCBT, after learning, performs better than Nash-portfolio against Uniform, showing that even against a stochastic opponent it can perform well, and in particular better than the Nash. This is not a contradiction with the Nash optimality; the Nash portfolio is optimal in an agnostic sense, whereas UCBT tries to overfit its opponent and can therefore exploit it better.

Generalization ability of online portfolios

We validated offline portfolios both against the GPPs used in the training, and against other GPPs. For online learning, the generalization ability does not have the same meaning, because online learning is precisely aimed at exploiting a given opponent. Nonetheless, we can consider what happens if we online learn random seeds against the uniform portfolio, and then play games against the original GPP.

The answer can be derived mathematically. From the consistency of UCBT, we deduce that UCBT-portfolio, against a randomized seed, will converge to Best Arm. Therefore, the asymptotic winning rate of UCBT-portfolio when learning against the original GPP, using a training against a fixed number of random seeds, is the same as shown for Best Arm in Section [\ref=xp:offline]: 62% in Go, 54% in Havannah, 53.5% in Chess, 71% in Batoo. In the case of Batoo we see that this generalization success rate is better than the empirical success rate from Fig. [\ref=fig:batoo]; this is not surprising as we consider the asymptotic success rate whereas we clearly see on Figure [\ref=fig:batoo] that the asymptotic rate is not yet reached.

Robustness: the transfer to other opponents

Results above were performed in a classical machine learning setting, i.e. with cross-validation; we now check the transfer, i.e. the fact that we improve a GPP not only in terms of winning rate against the baseline version, but also in terms of better performance when we test its performance

by playing against another, distinct, GPP;

by analysis with a reference GPP, stronger thanks to huge thinking time.

This means, that whereas previous sections have obtained results such as

"When our algorithm takes A as baseline GPP, the boosted counterpart A' outperforms A by XXX % winning rate. (with XXX>  50%)"

we get results such as:

"When our algorithm takes A as baseline GPP, the boosted counterpart A' outperforms A in the sense that the winning rate of A' against B is greater than the winning rate of A against B, for each B in a family { B1, B2, , Bk } of programs different from A."

Transfer to GnuGo

We applied BestArm to GnuGo, a well known AI for the game of Go, with Monte Carlo tree search and a budget of 400 simulations. The BestArm approach was applied with a 100x100 learning matrix, corresponding to seeds [formula] for Black and seeds [formula] for White.

Then, we tested the performance against GnuGo "classical", i.e. the non-MCTS version of GnuGo; this is a really different AI with different playing style. We got positive results as shown in Table [\ref=transfergo]. Results are presented for Black; for White the BestArm had a negligible impact.

Transfer: validation by a MCTS with long thinking time

Figure [\ref=compargo] provides a summary of differences between moves chosen (at least with some probability) by the original algorithm, and the ones chosen in the same situation by the algorithm with optimized seed. These situations are the 8 first differences between games played by the original GnuGo and by the GnuGo with our best seed.

We use GnugoStrong, i.e. Gnugo with a larger number of simulations, for checking if Seed 59 leads to better moves.

GnugoStrong is precisely defined as "gnugo -monte-carlo -mc-games-per-level 100000 -level 1". We provide below some situations in which Seed 59 (top) proposes a move different from the original Gnugo with the same number of simulations. Gnugo is not deterministic; therefore this is simple the 8 first differences found in our sample of games (we played games until we find 8 differences).

We consider that GnugoStrong concludes that a situation is a win (resp. loss) if, over 5 games played from this situation, we always get a win (resp. loss). The conclusions from this GnugoStrong experiment (8 situations) are as follows, for the 8 situations above respectively:

GnugoStrong prefers Top; Bottom is considered as a loss.

Here black moves are the same up to symmetries. Both are considered as wins for Black.

Both choices are considered as wins for Black.

Both choices are considered as wins for Black.

Both choices are considered as wins for Black.

GnugoStrong prefers Top; Bottom is considered as a loss.

GnugoStrong prefers Top; Bottom is considered as a loss.

GnugoStrong prefers Top; Bottom is considered as a loss.

As a conclusion, in 4 cases, GnugoStrong prefers the move chosen by the modified MCTS (with seed chosen by BestArm). In 4 cases, moves are equivalent.

Additional experiments: simpler tools

We have seen that the Nash portfolio has some advantages compared to the BestArm method, namely robustness against a learning opponent. On the other hand, the Nash mechanism induces a computational overhead and some implementation complexity. Therefore, we here investigate the relative performance of different methods:

Nash portfolio.

BestArm portfolio.

BestHalf, which is a uniform random choice among the options with performance better than the median performance over the learning set. In other words, given the matrix M, it selects the rows i with sum [formula] above the median. These options (up to ties and rounding, there are K / 2 such options) are played with the same probability.

Uniform, i.e. randomly choosing the option.

Exploiter, who "cheats" by choosing the best element in its portfolio . By definition, Exploiter can not play against itself.

The four first are policies which can be used in real life. The fifth one requires offline training, which is difficult in a competition unless your opponent has shared his program.

All results are obtained with learning matrices distinct for Black and White so that there is no overfitting bias.

Results are provided in Fig. [\ref=OTchess] for Chess (portfolio of seeds) and Go (portfolio of variants), [\ref=OThavannah] for Havannah and Batoo (portfolio of seeds). As a conclusion:

BestHalf is a good solution, performing similarly to Nash, including in terms of results against Exploiter, whereas it is very simple.

The case of "variants" has non binary outputs; and, consequently, it is less prone to overfitting; even BestArm performs reasonably well against Exploiter.

Conclusion

We proposed three algorithms for combining policies:

Three offline algorithms, namely the Nash-Portfolio, the simpler BestHalf-portfolio which approximates Nash-Portfolio quite well, and the Best Arm Portfolio. Given a family of algorithms, or a single algorithm "exploded" into several algorithms (by the use of random seeds or by varying its parameterizations), they build, offline, a new algorithm.

The Bandit(UCBT)-Portfolio, which learns online, given an opponent; this one is adaptive.

We tested them on Go, Chess, Havannah, Batoo. Other publications include experiments on Domineering, Atari, Breakthrough [\cite=demoCIG]. A work on Tsumego has investigated the use of seeds learnt online (see results comparing MCTS(1) to other methods in  [\cite=nashtsumegos]).

We have seen that:

The Nash-Portfolio is more diversified than any of its components in the sense that it is harder to learn against (i.e. harder to exploit) than any of its components and harder to learn against than the uniform-Portfolio. Best Arm is less resilient (easy to exploit, converging to 0% success rate against UCBT portfolio), though quite efficient against the uniform-Portfolio.

The UCBT-Portfolio can learn a combination until reaching optimal exploitation of a stationary opponent (this is mathematically guaranteed by properties of UCBT, which is a consistent bandit algorithm in the discrete setting). In particular, it defeated clearly each deterministic variants in Fig. [\ref=learning99go], reaching 100% winning rate. It also performs quite well against the default policy, which is uniformly randomized random seed. This shows that we can enhance a randomized algorithm a lot, just by biasing the choice of the random seed. The Nash-Portfolio resists much better (than uniform or Best Arm) to a UCBT-portfolio, showing that the Nash portfolio is hard to exploit. Best Arm is, by construction, the asymptotic limit of UCBT learning against the uniform portfolio, and against the original algorithm.

The Nash-portfolio and Best Arm portfolio perform well against the original GPP (Section [\ref=xp:offline]). Therefore our tools provide an easy improvement on top of randomized algorithms, or on top of variants of an algorithm. Results in Section [\ref=robust] show the robustness of the results, in the sense that the improved GPP is not only winning against the original GPP, but it also has a better winning rate against other opponents than the ones used in the learning.

The computational cost could potentially become an issue when combining parameterizations (in Section [\ref=ssec:algo] - with large computational cost because we averaged multiple games for building the matrix). It is not the case for combining random seeds (Section [\ref=ssec:set]), where deterministic games are used and therefore there is no point in duplicating games. It should also be pointed out that solving Nash is fast, and if needs be, we can further the speed of the computation using algorithms that can ε-approximate a NE in sublinear time [\cite=grigoriadis] [\cite=auer95gambling] - so that a number of games linear in max (K,K') log (KK') / ε2 (i.e. far less than the number K  ×  K' of elements in the matrix) is sufficient for a precision ε. Importantly, even in the case of combining variants, all computational costs are offline, so that there is no significant online computational overhead. Results are threefold:

An improvement in terms of playing strength measured in direct games, as our Best Arm, the Best Half policy suggested by a reviewer, and our Nash-Portfolio all outperform the original randomized algorithm.

An improvement in terms of eTeaching (use of our program as a teaching tool); our UCBT-Portfolio algorithm is adaptive and difficult to overfit. This does not involve any additional computational power as UCBT is online and has a negligible internal cost. This makes our UCBT tool suitable for online learning.

An improvement in terms of resilience; for learning size at least 40 seeds, our Nash-Portfolio is harder to overfit than any of its components and than the uniform portfolio (in Fig. [\ref=learning99go] - [\ref=32:learning99go]). At the end of the offline computation of the Nash equilibrium, it is just a bias in the random seed distribution, so the additional computational cost is negligible. The non-intuitive key point in the "random seed" part of this work is that biasing the random seed has an impact.

Further work.

The easiest and most promising further work consists in using fast approximate Nash equilibria, so that the full matrix does not have to be computed. This should extend by far the number K of arms that our method can handle. Another further work is the use of discounted bandits for the UCBT bandit, so that very old games have little influence on the current games - this should bring improvements in terms of adaptivity, when playing against a non-stationary opponent such as a human. An infinite set of seeds should be considered in UCBT, so that exact optimality might, asymptotically, be reached. An adapted bandit algorithm already exists for such cases [\cite=wamcorr]. We have considered random seeds for whole runs; more specialized representations are under work. The present work might indeed be used for deterministic algorithms without any parameterization: we might e.g. randomize the parameters of an evaluation function used in an alpha-beta algorithm.

Acknowledgements

We are grateful to the Dagstuhl seminar for interesting discussions around coevolution.