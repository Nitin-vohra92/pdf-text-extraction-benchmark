The Constrained Maximum Likelihood Estimation For Parameters Arising From Partially Identified Models

Introduction

In some scientific studies, due to constraints of logistics and/or resources, data are not collected in the ideal way. Consequently, the available data may only partially identify the statistical model under consideration, i.e., parameters of the statistical model are identified up to a set of possible values instead of just one single value. The set of parameter values that correspond to the same distribution of observables is usually termed the identification region. [\citet=Manski2003] gives an overview of partial identification and covers many scenarios where partial identification may arise.

Of course, point-identification is preferred as it is fundamental for consistent point estimation and ensures many nice properties of model-based parameter estimators. With a partially identified model, when possible one may impose some reasonable assumptions to achieve point-identification. Under such assumptions, the parameter vector will be restricted to a subset of the original parameter space. If this constrained parameter space has only a single point of intersection with the identification region, then the parameter vector is uniquely identified.

In this paper, we study the maximum-likelihood estimation of parameters arising from a partially identified model with some equality constraints introduced by additional assumptions. In particular, we consider the scenario where there exists a special re-parameterization of all parameters of the model, which is termed a transparent re-parameterization by [\citet=GustafsonGelfandSahuEtAl2005], such that the distribution of observables is completely determined by a proper subset of parameters after transformation.

In the situation of adding parameter constraints to a model which is identified even without the constraints, [\citet=AitchisonSilvey1958] characterized the large-sample behavior of maximum-likelihood estimators via a Lagrange multiplier approach. However, the assumption that the unconstrained version of the model is identified is embedded in their approach. Therefore, our work extends their theory to the situation that identification is only obtained via imposition of the constraints.

Of course, point-identification is preferred as it is fundamental for consistent point estimation and ensures many nice properties of model-based parameter estimators. With a partially identified model, when possible one may impose some reasonable assumptions to achieve point-identification. Under such assumptions, the parameter vector will be restricted to a subset of the original parameter space. If this constrained parameter space has only a single point of intersection with the identification region, then the parameter vector is uniquely identified.

The paper is organized as follows. We first introduce some general notation and give a mathematical formulation of the problem. We then prove the existence of the constrained maximum likelihood estimate and show that the estimator is asymptotically normally distributed. A numerical algorithm for computing the constrained maximum likelihood estimate is also developed. We then use a simulation study to compare the performance of the proposed method and the general method, which does not depend on constraints, to investigate the effect of imposing additional assumptions with a partially identified model. Moreover, we comment on a special situation where there is no benefit in terms of estimation efficiency. Finally, we present some concluding thoughts.

Statistical problem

Suppose our data consist of n observations [formula]. The statistical model underlying the data is assumed to be initially parameterized in scientific terms via a vector of s parameters. Let [formula] be a re-parameterization of the original parameters such that the log-likelihood function [formula] for the observed data can be completely determined by its first r elements, say [formula], through

[formula]

where [formula] denotes the probability density function for an individual observation x. The remaining s - r parameters of [formula] are represented by another vector [formula], which cannot be learned from the observed data. Thus, [formula] is partially identified with the identified part [formula] and the unidentified part [formula]. To further identify [formula], we make additional assumptions that impose t equality constraints on [formula]:

[formula]

To identify s - r unidentified parameters, we need at least s - r equations. Also, it should be reasonable to assume that the number of constraints does not exceed the number of identified parameters, which is necessary for the development of our method. Thus, we assume that s - r  ≤  t  ≤  r. Note that the true, though unknown, parameter value [formula] is presumed to satisfy these constraints itself, i.e., [formula].

The objective is to find the constrained maximum likelihood estimate [formula] that maximizes the log-likelihood function [formula] subject to the condition [formula]. Let (u) denote the unconstrained maximum likelihood estimate obtained by the general method concerning purely the log-likelihood function [formula]. If the equation [formula] with respect to [formula] has a solution, say (c), then   =  ((u),(c)) forms the constrained maximum likelihood estimate of the problem. This approach may fail, however, since the equation [formula] with respect to [formula] may not necessarily have a solution for some values of [formula]. Alternatively, we propose to estimate [formula] by maximizing [formula], where [formula] is a Lagrange multiplier. Suppose (,) solves the following s + t equations:

[formula]

where [formula] is the score vector of length r whose i-th component is [formula], for [formula], [formula] is the r  ×  t matrix [formula], for [formula], [formula], and [formula] is the (s - r)  ×  t matrix [formula], for [formula], [formula]. Then [formula] should be the constrained maximum likelihood estimate.

The constrained maximum likelihood estimation

In this section, we will show that, under some general conditions, if [formula] belongs to a set whose probability measure tends to 1 as n approaches infinity, then the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) have a solution (,) such that [formula] is within a small neighborhood of the true value [formula]. This solution is proved to be the constrained maximum likelihood estimate that maximizes [formula] subject to [formula]. We then extend the definition of (,) for all [formula], and show the asymptotic distribution of the random variable thus defined. Finally, we propose an algorithm for numerically computing (,). The development of this section is based on the work by [\citet=AitchisonSilvey1958]. However, due to the presence of the unidentified component [formula], our work is more than a simple generalization of their theory.

We first impose some conditions on [formula] and [formula] within some neighborhood of [formula], say [formula]. We assume that [formula] satisfies the conditions (F1) - (F4) as defined in [\citep=AitchisonSilvey1958]. These conditions are quite general and will be satisfied in most practical estimation problems. Here, we just write one important result implied by these conditions for later reference. If the conditions on [formula] are satisfied, for any given positive numbers δ  <  α and ε  <  1 and for sufficiently large n  ≥  n(δ,ε), there exists a set [formula] with the properties

[formula].

[formula], if [formula].

[formula] can be expressed in the form [formula], where [formula] is the matrix [formula], [formula], [formula] is a certain positive definite matrix, and [formula] is an r  ×  r matrix, the moduli of whose elements are bounded by 1, if [formula].

For every [formula] there exists a constant, say κ1, such that

[formula]

for all [formula], if [formula].

On the other hand, some conditions are assumed for the constraint function [formula] as follows.

For all [formula], the first order partial derivatives [formula], [formula], [formula], exist and they are continuous function of [formula].

For all [formula], the second order partial derivatives [formula], [formula], [formula], exist and [formula] is bounded by a given constant, say 2κ2, for all i, j and k.

The r  ×  t matrix [formula] and the (s - r)  ×  t matrix [formula] are both of full rank, i.e., [formula] and [formula].

Existence of the constrained maximum likelihood estimate

We begin by establishing a necessary and sufficient condition for the existence of a solution of the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) under some general conditions. It should be noted that the following lemma cannot be directly generalized from the Lemma 1 in [\citep=AitchisonSilvey1958] by simply viewing the log-likelihood function as a function of [formula] and letting [formula] be the s  ×  s matrix that naturally extends [formula], due to the singularity of [formula] thus defined. Therefore, some modifications are required.

Subject to conditions on f and [formula], if δ  <  α and ε < 1 are some given positive numbers and if [formula], then the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) have a solution (, ) such that ∈Uδ, if and only if [formula] satisfies a certain equation of the form [formula], in which [formula] is an s  ×  s matrix with two blocks on the diagonal being [formula] and [formula], and [formula] is a continuous function on Uδ and [formula] is bounded for ω∈Uδ by a positive number [formula].

We first prove the necessity of the condition. By expanding the components of [formula] around [formula] in the equation ([\ref=eq:lag:e1]), and the components of [formula] around [formula] in the equation ([\ref=eq:lag:e3]), we find that the solution of the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) should also satisfy:

[formula]

where

[formula] is a vector of length r whose m-th component is

[formula]

where [formula] is the matrix [formula], [formula], with [formula] being a point such that [formula], and

[formula] is a vector of length s whose m-th component is

[formula]

where [formula] is the matrix [formula], [formula], with [formula] being a point such that [formula].

Further, given property (X3) , we can re-write the equations ([\ref=eq:lemma:e1]) and ([\ref=eq:lemma:e2]) in the following form:

[formula]

where

[formula]

Moreover, by properties (X2) - (X4), we obtain a bound for [formula] as

[formula]

and, by condition (H2), we have a bound for [formula] as

[formula]

Next, since [formula] is positive definite, we can pre-multiply the equation ([\ref=eq:lemma:e3]) by [formula] to get an expression for [formula], which is then plugged into the equation ([\ref=eq:lemma:e4]) to obtain the following equation

[formula]

Now the condition (H3) implies that [formula] is also positive definite. Besides, according to the condition (H1), the elements of [formula] are all continuous functions of [formula]. It then follows that [formula] is also non-singular within Uδ for sufficiently small δ. Thus, we can solve the equation ([\ref=eq:lemma:e5]) with respect to [formula] and express it in terms of [formula]

[formula]

where we define the notation [formula].

So far, we are basically replicating the steps of the proof given by [\citet=AitchisonSilvey1958]. Now, we need to take some extra steps to find the expression for [formula]. By applying the equation ([\ref=eq:lemma:e6]) to substitute for [formula], the equation ([\ref=eq:lag:e2]) becomes:

[formula]

Following the same argument for [formula], the condition (H4) ensures that the matrix [formula] is not singular within a sufficiently small neighborhood of [formula]. Thus, we can solve the equation ([\ref=eq:lemma:e7]) with respect to [formula] and get

[formula]

where

[formula]

We then plug the equation ([\ref=eq:lemma:e8]) into the equation ([\ref=eq:lemma:e6]) and derive an updated expression for [formula]:

[formula]

where

[formula]

By combining the equations ([\ref=eq:lemma:e3]) and ([\ref=eq:lemma:e8]), with [formula] substituted using the equation ([\ref=eq:lemma:e9]), we find that the solution of the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) should also satisfy

[formula]

where

[formula]

and

[formula]

Finally, we have shown in the inequalities ([\ref=eq:lemma:ie1]) and ([\ref=eq:lemma:ie2]) that [formula] and [formula] are bounded within Uδ. Also, given that [formula] and [formula] are positive definite within the closed set Uδ, their determinants are both positive within Uδ. Therefore, the continuity of the elements of these two matrices ensures that their determinants are uniformly bounded within Uδ. Then it follows that [formula] is a continuous function on Uδ and [formula] is bounded by a positive number, say [formula], for all [formula].

Now, we prove the sufficiency of the condition. Suppose the equation ([\ref=eq:lemma:e10]) has a solution [formula]. That is, [formula] satisfies

[formula]

By pre-multiplying the equation ([\ref=eq:lemma:e11]) by the t  ×  s matrix [formula], we have

[formula]

We first write [formula] and [formula] as the remainders after expanding [formula] and [formula], respectively,

[formula]

Applying the equations ([\ref=eq:lemma:nu1]) and ([\ref=eq:lemma:nu2]) to substitute for [formula] and [formula] in the equations ([\ref=eq:lemma:nu3]) and ([\ref=eq:lemma:nu4]), respectively, we get

[formula]

Finally, we substitute for [formula] in the equation ([\ref=eq:lemma:e12]) using the equation ([\ref=eq:lemma:nu4:new]). It immediately follows that [formula].

Next, we apply the equations ([\ref=eq:lemma:nu3:new]) and ([\ref=eq:lemma:nu4:new]) to substitute for [formula] and [formula] in the equations ([\ref=eq:lemma:nu5]) and ([\ref=eq:lemma:nu6]), and end with the following expressions for [formula] and [formula]:

[formula]

where [formula] is defined as [formula]. Now, by using the equations ([\ref=eq:lemma:nu3:new]), ([\ref=eq:lemma:nu5:new]) and ([\ref=eq:lemma:nu6:new]) to substitue for [formula], [formula] and [formula] in the equation ([\ref=eq:lemma:e11]), respectively, we can see that [formula] satisfies

[formula]

As we have shown earlier that [formula], it is easy to see that [formula], jointly with [formula], solves the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]).

We now give the following theorem to show the existence of a solution of the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]).

Subject to conditions on f and [formula], if δ is a sufficiently small given positive number, ε is a given positive number less than 1 and if [formula], then the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) have a solution (,) such that ∈Uδ.

The proof of Theorem 1 in  [\citep=AitchisonSilvey1958] works here, provided the modified version of Lemma [\ref=thm:l1] given above is used. Also, it is important to notice that the matrix [formula] defined in Lemma 1 is positive definite provided that [formula] is positive definite, and its minimum latent root is min {μ0,1}, where μ0 is the latent minimum root of [formula]. Details are omitted.

For the remainder of this section, we are going to show that the solution of the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) as stated in Theorem [\ref=thm:t1] locally maximizes the log-likelihood subject to the constraints. This result was proved in [\citep=AitchisonSilvey1958] for the identified model. However, we are not able to prove this result for the partially identified model with a direct extension of their proof. Alternatively, we take another route and use the approach detailed by [\citet=Spring1985].

To match with the set-up in [\citep=Spring1985], we change the order of variables and let [formula]. Let [formula] denote the second order partial derivatives of the Lagrangian function [formula] evaluated at the critical point   =  (,)

[formula]

where

[formula]

with [formula] being the upper-left r  ×  r block matrix, [formula] being the bottom-left r  ×  (s - r) block matrix, and [formula] being the bottom-right (s - r)  ×  (s - r) block matrix. Let Λ(n)k denote the principal upper left k-th order minor of the Hessian Matrix [formula]. According to Theorem 1 in [\citep=Spring1985], [formula] locally maximizes the log-likelihood function subject to the constraints, so long as ( - 1)t + pΛ(n)2t + p, [formula], are all positive.

Note that [formula] was defined as [formula]. For any small number δ, by the equation ([\ref=eq:lemma:nu3:new]) and the inequality ([\ref=eq:lemma:ie1]), if n is sufficiently large, we have

[formula]

where κ3 is a positive number that depends only on the elements of [formula]. Also, the elements of [formula] are bounded by a number independent of δ for ∈Uδ. Therefore, we have

[formula]

where κ4 and κ5 are positive numbers independent of δ. That is, [formula] converges to [formula] as n goes to infinity. By condition H2, the second partial derivatives [formula], [formula], [formula], are all bounded by a constant 2κ2. Thus, it follows that [formula], [formula], and [formula]. Also, it is easy to see from Theorem 1 that, for ∈Uδ with sufficiently small value of δ, [formula] converges to [formula] as n goes to infinity. By condition (H1), the elements of [formula] and [formula] are all continuous functions of [formula]. Thus, as n goes to infinity, [formula], [formula], and [formula] approach [formula], [formula] and [formula], respectively. Furthermore, by property (X2), we have [formula] approaches [formula] as n goes to infinity. Finally, we have [formula] converges to [formula] as n goes to infinity, where

[formula]

Then, for sufficiently large n, the signs of the leading principal minors of [formula] are the same as those of their corresponding minors of [formula]. Therefore, we can instead study the signs of the leading principal minors of [formula].

For brevity, we suppress the subscripts [formula] and [formula]. First, given that [formula] is positive definite, by Sylvester's criterion the upper left d  ×  d corner matrix of [formula], denoted by [formula], is also positive definite, for [formula]. Next, since [formula], with some re-ordering of the rows if necessary, the first d rows of [formula], denoted by [formula], is a d  ×  t matrix of full column rank t, and thus the matrix [formula] is positive definite, for [formula]. Similarly, as [formula], the first d rows of [formula], denoted by [formula], is a d  ×  t matrix of full row rank d, and thus the matrix [formula] is again positive definite, for [formula]. Now we are ready to study the sign of ( - 1)t + pΛ(  ∞  )2t + p, for [formula]. On one hand, for [formula], we have

[formula]

On the other hand, for [formula], we have

[formula]

Therefore, we have shown that ( - 1)t + pΛ(  ∞  )2t + p, [formula], is always positive, and so is ( - 1)t + pΛ(n)2t + p for sufficiently large n. Thus, it follows that [formula] is the constrained maximum likelihood estimator of the problem.

Asymptotic distributions

In this section, we define sequences [formula] that extends (,), as stated in the Theorem 1, for all [formula], and develop the asymptotic distribution for (n,n). Note that this section differs from the Section 5 of [\citep=AitchisonSilvey1958] in that the covariance matrix here becomes a partitioned matrix of 3  ×  3 blocks.

The following partitioned matrix is non-singular.

[formula]

For brevity, we omit the suffix [formula] and [formula]. Then we wish to find a matrix

[formula]

such that

[formula]

Since [formula] is positive definite, and [formula] and [formula] are of full rank, it can be solved that

[formula]

and [formula], [formula], and [formula] are the transposes of [formula], [formula], and [formula], respectively, as it is easy to see that the matrix is symmetric.

Suppose [formula], δ is small enough for Theorem [\ref=thm:t1] to apply, and (,) is a solution of equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) such that ∈Uδ. We now write the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) in a different form:

[formula]

where [formula], [formula], [formula], [formula], and [formula] are matrices whose elements tend to 0 as δ goes to 0. Thus, by Lemma [\ref=thm:l2], if δ is sufficiently small, then the matrix

[formula]

is also non-singular and we write its inverse as

[formula]

Thus, if δ is sufficiently small and if [formula], we can solve from the equation ([\ref=eq:lag:mf1]) that

[formula]

Since the asymptotic distribution of [formula] is known, we can use the above relationship to induce the asymptotic distribution of (,). However, this may only be valid for [formula], and we need to extend it to also account for [formula].

Let (δm), (εm) be two decreasing sequences of positive real numbers, such that δ1  <  μ1  /  κ3, ε1  <  1, and δm and εm both tend to 0 as m goes to infinity. Define an increasing sequence (nm) of integers such that, if n  ≥  nm, there exists a set [formula] with properties (X1) - (X4) for ε  =  εm and δ  =  δm. For [formula], if nm  ≤  n  <  nm + 1, we choose a set [formula] with properties (X1) - (X4) for ε  =  εm and δ  =  δm. When [formula], the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]) have a solution (n,n) such that [formula], with n being the constrained maximum likelihood estimate for [formula]. Thus, n and n satisfy the equation ([\ref=eq:lag:mf2]). When [formula], we define

[formula]

where [formula], i,j  =  1,2,3, are defined in the proof of Lemma [\ref=thm:l2]. Note that the probability of [formula] goes to zero as n goes to infinity. Thus, we have defined two sequences of random variables, (n) and (n), [formula], which have the property that n converges in probability to [formula] as n goes to infinity. Moreover, n and n jointly satisfy the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]).

[formula]

If [formula], we define [formula], i,j  =  1,2,3. Then, for sufficiently large n, we have

[formula]

Since [formula], [formula], [formula], [formula], and [formula] all tend to [formula] as δ  →  0, it follows that the elements of

[formula]

converge in probability to the elements of

[formula]

Moreover, it is known that the asymptotic distribution of [formula] is normal with mean zero and asymptotic variance [formula]. Thus, we have

[formula]

It then follows that the asymptotic distribution of [formula] is

[formula]

Finally, using the expressions for [formula], i,j  =  1,2,3, that were derived in the proof of Lemma [\ref=thm:l2], it can be verified that the asymptotic variance is

[formula]

The result then follows.

Numerical algorithm

The solution of the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]), say   =  (,), usually does not have a closed form, and thus must be computed numerically. We may immediately consider the Newton-Raphson method to solve the problem. However, that method requires the form of the Hessian matrix of [formula], which is an s  ×  s matrix and may be very complicated, especially when s is large. Thus, we follow the approach proposed by [\citet=AitchisonSilvey1958] and develop an algorithm that is easier to implement.

Suppose [formula] is an initial guess for [formula] such that [formula] is small. Then we consider a first order of approximation to [formula] and [formula]:

[formula]

Also, we assume that [formula] is close to [formula] when n is large. Then to a first order of approximation, we have

[formula]

Since [formula] and [formula] jointly satisfy the equations ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]), they should also approximately satisfy

[formula]

When n is large, [formula] should be close to [formula]. Thus, we use [formula] to approximate [formula]. Finally, we have the formula for updating [formula], and in general for updating [formula] in the r-th iteration,

[formula]

If the sequence [formula] converges, then it converges to a solution of the equation ([\ref=eq:lag:e1]) - ([\ref=eq:lag:e3]). Finally, it should be noted that [formula] is actually missing from the right hand side of the above equation. Thus, the updating procedure only needs to store the current value of [formula] for the next iteration.

Example problem and simulation study

In this section, we use the proposed method to solve a missing data problem, where parameters associated with the missing mechanism may only be identified with additional assumptions. This sort of problem might otherwise be tackled with an expectation-maximization algorithm. More specifically, consider a binary response variable Y and two binary explanatory variables X1 and X2. The probability of having Y = 1 given (X1,X2) is assumed to be determined through a logistic model:

[formula]

Suppose we can observe X1 and X2 for everyone sampled, but the status of Y is missing for some people. Let R indicate missingness. The data structure is displayed in Table 1, where nijk is the number of subjects with complete data of (Y = i,X1  =  j,X2  =  k,R = 1), and mjk is the number of subjects with incomplete data of (X1  =  j,X2  =  k,R = 0), i,j,k  =  0,1. The corresponding cell probabilities, as enclosed in parentheses in the Table [\ref=tab:example], are

[formula]

for i,j,k  =  0,1. Based on Table [\ref=tab:example], the log-likelihood of data is:

[formula]

In order to understand the relationship between Y and (X1,X2), we need to infer the proportions of subjects with Y = 1 among the groups of incomplete data

[formula]

for j,k  =  0,1. However, these quantities are not identifiable from data without additional assumptions.

Now, we make two assumptions. First, we assume that the status of Y is missing at random, i.e., R and Y are conditionally independent given (X1,X2). This assumption imposes four constraints on parameters, and implies

[formula]

for j,k  =  0,1. Secondly, we assume that the effects of X1 and X2 on Y are additive on the logit scale, which means that the interaction effect β3 is zero. This assumption introduces one more constraint on parameters as

[formula]

Under these two assumptions, we can apply the proposed method to obtained the maximum likelihood estimates r̂ijk, jk, and jk, i,j,k  =  0,1, subject to the above five constraints. Next, the constrained maximum likelihood estimates for the main effects of X1 and X2 can be deduced through

[formula]

and the corresponding estimated variances can be obtained by the delta method.

Finally, based on the above problem, we conduct a simulation study to illustrate the performance of the proposed method. In particular, we randomly generate 10000 datasets of size 1000 under the parameter setting [formula], β1  =   log 2, β2  =   log 3, β3  =  0, and

[formula]

For each dataset, we apply the proposed method to obtain the constrained maximum likelihood estimates for 1 and 2, and the associated 95% confidence intervals. Our simulation results show that the empirical biases for the estimators of β1 and β2 are 0.0033 and 0.0029, respectively. Correspondingly, the coverage probabilities of the 95% confidence intervals are 95.1% and 95.2%, which match well with the nominal level. We can see that the proposed method performs very well.

Just- and over-identified Situations

In the previous section, we have considered a partially identified model with four non-identifiable parameters and made additional assumptions that impose five constraints on parameters. Consequently, the constrained maximum likelihood estimators for the identifiable parameters, rijk's and sjk's, i,j,k  =  0,1, differ from their unconstrained estimators. More importantly, comparing to the unconstrained estimators, the constrained estimators are associated with smaller variances. For example, under the parameter setting considered in the previous section, the asymptotic distribution of the the unconstrained estimator for [formula] is

[formula]

and the asymptotic variance of the corresponding constrained estimator is

[formula]

By comparing the elements along the diagonal of these two matrices, it is clear that the constrained estimator is more efficient than the unconstrained estimator for the problem considered in the previous section.

However, if we only make the missing at random assumption and allow the model for Y|X1,X2 to be saturated, then we have only four constraints for four non-identifiable parameters. In this case, we find that the constrained and unconstrained maximum likelihood estimators for the identifiable parameters always coincide and have the same asymptotic distribution. Thus, making the missing at random assumption alone leads to no efficiency gain.

Generally, we say that the parameters are over-identified when the number of constraints is greater than the number of unidentified parameters. In this case, the constrained maximum likelihood estimator differs from the unconstrained estimator and achieves better efficiency. On the other hand, we say that the parameters are just-identified when the number of constraints is equal to the number of unidentified parameters. If that is the case, the constrained estimator will coincide with the unconstrained estimator, at least asymptotically. Moreover, identifying the unidentified parameters uses up the information provided by the additional constraints and thus an more efficient estimator is not available. This phenomena was also observed by [\citet=ChenChen2011] in the context of a gene-environment independence problem.

Discussion

Parameters arising from a partially identified model can be estimated when we have enough equality constraints enforced by additional assumptions. The constrained maximum likelihood estimate for the identified part may or may not coincide with its unconstrained counterpart. When they do not coincide, the constrained version will have a lower estimated variance.

Another possibility for estimating parameters of a partially identified model subject to constraints is to exploit a reduced-form parameterization that is free of constraints. However, the capability of such approach is limited, as a closed form for a reduced-form parameterization is often very complicated or even sometimes not available. In contrast, the method presented in this paper is applicable in more general settings. Moreover, since the log-likelihood function is usually expressed in its simplest form with a transparent re-parameterization, taking the second partial derivatives of the log-likelihood function becomes much more straightforward. Thus, the proposed method is also advantageous in terms of calculation.

Finally, the proposed method assumes that the partially identified model can be understood through a transparent re-parameterization that separates the identifiable parameters from non-identifiable parameters. Unfortunately, such kind of re-parameterization does not always exist. [\citet=Gustafson2009] gives two examples that do not admit a transparent re-parameterization. In that case, the proposed method may not be applicable.