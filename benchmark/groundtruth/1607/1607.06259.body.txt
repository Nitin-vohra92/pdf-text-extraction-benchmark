N(N)LO event files: applications and prospects

n-Tuples

The calculation of high multiplicity processes at NLO in now possible for a large number of processes in a largely automated way (see e.g. [\cite=Badger:2016bpw] for a review). While they can be automated, high multiplicity NLO calculations still require a large amount of CPU time to be performed with an acceptable statistical precision. The most expensive part of the calculation is the computation of the matrix elements. Other parts of the calculation such as the evaluation of the parton distribution functions, jet clustering and the construction of observables and differential cross sections to be histogrammed are typically much less demanding. In practice this also requires to evaluate very similar expressions, differing only by a factor in the factorisation or renormalisation scales or by the PDF used. To avoid recalculations of the same matrix element many times one can store the matrix elements, the kinematic information and the coefficients of logarithms involving the renormalisation and factorisation scales in a file and read them back in, instead of recalculating the whole matrix element. This strategy, using ROOT [\cite=ROOT] as the storage back-end, has been described in [\cite=Bern:2013zja].

Besides the computational efficiency, this strategy has the advantage of facilitating the dissemination of the NLO results. The clear disadvantage is that the event files tend to be very large. The advantages outweigh this disadvantage for NLO calculations and this strategy has been used to produce NLO predictions [\cite=Z4] [\cite=W4] [\cite=pureQCD] [\cite=Greiner:2015jha] [\cite=Badger:2013yda] [\cite=Badger:2013ava] for many Standard Model measurements [\cite=WD0] [\cite=W7TeVCMS] [\cite=Z7TeVCMS] [\cite=Jets8TeVAtlas] [\cite=RratioAtlas] [\cite=W7TeVAtlas] [\cite=Z7TeVAtlas].

It is natural to wonder whether a similar approach would facilitate the calculation and dissemination of NNLO calculations. The advantages and disadvantages are the same: the matrix elements are computationally expensive to calculate and would benefit from being stored but the amount of events needed would lead to large file sizes. A first study in ref. [\cite=Badger:2016bpw] investigated the size of NNLO event files using the EERAD3 program [\cite=EErad3].

One significant change encountered when going from NLO to NNLO is the increase in the number of subtraction terms, which leads to a large number of phase-space configurations that need to be stored in the event file. Instead of storing each set of mapped momenta, one can store the original momenta they are mapped from, along with a reference to the procedure with which the mapped momenta are arrived at. This saves about an order of magnitude of storage space, at the cost of delegating the calculation of the mapped momenta to the user of the event file. Figure [\ref=fig:sizes] shows the storage sizes as a function of the number of phase-space points.

Based on the extrapolation in figure [\ref=fig:sizes] the usage of NNLO event files appears feasible in this case. The study in [\cite=Badger:2016bpw] used a leptonic initial state. One can anticipate that processes with hadronic initial states will be more challenging and require higher statistics, as there are additional integrations over the initial momentum fraction of the partons. This can make the prospects of using event files less favourable. However, there are several reasons to think hadronic processes might still be tractable: a) so far NNLO calculations have been optimised for CPU time, one could optimise them with respect to storage space (using the mapping information is an example of such an optimisation), b) new subtraction techniques such as qT-subtraction [\cite=Catani:2007vq] and n-jettiness [\cite=Stewart:2010tn] [\cite=Gaunt:2015pea] [\cite=Boughezal:2015dva] offer an easier subtraction structure which is closer to that of two separate NLO calculations than "genuine" NNLO subtraction schemes [\cite=GehrmannDeRidder:2005cm] [\cite=Currie:2013vh] [\cite=Czakon:2010td]. This "NLO-like" structure should lead to more efficient storage possibilities.

Strong coupling determination

The ability to recalculate the NLO predictions for a high multiplicity process relatively cheaply offers new opportunities for these processes to be used in phenomenological applications. We explore one of them in this section, namely extracting the value of the strong coupling constant from Z + 2,3,4   measurements. The reason for using high multiplicity processes is that they display an increasingly strong dependence on αS(MZ) as the multiplicity increases, as illustrated in figure [\ref=fig:alpha].

For our αS extraction we use the data from [\cite=Khachatryan:2014waa] for the rapidity and transverse momentum distribution of the n-th jet in Z + n  jets events. We compare it with predictions obtained from the BlackHat+Sherpa collaboration [\cite=Z4]. We used the n-Tuples they provide to generate a fastNLO grid [\cite=fastNLO] to speed up the calculation of the PDF covariance matrices and scale variations. The fit is obtained by minimising the χ2 function

[formula]

where yt is the theory prediction and yd are the experimental values. The covariance matrix C is given by

[formula]

with Cexp the experimental error covariance matrix, Cpdf the PDF covariance matrix obtained using the LHAPDF library [\cite=LHAPDF] and Ctheory the statistical covariance matrix of the theory prediction.

Figure [\ref=fig:all] shows the preliminary results for the three PDF sets MSTW2008 [\cite=MSTW2008], CT10 [\cite=CT10] and NNPDF 2.3 [\cite=NNPDF23]. These sets were chosen to facilitate the comparison with the results obtained in ref. [\cite=Khachatryan:2014waa]. The results are given for fits to different sets of histograms: a) to the transverse momentum and rapidity of the n-th pT-ordered jet for each multiplicity individually, b) for the combination of the transverse momentum and rapidity for each multiplicity, c) for the combination of all three transverse momentum distributions, d) for the combination of all the rapidity distributions, e) for a combination of all histograms.

Conclusion

In this contribution we reported on an investigation of the suitability of the n-Tuple strategy for NNLO calculations. This strategy seems tractable for processes with no hadronic initial states and while the jury is still out for the case of hadronic initial states we have good reasons to think it will also be tractable in these cases. We also presented preliminary results for an extraction of the strong coupling constant from high multiplicity processes. The accuracy obtained for the value of αS(MZ) seems comparable with other determinations at the LHC.