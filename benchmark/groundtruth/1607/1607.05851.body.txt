hyperref

Learning to Recognize Objects by Retaining other Factors of Variation

Introduction

Images are generated under factors of variation, including shape, texture, pose, illuminations etc. Recently, deep ConvNet architectures learn rich and high-performance features by leveraging millions of labeled images, and have achieved state-of-the-art object recognition performances. Contemporary CNNs, such as AlexNet [\cite=krizhevsky2012imagenet], VGG [\cite=simonyan2014very], GoogLeNet [\cite=szegedy2015going] and ResNet [\cite=he2015deep], pose object recognition as a single task learning problem, and learn features that are sensitive to object categories but invariant to other nuisance information (e.g., pose and illumination) as much as possible. To achieve this, current CNNs usually stack several stages of subsampling/pooling [\cite=lecun1998gradient] and apply various normalization operations [\cite=krizhevsky2012imagenet] [\cite=ioffe2015batch] to make representations invariant to small pose variations and illumination changes. However, as argued by Hinton et al [\cite=hinton2011transforming], to recognize objects, neural networks should use "capsules" to encode both identity and other instantiation parameters (including pose, lighting and shape deformations). In [\cite=bengio2009learning] [\cite=reed2014learning], authors argue as well that image understanding is to tease apart these factors, instead of emphasizing on the current factor by disregarding other factors.

In this work, we formulate object recognition as a multi-task learning (MTL) problem by taking images as inputs and learning both object categories and other image generating factors (pose in our case) simultaneously. It is straightforward to take illuminations into account, but for simplicity and availability of labels, we consider object identity and its pose here. Contrary to the usual way to learn representations invariant to pose changes as much as possible, we take the opposite by retaining the pose information and learning it jointly with object identities during the training process.

We leverage the power of ConvNets for high-performance representation learning, and build our MTL framework based on it. Concretely, our architecture is a two-streams ConvNet by taking a stereo-pair of images as inputs and predicting the object category inside it and pose transformations between two images. Both streams share the same CNN architecture (e.g. AlexNet) with the same weights and the same operations on each layer. Each stream independently extracts features from one image. In the top layer, we explicitly partition the representation units into two groups, with one group representing object identity and the other group representing its pose. Object identity representations are passed down to predict object categories, while two pose representations are concatenated to predict the pose transformation between images. Our architecture is shown in Fig. [\ref=fig:Architecture]. By explicitly partitioning the top CNN layer units into groups, we learn the ConvNet in a way such that each group extracts features useful for its own task and explains one factor of variation in the image. We refer our architecture as disentangling CNN (disCNN), with disentangled representations for identity and pose.

During training, disCNN takes a stereo-pair of images as inputs, and learns features by using both object categories and pose-transformations as supervision. The goal of disCNN is to recognize objects, therefore, in test, we take only one stream of the trained disCNN, use it to compute features for the test image, and only the identity representations in top layer are used and fed into the object category layer for categorization. In other words, pose representations are not used in test, and the pose-transformation prediction task in the training is auxiliary to the object recognition task, but essential for better feature learning.

To show our disentangling architecture learns better features than its building base (e.g. AlexNet) for object categorization, we tested both AlexNet and disCNN on the large scale iLab-20M dataset, which contains ~20M images with detailed camera viewpoints and lighting information for each image, and results show that disCNN outperforms AlexNet by 4%. To our best knowledge, iLab-20M is the first large-scale dataset with detailed pose and lighting information, which matches the capacity of the current deep architectures (e.g. AlexNet). Therefore, we could evaluate the utilities of high-capacity disCNN and AlexNet on this dataset, without resorting to pretain these architectures on other large scale datasets first (e.g. ImageNet).

To show features learned by pretraining disCNN and AlexNet on iLab-20M are useful for object recognition on other datasets, we trained AlexNet on two other datasets, Washington RGB-D [\cite=lai2011large] and ImageNet [\cite=deng2009imagenet], from random initializations and from weights of the pretrained models. Results show that given the availability of limited class label resources, fine-tuning AlexNet from the learned weights performs significantly better than training it from scratch, and moreover, fine-tuning from pretrained disCNN outperforms fine-tuning from pretrained AlexNet. This on the one hand shows the generality of the pretrained features, and on the other hand, shows the advantage of our disentangling architecture over the conventional single task ConvNets for learning features for object recognition.

Related work

In this section, we discuss prior work related to our work. Our central idea is to use ConvNets to build a multi-task learning framework by enforcing the disentangled representations for different image generating factors. We review ConvNets, MTL and disentangling respectively.

ConvNets: over the past several years, convolutional neural networks (CNNs) [\cite=lecun1998gradient] have dramatically push forward the state-of-the-art in many vision tasks, including image classification [\cite=krizhevsky2012imagenet] [\cite=simonyan2014very] [\cite=szegedy2015going] [\cite=he2015deep], object detection [\cite=sermanet2013overfeat] [\cite=girshick2014rich], image segmentation [\cite=chen2014semantic] [\cite=long2015fully], activity recognition [\cite=simonyan2014two] [\cite=gkioxari2015contextual] etc. These tasks leverage the power of CNNs to learn rich features useful for the target tasks, and [\cite=agrawal2015learning] show features learned by CNNs on one task can be generalized to other tasks. In our paper, we aim to learn feature representations for different image generating factors, and we employ ConvNets as our building base as well.

Multitask learning: several efforts have explored multi-task learning using deep neural networks, for face detection, phoneme recognition, and scene classification [\cite=seltzer2013multi] [\cite=zhang2014facial] [\cite=zhang2014improving] [\cite=huang2013multi]. All of them use a similar linear feed-forward architecture, with all task label layers directly appended onto the top representation layer. In the end, all tasks in these applications share the same representations. More recently, Su et al [\cite=su2015render] use CNN to estimate the camera viewpoint of the input image. They pose their problem as MTL by assuming that viewpoint estimate is object-class-dependent, and stack class-specific viewpoint layers onto the top of CNN. Our work differs from the above in that: we use two-stream CNNs and we explicitly partition the top layer representation into groups with each group representing one task, therefore we have task-exclusive representations while in above works, all tasks share the same top layer representations.

Disentangling: our work falls within the line of research related to disentangle latent factors that generate natural images. As argued by Bengio [\cite=bengio2009learning], one of the key challenge to understand image is to disentangle different factors, e.g. shape, texture, pose and illumination, that generate natural images. In computer graphics, we render 2D images from 3D shapes by independently controlling lighting, camera viewpoints and textures, and in computer vision, there is a line of research devoted to inverting the image generating process and recover these factors given 2D images as inputs.

Reed et al [\cite=reed2014learning] proposed disentangling Boltzmann Machine (disBMs), which augments the regular RBM by partitioning the hidden units into distinct factors of variation and modeling their high-order interactions. They showed that disBMs disentangle face identity and pose/expression better than previous methods. In [\cite=zhu2014multi], the authors build a stochastic neural network, multi-view perceptron, to factorize the face identity and its view representations with different sets of neurons, such that view-invariant face recognition is achieved. Our work is similar to the above two in spirit that we explicitly partitions the representations into distinct groups to force different factors disentangled, however, our model is deterministic and scales to large datasets, while the above methods are restricted to small datasets and often require expensive sampling inferences (e.g. disBMs).

Dosovitskiy et al [\cite=dosovitskiy2015learning] proposed to use CNN to generate images of objects given object style, viewpoint and color. Their model essentially learns to simulate the graphics rendering process, but does not directly apply to image interpretation - infer the latent factors given the images. Kulkarni et al [\cite=kulkarni2015deep] presented the Inverse Graphics Network (IGN), which is an encoder-decoder architecture and learns to generate new images of the same object under varying poses and lighting. The encoder of IGN learns a disentangled representation of various transformations including pose, light and shape. Yang et al [\cite=yang2015weakly] proposed a recurrent convolutional encoder-decoder network to render 3D views from a single image. In their architecture, they explicitly split the top layer representations of the encoder into identity and pose units and achieve robust pose-invariant identity recognition. Our work is similar to [\cite=kulkarni2015deep] [\cite=yang2015weakly] by using distinct, non-overlapping units to represent different factors, but we differ in that: (1) our architecture is a MTL CNN which maps images to discrete labels, while theirs are autoencoders mapping images to images; (2) our model directly applies to large number of categories with complex scenes, but [\cite=kulkarni2015deep] [\cite=yang2015weakly] only tested their models on face and chair datasets with pure backgrounds.

Our work is most similar to [\cite=agrawal2015learning], which shows that freely available egomotion data of mobile agents provides as good supervision as the expensive class-labels for CNNs to learn useful features for different vision tasks. Here, we use stereo-pairs of images as inputs to learn the camera motions as well, however, we are different in: (1) our architecture is a MTL framework, in which the task of camera-motion (pose-transformation) prediction serves as an auxiliary task to help object recognition; (2) our network is more flexible, which could take in both one image or a stereo-pair; (3) our MTL disCNN learns much better features for object-recognition than AlexNet using class-label as supervision, while their single task two-streams CNNs only learn comparable features.

The rest of the paper are organized as follows: in Sec. [\ref=sec:Method], we present our method, in Sec. [\ref=sec:Experiments], we do comprehensive experimental evaluations on 3 datasets, and conclusions are drawn in Sec. [\ref=sec:Conclusions].

Method

Natural images are generated under many factors of variation, such as shape, pose and illumination. Contemporary CNNs [\cite=krizhevsky2012imagenet] [\cite=simonyan2014very] [\cite=szegedy2015going] [\cite=he2015deep] pose object recognition from images as a single task learning problem, leverage the large scale dataset ImageNet containing objects under diverse poses and illuminations as supervision and learn a representation which is sensitive to object categories but invariant to poses and illuminations as much as possible. Top layer representations learned in the above way contain minimal information about poses and illuminations. Inspired by arguments in [\cite=hinton2011transforming] [\cite=bengio2009learning] [\cite=reed2014learning], we take the opposite way: we formulate object recognition as a MTL problem, and learn the object identity jointly with other factors (pose, illumination, ...). Our MTL architecture retains information about other image generating factors, instead of discarding them. We assume two factors in our experiments, which are object identity and pose, but it is straightforward to take additional factors (e.g. lighting) into account and build them into our architecture if labels are available.

Object identity is defined to be the identity of one instance. Distinct instances, no matter they belong to the same class or not, have different object identities. Object pose refers to the extrinsic parameters of the camera taking the image, but given a single natural image taken by a consumer camera, it is hard to obtain the camera extrinsics, therefore the ground-truth object poses are expensive and even impossible to collect in real cases. Camera extrinsics are known when we render 2D images from 3D models[\cite=su2015render], but rendered 2D images are very different from natural images. Although single camera extrinsics are hard to get in real cases, the relative translation and orientation (a.k.a camera motion), represented by an essential matrix E, between a stereo-camera-pair is relatively easier to compute, e.g., for calibrated cameras, first find 8 pairs of matched points, then use the normalized 8-point algorithm [\cite=hartley2003multiple] to estimate. The camera motion between a stereo-camera-pair captures the pose transformation of the object in two images. In our work, we would use the relative pose transformation between object pairs, instead of the absolute object pose, as supervision.

Our work is similar to [\cite=agrawal2015learning] in that we learn to estimate motions between stereo-pairs as well, however, we have key differences: in [\cite=agrawal2015learning], there is no notion of objects, and two consecutive egomotion frames are any image-pairs, which may contain no objects, or have no objects in common. Their architecture is used to pretrain CNNs using egomotion as supervision, but is not directly designed for object recognition. However, in our case, the input image-pair is guaranteed to contain objects, and objects are the same object instance (the same object identity) with different poses, and our emphasis is to estimate object categories jointly with the pose transformations, such that better object recognition performances can be achieved. In the following texts, we use "pose transformation" and "camera motion" exchangeably.

Our system is designed to estimate any numeric pose transformations, but in experiments, we always have a limited number of camera-pairs, with motion between each pair fixed. Therefore, we could further discretize the pose transformation using the fact that every image-pair taken under the same camera-pair has the same pose transformation, and the number of the camera-pairs determines the number of discrete pose-transformations. In this way, "pose transformation" estimation is transformed into a classification problem - classifying which camera-pair takes the image-pair, with the number of labels equal to the number of camera-pairs and one label referring to one camera-pair.

Network Architecture

Our ultimate goal is to learn object identity representations for object recognition, but we further simultaneously learn the object pose transformation as an auxiliary task. Building a ConvNet that can predict the pose transformation between a stereo-pair of images is straightforward: the ConvNet should take the pair as input, after several layers of convolutions, it produces an output which assigns a probability to each camera-pair under which that image-pair could be taken. But note that the image-pair contains the same object instance taken under different camera viewpoints, we wish to learn an object identity representation, such that the same pair should have as similar object identity representations as possible.

We build a two-stream CNN architecture shown in Fig. [\ref=fig:Architecture], named it as disentangling CNN (disCNN). Each stream is a ConvNet independently extracting features from one image, and two ConvNets have the same architecture and share the same weight. Here we use AlexNet [\cite=krizhevsky2012imagenet] as the ConvNet, but we can use VGG [\cite=simonyan2014very] and GoogLeNet [\cite=szegedy2015going] as well. After getting fc7 representations, we explicitly partition the fc7 units into two groups, with one group representing object identity and the other representing object pose in a single image. Since object instances in a stereo-pair are the same, we enforce two identity representations to be similar by penalizing their [formula]-norm differences, i.e. [formula], where id1 and id2 are identity representations of two stereo images. One identity representation (either id1 or id2) is further fed into object-category label layer for object-category prediction. Two pose representations, pose1and pose2, are fused to predict the relative pose transformation, i.e., under which camera-pair the stereo-image-pair is taken. The objective function of our architecture is therefore the summation of two soft-max losses and one [formula] loss, i.e.,

[formula]

We follow AlexNet closely, which takes a [formula] image as input, and has 5 convolutional layers and 2 fully connected layers. ReLU non-linearities are used after every convolutional/fully-connected layer, and dropout is used in both fully connected layers, with dropout rate 0.5. The only change we make is to change the number of units on both fc6 and fc7 from 4096 to 1024, and one half of the units (512) are used to represent identity and the other half to represent pose. If we use abbreviations Cn, Fn, P, D, LRN, ReLU to represent a convolutional layer with n filters, a fully connected layer with n filters, a pooling layer, a dropout layer, a local response normalization layer and a ReLU layer, then the AlexNet-type architecture used in our experiments is: C96-P-LRN-C256-P-LRN-C384-C384-C256-P-F1024-D-F1024-D (we omit ReLU to avoid cluttering). If not explicitly mentioned, this is the default architecture for all experiments.

Notes: (1) the proposed two-stream CNN architecture is quite flexible in that: it could either take a single image or a stereo image-pair as inputs, for a single image input, no pose transformation label is necessary, while for a stereo image-pair input, it is not required to have a object-category label. For a pair of images without the object label, its loss reduces to two terms: [formula], the soft-max loss of the predicted pose-transformation and the [formula] loss of two identity representations. Given a single image with a object label, the loss incurred by it reduces to only one term: the soft-max loss of the predicted category label L(object).

(2) Scaling the same image-pair by different scales does not change its pose transformation label. In our case, each camera-pair has a unique essential matrix (up to some scale), and defines one pose-transformation label. By up/down scaling a image-pair simultaneously, the estimated essential matrix differs only by a scale factor from the essential matrix estimated from the raw image-pair. Since the essential matrix estimated from the raw image-pair is even uncertain up to a scale factor (e.g. using the eight-point method for estimation [\cite=hartley2003multiple]), the essential matrix estimated from the scaled pairs is equivalent to that estimated from the raw pair. Therefore, a scaled pair has the same pose-transformation label as the raw pair. So, we can scale different image-pairs under the same camera-pair differently, and they still share the same pose-transformation label. This is useful when objects have large scale differences, then, we could scale them differently to make them have similar scales (see experiments on Washington RGB-D dataset).

Experiments

In experiments, we first show the effectiveness of disCNN for object recognition against AlexNet on both iLab-20M and Washington RGB-D datasets. We further demonstrate that the pretrained disCNN on the iLab-20M dataset learns useful features for object recognition on the ImageNet dataset [\cite=deng2009imagenet]: a AlexNet initialized with disCNN weights performs significantly better than a AlexNet initialized with random Gaussian weights.

iLab-20M dataset

iLab-20M dataset [\cite=Borji_2016_CVPR] is a controlled, parametric dataset collected by shooting images of toy vehicles placed on the turntable using 11 cameras at different viewingpoints. There are totally 15 object categories with each object having 25~  160 instances. Each object instance was shot on more than 14 backgrounds (printed satellite images), in a relevant context (e.g., cars on roads, trains on railtracks, boats on water). In total, 1,320 images were captured for each instance and background combinations: 11 azimuth angles (from the 11 cameras), 8 turntable rotation angles, 5 lighting conditions, and 3 focus values (-3, 0, and +3 from the default focus value of each camera). The complete dataset consists of 704 object instances, with 1,320 images per object-instance/background combination, or almost 22M images.

Training and test instances: we use 10 (out of 15) object categories in our experiments (see Fig. [\ref=fig:Learned-filters-and-CM-iLab-20M]), and within each category, we randomly choose 3/4 instances as training and the remaining 1/4 instances for testing. Under this partition, instances in test are never seen during training.

Image-pairs: we only take images shot under one fixed lighting condition (with all 4 lights on) and camera focus (focus = 0), but all 11 camera azimuths and all 8 turntable rotations as training and test images. The setting that 11 cameras mounted on a static semi-circular arch shoot images under 8 turntable rotations is equivalent to the setting that 88 cameras mounted on a semi-sphere shoot images of objects on a static turntable. One camera under 8 rotations in the former case maps to 8 evenly distributed cameras on the same latitude in the latter case. In principle, we can take image-pairs taken under any camera-pairs (e.g. any pair from C288 combinations), however, one critical problem is that image-pairs taken under camera-pairs with large viewpoint differences have little overlap, which makes it difficult, or even impossible to predict the pose-transformation (e.g., difficult to estimate the essential matrix). Therefore, in experiments, we only consider image-pairs taken by neighboring camera-pairs. All image-pairs shot under a fixed camera-pair share the same pose-transformation label, and finally the total number of pose-transformation labels is equal to the number of camera-pairs. In experiments, we consider different numbers of camera-pairs, and evaluate the influence on the performance of disCNN.

Fig. [\ref=fig:Exemplar-iLab-20M-images-camera-pairs] shows images of one instance shot under different cameras and rotations: each row is shot by the same camera under different turntable rotations, and each column is shot by different cameras under the same turntable rotation. In experiments, we use different numbers of camera-pairs as supervision, therefore, only take image-pairs shot under the chosen camera-pairs as training. Case one (Fig. [\ref=fig:Exemplar-iLab-20M-images-camera-pairs] (a) topleft): we take two neighboring cameras as one stereo-pair (we skip 1 camera, i.e., Ci - Ci + 2 is a camera-pair), resulting in 7 camera-pairs, therefore 7 pose-transformation labels. Image pairs taken by the same camera-pair under different rotations share the same pose-transformation label. Case two (Fig. [\ref=fig:Exemplar-iLab-20M-images-camera-pairs] (b) topright): two images taken by one camera under two adjacent rotations ((CiRj,  CiRj + 1)) can be imagined to be taken by a pair of virtual cameras, resulting in 11 camera-pairs with 1 pair referring to one camera under two adjacent rotations. Case three (Fig. [\ref=fig:Exemplar-iLab-20M-images-camera-pairs] (c) bottomleft): we combine 7 camera-pairs in case one and 11 camera-pairs in case two, and a total of 18 camera pairs. Case four (Fig. [\ref=fig:Exemplar-iLab-20M-images-camera-pairs] (d) bottomright): in addition to take image-pairs taken under neighboring cameras (the same rotation) and neighboring rotations (the same camera), we further take diagonal image-pairs taken under neighboring-cameras and neighboring-rotations (i.e., (CiRj,  Ci + 1Rj + 1) and (CiRj + 1,  Ci + 1Rj)). At last we have 56 camera-pairs. By taking image-pairs from the chosen camera-pairs, we end up 0.42M, 0.57M, 0.99M and 3M training image-pairs in 4 cases respectively. After training, we take the trained AlexNet-type architecture out and use it to predict the object category of a test image. We have a total of 0.22M test images by split.

Implementation details: One thing we have to mention first: what is the training data of AlexNet? Since we have prepared training pairs for disCNN, we use the left images of training pairs as the training data for AlexNet. Therefore AlexNet and disCNN have the same number of training samples, with one image in AlexNet corresponding to a image pair in disCNN (Note: duplicate training images exist in AlexNet). To do a fair comparison, we train both AlexNet and disCNN using SGD under the same learning rate, the same number of training epochs and the same training order within each epoch. We set λ1 = 1 and λ2 = 0.1 in the objective function [\ref=eq:loss-objective] of disCNN. Practically, λ1 and λ2 are set such that the derivatives of three separate loss terms to the parameters are at a similar scale. Both AlexNet and disCNN are trained for 20 epochs under 4 cases. The initial (final) learning rate is set to be 0.01 (0.0001), which is reduced log linearly after each epoch. The ConvNets are trained on one Tesla K40 GPU using the toolkit [\cite=vedaldi2015matconvnet].

Results: the object recognition performances are shown in Table [\ref=tab:Object-recognition-iLab-20M]. We have the following observations: (1) disCNN consistently outperforms AlexNet under different numbers of camera pairs, with the performance gain up to [formula]; (2) when we have more camera-pairs, the performance gap between disCNN and AlexNet enlarges, e.g., [formula] gain under 11,18,56 camera pairs compared with [formula] gain under 7 camera pairs. One potential reason is that when more camera pairs are used, more views of the same instance are available for training, therefore, a higher recognition accuracy is expected. But as observed, the performances of disCNN flatten when more camera pairs are used, e.g. the same performance under 18 and 56 camera pairs. One possible interpretation is: although we have 56 camera pairs, the diagonal camera-pairs in the case of 56 pairs do provide new pose transformation information, since the motion between a diagonal pair could be induced from motions of two camera pairs in the case of 18 pairs, a horizontal camera pair and a vertical camera pair.

Qualitative visualizations: Fig. [\ref=fig:Learned-filters-and-CM-iLab-20M] (a,b) shows the learned conv1 filters of AlexNet and disCNN in case 3, and (c,d) show their corresponding between-class confusion matrices. As seen, disCNN learns more edge-shaped filters, and disCNN improves the recognition accuracies for 8 class, draws on one and loses one. Fig. [\ref=fig:disentangling] shows k nearest neighbors of the query image, based on the [formula] distances between their fc7-identity (disCNN, 512D) and fc7 (AlexNet, 1024D) representations. We can see clearly that disCNN successfully retrieves images of the same instance under different poses as the nearest neighbors (Fig. [\ref=fig:disentangling] (a)). Although in some cases (Fig. [\ref=fig:disentangling] (b)), AlexNet find different images of the same instance as the nearest neighbors, these retrieved neighbors clearly share similar poses as the query image. These qualitative results show that disCNN disentangles the representations of identity from pose, to some extent.

Washington RGB-D dataset

The RGB-D dataset [\cite=lai2011large] is a dataset of 300 common household objects organized into 51 categories. This dataset was recorded using a Kinect style 3D camera that records synchronized and aligned 640x480 RGB and depth images at 30 Hz. Each object was placed on a turntable and video sequences were captured for one whole rotation. For each object, there are 3 video sequences, each recorded with the camera mounted at a different height so that the object is viewed from different angles with the horizon. The dataset has a total of 250K images from different views and rotations. Two adjacent frames have small motions, therefore visually very similar, and in experiments, we pick one frame from each 5 consecutive frames, resulting in ~  50K image frames. Since the scale of the datasets does not match the scale of ConvNets, we adopt the "pretrain-finetuning" paradigm to do object recognition in this dataset, using the pretrained ConvNets weights on the iLab-20M dataset as initializations.

Training and test sets: [\cite=lai2011large] provided 10 partition lists of training/test. They use leave-one-out to partition: randomly choose 1 instance within a category as test, and use the remaining instances as training. Due to the training time limitation, we evaluate performances using the first 3 partitions and report the mean accuracies. We use the provided object masks to crop the objects from the raw frames and resize them to the size 227×  227. Since objects are located at the image center, by first cropping and then rescaling an image-pair does not change the pose-transformation of the raw pair.

Camera pairs: similarly we take different numbers of camera-pairs and evaluate its influences on the performances. In one video sequence, every frame-pair with a fixed temporal gap could be imagined to be taken under a virtual camera-pair, thus all such pairs share the same pose-transformation label. As an example, two pairs, Fi - Fi + Δ and Fj - Fj + Δ, whose temporal gap between frames are both Δ, then they have the same pose-transformation label. One Δ defines one camera-pair, and in experiments, we let Δ  =  {5,10,15,20}. Case one: we take image-pairs with Δ  =  {5} from each video sequence, and all these pairs could be thought as taken by one virtual camera pair, therefore have the same pose-transformation label. Since we have 3 video sequences, finally all pairs have in total 3 pose-transformation labels, thus equivalently 3 virtual camera pairs; Case two: take image-pairs with Δ  =  {5,10}, end in 6 camera pairs; Case three: Δ  =  {5,10,15}, end in 9 camera-pairs; case four: Δ  =  {5,10,15,20}, end in 12 camera-pairs. The total number of training image pairs under each case is 67K, 99K and 131K respectively. The number of test images in all cases is 6773.

Implementation details: we use the same training settings as in iLab-20M experiments to train AlexNet and disCNN, i.e., the same learning rates (start from 0.01, end with 0.0001, with rate decreasing log linearly), the same number of training epochs (15), and the same training order within each epoch. We set λ1 = 1 and λ2 = 0.05 in experiments.

Results: we do two comparisons: first compare disCNN (AlexNet) trained from scratch against from the pretrained weights on the iLab-20M dataset, then compare disCNN against AlexNet, both fine-tuned from the pretrained CNN features on iLab-20M. Results are shown in Table [\ref=tab:Object-recognition-rgbd], our observations are: (1) disCNN (AlexNet) trained by fine-tuning the pretrained AlexNet features on the iLab-20M wins disCNN (AlexNet) trained from scratch by [formula]([formula]), and their fine-tuned performances are better than the published accuracies, 74.7%, in [\cite=lai2011large] by a large margin. This shows the features learned from the iLab-20M dataset generalize well to the RGB-D dataset; (2) disCNN outperforms AlexNet in both cases, either trained from scratch or from the pretrained AlexNet features, which shows the superiority of the disentangling architecture over the linear chain, single task CNNs; (3) similarly, we observe that the performance of disCNN increases as the number of camera pairs increase. We further compute [formula] distances between categories using fc7-identity (disCNN, 512D) and fc7 (AlexNet, 1024D) representations, and plot them in Fig. [\ref=fig:Learned-filters-and-l2-rgbd]. Visually the off diagonal elements in disCNN are brighter and the diagonal elements are darker, showing smaller within-category distances and larger between-category distances.

ImageNet

ImageNet has millions of labeled images, and training a ConvNet on a large dataset from pretrained models against from scratch has insignificant effects [\cite=hinton2012deep] [\cite=lecun2015deep]. In order to show the pretrained disCNN on the iLab-20M datasets learns useful features for object recognition, we fine-tune the learned weights on ImageNet when only a small amount of labeled images are available. We fine-tune AlexNet using 5, 10, 20, 40 images per class (5K,10K,20K and 40K training images in total) from the ILSVRC-2010 challenge. The AlexNet is fine-tuned under three scenarios: (1) from scratch (random Gaussian initialization), (2) from pretrained AlexNet on iLab-20M, (3) from pretrained disCNN on iLab-20M, and top-5 object recognition accuracies are presented in Table [\ref=tab:Top-5-object-recognition-ImageNet]. When we pretrain AlexNet and disCNN on the iLab-20M dataset, we use the AlexNet with the units on the last two fully connected layers reset to 4096.

Results: (1) when only a limited number of labeled images are available, fine-tuning AlexNet from the pretrained features on the iLab-20M dataset performs much better than training AlexNet from scratch, e.g., the relative improvement is as large as [formula] when we have only 5 samples per class, and the improvement decreases when more labeled images are available, but we still gain [formula] improvements when 40 labeled images per class are available. This clearly shows features learned on the iLab-20M dataset generalizes to ImageNet. (2) fine-tuning from the pretrained disCNN on iLab-20M performs even better than from the pretrained AlexNet on iLab-20M, and this shows that disCNN learns even more effective features for general object recognition than AlexNet. These empirical results show the advantage of our disentangling architecture to the traditional single task linear architecture.

Conclusions

In this paper, we design a multi-task learning ConvNet to learn to predict object categories. Unlike traditional ConvNets for object recognition, which is usually a single task architecture and learns features sensitive to the current task (i.e., object category) but invariant to other factors of variation as much as possible (e.g., pose), disCNN retains all image generating factors of variation (object category and pose transformation in our case), and learn them simultaneously by explicitly disentangling representations of different factors. Experiments on the large scale iLab-20M dataset show that features learned by disCNN outperforms features learned by AlexNet significantly for object recognition. If we fine tune object recognition on the ImageNet dataset using pretrained disCNN and AlexNet features, disCNN-pretrained features are consistently better than AlexNet-pretrained features. All experiments show the effectiveness of our disentangled training architecture.

As shown in [\cite=agrawal2015learning], features learned using egomotion as supervision are useful for other vision tasks, including object recognition, and the egomotion-pretrained features compare favorably with features learned using class-label as supervision. In our paper, we further showed that when our model has access to both object categories and camera motions, it learns even better features than using only class-label as supervision. One possible explanation is: although egomotion learns useful features for object recognition, it does not necessarily guarantee that feature representations of different instances of the same class are similar since egomotion does not has access to any class-label information. In our work, we showed, by feeding ConvNets with additional class labels, the feature learning process are further guided toward the direction that objects of the same class tend to have spatially similar representations.