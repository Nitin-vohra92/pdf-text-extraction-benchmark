Sequential testing over multiple stages and performance analysis of data fusion

Introduction

The JIEDDO Analytic Decision Engine (JADE) is a flexible software toolkit for studying the performance of sensor configurations for the detection of person-borne explosive compounds and other threat substances. JADE is designed to enable performance and tradeoff analyses between different, user-specified scenarios with given sensor placements and data fusion networks. JADE contains fundamental physics-based models of several sensor technologies of interest, such as nonlinear acoustic and radar-based detectors, along with a data fusion system that we focus on in this paper. The fusion system consists of a static component that combines the decisions of individual sensors at a fixed point in time, and a dynamic, time-dependent component that in turn fuses the outputs of the static structure at different times. The static component is based on a probabilistic graphical model, or Bayesian network, and accepts probability matrices from the physics-based sensor models as inputs (the details of which are abstracted from the fusion system). Its outputs are fed into the dynamic fusion framework, which is based on sequential hypothesis testing and produces performance metrics for the entire, fused sensor configuration. The purpose of the system is to determine the performance of a given fusion structure, as opposed to doing fusion on actual measurements.

We first discuss the static framework in Section [\ref=SecStatic], which allows elementary fusion structures to be stacked and analyzed efficiently. This material is fairly standard but serves as a background for the rest of the paper. We then describe an extension of the Wald sequential test in Section [\ref=SecDynamic] that involves multiple, distinct stages, where the evidence accumulated from each stage is carried over into the next one. We show how the performance characteristics and decision times of such a test can be computed efficiently for time-dependent statistics and illustrate this approach on examples in Section [\ref=SecExamples]. This setup models a bank of anomaly sensors that observe a moving target over time, reach an initial fused decision, and if justified, activate additional sensors that continue to collect static fused evidence over time until a final decision is made about the target. The multiple-stage configuration allows sensors that have a high cost of operation to remain inactive unless specifically called upon.

Static fusion using Bayesian networks

The static fusion structure is formulated as a Bayesian network, i.e. a directed acyclic graph with each vertex representing a random variable and edges describing dependencies between the variables. A Bayesian network has the defining property that every vertex is conditionally independent of its ancestor vertices given its immediate parent vertices [\cite=Pe97]. Such networks are an intuitive framework for performing probabilistic inference among interconnected events in many different contexts, and are well suited for formulating a sensor fusion system. The vertices in our network represent the object, sensors and fusion centers.

Suppose we have N sensors to be fused, each of which outputs hard decisions between M possibilities (with the first one corresponding to the case where no threat is present). Let H be the true object (or the hypothesis in a Bayesian setting) and S be the local decision of a given sensor. The performance of the sensor is described by the M  ×  M matrix {P(S = m|H = m')}1  ≤  m,m'  ≤  M, which we write concisely as P(S =   ·  |H). In this paper, we will generally focus on M = 2, corresponding to binary decision-level fusion, but the discussion in this section applies to other M as well. At any fusion center F with V parent vertices {Sn}1  ≤  n  ≤  V, we can describe the fusion rule by the V-dimensional tensor P(F =   ·  |{Sn}1  ≤  n  ≤  V), which for deterministic fusion rules consists only of 0 and 1 elements. The performance of the entire system is given by P(D =   ·  |H), where H is the root vertex in the graph and the system's final decision D is the last child vertex. This formulation enables the graph to take on essentially any desired form and allows different combinations of fusion centers and sensors to be stacked together, subject to the following rules that ensure that the fusion structure is meaningful.

Each sensor vertex must have the object and at most one fusion center as its parent.

At least one sensor must have only the object as its parent.

Each fusion center can have any combination of sensors and/or fusion centers as parents, as long as no cycles are formed in the graph.

No fusion center can have the object as a parent.

There must be exactly one fusion center with no children, representing the final decision.

These rules ensure that all sensors in the graph observe the object and that all intermediate decisions are ultimately combined at a single, final fusion center. An example fusion network of this type is shown in Figure [\ref=FusionGraph].

To determine P(D =   ·  |H), we choose small subgraphs of the Bayes network at a time, each containing one fusion center and all its parent vertices, and marginalize over them using the standard method of belief propagation [\cite=Pe85] [\cite=Pe97]. This is done iteratively for each fusion center in parent-to-child order until all the fusion centers have been covered. For certain fusion rules, the probability matrix at any child fusion centers may depend on the outputs of any parent fusion centers, so this iterative procedure is much more simple and efficient than having the child fusion centers' conditional probabilities account for this dependence and computing marginal probabilities over the entire Bayes network at once.

At each fusion center, JADE allows the user to choose between five elementary hard-decision fusion rules: the "and," "or," majority, Neyman-Pearson optimal and Bayes optimal rules [\cite=Va96]. Any of the five fusion rules can be used in the decision-level case of M = 2, while for M > 2, only the Bayes and majority rules are meaningful. At any given fusion center, let {Sn}1  ≤  n  ≤  V be the local decisions of V sensors feeding into it, with 0  ≤  Sn  ≤  M - 1, and let F be the fused decision. The "and" rule simply chooses F = 1 if all the Sn = 1, and F = 0 otherwise. Similarly, the "or" rule chooses F = 0 if all the Sn = 0, and F = 1 otherwise. It is clear that the "and" rule minimizes PF while keeping PD > 0 and the "or" rule maximizes PD while keeping PF < 1, so they can be thought of respectively as the least and most sensitive fusion rules available. The majority rule takes a majority vote between the sensors, i.e. [formula], with a random, uniformly distributed decision taken if there is a tie between multiple choices. This is the only rule where the fused decision is potentially random. These three rules generally do not satisfy any good optimality criteria, but are conceptually simple and useful as a baseline for comparison against the two optimal rules.

At a given fusion vertex in the network, the Neyman-Pearson rule (for M = 2) has the user specify a target false alarm probability PF', and the system chooses the (deterministic) fusion rule that maximizes the local PD at that vertex, subject to the constraint PF  ≤  PF'. The optimal rule is found by computing the likelihood ratios [formula] for every combination of individual sensor decisions {Sn} and arranging them in increasing order. The combinations are then partitioned into two subsets I and J such that for {Sn}∈I, [formula] and for {Sn}∈J, [formula]. The solution is given by the rule that chooses F = 0 for {Sn}∈I and F = 1 for {Sn}∈J.

For the Bayes fusion rule [\cite=Ba95] [\cite=Mi07], the user specifies the costs of a false alarm CF, a missed detection CM and (for M > 2) a mix-up between two threat possibilities CX. For each combination of individual sensor decisions {Sn}, the system finds a fused decision F that minimizes the Bayes risk, or the expected cost of a wrong decision,

[formula]

where Cj,j = 0, Cj,1 = CF and C1,j = CM for [formula], and Cj,k = CX for all other (j,k). The optimal fusion rule can be found by simply looking at every combination {Sn} individually and taking the best of the M possible fused decisions for each one. In general, finding this fusion rule is a computationally difficult discrete optimization problem, but this simple, brute-force approach is fast as long as V and M are fairly small (e.g. less than 10), as is the case in our scenarios of practical interest.

Dynamic fusion using multiple-stage sequential testing

Suppose now that we have two static fusion networks of sensors, each represented by a Bayesian network of the type described in Section [\ref=SecIntro]. The sensors collect measurements from a target moving along a specified path, with the static network producing a fused decision at every point in time based on the sensors' individual probabilities at each position. These fused decisions can be combined over time using Wald's theory of sequential probability testing. The classical Wald sequential test is essentially a one-dimensional random walk where the total likelihood ratio of the system makes "steps" in either direction, corresponding to different incoming binary decisions. The system reaches a fused decision when a specified upper or lower threshold has been crossed. We refer to [\cite=Po94] or [\cite=Ve99] for more details.

We develop an extension of the classical sequential test to cover the following scenario. Only the sensors in the first static network are initially active, and the sequential test accepts and combines the fused outputs from that network at each point in time. If the system detects an anomaly, it switches over to the second static network and continues to pick up and combine measurements in the same manner until a final decision has been reached. The motivation for this two-stage setup is that there is typically a cost to activating and operating the sensors in the second-stage network, so they are to be switched on only if the first-stage sensors decide that there is a good chance of a threat. The two networks do not need to be disjoint and can contain some of the same sensors, although possibly with different graph linkages or fusion rules. In practical scenarios, the second-stage graph is a superset of the first-stage one that includes additional sensors, reflecting the fact that the first-stage sensors continue to collect observations after they trigger any additional sensors in the second-stage network. It is also straightforward to add additional stages in the same manner. For example, a third stage might correspond to an object being acquired by video before sensors begin to collect measurements on it (known as "track before detect"). For clarity, however, we focus on two stages in what follows.

Let H be the object as before, and Dn and Dn' respectively be the decision outputs of the first and second stage fusion networks (at their respective final fusion centers) at time n. Assume that the [formula] are mutually independent. We restrict M = 2 for the rest of the paper, so the static fusion system gives us the sequences of [formula] matrices P(Dn  =    ·  |H) and P'(Dn' =   ·  |H) for all times 1  ≤  n  ≤  N. We use these inputs to set up the following type of sequential test. We write [formula], [formula], K, η0 and η1 for respectively the lower stopping time, upper stopping time, stopping time, lower threshold and upper threshold for the first stage of the test. The thresholds η0 and η1 are fixed parameters that control the overall sensitivity of the test, while the stopping times are random variables that we will specify below. Similarly, we write ', [formula], K', η0' and η1' for the corresponding variables for the second stage of the test, where we require that η0'  ≤  η0 and η1'  ≥  η1. Let [formula] denote the set of decisions of the system's active static fusion network at each time n, up to time k. For any sequence of decision possibilities [formula], we define the likelihood ratio recursively by

[formula]

where [formula]. This allows us to define the stopping times by

[formula]

If any of the above sets is empty, we define the corresponding stopping time to be N + 1. The test starts with the first stage static network P(Dn  =    ·  |H) and runs like a conventional sequential test until either k = K at time k (i.e. when [formula] moves outside the region (η0,η1)), at which point it switches to the second stage network, or until k = N + 1, when it is forced to stop and make a decision by comparing [formula] to the geometric midpoint [formula]. If the second stage is triggered, the test continues running with the second stage network P'(Dn' =   ·  |H) and stops with a final decision when K' = k or K' = N + 1. The first stage's result (representing an initial decision) affects whether the second stage starts below η0 or above η1. We want to find the statistics of K and K' and the detection and false alarm probabilities of the first stage at each time k, denoted PkD and PkF, and of the entire test, PkD  ' and PkF  '.

We describe a simple, deterministic algorithm to compute these quantities for incoming, fused sensor measurements from the two static networks. Let Gk be the event {K  ≥  k}, i.e. that the first stage was still running at time k - 1. We can expand [formula] for all 1  ≤  k  ≤  N by writing

[formula]

where

[formula]

We can express P( = k|H) and the k = N + 1 cases in a similar manner. The number of terms in Ak(η0,η1) generally grows exponentially, but the sum can be computed iteratively by keeping track of likelihood sets Lk over time, where each Lk consists of elements [formula]. For k = 1, we set L1: = {P(D1 = d1|H)}. For each k > 1, we find the likelihoods of all possible sample paths, [formula], where ×   denotes the outer product between vectors in [formula]. We then compute

[formula]

store the likelihoods of paths that escaped [formula], keep the remaining likelihoods [formula] for the next step, and increment k. Note that at each time k, we only need to keep Lk and Lk - 1 in memory. At k = N + 1, any likelihoods LN + 1 still left are summed over in a similar manner. From this, we can find

[formula]

In the same way, we can calculate the second stage probabilities. For example, with Gk' = {K' < k}, we have

[formula]

These sums are evaluated in the same way as the first stage probabilities by keeping sets of likelihoods Lk' in memory, with the only differences being that for each k  ≤  N, we set [formula] after computing Lk' as before (adding paths that cross over between stages), and for k = N + 1, we set [formula]. Finally, we can calculate

[formula]

and any other statistics of K and K' can be found in the same manner.

This iterative approach can provide a big performance improvement over directly computing ([\ref=ProbK]) in certain situations. The likelihood set Ak generally grows like O(Rk) for some R∈[1,2], but in practice, R is fairly close to 1 if either P(Dn = 1|H = 1) is increasing or P(Dn = 1|H = 0) is decreasing, which physically corresponds to the target moving closer to the sensor network over time.

We finally remark that the thresholds η0 and η1 are selected in practice using the Wald approximations, [formula] and [formula], for some target probabilities P*D,P*F∈(0,1). It can be shown that at the mean stopping time k = E(K), [formula] and [formula] ([\cite=Po94], p. 104). The second stage thresholds η0' and η1' can be chosen in a similar manner with some given P*D  ' and P*F  '.

Numerical examples

In this section, we consider a few example scenarios that illustrate different properties of the static and dynamic elements described above. To clarify the discussion, we consider a simple situation with two sensors, S1 and S2, where the first-stage static network includes only S1, with no fusion, and the second-stage network includes both S1 and S2 and combines them at a Bayes fusion center with uniform costs and priors. At each time k for 1  ≤  k  ≤  N, N = 25, we assume the sensor Sj has false alarm and detection probabilities of 0.5 - Aj - Bjk and 0.5 + Aj + Bjk respectively, where the Aj and Bj are some fixed constants and j∈{1,2}. This setup loosely models a target object moving over time at a constant speed, with only S1 being initially active and triggering S2 as needed. The thresholds are set using the Wald approximations as described in Section [\ref=SecDynamic].

The sequential test statistics are shown in Figures [\ref=FigSeq1] and [\ref=FigSeq2] for various choices of the above parameters. The first scenario corresponds to the target moving closer to both sensors over time, while the second one describes a situation where the target maintains a fixed distance from S1, but moves towards S2. In both cases, the first-stage thresholds are set up to be modest targets, so that S1 quickly makes its preliminary decision and activates S2 well before the system reaches its final decision. The stopping time distributions generally have large variances and are highly oscillatory in the scenario with stationary sensor statistics, but are smooth in the one with a moving target.

We also calculate estimates of the base R in Section [\ref=SecDynamic], based on the number of sample paths still present at the final time N = 25. This gives an indication of how much computation time was saved by culling out the sample paths that crossed the thresholds at each time step. We find that R is significantly less than 2, especially in the first case, and the running time is several orders of magnitude less than it would be if we computed ([\ref=ProbK]) directly.

Conclusion

We have described a general framework for decision-level data fusion performance and tradeoff analysis between different sensor configurations. We have discussed an extension of the classical Wald sequential test to cover a multiple-stage cueing scenario where the decisions from one sensor network are used to activate a second network for a closer look at a target. We have described some numerical examples illustrating the behavior of the resulting statistical quantities. These results motivate future work on better characterizing the stopping time distributions as well as the dependencies between the first and second stage times.