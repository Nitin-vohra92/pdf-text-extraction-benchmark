On Optimal Jamming Over an Additive Noise Channel

Introduction

The interaction between communication and control has been an important research area for decades. We consider the problem of optimal jamming, by a power constrained agent, over additive noise channel. This problem was already solved in [\cite=basar1983gaussian] [\cite=basar1985complete] for Gaussian source and channel. The saddle point solution to this zero-sum game, derived for Gaussian source-channel pair, involves randomized linear mapping for the transmitter and generating independent, Gaussian noise as the jammer output and a linear decoder. In this paper, by leveraging the recent results on conditions for linearity of optimal estimation and communication mappings, [\cite=akyol2012conditions] [\cite=mapping], we extend the analysis to non-Gaussian sources and channels. The contributions of this paper are:

We show that linearity is essential in the jamming problem. The jammer, whenever possible, forces the transmitter and the receiver to be linear. In the Gaussian source-channel setting, this effect corresponds to generating a Gaussian jamming noise, while in general the optimal jamming noise is not Gaussian.

We derive the necessary and sufficient condition (called the "matching condition") on the jamming noise density to ensure linearity of the optimal transmitter and the receiver.

Based on the matching condition, we derive asymptotic (in channel signal-to-noise ratio (CSNR)), optimal jamming strategies.

We present a numerical method to approximate the optimal jammer strategy, in cases where a matching jamming density does not exist and it cannot force the transmitter and receiver to be exactly linear.

This paper is organized as follows. In Section II, we present the problem definition and preliminaries. In Section III, we review the prior results related to jamming, estimation and communication problems. In Section IV, we derive the linearity result, which leads to our main result in Section V. In Section VI, we study the implications of the main result, and in Section VII, we present a procedure to approximate the optimal jamming density in the non-matching case. Finally, we discuss the future directions in Section VIII.

Problem Definition

Let [formula] and [formula] denote the respective sets of real numbers and positive real numbers. Let [formula], [formula] and [formula] denote the expectation, probability and convolution operators, respectively. Let Bern(p) denote the Bernoulli random variable, taking values in { - 1,1} with probability {p,1 - p}. The Gaussian density with mean μ and variance σ2 is denoted as N(μ,σ2). Let [formula] denote the first order derivative of the function f(  ·  ). Let δ(  ·  ,  ·  ) denote the Kronecker delta function. All logarithms in the paper are natural logarithms and may in general be complex, and the integrals are, in general, Lebesgue integrals. Let us define the set S as the set of Borel measurable [formula] mappings.

We consider the general communication system whose block diagram is shown in Figure 1. A scalar zero mean source [formula] is mapped into [formula] by function gT(   ·   )∈S and transmitted over an additive noise channel. The adversary receives the same signal X and generates the jamming signal Z through function gA(   ·   )∈S which is added to the channel output, and aims to compromise the transmission of the source. The received signal U  =  Y + Z + N is mapped by the decoder to an estimate X̂ via function h(   ·   )∈S. The zero mean noise N is assumed to be independent of the source X. The source density is denoted fX(  ·  ) and the noise density is fN(  ·  ) with characteristic functions FX(ω) and FN(ω), respectively. We assume that the source and the noise variances are finite, i.e., [formula] and [formula].

The overall cost, measured as the mean squared error (MSE) between source X and its estimate at the decoder X̂, is a function of the transmitter, jammer and the receiver mappings:

[formula]

Transmitter [formula] and receiver [formula] seek to minimize this cost while the adversary (jammer) seeks to maximize it by appropriate choice of [formula]. Power constraints must be satisfied by the transmitter

[formula]

and jammer

[formula]

The conflict of interest underlying this problem implies that the optimal transmitter-receiver-adversarial policy is the saddle point solution (g*T(  ·  ),g*A(  ·  ),h*(  ·  )) satisfying the set of inequalities

[formula]

Prior Work

The jamming problem, in the form defined here, was studied in [\cite=basar1983gaussian] [\cite=basar1985complete], for Gaussian sources and channels. The problem of interest is intrinsically connected to the fundamental problems of estimation theory and the theory of zero-delay source-channel coding. In particular, conditions for linearity of optimal estimation[\cite=akyol2012conditions] and optimal mappings in communications [\cite=mapping] are relevant to our problem here. We start with the estimation problem.

Estimation Problem

Consider the setting in Figure 2. The estimator receives U, the noisy version of the source X and generates the estimate X̂ by the function [formula] such that MSE, [formula] is minimized. It is well known that, when a Gaussian source is contaminated with Gaussian noise, a linear estimator minimizes MSE. Recent work [\cite=akyol2012conditions] analyzed, more generally, the conditions for linearity of optimal estimators. Given a noise (or source) distribution, and a specified signal to noise ratio (SNR), conditions for existence and uniqueness of a source (or noise) distribution for which the optimal estimator is linear were derived. Also, the asymptotic linearity of the optimal estimators was shown for low SNR if the channel is Gaussian regardless of the source and, vice versa, for high SNR if the source is Gaussian regardless of the channel.

Here, we present the basic result pertaining to the jamming problem considered here. Specifically, we present the necessary and sufficient condition for source and channel distributions such that the linear estimator [formula] is optimal where [formula] is the SNR.

Given SNR level κ, and noise Z with characteristic function FZ(ω), there exists a source X for which the optimal estimator is linear if and only if

[formula]

Given a valid characteristic function FZ(ω), and for some [formula], the function FκZ(ω) may or may not be a valid characteristic function, which determines the existence of a matching source. For example, matching is guaranteed for integer κ and it is also guaranteed for infinitely divisible Z. More comprehensive discussion of the conditions on κ and FZ(ω) for FκZ(ω) to be a valid characteristic function can be found in [\cite=akyol2012conditions].

Communication Problem

In [\cite=mapping], a communication scenario whose block diagram is shown in Figure 3 was studied. In this setting, a scalar source [formula] is mapped into [formula] by function g∈S, and transmitted over an additive noise channel. The channel output U  =  Y + Z is mapped by the decoder to the estimate X̂ via function h∈S. The zero mean noise Z is assumed to be independent of the source X. The source density is denoted fX(  ·  ) and the noise density is fZ(  ·  ) with characteristic functions FX(ω) and FZ(ω), respectively.

The objective is to minimize, over the choice of encoder g and decoder h, the distortion

[formula]

subject to the average transmission power constraint,

[formula]

A result pertaining to the simultaneous linearity of optimal mappings is summarized in the next theorem.

The optimal mappings are either both linear or they are both nonlinear.

The necessary and sufficient condition for linearity for both mappings is given by the following theorem.

For a given power constraint PT, noise Z with variance σ2Z and characteristic function FZ(ω), source X with variance σ2X and characteristic function FX(ω), the optimal encoder and decoder mappings are linear if and only if

[formula]

where [formula] and [formula].

Gaussian Jamming Problem

The problem of transmitting independent and identically distributed Gaussian random variables over a Gaussian channel in the presence of an additive jammer was considered in [\cite=basar1983gaussian] [\cite=basar1985complete]. In [\cite=basar1985complete] a game theoretic approach was developed and it was shown that the problem admits a mixed saddle point solution where the optimal transmitter and receiver employ a "randomized" strategy. The randomization information can be sent over a side channel between transmitter and receiver or it could be viewed as the information generated by the third party and observed by both transmitter and receiver . Surprisingly, the optimal jamming strategy ignores the input to the jammer and merely generates "Gaussian" noise, independent of the source.

The optimal encoding function for the transmitter is randomized linear mapping:

[formula]

where γ(i) is i.i.d. Bernoulli ([formula]) over the alphabet { - 1,1}

[formula]

and [formula]. The optimal jammer generates i.i.d. Gaussian output Z(i)

[formula]

where Z(i) is independent of the source X(i). The optimal receiver is

[formula]

and total cost is

[formula]

In this paper, we study the generalized jamming problem which does not limit the set of sources and channels to Gaussian random variables. Surprisingly, as we show in Section V, the linearity property of the optimal transmitter and receiver at the saddle point solution still holds, while the Gaussianity of the jammer output in the early special case was merely a means to satisfy this linearity condition, and does not hold in general.

The proof of Theorem [\ref=th2] relies on the fact that for a Gaussian source over a Gaussian channel, zero-delay linear mappings achieve the performance of the asymptotically high delay optimal source-channel communication system [\cite=cover2006elements]. This fact is unique to the Gaussian source-channel pair, hence it might be tempting to conclude that the saddle point solution in Theorem [\ref=th2] can only be obtained in the "all Gaussian" setting. Perhaps surprisingly, in this paper, we show that there are infinitely many source-noise pairs that yield a saddle point solution similar to Theorem [formula] (see Theorem [\ref=th] and Remark [\ref=rem]).

A Simple Upper Bound Based on Linearity

In this section, we present a new lemma that is used to upper bound the distortion of any zero-delay communication system by that of the fixed, best linear encoder and decoder. Although the main idea is quite simple, it is nevertheless presented in a separate lemma, due to its operational significance here.

Consider the problem setting in Figure [\ref=jammingfig]. For any given jammer fZ(z), the distortion achievable by the transmitter-receiver, D, is upper bounded by the distortion achieved by linear encoder and decoder [formula] which is determined by second moments, regardless of the normalized densities. Hence, the linear mappings will maximize the distortion.

Clearly, the the encoder and decoder can utilize the linear mappings that satisfy the power constraint PT for any source and channel density. Hence, it is straightforward to achieve D = DL in any source-channel density by using linear mappings.

Lemma [\ref=mainlemma] is the main result that connects the recent results on "linearity" of optimal estimation and communication mappings to the jamming problem. Lemma [\ref=mainlemma] implies that the optimal strategy for a jammer which can only control the density of the additive noise channel, is to force the transmitter and receiver to use linear mappings.

Main Result

Our main results concerns the optimal strategy for the transmitter, the adversary and the receiver in Figure 1 for the transmission index i. Let us introduce a quantitiy β as

[formula]

Assumption: Throughout this section, we assume that FβX(ω) is a valid characteristic function for a given [formula]. The case where this assumption does not hold is analyzed in Section VII.

Next, we present our main result which pertains to optimal jamming.

For the jamming problem, the optimal encoding function for the transmitter is randomized linear mapping:

[formula]

where γ(i) is i.i.d. Bernoulli ([formula]) over the alphabet { - 1,1}

[formula]

and [formula]. The optimal jamming function is to generate i.i.d. output Z(i) with characteristic function

[formula]

where Z(i) is independent of the adversarial input X(i).

The optimal receiver is

[formula]

and total cost is

[formula]

Moreover, this saddle point solution is (almost surely) unique.

We prove this result by verifying that the mappings in this theorem satisfy the saddle point inequalities given in ([\ref=saddle]), following the approach in [\cite=basar1986solutions].

RHS of ([\ref=saddle]): Suppose the policy of the jammer is given as in Theorem [\ref=th3]. Then, the communication system at hand becomes identical to the communication problem considered in Section II.B, for which the linear encoder, i.e., Y(i) = αTX(i) is optimal (see Theorem 3). Any probabilistic encoder, given in the form of ([\ref=rand3]) (irrespective of the density of γ) yields the same cost with deterministic encoders and hence is optimal.

LHS of ([\ref=saddle]): Let us derive the overall cost conditioned on the realization of the transmitter mappings (i.e., γ = 1 and γ =  - 1) used in conjunction with optimal linear decoder, given in ([\ref=dec]). If γ = 1

[formula]

for some constants ξ,ψ, and similarly if γ =  - 1

[formula]

where the overall cost is

[formula]

Clearly, for [formula] overall cost J is only a function of the second order statistics of the adversarial outputs, irrespective of the distribution of Z; hence the solution presented here is a saddle point.

Towards showing (almost sure) uniqueness, we start by restating the fact that the optimal solution for transmitter is in the randomized form given in ([\ref=rand3]). Let us prove the properties which were not covered by the proof of the saddle point.

Characteristic function of Z: The choice [formula] renders the transmitter and receiver mapping linear, due to Theorem 3 and maximizes the overall cost due to Lemma 1.

Independence of Z of X and N: If the jammer introduces some correlation, i.e., if [formula], the transmitter can adjust its Bernoulli parameter to decrease the distortion. Hence, the optimal adversarial strategy is setting [formula] which is guaranteed by the independence of zero mean random variables Z and X.

Choice of Bernoulli parameter: Note that the optimal choice of the Bernoulli parameter for the transmitters is [formula] since other choices will not cancel out the cross terms in ([\ref=c1]) and ([\ref=c2]). These cross terms can be exploited by the adversary to increase the cost, and hence an optimal strategy for transmitter is to set γ = Bern(1 / 2).

Note that Theorem [\ref=th3] recovers the previous results that focus on the Gaussian source [\cite=basar1983gaussian] [\cite=basar1985complete]. When X  ~  N(0,σ2X), then the unique matching noise, determined by ([\ref=main]) is also Gaussian Z  ~  N(0,PA) for all power levels PA and PT. Hence, Theorem 5 strictly subsumes Theorem 4.

Implications of the Main Result

In this section, we explore some special cases obtained by varying β and utilizing the matching condition ([\ref=main]). We start with a simple but perhaps surprising result.

In the case of identically distributed source and channel, i.e., X  ~  N and PT = PA  =  σ2N, then optimal jamming strategy would be generating a random variable identically distributed with X (and N), and optimal transmitter functions are as given in Theorem [\ref=th3].

It is straightforward to see from ([\ref=main]) that, at β = 2, the characteristic functions must be identical FZ(ω) = FX(ω) almost everywhere. Since the characteristic function uniquely determines the density [\cite=billingsley2008probability], Z  ~  X.

Theorem [\ref=th] demonstrates that there is indeed a rich set of source and channel densities, that makes the optimal mappings linear. Hence, Gaussianity assumption of the source and channel is not necessary to achieve the saddle point solution.

Let us next consider a case where the jammer does not need to know the density of the source, i.e., can perform optimally regardless of the source density.

At asymptotically low CSNR level, i.e., β  →    ∞  , for a Gaussian channel, optimal jamming strategy is to generate Gaussian noise independent of the source, regardless of the source density.

As we have shown in the proof of Theorem 4, the jammer's aim is to force the transmitter and the receiver to use linear mappings. Hence, the matching jamming noise (if exists) satisfies the following:

[formula]

As β  →    ∞  , RHS of ([\ref=upp]) converges to Gaussian characteristic function, due to central limit theorem [\cite=billingsley2008probability], and hence ([\ref=main]) is asymptotically satisfied.

Another interesting case is the high CSNR level (β  →  0) and Gaussian source case where any jammer output Z, independent of the source, is asymptotically optimal regardless of the noise density.

At an asymptotically high CSNR level, i.e., β  →  0, for a Gaussian source, optimal jamming strategy is to generate noise independent of the source regardless of the noise density.

Again, the matching jamming noise (if exists) must satisfy

[formula]

As β  →  0, LHS of ([\ref=upp2]) converges to the Gaussian characteristic function and, hence ([\ref=main]) is asymptotically satisfied.

The Non-matching Case

What is the optimal jammer density fZ(  ·  ), when the jammer cannot make the optimal mappings linear, i.e., FβX(ω) is not a valid characteristic function? In the following, we first examine the case of the basic estimation setting, then extend our analysis to jamming setting.

Estimation Setting

The problem of interest is open, to our best knowledge, even in the more fundamental setting, i.e., for estimation problem depicted in Figure 2. Particularly, we are interested in the noise density fZ(  ·  ) that maximizes minimum mean square error, [formula]. Clearly, if FβX(ω) is a valid characteristic function, the worst-case noise will have the characteristic function FZ(ω) = FβX(ω) and make the optimal (MMSE) estimator linear. Intuitively, it is expected that in the case where FβX(ω) is not a valid characteristic function, worst-case noise would be the one that forces the optimal estimator as close to linear as possible in some sense. In the following, we derive results, based on optimal estimation which show in what precise sense this intuition is correct. Let us restate, using Bayes' rule, the optimal estimator [formula] as:

[formula]

which can also be written as:

[formula]

We then replace FZ(ω) and FX(ω) with their polynomial expansions, particularly Gram-Charlier expansion over the Gaussian density (see e.g. [\cite=cramer1999mathematical] for details):

[formula]

where αm and θm are the polynomial coefficients associated with FZ(ω) and FX(ω) respectively. Observe that plugging ([\ref=a1]) and ([\ref=a2]) in ([\ref=a]), the optimal estimator can be expressed as a ratio of two polynomials:

[formula]

Let Pn(u) be a sequence of polynomials orthonormal with respect to P(u) (note that P(u) is a probability density function, particularly it is fU(  ·  ), i.e., the density of U = X + Z.)

[formula]

Then h(u) can expanded in terms of Pm(u)

[formula]

where

[formula]

Then, MMSE is

[formula]

where ([\ref=eq1]) follows from the orthogonality principle, and ([\ref=eq3]) follows from ([\ref=eqq]). Worst case noise aims to maximize J and hence minimize [formula]. Observe that c0 = 0 and [formula]. These two coefficients clearly are determined by the second order statistics of the source and the noise, while higher order coefficients, i.e., [formula] depend on the higher order statistics. Note also that the polynomials associated with these coefficients are P0(u) = 1 and [formula]. We present our main result regarding this setting.

The worst-case noise for the estimation setting minimizes [formula], where cm are the coefficients of the orthonormal polynomial expansion with measure fY(  ·  ).

Given the source density, we can find the optimal Mth order polynomial approximation to the optimal estimator used in conjunction with the worst-case noise. In the following, we focus on finding the worst-case noise, that matches this estimator.

Let us assume [formula] for [formula]. Then, the following holds:

[formula]

Expanding ([\ref=upeq1]), and expressing integrals as convolutions, we have

[formula]

Taking the Fourier transforms of both sides, we obtain

[formula]

Hence, given the optimal estimator, we can find the worst-case noise by solving the differential equation given in ([\ref=upeq3]).

Jamming Setting

Let us focus on the original problem of jamming. We carry a similar analysis to derive the best Mth order polynomial expansion of the decoder, given the encoder. For simplicity, we assume the the transmitter function is linear, i.e., as given ([\ref=rand3]). Note however that, as Theorem [\ref=simultenous] implies, if the optimal decoder is nonlinear so must be the encoder. Hence, this approach will yield an approximate solution.

Towards deriving the optimal approximation of the decoder, we again express the decoder as

[formula]

where [formula] and fN + Z is the density of N + Z. Nothing that X and Z are independent, we have

[formula]

which implies that, plugging the appropriate polynomial expression for FX(ω), FN(ω) and FZ(ω), we can express h(u) as the ratio of two polynomials, i.e.,

[formula]

Again, expanding h(u) the polynomials which are orthonormal under the measure P(u) (which is the density of αTX + Z + N), and following the same steps that led to ([\ref=eq3]), we obtain

[formula]

where cm's are the coefficients of the polynomials that are orthonormal with respect to the density of the channel output U = αTX + Z + N. Hence, we can characterize the optimal jamming density, as the one that minimizes [formula]. Similar to estimation setting, once the best polynomial approximation is found, the optimal jamming density can be obtained by solving a differential equation which can be obtained following the same steps that yield ([\ref=upeq3]).

Discussion

In this paper, we studied the problem of optimal zero-delay jamming over an additive noise channel. Utilizing the recent results on conditions for linearity of optimal estimation, and of optimal mappings in source-channel coding, we obtained the saddle-point solution to the jamming problem for general sources and channels. We showed that linearity is essential in the jamming problem, in the sense that the optimal jamming strategy is to effectively force both transmitter and receiver to linear mappings. We analyzed conditions and general settings where such strategy can indeed be achieved by the jammer, and provided a "matching condition" which strictly subsumes the prior results specialized to all Gaussian settings. Finally, we provided a procedure to approximate optimal jamming in the cases where the jammer cannot impose linearity on the transmitter and the receiver.

Analysis in this paper is limited to scalar sources and channels. An important extension of this work, currently under investigation, will be on vector sources and/or channels. Another line of future work involves the precise characterization of jamming noise in the non-matching case.