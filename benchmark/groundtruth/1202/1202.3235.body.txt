Proposition Example Remark Problem Conjecture Definition

Computing a partial Schur factorization of nonlinear eigenvalue problems using the infinite Arnoldi method

Introduction

The nonlinear eigenvalue problem (NEP) will in this paper be used to refer to the problem to find [formula] and [formula] such that

[formula]

where [formula] is analytic in Ω, which is an open disc centered at the origin.

This problem class has received a considerable amount of attention in the literature. See, e.g., the survey papers [\cite=Mehrmann:2004:NLEVP] [\cite=Ruhe:1973:NLEVP] and the monographs [\cite=Lancaster:2002:LAMBDA] [\cite=Gohberg:1982:MATRIXPOLYNOMIALS]. The results for [\eqref=eq:nlevpM] are often (but not always) presented with some restriction of the generality of M, such as the theory and algorithms for polynomial eigenvalue problems (PEPs) in [\cite=Lancaster:2002:LAMBDA] [\cite=Lancaster:2005:PSEUDOSPECTRA] [\cite=Mackey:2006:STRUCTURED] [\cite=Fassbender:2008:PCP], in particular the algorithms for quadratic eigenvalue problems (QEPs) [\cite=Tisseur:2001:QUADRATIC] [\cite=Bai:2005:SOAR] [\cite=Meerbergen:2001:LOCKING], but also recent approaches for rational eigenvalue problems (REPs) [\cite=Su:2008:REP] [\cite=Voss:2003:MAXMIN]. The results we will now present are directly related to [\cite=Jarlebring:2010:TRINFARNOLDI] where an algorithm is presented which we here call the infinite Arnoldi method. An important aspect of the algorithm in this paper, and the infinite Arnoldi method, is generality. Although the algorithm and results of this paper are applicable to PEPs, QEPs and REPs, the primary goal of the paper is not to solve problems for the most common structures, but rather to construct an algorithm which can be applied to other, less common NEPs in a somewhat automatic fashion. Some less common NEPs are given in the problem collection [\cite=Betcke:2010:NLEVPCOLL]; there exists NEPs with exponential terms [\cite=Jarlebring:2010:DELAYARNOLDI] and implicitly stated NEPs such as [\cite=Rott:2010:ITERATIVE]. In this paper we will present a procedure to compute a partial Schur factorization in the sense of the concepts of partial Schur factorizations and invariant pairs for nonlinear eigenvalue problems introduced in [\cite=Kressner:2009:BLOCKNEWTON]. These concepts can be summarized as follows. First note that the function M is in this work assumed to be analytic and can always be decomposed as a sum of products of constant matrices and scalar nonlinearities,

[formula]

where [formula], [formula] are analytic in Ω. We define

[formula]

where fi(Λ), [formula] are the matrix functions corresponding to fi, which are well defined if σ(Λ)  ⊂  Ω. An invariant pair [formula] (in the sense of [\cite=Kressner:2009:BLOCKNEWTON]) satisfies

[formula]

Additional appropriate orthogonality conditions for Y and Λ yield a consistent definition of invariant pairs and the eigenvalues of Λ are solutions to the nonlinear eigenvalue problem. In this setting, a partial Schur factorization corresponds to a particular invariant pair where Λ is an upper triangular matrix.

The results of this paper are based on a reformulation of the problem of finding an invariant pair of the nonlinear eigenvalue problem as a corresponding problem formulated with a (linear) infinite dimensional operator denoted B, also used in the infinite Arnoldi method [\cite=Jarlebring:2010:TRINFARNOLDI]. In [\cite=Jarlebring:2010:TRINFARNOLDI], we presented an algorithm which can be interpreted as Arnoldi's method applied to the operator B. Although the operator B maps functions to functions, it turns out that the algorithm can be implemented with finite-dimensional linear algebra operations if the Arnoldi method (for B) is started with a constant function. This results in a Krylov subspace consisting of polynomials. Unlike the polynomial setting in [\cite=Jarlebring:2010:TRINFARNOLDI], we will in this work consider linear combinations of exponentials and polynomials allowing us to carry out an efficient restarting process. We will show that similar to the polynomial setting [\cite=Jarlebring:2010:TRINFARNOLDI], the Arnoldi method for B applied to linear combinations of polynomials and exponentials can be carried out with finite-dimensional linear algebra operations.

The reformulation with the operator B allows us to adapt a procedure based on the Arnoldi method designed for the computation of a partial Schur factorization for standard eigenvalue problems. We will use a construction inspired by the implicitly restarted Arnoldi method (IRAM) [\cite=Morgan:1996:RESTARTING] [\cite=Sorensen:1992:IMPLICIT] [\cite=Lehoucq:2001:IRAM] [\cite=Lehoucq:1996:DEFLATION]. The construction is first outlined adaption is outlined in Section [\ref=sect:linearprobs] and consists of two steps respectively given in Section [\ref=sect:lockedinfarn] and Section [\ref=sect:restart]. They correspond to carrying out the Arnoldi method for the operator B with a locked invariant pair and a procedure to restart it.

We finally wish to mention that there exist restarting schemes for algorithms for special cases of [\eqref=eq:nlevpM], in particular for QEPs [\cite=Zhou:2010:RESTARTED] [\cite=Jia:2010:RSOAR].

Reformulation as infinite-dimensional operator problem

In order to characterize the invariant pairs of [\eqref=eq:nlevpM] for our setting we first need to introduce some notation. The function [formula], will be defined by

[formula]

for [formula] and defined as the analytic continuation at λ = 0. Note that B is also analytic in Ω, under the condition that λ = 0 is not a solution to [\eqref=eq:nlevpM]. We will in this work assume that the NEP is such that λ = 0 is not an eigenvalue. From the definition [\eqref=eq:Bdef] we reach a transformed nonlinear eigenvalue problem

[formula]

We will also use a decomposition of B similar to the decomposition [\eqref=eq:M1f1] of M. That is, we let

[formula]

where [formula], [formula] are analytic in Ω. Moreover, we will use the straightforward coupling of the decomposition of M, by setting

[formula]

We will use the following notation in order to express the operator and carry out manipulations of the operator in a concise way. Let the differentiation operator [formula] be defined by the Taylor expansion in a consistent way, i.e.,

[formula]

where [formula] is a smooth function. We are now ready to introduce the operator which serves as the basis for the algorithm.

Let B denote the map defined by the domain [formula] and the action

[formula]

where

[formula]

Several properties of the operator B are characterized in [\cite=Jarlebring:2010:TRINFARNOLDI]. Most importantly, its reciprocal eigenvalues are the solutions to [\eqref=eq:nlevpB] and hence also to [\eqref=eq:nlevpM] if λ  ≠  0. In this work we will need a more general result, characterizing the invariant pairs of B.

To this end we first define the application of the operator B to block functions, and say that if [formula] with columns given by

[formula]

then BΨ is interpreted in a block fashion, i.e.,

[formula]

With this notation, we can now consistently define an invariant pair as a pair (Ψ,R) of the operator B, where [formula] and [formula] such that

[formula]

The following theorem explicitly shows the structure of the function Ψ and relates invariant pairs of the operator with invariant pairs [\eqref=eq:invpairMM], i.e., invariant pairs in the setting in [\cite=Kressner:2009:BLOCKNEWTON].

Suppose [formula] is invertible and suppose (Ψ,Λ- 1) is an invariant pair of B. Then, Ψ can be expressed as,

[formula]

for some matrix [formula]. Moreover, given [formula] and [formula] where Λ is invertible, the following statements are equivalent:

The pair (Ψ,Λ- 1), where Ψ(θ): = Y exp (θΛ), is an invariant pair of the operator B, i.e.,

[formula]

The pair (Y,Λ) is an invariant pair of the nonlinear eigenvalue problem [\eqref=eq:nlevpM] in the sense of [\cite=Kressner:2009:BLOCKNEWTON], i.e.,

[formula]

By differentiating (with respect to θ) the left and right-hand side of the definition of an invariant pair [\eqref=eq:invpairdef] and using that the action of B is integration, we find that Ψ satisfies the matrix differential equation,

[formula]

By multiplying by Λ and vectorizing the equation, we have

[formula]

and we can form an explicit solution,

[formula]

The conclusion [\eqref=eq:Fstruct] follows by reversing the vectorization and setting [formula].

The equivalence between statements i) and ii) follows directly from the fact that M(0) is invertible (since Λ is invertible and λ = 0 is not an eigenvalue) and the application of Lemma [\ref=thm:structinvres].

Outline of the algorithm

We now know (from Theorem [\ref=thm:invpairs]) that an invariant pair of the NEP [\eqref=eq:nlevpM] is equivalent to an invariant pair of the linear operator B. The general idea of the procedure we will present in later sections is inspired by the procedures used to compute partial Schur factorizations for standard eigenvalue problems with the Arnoldi method [\cite=Lehoucq:1996:DEFLATION] [\cite=Sorensen:1992:IMPLICIT] [\cite=Stewart:2001:KRYLOVSCHUR]. We will carry out a variant of the corresponding algorithm for the operator B. More precisely, we will repeat the following two steps.

In the first step (described in Section [\ref=sect:lockedinfarn]) we compute, in a particular way, an orthogonal projection of the operator B onto a Krylov subspace. The projection is constructed such that it possesses the feature known as locking. This here means that given a partial Schur factorization (or an approximation of the partial Schur factorization) the projection respects the invariant subspace and returns an approximation containing the invariant pair (without modification) and also approximations of further eigenvalues. This prevents repeated convergence to the eigenvalues in the locked partial Schur factorization in a robust way.

In the literature (for standard eigenvalue problems) this projection is often computed with a variation of the Arnoldi method. More precisely, we will start the Arnoldi algorithm with a state containing the (locked) partial Schur factorization. The result of the infinite Arnoldi method can be expressed as what is commonly called an Arnoldi factorization,

[formula]

where [formula] is a Hessenberg matrix, [formula] is an orthogonal basis of the Krylov subspace and Fk is the first k columns of Fk + 1. In this paper we use a common notation for Hessenberg matrices; the first k rows of the matrix k will be denoted [formula]. A property of the locking feature is that the Hessenberg matrix k has the structure

[formula]

where [formula] is an upper triangular matrix. The upper left block of the k is called the locked part, since the first pl columns of [\eqref=eq:arnfact] is the equation for an invariant pair [\eqref=eq:invpairdef].

In the first step we show how the Arnoldi method with locking can be carried out if we represent the functions in the algorithm (and in the factorization [\eqref=eq:arnfact]) in a structured way. Unlike the infinite Arnoldi method in [\cite=Jarlebring:2010:TRINFARNOLDI] we will need to work with functions which are linear combinations of exponentials and polynomials. It turns out that, similar to [\cite=Jarlebring:2010:TRINFARNOLDI], the action of the operator as well as the entire Arnoldi algorithm can be carried out with finite-dimensional arithmetic, while the use of exponentials is benificial also for the second step.

In the second step (described in Section [\ref=sect:restart]), i.e., after computing the Arnoldi factorization [\eqref=eq:arnfact], we process the factorization such that two types of information can be extracted.

We extract converged eigenvalues from the Arnoldi factorization [\eqref=eq:arnfact] and store those in a partial Schur factorization. Due to the locking feature, the updated partial Schur factorization will be of the same size as the locked part of [\eqref=eq:arnfact] or larger.

We extract a function with favorable approximation properties for those eigenvalues of interest, which have not yet converged.

This information is extracted in a fashion similar to implicitly restarted Arnoldi (IRAM) [\cite=Morgan:1996:RESTARTING] [\cite=Lehoucq:2001:IRAM] [\cite=Lehoucq:1996:DEFLATION] [\cite=Stewart:2001:KRYLOVSCHUR]. However, several modifications are necessary in order to restart with the structured functions.

The two steps are subsequently iterated by starting the (locked version) of Arnoldi's method with the extracted function and with (the possibly larger) partial Schur factorization. Thus, nesting the infinite Arnoldi method with a restarting scheme which is expected to eventually converge to a partial Schur factorization.

The infinite Arnoldi method with locked invariant pair

In the first step of the conceptual algorithm described in Section [\ref=sect:linearprobs], we need to carry out an Arnoldi algorithm for B with the preservation feature that the given partial Schur factorization is not modified. In an infinite-dimensional setting, the adaption to achieve this feature with the Arnoldi method is straightforward by initiating the state of the Arnoldi method with the invariant pair. The procedure is given in Algorithm [\ref=alg:arnoldiBBB], where the basis of the invariant subspace associated with the partial Schur factorization is assumed to be orthogonal with respect to a given scalar product <    ·  ,  ·    >  .

Representation of structured functions

In later sections we will provide a specialization of all the steps in the abstract algorithm above (Algorithm [\ref=alg:arnoldiBBB]) such that we can implement it in finite-dimensional arithmetic. The first step in the conversion of Algorithm [\ref=alg:arnoldiBBB] into a finite-dimensional algorithm is to select an appropriate starting function and an appropriate finite-dimensional representation of the functions.

In this work, we will consider functions which are sums of exponentials and polynomials with the structure

[formula]

where [formula], [formula], [formula] and [formula] is a vector of polynomials. Moreover, we let S be a block triangular matrix

[formula]

and set [formula] where pl  ≤  p, where R will later be chosen such that it is an approximation of the matrix in the Schur factorization. This structure has a number of favorable properties important for our situation.

The action of B applied to functions of the type [\eqref=eq:exppolystruct] can be carried out in an efficient way using only finite-dimensional operations. This stems from the property that the action of B corresponds to integration and the set of polynomials and exponentials under consideration are closed under integration. Algorithmic details will be given in Section [\ref=sect:action].

This particular structure allows the storing and orthogonalization against an invariant subspace, which, according to Theorem [\ref=thm:invpairs], has exponential structure.

The structure provides a freedom to choose the blocks S12 and S22. This allows us to appropriately restart the algorithm. Due to the exponential structure illustrated in Theorem [\ref=thm:invpairs], it will turn out to be natural to impose an exponential structure on the Ritz functions in order to construct a function f to be used in the restart. The precise choice of S12, S22 and Y will be further explained in Section [\ref=sect:restart].

In practice we also need to store the structured functions in some fashion, preferably with matrices and vectors. It is tempting to store the exponential part and polynomial part of [\eqref=eq:exppolystruct] separately, i.e., to store the exponential part with the variable Y, S and c and the polynomial part by coefficients in some polynomial basis, e.g., the coefficients [formula] in the monomial basis [formula]. Although such an approach is natural from a theoretical perspective, it is not adequate from a numerical perspective. This can be seen as follows. Note that the Taylor expansion of the structured function [\eqref=eq:exppolystruct] is

[formula]

A potential source of cancellation is apparent for the first N terms in [\eqref=eq:naive] if the polynomial q(θ) approximates - Y exp (θS)c. This turns out to be a situation appearing in practice in this algorithm, making the storing of the structured functions in this separated form inadequate.

We will instead use a function representation where the coefficients in the Taylor expansion are not formed by sums. This can be achieved by replacing the first N terms in [\eqref=eq:naive] by new coefficients [formula], i.e.,

[formula]

The structured functions [\eqref=eq:exppolystruct] will be represented with the four variables [formula], [formula], [formula], [formula], where [formula]. Note that this representation does not suffer from the potential cancellation effects present in the naive representation [\eqref=eq:naive].

Throughout this work we will need to carry out many manipulations of functions represented in the form [\eqref=eq:phistruct] and we need a concise notation. Let exp N denote the remainder part of the truncated Taylor expansion of the exponential, i.e.,

[formula]

This can equivalently be expressed as,

[formula]

with

[formula]

With this notation, we can now concisely express [\eqref=eq:phistruct] with exp N and Kronecker products,

[formula]

Action for structured functions

We have now (in Section [\ref=sect:struct]) introduced the function structure and shown how we can represent these functions with matrices and vectors. An important component in Algorithm [\ref=alg:arnoldiBBB] is the action of B. We will now show how we can compute the action of B applied to a function given with the representation [\eqref=eq:phistruct2].

Analogous to the definition of exp N, it will be convenient to introduce a notation for the remainder part of the nonlinear eigenvalue problem [formula] after a Taylor expansion to order N. We define,

[formula]

or equivalently,

[formula]

Note that with this definition

[formula]

We are now ready to express the action of B applied to functions with the structure [\eqref=eq:phistruct2]. Note that the construction of the new function φ+  =  Bφ in the following result only involves standard linear algebra operations of matrices and vectors.

Let [formula] and [formula] be given constants, where S is invertible. Suppose

[formula]

Then,

[formula]

where

[formula]

[formula]

and

[formula]

We show that φ+ constructed by [\eqref=eq:c+], [\eqref=eq:X+] and [\eqref=eq:y0] satisfies

[formula]

by first showing that the derivative of the left and the derivative of the right-hand side of [\eqref=eq:y0proof1] are equal and then showing that they are also equal in one point θ = 0. From the property [\eqref=eq:defexpN2], we have

[formula]

Moreover, the relation [\eqref=eq:X+] implies that

[formula]

Note that B corresponds to integration and the left-hand side of [\eqref=eq:y0proof1] is φ. The right-hand side can be differentiated using [\eqref=eq:y0proofdexp] and [\eqref=eq:y0proofdpoly]. We reach that the right-hand side of [\eqref=eq:y0proof1] is φ by using [\eqref=eq:c+].

We have shown that the derivative of the left and the derivative of the right-hand side of [\eqref=eq:y0proof1] are equal.

We now evaluate [\eqref=eq:y0proof1] at θ = 0. From the definition of B we have that that [formula], i.e., we wish to show that

[formula]

when N > 0. (The relation obviously holds for N = 0.) Note that by construction φ+ is a primitive function of φ. From the relations between fi, bi, Mi, Bi, in [\eqref=eq:BiMi] it follows that [\eqref=eq:y0proof2] is equivalent to

[formula]

We now consider the terms of φ+ in [\eqref=eq:phi+] separately. Note that for any analytic function [formula], we have

[formula]

where gN is the remainder term in the truncated Taylor expansion, analogous to exp N. It follows that,

[formula]

For the polynomial part of φ+ we have

[formula]

Note that [formula] is the sum of [\eqref=eq:y0proofexp]. Hence, we have shown [\eqref=eq:y0proofphi+] (and hence also [\eqref=eq:y0proof2]) by using [\eqref=eq:y0proofexp], [\eqref=eq:y0proofpoly] and the definition of x+ ,0 in [\eqref=eq:y0].

Scalar product and finite-dimensional specialization of Algorithm [\ref=alg:arnoldiBBB]

Since the goal is to completely specify all operations in Algorithm [\ref=alg:arnoldiBBB] in a finite-dimensional setting, we also need to provide a scalar product. In [\cite=Jarlebring:2010:TRINFARNOLDI] we worked with polynomials and we defined the scalar product via the Euclidean scalar product on monomial or Chebyshev coefficients. The structured functions described in Section [\ref=sect:struct] are not polynomials. We can however still define the scalar product consistent with [\cite=Jarlebring:2010:TRINFARNOLDI]. In this work we restrict the presentation to the consistent extension of the definition of the scalar products via the monomial coefficients. Given two functions

[formula]

we define

[formula]

It is straightforward to show that [\eqref=eq:scalarprod_def0] satisfies the properties of a scalar product and that the sum in [\eqref=eq:scalarprod_def0] is always finite for functions of the considered structure. The computational details for the scalar product and the orthogonalization process are postponed until the next section (Section [\ref=sect:scalarprod]).

The combination of the above results, i.e., the choice of the representation of the function structure (Section [\ref=sect:struct]), the operator action (Section [\ref=sect:action]) and the scalar product [\eqref=eq:scalarprod_def0], forms a complete specialization of all the operations in Algorithm [\ref=alg:arnoldiBBB]. For reasons of numerical efficiency, we will slightly modify the direct implementation of the operations.

Instead of representing the individual functions [formula] of the basis [formula] we will use a block representation and denote

[formula]

Now note that variables Y and S in the function structure [\eqref=eq:phistruct2] are not modified in Theorem [\ref=thm:structaction] and obviously not modified when forming linear combinations. Hence, the variables Y and S can be kept constant throughout the algorithm. This allows us to also use the structured representation [\eqref=eq:phistruct2] directly for the block function Fk instead of individually for [formula]. In every point in the algorithm, there exist matrices Ck and Vk such that

[formula]

with an appropriate choice of N.

The variable N defining the length of the polynomial part of the structure needs to be adapted during the iteration. This stems from the fact that functions φ+ and φ in Theorem [\ref=thm:structaction] are represented with polynomial parts of different length (N - 1 and N). Hence, we need to increase N by one after each application of B. Fortunately, the corresponding increase of N can be easily achieved by treating the leading element of exponential part as an element of the polynomial part. Here, this means using the fact that

[formula]

In this work, the starting function f will be an exponential function, and after the first application of B, we need to expand the polynomial part with one block consisting of [formula] with N = 0. Since k = pl + 1 at the first application of B, for an iteration corresponding to a given k, we need to expand the polynomial part of Fk with one block row consisting of [formula] with N = k - pl - 1.

With the block structure representation [\eqref=eq:Fk] we can now specialize Algorithm [\ref=alg:arnoldiBBB] for the structured functions. The finite-dimensional implementation of Algorithm [\ref=alg:arnoldiBBB] is given in Algorithm [\ref=alg:infarnoldi] and visually illustrated in Figure [\ref=fig:infarnoldi].

The input and output of the algorithm should be interpreted as follows. The variables Y, S, c specify the starting function f as well as the locked part of the factorization. The starting function is given by

[formula]

and the locked part of the factorization (in Algorithm [\ref=alg:arnoldiBBB] denoted (Ψ,R)) corresponds to

[formula]

where [formula] is defined as the inverse of the leading block of S. Recall that S is assumed to have the block triangular structure [\eqref=eq:Sstruct0], i.e.,

[formula]

The output is a finite-dimensional representation of the factorization

[formula]

where the block function Fkmax + 1 is given

[formula]

and Fkmax is the first kmax columns of Fkmax + 1.

Gram-Schmidt orthogonalization

Computing the scalar product for structured functions

For structured functions, i.e., functions of the form [\eqref=eq:phistruct], the consistent extension of the definition [\eqref=eq:scalarprod_def0] is the following. Let

[formula]

and

[formula]

Then,

[formula]

In practice, we can compute the scalar product by truncating the infinite sum and exploiting the structure of the sum.

Suppose the two functions [formula] and [formula] are given by [\eqref=eq:varphi_sp] and [\eqref=eq:psi_sp]. Then

[formula]

with

[formula]

provides an approximation to accuracy

[formula]

By comparing the infinite sum [\eqref=eq:scalarprod_def] with [\eqref=eq:scalarprod_approx], we can solve for εNmax and bound the modulus,

[formula]

The sum in the right-hand side can be interpreted as the remainder term in the Taylor approximation of [formula]. The bound [\eqref=eq:vepsapprox] follows by applying Taylor's theorem.

The lemma above has some properties important from a computational perspective:

The sum in [\eqref=eq:Wdef] involves only matrices of size p  ×  p, i.e., it does not involve very large matrices, under the condition that YHY is precomputed.

The matrix WN,M defined by [\eqref=eq:Wdef] is constant if S and Y are constant. Hence, in combination with Algorithm [\ref=alg:infarnoldi] it only needs to be computed once in order to construct the Arnoldi factorization.

An appropriate value of Nmax such that εNmax is smaller than or comparable to machine precision can be computed from [formula] by increasing Nmax until the right-hand side of [\eqref=eq:vepsapprox] is sufficiently small.

Computing the Gram-Schmidt orthogonalization for structured functions

One step of the Gram-Schmidt orthogonalization process can be seen as a way of computing the orthogonal complement, followed by normalizing the result. When working with matrices, the process is compactly expressed as follows. Consider an orthogonal matrix [formula]. The orthogonal complement of a vector [formula], with respect to the space spanned by the columns of X and the Euclidean scalar product is given by,

[formula]

where

[formula]

In the setting of Arnoldi's method, the orthogonalization coefficients h and the norm of the orthogonal complement β needs to be returned to the Arnoldi algorithm.

Due to the fact that the considered scalar product [\eqref=eq:scalarprod_def] is the Euclidean scalar product on the Taylor coefficients, we can, similar to [\eqref=eq:uorth] and [\eqref=eq:horth], compute the orthogonal complement using matrices. The corresponding operations for our setting are presented in the following theorem.

Let [formula], [formula], [formula], [formula] be the matrices representing the block function [formula],

[formula]

where the columns are orthonormal with respect to <    ·  ,  ·    >   defined by [\eqref=eq:scalarprod_def]. Consider the function φ, represented by [formula] and [formula] and defined by

[formula]

and let [formula],

[formula]

Then, the function [formula], represented by the vectors

[formula]

and defined by

[formula]

is the orthogonal complement of φ with respect to the space span by the the columns of F and the scalar product <    ·  ,  ·    >   defined by [\eqref=eq:scalarprod_def].

The construction is such that [formula] is a linear combination of φ and the columns of F (due to linearity in coefficients c and x). Remains to check that [formula] is orthogonal to columns of F.

The Gram-Schmidt process with reorthogonalization can hence be efficiently implemented with operations on matrices and vectors. This is presented in Algorithm [\ref=alg:taylorgm], where we used iterative reorthogonalization [\cite=Bjorck:1994:GM] with (as usual) at most two steps. In the numerical simulations we used REORTH_TOL[formula].

Extracting and restarting

Recall the general outline described in Section [\ref=sect:linearprobs] and that we have now (in the Section [\ref=sect:lockedinfarn]) described the first step in detail. In what follows we discuss the second step. We propose a procedure to carry out some operations of the result of the first step, i.e., Algorithm [\ref=alg:infarnoldi], and restart it such that we expect that the outer iteration eventually converges to a partial Schur factorization.

Manipulations of the Arnoldi factorization

First recall that Algorithm [\ref=alg:infarnoldi] is an Arnoldi method in a function setting and the output corresponds to an Arnoldi factorization,

[formula]

where, the block function Fk + 1 is given by the output of Algorithm [\ref=alg:infarnoldi] with the defintion

[formula]

and Fk is the first k columns of Fk + 1. To ease the notation, we have denoted k = kmax.

Although the Arnoldi factorization [\eqref=eq:arnfact2] is a function relation, we will now see that several parts of the steps for implicit restarting (cf. [\cite=Morgan:1996:RESTARTING] [\cite=Lehoucq:2001:IRAM] [\cite=Lehoucq:1996:DEFLATION] [\cite=Stewart:2001:KRYLOVSCHUR]) for Arnoldi's method (for linear matrix eigenvalue problems) can be carried out in a similar way by working with functions.

We will start by computing an ordered Schur factorization of Hk

[formula]

where [formula], [formula] and [formula] are upper triangular matrices. The ordering is such that the eigenvalues of R11 are very accurate (and from now on called the locked Ritz values), the eigenvalues of R22 are wanted eigenvalues (selected according to some criteria) which have not converged, and the eigenvalues of R33 are unwanted.

Hence,

[formula]

Note that a1 is a measure of the (unstructured) backward error of the corresponding eigenvalues of R11 and [formula] is often used as stopping criteria. Hence, [formula] will be zero if the eigenvalues of R11 are exact and will in general be small (or very small) relative to k since the eigenvalues of R11 are very accurate solutions. By successive application of Householder reflections (see e.g. [\cite=Meerbergen:2008:QUADARNOLDI]) we can now construct an orthogonal matrix P2 such that

[formula]

where

[formula]

is a Hessenberg matrix.

By considering the leading two blocks and columns of [\eqref=eq:orderedschur2] and the result of the Householder reflection transformation [\eqref=eq:backhessenberg] we find that

[formula]

These operations yield a transformation of the Arnoldi factorization where the first block is triangular (to order [formula]). We reach the following result, which is an Arnoldi factorization similar to [\eqref=eq:arnfact2] but only of length p. Moreover, the Hessenberg matrix does not contain the unwanted eigenvalues and has a leading block which is almost triangular.

Consider an Arnoldi factorization given by [\eqref=eq:arnfact2] and let Fk + 1(θ) = (Fk(θ),f(θ)). Let Q1 and Q2 represent the leading blocks in the ordered Schur decomposition [\eqref=eq:orderedschur] and let P2, R11 and [formula] be the result of the Householder reflections in [\eqref=eq:backhessenberg2]. Moreover, let

[formula]

Then, Gp + 1 approximately satisfies the length p < k Arnoldi factorization

[formula]

Extraction and imposing structure

Restarting in standard IRAM for matrices essentially consists of assigning the algorithmic state of the Arnoldi method to that corresponding to the factorization in Theorem [\ref=thm:arnfactp]. The direct adaption of this procedure is not suitable in our setting due to a growth of the polynomial part of the structured functions. This can be seen as follows. Suppose we start Algorithm [\ref=alg:arnoldiBBB] with a constant function (as done in [\cite=Jarlebring:2010:TRINFARNOLDI]) and carry out the construction of Gp as in Theorem [\ref=thm:arnfactp]. Then, Gp + 1 will be a matrix with polynomials of degree k. We hence need to start with a state consisting of polynomials of degree k. The degree of the polynomial will grow with each restart and after M restarts, the polynomials will be of degree Mk. The representation of this polynomial will hence quickly limit the efficiency of the restarting scheme.

Instead of restarting with polynomials we will perform an explicit restart using Algorithm [\ref=alg:infarnoldi] with a particular choice of the input which we here denote Ŷ, Ŝ, [formula]. This choice is inspired by the factorization in Theorem [\ref=thm:arnfactp].

We will first impose exponential structure on Gp in the sense that we consider a function Ĝp, with the property

[formula]

and defined by

[formula]

where

[formula]

Note that we can express Gp(0) explicitly from [\eqref=eq:Gpdef] as

[formula]

where Vk + 1,1 is the upper n  ×  (k + 1)-block of Vk.

Assume for the moment that [formula]. Then, the first pl columns of [\eqref=eq:arnfactp] correspond to the definition of an invariant pair (Ψ,R), where Ψ(θ) = Gpl(θ) and R = R11. From Theorem [\ref=thm:invpairs] we know that Ψ is of exponential structure, and imposing the structure as in [\eqref=eq:impstruct] does not modify the function, i.e., if [formula], then Ĝpl(θ) = Gpl(θ). Hence, the first pl columns of the equation [\eqref=eq:arnfactp] are preserved also if we replace Gp(θ) with Ĝp(θ). Due to the fact that [formula] is small (or very small) we expect that imposing the structure as in [\eqref=eq:impstruct] gives an approximation of the pl columns of [\eqref=eq:arnfactp], i.e.,

[formula]

if [formula] is small and equality is achieved if [formula].

With the above reasoning we have a justification to use the first pl columns of [\eqref=eq:impstruct], i.e.,

[formula]

in the initial state for the restart. In the approximation of the p - pl last columns of Gp by the p - pl last columns of [\eqref=eq:impstruct], the Arnoldi relation in the function setting is in general lost, because Ritz functions only have exponential structure upon convergence. Therefore, we will only use the (pl + 1)st column in the restart, from which the Krylov space will be extended again in the next inner iteration This leads us to a restart with the function

[formula]

which corresponds to setting  = epl + 1 and initial function

[formula]

By these modifications of the factorization [\eqref=eq:arnfactp] we have now reached a choice of Ŷ given by [\eqref=eq:setY], Ŝ given by [\eqref=eq:setS] and  = epl + 1. This choice of variables satisfy all the properties necessary for the input of Algorithm [\ref=alg:infarnoldi], except the orthogonality condition. The first columns of Ĝpl are automatically orthogonal (at least if [formula]). The (pl + 1)st column will however in general not be orthogonal to Ĝpl, which is an assumption needed for Algorithm [\ref=alg:infarnoldi]. It is fortunately here easily remedied by orthogonalizing the function corresponding to  = epl + 1 using the function gram_schmidt, i.e., Algorithm [\ref=alg:taylorgm].

The details of this selection as well as the manipulations in Section [\ref=sect:manips] are summarized in the outer iteration Algorithm [\ref=alg:exprestart].

Note that a restart which is theoretically very similar to what we have here proposed, can be achieved by starting the infinite Arnoldi method with the function of the first column of [\eqref=eq:impstruct], without taking the "locked part" of the factorization directly into account. Such an explicit restarting technique (without locking) does unfortunately have unfavorable numerical properties and will not be persued here. From reasoning similar to [\cite=Morgan:1996:RESTARTING] we know that the first column of [\eqref=eq:impstruct] is an approximation of an element of an invariant subspace and the first pl steps of Arnoldi's method started with this vector is expected to recompute the pl converged Ritz vectors after pl iterations. In the (pl + 1)st iteration, the Arnoldi vector is corrupted due to cancellation.

Examples

A small example of Hadeler

The nonlinear eigenvalue problem presented in [\cite=Hadeler:1967:MEHRPARAMETERIGE], which is available with the name hadeler in the problem collection [\cite=Betcke:2010:NLEVPCOLL], is given by

[formula]

where [formula], [formula] with n = 8 and μ is a shift which we will use to select a point close to which we will find the eigenvalues.

In order to apply Algorithm [\ref=alg:infarnoldi] we need to derive a formula for x+ ,0 in [\eqref=eq:y0]. The derivatives for M are straightforward to compute and we compute [formula], using [\eqref=eq:MMNdef] and [\eqref=eq:MMNdef2]. More precisely, we use the following computational expressions,

[formula]

In the last formula, imax is chosen such that the expression has converged to machine precision. Since this is not computationally expensive, we can roughly overestimate imax. In this example it was sufficient to take imax = 40.

In the outer algorithm (Algorithm [\ref=alg:exprestart]) we classified a Ritz value as converged (locked) when the absolute residual was smaller than 1000  ×  εmach. We selected the largest eigenvalues of Hk as the wanted eigenvalues.

The convergence is illustrated for two runs in Figure [\ref=fig:combined_hadeler2] and Figure [\ref=fig:combined_hadeler3]. In order to illustrate the similarity with implicit restarting in [\cite=Sorensen:1992:IMPLICIT], we also carried out the infinite Arnoldi method with true implicit restarting by restarting only with polynomials, instead of using Algorithm [\ref=alg:exprestart]. We clearly see that at least in the beginning of the iteration, the convergence of Algorithm [\ref=alg:exprestart] is similar to the convergence of IRAM. Note that IRAM in this setting exhibits a growth of the basis matrix and it is hence considerably slower. We show the number of locked Ritz-values Table [\ref=tbl:indicators]. Moreover, we quantify the impact of the procedure to impose the structure in the restart by inspecting the approximation in [\eqref=eq:lockedfact]. We define γ as the norm of the difference of the left and right-hand side of [\eqref=eq:lockedfact]. Lemma [\ref=thm:structinvres] shows that this difference is independent of θ and provides a computable expression. More precisely,

[formula]

where we used that Ĝpl has the structure Ĝpl(θ) = Y exp (θR- 111). The values of γ are also given in Table [\ref=tbl:indicators]. They are, as expected, of the same order of magnitude as the locking tolerance.

A large-scale square-root example

We considered the same example as in [\cite=Jarlebring:2010:TRINFARNOLDI], which is the problem called gun in the problem collection [\cite=Betcke:2010:NLEVPCOLL] and stems from [\cite=Liao:2006:SOLVING]. It is currently the largest example, among those examples in the collection [\cite=Betcke:2010:NLEVPCOLL] which are neither polynomial eigenvalue problems nor rational eigenvalue problems.

In order to focus on a particular region in the complex plane we introduce (as in [\cite=Jarlebring:2010:TRINFARNOLDI]) a shift μ and a scaling γ, for which the nonlinear eigenvalue problem is

[formula]

where σ1 = 0 and σ2 = 108.8774 and ι2 =  - 1. We selected γ = 3002 - 2002 and μ = 2502 since this transforms the region of interest to be essentially within the unit circle.

In order to compute a formula for x+ ,0 in [\eqref=eq:y0], we need in particular

[formula]

where [formula] denotes the matrix square root (principal branch).

We will partially base the formulas on the Taylor coefficients of the square root in order to compute [formula] (needed in the computation of x0, + in [\eqref=eq:y0]). We will use

[formula]

where

This can be used to compute of [formula], as follows, Note that the sums in [\eqref=eq:sqrtMMNp] are operations with vectors of relatively small dimension and can be computed efficiently. We selected the number of terms imax adaptively such that [formula].

This results in the following formulas which we used for the computation of x+ ,0

[formula]

[formula]

and for N > 1,

[formula]

The matrix M(0) was factorized (with an LU-factorization) before starting the iteration, such that M(0)- 1b could be computed efficiently.

We first wish to illustrate that the restarting and structure exploitation can considerably reduce both memory and CPU usage. In Table [\ref=tbl:sqrt_resources] we compare runs for the standard version of the infinite Arnoldi method [\cite=Jarlebring:2010:TRINFARNOLDI] (first column) with the restarting algorithm (Algorithm [\ref=alg:exprestart]) for two choices of the parameter kmax. The iteration was terminated when p = 10 eigenvalues were found. We clearly see that for the choices of kmax there is a considerable reduction in memory and some reduction in computation time.

In Figure [\ref=fig:sqrt_p9] and Figure [\ref=fig:sqrt_p14] we illustrate that the algorithm scales reasonably well with p, i.e., the number of wanted eigenvalues. When we increase p, we need more outer iterations, but eventually the algorithm usually converges for reasonably large p.

Concluding remarks

We have in this work shown how the partial Schur factorization of an operator B can be computed using a variation of the procedures to compute partial Schur factorization for matrices. Several variations of the results for matrices appear to be possible to adapt. Concepts like thick restarting, purging and other selection strategies, appear to carry over but deserve further attention. We also wish to point that many of the results allow to be adapted or used in other ways. In this paper we also presented a Taylor-like scalar product, which could, essentially be replaced by any suitable scalar product.

A technical lemma

Consider [formula] and [formula], where S is invertible. Let F(θ): = Y exp (θS). Then,

[formula]

We prove the theorem by showing that the derivative of the function relation [\eqref=eq:structinvres] holds for any θ and that the relation holds in one point θ = 0. Note that the right-hand side of [\eqref=eq:structinvres] is constant (with respect to θ) and the derivative of the left-hand side reduces to

[formula]

by definition of B and differentiation of exp (θS).

From the definition of B and evaluation of the left-hand side of [\eqref=eq:structinvres] at θ = 0 we have,

[formula]

Note that for an analytic scalar function [formula],

[formula]

Hence,

[formula]

Moreover, by using the relation between bi and fi and Mi and Bi given by [\eqref=eq:BiMi] we have,

[formula]

The proof is completed by cancelling the term YS- 1 when inserting [\eqref=eq:BYbk] into [\eqref=eq:BBBF0].