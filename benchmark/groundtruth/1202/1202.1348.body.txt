=6 =6

Selecting Two-Bit Bit Flipping Algorithms for Collective Error Correction

Introduction

With the introduction of high speed applications such as flash memory, fiber and free-space optical communications comes the need for fast and low-complexity error control coding. Message passing algorithms for decoding low-density parity-check (LDPC) codes such as the sum-product algorithm (SPA) offer very attractive error performance, especially for codes with column-weight [formula]. However, the complexity of these algorithms is still high and the decoding speed is limited, mostly due the fact that the operations at variable and check nodes must be carried out for every edge in the Tanner graph. For regular column-weight-three LDPC codes, which allow lower complexity implementation, message passing algorithms (as well as other classes of decoding algorithms) usually suffer from high error floor. This weakness of message passing algorithms in regular column-weight-three LDPC codes justifies the search for alternatives which offer better trade-offs between complexity, decoding speed and error performance.

Among existing decoding algorithms for LDPC codes on the binary symmetric channel (BSC), bit flipping algorithms are the fastest and least complex. The check node operations of these algorithms are modulo-two additions while the variable node operations are simple comparisons. The simplicity of these algorithms also makes them amenable to analysis. Many important and interesting results on the error correction capability of the serial and parallel bit flipping algorithms have been derived (see [\cite=NMV_11_ISIT] for a list of references). Unfortunately, their error performance is typically inferior. As a result, bit-flipping-oriented algorithms have been largely considered to be impractical, even after the introduction of some improved versions, such as the one in [\cite=miladinovic].

In [\cite=NMV_11_ISIT], a class of bit flipping algorithms that employ two bits for decoding LDPC codes over the BSC was proposed. Compared to serial and parallel bit flipping, a two-bit bit flipping (TBF) algorithm employs one additional bit at a variable node and one at a check node. The additional bits introduce memory in the decoding process, which slows down the decoding when necessary. Initial results showed that decoders which employ a group of these algorithms operating in parallel lower the error floor while maintaining low complexity. However, in [\cite=NMV_11_ISIT] we have not given a complete failure analysis of these algorithms, nor have we established the methodology to derive good algorithms and/or a collection of mutually good algorithms.

In this paper, we provide complete failure analysis for TBF algorithms. More importantly, we give a rigorous procedure to select groups of algorithms based on their complementariness in correcting different error patterns. Decoders that employ algorithms selected using this procedure have provably good error performance and, by the nature of bit flipping, high speed. As one can expect, a TBF algorithm (like other sub-optimal graph-decoding algorithms) fails on some low-weight error patterns due to the presence of certain small subgraphs in the Tanner graph. In this paper, we characterize a special class of these subgraphs and refer to them with the common term "trapping sets." Our definition of a trapping set for a given algorithm readily gives a sufficient condition for successful decoding. The set of all possible trapping sets of a given decoding algorithm constitutes the algorithm's trapping set profile. A unique property of trapping sets for TBF algorithms is that a trapping set profile may be obtained by a recursive procedure. The diversity among trapping set profiles of different algorithms allows us to select groups of algorithms such that they can collectively correct error patterns that are uncorrectable by individual algorithms.

The rest of the paper is organized as follows. Section [\ref=sect_pre] gives the necessary background. Section [\ref=sect_mot] gives motivation. In Section [\ref=sect_analysis], we define trapping sets, trapping set profiles and describe the recursive procedure for constructing a trapping set profile. Section [\ref=sect_select] discusses the process of selecting algorithms. Numerical results are given in Section [\ref=sect_sim].

Preliminaries

Let C denote an (n,k) binary LDPC code. C is defined by the null space of H, an m  ×  n parity-check matrix of C. H is the bi-adjacency matrix of G, a Tanner graph representation of C. G is a bipartite graph with two sets of nodes: n variable (bit) nodes [formula] and m check nodes [formula]; and a set of edges E(G). A ([formula])-regular LDPC code has a Tanner graph G in which all variable nodes have degree [formula] and all check nodes have degree [formula]. In this paper, we only consider ([formula])-regular LDPC codes. A subgraph of a bipartite graph G is a bipartite graph U such that V(U)  ⊂  V(G), C(U)  ⊂  C(G) and E(U)  ⊂  E(G). G is said to contain U. Furthermore, if Y is a graph which is isomorphic to U then G is also said to contain Y. In a bipartite graph G, the induced subgraph on a set of variable nodes [formula] is a bipartite graph U with [formula], [formula] and [formula].

A vector [formula] is a codeword if and only if [formula], where [formula] is the transpose of H. Assume the transmission of the all-zero codeword over the BSC. Denote by [formula] the channel output vector and denote by [formula] the decision vector after the lth iteration of the iterative algorithm, where l is a positive integer. At the end of the lth iteration, a variable node v is said to be corrupt if x̂lv  =  1, otherwise it is correct. For the sake of convenience, we let [formula]. A variable node v with x̂0v  =  1 is initially corrupt, otherwise it is initially correct. Let [formula] denote the syndrome vector of the decision vector after the lth iteration, i.e., [formula]. A check node c is said to be satisfied at the beginning of the lth iteration if sl - 1c  =  0, otherwise it is unsatisfied. TBF algorithms are defined as follows.

The class [formula] of TBF algorithms is given in Algorithm [\ref=algo_tbfa], where [formula] gives the states of the check nodes at the beginning of the lth iteration while [formula] gives the states of the variable nodes at the end of the lth iteration. A variable node v takes its state from the set [formula], i.e., it can be strong zero, weak zero, weak one or strong one. A check node takes its state from the set [formula], i.e., it can be previously satisfied, newly satisfied, previously unsatisfied or newly unsatisfied. The state w0v of a variable node v is initialized to [formula] if yv = 0 and to [formula] if yv = 1. The state z1c of a check node c is initialized to [formula] if s0c = 0 and to [formula] otherwise. A TBF algorithm [formula] iteratively updates [formula] and [formula] until all check nodes are satisfied or until a maximum number of iteration [formula] is reached. The check node update function [formula] is defined as follows: [formula] and [formula]. The variable node update is specified by a function [formula], where [formula] is the set of all ordered 4-tuples [formula] such that [formula] and [formula]. [formula] and [formula] give the number of check nodes with states [formula] and [formula], respectively, that are connected to v. The function f must be symmetric with respect to 0 and 1 and must allow every state of a variable node to be reachable from any other state.

What makes a TBF algorithm novel is that a variable node has "strength" and a check node's reliability is evaluated based on its state in the previous iteration.

Motivation

Consider a collection [formula] of iterative decoding algorithms for LDPC codes. Let us assume for a moment that the set of all uncorrectable error patterns for each and every algorithm in [formula] is known. More precisely, in the context of LDPC codes, we assume that the induced subgraphs on such error patterns can be enumerated for each decoding algorithm. This naturally suggests the use of a decoder D which employs multiple algorithms drawn from [formula]. The basis for this use of multiple algorithms is rather simple: If different algorithms are capable of correcting different error patterns, then a decoder employing a set of properly selected algorithms can achieve provably better error performance than any single-algorithm decoder. Disappointingly, the above hypothetical assumption is not valid for most iterative algorithms. For message passing algorithms such as the SPA, there is no simple criterion to verify weather or not an arbitrary error pattern is correctable, much less an explicit methodology to design a decoder which employs multiple algorithms in a collaborative manner.

Interestingly, for TBF algorithms, we are able to establish a framework to analyze and enumerate all uncorrectable error patterns, and this is the main contribution of this paper. In particular, we characterize the decoding failures of TBF algorithms by redefining trapping sets and introducing the definition of trapping set profiles. It is an important property of the newly defined trapping sets that enable us to enumerate them using a recursive procedure. We remark that the enumeration of trapping sets is code independent. More importantly, the concept and explicit construction of trapping set profiles allow rigorous selections of multiple algorithms which can collectively correct a fixed number of errors with high probability. Given that the selection of multiple algorithms would become straightforward once the trapping sets/trapping set profiles have been defined and constructed, we devote a considerable portion of the paper to introducing these two objects. We also focus on giving criteria for selecting algorithms rather than explicitly describing the selection process.

Trapping Sets and Trapping Set Profiles

Trapping Sets of TBF Algorithms

Although the term trapping set was originally defined as a set of variable nodes that are not eventually correctable by an iterative decoding algorithm [\cite=errorFloor_richarson], in the literature it has been used more frequently to refer to a combinatorially defined subgraph that may be harmful to decoding. The justification for this less rigorous use of terminology is that the variable node set of a so-called trapping set (a subgraph) would be an actual set of non-eventually-correctable variable nodes if the parallel bit flipping algorithm were used (see [\cite=NCMV_11_IT] for details). Examples of such trapping sets are fixed sets [\cite=NCMV_11_IT] and absorbing sets [\cite=absorbingSet_dolecek]. For TBF algorithms, failure analysis can no longer solely rely on these combinatorial objects. For certain TBF algorithms, the smallest subgraphs that cause decoding failures are neither absorbing sets nor fixed sets. We therefore (re)define the notion of a trapping set for TBF algorithms, as we now explain. We first introduce the following definition on failures of a TBF algorithm.

Consider a TBF algorithm F and a Tanner graph G. Let [formula] denote the set of variable nodes that are initially corrupt and let I denote the induced subgraph on [formula]. If the algorithm F does not converge on G after [formula] iterations, then we say that F fails on the subgraph I of G.

It can be seen that the decoding failure of F is defined with the knowledge of the induced subgraph on the set of initially corrupt variable nodes. To characterize failures of F, a collection of all induced subgraphs I must be enumerated. While this is difficult in general, for practically important cases of small numbers of initial errors (less than 8) and small column-weight codes ([formula] or 4), the enumeration of such induced subgraphs is tractable.

Consider a given Tanner graph I. Let [formula] denote a set of Tanner graphs containing a subgraph J isomorphic to I such that F fails on J. Since [formula] is undeniably too general to be useful, we focus our attention on a subset [formula] of [formula], described as follows.

Consider a Tanner graph [formula] such that F fails on the subgraph J1 of S1. Then, [formula] if there does not exist [formula] such that:

F fails on the subgraph J2 of S2, and

there is an isomorphism between S2 and a proper subgraph of S1 under which the variable node set V(J2) is mapped into the variable node set V(J1).

Now we are ready to define trapping sets and trapping set profiles of a TBF algorithm.

If [formula] then S is a trapping set of F. I is called an inducing set of S. [formula] is called the trapping set profile with inducing set I of F.

The following proposition states an important property of a trapping set.

Let S be a trapping set of F with inducing set I. Then, there exists at least one induced subgraph J of S which satisfies the following properties:

J is isomorphic to I, and

F fails on J of S, and

Consider the decoding of F on S with V(J) being the set of initially corrupt variable nodes. Then, for any variable node v∈V(S), there exist an integer [formula] such that [formula].

The proof is omitted due to page limits.

From Proposition [\ref=allCProp], one can see that the trapping set profile [formula] of F contains the graphs that are most "compact." We consider these graphs most compact because for at least one J isomorphic to I, the decoding of F on such a graph with V(J) being the set of initially corrupt variable nodes could be made successful by removing any variable node of the graph. This special property of trapping sets is the basis for an explicit recursive procedure to obtain all trapping sets up to a certain size, which compensates for the lack of a fully combinatorial characterization of trapping sets. We remark that for certain reasonably good algorithms, the necessary condition for a Tanner graph to be a trapping set can be easily derived. Before describing the recursive procedure for constructing trapping set profiles, we state the following proposition, which gives a sufficient condition for the convergence of an algorithm F on a Tanner graph G.

Consider decoding with an algorithm F on a Tanner graph G. Let [formula] be the set of initially corrupt variable nodes and I be the induced subgraph on [formula]. Then, algorithm F will converge after at most [formula] decoding iterations if there does not exist a subset [formula] of V(G) such that [formula] and the induced subgraph on [formula] is isomorphic to a graph in [formula].

Follows from the definition of [formula]. Remark: Proposition [\ref=profF] only gives a sufficient condition because the existence of [formula] which satisfies the above-mentioned conditions does not necessarily indicate that [formula].

Constructing a Trapping Set Profile

The recursive procedure for constructing a trapping set profile [formula] relies on Proposition [\ref=allCProp]. Let us assume that we are only interested in trapping sets with at most [formula] variable nodes. Consider the decoding of F on a Tanner graph I with V(I) being the set of initially corrupt variable nodes. Let nI  =  |V(I)|. If F fails on the subgraph I of I then [formula] and we have found the trapping set profile. If F does not fail on the subgraph I of I, then we expand I by recursively adding variable nodes to I until a trapping set is found. During this process, we only add variable nodes that become corrupt at the end of a certain iteration.

Consider all possible bipartite graphs obtained by adding one variable node, namely vnI + 1, to the graph I such that when the decoding is performed on these graphs with V(I) being the set of initially corrupt variable nodes, the newly added variable node is a corrupt variable node at the end of the first iteration, i.e., [formula]. Let [formula] denote the set of such graphs. Take one graph in [formula] and denote it by U. Then, there can be two different scenarios in this step. First, F does not fail on the subgraph I of U. In this case, U is certainly not a trapping set and we put U in a set of Tanner graphs denoted by [formula]. Second, F fails on the subgraph I of U. In this case, U can be a trapping set and a test is carried out to determine if U is indeed one. If U is not a trapping set then it is discarded. We complete the formation of [formula] by repeating the above step for all other graphs in [formula].

Let us now consider a graph [formula]. Again, we denote by [formula] the set of Tanner graphs obtained by adding one variable node, namely vnI + 2, to the graph U such that when the decoding is performed on these graphs with V(I) being the set of initially corrupt variable nodes, the newly added variable node is a corrupt variable node at the end of the first iteration, i.e., [formula]. It is important to note that the addition of variable node vnI + 2, which is initially correct, cannot change the fact that variable node vnI + 1 is also corrupt at the end of the first iteration. This is because the addition of correct variable nodes to a graph does not change the states of the existing check nodes and the decoding dynamic until the newly added variable nodes get corrupted. Similar to what have been discussed before, we now take a graph in [formula] and determine if it is a trapping set, or it is to be discarded, or it is a member of the set of Tanner graph [formula]. By repeating this step for all other graphs in [formula], all graphs in [formula] can be enumerated. In a similar fashion, we obtain [formula]. For the sake of convenience, we also let [formula].

At this stage, we have considered one decoding iteration on I. It can be seen that if S is a trapping set with at most [formula] variable nodes then either S has been found, or S must contain a graph in [formula]. Therefore, we proceed by expanding graphs in [formula].

Let K denote a Tanner graph in [formula]. We now repeat the above graph expanding process with K being the input. Specifically, we first obtain [formula], which is defined as the set of all Tanner graphs obtained by adding one variable node vnK + 1 to the graph K such that when decoding is performed on these graphs with V(I) being the set of initially corrupt variable nodes, the newly added variable node is a corrupt variable node at the end of the second iteration, but not a corrupt variable node at the end of the first iteration, i.e., [formula] and [formula]. Graphs in [formula] that are not trapping sets are either discarded or to form the set [formula]. By recursively adding variable nodes, graphs in [formula] are enumerated.

One can see that there are two recursive algorithms. The first algorithm enumerates graphs in [formula] for a given graph K by recursively adding variable nodes. The second algorithm recursively calls the first algorithm to enumerates graphs in [formula] for each graph K in [formula]. Each recursion of the second algorithm corresponds to a decoding algorithm. As a result, the trapping set profile is obtained after [formula] recursions of the second algorithm.

Selecting TBF Algorithms

Due to page limits, we only summarize the most important criteria for selecting TBF algorithms. Let us first briefly discuss the number of possible algorithms.

On the Number of Algorithms

Let Q be the set of all functions from [formula] that satisfy the symmetry and the irreducibility condition. Due the symmetry condition, [formula]. There are two possible values of [formula], and two possible values of [formula]. However, with a given [formula], the two sets of algorithms F that correspond to two possible [formula] are identical (as [formula] and [formula], [formula] and [formula] can be interchanged). Consequently, if we disregard the maximum number of iterations, then [formula]. One can easily show that [formula]. Therefore, an upper-bound on the number of TBF algorithms is:

[formula]

For example, this upper-bound is 281 when [formula], and is 2141 when [formula]. Due to the huge number of possible algorithms, it is necessary to focus on a small subset of algorithms. This subset of algorithms may be obtained by imposing certain constraints on the function f. One example of such a constraint is as follows: if [formula] then [formula]. This constraint requires that when a strong zero variable node is flipped with a given combination of check nodes, a weak variable node is also flipped with the same check node combination. Other constraints on f are derived by analyzing possible transitions of variable nodes and check nodes for a small number of iterations.

Selecting a TBF Algorithm

We first discuss the main criterion to select one algorithm among all possible algorithms. Let [formula] be the smallest number of variable nodes of Tanner graphs in [formula]. We would like to select an algorithm F such that [formula] is maximized. The justification for this selection criterion relies on the following proposition, whose proof is omitted due to page limits.

Given three random Tanner graph G,S1,S2 with 0 < |V(S1)| < |V(S2)| < |V(G)|, the probability that G contains S2 is less than the probability that G contains S1.

From Proposition [\ref=profG], one can see that the larger the number |V(S)| of a given Tanner graph S is, the easier it would be (if at all possible) to construct a Tanner graph G that does not contain S. Therefore, a larger [formula] means that the sufficient condition for the convergence of F can be met with higher probability. In this sense, an algorithm F with a larger [formula] is more favorable.

If for two algorithms F1 and F2, [formula] = [formula], then one can derive other comparison criteria based on [formula] and [formula], and/or compare F1 and F2 with a different assumption of I. For example, the probability of a graph G containing a trapping set S can be also be evaluated based on |C(S)|.

Selecting Multiple TBF Algorithms

We now consider the problem of selecting of multiple algorithms. The basis for this selection is that one should select good individual algorithms with diverse trapping set profiles. In this paper, we only consider decoder D with algorithms [formula] operating in parallel, i.e., the received vector of the channel is the input vector for all algorithms. Note that one can also use trapping set profiles to select algorithms that operate in serial, i.e., the output from one algorithm is the input to another. For a decoder D that employs parallel algorithms, the concept of trapping sets and trapping set profiles can be defined in the same manner as trapping sets and trapping set profiles for a single TBF algorithm. One can easily modify the recursive procedures given in Section [\ref=sect_algo] to generate trapping set profiles of the decoder D. Then, D can be designed with the same criterion discussed in the previous subsection.

Remark: Knowledge on the Tanner graph of a code C can be used in the selection of algorithms. For example, if it is known that the Tanner graph of C does not contain a certain subgraph Y, then all graphs containing Y must be removed from a trapping set profile.

Numerical Results

As an example, we describe a selection of TBF algorithms for regular column-weight-three LDPC codes with girth g = 8. For simplicity, we let [formula], [formula] and [formula] for all algorithms. By imposing certain constraints on the functions f, we obtain a set of 21,962,496 TBF algorithms. Out of these, there are 360,162 algorithms which can correct any weight-three error pattern. Such an algorithm is capable of correcting any weight-three error pattern because its trapping set profile [formula] with any inducing set I containing three variable nodes is empty. Since all weight-three error patterns can be corrected with a single algorithm, our next step is to select a collection of algorithms which can collectively correct weight-four and -five error patterns with high probability. To achieve this goal, we construct all trapping set profiles with inducing sets containing four and five variable nodes for each algorithm. Note that there are 10 possible inducing sets (Tanner graphs with girth g = 8) containing four variable nodes and 24 possible inducing sets containing five variable nodes. Hence, for each algorithm, we construct a total of 34 trapping set profiles. From the trapping set profiles of all algorithms, we select a collection of 35 algorithms based on the criterion mentioned in the previous section. Then, we simulate the performance of a decoder D which employs these algorithms in parallel. The maximum total number of iterations of D is 35  ×  30  =  1050.

Figure [\ref=fig_fer1] shows the frame error rate (FER) performance of D on the (155,64) Tanner code. This code has [formula], [formula] and minimum distance [formula]. For comparison, the FER performance of the SPA with a maximum of 100 iterations is also included. It can be seen that the FER performance of D approach (and might surpasses) that of the SPA in the error floor region. It is also important to note that if we eliminate all trapping sets containing subgraphs that are not present in the Tanner graph of this code, then all the obtained trapping set profiles are empty. This indicates that D can correct any error pattern up to weight 5 in the Tanner code.

Figure [\ref=fig_fer1] also shows the FER performance of D on a quasi-cyclic code C732 of length n = 732, rate R = 0.75 and minimum distance [formula]. The FER performance of the SPA is also included for comparison. It can be seen that the slope of the FER curve of D in the error floor region is higher than that of the SPA. Finally, we remark that the slope of the FER curve of D in the error floor region is between 5 and 6, which indicates that D can correct error patterns of weight 4 and 5 with high probability. This also agrees with the fact that in our simulation, no weight-four error pattern that leads to decoding failure of D was observed.

We remark that the implementation of TBF algorithms operating in parallel can be done with a relatively small number of common logic gates. For example, if a decoder D employs both the TBFA1 and the TBFA2 given in [\cite=NMV_11_ISIT], then the implementation of the variable node updates require less than 800 AND-gate inputs and 100 OR-gate inputs. In comparison, the implementation of a 6-bit adder requires 2196 AND-gate inputs and 355 OR-gate inputs while that of a 6-bit comparator requires 1536 AND-gate inputs and 190 OR-gate inputs. One can also expect that the complexity introduced by an additional algorithm would decrease as the number of algorithms increases, because many min-terms in the variable node update logic functions would be already available. More details will be provided in the journal version of this paper.

Acknowledgment

This work is funded by NSF under the grants CCF-0963726, CCF-0830245.