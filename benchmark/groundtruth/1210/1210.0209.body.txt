>p

Optimal pair generation rate for Entanglement-based QKD

Introduction

In entanglement-based QKD a choice must be made between increasing the rate of pair generation to increase the key rate, or decreasing the rate of pair production to decrease the error rate [\cite=Lim:08]. The total loss an entanglement-based QKD system can tolerate, and thus the longest transmission distance attainable, is limited by the rate of pair generation as well as detector characteristics and error rate.

The photon pair production rate of other experimental set-ups have optimized using numerical simulations [\cite=Lim:08] [\cite=2008arXiv0809.0923R] [\cite=PhysRevA.81.023835] [\cite=2008arXiv0804.0891K]. In other cases, a few different pump powers are attempted before settling on the observed power with the best visibility [\cite=springerlink:10.1007/978-3-642-11731-2_14] [\cite=1367-2630-11-6-065004] [\cite=1367-2630-11-4-045013] [\cite=PhysRevLett.94.150501]. Some experiments are limited by classical communication between detectors, and so the two-fold coincidence rate is optimized for the processing of detection events [\cite=PhysRevLett.98.060503] [\cite=2005OExpr..13..202R].

Ma, Feng and Lo [\cite=PhysRevA.76.012307] found a numeric solution for optimal squeezing parameter for QKD with entanglement using SPDC sources, but it requires root finding and produces negative values for certain channel efficiencies. Although the two-fold coincidence rate can be easily measured, the squeezing parameter is unmeasurable in practice, because the channel efficiency, dark count rate, and multi-order photon terms taint the measurement. We provide a practically useful, predictive model for the optimal two-fold coincidence rates. Such a model must operate with realistic bucket detectors and dark counts, and must require as input and produce as output only variables that are easily experimentally measureable. This model will help optimize QKD systems in real-time, and provides insights into the maximum possible distance and bit rate of entanglement-based QKD. We are able to produce this model by eschewing first-principles modeling in favor of a symbolic regression approach.

Background

Quantum key distribution (QKD) uses the Heisenberg uncertainty principle to ensure secure key distribution protected from eavesdropping in an information theoretic secure manner[\cite=paterson2004quantum].

QKD involves one party (Alice) sending quantum states (e.g. polarized photons) to a second party (Bob) [\cite=bennett1984quantum].The system's security depends upon the non-orthogonal complementary bases used to measure the quantum states. If Bob, or an eavesdropper, measures in a basis other than the one Alice used to prepare her state, his measurement will be random noise. If an eavesdropper measures in the wrong basis, they will introduce detectable errors in Bob’s measurement results, allowing Alice and Bob to abort key generation before any secrets are shared.

An example implementation of QKD involves generating polarization correlated entangled photon pairs with Alice and Bob measuring a photon from each pair in one of two random bases [\cite=bennett1992quantum]. Alice and Bob measure a shared source of entangled photons in random bases. The key is formed from the results in which Alice and Bob measured in the same basis. In this paper, we call the coincidence rate the number of times per second where one of Alice's and one of Bob's detectors click within the same time window.

In the implementation described above, entangled photon states are created using two sources of pairs of polarization-correlated photons created by spontaneous parametric down-conversion (SPDC) in nonlinear material. These pairs can be made indistinguishable either by using a Sagnac interferometer loop [\cite=shi2004generation], by selecting overlapping spatial modes in Type-II SPDC processes [\cite=kwiat1995new], or by stacking two nonlinear materials at orthogonal angles (known as sandwich sources) [\cite=kwiat1999ultrabright].

While it is photon pairs that make the desired entangled states, SPDC produces unwanted higher order photon states as well. More than one photon can be detected by Alice or Bob during the same timing window. If two orthogonal detectors click, the result must be assigned to a random result for security reasons [\cite=PhysRevA.81.052342], as depicted in fig. [\ref=fig:experiment_setup]. In QKD protocols, the more errors there are in a sifted key, the more of the sifted key needs to be revealed, and thus rendered useless, in order to perform error correction and privacy amplification.

Simulation

Our simulation is written in Python using the QuTIP Quantum Toolbox in Python [\cite=johansson2012qutip] . We use a fock state representation following Jennewein et al. [\cite=jenneweinjmo:11] and the mathematical description of SPDC and bucket detectors from Kok et al. [\cite=RevModPhys.79.135]. The secure key rate is calculated using the QBER and two-fold coincidence probability in the inifinite key limit described in [\cite=PhysRevA.76.012307].

Scarani and Renner found that [formula] raw bits must be exchanged in order to get a positive key [\cite=scaranirenner08]. We define a system being usable if we have a secure key rate above 14 bits/s so that a positive key can be exchanged over the course of an hour.

Note that in order to calculate the secure key rate and two-fold coincidence rate per second, the secure key bit and two-fold coincidence probabilities must be divided by the experimentally defined coincidence window. The coincidence window is the maximum amount of time allowed to pass between Alice and Bob's detections in order for two detections to be considered a coincidence.

Results

We found the squeezing parameter ε that maximized the secure key rate using Scipy Optimize. We used this optimal ε to calculate two-fold coincidence probabilities and secure key bit probabilities for 500,000 different combinations of the detector dark count probability per coincidence window, d = {0..0.1} and the channel efficiency, η  =  {0..1}. These values and free parameters were then provided to the symbolic regression tool Eureqa [\cite=Schmidt03042009]. Symbolic regression produces a predictive model from data comprized of arbitrary algebraic functions of the input. Eureqa produced a model for two-fold coincidence probability that maximized the secure key rate:

[formula]

where A  =  0.03579 , B  =  0.23 , C  =  1.162 , D  =  2.496 , and E =  - 0.002444 , and ηa, ηb, da, db are the channel efficiencies and the background noises to Alice and Bob, respectively. In order to determine the two-fold coincidence rate that this corresponds to, this number must be divided by the coincidence window. This model is plotted in figure [\ref=fig:skr_graphs] (for the case when da  =  db  =  0).

Experimental Verification

We verified the model by comparing the simulated data and the model to experimental data. We used a Sagnac source of entangled photons and passive basis choice polarization analyzers with Si-APDs for detection [\cite=Erven:08]. Our experimental apparatus is demonstrated in fig. [\ref=fig:experiment_setup]. We use passive measurements, while our simulation uses post-processing for active measurements, using passive measurements should be the same as active measurements if a post-processing scheme where measurements with conflicting results are discarded.

In typical operation, we used band-pass filters to prevent the pump from entering the detector modules and to minimize errors due to dispersion. Typical operation also involved shielding the source from overhead light. Shielding the source reduced the dark count rate, as did introducing band-pass filters, although the latter also reduced channel efficiency.

In order to gather experimental data from a wide variety of dark count rates and channel efficiencies, we took experimental measurements under three conditions: covered polarization analyzers with neutral density filters ([formula], [formula]), uncovered polarization analyzer without neutral density filters ([formula], [formula]), and after a free space channel ([formula], [formula]). The coincidence window for the first experiment was 2.5 ns, for the second two experiments it was 3.5 ns. In order to use the equation for optimal two-folds, the dark count rate must be scaled by the coincidence window.

Removing the band-pass filters reduced the channel loss but introduced far more dark counts. Adding neutral density filters partially occluded the source's output, decreasing channel efficiency. Removing the shielding of the Sagnac source increased the number of dark counts by a factor of 10.

The pump power was varied between 0 and 50 mW in increments of 5 mW. The average coincidence rate and average QBER were then used to estimate the SKR using the asymptotic key rate. Results of these experiments are presented in fig. [\ref=fig:exp_data]. The upper limit of the two-fold coincidences is limited by the maximum power of the pump laser. Fewer data points were collected on the noisy channel due to the limitation of the memory of the timetaggers not being able to handle the coincidence rate.

It is not possible to directly measure μ with existing experimental equipment, however, by comparing the empirically observed counts of detection coincidences and the calculated estimate of SKR to the corresponding values produced by the simulation, we can empirically validate the simulation.

Discussion

From the experimental data and the numerical solution, we can estimate the optimal coincidence rate for the three channels, except for the free space channel where our laser power is insufficient to reach the coincidence rate. For the channels with experimentally determined optimal coincidence rates, the optimal determined from the numerical solution is within the margin of error for the experimental optimal. We now apply our model to investigate two QKD channels.

Application: Optimizing QKD with Satellites

We simulate loss and detector dark counts in a satellite uplink scenario [\cite=JP2012inpreparation], meaning a source on the ground with one photon going to a LEO satellite while the other is measured on the ground, over a year of continuous usage. The ground stations are located on mountains (2.4 km above the ground) and 45 km outside of Ottawa, Canada. The ground telescopes had an aperture of 25 cm and the satellite telescope had an aperture of 20 cm. A low-earth-orbit satellite has a period of 1.6 hours, and the values of loss and dark counts are constantly changing as the satellite passes from horizon to horizon in the transmitter's field of view.

Loss and detector dark counts are used to calculate the optimal two-fold coincidence rates. We then estimate the SKR using this optimal two-fold coincidence rate, given detectors with a quantum efficiency of 50% and a dark count rate of 100 c/s (figure [\ref=satellite_models]). We also calculate the pair generation rate by using bucket detectors of unit efficiency at the source. Passes are ordered by the length of visible time between the transmitter and the satellite. We compare this against the estimated SKR from the pair generation rate fixed at a power that maximizes the secure key rate over all passes. We assume a coincidence window of 0.5 ns.

Adjusting the two-fold coincidence rate dynamically does not improve the key rate for good satellite passes (an increase of 0.75% for the best link, and 2.65% during the course of the 75th percentile link, for the transmitter on 45 km from Ottawa on a mountain, the best pass both in terms of background counts and loss). The total key, additional key and percent increase for a variety of simulations and passes are presented in table [\ref=additional_key]. Adjusting the source rate gives the biggest increase for passes where the loss is worst or the background counts increase. On the best passes, the usable time of the satellite pass increases by 10 s over the course of a 250 s pass. On median pass, the usable time more than doubles. This means that with optimization, many more satellite passes which were previously infeasible due to the high loss are now usable.

Application: Optimizing QKD with Fiber Optics

We use the values reported for time resolution, detector efficiency, and detector dark counts for several types of detectors [\cite=buller2010single] to compute loss budgets. The loss budget is the largest loss, given an optimal pumping rate, for which it is possible to transmit a secure key of at least 50,000 bits [\cite=scaranirenner08] in a given time period. For each detector, we compute loss budgets for periods of one hour and the asymptotic limit of time, including loss from imperfect detectors. To obtain the maximum loss permissible for a channel using these detectors, we then subtract the detector ineffiencies from [\cite=buller2010single] off of the loss budget. Our findings are summarized in table [\ref=detector_table].

Our estimates should be taken with a caveat that they do not account for finite size effects, which increase the QBER at very low coincidence rates [\cite=scaranirenner08].

The largest loss budget for a key in an hour from table [\ref=detector_table] is 22.4 dB at 1550 nm. If we assume a continuous single mode fiber channel link, no loss from other sources (such as insertion loss), a fiber loss of 0.17 dB/km [\cite=corningsheet], and symmetric links [\cite=scheidl2009feasibility], the maximum possible distance predicted for entanglement-based QKD systems in fiber optic cables is 263.5 km. This number is calculated by dividing the loss budget by the loss per kilometer and multiplying by two for the symmetric links. In the asymptotic limit, this loss budget goes up to 34.1 dB or 401.2 km.

Visible wavelengths suffer from much higher losses in fiber, from 3 dB/km at 800 nm [\cite=meyer2010quantum], to 30 dB/km at 515 nm [\cite=thorlabss405]. Using the calculation above, this means that the furthest a visible-wavelength implementation could travel in fiber is 16.3 km for a key in an hour and 25.5 km for a key in the asymptotic limit at 800 nm. Thus, although visible light detectors have greater detection efficiency and fewer dark counts, they are less useful for long-distance fiber implementations due to the attenuation of visible light in fiber.

QKD with entanglement distribution has been implemented on bright (carrying classical data) standard single-mode telecommunications fibers. This can be done by sending the quantum information at an unused wavelength in dense wavelength devision multiplexing protocols [\cite=chapuran2009optical] [\cite=1367-2630-12-6-063027] [\cite=1367-2630-11-4-045012], or by using a wavelength in the visible range, far from infrared telecommunications wavelengths [\cite=Holloway:11]. Using multiple close wavelengths on the same fiber leads to wave-mixing processes, such as stimulated brillouin scattering and four-wave mixing [\cite=agrawal2001nonlinear]. Four-wave mixing processes are a concern for experimentalists of telecommunications systems, but are an obstacle to QKD systems. QKD systems operate at much lower optical powers than the classical communications traffic (0.1-10 pW compared to 0.1-100 mW), so it is more likely for the classical channels to mix and spread into the quantum channels than vice-versa. The photons produced by wave-mixing processes are generated in random bases, and can be interpreted as detector dark counts in analysis.

In simulation, we can determine the maximum 'noise budget' - meaning the maximum dark count probability that can be tolerated given a channel efficiency. We find that this noise budget approximately follows a rational equation, where in order to get a positive key, the maximum tolerable dark count probability is:

[formula]

where ηa and ηb are the channel efficiencies to Alice and Bob, respectively. An experiment could be optimized by following this limit.

For implementations with visible wavelengths, the impact of mixing and scattering processes is negligible due to the wavelength distance between classical and quantum signals. Therefore, systems on bright fibers with visible wavelengths have the same maximum distance as visibile wavelength implementations on dark fibers ( 16 km).

Conclusions

We have used realistic detector models with correct treatment of double pairs to determine the two-fold coincidence probability that would be measured in a given entanglement-based QKD system when the system has the largest secure key bit probability. We have used symbolic regression to create an equation relating the optimal two-fold coincidence probability to the detector dark count probability and the system loss. We have also taken experimental data to show that our simulation matches reality and that our model accurately indicates the maximum under extreme experimental conditions..

We hope that in finding this relation, we have provided future experimentalists with a useful tool. At the moment many demonstrations of QKD with entangled photon pairs rely on low numbers of coincidences where the visibility is high [\cite=Erven:08] [\cite=Holloway:11]. However, as detectors and sources improve and experimentalists compete for the new distance record, the issue of the tradeoffs between coincidence rate and visibility will have to be adressed. Our model provides a simple method for maximizing the throughput of QKD systems, which relies only on presently measurable variables. We believe this model will allow for near real-time optimisation in pump power in real-world implementations such as on active telecommunications networks and satellite transmission, where background and losses change quickly and unpredictably. It could also provide a starting point for future theoretical exploration of this phenomenon.

Acknowledgements

The authors would like to acknowledge Xiongfeng Ma for sharing his data. We also gratefully acknowledge support for this work from NSERC, CSA, CIFAR, CFI, the Ontario Ministry of Training, Colleges and Universities, the David R. Cheriton school of Computer Science.

Comparison between our optimal μ and Ma's optimal μ

In the appendix of Ma, Fung and Lo's paper on QKD with entangled photon sources, they use their calculations for photon gain, secure key rate in the asymptotic limit of shared bits, and a model of loss and error rates in order to determine the optimal value for the photon pair production rate, μ. They simplify for two cases, for a lossless channel η  =  1 and a very lossy channel η <  < 1. They come up with an approximate relation which must be numerically solved in order to determine μ in terms of the intrinsic detector error.

Our simulation differs from Ma's equations in three ways. We use detector models with poissonian distributions of dark counts. We do post-processing to assign double clicks to random bases, and we look at detector dark counts, not detector error, which flips the state of some incoming photons instead of adding noise to the detection probability. Although our model determines the optimal measured two-fold coincidence rate, we calculate the theoretical μ at the same time for various detector dark counts and loss. Our definitions of error are slightly different but we believe that they are similar enough to allow for direct comparison between our model and theirs, which we do in figure [\ref=comparison_models].

Theory

The Hamiltonian for generating an entangled state from SPDC is a squeezing operator on output modes a and b, which correspond to the channels of delivery for Alice and Bob:

[formula]

where ε is the squeezing parameter.

When this operator is applied to the vacuum state, it produces:

[formula]

where

[formula]

In order to turn this into an entangled state, two such SPDC states must be created, and then tensored together. The dimensions are then permuted so that the representation of Fock states are:

[formula]

Existing QKD implementations utilize 'bucket'-type single photon detectors, which ‘click’ if they detect any light, and do not click otherwise. The mathematical model for 'bucket'-type detectors arises from the model for photon-counting detectors, although bucket detectors lack photon-number counting capabilities. Dowling et al.[\cite=dowling] give a model for photon-counting detectors in terms of the probability of detecting k photons given a number of photons incident on the detector i, which follows a Poissonian distribution:

[formula]

Where η is the detector efficiency, n is the detector dark count probability within a coincidence window and [formula]. Summing over all incident photons greater than 1 in the photon-counting detector model gives the model for bucket detectors. Bucket detectors click with probability:

[formula]

Similarly, they remain silent with probability [formula].

Entangled state measurements operate on single photon states. We use bucket detectors on multiple modes with a passive basis choice, so single photon states are a subspace of the measurement we perform with our detectors. In order to correctly measure this subspace we must apply a post-processing scheme to the expectation values for our single photon measurements [\cite=PhysRevA.59.3301] [\cite=PhysRevA.81.052342] to account for the detectors of two perpendicular measurements (H and V) clicking within the same coincidence window:

[formula]

Measurements with conflicting basis information, for example a click on either of the +   or -   detectors within the same coincidence window as a click on the H or V detectors, are discarded.

After the detection events occur, the quantum bit error rate (QBER) and coincidence rate are calculated. If Alice and Bob correlate their measurements in time, then they can count the different types of coincidence events. Pairs where Alice and Bob measured in different bases are discarded.Those that are measured in the same basis are kept to form the secret key. Ideally measurements of these pairs should be identically correlated, however, owing to experimental difficiencies there will be errors. Dividing the errors by the total number of revealed bits gives the QBER. An error rate below 11% [\cite=RevModPhys.74.145] means that any potential eavesdropper has not learned enough information about the photon pairs to break the security of the scheme. The SKR can then be estimated based on the coincidence rate and the QBER.

Ma, Fung and Lo, give the key generation rate in the infinite key limit as: [\cite=PhysRevA.76.012307]:

[formula]

where H(δ) is the binary entropy function, H(δ)  =    -  δ log 2(δ) - (1 - δ)log2(1 - δ), δ1 is the bit error rate, δ2 is the phase error rate, f(δ1) is the error correction efficiency, Q is the coincidence rate, and q is the basis reconciliation factor, which is [formula] in BB84[\cite=bennett1984quantum]. Error correction is typically completed with Bennett and Salvail's CASCADE algorithm [\cite=brassard1994secret], which produces unpredictable rates for error correction efficiency with an overall trend of f(δ)  =  1.169  +  δ. In modern implementations of QKD, LDPC codes are used for error correction. The error correction efficiency is slightly better with LDPC codes.