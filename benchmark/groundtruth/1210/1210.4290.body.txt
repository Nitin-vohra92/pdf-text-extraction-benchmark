A Fast Iterative Algorithm for Recovery of Sparse Signals from One-Bit Quantized Measurements

Introduction

Conventional compressed sensing framework recovers a sparse signal [formula] from only a few linear measurements:

[formula]

where [formula] denotes the acquired measurements, [formula] is the sampling matrix, and m  ≪  n. Such a problem has been extensively studied and a variety of algorithms that provide consistent recovery performance guarantee were proposed, e.g. [\cite=CandesTao05] [\cite=TroppGilbert07] [\cite=Wainwright09]. In practice, however, measurements have to be quantized before being further processed. Moreover, in distributed systems where data acquisition is limited by bandwidth and energy constraints, aggressive quantization strategies which compress real-valued measurements into one or only a few bits of data are preferred. This has inspired recent interest in studying compressed sensing based on quantized measurements. Specifically, in this paper, we are interested in an extreme case where each measurement is quantized into one bit of information

[formula]

where "[formula]" denotes an operator that performs the sign function element-wise on the vector, the sign function returns 1 for positive numbers and - 1 otherwise. Clearly, in this case, only the sign of the measurement is retained while the information about the magnitude of the signal is lost. This makes an exact reconstruction of the sparse signal [formula] impossible. Nevertheless, if we impose a unit-norm on the sparse signal, it has been shown [\cite=JacquesLaska11] [\cite=PlanVershynin11] that signals can be recovered with a bounded error from one bit quantized data. Besides, in many practical applications such as source localization, direction-of-arrival estimation, and chemical agent detection, it is the locations of the nonzero components of the sparse signal, other than the amplitudes of the signal components, that have significant physical meanings and are of our ultimate concern. Recent results [\cite=WimalajeewaVarshney12] show that asymptotic reliable recovery of the support of sparse signals is possible even with only one-bit quantized data.

The problem of recovering a sparse or compressible signal from one-bit measurements was firstly introduced by Boufounos and Baraniuk in their work [\cite=BoufounosBaraniuk08]. Following that, the reconstruction performance from one-bit measurements was more thoroughly studied [\cite=JacquesLaska11] [\cite=PlanVershynin11] [\cite=WimalajeewaVarshney12] [\cite=LaskaBaraniuk12] and a variety of one-bit compressed sensing algorithms such as binary iterative hard thresholding (BIHT) [\cite=JacquesLaska11], matching sign pursuit (MSP) [\cite=Boufounos09], l1 minimization-based linear programming (LP) [\cite=PlanVershynin11], and restricted-step shrinkage (RSS) [\cite=LaskaWen11] were proposed. Although achieving good reconstruction performance, these algorithms either require the knowledge of the sparsity level [\cite=JacquesLaska11] [\cite=Boufounos09] or are l1-based methods that often yield solutions that are not necessarily the sparsest [\cite=PlanVershynin11] [\cite=LaskaWen11]. In this paper, we study a new method that uses the Gaussian entropy-based penalty function for sparse signal recovery. The Gaussian entropy has the potential to be much more sparsity-encouraging than the l1 norm. By resorting to a bound optimization approach, we propose an iterative reweighted algorithm that successively minimizes a sequence of convex surrogate functions with its weights for the next iteration computed based on the current estimate. The proposed algorithm has the advantage that it does not need the cardinality of the support set, K, of the sparse signal. Moreover, our analysis and simulation results show that the proposed algorithm outperforms the l1-type methods [\cite=PlanVershynin11] [\cite=LaskaWen11] in identifying the support set of the sparse signal.

One-Bit Compressed Sensing

Since the only information we have about the original signal is the sign of the measurements, we hope that the reconstructed signal [formula] yields estimated measurements that are consistent with our knowledge, that is

[formula]

or in other words

[formula]

where [formula] denotes the transpose of the ith row of the sampling matrix [formula], bi is the ith element of the sign vector [formula]. This consistency can be enforced by hard constraints [\cite=PlanVershynin11] [\cite=LaskaWen11] or can be quantified by a well-defined metric which is meant to be maximized/minimized [\cite=Boufounos09] [\cite=JacquesLaska11] [\cite=YanYang12]. In this paper, we introduce the sigmoid function to quantify the consistency between what we acquired and what we estimated. The metric is defined as

[formula]

where

[formula]

is the sigmoid function. The sigmoid function, with an 'S' shape, approaches one for positive x and zero for negative x. Hence it is a good measure to evaluate the consistency between bi and [formula]. Also, the sigmoid function, differentiable and log-concave, is more amiable for algorithm development than the indicator function adopted in [\cite=Boufounos09] [\cite=JacquesLaska11] [\cite=YanYang12]. Note that the sigmoid function, also referred to as the logistic regression model, has been widely used in statistics and machine learning to represent the posterior class probability.

Naturally our objective is to find [formula] to maximize the consistency between the acquired data and the reconstructed measurements, i.e.

[formula]

This optimization, however, does not necessarily lead to a sparse solution. To obtain sparse solutions, a sparsity-encouraging term needs to be incorporated to encourage sparsity of the signal coefficients. The most commonly used sparsity-encouraging penalty function is l1 norm. An attractive property of the l1 norm is its convexity, which makes the l1-based minimization a well-behaved numerical problem. Despite its popularity, l1 type methods suffer from the drawback that the global minimum does not necessarily coincide with the sparsest solution, particularly when only a few measurements are available for signal reconstruction. In this paper, we consider the use of an alternative sparsity-encouraging penalty function for sparse signal recovery. This penalty function, also referred to as the Gaussian entropy, is defined as

[formula]

where xi denotes the ith component of the vector [formula]. Such a log-sum penalty function was firstly introduced in [\cite=CoifmanWickerhauser92] for basis selection. This penalty function has the potential to be much more sparsity-encouraging than the l1 norm. It can be readily observed that the log-sum (Gaussian entropy) penalty function, like l0 norm, has infinite slope at [formula], which implies that a relatively large penalty is placed on small nonzero coefficients to drive them to zero. The reason why the Gaussian entropy is more sparsity-encouraging than l1 function will also be explained from an algorithmic perspective later in our paper. Using this penalty function, the problem of finding a sparse solution to maximize the consistency can be formulated as follows

[formula]

where λ is a parameter controlling the trade-off between the degree of sparsity and the quality of consistency.

Proposed Iterative Algorithm

Due to the concave and unbounded nature of the Gaussian entropy, the objective function ([\ref=opt1]) is non-convex and unbounded from below. Instead of resorting to the gradient descend method, we propose an iterative reweighted algorithm which provides fast convergence and guarantees that every iteration results in a decreasing objective function value. The algorithm is developed based on a bound optimization approach [\cite=LangeHunter00]. The idea is to construct a surrogate function [formula] such that

[formula]

and the minimum is attained when [formula], i.e. [formula]. Optimizing [formula] can therefore be replaced by minimizing the surrogate function [formula] iteratively. Suppose that

[formula]

We can ensure that

[formula]

where the first inequality follows from the fact that [formula] attains its minimum when [formula]; the second inequality comes by noting that [formula] is minimized at [formula]. We see that, through minimizing the surrogate function iteratively, the objective function [formula] is guaranteed to decrease at each iteration.

We now discuss how to find a surrogate function for the original problem ([\ref=opt1]). Ideally, we hope that the surrogate function is differentiable and convex so that the minimization of the surrogate function is a well-behaved numerical problem. Since the consistency evaluation term is convex, our objective is to find a convex surrogate function for the Gaussian entropy defined in ([\ref=eq2]). An appropriate choice of such a surrogate function has a quadratic form and is given by

[formula]

It can be easily verified that

[formula]

with the minimum 0 attained when [formula]. Therefore the convex function [formula] is a desired surrogate function for the Gaussian entropy [formula]. As a consequence, the surrogate function for the objective function [formula] is given by

[formula]

where

[formula]

Optimizing [formula] now reduces to minimizing the surrogate function [formula] iteratively. For clarity, the iterative algorithm is briefly summarized as follows.

Given an initialization [formula].

At iteration t > 0, minimize [formula], which yields a new estimate [formula]. Based on this new estimate, construct a new surrogate function [formula].

Go to Step 2 if [formula], where ε is a prescribed tolerance value; otherwise stop.

Remark 1: The second step involves optimization of the surrogate function [formula]. Since the surrogate function is differentiable and convex, minimization of [formula] is a well-behaved numerical problem. Any gradient-based search such as Newton's method which has a fast convergence rate can be used and is guaranteed to converge to the global maximum.

Remark 2: The above algorithm results in a non-increasing objective function value of [formula]. In this manner, the iterative algorithm eventually converges to a local minimum of [formula]. It should be emphasized that the cost function [formula] is non-convex. Therefor it is important to choose a suitable starting point for the algorithm. Our simulations suggest that initializing with [formula] usually delivers good reconstruction performance. Moreover, we found from our simulations that, despite starting from different (randomly generated) initial points, if the number of bits, m, is sufficiently large, the support of the reconstructed sparse solution is guaranteed to coincide with, or at least be a subset of, the true support of the sparse signal [formula].

Remark 3: The proposed iterative algorithm can be considered as composed of two alternating steps. First, we estimate [formula] through minimizing the current surrogate function [formula]. Second, based on the estimate of [formula], we update the weights of the weighted l2 norm penalty of the surrogate function. This alternating process finally results in sparse solutions. To see this, note that the weighted l2 norm of [formula] has their weights specified as {(x̂(t)i)- 2}. The penalty term has the tendency to decrease these entries in [formula] whose corresponding weights are large, i.e., whose current estimates {x̂(t)i} are already small. This negative feedback mechanism keeps suppressing these entries until they reach machine precision and becomes zeros, while leaving only a few prominent nonzero entries survived to meet the consistency requirement.

Related Work

Our developed algorithm has a close connection with the FOcal Underdetermined System Solver (FOCUSS) algorithm [\cite=GorodnitskyRao97] since the latter algorithm also uses an iterative reweighted approach to find sparse solutions to underdetermined systems [formula]. Specifically, at each iteration, FOCUSS solves a reweighted l2 minimization with weights w(t + 1)i = 1 / |x(t)i|p, where p∈[null]. It was also shown that when p = 2, FOCUSS is equivalent to a modified Newton's method minimizing the Gaussian entropy function. As compared with FOCUSS, our paper considers a more general framework in which the data model allows measurement errors/noise and is not confined to be linear, and a general connection between the sparsity-promoting penalty function and the iterative reweighted process is established through the use of the surrogate function. In [\cite=CandesWakin08], a similar iterative reweighted algorithm was proposed to enhance sparsity of recovered signals. The algorithm consists of solving a sequence of weighted l1-minimization problems with its weights for the next iteration computed based on the current estimate. Such an algorithm, interestingly, is also shown closely related to the log-sum penalty function, i.e. the Gaussian entropy function.

We now explore the relationship between our proposed algorithm and the l1-minimization type methods. Following ([\ref=opt1]), we can formulate a l1 norm-based sparsity-promoting optimization

[formula]

Similarly, a surrogate function can be constructed to bound the above optimization, from which an iterative algorithm can be developed to solve ([\ref=opt2]). Note that the l1 function |x| can also be upper bounded by a quadratic function. It can be readily verified that the surrogate function for l1 norm is given by

[formula]

Therefore the solution to ([\ref=opt2]) can be found by iteratively minimizing the following surrogate function

[formula]

where

[formula]

The surrogate function [formula] has a similar form as ([\ref=surrogate-function]), except that the surrogate function for the Gaussian entropy updates its weights using (x̂(t)i)- 2, while the surrogate function for the l1 norm updates its weights with (|x̂(t)i|)- 1. This seemingly slight difference, however, results in very different convergence behavior. The latter update equation guarantees converging to a unique global minimum. On the other hand, the update method used in our algorithm renders a more intense effect in de-emphasizing the entries in [formula]. Therefore the proposed algorithm is more sparsity-encouraging than the l1 type methods, and generally yields a more sparse solution.

Numerical Results

We now carry out experiments to illustrate the performance of our proposed one-bit compressed sensing algorithm. In our simulations, the K-sparse signal is randomly generated with the support set of the sparse signal randomly chosen according to a uniform distribution. The signals on the support set are independent and identically distributed (i.i.d.) Gaussian random variables with zero mean and unit variance. The measurement matrix [formula] is randomly generated with each entry independently drawn from Gaussian distribution with zero mean and unit variance, and then each column of [formula] is normalized to unity for algorithm stability. We compare our proposed algorithm with the other two algorithms, namely, the l1 minimization-based linear programming (LP) algorithm [\cite=PlanVershynin11] (referred to as "one-bit LP"), and the iterative algorithm proposed in Section [\ref=sec:relation] to solve the optimization ([\ref=opt2]) (referred to as "L1-optimization ([\ref=opt2])"). Both the latter two algorithms are l1 type methods, with the consistency evaluated by different criteria.

We investigate the support recovery performance of respective algorithms. Support recovery accuracy are measured by the false alarm (misidentified) rate and the miss rate. A false alarm event represents the case where coefficients that are zero in the original signal are misidentified as nonzero after reconstruction, while a miss event stands for the case where the nonzero coefficients are missed and determined to be zero. Fig. [\ref=fig1] depicts the false alarm and miss rates of respective algorithms as a function of the sparsity level K, where we set m = 100, n = 50, and λ = 1 / 2 in our simulations. Results are averaged over 104 independent runs. We see that the proposed algorithm provides more accurate identification of the true support set: it has a similar (or slightly higher) miss rate as (than) that of the l1-based methods, while meanwhile achieves a considerably lower false alarm rate than both l1 type methods. Our result also indicates that the proposed algorithm yields solutions more sparse than l1 type methods, which corroborates our theoretical analysis.

Conclusions

We proposed an iterative reweighted algorithm for sparse signal reconstruction from one-bit quantized measurements. The proposed algorithm consists of solving a sequence of minimization problems whose weights are updated based on the current estimate. Analyses and simulation results show that the proposed algorithm that uses the log-sum penalty function is more sparsity-encouraging than l1-based methods, and outperforms l1 type methods in recovering the true support of the sparse signal.