Temporal Autoencoding Restricted Boltzmann Machine

Introduction

In the early days of Machine Learning, feature extraction was usually approached in a task-specific way. The complexity and high dimensionality involved in doing so in an unsupervised fashion was seen as a major barrier and expert features were thought to yield the best results for classification and representation tasks [\cite=fukunaga1990introduction]. Recently however, a number of advances have brought the field of unsupervised feature extraction back into the center stage of machine learning. Increases in computational power, allowing for algorithms trained on very large datasets, together with new techniques to train deep architectures have yielded insightful results in unsupervised feature learning even in uncurated sets of natural images [\cite=le2012]. Examples of such algorithms are denoising Autoencoders (dAEs) and Restricted Boltzmann Machines (RBMs) [\cite=le2012] [\cite=mohamed2011deep] [\cite=hinton2012improving].

In unsupervised feature learning, it is the structure of the data that defines the features to be learnt by a given model. In Computational Neuroscience, this link between the ensemble of natural stimuli an organism is exposed to and the shape of the tuning functions in their sensory systems has been a subject of great interest [\cite=karklin2011] [\cite=lewicki2002] [\cite=sprekeler2011]. Specifically in the field of vision neuroscience, a number of principles have been proposed to explain the shape of tuning functions in primary visual cortex based on the properties of natural images, for example redundancy minimization [\cite=atick1992] and predictive coding [\cite=rao1999]. In recent years, it has been shown that simple unsupervised learning algorithms such as Sparse Coding, dAEs and RBMs can also be used to learn structure from natural stimuli, independently of labels and supervision, and that the types of structure learnt can be related back to cortical receptive fields found in the mammalian brain [\cite=saxe2011] [\cite=lee2008] .

While most of this research in vision has focused on finding optimal filters for representing and decoding sets of static natural images [\cite=olshausen1996] [\cite=bell1997], here we seek to understand how these optimal filters extend to the temporal domain. We build on existing work in the field to develop the Temporal Autoencoding Restricted Boltzmann Machine (TARBM) and show that it is able to learn high level structure in a natural movie dataset and account for the transformation of these features over time.

Existing Models

Restricted Boltzmann Machines (RBMs) [\cite=hinton2006fast] [\cite=hinton2006reducing] and Autoencoders (AEs) [\cite=bengio2007] [\cite=ranzato2007] have in recent years become prominent methods of unsupervised feature learning with applications in a wide variety of machine learning fields. As both of these models are well known and discussed at length in many other papers, we will introduce them only briefly here.

Both models are two-layer neural networks, all to all connected between the layers but with no intralayer connectivity. The models consist of a visible and a hidden layer, where the visible layer represents the input to the model whilst the hidden layer's job is to learn a meaningful representation of the data in some other dimensionality. We will represent the visible layer activation variables by vi, the hidden activations by hj and the vector variables by [formula] and [formula].

Autoencoders are a deterministic model with two weight matrices [formula] and [formula] representing the flow of data from the visible-to-hidden and hidden-to-visible layers respectively (see figure [\ref=fig:rbm_and_autoencoder]b). AEs are trained to perform optimal reconstruction of the visible layer, often by minimizing the mean-squared error (MSE) in a reconstruction task. This is usually evaluated as follows: Given an activation pattern in the visible layer [formula], we evaluate the activation of the hidden layer by [formula]. These activations are then propagated back to the visible layer through [formula] and the weights [formula] and [formula] are trained to minimize the distance measure between the original and reconstructed visible layers. For example, using the squared euclidian distance we have a cost function of

[formula]

where we have denoted the dataset by [formula] and the biases of the visible and hidden layer as [formula] and [formula] respectively. The weights can then be learned through stochastic gradient descent on the cost function.

Restricted Boltzmann Machines on the other hand are a stochastic model that assumes symmetric connectivity between the visible and hidden layers (see Figure [\ref=fig:rbm_and_autoencoder]a) and seeks to model the structure of a given dataset. They are generally viewed as energy-based models, where the energy of a given configuration of activations {vi} and {hj} is given by

[formula]

RBMs are usually trained through contrastive divergence, the central idea of which is to stabilize the transient induced by the presentation of data to the visible layer, therefore representing it in the hidden layer optimally. In practice this is achieved by learning the weights via the difference between the transient and the equilibrium correlations between visible and hidden layers. Sample correlations in the first presentation are taken as a proxy for the transient and correlations after n successive Gibbs samples are taken as a proxy for the equilibrium correlation. The weight update is then defined as

[formula]

A number of auxiliary strategies have been used to improve the training process of RBMs such as mini-batch training, free energy minimization, Parzen windows, early stopping and sparsity constraints. In addition, RBMs can be stacked to form what is called a Deep Belief Network (DBN) [\cite=hinton2006reducing] where each additional RBM models the output of the previous one to form a more abstract/high level representation.

To date, a number of RBM based models have been proposed to capture the sequential structure in time series data. Two of these models, the Temporal Restricted Boltzmann Machine and the Conditional Restricted Boltzmann machine, are introduced below.

Temporal Restricted Boltzmann Machine (TRBM)

The Temporal Restricted Boltzmann Machine [\cite=sutskever2007learning] is a temporal extension of the standard RBM whereby feed forward connections are included from previous time steps between hidden layers, from visible-to-hidden layers and from visible-to-visible layers. Learning is conducted in the same manner as a normal RBM using contrastive divergence and it has been shown that such a model can be used to learn non-linear system evolutions such as the dynamics of a ball bouncing in a box [\cite=sutskever2007learning]. A more restricted version of this model, discussed in [\cite=sutskever2008recurrent] can be seen in Figure [\ref=fig:tarbm]b and only contains temporal connections between the hidden layers.

If we denote by [formula] the hidden layers and by [formula] the visible layers,the energy of the model is given by

[formula]

where the weights are as given in Figure [\ref=fig:tarbm]b. We denoted [formula], where [formula] are the static weights and [formula] to [formula] are the delayed weights. These models have been shown to be amenable to stacking in deep architectures in the same manner as RBMs and AEs.

Conditional Restricted Boltzmann Machine (CRBM)

The Conditional Restricted Boltzmann Machine described in [\cite=taylor2007modeling] contains no temporal connections from the hidden layer but includes connections from the visible layer at previous time steps to the current hidden and visible layers. The model architecture can be seen in Figure [\ref=fig:tarbm]a. Again, learning with this architecture requires only a small change to the energy function of the RBM and can be achieved through contrastive divergence. The CRBM is likely the most successfull of the temporal RBM models to date and has been shown to both model and generate data from complex dynamical systems such as human motion capture data and video textures [\cite=taylor2009composable].

Temporal Autoencoding Restricted Boltzmann Machines (TARBM)

Here we present a new model, the TARBM, an extension of the Temporal RBM (with only hidden-to-hidden temporal connections) where a denoising Autoencoder approach is used to pretrain the temporal weights. We show that this approach provides a marked advantage over contrastive divergence training alone and that our model is able to outperform both the TRBM and CRBM on a classical temporal sequence task while yielding a deeper insight into the temporal representation of natural image sequence data.

The Model

Much of the motivation for this work is to gain insight into the typical evolution of learned hidden layer features present in natural movie stimuli. With the CRBM this is not possible as it is unable to explicitly model the evolution of hidden features without resorting to a deep network architecture. We address this by using a layerwise approach, much in the same vein as that used when stacking RBMs to form a Deep Belief Network [\cite=hinton2006reducing], but through time. We stack a given number of RBMs side by side in time and train the temporal connections between the hidden layers (see Figure [\ref=fig:tarbm]b) to minimize the reconstruction error, in a process similar to Autoencoder training [\cite=bengio2007]. A simple autoregressive model is used to account for the dynamics of the hidden layer allowing us to train a dynamic prior over the temporal evolution of the stimulus.

Training Algorithm

We model our network as an energy-based function with interactions between the hidden layers at different time lags. The energy of the model is given by Equation [\ref=eqn:energy_trbm] as in the case of the TRBM and is essentially an M-th order autoregressive RBM model and can be trained through standard contrastive divergence. The individual RBM visible-to-hidden weights [formula] are initialized through contrastive divergence with a sparsity constraint on static samples of the dataset. After that, to ensure that the weights representing the hidden-to-hidden connections ([formula]) encode the dynamic structure of the ensemble, we initialize them by pre-training in the fashion of a denoising Autoencoder. We do this by treating the visible layer activation at time t - d, where d is the temporal delay, as a corrupted version of the true visible activation at time t. With this view, the model should learn to reconstruct the visible layer at time t by transforming the corrupted input at t - d through the model as in the case of a denoising Autoencoder. The pretraining is described in Algorithm [\ref=alg:pretraining].

One can regard the weights w as a representation of the static patterns contained in the data and the wd as representing the transformation undergone by these patterns over time in the data sequences. This allows us to separate the representation of form and motion in the case of natural image sequences, a desirable property that is frequently studied in natural movies (see [\cite=cadieu2012]).

Experiments

We first assess the TARBM's ability to learn multi-dimensional temporal sequences by applying it to the 49 dimensional motion capture data described in [\cite=taylor2007modeling] and comparing the performance to a TRBM and Graham Taylor's example CRBM implementation . All three models are implemented using Theano [\cite=theano], have a temporal dependancy of 6 frames and were trained using minibatches of 100 samples for 500 epochs . The training time for the models was approximately equal. Training was performed on the first 2000 samples of the dataset after which the models were presented with 1000 snippets of the data not included in training set and required to generate the next frame in the sequence. The results of a single trial prediction for 4 dimensions of the dataset can be seen in Figure [\ref=fig:motion] and the mean squared error of the model predictions over 100 repetitions of the task can be seen in Table [\ref=tbl:motion]. The TARBM by far outperforms the TRBM model in this task and is also somewhat better than the CRBM . The gain in performance from the TRBM to TARBM model, which are both structuraly identical, would suggest that our approach of Autoencoding the temporal dependancies gives the model a more meaningful temporal representation than is achievable through contrastive divergence alone.

The second experiment was to model a natural movie dataset and investigate the types of filters learned. Here we take the Hollywood2 dataset introduced in [\cite=marszalek09], consisting of a number of snippets from various Hollywood films and compare the CRBM implementation referenced with our TARBM model. From the dataset, 8x8 pixel patches are extracted in sequences 30 frames long. They are then contrast normalised and whitened to provide a training set of approximately 250,000 samples. The models, each with 400 hidden units and a temporal dependancy of 3 frames, are trained initially for 100 epochs on static frames of the data to initialise the [formula] weights and then until convergence on the full temporal sequences.

Visualisation of the temporal receptive fields learnt by the CRBM involves displaying the weight matrix [formula] and the temporal weights [formula] to [formula] for each hidden unit as a projection into the visible layer (an 8x8 patch). This shows the temporal dependance of each hidden unit on the past visible layer activations and is plotted with time running from left to right. The visualisation process for the TARBM is somewhat more complicated as each hidden unit is also dependant on a number of hidden units from each delay time in the model and as such cannot be visualised as a direct projection of the weights into visible layer. To understand how these units depend on the past we use a forward projection method through the temporal delays whereby a hidden unit h at delay time t - d is chosen as the starting point. We then use the relative weights for unit h in w1 to find the n most likely units to be active at time t - (d - 1) given that unit h was active at t - d. For each of the n active units at t - (d - 1), we choose n active units at time t - (d - 2) given the activations of unit h at t - d propogated through [formula] and one of the n units at t - (d - 1) propogated through [formula]. This process is repeated until the full delay of the network is mapped out. For each of the active hidden units, the projection onto an 8x8 patch of the hidden layer is defined in the weight matrix [formula]. When plotted for n = 1, this trace displays the most likely evolution of the hidden layer over the delay period of the model for each hidden unit.

A subset of the temporal filters learned by each of the models can be seen in Figure [\ref=fig:temporal] with the TARBM on the left and the CRBM on the right. While both the TARBM and the CRBM learn gabor like filters at time t, their dependance on the past is markedly different. Most hidden units in the CRBM fail to capture any structured dependance on delay times greater than d = 1. This makes the CRBMs temporal filters difficult to interperet with respect to structure in the image. The layerwise training of the temporal weights in the TARBM along with the forced reliance on filters learned in w for its delay input give the TARBM not only a longer temporal dependance, but also allow the weights learned to be easily interpereted as a transformation of the learned filters.

Figure [\ref=fig:temporal_joined] shows the forward projection method of visualising the TARBM for n = 3 from selected hidden units. This means that for each delay step, three most likely filters to be active at the next point in time are shown. The model is able to learn multiple trainformations over time for each of the hidden unit receptive fields. The transformations often represent simple operations such as rotation and translation of the static features, seperating the modeling of form and motion.

Discussion and Future Work

We have shown that by using an Autoencoder to initialise the temporal weights of a TRBM, forming what we call a TARBM, a significant performance increase can be achieved in modelling and generating from a sequential motion capture dataset. We also show that the TARBM is able to learn high level structure from natural movies and account for the transformation of these features over time. Additionally, the evolution of the learned temporal filters are easily interpretable and help to better understand how the model represents the trained data.

The presented model could with minimal effort be adapted into a deep architecture, allowing us to represent higher order features in the same temporal manner. We propose that learning higher order temporal features might prove to be useful for control tasks such as image stabilization and object tracking. In addition, we hope to study the relation of the presented encoding strategy with strategies employed by the mammalian visual cortex [\cite=kampa2011]. Another interesting avenue of research will be to apply the current model to classification and generative tasks.

The work of Chris Häusler and Alex Susemihl were supported by the DFG Research Training Group 1589/1. We would like to thank Prof. Martin Nawrot of the Freie Universität Berlin for his enthusiastic support and invaluable feedback.