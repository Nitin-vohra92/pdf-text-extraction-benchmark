Listing All Maximal Cliques in Large Sparse Real-World Graphs

Introduction

Clique finding procedures arise in the solutions to a wide variety of important application problems. The problem of finding cliques was first studied in social network analysis, as a way of finding closely-interacting communities of agents in a social network [\cite=harary57]. In bioinformatics, clique finding procedures have been used to find frequently occurring patterns in protein structures [\cite=grindley93] [\cite=koch2001] [\cite=KocLenWan-JCB-96], to predict the structures of proteins from their molecular sequences [\cite=samudrala98], and to find similarities in shapes that may indicate functional relationships between proteins [\cite=gardiner99]. Other applications of clique finding problems include information retrieval [\cite=auguston70], computer vision [\cite=HorSko-PAMI-89], computational topology [\cite=Zom-SoCG-10], and e-commerce [\cite=ZakParOgi-KDD-97].

For many applications, we do not want to report one large clique, but all maximal cliques. Any algorithm which solves this problem must take exponential time in the worst-case because graphs can contain an exponential number of cliques [\cite=moon-moser65]. However, graphs with this worst-case behavior are not typically encountered in practice. More than likely, the types of graphs that we will encounter are sparse [\cite=goel2006]. Therefore, the feasibility of clique listing algorithms lies in their ability to appropriately handle sparse input graphs. Indeed, it has long been known that certain sparse graph families, such as planar graphs and graphs with low arboricity, contain only a linear number of cliques, and that all maximal cliques in these graphs can be listed in linear time [\cite=chiba85] [\cite=chrobak91]. In addition, there are also several methods to list all cliques in time polynomial in the number of cliques reported [\cite=tsukiyama77], which can be done faster if parameterized on a sparsity measure such as maximum degree [\cite=makino2004].

Many different clique-finding algorithms have been implemented, and an algorithm of Tomita et al. [\cite=tomita2006], based on the much earlier Bron-Kerbosch algorithm [\cite=bron73], has been shown through many experiments to be faster by orders of magnitude in practice than others. An unfortunate drawback of the algorithm of Tomita et al., however, is that both its theoretical analysis and implementation rely on the use of an adjacency matrix representation of the input graph. For this reason, their algorithm has limited applicability for large sparse graphs, whose adjacency matrix may not fit into working memory. We therefore seek to have the best of both worlds: we would ideally like an algorithm that rivals the speed of the Tomita et al. result, while having linear storage cost.

Recently, together with Maarten Löffler, the authors developed and published a new algorithm for listing maximal cliques, particularly optimized for the case that the input graph is sparse [\cite=els2010]. This new algorithm combines features of both the algorithm of Tomita et al. and the earlier Bron-Kerbosch algorithm on which it was based, and maintains through its recursive calls a dynamic graph data structure representing the adjacencies between the vertices that remain relevant within each call. When analyzed using parameterized complexity in terms of the degeneracy of the input graph (a measure of its sparsity) its running time is near-optimal in terms of the worst-case number of cliques that a graph with the same sparsity could have. However, the previous work of the authors with Löffler did not include any implementation or experimental results showing the algorithm to be good in practice as well as in theory.

Our Results

We implement the algorithm of Eppstein, Löffler, and Strash for listing all maximal cliques in sparse graphs [\cite=els2010]. Using a corpus of many large real-world graphs, together with synthetic data including the Moon-Moser graphs as well as random graphs, we compare the performance of our implementation with the algorithm of Tomita et al. We also implement for comparison, a modified version of the Tomita et al. algorithm that uses adjacency lists in place of adjacency matrices, and a simplified version of the Eppstein-Löffler-Strash algorithm that represents its subproblems as lists of vertices instead of as dynamic graphs. Our results show that, for large sparse graphs, the new algorithm is as fast or faster than Tomita et al., and sometimes faster by very large factors. For graphs that are not as sparse, the new algorithm is sometimes slower than the algorithm of Tomita et al., but remains within a small constant factor of its performance.

Preliminaries

We work with an undirected graph G  =  (V,E) with n vertices and m edges. For a vertex v, let Γ(v) be its neighborhood {w|(v,w)∈E}, and similarly for a subset W  ⊂  V let Γ(W) be the set [formula], the common neighborhood of all vertices in W.

Degeneracy

The degeneracy of a graph G is the smallest number d such that every subgraph of G contains a vertex of degree at most d.

Every graph with degeneracy d has a degeneracy ordering, a linear ordering of the vertices such that each vertex has at most d neighbors later than it in the ordering. The degeneracy of a given graph and a degeneracy ordering of the graph can both be computed in linear time [\cite=batagelj2003].

The Algorithm of Tomita et al.

The algorithm of Tomita et al. [\cite=tomita2006] is an implementation of Bron and Kerbosch's algorithm [\cite=bron73], using a heuristic called pivoting [\cite=koch2001] [\cite=cazals2008]. The Bron-Kerbosch algorithm is a simple recursive algorithm that maintains three sets of vertices: a partial clique R, a set of candidates for clique expansion P, and a set of forbidden vertices X. In each recursive call, a vertex v from P is added to the partial clique R, and the sets of candidates for expansion and forbidden vertices are restricted to include only neighbors of v. If [formula] becomes empty, the algorithm reports R as a maximal clique, but if P becomes empty while X is nonempty, the algorithm backtracks without reporting a clique.

In the basic version of the algorithm, |P| recursive calls are made, one for each vertex in P. The pivoting heuristic reduces the number of recursive calls by choosing a vertex u in [formula] called a pivot. All maximal cliques must contain a non-neighbor of u (counting u itself as a non-neighbor), and therefore, the recursive calls can be restricted to the intersection of P with the non-neighbors.

The algorithm of Tomita et al. chooses the pivot so that u has the maximum number of neighbors in P, and therefore the minimum number of non-neighbors, among all possible pivots. Computing both the pivot and the vertex sets for the recursive calls can be done in time [formula] within each call to the algorithm, using an adjacency matrix to quickly test the adjacency of pairs of vertices. This pivoting strategy, together with this adjacency-matrix-based method for computing the pivots, leads to a worst-case time bound of O(3n / 3) for listing all maximal cliques [\cite=tomita2006].

The Algorithm of Eppstein, Löffler, and Strash

Eppstein, Löffler, and Strash [\cite=els2010] provide a different variant of the Bron-Kerbosch algorithm that obtains near-optimal worst-case time bounds for graphs with low degeneracy. They first compute a degeneracy ordering of the graph; the outermost call in the recursive algorithm selects the vertices v to be used in each recursive call, in this order, without pivoting. Then for each vertex v in the order, a call is made to the algorithm of Tomita et al. [\cite=tomita2006] to compute all cliques containing v and v's later neighbors, while avoiding v's earlier neighbors. The degeneracy ordering limits the size of P within these recursive calls to be at most d, the degeneracy of the graph.

A simple strategy for determining the pivots in each call to the algorithm of Tomita et al., used as a subroutine within this algorithm, would be to loop over all possible pivots in [formula] and, for each one, loop over its later neighbors in the degeneracy ordering to determine how many of them are in P. The same strategy can also be used to perform the neighbor intersection required for recursive calls. With the pivot selection and set intersection algorithms implemented in this way, the algorithm would have running time O(d2n3d / 3), a factor of d larger than the worst-case output size, which is O(d(n - d)3n / 3).

However, Eppstein et al. provide a refinement of this algorithm that stores, at each level of the recursion, the subgraph of G with vertices in [formula] and edges having at least one endpoint in P. Using this subgraph, they reduce the pivot computation time to [formula], and the neighborhood intersection for each recursive call to time [formula], which reduces the total running time to O(dn3d / 3). This running time matches the worst-case output size of the problem whenever d  ≤  n - Ω(n). As described by Eppstein et al., storing the subgraphs at each level of the recursion may require as much as O(dm) space. But as we show in Section [\ref=sec:details], it is possible to achieve the same optimal running time with space overhead O(n + m).

Tomita et al. with Adjacency Lists

In our experiments, we were only able to run the algorithm of Tomita et al. [\cite=tomita2006] on graphs of small to moderate size, due to its use of the adjacency matrix representation. In order to have a basis for comparison with this algorithm on larger graphs, we also implemented a simple variant of the algorithm which stores the input graph in an adjacency list representation, and which performs the pivot computation by iterating over all vertices in [formula] and testing all neighbors for membership in P. When a vertex v is added to R for a recursive call, we can intersect the neighborhood of r with P and X by iterating over its neighbors in the same way.

Let Δ be the maximum degree of the given input graph; then the pivot computation takes time [formula]. Additionally, preparing subsets for all recursive calls takes time [formula]. Fitting these facts into the analysis of Tomita et al. gives us a O(Δ(n - Δ)3Δ / 3) time algorithm. Δ may be significantly larger than the degeneracy, so this algorithm's theoretical time bounds are not as good as those of Tomita et al. or Eppstein et al.; nevertheless, the simplicity of this algorithm makes it competitive with the others for many problem instances.

Implementation and experiments

We implemented the algorithm of Tomita et al. using the adjacency matrix representation, and the simple adjacency list representation for comparison. We also implemented three variants of the algorithm of Eppstein, Löffler, and Strash: one with no data structuring, using the fact that vertices have few later neighbors in the degeneracy ordering, an implementation of the dynamic graph data structure that only uses O(m + n) extra space total, and an alternative implementation of the data structure based on bit vectors. The bit vector implementation executed no faster than the data structure implementation, so we omit its experimental timings and any discussion of its implementation details.

Implementation Details

We maintain the sets of vertices P and X in a single array, which is passed between recursive calls. Initially, the array contains the elements of X followed by the elements of P. We keep a reverse lookup table, so that we can look up the index of a vertex in constant time. With this lookup table, we can tell whether a vertex is in P or X in constant time, by testing that its index is in the appropriate subarray. When a vertex v is added to R in preparation for a recursive call, we reorder the array. Vertices in [formula] are moved to the end of the X subarray, and vertices in [formula] are moved to the beginning of the P subarray (see Figure [\ref=fig:pandx]). We then make a recursive call on the subarray containing the vertices [formula]. After the recursive call, we move v to X by swapping it to the beginning of the P subarray and moving the boundary so that v is in the X subarray. Of course, moving vertices between sets will affect P and X in higher recursive calls. Therefore, in a given recursive call, we maintain a list of the vertices that are moved from P to X, and move these vertices back to P when the call ends.

The pivot computation data structure is stored as set of arrays, one for each potential pivot vertex u in [formula], containing the neighbors of u in P. Whenever P changes, we reorder the elements in these arrays so that neighbors in P are stored first (see Figure [\ref=fig:pandx-ds]). Computing the pivot is as simple as iterating through each array until we encounter a neighbor that is not in P. This reordering procedure allows us to maintain one set of arrays throughout all recursive calls, requiring linear space total. Making a new copy of this data structure for each recursive call would require space O(dm).

Results

We implemented all algorithms in the C programming language, and ran experiments on a Linux workstation running the 32-bit version of Ubuntu 10.10, with a 2.53 GHz Intel Core i5 M460 processor (with three cache levels of 128KB, 512KB, and 3,072KB respectively) and 2.6GB of memory. We compiled our code with version 4.4.5 of the gcc compiler with the -O2 optimization flag.

In our tables of results, "tomita" is the algorithm of Tomita et al., "maxdegree" is the simple implementation of Tomita et al.'s algorithm for adjacency lists, and "hybrid" and "degen" are the implementations of Eppstein, Löffler, and Strash with no data structure and with the linear space data structure, respectively. We provide the elapsed running running times (in seconds) for each of these algorithms; an asterisk indicates that the algorithm was unable to run on that problem instance due to time or space limitations. In addition, we list the number of vertices n, edges m, the degeneracy d, and the number of maximal cliques μ.

Our primary experimental data consisted of four publicly-available databases of real-world networks, including non-electronic and electronic social networks as well as networks from bioinformatics applications.

A data base curated by Mark Newman [\cite=marknewman] (Table [\ref=table-uci]) which consists primarily of social networks; it also includes word co-occurrence data and a biological neural network. Many of its graphs were too small for us to time our algorithms accurately, but our algorithm was faster than that of Tomita et al. on all four of the largest graphs; in one case it was faster by a factor of approximately 130.

The BioGRID data [\cite=BioGRID2006] (Table [\ref=table-biogrid]) consists of several protein-protein interaction networks with from one to several thousand vertices, and varying sparsities. Our algorithm was significantly faster than that of Tomita et al. on the worm and fruitfly networks, and matched or came close to its performance on all the other networks, even the relatively dense yeast network.

We also tested six large social and bibliographic networks that appeared in the Pajek data set but were not in the other data sets [\cite=pajek] (Table [\ref=table-pajek]). Our algorithm was consistently faster on these networks. Due to their large size, the algorithm of Tomita et al. was unable to run on two of these networks; nevertheless, our algorithm found all cliques quickly in these graphs.

We also tested a representative sample of graphs from the Stanford Large Network Dataset Collection [\cite=snap] (Table [\ref=table-snap]). These included road networks, a co-purchasing network from Amazon.com data, social networks, email networks, a citation network, and two Web graphs. Nearly all of these input graphs were too large for the Tomita et al. algorithm to fit into memory. For graphs which are extremely sparse, it is no surprise that the maxdegree algorithm was faster than our algorithm, but our algorithm was consistently fast on each of these data sets, whereas the maxdegree algorithm was orders of magnitude slower than our algorithm on the large soc-wiki-Talk network.

As a reference point, we also ran our experimental comparisons using the two sets of graphs that Tomita et al. used in their experiments. First, Tomita et al. used a data set from a DIMACS challenge, a collection of graphs that were intended as difficult examples for clique-finding algorithms, and that have been algorithmically generated (Table [\ref=table-dimacs]). And second, they generated graphs randomly with varying edge densities; in order to replicate their results we generated another set of random graphs with the same parameters (Table [\ref=table-random]). The algorithm of Eppstein, Löffler, and Strash runs about 2 to 3 times slower than that of Tomita et al. on many of these graphs; this confirms that the algorithm is still competitive on graphs that are not sparse, in contrast to the competitors in Tomita et al.'s paper which ran 10 to 160 times slower on these input graphs. The largest of the random graphs in the second data set were generated with edge probabilities that made them significantly sparser than the rest of the set; for those graphs our algorithm outperformed that of Tomita et al by a factor that was as large as 30 on the sparsest of the graphs. The maxdegree algorithm was even faster than our algorithm in these cases, but it was significantly slower on other data.

Conclusion

We have shown that the algorithm of Eppstein, Löffler, and Strash is a practical algorithm for large sparse graphs. This algorithm is highly competitive with the algorithm of Tomita et al. on sparse graphs, and within a small constant factor on other graphs. The advantage of this algorithm is that it requires only linear space for storing the graph and all data structures. It does not suffer from the drawback of requiring an adjacency matrix, which may not fit into memory. Its closest competitor in this respect, the Tomita et al. algorithm modified to use adjacency lists, is sometimes faster by a small factor but is also sometimes slower by a large factor. Thus, the algorithm of Eppstein et al. is a fast and reliable choice for listing maximal cliques, especially when the input graphs are large and sparse.

For future work, it would be interesting to compare our results with those of other popular clique listing algorithms. We attempted to include results from Patric Östergå rd's popular Cliquer program [\cite=cliquer] in our tables; however, at the time of writing, its newly implemented functionality for listing all maximal cliques returns incorrect results.