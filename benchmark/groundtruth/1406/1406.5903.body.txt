Blind Sensor Calibration using Approximate Message Passing

, Francesco Caltagirone

and  Lenka Zdeborová

Introduction

Compressed sensing (CS) has made it possible to algorithmically invert an underdetermined linear system, provided that the signal to recover is sparse enough and that the mixing matrix has certain properties [\cite=CandesRombergTao06]. In addition to the theoretical interest raised by this discovery, CS is already used both in experimental research and in real world applications, in which it can lead to significant improvements. CS is particularly attractive for technologies in which an increase of the number of measurements is either impossible, as sometimes in medical imaging [\cite=lustig2007sparse] [\cite=otazo2010combination], or expensive, as in imaging devices that operate in certain wavelength [\cite=duarte2008single]. CS was extended to the setting in which the mixing process is followed by a sensing process which can be nonlinear or probabilistic, as shown in Fig. [\ref=fig:mixAndSense], with an algorithm called the generalized approximate message passing (GAMP) [\cite=Rangan10b]. This has opened new applications of CS, such as phase retrieval [\cite=schniter2012compressive].

One issue that can arise in CS is a lack of knowledge or an uncertainty on the exact measurement process. A known example is dictionary learning, where the measurement matrix [formula] is not known. The dictionary learning problem can also be solved with an AMP-based algorithm if the number P of available signal samples grows as N [\cite=krzakala2013phase].

A different kind of uncertainty is when the linear transformation [formula], corresponding to the mixing process, is known, but the sensing process is only known up to a set of parameters. In some cases, it might be possible to estimate these parameters prior to the measurements in a supervised sensor calibration process, during which one measures the outputs produced by known training signals, and in this way estimate the parameters for each of the sensors. In other cases, this might not be possible or practical, and the parameters have to be estimated jointly with a set of unknown signals: this is known as the blind sensor calibration problem. It is schematically shown on Fig. [\ref=fig:mixAndSenseCalP].

Some examples in which supervised calibration is impossible are given here:

For supervised calibration to be possible, one must be able to measure a known signal. This might not always be the case: in radio astronomy for example, calibration is necessary [\cite=rau2009advances], but the only possible observation is the sky, which is only partially known.

Supervised calibration is only possible when the system making the measurements is at hand, which might not always be the case. Blind image deconvolution is an example of blind calibration in which the calibration parameters are the coefficients of the imaging device's point spread function. It can easily be measured, but if we only have the blurred images and not the camera, there is no other option than estimating the point spread function from the images themselves, thus performing blind calibration [\cite=levin2009understanding].

For measurement systems integrated in embedded systems or smartphones, requiring a supervised calibration step before taking a measurement might be possible, but is not user-friendly because it requires a specific calibration procedure, which blind calibration does not. On the other hand, regular calibration might be necessary, as slow decalibration can occur because of aging or external parameters such as temperature or humidity.

Several algorithms have been proposed for blind sensor calibration in the case of unknown multiplicative gains, relying on convex optimization [\cite=GribonvalChardon11] or conjugate gradient algorithms [\cite=shen2013conjugate]. The Cal-AMP algorithm that we propose, and whose preliminary study was presented in [\cite=schulke2013blind], is based on GAMP and is therefore not restricted to a specific output function. Furthermore, it has the same advantages in speed and scalability as the approximate message passing (AMP), and thus allows to treat problems with big signal sizes.

Blind sensor calibration: Model and notations

Notations

In the following, vectors and matrices will be written using bold font. The i-th component of the vector [formula] will be written as ai. In a few cases, notations of the type [formula] are used, in which case [formula] is a vector itself, not the i-th component of vector [formula]. The complex conjugate of a complex number [formula] will be noted x*, and its modulo |x|. The transpose (resp. complex transpose) of a real (resp. complex) vector [formula] will be noted [formula]. The component-wise product between two vectors or matrices [formula] and [formula] will be noted [formula]. The notations [formula], and [formula] are component-wise divisions, and [formula]. We will call a probability distribution function (pdf) on a matrix or vector variable [formula] separable if its components are independently distributed: [formula]. Finally, we will write [formula] if p and f are proportional and we will write x  ~  pX(x) if x is a random variable with probability distribution function pX.

Measurement process

Let [formula] be a set of P signals [formula] to be recovered and N be their dimension: [formula]. Each of those signals is sparse, meaning that only a fraction ρ of their components is non-zero.

The measurement process leading to [formula] is shown in Fig. [\ref=fig:mixAndSenseCalP]. In the first, linear step, the signal is multiplied by a matrix [formula] and gives a variable [formula]

[formula]

or, written component-wise

[formula]

We will refer to α  =  M  /  N as the measurement rate. In standard CS, the measurement [formula] is a noisy version of [formula], and the goal is to reconstruct [formula] in the regime where the rate α < 1. In the broader GAMP formalism, [formula] is only an intermediary variable that cannot directly be observed. The observation [formula] is a function of [formula], which is probabilistic in the most general setting.

In blind calibration, we add the fact that this function depends on an unknown parameter vector [formula], such that the output function of each sensor is different,

[formula]

with

[formula]

and the goal is to jointly reconstruct [formula] and [formula].

Properties AMP for compressed sensing

It is useful to remind basic results known about the AMP algorithm for compressed sensing [\cite=DonohoMaleki09]. The AMP is derived on the basis of belief propagation [\cite=YedidiaFreeman03]. As is well known, belief propagation on a loopy factor graph is not in general guaranteed to give sensible results. However, in the setting of this paper, i.e. random iid matrix [formula] and signal with random iid elements of known probability distribution, the AMP algorithm was proven to work in compressed sensing in the the limit of large system size N as long as the measurement rate α  ≥  αCS(ρ) [\cite=DonohoMaleki09] [\cite=BayatiMontanari10] [\cite=DonohoMaleki10] [\cite=KrzakalaMezard12]. The threshold αCS(ρ) is a phase transition, meaning that in the limit of large system size, AMP fails with high probability up to the threshold αCS(ρ)∈(ρ,1) and succeeds with high probability above that threshold.

Technical conditions

The technical conditions necessary for the derivation of the Cal-AMP algorithm and its good behavior are the following:

Ideally, the prior distributions of both the signal, [formula], and the calibration parameters, [formula], are known, such that we can perform Bayes-optimal inference. As in CS, a mismatch between the real distribution and the assumed prior will in general affect the performance of the algorithm. However, parameters of the real distribution can be learned with expectation-maximization and improve performance [\cite=KrzakalaMezard12].

The Cal-AMP can be tested for an arbitrary operator [formula]. However, in its derivation we assume that [formula] is an iid random matrix, and that its elements are of order [formula], such that [formula] is O(1) (given that [formula] is O(1)). The mean of elements of [formula] should be close to zero for the AMP-algorithms to be stable, in the opposite case the implementation has to be adjusted by some of the methods known to fix this issue [\cite=caltagirone2014convergence].

The output function [formula] has to be separable, as well as the priors on [formula] and [formula]. This condition could be relaxed by using techniques similar to those allowing to treat the case of structured sparsity in [\cite=rangan2012hybrid].

Under the above conditions we conjecture that in the limit of large system sizes the Cal-AMP algorithm matches the performance of the Bayes-optimal algorithm (except in a region of parameters where the Bayes-optimal fixed point of the Cal-AMP is not reached from an non-informed initialization, the same situation was described in compressed sensing [\cite=KrzakalaMezard12]). This conjecture is based on the insight from the theory of spin glasses [\cite=MezardMontanari09], and it makes the Cal-AMP algorithm stand out among other possible extensions of GAMP that would take into account estimation of the distortion parameters. Proof of this conjecture is a non-trivial challenge for future work.

Relation to GAMP and some of its existing extensions

Cal-AMP algorithm can be seen as an extension of GAMP [\cite=Rangan10b].

Cal-AMP reduces to GAMP for the particular case of a single signal sample P = 1. Indeed, if the measurement yμ depends on a parameter dμ via a probability distribution function pY|Z,D, then pY|Z can be expressed by:

[formula]

When, however, the number of signal samples is greater than one, P > 1, the two algorithms differ: while GAMP treats the P signals independently, leading to the same reconstruction performances no matter the value of P, Cal-AMP treats them jointly. As our numerical results show, this can lead to great improvements in reconstruction performances, and can allow exact signal reconstruction in conditions under which GAMP fails.

One work on blind calibration that used a GAMP-based algorithm is [\cite=kamilov2013autocalibrated], where the authors combine GAMP with expectation maximization-like learning. That paper, however, considers a setting different from ours in the sense that the unknown gains are on the signal components not on the measurement components. Whereas both these cases are relevant in practice, from an algorithmic point of view they are different.

Another work where distortion-like parameters are included and estimated with a GAMP-based algorithm is [\cite=schniter2011message] [\cite=nassar2014factor]. Authors of this work consider two types of distortion-like parameters. Parameters S that are sample-dependent and hence their estimation is more related to what is done in the matrix factorization problem rather than to the blind calibration considered here. And binary parameters b that are estimated independently of the main loop that uses GAMP. The problem considered in that work requires a setting and a factor graph more complex that the one we considered here and it is far from transparent what to conclude about performance for blind calibration from the results presented in [\cite=schniter2011message] [\cite=nassar2014factor].

The Cal-AMP algorithm

In this section, we give details of the derivation of the approximate message passing algorithm for the calibration problem (Cal-AMP). It is closely related to the AMP algorithm for CS [\cite=DonohoMaleki09] and the derivation was made using the same strategy as in [\cite=KrzakalaMezard12]. First, we express the blind sensor calibration problem as an inference problem, using Bayes' rule and an a priori knowledge of the probability distribution functions of both the signal and the calibration parameters. From this, we obtain an a posteriori distribution, which is peaked around the unique solution with high probability. We write belief propagation equations that lead to an iterative update procedure of signal estimates. We realize that in the limit of large system size the algorithm can be simplified by working only with the means and variances of the corresponding messages. Finally, we reduce the computational complexity of the algorithm by noting that the messages are perturbed versions of the local beliefs, which become the only quantities that need updating.

Probabilistic approach and belief propagation

We choose a probabilistic approach to solve the blind calibration problem, which has been shown to be very successful in CS. The starting point is Bayes' formula that allows us to estimate the signal [formula] and the calibration parameters [formula] from the knowledge of the measurements [formula] and the measurement matrix [formula], assuming that [formula] and [formula] are statistically independent,

[formula]

Using separable priors on [formula] and [formula] as well as separable output functions, this posterior distribution becomes

[formula]

where Z is the normalization constant. Even in the factorized form of ([\ref=posterior]), uniform sampling from this posterior distribution becomes intractable with growing N.

Representing ([\ref=posterior]) by the factor graph in Fig. [\ref=factorGraph] allows us to use belief propagation for approximate sampling. As the factor graph is not a tree, there is no guarantee that running belief propagation on it will lead to the correct results. Relying on the success of AMP in compressed sensing and the insight from the theory of spin glasses [\cite=MezardMontanari09], we conjecture belief propagation to be asymptotically exact in blind calibration as it is in CS.

In belief propagation there are two types of pairs of messages: (ψ,ψ̃) and (φ,φ̃), connected to the signal components and to the calibration parameters respectively. Their updating scheme in the sum-product belief propagation is the following [\cite=YedidiaFreeman03]: for the (φ,φ̃) messages,

[formula]

whereas for the (ψ,ψ̃) messages,

[formula]

When belief propagation is successful, these messages converge to a fixed point, from which we obtain the marginal distribution of [formula] sampled with ([\ref=posterior]):

[formula]

These distributions are called beliefs, and from them we obtain the minimal mean square error (MMSE) estimator:

[formula]

Simplifications in the large N limit

The above update equations are still intractable, given the fact that in general, xil and dμ are continuous variables. In the large N limit, the problem can be greatly simplified by making leading-order expansions of certain quantities as a function of the matrix elements Fμi, that are of order [formula]. The notation O(Fμi) is therefore equivalent to [formula].

This allows to pass messages that are estimators of variables and of their uncertainty, instead of full probability distributions. The table in Fig. {[\ref=table:notations] is a summary of notations used and their significations: estimators of variables are noted with a hat, whereas their uncertainties are noted with a bar.

The messages can then be expressed in simpler ways by using Gaussians. As these will be ubiquitous in the rest of the paper, let us introduce the notation

[formula]

and note the expression of the following derivative:

[formula]

We will also use convolutions of a function g, with optional parameters {u}, with a Gaussian

[formula]

and from ([\ref=eq:gaussianderivate]), we obtain the relations

[formula]

Let us show how simplifications come about in the large N limit. Both in ([\ref=mess_n_tilde]) and in ([\ref=mess_m_tilde]), the term [formula] appears. zμl is a sum of the N random variables Fμixil, and each xil is distributed according to the distribution ψtil  →  μl(xil). Let us call x̂il  →  μl and x̄il  →  μl the means and variances of these distributions,

[formula]

In the N  →    ∞   limit, we can use the central limit theorem, as the assumption of independence of the variables is already made when writing the belief propagation equations. Then, zμl has a normal distribution with means and variances:

[formula]

In ([\ref=mess_n_tilde]) we therefore obtain

[formula]

where fZ0(Ẑ,,y,d) is a lighter notation for fpY|Z,D0(Ẑ,,{y,d}) given by the formula in ([\ref=gaussConvs]).

For the φ messages, we obtain that

[formula]

The same procedure can be applied to the ψ̃ messages, the only difference being that xil is fixed, leading to

[formula]

with

[formula]

In analogy to the functions defined in ([\ref=gaussConvs]), we introduce the functions of the P-dimensional vectors [formula], [formula] and [formula]:

[formula]

and as for the functions fk, we can use ([\ref=eq:gaussianderivate]) to show that

[formula]

With these functions, we define new estimators ẑ and [formula] of z:

[formula]

Here, we use [formula] as a compact notation for the P-dimensional vector {Ẑt + 1μl  →  il,{Ẑtμm}m  ≠  l}, similarly for [formula], and [formula] for the P-dimensional vector {yμl,{yμm}m  ≠  l}}. In appendix [\ref=app:A], we show how we can obtain the following approximation for the ψ messages:

[formula]

with

[formula]

In the N  →    ∞   limit, the means and variances of ψμl  →  il(xil) are therefore given by:

[formula]

where we have simplified the notations pX and pX to X and X.

Resulting update scheme

The message passing algorithm obtained by those simplifications is an iterative update scheme for means and variances of Gaussians. Given the variables at a time step t, the first step consists in producing estimates of [formula]:

[formula]

This step is purely linear and produces estimates Ẑt + 1μl of zμl along with estimates of the incertitude t + 1μl. The corresponding variables with arrows exclude one term of the sum, and are necessary in the belief propagation algorithm.

The next step produces a new estimate of [formula] from a nonlinear function of the previous estimates and the measurements [formula]:

[formula]

Next, the previous estimates of [formula] are used in a linear step producing new estimates of [formula]:

[formula]

Finally, a nonlinear function is applied to these estimates in order to take into account the sparsity constraint:

[formula]

TAP algorithm with reduced complexity

In the previous message passing equations, we have to update O(MPN) variables at each iteration. It turns out that this is not necessary, considering that the final quantities we are interested in are not the messages x̂il  →  μl, but rather the local beliefs x̂il. With that in mind, we can use again the fact that Fμi is small to make expansions that will reduce the number of variables to actually update. Similarly to the messages ([\ref=eq:mess_z_bar]), ([\ref=eq:mess_z_hat]), ([\ref=mess_a]) and ([\ref=mess_v]), we define following quantities:

[formula]

with

[formula]

and

[formula]

Note that x̂il is the MMSE estimator defined in ([\ref=eq:MMSE]) and x̄il is the variance of the local belief ([\ref=eq:beliefsx]). ẑμl and μl are defined in analogy.

We can then write the ẑμl  →  il as perturbations around ẑμl using the relations ([\ref=eq:gderivatives]). It is sufficient to compute the first order corrections with respect to the matrix elements Fμi, as those lead to corrections of order 1 once summed. On the other hand, the corrective terms of higher order will remain of order [formula] or smaller once summed, and do therefore not need to be explicitly calculated. This gives:

[formula]

and we can do the same for the x̂il  →  μl messages, written as perturbations around x̂il using the relations ([\ref=eq:fderivatives])

[formula]

Using each of these equations in the other one, we obtain the perturbations:

[formula]

In the N  →    ∞   limit, we therefore have

[formula]

This makes it possible to evaluate Ẑμl and X̂il with only the local beliefs x̂il and variances x̄il, such that in the N  →    ∞   limit,

[formula]

With those steps made, we can greatly simplify the complexity of the message passing algorithm. The resulting version of algorithm [\ref=algo:TAPMatrix] is called "TAP" version, referring to the Thouless-Anderson-Palmer equations used in the study of spin glasses [\cite=ThoulessAnderson77] with the same technique.

Note that in this general version, we do not explicitly calculate estimates of dμ. The initialization can also be chosen using the probability distributions pX and pY|Z,D, but random initialization provides good results. The use of the notations X, X, ĝ and [formula] is abusive and refers to their component-wise use in ([\ref=eq:beliefs]). The algorithm remains valid for complex variables, in which case (.)T indicates complex transposition.

Comparison to GAMP and perfectly calibrated GAMP

When P = 1, Cal-AMP is strictly identical to GAMP, with:

[formula]

For P > 1, the step involving ĝ and [formula] is the only one in which the P samples are not treated independently.

If it is possible to perform perfect calibration of the sensors by supervised learning, one can replace the prior distribution pD(dμ) in the expressions for ĝ and [formula] by δ(dμ  -  dcalμ). In that case ĝ and [formula] can be calculated independently for the P samples, and Cal-AMP is once again identical to GAMP with perfectly calibrated sensors, which leads to:

[formula]

Note that GAMP is usually written in a different way using

[formula]

Damping scheme

The stability of the algorithm can be improved with damping scheme proposed in [\cite=heskes2003stable], which corresponds to damping the variances ,X̄ and the means Ẑ,X̂ with the following functions:

[formula]

where β∈(0,1], β' = βvart + 1  /  var0t + 1 and the quantities with index 0 are before damping.

Examples of applications

In this section, we give two examples of how a sensor could introduce a distortion via the function pY|Z,D.

Faulty sensors

In the non-CS case, the following setting has been studied before in the context of wireless sensor networks, for example in [\cite=lo2013efficient] [\cite=farruggia2011detecting]. For one signal sample P = 1 this was also treated by GAMP in [\cite=ziniel2014binary].

We assume that a fraction ε of sensors is faulty (denoted by dμ = 0) and only records noise ~  N(yμl;mf,σf), whereas the other sensors (with dμ = 1) are functional and record zμl. We then have

[formula]

and this leads to analytical expressions for the functions ĝ and [formula], given in appendix [\ref=app:B].

If mf and σf are sufficiently different from the mean and variance of the measurement taken by working sensors, the problem can be expected to be easy. But if mf and σf are exactly the mean and variance of the measurements taken by working sensors, nothing indicates which are the faulty sensors. The algorithm thus has to solve a problem of combinatorial optimization consisting in finding which sensors are faulty.

Perfect calibration: If the sensors have been calibrated before, the problem can be solved by a CS algorithm that discards the fraction ε of the measurements corresponding to the faulty sensors, leading to an effective measurement rate αeff  =  α(1 - ε). The algorithm would then succeed in finding the solution if αeff  >  αCS. Therefore a perfectly calibrated algorithm would have a phase transition at:

[formula]

Results of numerical experiments are presented on Fig. [\ref=faultySensors], and show the comparisons with the perfectly calibrated case as well as the increase in performance as the number of samples P grows.

Gain calibration

In this setting, studied in [\cite=GribonvalChardon11] [\cite=schulke2013blind], each sensor multiplies the component zμl by an unknown gain d- 1μ. One possible application is in the context of time-interleaved ADC converters, where gain calibration has been studied before [\cite=saleem2011adaptive]. In noisy real gain calibration, the measurement process at each sensor is given by

[formula]

with w being Gaussian noise of mean 0 and variance Δ. Then the output channel is

[formula]

and from this we can obtain that:

[formula]

This allows us to calculate ĝ and [formula], for which we obtain

[formula]

with

[formula]

where D stands for |d|PpD(d).

Perfect calibration: In this setting, if the sensors have been perfectly calibrated beforehand, the problem is equivalent to compressed sensing, therefore

[formula]

Another interesting lower bound for the necessary number of measures can be found. Consider an oracle algorithm that knows the location of all the zeros in the signal, but not the calibration coefficients. For each of the M sensors, the P measurements can be combined into P - 1 independent equations of the type:

[formula]

There are M(P - 1) such linear equations and PρN unknowns (as the algorithm knows all the zeros), therefore it can find the solution only if M(P - 1) > PρN, which leads to the lower bound:

[formula]

Complex gain calibration: Cal-AMP also applies to the setting where [formula], [formula], [formula] and [formula] are complex instead of real. The algorithm is the same, with the difference that the update functions f and g calculated with priors on complex numbers and with complex instead of real normal distributions.

Experimental results

Fig. [\ref=faultySensors] and [\ref=gainCalibration] show the results of numerical experiments made for the faulty sensors problem and the gain calibration problem. All experiments were carried out on synthetic data and with priors matching the real signal distributions,

[formula]

and the corresponding update functions X and X have analytical expressions, given in appendix [\ref=app:B].

Effects of prior mismatch for CS has been studied in [\cite=KrzakalaMezard12], as well as the possibility to learn parameters of the priors with expectation-maximization procedures. The measurement matrix was taken with random iid Gaussian elements with variance 1  /  N, such that [formula] is of order one,

[formula]

A MATLAB implementation of Cal-AMP algorithm [\ref=algo:TAPMatrix] was used. It is available at . For the priors used in the experiments, the integrals in [formula] and [formula] have simple analytical expressions, and therefore the computational cost of the algorithm is dominated by matrix multiplications.

In order to assess the quality of the reconstruction on synthetic data, we will look at the normalized cross-correlation between the generated and the reconstructed signal, [formula] and [formula]: used for instance in [\cite=gribonval2012blind] [\cite=corbella2000analysis]:

[formula]

where we have used the empirical means

[formula]

Choosing this evaluation metric instead of the mean square error (MSE) allows to take into account the fact that in some applications, there are ambiguities that are unliftable, in which case the MSE might be a poor indicator of success and failure. This is the case for complex gain calibration, where signal and calibration coefficients can only be recovered up to a global phase at best, and for real gain calibration in case of a mismatching prior pD. The normalized cross-correlation μ tends to 1 for a perfect reconstruction, and it is therefore convenient to look at the quantity log 10(1 - μ). In all phase diagrams, the horizontal axis is the sparsity ρ of the signal and the vertical axis is the measurement rate α.

Faulty sensors

Fig. [\ref=faultySensors] shows the results of experiments made on the faulty sensors problem. For a fraction ε of the sensors, the measurements are replaced by noise, such that if sensor μ is faulty, then

[formula]

independently of zμl. In order to consider the hardest case, in which these measurements have the same distribution as zμl, we take the mean and variance to be

[formula]

The results correspond well to the analysis made previously. GAMP can be applied and allows perfect reconstruction in some cases. However, using Cal-AMP and increasing P allows to close the gap to the performances of a perfectly calibrated algorithm.

Real gain calibration

For the numerical experiments, the distribution chosen for the calibration coefficients was a uniform distribution centered around 1 and width wd < 2,

[formula]

Experiments were made with a very low noise (Δ  =  10- 15), as taking Δ = 0 leads to occasional diverging behavior of the algorithm. A damping coefficient β = 0.8 was used, increasing the stability of the algorithm, while not slowing it down significantly.

Bayes-optimal update functions

In that case, the update functions fD can be expressed analytically:

[formula]

with

[formula]

where Γ is the gamma function, γ is the incomplete gamma function

[formula]

and σix is 1 if i is even and the sign of (x - R) if i is uneven.

Note that the fact that this prior has a bounded support can lead to a bad behavior of the algorithm. However, using a slightly bigger wd (by a factor 1.1 in our implementation) in the prior than in the distribution used for generating [formula] solves this issue.

Results

Fig. [\ref=gainCalibration] shows the results in the case of the gain calibration problem. Here, signal recovery is impossible for P = 1. Furthermore, for P > 1, the empirical phase transition closely matches the lower bounds given by an uncalibrated oracle algorithm ([\ref=eq:alpha_min]) and a perfectly calibrated algorithm ([\ref=eq:alphaCalGain]). Note that the exact position of the phase transition depends on the amplitude of the decalibration, given by wd, as illustrated on Fig. [\ref=gainCalibrationDetails].

Fig. [\ref=L1CalAMP] shows the comparison of performances of Cal-AMP with the algorithm relying on convex optimization used in [\cite=GribonvalChardon11]. Such an approach is possible in the case of gain calibration because the equation

[formula]

is convex both in dμ and in xil. However, such a convex formulation is specific to this particular output model and is not generalizable to every type of sensor-induced distortion. The algorithm is implemented very easily using the CVX package [\cite=cvx] by entering ([\ref=cvxeq]) and adding an L1 regularizer on [formula]. The figure shows that Cal-AMP needs significantly less measurements for a successful reconstruction, and as shown in Fig. [\ref=gainCalibrationDetails], it is also substantially faster than its L1 counterpart.

Complex gain calibration

For the numerical experiments, the distribution chosen for the calibration coefficients, the signal and the measurement matrix use the complex normal distribution with mean R and variance Σ, which we note (x;R,Σ) :

[formula]

The corresponding Bayes-optimal update functions X and X have analytical expressions [\cite=barbier2013compressed], given in appendix [\ref=app:B]. For the update functions D and D, we use

[formula]

Though not Bayes-optimal, they lead to good results, presented in Figure [\ref=fig:ccal].

Conclusion

In this paper, we have presented the Cal-AMP algorithm, designed for blind sensor calibration. Similar to GAMP, the framework allows to treat a variety of different problems beyond the case of compressed sensing. The derivation of the algorithm was detailed, starting from the probabilistic formulation of the problem and the message-passing algorithm derived from belief propagation. Two examples of problems falling into the Cal-AMP framework were studied numerically. Both for the faulty sensors problem and the gain calibration problem, the performance of Cal-AMP was found to be close to problem-specific lower bounds.

Cal-AMP could find concrete applications in experimental setups using physical devices for data acquisition, in which the ability to blindly calibrate the sensors might be either indispensable for good results, or allow substantial cuts in hardware costs.

In compressed sensing the asymptotic behavior of the AMP algorithm was analyzed via the state evolution equations [\cite=DonohoMaleki09] [\cite=BayatiMontanari10]. We attempted to derive the corresponding theory for Cal-AMP, but even on the heuristic level the corresponding generalization turns out to be non-trivial. This analysis is hence left as an interesting open problem.

Approximation of ψ

We start by rewriting the messages ([\ref=eq:exactmesstildex]) using the function g0 introduced in ([\ref=eq:gfunctions]):

[formula]

where [formula] is a P-dimensional vector with first component Ẑt + 1μl  →  il, and its other components are Ẑtμm for m  ≠  l. The definition of [formula] is the same, replacing Ẑ by [formula], and [formula] is the P-dimensional vector with first component yμl and other components yμm with m  ≠  l. Notice that due to the definition of g0, the order of the components 2 to P of those vectors is unimportant as long as it is the same for each of them. [formula] is the unit vector along the first direction of the P-dimensional space. Making a Taylor expansion of ([\ref=eq:taylor_messx]), we obtain

[formula]

Let us now note that, for a and b of order one,

[formula]

We can now identify the coefficients of the expansion ([\ref=eq:expansion]) with those in ([\ref=eq:exp2]) to approximate the messages ψ̃(xil) as Gaussians around Fμixil = 0, with mean p̂ and variance [formula]:

[formula]

were p̂ and [formula] have following expressions, found by expressing the derivatives of g0 with the functions ĝ and [formula] using the relations ([\ref=eq:gderivatives]):

[formula]

This expression ([\ref=simple_tilde_m]) can now be used in ([\ref=mess_m]):

[formula]

The product of Gaussians that appears is proportional to another Gaussian. In fact,for any product of Gaussians,

[formula]

with

[formula]

Moreover, the logarithm of the second product is [formula], so the product is [formula]. The messages ψ can therefore be written in the following way:

[formula]

with

[formula]

Analytical expressions of update functions

Faulty sensors problem

ẑ and [formula] are obtained from the functions ĝ and [formula] such that:

[formula]

with

[formula]

For Bernouilli-Gauss prior

the update functions [formula] and [formula] corresponding to the priors ([\ref=eq:bernouilligaussprior]) and ([\ref=eq:complexXdist]) can be found in [\cite=KrzakalaMezard12] and [\cite=barbier2013compressed] and obtained from:

[formula]

In the complex case, all N are replaced by [formula].