Theorem Corollary Proposition

Beta and Kumaraswamy distributions as non-nested hypotheses in the modeling of continuous bounded data

Introduction

In Statistics, the beta distribution is a well-known and established model to fit continuous bounded data. A random variable X following a beta distribution with shape parameters a and b has density function given by

[formula]

where [formula] is the beta function; we denote X  ~  B(a,b). We restrict our attention to the interval (0,1) since a beta distribution on an interval (c,d) (with c < d) is obtained by the simple linear transformation (d - c)X + c.

As an alternative to the beta distribution, [\citet=kw1980] introduced a two-parameter distribution on (0,1), the so-called Kumaraswamy distribution. A random variable Y following a Kumaraswamy distribution has density given by

[formula]

where α > 0 and β > 0 are shape parameters. We denote Y  ~  K(α,β). Similarly as discussed above, we also restrict our attention to the Kumaraswamy distribution on the interval (0,1). The Kumaraswamy distribution was initially proposed for applications in hydrology. Since then, it has been frequently used in several areas of Statistics in the last years. For instance, see the most recent papers by [\citet=nad2008], [\citet=jones2009], [\citet=lem2011], [\citet=mit2013], [\citet=mitbae2013] and the references contained therein. One factor for this increased interesting on the Kumaraswamy distribution is due to its simple mathematical form of the distribution function, in constrast with the beta distribution. On the other hand, the ordinary moments of the beta distribution are obtained explicitly, while those of the Kumaraswamy distribution depend on the gamma function. There exist several advantages (and evidently disadvantages) of the Kumaraswamy distribution over the beta distribution. We recommend the paper by [\citet=jones2009] to the readers interested in a detailed comparison between the beta and Kumaraswamy distributions.

Nowadays, the beta and Kumaraswamy distributions are the most popular models to fit continuous bounded data. Further, these models have many features in common and in a practical situation one question of interest is how to select the most adequate model (between the beta and Kumaraswamy distributions) to fit a certain continuous bounded data set. To the best of our knowledge, it does not exist a way to discriminate the beta and Kumaraswamy models. In practical situations, the Akaike criteria has been used to do this, but this relies only on checking what is the model with great value of the maximized log-likelihood (since both have the same number of parameters).

Our chief goal in this paper is to propose a selection criterion between the beta and Kumaraswamy distributions based on the asymptotic distribution of the likelihood ratio statistic proposed by [\citet=cox1961] [\citet=cox1962]. With this, we obtain the probability of correct selection under the hypotheses that the data comes from the beta or Kumaraswamy distributions and select the model that maximizes it.

In a pioneering work, [\citet=cox1961] [\citet=cox1962] proposed a way to discriminate non-nested families of hypotheses. The test statistic is the logarithm of the ratio of the maximized log-likelihoods under both null and alternative hypotheses. This statistic is compared with its expected value under the null hypothesis. Small deviations of the expected mean imply evidences in favor of the null hypothesis, while large deviations indicate evidences against. In a non-rigorous way, [\citet=cox1962] showed that the normalized logarithm of the ratio of the maximized log-likelihoods is asymptotically normal distributed. Regularity conditions and a rigorous proof of the asymptotic normality of the Cox's test statistic was provided by [\citet=white1982].

The major part of the works dealing on this subject lies in discriminating between two non-nested lifetime distributions. For instance, see the papers by [\citet=baieng1980], [\citet=feaneb1991], [\citet=gupkun2004], [\citet=kunetal2005], [\citet=deykun2012] and [\citet=barsil2013]. References about discrimination between separate families of hypotheses are widespread and we recommend the reader to see references contained in the above papers.

This paper is outlined as follows. In Section [\ref=Likelihoodratiotest] we present the test statistic to discriminate beta and Kumaraswamy models and obtain its asymptotic distribution under two null hypotheses (that are, data come from the beta or Kumaraswamy distributions). In Section [\ref=selectionmodel] we present our selection criterion based on the results given in the previous section. The minimum sample size required to discriminate beta and Kumaraswamy distributions when the probability of correct selection is beforehand is provided in Section [\ref=mss]. Simulation issues and two applications to real data sets involving proportions are presented in Sections [\ref=simulation] and [\ref=applications], respectively.

Asymptotic distribution of the likelihood ratio statistic

Let [formula] be a sequence of independent and identically distributed (iid) random variables, with observed values [formula], either from a B(a,b) distribution or K(α,β) distribution, with densities given by ([\ref=pdfbeta]) and ([\ref=pdfkw]), respectively. These hypotheses are denoted by

[formula]

The log-likelihood function associated to the beta distribution is given by

[formula]

The maximum likelihood estimates (MLEs) ân and n of a and b, respectively, are obtained as solutions of the nonlinear equations

[formula]

where ψ(  ·  ) is the digamma function. On the other hand, the log-likelihood function corresponding to the Kumaraswamy distribution is

[formula]

The MLEs n and n of α and β, respectively, are the solution of the nonlinear system of equations

[formula]

With the above results, we define our test statistic by

[formula]

where (ân,n) and (n,n) are the MLEs of (a,  b) and (β,α), respectively. In words, our test statistic is the difference between the maximized log-likelihoods. Since both models have the same number of parameters, this corresponds to the Akaike statistic ([\citet=aka1974]). More explicitly, the statistic Tn can be expressed as

[formula]

In practical situations, based on the Akaike criterion, the following selection criterion is commonly adopted: we choose the beta distribution if Tn > 0, otherwise we choose the Kumaraswamy distribution. We here adopt a different selection criterion, which is based on the asymptotic distribution of a normalized version of Tn under the hypotheses HB and HK. This criterion will be present in the next section. Now we concentrate our attention to find the asymptotic distribution of the test statistic. We now define some function which will appear along the paper. For x,y,z > 0, define the real functions

[formula]

[formula]

[formula]

[formula]

and

[formula]

where ψ'(  ·  ) is the first derivative of the digamma function ψ(  ·  ).

Beta distribution as the null hypothesis

In this subsection we present the asymptotic distribution of Tn under the hypothesis HB. The alternative hypothesis is HK. Suppose that the random variables [formula] come from the B(a,b) distribution. For any Borel measurable function h(  ·  ), the underscript B in EB(h(X1)) means that the expectation is taken with respect to the beta distribution with density given by ([\ref=pdfbeta]). More explicitly, we have [formula].

Under the hypothesis HB, as n  →    ∞   we have that

(i) ân  →  a and n  →  b almost surely, where

[formula]

(ii) [formula] and [formula] almost surely, where

[formula]

The quasi-maximum likelihood estimators [formula] and [formula] are functions of a and b, which is not explicited in order to simplify the notation. The above convergences follow from the results stated and proved by [\citet=white1982b].

We now discuss how to obtain [formula] and [formula]. Define [formula]. We have that

[formula]

With this, we have that [formula] and [formula] are obtained as the solution of the system of nonlinear equations [formula], that is

Now, in order to present the asymptotic distribution of the test statistic Tn under HB, we need to compute the mean and variance of the random variable [formula] (with X  ~  B(a,b)), which we will be denoted by B(a,b) and B(a,b), respectively.

An explicit expression for B(a,b) is given by

[formula]

where the real function F(  ·  ,  ·  ,  ·  ) was defined in ([\ref=F]). The variance B(a,b) is given by

[formula]

where the variances and covariances above can be expressed by

[formula]

with F(  ·  ,  ·  ,  ·  ), M(  ·  ,  ·  ,  ·  ), V(  ·  ,  ·  ,  ·  ) and W(  ·  ,  ·  ,  ·  ) as defined in ([\ref=F]), ([\ref=M]), ([\ref=V]) and ([\ref=W]), respectively.

Table [\ref=numericbe] lists the values of B(a,b), B(a,b), [formula] and [formula] for b = 2.5 and some values of the parameter a.

We now present the asymptotic distribution of n- 1 / 2(Tn - EB(Tn)). Define [formula].

Under the null hypothesis HB, we have that

[formula]

as n  →    ∞  , where B(a,b) and B(a,b) are given by ([\ref=AMbe]) and ([\ref=AVbe]), respectively, and "~  " denotes "asymptotically equivalent".

We now justify that the above result is in fact true. From the Central Limit Theorem, it follows that [formula] as n  →    ∞  . Therefore, the major work in proving ([\ref=theorem-be]) lies in showing the asymptotic equivalence between n- 1 / 2(Tn  -  B(Tn)) and [formula]. This follows from an adaptation of the results given in [\citet=white1982]. This adaptation is made in [\citet=barsil2013] for the discriminating between the exponential-Poisson and gamma distributions. Following exactly as made there, the results here presented follows.

Kumaraswamy distribution as the null hypothesis

We now suppose that HK and HB are the null and alternative hypotheses, respectively. Let [formula] be iid random variables following a K(α,β) distribution. Similarly as in the previous case, for any Borel measurable function h(  ·  ), the underscript K in [formula] means that the expectation is taken with respect to the Kumaraswamy distribution with density given in ([\ref=pdfkw]), that is, [formula].

Under the hypothesis HK, as n  →    ∞   we have that

(i) n  →  α and n  →  β almost surely, where

[formula]

(ii) [formula] and [formula] almost surely, where

[formula]

As before, we call attention of the reader that the quasi-maximum likelihood estimators [formula] and [formula] are functions of α and β, which is not explicited for brevity.

We now show how to obtain [formula] and [formula]. Define [formula]. We have that

[formula]

Hence, [formula] and [formula] are obtained as solution of the system of nonlinear equations [formula]. These equations are given by

[formula]

We now compute the mean and variance of the random variable [formula] (with X  ~  K(α,β)), which we will denote by K(α,β) and K(α,β), respectively. As in the previous case, these results will be important to present the asymptotic distribution of the test statistic Tn under HK. After some algebra, it can be shown that these quantities can be expressed by

[formula]

and

[formula]

where the variances and covariances above can be expressed by

[formula]

The real functions F(  ·  ,  ·  ,  ·  ), M(  ·  ,  ·  ,  ·  ) and V(  ·  ,  ·  ,  ·  ) that appear above are defined in ([\ref=F]), ([\ref=M]) and ([\ref=V]), respectively. Table [\ref=numerickw] presents the values of K(α,β), K(α,β), [formula] and [formula] for β = 2.5 and some values of the parameter α.

Define now the quantity [formula]. Under the hypothesis HK, we have that

[formula]

as n  →    ∞  , where K(α,β) and K(α,β) are given by ([\ref=AMkw]) and ([\ref=AVkw]), respectively. As before, "~  " denotes "asymptotically equivalent". The justification of the validality of the convergence given in ([\ref=theorem-kw]) is exactly the same of the justification of the result ([\ref=theorem-be]).

Selection criterion

With the results presented in the previous section, we are ready to give our selection criterion. For this, let us first to present asymptotic forms for the probabilities of correct selection (in short PCS) [formula] and [formula] under the hypotheses HB and HK, respectively.

Assume that the null and alternative hypotheses are HB and HK, respectively. From the result ([\ref=theorem-be]), we have that PCSB(a,b) may be approximated by

[formula]

where Φ(  ·  ) is the distribution function of the standard normal distribution and B(a,b) and B(a,b) are given in ([\ref=AMbe]) and ([\ref=AVbe]), respectively.

Now consider the null and alternative hypotheses are HB and HK, respectively. Based on the convergence in distribution given in ([\ref=theorem-kw]), we have that PCSK(α,β) may be approximated by

[formula]

where Φ(  ·  ) is the distribution function of the standard normal distribution and K(α,β) and K(α,β) are given in ([\ref=AMkw]) and ([\ref=AVkw]), respectively.

The probabilities of correct selection ([\ref=pcsbe]) and ([\ref=pcskw]) depend on the parameters. In practice, we replace the parameters by their maximum likelihood estimators. With this, we define our selection criterion as follows:

If PCSB(â,) > PCSK(,), choose the beta distribution, otherwise select the Kumaraswamy distribution, where (â,) and (,) are respectively the maximum likelihood estimators of (a,b) and (α,β) given in the previous section.

The above selection criterion is alternatively equivalent to the following one:

If [formula], choose the beta distribution, otherwise select the Kumaraswamy distribution.

Distances and minimum sample size

We now propose a method to determine the minimum sample size required in order to discriminate between the beta and Kumaraswamy distributions for a specified PCS and a given tolerance level, which is defined in terms of some distance to measure the closeness between the beta and Kumaraswamy distributions.

There are several ways to measure the closeness or the distance between two probability distributions. The most common measures are the Kolmogorov-Smirnov ([formula]) and Hellinger (H) distances and we will use both in this paper.

Let f and g (with same support Ω) be two absolutely continuous density functions with distribution functions F(x) and G(x), respectively. The Kolmogorov-Smirnov distance between F and G is given by

[formula]

The Hellinger distance between f and g is defined by

[formula]

It is not possible to find an explicit expression for the Kolmogorov-Smirnov distance in our case. On the other hand, we find an explicit expression for the Hellinger distance between beta and Kumaraswamy distributions, that is

[formula]

The above expression can be obtained by using the binomial expansion in (1 - xα)(β - 1) / 2 in [formula] and hence using the Dominate Convergence Theorem.

For small distances between two probability distributions, it is expected that the minimum sample size required to discriminate them be large. Otherwise, a small or moderate sample size is sufficient to discriminate the models. We assume that the user will specify beforehand the PCS and the tolerance level in terms of the distance between the beta and Kumaraswamy distributions. When a tolerance level is specified (by means of some distance), the two distribution functions are not considered to be significantly different if their distance does not exceed the tolerance level. PCS and tolerance level play a similar role that the power and Type-I error in the corresponding testing of hypotheses problem.

Based on PCS and tolerance level we can determine the minimum sample size required to discriminate between the beta and Kumaraswamy distributions. The tolerance level here is defined for the [formula] and H distances. We are now interested in finding the required sample size n such that PCS achieves a certain protection level p for a stated tolerance level D.

We explain the procedure under the null hypothesis HB. The procedure under HK follows in a similar way and therefore is omitted.

To determine the sample size needed to achieve at least a protection level p, we equate B(a,b) = p. Hence, using the asymptotic result given in ([\ref=pcsbe]) we get

[formula]

By solving for n we obtain

[formula]

where zp is the 100p percentile point of the standard normal distribution and

[formula]

Numerical experiments

Simulation

We here perform some numerical experiments to observe how our asymptotic results derived in Section [\ref=Likelihoodratiotest] work for different sample sizes. We are interested in comparing the asymptotic PCSs under the hypothesis HB and HK with respect to the simulated probabilities based on Monte Carlo simulations.

Let us now to describe how the simulated results are obtained. We begin with the case where the null hypothesis is HB. The following procedure holds in a similar way for the null hypothesis HK and therefore is omitted. Let N be the number of loops of the Monte Carlo simulation and [formula] be a vector of length N. The steps, for each loop j, are as follows:

Generate a random sample from the B(a,b) distribution with size n;

Find the MLEs of (a,b) and (α,β) based on the beta and Kumaraswamy distributions, respectively;

Compute the statistic [formula];

If Tn > 0 take Ij = 1, otherwise Ij = 0.

After running the above Monte Carlo simulation, the simulated PCS is given by [formula]. We also compute the PCS based on the asymptotic results derived in Section [\ref=Likelihoodratiotest]. The simulation study was carried out using the software R; see http://www.r-project.org.

We set n = 20,40,60,80,100,200,500 and a = 0.2,0.5,0.9,1.5,2.0,3.0,5.0. These results are presented in Table [\ref=simbe]. It is quite clear that there is a good agreement between the asymptotic and empirical probabilities, mainly for moderate and large sample sizes. We also observe that, when a approaches 1, the PCSs approaches 0.5. This was expected since when α goes to 0 both beta and Kumaraswamy distributions converge to the same law. Another expected result we observed is that when n increases the PCS approaches one.

In Table [\ref=simkw] we present the asymptotic and simulated PCSs under the null hypothesis HK for α = 0.2,0.5,0.9,1.5,2.0,3.0,5.0 and n = 20,40,60,80,100,200,500. In this case we also observe a good agreement between the asymptotic and empirical PCSs. When α is close to one, the PCSs are close to 0.5, and as n increases, the probabilities goes to one, as expected and discussed in the previous case.

Empirical illustrations

We now apply our results in two real data sets. In the first application, we consider the percentage of muslim population in 152 countries. The data can be found in http://www.qran.org/a/a-world.htm and is based on 2004 Census projection. The sources include HFE.org, IslamicPopulation.com, StrategicNetwork.org, State.gov, among others.

The MLEs of the parameters of the beta and Kumaraswamy distributions are given by (â,)  =  (0.2976,0.5159) and (,)  =  (0.3515,0.5906), respectively. Figure [\ref=muslim] shows the histogram and the plots of the fitted beta and Kumaraswamy densities. Empirical and fitted cdfs are also displayed in this figure. The test statistic equals Tn  =  118.4542 - 114.8334  =  3.6207  >  0, which indicates that the beta model should be chosen according Akaike criterion. Under the hypothesis that the data come from a B(0.2976,0.5159) distribution, we obtain the estimated quantities B(â,)  =   - 0.0188 and B(â,)  =  0.1617. Thus, we have [formula], while the simulated PCS equals 0.7370. Similarly, under the hypothesis that the data come from a Kumaraswamy distribution, we have K(,)  =   - 0.0026 and K(,)  =  0.0380, which yields [formula] (the simulated PCS equals 0.6180). Therefore, the probability of correct selection (based on the asymptotic result) is at least equal to [formula]. Since the PCS is maximum under the hypothesis HB, we choose the beta distribution. Based on the simulated PCSs, we obtain the same conclusion.

The second application considers the proportion of atheists in the populations of 137 countries. This data set was also used by Lynn et al. (2009) and collected from surveys mostly carried out in 2004, although in a few countries the surveys were a year or two earlier.

The MLEs of the beta and Kumaraswamy parameters are (â,)  =  (0.4368,3.6347) and (,)  =  (0.5091,3.0914). The histogram of the data and the beta and Kumaraswamy estimated densities are shown in Figure [\ref=fig.estimation4]. For comparison purposes, we also plot empirical and the two fitted cdfs. In this case, the test statistic equals Tn  =  205.9754  -  210.8923  =   - 4.9169 < 0, thus indicating that the Kumaraswamy model yields the best fit (based on the Akaike criterion). Under the hypothesis HK, we have K(,)  =  0.0035 and K(,)  =  0.0072, and hence we obtain [formula]; the simulated PCS equals 0.8044. On the other hand, under the hypothesis HB, we obtain B(â,)  =   - 0.0032 and B(â,)  =  0.0063. With these results we find [formula] and the simulated PCS equals 0.7060. The probability of correct selection (based on the asymptotic results) is at least min (0.7872,0.6812)  =  0.6812. The PCS is maximum under the hypothesis HK and therefore we choose the Kumaraswamy distribution. The same conclusion is obtained by considering the simulated results.

Acknowledgements

The authors gratefully acknowledge financial support from CAPES (Brazil) and CNPq (Brazil).