.def

Lemma Corollary Proposition Conjecture

Definition Question Problem Answer Remark Claim Example

PAC Learning, VC Dimension, and the Arithmetic Hierarchy

Introduction

A common method to characterize the complexity of an object is to describe the degree of its index set [\cite=ecl2] [\cite=d2eqs] [\cite=d2apg] [\cite=idxsets] [\cite=idxpi01] [\cite=gk] [\cite=melnikovnies]. In the present paper, we carry out this computation for the class of objects which are machine-learnable in a particular model.

There have been several models of machine learning, dating back at least to Gold's seminal 1967 paper [\cite=gold1967]. In Gold's basic model, the goal is that the machine should determine a Σ01-index for a computably enumerable set of natural numbers -- that is, an index for a computable function enumerating it, by receiving an initial segment of the string. Of course, many variations are possible, involving, for instance, the receipt of positive or negative information and the strength of the convergence criteria in the task of "determining" an index. This family of models has been studied by the recursion theory community (see, for instance, [\cite=friendgoetheharizanov] [\cite=harizanovstephan] [\cite=stephanventsov]), but is not the primary focus of this paper. One particular result, however, is of interest to us.

The set of Σ01 indices for uniformly computably enumerable families learnable in each of the following models is m-complete in the corresponding class.

TxtFin -- Σ03

TxtEx -- Σ04

TxtBC -- Σ05

TxtEx* -- Σ05

PAC Learning

The model of learning that concerns us here (PAC learning, for "Probably Approximately Correct") was first proposed by Valiant in [\cite=Valiant]. Much of our exposition of the subject comes from [\cite=KearnsVazirani]. The idea of the model is that it should allow some acceptably small error of each of two kinds: one arising from targets to be learned which are somehow too close together to be easily distinguished, and the other arising from randomness in the examples shown to the learner. Neither aspect is easily treated in Gold's framework of identifying indices for computable enumerations of natural numbers by inspecting initial segments -- neither a notion of "close" nor randomness in the inputs.

In the present paper, we will describe a framework in which to model PAC learning in a way which is suitable for recursion-theoretic analysis and which is broad enough to include many of the benchmark examples. We will then calculate the m-degree of the set of indices for learnable concept classes.

Let X be a set, called the instance space.

Let C be a subset of P(X), called a concept class.

The elements of C are called concepts.

We say that C is PAC Learnable if and only if there is an algorithm φe such that for every c∈C, every [formula] and every probability distribution D on X, the algorithm φe behaves as follows: On input (ε,δ), the algorithm φe will ask for some number n of examples, and will be given [formula] where xj are independently randomly drawn from D, and ij  =  χc(xj). The algorithm will then output some h∈C so that with probability at least 1 - δ in D, the symmetric difference of h and c has probability at most ε in D.

This is a well-studied model -- so well-studied, in fact, that it is more usual to talk about the complexity of the algorithm (in both running time and the number of example calls) than about its existence. For the present paper, though, we restrict ourselves to the latter problem. Several examples are well-known.

Let X  =  2n, interpreted as assignments of truth values to Boolean variables. Then the class C of k-CNF expressions is PAC learnable (where each expression c∈C is interpreted as the set of truth assignments that satisfy it).

Let [formula]. Then the class C of linear half-spaces is PAC learnable.

Let [formula]. Then the class of convex d-gons is PAC learnable for any d.

The Vapnik-Chervonenkis Dimension

An alternate view of PAC learnability arises from work of Vapnik and Chervonenkis [\cite=VC]. Again, we follow the exposition of [\cite=KearnsVazirani].

Let C be a concept class.

Let S  ⊆  X. Then [formula].

The VC dimension of C is the greatest integer d such that [formula] for some S with cardinality d, if such an integer exists. Otherwise, the VC dimension of C is ∞  .

For example, if C is the class of linear half-spaces of [formula], and if S is a set of size 4, suppose that k is the least such that all of S is contained in the convex hull of k  ≤  4 points. If k < 4, take a set S0 of size k such that the convex hull of S0 contains S. The subset S0  ⊂  S cannot be defined by intersecting S with a linear half-space. If k = 4, then let S0 be a diagonal pair, which again cannot be defined by intersection with a linear half-space. Consequently, the VC dimension of C must be at most 3. One can also show that this bound is sharp.

The connection of VC dimension with learnability is a theorem of Blumer, Ehrenfeucht, Haussler, and Warmuth showing that under some reasonable measure-theoretic hypotheses (which hold in all examples shown so far, and in all examples that will arise in the present paper), finite VC dimension is equivalent to PAC learnability [\cite=BEHW].

Let R  ⊆  P(X), and let D be a probability distribution on X, and ε > 0.

We say that N  ⊆  X is an ε-transversal for R with respect to D if and only if for any c∈R with PD(c)  >  ε we have [formula].

For each m  ≥  1, we denote by Qmε(R) the set of [formula] such that the set of distinct elements of [formula] does not form an ε-transversal for R with respect to D.

For each m  ≥  1, we denote by J2mε(R) the set of all [formula] with [formula] and [formula] each of length m such that there is c∈R with [formula] such that no element of c occurs in [formula], but elements of c have density at least [formula] in [formula].

We say that a concept class C is well-behaved if for every Borel set b, the sets Qmε(R) and J2mε(R) are measurable where [formula].

This notion of "well-behaved" is exactly the necessary hypothesis for the equivalence:

Let C be a nontrivial, well-behaved concept class. Then C is PAC learnable if and only if C has finite VC dimension.

Concepts and Concept Classes

The most general context in which PAC learning makes sense is far too broad to say anything meaningful about the full problem of determining whether a class is learnable. If we were to allow the instance space to be an arbitrary set, and a concept class an arbitrary subset of the powerset of the instance space, we would quickly be thinking about a non-trivial fragment of set theory.

In practice, on the other hand, one usually fixes the instance space, and asks whether (or how efficiently, or just by what means) a particular class is learnable. This approach is too narrow for the main problem of this paper to be meaningful. The goal of this section, then, is to describe a context broad enough to cover many of the usual examples, but constrained enough to be tractable.

Many of the usual examples of machine learning problems can be systematized in the framework of Π01 classes, which will now be introduced. The following result is well-known, but a proof is given in [\cite=Pi01], which is also a good general reference on Π01 classes.

Let c  ⊆  2ω. Then the following are equivalent:

c is the set of all infinite paths through a computable subtree of 2ω

c is the set of all infinite paths through a Π01 subtree of 2ω (i.e. a co-c.e. subtree)

[formula] for some computable relation R, i.e. a relation R for which there is a Turing functional Φ such that R(n,x) is defined by Φx(n).

This equivalence (and other similar formulations could be added) gives rise to the following definition:

Let c  ⊆  2ω. Then we say that c is a Π01 class if and only if it satisfies one of the equivalent conditions in Theorem [\ref=pi01eq].

There is a natural and uniform representation of all well-formed formulas of classical propositional calculus, each as a Π01 class. We regard 2ω as the assignment of values to Boolean variables, so that for f∈2ω, the value f(n)  =  k indicates a value of k for variable xn. Let φ be a propositional formula. We construct a Π01 subtree Tφ  ⊆  2ω such that f∈Tφ if and only if f satisfies φ. At stage n, for each σ∈2<  ω of length n, we include σ∈Tφ if and only if there is an extension f  ⊃  σ such that [formula]. This condition can be checked effectively. Consequently, Tφ is a Π01 subtree of 2ω -- intuitively, an infinite path f may fall out of Tφ at some point when we see a long enough initial segment to detect non-satisfiability, but unless it falls out at some finite stage, it is included.

There is a natural and uniform representation of all closed intervals of [formula] with computable endpoints, each as a Π01 class. We take the usual representation of real numbers by binary strings. Let I be a closed interval with computable endpoints. We construct a Π01 tree TI  ⊆  2ω such that the set of paths through TI is equal to I. At stage s, we include in TI all binary sequences σ of length s such that there is an extension f  ⊃  σ with f∈I. This condition can be checked effectively, by the computability of the endpoints of I. Consequently, TI is a Π01 subtree of 2ω.

There is a natural and uniform representation of all closed linear half-spaces of [formula] which are defined by hyperplanes with computable coefficients, each half-space as a Π01 class. We encode [formula] as 2ω in the following way: the ith coordinate of the point represented by the path f is given by the sequence [formula]. Now we encode a linear subspace into a subtree in the same way as with intervals in the previous example.

There is a natural and uniform representation of all convex d-gons in [formula] with computable vertices, with each d-gon represented by a Π01 class. A convex d-gon is an intersection of d closed linear half-spaces, and so we exclude a node σ∈2ω from the tree for our d-gon if and only if it is excluded from the tree for at least one of those linear half-spaces.

Note that the requirement of computable boundaries of these examples is not a practical restriction.

For any probability measure μ on [formula] absolutely continuous with respect to Lebesgue measure, and for any hyperplane given by [formula], there is a hyperplane given by [formula] where [formula] has computable coefficients, and where the linear half-spaces defined by these hyperplanes are close in the following sense: If Hf is defined by [formula], if H0f is defined by [formula], and [formula] is defined by [formula], then [formula] and [formula].

Since a hyperplane has Lebesgue measure 0, it suffices to show that we can achieve [formula]. Now by using the cumulative distribution function, we can construct a bounded d-orthotope [formula] with computable vertices such that [formula].

Since computable points are dense in [formula], we can find, in each face Fi of B, a computable point [formula] so close to [formula] that if [formula] is the hyperplane determined by the set of points [formula], then

[formula]

The coefficients of [formula] are computable since the points [formula] are computable. Furthermore,

[formula]

Examples could be multiplied, of course, and it seems likely that many of the more frequently encountered machine learning situations could be included in this framework -- certainly, for instance, any example in [\cite=bishop], [\cite=KearnsVazirani], or [\cite=russellnorvig].

We will work, for the purposes of the present paper, with instance space 2ω and with concepts which are Π01 classes. It remains to describe the concept classes to be used.

There is an unfortunate clash of terminology in that the concept classes will have, for their members, Π01 classes. In this paper, we will never use the term ambiguously, but because both terms are so well-established it will be necessary to use both of them.

A weakly effective concept class is a computable enumeration [formula] such that φe(n) is a Π01 index for a Π01 tree Te,n.

Naturally, we interpret each index enumerated as the Π01 class of paths through the associated tree. We also freely refer to the indices (or trees, or Π01 classes) in the range of a concept class as its elements.

This definition is almost adequate to our needs. We would like, however, one additional property: that a finite part of an effective concept class should not be able to distinguish a non-computable point of 2ω from all computable points. This is reasonable: it would strain our notion of an "effective" concept class if it should fail. And yet it can fail with a weakly effective concept class: our classes may have no computable members at all, for instance. For that reason, we define an effective concept class as follows.

An effective concept class is a weakly effective concept class φe such that for each n, the set cn of paths through Te,n is computable in the sense that there is a computable function [formula] such that

[formula]

where Br(σ) is the set of all paths that either extend σ or first differ from it at the -  ⌈lg(r)⌉ place or later (see [\cite=bravermanyampolskybk] [\cite=weihrauch]).

In addition to the useful property mentioned above, which we will soon prove, there is another reason for preferring this stronger definition: Typically when we want a computer to learn something, it is with the goal that the computer will then be able to act on it. Computability of each concept is a necessary condition for this. The restriction corresponds, in the examples, to the restriction that a linear half-space, for instance, be defined by computable coefficients. The classes we consider in this paper will be effective concept classes.

Let C be an effective concept class, and let [formula]. Then for any y∈2ω, there is a computable x∈2ω such that for each [formula], we have x∈ci if and only if y∈ci.

Let [formula] be as described in the statement of the Proposition. Let I be the set of i such that y∈ci and J be the set of i such that y∉ci.

Suppose first that y is not in the boundary ∂ci of ci for each i. Then

[formula]

where [formula] denotes the interior of s and S̄ the complement of S. Since I and J are finite, N is open. Since y∈N, the set N is nonempty, and must contain a basic open set of 2ω, and so must contain a computable member, x, as required.

Now suppose that y is in ∂ci for some i. Then we can compute y, using the function fci, so that y is itself computable and we take x  =  y.

We note that all of the examples given so far are effective concept classes.

The class of well-formed formulas of classical propositional calculus, and the class of k-CNF expressions (for any k) are effective concept classes, by the example above. Whether a given y∈2ω satisfies a particular formula can be determined by examining only finitely many terms of y.

The class C of linear half-spaces in [formula] bounded by hyperplanes with computable coefficients is an effective concept class. Recall that each linear half-space with computable coefficients is a computable set, since the distance of a point from the boundary can be computed.

The class of convex d-gons in [formula] with computable vertices is an effective concept class.

Again, it appears that any example in any of the standard references is an effective concept class.

A pleasant feature of the effective concept classes is that they are always well-behaved.

A weakly effective concept class has finite VC dimension if and only if it is PAC learnable.

Let C be an effective concept class. In [\cite=BEHW], a proof of Ben-David is given that if C is universally separable -- that is, if there is a countable subset C* such that every point in C can be written as the pointwise limit of some sequence in C* -- then C is well-behaved. Since an effective concept class is always countable (i.e. it contains only countably many Π01 classes), C is trivially universally separable. By Theorem [\ref=wellbehaved], the conclusion holds.

Bounding the Degree of the Index Set

We now turn toward the main problem of the paper, which we can now express exactly.

Determine the m-degree of the set of all natural numbers e such that φe is a PAC-learnable effective concept class.

One minor refinement in the problem remains: the difficulty of saying that e is the index for an effective concept class competes with that of saying that this concept class is learnable. Indeed, since determining that n is an X-index for an X-computable tree is m-complete Π02(X) (see [\cite=gk] [\cite=soare]), it follows that determining that n is a Π01 index for a Π01 tree is m-complete Π03.

Since we will see that finite VC dimension can be defined at Σ03, a driving force in the m-degree described in the problem above will be that it must compute all Π03 sets. This tells us nothing about the complexity of learnability, but only about the complexity of determining whether we have a concept class. The usual way to deal with this issue is by the following definition.

Let A  ⊆  B, and let Γ be some class of sets (e.g. Π03).

We say that A is Γ within B if and only if [formula] for some R∈Γ.

We say that S  ≤  mA within B if and only if there is a computable f:ω  →  B such that for all n we have n∈S  ⇔  f(n)∈A.

We say that A is m-complete Γ within B if and only if A is Γ within B and for every S∈Γ we have S  ≤  mA within B.

We can now present the question in its final form.

Let L be the set of indices for effective concept classes, K the set of indices for effective concept classes which are PAC learnable. What is the m-degree of K within L?

The solution to the problem will have two parts. In the present section, we will show that K is Σ03 within L. In the following section, we show that K is m-complete Σ03 within L.

We first reduce the problem to one on computable paths through 2ω.

An effective concept class C has infinite VC dimension if and only if for every d there are (not necessarily uniformly) computable elements

[formula]

such that [formula]..

Let [formula] witness that C has VC dimension at least d, and denote by [formula] elements of C which distinguish distinct subsets of [formula]. For each i < d, there is a computable element xi such that for every j  ≤  2d we have xi∈Dj if and only if yi∈Dj, by Proposition [\ref=comprepl]. Then [formula] witness that C has VC dimension at least d. The converse is obvious.

The set of indices for effective concept classes of infinite VC dimension is Π03 within L.

We begin by noting that if f is a computable function and T is a Π01 tree, then it is a Π01 condition that f is a path of T, and a Σ01 condition that it is not, uniformly in a Π01 index for T and a computable index for f. Further, if C  =  φe is an effective concept class, then for any k∈ω, the condition that k∈ran(φe) is a Σ01 condition, uniformly in e and k.

Let [formula] be a sequence of computable functions, [formula], and c a Π01 class, represented by a Π01 index for a tree in which it is the set of paths. We abbreviate by [formula] the statement that for each [formula], we have xi∈c if and only if i∈S. Now [formula] is a d-Σ01 condition, uniformly in the indices for the xi and c.

We now note that C  =  φe has infinite VC dimension if and only if

[formula]

From the comments above, this definition is Π03.

Sharpness of the Bound

The completeness result in this section will finish our answer to the main question of the paper.

The set of indices for effective concept classes of infinite VC dimension is m-complete Π03 within L, and the set of indices for effective concept classes of finite VC dimension is Σ03 within L.

It only remains to show completeness. For each Π03 set S, we will construct a sequence of effective concept classes [formula] such that Cn has infinite VC dimension if and only if n∈S. In the following lemma, to simplify notation, we suppress the dependence of f on n.

There is a Δ02 function [formula] such that f(s)  =  1 for infinitely many s if and only if n∈S.

It suffices (see [\cite=soare]) to consider S of the form [formula]. Now we set

[formula]

This function is Δ02-computable, and has the necessary properties.

Now by the Limit Lemma, there is a uniformly computable sequence [formula] of functions such that for each x, for sufficiently large s, we have fs(x)  =  f(x).

We now take a set of functions that will serve as the elements that may eventually witness high VC dimension. Let [formula] be a discrete uniformly computable set of distinct elements of 2ω such that πs,t,j(q)  =  πs,t',j'(q) whenever q  <   min {t,t'}.

We also initialize [formula] for each s. Denote by Pt a bijection

[formula]

At stage s of the construction, we consider fs(t) for each t  ≤  s. If fs(t)  =  0, then no action is required.

If fs(t)  =  1, then we find the least k such that k∉Gt,s. Let {et,i:i < 2t} be Π01 indices for trees such that Tet,i consists exactly of the initial segments τ of πt,k,j where j  =  Pt(S) for some [formula] and |τ| is less than the first z > s such that fz(t)  =  0. This can be done effectively exactly because we are looking for Π01 indices, and the search is uniform. We then let is be the least such that Cn(is) is undefined, and take [formula] for each [formula]. We also set [formula].

Now for each t with f(t)  =  1, there will be some s such that fs'(t)  =  fs(t)  =  1 for all s' > s. Then at stage s we have added to Cn the Π01 indices {et,i:i < 2t} guaranteeing that {πt,k,j:j < t} is shattered for some k.

For each t such that f(t)  =  0 and each s such that fs(t)  =  1, there is some later stage s' such that fs'(t)  =  0, so any indices added at stage s will be indices for a tree with no paths -- that is, for the empty concept.

Note that if the same t receives attention infinitely often -- that is, if infinitely many different sets of classes are added to Cn to guarantee that the VC dimension of Cn is at least t, this does not inflate the VC dimension beyond t. Indeed, the sets of witnesses will be pairwise disjoint, so no concept in Cn will include any mixture of witnesses from different treatments; the resulting sets will not be shattered.

We further note that all the Π01 classes in Cn are computable. Indeed, each c∈Cn consists of finitely many (perhaps no) computable paths. Thus, [formula] is an effective concept class.

Now if n∉S, then f(s)  =  1 for at most finitely many s, so that the VC dimension of Cn is finite. If n∈S, then f(s)  =  1 for infinitely many s, so that the VC dimension of Cn is infinite (since sets of arbitrarily large size will be shattered).