Improving the estimation of the odds ratio using auxiliary information

Abstract

The odds ratio measure is used in health and social surveys where the odds of a certain event is to be compared between two populations. It is defined using logistic regression, and requires that data from surveys are accompanied by their weights. A nonparametric estimation method that incorporates survey weights and auxiliary information may improve the precision of the odds ratio estimator. It consists in B-spline calibration which can handle the nonlinear structure of the parameter. The variance is estimated through linearization. Implementation is possible through standard survey softwares. The gain in precision depends on the data as shown on two examples.

Key Words: B-spline functions, calibration, estimating equation, influence function, linearization, logistic regression.

Running title: Odds ratio estimation in surveys

Introduction

We study the use of nonparametric weights for estimating the odds ratio when the risk variable, which is the explanatory variable in the logistic regression, is either a continuous or a binary variable. The odds ratio is used to describe the strength of association or non-independence between two binary variables defining two groups experiencing a particular event. One binary variable defines a group at risk and a group not at risk; the second binary variable defines the presence or absence of an event related to health. The odds ratio is the ratio of the odds of the event occurring in one group to the odds of the same event occurring in the other group. An odds ratio equal to 1 means that the event has the same odds in both groups; an odds ratio greater than 1 means that the event has a larger odds in the first group; an odds ratio under 1 means that the event has a smaller odds in the first group.

When both variables are categorical, the odds ratio estimator is obtained from a contingency table, as the ratio of the estimated row ratios, then, as a function of four numbers. As suggested by a reviewer, this definition leads to an estimator which takes survey weights into account and yields confidence intervals after linearization. However, this simple definition is not adapted to a continuous risk variable. In this case, the odds ratio measures the change in the odds for an increase of one unit in the risk variable. It is defined through the logistic regression.

For a binary risk variable, the odds ratio is the exponential of the difference of two logits, the logit function being the link function in the logistic regression. So the logistic regression coefficient for a binary risk variable corresponds to the logarithm of the odds ratio associated with this risk variable, net the effect of the other variables. When the risk variable is continuous, the regression coefficient represents the logarithm of the odds ratio associated with a change in the risk variable of one unit, net the effect of the other variables. The regression coefficient is a solution of a population estimating equation using the theory developed in Binder (1983) for making inference. The sampling design must not be neglected especially for cluster sampling (Lohr, 2010). Korn and Graubard (1999) and Heeringa et al. (2010) give details and examples of estimating an odds ratio but ignore auxiliary information. Korn and Graubard (1999: 169-170) advocate the use of weighted odds ratios contrary to Eideh and Nathan (2006). Rao et al. (2002) suggest using post-stratification information to estimate parameters of interest obtained as solution of an estimating equation. The vector of parameters in the logistic regression is an example. Deville (1999) suggested "allocating a weight wk to any point in the sample and zero to any other point, regardless of the origin of the weights (Horvitz-Thompson or calibration)." Goga and Ruiz-Gazen (2014) use auxiliary information to estimate nonlinear parameters through nonparametric weights. The solutions of estimating equations are nonlinear but Goga and Ruiz-Gazen (2014) give no detail. Our project is the estimation of the odds ratio with auxiliary information.

In Section 2, we recall the definition of the odds ratio and express the B-spline calibration estimator. In Section 3, we use linearization to derive the asymptotic variance of the estimator under broad assumptions. We infer a variance estimator together with asymptotic normal confidence intervals. In Section 4, we draw guidelines for practical implementation and show the properties of our estimator on two case studies.

Estimation of the odds ratio with survey data

Definition of the parameter

The odds ratio, denoted by OR, is used to quantify the association between the levels of a response variable Y and a risk variable X. The value taken by Y is yi and the value taken by X is xi for the i-th individual in a population [formula]

The logistic regression

[formula]

where pi = P(Y = 1|X = xi) implies that

[formula]

The odds ratio is (Agresti, 2002):

[formula]

With a binary variable X, the OR has a simpler form and can be derived from a contingency table. The OR is equal to

[formula]

where N00, N01, N10, and N11 are the population counts associated with the contingency table. In order to estimate the OR of Eq. ([\ref=expbeta]), we estimate first the regression coefficient β' = (β0,β1) by ' = (0,1), where ' denotes the transpose of . Eq. ([\ref=expbeta]) yields the estimator of OR:

[formula]

The regression parameters β0 and β1 are obtained by maximization of the population likelihood:

[formula]

The maximum likelihood estimator of β satisfies:

[formula]

Let i = (1  xi)' and μ('iβ) =  exp ('iβ)(1 +  exp ('iβ))- 1. We write Eq. ([\ref=eq:ml1]) and ([\ref=eq:ml2]) in the equivalent form

[formula]

or, with i(β) = i(yi  -  μ('iβ)),

[formula]

The regression estimator of β is defined as an implicit solution of the estimating Eq. ([\ref=maxvrais]). We use iterative methods to compute it.

The B-spline nonparametric calibration

For s a sample selected from the population U according to a sample design p(  ·  ) , we denote by πi > 0 the probability of unit i to be selected in the sample and πij > 0 the joint probability of units i and j to be selected in the sample with πii  =  πi. We look for an estimator of β and of OR taking the auxiliary variable Z, with values [formula], into account.

Deville and Särndal (1992) suggest deriving the calibration weights wks as close as possible to the Horvitz-Thompson sampling weights di = 1 / πi while satisfying the calibration constraints on known totals Z:

[formula]

This method works well for a linear relationship between the main and the auxiliary variables. When this relationship is no longer linear, the calibration constraints must be changed while keeping the property that the obtained weights do not depend on the main variable. Basis functions that are more general than the ones defined by constants and zi, include B-splines, which are simple to use (Goga and Ruiz-Gazen, 2013), truncated polynomial basis functions, and wavelets.

B-spline functions

Spline functions are used to model nonlinear trends. A spline function of degree m with K interior knots is a piecewise polynomial of degree m - 1 on the intervals between knots, smoothly connected at knots.

The B-spline functions [formula] of degree m with K interior knots, q = m + K are among the possible basis functions (Dierckx, 1993). Other basis functions exist such as the truncated power basis (Ruppert et al., 2003). For m = 1, the B-spline basis functions are step functions with jumps at the knots; for m = 2, they are piecewise linear polynomials, and so on. Figure [\ref=base_Bspline] shows the six B-spline basis functions obtained for m = 3 and K = 3. Figure [\ref=approx_sinus] gives the approximation of the curve f(x) = x +  sin (4πx) taking the noisy data points into account and using the B-spline basis. Even if the function f is nonlinear, the B-spline approximation almost coincides with it. The user chooses the spline degree m and the total number K of knots. There is no general rule giving the total number of knots but Ruppert al. (2003) recommend m = 3 or m = 4 and no more than 30 to 40 knots. They also give a simple rule for choosing K (Ruppert et al., 2003: 126). Usually, the knots are located at the quantiles of the explanatory variable (Goga and Ruiz-Gazen, 2013).

Nonparametric calibration with B-spline functions

The B-splines calibration weights wbis are solution of the optimization problem:

[formula]

subject to

[formula]

where [formula] and qi is a positive constant. They are given by

[formula]

with [formula] [formula] The weights wbis depend only on the auxiliary variable and are similar to Deville and Särndal's weights. The calibration equation implies [formula] and [formula] If qi = 1 for all i∈U, we obtain (Goga, 2005):

[formula]

Goga and Ruiz-Gazen (2014) use these weights to estimate totals for variables, which are related nonlinearly to the auxiliary information and to estimate nonlinear parameters such as a Gini index. We use wbis to estimate the logistic regression coefficient and the odds ratio efficiently.

Estimation of OR using B-spline nonparametric calibration

The regression coefficient β is a nonlinear finite population function defined by the implicit Eq. ([\ref=maxvrais]). The functional method by Deville (1999), specified for the nonparametric case by Goga and Ruiz-Gazen (2014), is used to build a nonparametric estimator of β defined through the weights of Eq. ([\ref=poidsNP]). M is the finite measure assigning the unit mass to each yi, i∈U, and zero elsewhere:

[formula]

where δyi is the Dirac function at yi, δyi(y) = 1 for y = yi and zero elsewhere. The functional T defined with respect to the measure M and depending on the parameter β defined by

[formula]

The regression coefficient β is the solution of the implicit equation

[formula]

Eq. ([\ref=eq:sco]) is called the score equation. The measure M may be estimated using the Horvitz-Thompson weights dk = 1 / πk or the linear calibration weights (Deville, 1999). We suggest using the nonparametric weights derived in Eq. ([\ref=poidsNP]):

[formula]

and estimate M by

[formula]

Plugging [formula] into the functional expression of β given by Eq. ([\ref=betafunct]) yields the B-spline calibrated estimator [formula] of β:

[formula]

which means that [formula] is the solution of the implicit equation:

[formula]

The functional method allows us to incorporate auxiliary information for estimating the logistic regression coefficient and any parameter β defined as a solution of estimating equations.

The functional T is differentiable with respect to β and

[formula]

with  = ('i)i∈U and Λ(β) =  - (ν('iβ)) with ν('iβ) = μ('iβ)(1 - μ('iβ)) the derivative of μ. The 2  ×  2 matrix 'Λ(β) is invertible and (β) is definite negative. From Eq. ([\ref=jacobian_pop]), the matrix (β) is a total estimated using the nonparametric weights wbis by:

[formula]

where (β) =  - (wbisν('iβ))i∈s and s = ('i)i∈s.

An iterative Newton-Raphson method is used to compute . The r-th step of the Newton-Raphson algorithm is:

[formula]

where r - 1 is the value of [formula] obtained at the (r - 1)-th step. w(r - 1) is the value of w(β) and T(;r - 1) the value of T(;β) evaluated at β  =  r - 1. Iterating to convergence produces the nonparametric estimator [formula] and the estimated Jacobian matrix w(). The odds ratio is estimated by   =   exp () and w() is used in section [\ref=sec:var] to estimate the variance of .

Variance estimation and confidence intervals

Variance estimation

The coefficient β of the logistic regression is nonlinear and nonparametric weights wbis to estimate β add more nonlinearity. We approximate [formula] in Eq. ([\ref=estimbetafunct]) by a linear estimator in two steps: we first treat the nonlinearity due to β, and second the nonlinearity due to the nonparametric estimation. This procedure is different from Deville (1999).

From the implicit function theorem, there exists a unique functional [formula] such that

[formula]

Moreover, the functional [formula] is also Fréchet differentiable with respect to M. The derivative of [formula] with respect to M, called the influence function, is defined by

[formula]

where δξ is the Dirac function at ξ. We give a first-order expansion of T̃ in  / N around M / N,

[formula]

which is also:

[formula]

because [formula] is a functional of degree zero, namely [formula] and [formula] (Deville, 1999).

For all i∈U, the linearized variable i of [formula] is defined as the value of the influence function [formula] at ξ = yi:

[formula]

The linearized variable i = (ui,0,ui,1)' is a two-dimensional vector depending on the unknown parameter β and on totals contained in the matrix (β). Eq. ([\ref=vonmises2]) becomes:

[formula]

The second component ui,1 of i, is the linearized variable of β1. With binary data, the odds ratio is given by Eq. ([\ref=or_quali]), which implies that

[formula]

In this case, the linearized variable of β1 has the expression:

[formula]

and the same expression is obtained from Eq. ([\ref=eq_lin]) after some algebra. When the weights wbis are equal to the sampling weights, namely wbis = 1 / πi, Eq. ([\ref=linear1]) implies that the asymptotic variance of [formula] is:

[formula]

where [formula] is the Horvitz-Thompson variance of [formula] with i(β) = i(yi  -  μ('iβ)):

[formula]

Binder (1983) gives the same asymptotic expression for the variance.

For B-spline basis functions formed by step functions on intervals between knots (m = 1), the weights wbis yield the post-stratified estimator of β (Rao et al., 2002). Linear calibration weights lead to the case treated by Deville (1999).

For the general case of nonparametric calibration weights wbis, a supplementary linearization step is necessary. The right hand side of Eq. ([\ref=linear1]) is a nonparametric calibration estimator for the total of the linearized variable i. It can be written as a generalized regression estimator (GREG):

[formula]

where [formula] We explain the linearized variable by means of a piecewise polynomial function. This fitting allows more flexibility and implies that the residuals i  -  u'(zi) have a smaller dispersion than with a linear fitting regression.

In order to derive the asymptotic variance of the nonparametric calibrated estimator, we assume that ||i|| < C for all i∈U with C a positive constant independent of i and N. The Euclidian norm is denoted ||  ·  ||. The matrix norm ||  ·  ||2 is defined by ||||22  =  ('). The linearized variable verifies N||i|| = O(1) uniformly in i, because

[formula]

where the Jacobian matrix (β) contains totals

[formula]

and

[formula]

because ν(i'β) < 1. Under the assumptions of theorem 7 in Goga and Ruiz-Gazen (2014), the nonparametric calibrated estimator [formula] is asymptotically equivalent to

[formula]

where [formula] The variance of [formula] is approximated by the Horvitz-Thompson variance of the residuals [formula]

[formula]

Eq. ([\ref=linear2]) states that the B-spline nonparametric calibration estimator of [formula] is asymptotically equivalent to the generalized difference estimator. We interpret this result as fitting a nonparametric model on the linearized variable i taking into account the auxiliary information zi. Nonparametric models are a good choice when the linearized variable obtained from the first linearization step does not depend linearly on zi, as it is the case in the logistic regression, which implies a second linearization step.

We write the asymptotic variance in Eq. ([\ref=var_asymt]) in a matrix form similar to Eq. ([\ref=var_sans_inf]). Consider again i(β) = i(yi  -  μ('iβ)) and let [formula] We have

[formula]

and the asymptotic variance of [formula] is:

[formula]

where [formula] is the Horvitz-Thompson estimator of the residual [formula] of i(β) using B-spline calibration. Eq. ([\ref=varasymbeta]) shows that improving the estimation of β is equivalent to improving the estimation of the score equation i  =  i(yi  -  μ('iβ)). The quantity of interest is the asymptotic variance of [formula]. It is the (2,2) element of the matrix () given by Eq. ([\ref=var_asymt]). We have i = (ui,0,ui,1)' and

[formula]

where [formula] and [formula] We obtain

[formula]

The linearized variable i is unknown and is estimated by:

[formula]

where the matrix w is computed according to Eq. ([\ref=estimJ]) and i is the estimation of i(β) for β  =  . The asymptotic variance () given in Eq. ([\ref=var_asymt]) is estimated by the Horvitz-Thompson variance estimator with i replaced by i given in Eq. ([\ref=varlin_estimator]):

[formula]

where [formula] The variance estimator of 1 is given by

[formula]

The variance estimator given in Eq. ([\ref=estimator_variance]) can be written in a matrix form. Let [formula] and V̂() is written as:

[formula]

where [formula] is the Horvitz-Thompson variance estimator of d() obtained by replacing ei(β) with [formula]

[formula]

Confidence interval for the odds ratio

The variance estimator of 1 is obtained from Eq. ([\ref=variance_estim]) as:

[formula]

where ê2() is the second component of () so that, under regularity conditions, the (1 - α)% normal interval for β1 is:

[formula]

where zα / 2 is the upper α / 2-quantile of a N(0,1) variable. Then the confidence interval for OR is:

[formula]

which is not symmetric around the estimated odds ratio but provides more accurate coverage rates of the true population value for a specified α (Heeringa et al., 2010).

Implementation and case studies

Implementation

Compute the B-spline basis functions Bj, for [formula] The B-spline basis functions are obtained using SAS or R. The user has only to specify the degree m and the total number of knots.

Use the sampling weights di = 1 / πi and the B-spline functions to derive the nonparametric weights wbis and the estimated parameter β.

Compute the linearized variable i estimated by i.

Compute the estimated predictions [formula] with

[formula]

and the associated residuals [formula].

Use a standard computer software able to compute variance estimators and apply it to the previously computed residuals.

Case studies

We compare the asymptotic variance of different estimators of the odds ratio in the simple case of one binary risk variable for two data sets. In this case, the odds ratio is a simple function of four counts given by Eq. ([\ref=or_quali]). We focus on the simple random sampling without replacement and compare three estimators. The first one is the Horvitz-Thompson estimator which does not use the auxiliary variable and whose asymptotic variance is given by Eq. ([\ref=var_sans_inf]). The second estimator is the generalized regression estimator which takes the auxiliary variable into account through a linear model fitting the linearized variable against the auxiliary variable. The third estimator is the B-spline calibration estimator with an asymptotic variance given by Eq. ([\ref=varasymbeta]). In order to gain efficiency, the auxiliary variable is related to the linearized variable. In the context of one binary factor, the linearized variable is given by Eq. ([\ref=lin1]) and takes four different values, which depend on the values of the variables X and Y. In order to be related to the linearized variable, the auxiliary variable is related to the product of the two variables X and Y, which is a strong property. Moreover, because ui,1, X, and Y are discrete, using auxiliary information does not necessarily lead to an important gain in efficiency as the first health survey example will show. The gain in efficiency however is significant in some cases. In the second example using labor survey data, the gain in using the B-splines calibration estimator compared to the Horvitz-Thompson estimator is significant because the auxiliary variable is related to the variable Y but also to the factor X; X and Y being related to one another, too.

Example from the California Health Interview Survey

The data set comes from the Center for Health Policy Research at the University of California. It was extracted from the adult survey data file of the California Health Interview Survey in 2009 and consists of 11074 adults. The response dummy variable equals one if the person is currently insured; the binary factor equals one if the person is currently a smoker. The auxiliary variable is age and we consider people who are less than 60 years old. The data are presented in detail in Lumley (2010).

We compare the Horvitz-Thompson, the generalized regression, and the B-splines calibration estimators in terms of asympotic variance. In order to calculate the B-splines functions, we use the SAS procedure transreg and take K = 15 knots and B-splines of degree m = 3. The gain in using the generalized regression estimator compared to the Horvitz-Thompson estimator is only 0.01%. It is 1.5% when using B-splines instead of the generalized regression. When changing the number of knots and the degree of the B-spline functions, the results remain similar and the gain remains under 2%. In this example, there is no gain in using auxiliary information even with flexible B-splines, because the auxiliary variable is not related enough to the linearized variable. The linearized variable takes negative values for smokers without insurance and non smokers with insurance, positive values for smokers with insurance and non smokers without insurance. Age is not a good predictor for this variable, because we expect to find sufficient people of any age in each of the four categories (smokers/non smokers ×   insurance/no insurance). Incorporating this auxiliary information brings no gain.

Example from the French Labor Survey

We consider 14621 wage-earners under 50 years of age, from the French labour force survey. The initial data set consists of monthly wages in 2000 and 1999. A dummy variable W00 equals one if the monthly wage in 2000 exceeds 1500 euros and zero otherwise. The same for W99 in 1999. The population is divided in lower and upper education groups. The value of the categorical factor DIP equals one for people with a university degree and zero otherwise. W00 corresponds to the binary response variable Y while the diploma variable DIP corresponds to the risk variable X. The variable W99 is the auxiliary variable Z.

To compare the Horvitz-Thompson estimator with the generalized regression estimator and the B-splines calibration estimator, we calculate the gain in terms of asympotic variance. We consider K = 15 knots and the degree m = 3. The gain in using the generalized estimator compared to the Horvitz-Thompson estimator is now 20%. It is 33% when using B-splines. The result is independent of the number of knots and, of the degree of B-spline functions. When the total number of knots varies from 5 to 50 and the degree varies from 1 to 5, the gain is between 32% and 34%. The nonlinear link between the linearized variable of a complex parameter with the auxiliary variable explains the gain in using a nonparametric estimator compared to an estimator based on a linear model (Goga and Ruiz-Gazen, 2013). For the odds ratio with one binary factor, the linearized variable is discrete and the linear model does not fit the data.

Conclusion

Estimating the variance of parameter estimators in a logistic regression is not straightforward especially if auxiliary information is available. We applied the method of Goga and Ruiz-Gazen (2014) to the case of parameters defined through estimating equations. The method relies on a linearization principle. The asymptotic variance of the estimator incorporates residuals of the model that we assume between the linearized variable and the auxiliary variable. The gain in using auxiliary information is thus based on the fitting quality of the model for the linearized variable. Because of the complexity of linearized variables, linear models that incorporate auxiliary information seldom fit linearized variables and we use nonparametric B-spline estimators. A particular case is post-stratification. Using the influence function defined by Eq. ([\ref=if]), we derive the asymptotic variance of the estimators together with confidence intervals for the odds ratio.

Acknowledgement: we thank Benot Riandey for drawing our attention to the odds ratio and one rewiever for his/her constructive comments.

Bibliography