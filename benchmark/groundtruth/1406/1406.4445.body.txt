RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex Minimization

Introduction

As a general convex minimization algorithm, accelerated proximal gradient (APG) has been attracting more and more attention recently, and it has been widely used in many different research areas such as signal processing [\cite=Combettes_Pesquet], computer vision [\cite=conf/cvpr/BaoWLJ12], and data mining [\cite=Zhou:2010:NFG:1933307.1934518]. In general, APG solves the following problem:

[formula]

where [formula] denotes the closed and convex feasible set for variable [formula], and [formula] is a convex function, which consists of a convex and differentiable function f1 with Lipschitz constant [formula] and a convex but non-differentiable function f2.

In APG, proximal gradient is used to update variables based on the proximity operator, denoted as [formula]. The basic idea of proximity operator is to approximate a convex function using a strongly convex function whose minimizer in the feasible set is returned as an approximate solution for the original minimization problem. At the optimal solution, the solution returned by proximity operator is identical to the optimal. As an example among classic APG algorithms, the basic version of FISTA [\cite=Beck:2009:FIS:1658360.1658364] is shown in Alg. [\ref=alg:fista], where [formula] denotes the step size. From FISTA, we can see that APG generates an auxiliary variable (i.e. [formula] in Alg. [\ref=alg:fista]) for proximal gradient so that the convergence rate of APG for general convex optimization is [formula], which was proved to be optimal for first-order gradient descent methods [\cite=tagkey198475].

Generally speaking, the computational bottleneck in APG comes from the following two aspects:

( 1) Computation of proximal gradients. Evaluating the gradients of f1 could be time-consuming, because the evaluation is over the entire dataset. This situation is more prominent for high dimensional and large-scale datasets. Also, projecting a point into the feasible set may be difficult. Many recent approaches have attempted to reduce this computational complexity. Inexact proximal gradient methods [\cite=DBLP:conf/nips/SchmidtRB11] allow to approximate the proximal gradients with controllable errors in a faster way while guaranteeing the convergence. Stochastic proximal gradient methods [\cite=atchade2014stochastic] [\cite=rosasco2014convergence] allow to compute the proximal gradients using a small set of data in a stochastic fashion while guaranteeing the convergence as well. Distributed proximal gradient methods [\cite=chen2012fast] decompose the optimization problem into sub-problems and solve these sub-problems locally in a distributed way using proximal gradient methods.

( 2) Number of iterations. In order to minimize the number of iterations, intuitively in each iteration the resulting function value should be as close to the global minimum as possible. One way to achieve this is to optimize the step size in the proximal gradient (e.g. γt in Alg. [\ref=alg:fista]), which unfortunately may be very difficult for many convex functions. Instead, in practice line search [\cite=Beck:2009:FIS:1658360.1658364] [\cite=DBLP:conf/icml/LinX14] [\cite=DBLP:journals/focm/ScheinbergGB14] [\cite=DBLP:journals/siamjo/Xiao013] is widely used to estimate the step size so that the function value is decreasing. For instance, backtracking [\cite=Beck:2009:FIS:1658360.1658364] [\cite=DBLP:journals/focm/ScheinbergGB14] is a common line search technique to tune the step size gradually. In general, the line search in the proximal gradient step has to evaluate the function repeatedly by changing the step size so that numerically the learned step size is close to the optimal. Alternatively many types of restarting schemes [\cite=DBLP:conf/icml/LinX14] [\cite=DBLP:journals/mp/Nesterov13] [\cite=Brendan2013] have been utilized to reduce the number of iterations empirically. Here additional restarting conditions are established and evaluated periodically. If such conditions are satisfied, the algorithm will be re-initialized using current solutions.

Our Contributions

In this paper, we focus on reducing the number of iterations, and simply assume that the non-differentiable function f2 is simple [\cite=DBLP:journals/mp/Nesterov13] for performing proximal gradient efficiently, e.g. [formula] norm.

Our first contribution is to propose a new general algorithm, Rapidly Accelerated Proximal Gradient (RAPID), to speed up the empirical convergence of APG, where an additional simple line search step is introduced after the proximal gradient step. Fig. [\ref=fig:intuition](a) illustrates the basic idea of our algorithm in 2D. After the proximal gradient step, another line search is applied along the direction of the current solution [formula]. Ideally, we would like to find a scalar θ > 0 so that [formula]. Therefore, we can guarantee [formula]. The positiveness of θ guarantees that both [formula] and [formula] point to the same direction so that the information from gradient descent can be preserved. Geometrically, this additional line search tries to push [formula] towards the optimal solution, making the distance between the current and optimal solutions smaller. Unlike the line search in the proximal gradient, the computation of finding the optimal θ can be very cheap (e.g. with close-form solutions) for many convex optimization problems, such as LASSO [\cite=tibshirani96regression] and group LASSO [\cite=yuan2006model] (see Section [\ref=sec:app]). Also, in order to guarantee the convergence of our algorithm, we further propose two ways of constructing the auxiliary variables based on the intermediate solutions in the previous and current iterations, as illustrated in Fig. [\ref=fig:intuition](b).

Our second contribution is that theoretically we prove that at an arbitrary iteration t, the upper bound of the objective error [formula] in our algorithm is consistently smaller than that of [formula] in traditional APG methods such as FISTA. This result implies that in order to achieve the same precision, the number of iterations in our algorithm is probably no more than that in APG. In other words, empirically our algorithm will converge faster than APG.

Our third contribution is that we apply our general algorithm to several interesting convex optimization problems, i.e. LASSO, group LASSO, least square loss with trace norm [\cite=Ji:2009:AGM:1553374.1553434] [\cite=journals/mp/Tseng10], and kernel support vector machines (SVMs), and compare our performance with APG and other existing solvers such as SLEP [\cite=Liu:2009:SLEP:manual] and LIBSVM [\cite=CC01a]. Our experimental results demonstrate the correctness of our theorems on faster convergence than APG, and surprisingly in most cases, our algorithm can be comparable with those sophisticated solvers.

This paper is organized as follows. In Section [\ref=sec:alg], we explain the details of our RAPID algorithm, including the new line search step and how to construct the auxiliary variables. In Section [\ref=sec:app], we take LASSO, group LASSO, least square loss with trace norm, and kernel SVMs as examples to demonstrate the empirical performance of our algorithm with experimental results and comparison with APG and other existing solvers. The theoretical results on the convergence rate of our algorithm are proven in Section [\ref=sec:analysis], and finally we conclude the paper in Section [\ref=sec:con].

Algorithms

In general, there are two basic steps in each iteration in APG algorithms: (1) performing proximal gradients, and (2) constructing auxiliary variables. Proximal gradient is defined as applying proximity operator to a gradient descent step.

Alg. [\ref=alg:rapid] shows our RAPID algorithm, where in each iteration t(t  ≥  1) four steps are involved:

A proximal gradient step using the auxiliary variable [formula], same as APG.

A simple line search step along the direction of the current solution [formula]. Actually the definition of θt in Alg. [\ref=alg:rapid] is equivalent to the following equation:

[formula]

In other words, this line search step essentially adapts the current solution [formula] to a better one in a very efficient way (e.g. with close-form solutions).

Updating parameter ηt used for constructing the new auxiliary variable [formula], same as APG. Note that any number sequence {ηt} can be used here as long as the sequence satisfies [formula].

Updating the new auxiliary variable [formula] using one of the following two equations:

[formula]

[formula]

In this way, our algorithm guarantees its convergence, but with different convergence rate. See our comparison results in Section [\ref=sec:app].

Fig. [\ref=fig:intuition](b) illustrates the differences in constructing the auxiliary variable between APG and our RAPID. In APG, the auxiliary variable [formula] is constructed along the gradient of [formula] starting from [formula]. Similarly, in RAPID we would like to construct the auxiliary variable [formula] using [formula] and the other intermediate solutions in the previous and current iterations (i.e. [formula]). It turns out that all possible combinations of intermediate solutions for constructing [formula] end up with Eq. [\ref=eqn:rapid-1], with guaranteed better upper bounds over [formula] than those over [formula] in APG in arbitrary iteration t (see Theorem [\ref=thm:rapid-1] in Section [\ref=sec:analysis]). Under a mild condition, we can adopt the same way as APG to construct [formula] using the final solutions in the previous and current iterations, i.e. [formula] and [formula], which is exactly Eq. [\ref=eqn:rapid-2]. However, for this setting we lose the theoretical guarantee of better upper bounds than APG, as shown in Theorem [\ref=thm:rapid-2] in Section [\ref=sec:analysis]. Nevertheless, surprisingly, in our experiments our algorithm using Eq. [\ref=eqn:rapid-2] outperforms than that using Eq. [\ref=eqn:rapid-1] with significant improvement in terms of empirical convergence (see Section [\ref=sec:app] for details).

Numerical Examples

In this section, we will explain how to apply our algorithm to solve (1) sparse linear regression (i.e. LASSO, group LASSO, and least square fitting with trace-norm), and (2) binary kernel SVMs. We also compare our empirical performance with APG and some other existing solvers.

Sparse Linear Regression

Problem Settings

We denote [formula] as a data matrix, [formula] as a regression target vector, [formula] as a matrix consisting of M regression tasks, [formula] as a linear regressor, [formula] as a matrix consisting of M linear regressors, and [formula] as a regularization parameter. As follows, for each method we list its loss function, regularizer, proximity operation, and optimal line search scalar θ, which are used in Alg. [\ref=alg:rapid].

( 1) Loss functions (i.e. f1, convex and differentiable). Least square loss is used in all the methods, i.e. [formula] for LASSO and group LASSO, and [formula] for trace-norm.

( 2) Regularizers (i.e. f2, convex but non-differentiable). The corresponding regularizers in LASSO, group LASSO, and trace-norm are [formula], [formula], and [formula], respectively. Here, [formula] and [formula] denote [formula] and [formula] norms, (  ·  )T denotes the matrix transpose operator, g∈G denotes a group index, [formula] denotes a group of variables without overlaps, and σj denotes the jth singular value for matrix [formula].

( 3) Proximity operators. According to their regularizers, the proximity operators can be calculated efficiently as follows, where [formula] and [formula] are denoted as the variable vector and matrix after the gradient descent:

LASSO: [formula], where [formula] if [formula], otherwise, [formula]; and |uj| denotes its absolute value.

Group LASSO: [formula].

Trace norm: Letting [formula], where [formula] and [formula] are two matrices and [formula] is a vector with n singular values of [formula], then we have [formula]. Here [formula] is an entry-wise operator.

( 4) Optimal line search scalar θ. For each problem, we re-define θ as [formula] in arbitrary iteration t by setting λθ =  +   ∞   in Alg. [\ref=alg:rapid], because there exists a close-form solution in this case. Letting [formula], we have [formula] for LASSO or group LASSO, and [formula] for trace-norm.

Experimental Results

We test and compare our RAPID algorithm on some synthetic data. For each sparse linear regression method, we generate a 103 sample data matrix with 103 dimensions per sample as variable [formula], and its associated regression target vector (matrix) [formula] ([formula]) randomly by normal distributions. The APG and RAPID methods are modified based on the code in [\cite=Brendan2013], and SLEP [\cite=Liu:2009:SLEP:manual] is a widely used sparse learning toolbox for our comparison. Here, RAPID-I and RAPID-II denote our algorithm using Eq. [\ref=eqn:rapid-1] and Eq. [\ref=eqn:rapid-2], respectively. SLEP-0, SLEP-1, and SLEP-2 are the three settings used in SLEP with different parameters mFlag and lFlag (i.e. (mFlag, lFlag)=SLEP-#: (0,0)=0, (0,1)=1, (1,1)=2). Please refer to the toolbox manual for more details. For trace-norm, SLEP actually implements the APG algorithm as its solver.

Fig. [\ref=fig:synthetic] shows our comparison results. In Fig. [\ref=fig:synthetic](a), the performances of SLEP-0 and SLEP-1 are identical, and thus there is only one curve (i.e. the brown one) for both methods. Clearly, our RAPID-II algorithm works best in these three cases in terms of empirical convergence rate. Since we can only guarantee that the upper bound of the difference between the current and optimal objective values in each iteration in RAPID is no bigger than that in APG (see Section [\ref=sec:analysis]), sometimes the actual objective value using RAPID may be larger than that using APG, as shown in Fig. [\ref=fig:synthetic](a).

Kernel SVMs

We are interested in solving binary kernel SVMs as well, because it is a widely used constrained optimization problem.

Problem Settings

Given a kernel matrix [formula] and a binary label vector [formula] with N samples, a binary kernel SVM can be formulated as follows:

[formula]

where [formula], [formula] is the entry-wise product operator, [formula] denotes a vector of 1's, and [formula] is a predefined constant.

With RAPID, Eq. [\ref=eqn:binary-svms] can be solved using Alg. [\ref=alg:rapid-svm], where each iteration contains the following 5 steps:

Perform line search for the step size [formula] in gradient descent by ignoring the constraints.

Update [formula] using gradient descent with [formula] and learned [formula].

Alternatively project [formula] into one constraint set while fixing the other until both are satisfied.

Update θt with guarantee that [formula] satisfies the constraints.

Update the auxiliary variable [formula] as the same as in Alg. [\ref=alg:rapid].

Alg. [\ref=alg:rapid-svm] can be adapted to an APG solver by fixing θt = 1 and using the update rule in FISTA for [formula].

Experimental Results

We test and compare our RAPID-SVMs on the binary covtype dataset [\cite=conf/nips/CollobertBB01]. This dataset contains 581012 samples with 54 dimensions per sample. Duo to the memory issue, we randomly select 5% data as training and use the rest data as testing. For simplicity, we utilize linear kernels for creating [formula], and solve Eq. [\ref=eqn:binary-svms] with C equal to one of {0.1,1,10}.

Our results are shown in Fig. [\ref=fig:svm-comparison], where RAPID is compared with APG and another popular SVM solver, LIBSVM [\cite=CC01a]. The stop criterion for both RAPID is to check whether [formula] is satisfied. Similarly, for APG, [formula] is checked. For LIBSVM we use its default settings. As we see, in all these three cases RAPID converges significantly faster than APG. With the increase of C, RAPID begins to fluctuate. However, we can easily control this by checking the objective value in each iteration to ensure it will not increase. If increasing, the solution will not be updated. We will add this feature in our implementation in the future. Compared with LIBSVM, when C = 0.1 or C = 1, RAPID converges better, resulting in slightly better classification accuracies, while for C = 10, RAPID performs worse, since it does not converge yet. Still, RAPID-II performs better than RAPID-I in all the cases.

Algorithm Analysis

In this section, we present our main theoretical results on the convergence rate of our RAPID algorithm in Theorem [\ref=thm:rapid-1] and [\ref=thm:rapid-2], which is clearly better than those of conventional APG methods such as FISTA, leading to faster convergence in practice.

Let [formula] be the linear approximation of f in [formula] w.r.t. f1, i.e. [formula], where 〈  ·  ,  ·  〉 denotes the inner product between two vectors. Then

[formula]

If [formula], then for any [formula],

[formula]

In Alg. [\ref=alg:rapid], at an arbitrary iteration t, we have

[formula]

Clearly, the definition of θt in Alg. [\ref=alg:rapid] satisfies the condition in Lemma [\ref=lem:3-point]. Therefore, we have:

[formula]

Let [formula] and [formula]. If Alg. [\ref=alg:rapid] updates [formula] using Eq. [\ref=eqn:rapid-1], in iteration [formula] in Alg. [\ref=alg:rapid], we have

[formula]

where [formula] is a constant.

Since [formula] satisfies the conditions in Lemma [\ref=lem:Sandwich] and Lemma [\ref=lem:3-point], we have:

[formula]

where [formula], and [formula]. Due to the convexity of [formula], we can rewrite Eq. [\ref=eqn:2-terms-x] as follows:

[formula]

Based on Eq. [\ref=eqn:2-terms-theta] and [\ref=eqn:theta_tx], we have:

[formula]

Letting [formula] and [formula], we can rewrite Eq. [\ref=eqn:key] as follows:

[formula]

[formula] the sequence [formula] in Alg. [\ref=alg:rapid] satisfies [formula], [formula], leading to

[formula]

Letting [formula], we have g0 = 0 and

[formula]

Since the sequence [formula] also satisfies [formula], based on Eq. [\ref=eqn:deduction] we have [formula]. Therefore,

[formula]

Let [formula] and [formula]. If Alg. [\ref=alg:rapid] updates [formula] using Eq. [\ref=eqn:rapid-2], and suppose in any iteration [formula], [formula] and [formula], then in iteration [formula] we have

[formula]

where [formula] is a constant.

Based on the assumptions of the theorem and Eq. [\ref=eqn:2-terms-x], we have

[formula]

where [formula]. Following the same proof strategy for Theorem [\ref=thm:rapid-1], we can easily prove this theorem.

Conclusion

In this paper, we propose an improved APG algorithm, namely, Rapidly Accelerated Proximal Gradient (RAPID), to speed up the convergence of conventional APG algorithms. Our first idea is to introduce a new line search step after the proximal gradient step in APG to push the current solution [formula] towards a new one [formula] so that [formula] is minimized over scalar θ. Our second idea is to propose two different ways of constructing the auxiliary variable in APG using the intermediate solutions in the previous and current iterations. In this way, we can prove that our algorithm is guaranteed to converge with a smaller upper bound of the gap between the current and optimal objective values than those in APG algorithms. We demonstrate our algorithm using two applications, i.e. sparse linear regression and kernel SVMs. In summary, our RAPID converges faster than APG, in general, and for some problems RAPID based algorithms can be comparable with the sophisticated existing solvers.