8.5 in 11 in

The Kullback-Leibler (KL) divergence is a measure in statistics [\cite=Cover1991] that quantifies in bits how close a probability distribution p  =  {pi} is to a model (or candidate) distribution q  =  {qi},

[formula]

[formula] is non-negative (≥  0), not symmetric in p and q, zero if the distributions match exactly and can potentially equal infinity. A common technical interpretation - although bereft of intuition - is that the KL divergence is the "coding penalty" associated with selecting a distribution q to approximate the true distribution p [\cite=Cover1991]. An intuitive understanding, however, arises from likelihood theory - the probability that one observes a set of data given that a particular model were true [\cite=Duda2001]. Pretend we perform an experiment to measure a discrete, random variable - such as rolling a dice many times (or in neuroscience, the simultaneous binned firing patterns of multiple neurons). If we perform a long experiment and make n measurements, we can count the number of times we observe each face of the die (or similarly, each firing pattern of neurons), a histogram c  =  {ci}, where [formula]. This histogram measures the relative frequency of each face of the die (or, each type of firing pattern). If this experiment lasts forever, the normalized histogram counts [formula] reflect an underlying distribution [formula]. Pretend we have a candidate model for die (or firing patterns), the distribution q. What is the probability of observing the histogram counts c if the model q actually generated the observations? This probability is given by the multinomial likelihood [\cite=Duda2001],

[formula]

To gain some intuition, imagine that we performed n = 1 measurements - in this case, the likelihood would be the qi attributed to the single observed firing pattern. The likelihood L shrinks mutiplicatively as we perform more measurements (or n grows). Ideally, we want the probability to be invariant to the number of measurements - this is given by the average likelihood [formula], a number between 0 and 1. Matching intuition, as we perform more measurements, if [formula], then the average likelihood would be perfect, or   →  1. Conversely, as [formula] diverges from the model qi, the average likelihood [formula] decreases, approaching zero. The link between likelihood and the KL divergence arises from the fact that if we perform an infinite number of measurements (see Appendix; ; Section 12.1 of ),

[formula]

Thus, if the distributions p and q are identical,   =  1 and [formula] (or if  = 0, [formula]). The central intuition is that the KL divergence effectively measures the average likelihood of observing (infinite) data with the distribution p if the particular model q actually generated the data.

The KL divergence has many applications and is a foundation of information theory and statistics [\cite=Cover1991]. For example, one can ask how similar a joint distribution p(x,y) is to the product of its marginals p(x)p(y) - this is the mutual information, a general measure of statistical dependence between two random variables [\cite=Cover1991],

[formula]

The mutual information is zero if and only if the two random variables X and Y are statistically independent. In addition to its role in mutual information, the KL divergence has been applied extensively in the neural coding literature, most recently to quantify the effects of conditional dependence between neurons [\cite=Schneidman2003b] [\cite=Latham2005] [\cite=Amari2006] and to measure how well higher order correlations can be approximated by lower order structure [\cite=Schneidman2006] [\cite=Shlens2006a].

Derivation

In this appendix we prove that the relationship asserted in Equation [\ref=eq:kl-likelihood]. This derivation basically involves three main ideas: the application of Stirling's approximation, playing around with some algebra and recognizing an implicit probability distribution. First, we begin with some key some definitions.

Multinomial likelihood.    The multinomial likelihood expresses the probability of observing a histogram, c  =  {ci} given that a particular model q  =  {qi} is true.

[formula]

The term in front [formula] is a normalization constant that counts the number of combinations which could give rise to the particular histogram. Note that [formula] is the total number of measurements.

Stirling's approximation.    Stirling's approximation, log n!≃n log n  -  n, is a numerical approximation useful for large factorials that often appear in combinatorics. This approximation becomes quite good for n  >  O(100).

We now begin the derivation by remembering that independent observations constituting a histogram are multiplied together to recover the joint probability of all measurements. Thus, an invariant likelihood across histogram counts is the geometric mean of the multinomial likelihood [formula]. We term this quantity the average multinomial likelihood, or average likelihood for short. We start by defining the average log-likelihood as

[formula]

Plugging in Equation [\ref=eq:likelihood] and a little algebra later,

[formula]

We now plug in Stirling's approximation to simplify

[formula]

Finally, rearranging terms highlights an implicit probability distribution.

[formula]

In the limit of n  →    ∞  , the normalized histogram can be viewed as a probability distribution [formula] and substituted accordingly.

[formula]

where we now recognize the KL divergence (Equation [\ref=eq:kl]). The results can be summarized as

[formula]

or the KL divergence is negative logarithm of the average multinomial log-likelihood.

A closer look at this derivation reveals that the normalization constant in front of Equation [\ref=eq:likelihood] directly results in the term [formula], which is the entropy of the distribution. Thus, it is possible to derive the entropy of a distribution from purely combinatorial notions [\cite=Jaynes2003].