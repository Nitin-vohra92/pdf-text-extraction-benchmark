Lemma Corollary Conjecture Proposition Heuristic

The Use of a Single Pseudo-Sample in Approximate Bayesian Computation

Introduction

Approximate Bayesian computation (ABC) is a family of algorithms for Bayesian inference that address situations where the likelihood function is intractable to evaluate but where one can obtain samples from the model. These methods are now widely used in population genetics, systems biology, ecology, and other areas, and are implemented in an array of popular software packages [\citep=tavare1997inferring] [\citep=marin2012approximate]. Let [formula] be the parameter of interest with prior density [formula], [formula] be the observed data and [formula] be the model. The simplest ABC algorithm first generates a sample [formula] from the prior, then generates a pseudo-sample [formula]. Conditional on [formula], the distribution of [formula] is the posterior distribution [formula]. For all but the most trivial discrete problems, the probability that [formula] is either zero or very small. Thus the condition of exact matching of pseudo-data to the observed data is typically relaxed to [formula] where η is a low-dimensional summary statistic, [formula] is a distance function, and ε > 0 is a tolerance level. The resulting algorithm gives samples from the target density πε that is proportional to [formula]. If the tolerance ε is small enough and the statistic(s) η good enough, then πε can be a good approximation to [formula].

A generalized version of this method [\citep=wilk:08] is given in Algorithm [\ref=abc-gen], where K(η) is an unnormalized probability density function, M is an arbitrary number of pseudo-samples, and c is a constant satisfying c  ≥   sup ηK(η).

Using the "uniform kernel" [formula] and taking M = c = 1 yields the version described above. Algorithm [\ref=abc-gen] yields samples [formula] from the kernel-smoothed posterior density Although Algorithm [\ref=abc-gen] is nearly always presented in the special case with M = 1 [\citep=wilk:08], it is easily verified that M  >  1 still yields samples from [formula].

Many variants of Algorithm [\ref=abc-gen] exist. For any unbiased, nonnegative estimator [formula] of [formula] and any reversible transition kernel q on Θ, Algorithm [\ref=pseudomarg] gives a Markov chain Monte Carlo (MCMC) version that is based on the pseudo-marginal algorithm of [\citep=andrieu2009the-pseudo-marginal] (see also [\citep=marj:03] [\citep=wilk:08] [\citep=Andrieu2014establishing-some-order]). To obtain an algorithm that is similar to Algorithm [\ref=abc-gen], a good choice for [formula] is where [formula]. When we refer to Algorithm [\ref=pseudomarg], we always use this family of weights unless stated otherwise. Despite the fact that the expression ([\ref=eqn:KDE]) depends on M, as n  →    ∞   the distribution of x(n) in Algorithm [\ref=pseudomarg] converges to the same distribution πK under mild conditions [\citep=Andrieu2014establishing-some-order]. Although the choice of M in [\eqref=eqn:KDE] generally does not affect the limiting distribution of x(n), it does affect the evolution of the stochastic process [formula].

We address the effect of M on the efficiency of Algorithm [\ref=pseudomarg] when using the weight [\eqref=eqn:KDE]. Increasing the number of pseudo-samples M decreases the variance of [\ref=pseudomarg], which one might think could improve the efficiency of Algorithm [\ref=pseudomarg]. Indeed, increasing M does decrease the asymptotic variance of the associated Monte Carlo estimates ([\cite=Andrieu2014establishing-some-order]). However, increasing M also increases the computational cost of each step of the algorithm. Although the tradeoff between these two factors is quite complicated, our main result in this paper gives a good compromise: the choice M = 1 yields a running time within a factor of two of optimal. We use natural definitions of running time in the contexts of serial and parallel computing, which are extensions of those used by [\cite=pitt2012on-some], [\cite=sherlock2013on-the-efficiency], and [\cite=Doucet2014efficient-implementation], and which capture the time required to obtain a particular Monte Carlo accuracy. Our definition in the serial computing case is the number of pseudo-samples generated, which is an appropriate measure of running time when drawing pseudo-samples is more computationally intensive than the other steps in Algorithm [\ref=pseudomarg], and when the expected computational cost of drawing each pseudo-sample is the same, i.e. when there is no computational discounting due to having pre-computed relevant quantities.

Several authors have drawn the conclusion that in many situations the approximately optimal value of M in pseudo-marginal algorithms (a class of algorithms that includes Algorithm [\ref=pseudomarg]) is obtained by tuning M to achieve a particular variance for the estimator [formula] [\citep=pitt2012on-some] [\citep=sherlock2013on-the-efficiency] [\citep=Doucet2014efficient-implementation]. This often means a value of M that is much larger than 1. We demonstrate that in Algorithm [\ref=pseudomarg] such a tuning process is unnecessary, since near-optimal efficiency is obtained by using estimates based on a single pseudo-sample (Proposition [\ref=thm:unifcor] and Corollary [\ref=cor:M1best]); these estimates are lower-cost and higher-variance than those based on several pseudo-samples. This result assumes only that the kernel K(η) is the uniform kernel [formula] (the most common choice). In particular, and in contrast to earlier work, it does not make any assumptions about the target distribution [formula].

Our result is in contrast to particle MCMC methods [\citep=andrieu2010particle], which [\citet=flury2011bayesian] demonstrated could require thousands, millions or more particles to obtain sufficient accuracy in some realistic problems. This difference between particle MCMC and ABC-MCMC is largely due to the interacting nature of the particles in particle MCMC, allowing for better path samples.

Efficiency of ABC and ABC-MCMC

For a measure μ on space X let [formula] be the expectation of a real-valued function f with respect to μ and let L2(μ)  =  {f:μ(f2)  <    ∞  } be the space of functions with finite variance. For any reversible Markov transition kernel H with stationary distribution μ, any function f∈L2(μ), and Markov chain X(t) evolving according to H, the MCMC estimator of μ(f) is [formula]. The error of this estimator can be measured by the asymptotic variance: which is closely related to the autocorrelation of the samples f(X(t)) [\citep=tierney1998ordering].

If H is geometrically ergodic, v(f,H) is guaranteed to be finite for all f∈L2(μ), while in the non-geometrically ergodic case v(f,H) may or may not be finite [\citep=robe:rose:08]. When v(f,H) is infinite our results still hold but are not informative. The fact that our results do not require geometric ergodicity distinguishes them from many results on efficiency of MCMC methods [\citep=guan:kron:07] [\citep=wood:schm:hube:09a]. When [formula], we define [formula].

We will describe the running time of Algorithms [\ref=abc-gen] and [\ref=pseudomarg] in two cases: first, when the computations are done serially, and second, when they are done in parallel across M processors. Using [\eqref=eqn:nav], the variance of n from a single (reversible) Markov chain H of length n is roughly v(f,H) / n, so to achieve variance δ > 0 in the serial context we need v(f,H) / δ iterations. Similarly, the variance of n from a collection of M reversible Markov chains, each run for n steps, is roughly v(f,H) / (Mn). Thus, to achieve variance δ > 0 in the parallel context we need to run each Markov chain for n  >  v(f,H) / (δM) iterations.

Although our definitions make sense for any function f of the two values [formula] described by Algorithm [\ref=pseudomarg], throughout the rest of the note we restrict our attention to functions that depend only on the first coordinate, [formula]. That is, when discussing Algorithm [\ref=pseudomarg] or other pseudo-marginal algorithms, we restrict our attention to functions that satisfy [formula] for all [formula] and all T1,T2. We slightly abuse this in our notation, not distinguishing between a function [formula] of a single variable [formula] and a function [formula] of two variables [formula] that only depends on the first coordinate.

Let QM be the transition kernel of Algorithm [\ref=pseudomarg]; like all pseudo-marginal algorithms, QM is reversible [\citep=andrieu2009the-pseudo-marginal]. We assume that drawing pseudo-samples is the slowest part of the computation, and that drawing each pseudo-sample takes the same amount of time on average (as also assumed in Pitt et al. 2012, Sherlock et al. 2013, Doucet et al. 2014). Then the running time of QM in the serial context can be measured as the number of iterations times the number of pseudo-samples drawn in each iteration, namely [formula]. In the context of parallel computation across M processors, we compare two competing approaches that utilize all the processors. These are: (a) a single chain with transition kernel QM, where the M > 1 pseudo-samples in each iteration are drawn independently across M processors; and (b) M parallel chains with transition kernel Q1. The running time of these methods can be measured as the number of required Markov chain iterations to obtain accuracy δ, namely [formula] utilizing method (a) and [formula] utilizing method (b). Since both measures of computation time are based on the asymptotic variance of the underlying Markov chain, they ignore the initial 'burn-in' period and are most appropriate when the desired error δ is small. Other measures of computation time should be used if the Markov chains are being used to get only a very rough picture of the posterior (e.g. to locate, but not explore, a single posterior mode). Note, however, that in practice there is typically no burn-in period for Algorithm [\ref=pseudomarg], since it is usually initialized using samples from ABC [\citep=marin2012approximate].

For Algorithm [\ref=abc-gen] the running time, denoted by RM, is defined analogously. However, we must account for the fact that each iteration of RM yields one accepted value of [formula], which may require multiple proposed values of [formula] (along with the associated computations, including drawing pseudo-samples). The number of proposed values of [formula] to get one acceptance has a geometric distribution with mean equal to the inverse of the marginal probability pacc(RM) of accepting a proposed value of [formula]. So, similarly to QM, the running time of RM in the context of serial computing can be measured as [formula], and the computation time in the context of parallel computing can be measured as [formula] utilizing method (a) and [formula] utilizing method (b).

Using these definitions, we first state the result that M = 1 is optimal for ABC (Algorithm 1). This result is widely known but we could not locate it in print, so we include it here for completeness.

The marginal acceptance probability of ABC (Algorithm [\ref=abc-gen]) does not depend on M. For M > 1 the running times [formula] and [formula] of ABC in the serial and parallel contexts satisfy [formula] and [formula] for any f∈L2(πK) and any δ > 0.

The marginal acceptance probability of Algorithm [\ref=abc-gen] is

[formula]

which does not depend on M. The results for the running times follow immediately.

Our contribution is to show a similar result for ABC-MCMC. A potential concern regarding Algorithm [\ref=pseudomarg] is raised by [\cite=lee2013ergodicity], who point out that this algorithm is generally not geometrically ergodic when q is a local proposal distribution, such as a random walk proposal. This is due to the fact that in the tails of the distribution πK, the pseudo-data [formula] are very different from [formula] and so the proposed moves are rarely accepted. This problem can be fixed in several ways. Lee and Latuszynski (2013) give a sophisticated solution that involves choosing a random number of pseudo-samples at every step of Algorithm [\ref=pseudomarg], and they show that this modification increases the class of target distributions for which the ABC-MCMC algorithm is geometrically ergodic. One consequence of our Proposition 4 is that a simpler 'obvious' fix to the problem of geometric ergodicity does not work: increasing the number of pseudo-samples used in Algorithm [\ref=pseudomarg] from 1 to any fixed number M has no impact on the geometric ergodicity of the algorithm.

Our main tool in analyzing Algorithm [\ref=pseudomarg] will be the results of [\cite=Andrieu2014establishing-some-order]. Two random variables X and Y are convex ordered if [formula] for any convex function φ; we denote this relation by X  ≤  cxY. Let H1,H2 be the transition kernels of two pseudo-marginal algorithms with the same proposal kernel q and the same target marginal distribution μ. Denote by T1,x and T2,x the estimators of the unnormalized target used by H1 and H2 respectively. Recall the asymptotic variance v(f,H) from ([\ref=eqn:nav]); although f could be a function on [formula], we restrict our attention to functions f on the non-augmented state space X. Then if T1,x  ≤  cxT2,x for all x, Theorem 3 of [\cite=Andrieu2014establishing-some-order] shows that v(f,H1)  ≤  v(f,H2) for all f∈L2(μ). As shown in Section 6 of that work, this tool can be used to show that increasing M decreases the asymptotic variance of Algorithm [\ref=pseudomarg]:

For any f∈L2(πK) and any [formula] we have v(f,QM)  ≥  v(f,QN).

In the appendix we give a similar comparison result for the alternative version of ABC-MCMC described in [\cite=wilk:08].

We will show that, despite Corollary [\ref=thm:likFree], it is not generally an advantage to use a large value of M in Algorithm [\ref=pseudomarg]. To do this, we first give a result that follows easily from Theorem 3 of [\cite=Andrieu2014establishing-some-order]. For any 0  ≤  α  <  1 and i∈{1,2}, define the estimator Ti,x,α has the same mean as Ti,x, but is a worse estimator. In particular, Proposition 1.2 of [\cite=Lekela2014conditional-convex] implies Ti,x  ≤  cxTi,x,α for any 0  ≤  α  ≤  1 and any i∈{1,2} (Equation [\eqref=eqn:handicap] gives the coupling required by that proposition). We have:

Assume that H2 is nonnegative definite. For any 0  ≤  α  <  1, if T1,x  ≤  cxT2,x,α for all x, then for any f∈L2(μ) we have

Theorem [\ref=thm:R1RM] is proven in the appendix. The assumption that H2 is nonnegative definite is a common technical assumption in analyses of the efficiency of Markov chains [\citep=wood:schm:hube:09a] [\citep=nara:rakh:10], and is done here so we can compare v(f,H2) to v(f). It can easily be achieved, for example, by incorporating a "holding probability" (chance of proposing to stay in the same location) of 1/2 into the proposal kernel q [\citep=wood:schm:hube:09a].

Theorem [\ref=thm:R1RM] yields the following bound for Algorithm [\ref=pseudomarg], proven in the appendix.

If [formula] for some ε > 0, and if the transition kernel QM of Algorithm [\ref=pseudomarg] is nonnegative definite, then for any f∈L2(πK) we have

This yields the following bounds on the running times:

Using the uniform kernel and assuming that QM is nonnegative definite, the running time of Q1 is at most twice that of QM, for both serial and parallel computing:

[formula]

Corollary [\ref=cor:M1best] implies that, under the condition that a uniform kernel is being used and that all pseudosamples have the same computational cost, it is only possible to improve the running time of the Markov chain by a factor of 2 by choosing QM rather than Q1. Thus, under these reasonable conditions, there is never a strong reason to use QM over Q1, and in fact there can be a strong reason to use Q1 over QM.

Simulation study

We now demonstrate these results through a simple simulation study, showing that choosing M > 1 is seldom beneficial. We consider the model [formula] for a single observation [formula], where σy is known and [formula] is given a standard normal prior, [formula]. We apply Algorithm [\ref=pseudomarg] using proposal distribution [formula], summary statistic [formula], and [formula] equal to the uniform kernel with bandwidth ε, and subsequently study the algorithm's acceptance rate, which due to the independent proposal is analogous to effective sample size. We start by exploring the case where [formula] and σy = 1, simulating the Markov chain for 5 million iterations. Figure [\ref=fig:1] (left) shows the acceptance rate per generated pseudo-sample as a function of M. Large M does not provide a benefit in terms of accepted [formula] samples per generated pseudo-sample, and can even decrease this measure of efficiency, supporting the result of Corollary [\ref=cor:M1best]. In fact, replacing the uniform kernel with a Gaussian kernel (not shown) leads to indistinguishable results. In Figure [\ref=fig:1] (right) we look at the ε which results from requiring that a fixed percentage (0.4%) of [formula] samples are accepted per unit computational cost. This means that for M = 1 we require that 0.4% of samples are accepted, while for M = 64 we require that 0.4  ×  64  =  25.6% of samples are accepted.

In certain cases, there is an initial fixed computational cost to generating pseudo-samples, after which generating subsequent pseudo-samples is computationally inexpensive. If [formula] is a length-n sample from a Gaussian process, for instance, there is an initial O(n3) cost to decomposing the covariance, after which each pseudo-sample may be generated at O(n2) cost. In the case with discount factor δ > 1 (representing a δ  ×   cost reduction for all pseudo-samples after the first), the pseudo-sampling cost is (1 + (M - 1) / δ), so we require that 0.4  ×  (1 + (M - 1) / δ)% of [formula] samples are accepted. For example, a discount of δ = 16 with M = 64 requires that 2.0% of samples are accepted. Figure [\ref=fig:1] shows that, for δ = 1, larger M results in larger ε; in other words, for a fixed computational budget and no discount, M = 1 gives the smallest ε. For discounts δ > 1, however, increasing M can indeed lead to reduced ε.

We further explore discounting in Figure [\ref=fig:3], which uses a discount of δ  =  8 and varies [formula] from 2 to 8 (left plot) and σy from 0.01 to 2 (right plot). In both cases the changes are meant to induce a divergence between the prior and the likelihood, and hence the prior and posterior. In these figures, the requisite ε are scaled such that at M = 1 all normalized ε are 1. Figure [\ref=fig:3] (left) shows that as [formula] grows, the benefit associated with using higher values of M shrinks and eventually disappears. This is because for large [formula] a large value of ε is required in order to frequently get a nonzero value for the approximated likelihood and thus a reasonable acceptance rate; for instance, the unnormalized value of ε is 0.08 when [formula] and M = 1, while ε = 7.64 when [formula] and M = 1. As such, the increased diversity from multiple samples is dwarfed by the scale of ε. In Figure [\ref=fig:3] (right) we examine sensitivity of our conclusions to σy. For large σy, additional (discounted-cost) pseudo-samples provide a benefit, because they improve the accuracy of the approximated likelihood. However, for small values of σy, the variability of the pseudo-samples [formula] is low and so additional pseudo-samples do not provide much incremental improvement to the likelihood approximation. In summary, we only find a benefit of increasing the number of pseudo-samples M in cases where there is a discounted cost to obtain those pseudo-samples, and even then the benefit can be decreased or eliminated when [formula] is extreme under typical proposed values of [formula], or when the variability of [formula] under the model is low.

Discussion

In this paper, we have shown that despite the true likelihood leading to reduced asymptotic variance relative to the approximated likelihood constructed through ABC, in practice one should stick with simple, single pseudo-sample approximations rather than trying to accurately approximate the true likelihood with multiple pseudo-samples. Our results are obtained by bounding the asymptotic variance of Markov chain Monte Carlo estimates, which takes into account the autocorrelation of the Markov chain. This is not to say that M = 1 is optimal in all situations. In many cases, there is a large initial cost to the first pseudo-sample, with subsequent samples drawn at a much reduced computational cost. In this case M > 1 can lead to improved performance.

We hope that this work not only provides practical guidance on the choice of the number of pseudo-samples when using ABC, but also that it might lead to future research in the analysis of these algorithms. As a specific example, it would be fruitful to pursue the results in this paper extended to non-uniform kernels such as the popular Gaussian kernel. The main difficulty in doing so is extending the explicit calculation [\eqref=IneqMainCalcForProp] in the proof of Proposition [\ref=thm:unifcor]. Although we have found it possible to obtain similar bounds for specific kernels, target distributions and proposal distributions via grid search on a computer, and simulations suggest that our conclusions hold in greater generality, we have not been able to extend this inequality to general kernels and general target distributions simultaneously.

Acknowledgement

The authors thank Alex Thiery for his careful reading of an earlier draft, as well as Pierre Jacob, Rémi Bardenet, Christophe Andrieu, Matti Vihola, Christian Robert, and Arnaud Doucet for useful discussions. This research was supported in part by U.S. National Science Foundation grants 1461435, DMS-1209103, and DMS-1406599, by DARPA under Grant No. FA8750-14-2-0117, by ARO under Grant No. W911NF-15-1-0172, and by NSERC.

Proofs

Denote by H2,α the transition kernel of the pseudo-marginal algorithm with proposal kernel q, target marginal distribution μ, and estimator T2,x,α of the unnormalized target. If we denote by [formula] and [formula] the Markov chains driven by the kernels H2,α and [formula] respectively, then [formula] and [formula] have the same distribution.

If T1,x  ≤  cxT2,x,α then by Theorem 3 of [\cite=Andrieu2014establishing-some-order], for any f∈L2(μ). We also have

where the first inequality follows from Corollary 1 of [\cite=latuszynski2013clts] and the second follows from the fact that H2 has nonnegative spectrum. Combining this with [\eqref=IneqViholaAppAsymVar] yields the desired result.

For any M  ≥  1, let [formula] be the estimator [formula] of the target πK, so that [formula] is [formula] handicapped by α as defined in [\eqref=eqn:handicap] of the main document. To obtain ([\ref=eqn:firstBound]) of the main document, by Theorem [\ref=thm:R1RM] it is sufficient to take [formula] and show that [formula]. By Proposition 2.2 of [\cite=Lekela2014conditional-convex], it is furthermore sufficient to show that, for all [formula],

Let [formula] denote the binomial distribution with n trials and success probability ψ. For a given point [formula], let [formula]. Noting that [formula], we may then write [formula] and [formula] as the following mixtures where δ0 is the unit point mass at zero. Denote [formula] and [formula]. We will check condition [\eqref=IneqConvCond] for [formula] and 0  ≤  c  ≤  1, then separately for c  <  0 and c  >  1. For 0  ≤  c  ≤  1, we compute: For c  <  0, we have and the analogous calculation gives the same conclusion for c  ≥  M. Finally, For 1 < c  <  M, note Also, the functions [formula] and [formula] are continuous, convex and piecewise linear. For c  ≥  1, they satisfy where the derivative of f2 exists. Combining inequalities [\eqref=IneqSimpleC] and [\eqref=IneqDerC], we conclude that for all 1  <  c  <  M. Thus we have verified [\eqref=IneqConvCond] and the proposition follows.

Analysis of an Alternative ABC-MCMC Method

We give a result analogous to Corollary [\ref=thm:likFree] for the version of ABC-MCMC proposed in [\cite=wilk:08], given in Algorithm [\ref=likfree-mh] below. The constant c can be any value satisfying [formula].

Lemma [\ref=Thm:altmcmc] compares Algorithm [\ref=likfree-mh] (call its transition kernel [formula]) to Q∞.

For any f∈L2(πK) we have v(f,)  ≥  v(f,Q∞).

Both [formula] and Q∞ have stationary density πK, so by Theorem 4 of [\cite=tierney1998ordering], it suffices to show that [formula] for all [formula] and measurable A  ⊂  Θ. Since [formula] and Q∞ use the same proposal density q, it is furthermore sufficient to show that for every [formula] and [formula], the acceptance probability of Q∞ is at least as large as that of [formula]. Since [formula] is a probability density,

[formula]

So the acceptance probability of [formula], marginalizing over [formula], is The acceptance probability of the Metropolis-Hastings algorithm is Using [\eqref=Eqn:supBound], [\eqref=eqn:accabcmc], and [\eqref=eqn:accMH], [formula].