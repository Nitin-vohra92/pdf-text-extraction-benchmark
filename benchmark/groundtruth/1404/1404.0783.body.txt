Lemma Corollary

Task Assignment in Tree-Like Hierarchical Structures

Introduction

In the standard assignment problem (or as sometimes referred to linear assignment problem) [\cite=BDM09], the number of tasks and the number of agents are equal, and a scalar value is used to represent the cost/performance of assigning a task to an agent. The objective of the assignment problem is to determine an assignment such that each task is assigned to a different agent and the summation of the costs/profits of the assignment is minimized/maximized. Many different variations of this problem have already been studied including Generalized Assignment Problem [\cite=CKR06] [\cite=FGMS06] [\cite=G93] [\cite=A95]. In this work, we also investigate a new version of the standard assignment problem which appears in real-life applications.

In real-life, most of the large organizations such as corporations, governments, military etc., have hierarchical structures. Hierarchical organizations are nothing but trees where each node corresponds to an entity in the organization, and entity sub-entity relationships are represented as parent-child relationships.

In the standard assignment problem, agents are flat, and have got no structure imposed on them one task is assigned to one agent. However, in Maximum Weight Tree Matching (MWTM) problem, since agents are organized as a tree, and sub-entities in the tree represent sub-parts of the agents, an additional constraint, named hereafter as hierarchy constraint, is introduced to the assignment problem: When a task is assigned to an agent, no other assignment can be made to its sub-entities, as they are assumed to be a part of an agent already assigned. This constraint indirectly implies another constraint. Since an agent should be assigned to a task as a whole along with its sub-parts, when one of its sub-parts has already been assigned, then it cannot be assigned itself to any task. In other words, if an agent is assigned to a task, none of its ancestors in the tree can be assigned at all. In a more general term, on every path from the root to a leaf in a tree, there could be at most a single assignment. This, in turn, is easily seen to lead to the observation that the number of leaves in the tree should be at least equal to the number of tasks to be executed. Otherwise no feasible assignment exists.

A simpler version of MWTM problem where each node has the same assignment weight for all the tasks to be performed has been introduced in [\cite=GT10pack]. It is called tree like weighted set packing  in [\cite=GT10pack] since the set-subset relationships form a tree, and the weight assigned to each set (or each node in the tree) can be interpreted as the weight of assigning a task to that node. The same hierarchy (independence) constraint has been enforced to prevent the selection of two sets having set-subset relationships (either directly or indirectly), and finally the number of sets to be packed (selected) is given to maximize the total weight. That problem effectively becomes a simpler version of the problem studied in this paper, and an effective dynamic programming solution to it has been developed in [\cite=GT10pack].

Although many different versions of assignment problems have been defined and explored, there are only a very few problems remotely related to MWTM problem reported in the literature, such as [\cite=WZS13], and [\cite=SRSP06]. Similar to MWTM, both of these problems introduce different kinds of set constraints on the vertices of a bipartite graph, and they have both been shown to be NP-hard. Therefore, heuristic solutions have been proposed, namely a greedy heuristic for [\cite=WZS13], and a genetic algorithm based solution for [\cite=SRSP06], and these solutions have been shown to be quite effective.

MWTM problem has already been introduced in [\cite=GT10], and a generic heuristic (genetic algorithm) has been used to solve it. In [\cite=GT10], it has been shown that GA works quite effectively in terms of solution quality for randomly generated inputs. Although the number of iterations were not very large, due to the cost of each genetic operator among the chromosome populations, each iteration takes a considerably long time to complete, and therefore we have observed that the execution times are much higher to reach to the level of near-optimal results obtained with the approach proposed in this paper. Since GA approach uses a generic heuristic (slightly customized for the problem), it is actually not fair to compare it with our problem-specific heuristic, which is much more effective. Moreover, although GA approach has been applied to different sized inputs, significant input parameters have not been explored in its evaluation in [\cite=GT10] corresponding to the structure and the distributions of the weights. That is why we have compared the quality of the solutions of our heuristic proposed in this paper with that of ILP only which produces the optimal (whenever possible). This paper has the following additional contributions to [\cite=GT10]:

The problem is shown to be NP-hard,

An Integer Linear Programming (ILP) model of the problem is given,

An iterative Linear Programming (LP) relaxation solution is developed,

The effectiveness of the proposed iterative LP-relaxation solution is verified through extensive tests.

Iterative LP-relaxation or rounding algorithms have previously been used. A factor 2 approximation algorithm is presented in [\cite=J01] for finding a minimum-cost subgraph having at least a specified number of edges in each cut. This class of problems defined in [\cite=J01] includes the generalized Steiner network problem also known as the survivable network design problem. The algorithm in [\cite=J01] first solves the linear relaxation of ILP formulation of the problem, and then iteratively rounds off the solution. The approach taken in [\cite=J01] has been generalized and formalized in [\cite=JThesis00]. In order to exploit the full power of LP, a new technique called iterative rounding has been introduced in [\cite=JThesis00]. Iterative rounding is used in [\cite=JThesis00] to iteratively recompute the best fractional solution while maintaining the rounding of the previous phases. Although an iterative rounding based heuristic solution is developed in this paper for MWTM, the presence of the hierarchy constraint does not simply lend itself to the consideration of fractional values from the highest to the smallest.

The rest of the paper has been organized as follows. The next section formally introduces the problem, and proves its NP-hardness. Section [\ref=sec:ILP], describes a mathematical (integer linear programming) formulation, and Section [\ref=sec:bottom-up] presents how its relaxation to LP can be iteratively used as an effective heuristic. Section [\ref=sec:experiments] describes the experiments and their results. Finally, the last section presents concluding remarks.

Problem Description and its NP-Hardness

We will now introduce Maximum Weight Tree Matching (MWTM) problem formally.

A tree T with n nodes rooted at a node r, and a separate set of m tasks are given. Associated with each node i in T is a real valued function wi,j denoting the weight of assigning node i to task j for all i∈{1..n} and j∈{1..m} The problem of finding an assignment of all tasks to nodes in T with the maximum total weight in such a way that the assignment between nodes and tasks forms a matching, and no node assigned to a task is allowed to have any ancestors (or descendants) which have also been assigned to a task is named MWTM.

It should be noted that the requirement for the weight function to be defined for all combinations of nodes and tasks in MWTM stems from a deliberate decision. MWTM is more restricted than its possible variants where some combinations of nodes and tasks can be forbidden. As MWTM can be reduced directly to these more general forms, NP-hardness of them would easily follow once MWTM is shown to be NP-hard.

The constraint associated with the hierarchical structure of the tree dictates that no two nodes on the same path from the root r to a leaf node in T can ever be simultaneously assigned in a solution to an instance of MWTM.

Two paths in a tree from the root to any two distinct nodes are said to be independent paths if and only if none of the two paths is a subset of the other.

In the light of this definition, the hierarchy constraint can simply be restated as the requirement that the paths from the assigned nodes to the root are all pairwise independent.

MWTM can be shown to be NP-hard by a polynomial time reduction from E3-SAT which is a variant of 3-satisfiability (3-SAT) problem. E3-SAT (resp. 3-SAT) is defined to be the problem of deciding whether a satisfying truth assignment is possible for the variables of a given Boolean formula in Conjunctive Normal Form (CNF) where each clause is a disjunction of exactly (resp. at most) three literals each of which is either a variable or its negation. 3-SAT is one of Karp's 21 NP-complete problems [\cite=K72]. Any given instance of 3-SAT can be easily transformed to a corresponding instance of E3-SAT by introducing three new dummy variables, d1, d2, and d3. While only d1 is inserted into the clauses with one literal, both d1 and d2 are inserted into the clauses with two literals. In order to make sure in any satisfying assignment that the dummy variables can only be set to false, the conjunction of all maxterms of the dummy variables except d1  +  d2  +  d3 are finally appended to the clauses each with exactly three literals now. The NP-completeness of E3-SAT is hence confirmed.

A given instance of E3-SAT problem is transformed to a corresponding instance of MWTM in time polynomial in the size of the input Boolean expression. Let a given instance of E3-SAT have n variables denoted by xi where i∈[1..n] and a 3-CNF formula [formula] where each Ci represented by [formula] is a disjunction of three literals corresponding to either a variable or its negation. The transformation starts by introducing the root node designated by r to the initially empty tree T of the corresponding MWTM instance at level 0. The root node r is numbered as 1. For each variable xi, two child nodes to root r are then created numbered 2i for xi, and 2i + 1 for [formula] corresponding to assigning true and false respectively to this variable. The parents of all such nodes are set to point to node r. As there are n distinct variables in the given E3-SAT instance, the root r of T in the corresponding MWTM instance becomes populated with a total of 2n children at level 1 of T after this step. These are called variable nodes (see Figure [\ref=fig:reduction]). In the final step of the construction of T, for each literal Ci,j where i∈[1..m], and j∈[1..3], a node numbered 1 + 2n + 3(i - 1) + j is created. The parent of such a node is set to 2k if Ci,j  =  xk, and to 2k + 1 otherwise if [formula] where k∈[1..n]. What this step achieves in effect for each node corresponding to assigning true to xk or to its negation [formula] at level 1 is the creation of as many children at the next level 2 under the relevant variable node as there are occurrences of the corresponding variable in the clauses of the given E3-SAT instance. The tree T constructed is shown in Figure [\ref=fig:reduction]. While parent-child relationships are indicated by solid lines in this figure, dashed lines depict the weight function wi,j. It should be noted that the variable nodes at level 1 will have as many children as there are occurrences of the corresponding literal at level 2 which is implied by the existence of multiple edges emanating from a variable node while the nodes corresponding to literals in clauses at level 2 will have a single edge to their parent as shown in the figure. The nodes at level 2 are accordingly called literal nodes.

Once we obtain the tree T in MWTM instance corresponding to the given instance of E3-SAT, we also set the number of tasks to m  +  n. Each task ti for i∈[1..m] corresponds to satisfying a clause Ci. We call these clausal tasks. Each task ti for i∈[m  +  1..m  +  n] among the rest of the tasks , however, are used to enforce that the corresponding variable xi - m is set to either one of true or false consistently over all clauses. We call such tasks enforcement tasks.

Apparently, the total number of nodes in T in the corresponding instance of MWTM is given by 1  +  2n  +  3m where n and m are the number of variables and clauses respectively specified in the given E3-SAT instance. The number of tasks, on the other hand, is n  +  m. The concluding step of the transformation is to appropriately set the corresponding values wi,j for all nodes i∈[1..1  +  2n  +  3m] in T, and all tasks j∈[1..m  +  n] as shown in Equation [\ref=eqn:weight] below:

[formula]

The weights of carrying out any one task by the root node are all initialized to zero. For a variable node i∈[2..2n  +  1] at level 1 corresponding to [formula] or [formula] depending on whether i is even or odd respectively, however, the weights of executing tasks are set in such a way that a consistent assignment of truth values to individual variables can be enforced. The only task whose execution by node i can have a positive contribution to the solution is therefore the corresponding enforcement task [formula]. At level 2 are the literal nodes ranging from 2n + 2 to 1 + 2n + 3m corresponding to the literals in the clauses of the given E3-SAT instance. Since each literal can accordingly be set to satisfy a clause in which it occurs, the weight wi,j of assigning a level 2 node i representing a literal Cp,q to clausal task tj corresponding to the clause Cp itself is appropriately set to 1 to reflect a feasible assignment. Therefore, the equalities i = 1 + 2n + 3(p - 1) + q, and j = p must hold. Noting that q can only take on the values 1 through 3 inclusive readily gives [formula], and q  =  (i - 2n - 2) mod 3  +  1. All other combinations of nodes and tasks have weight 0.

It should be pointed out that an MWTM instance so constructed would always lend itself to feasible solutions since the number of leaf nodes in T is greater than or equal to the number of tasks. This last inequality can be seen to hold by noting that m  ≥  (2n - t) / 3 where t∈[null]. It is accordingly noted at this point that we can slightly modify the illustrated transformation from E3-SAT to MWTM to obtain a transformation also from MAX-E3-SAT to MWTM. First, the weights of assigning the variable nodes to the corresponding enforcement tasks are set to m (the number of clauses). Then, m additional dummy nodes whose weights of executing any one of the tasks have all been initialized to zero are introduced as children directly to the root. It is now easily seen that any given instance of MAX-E3-SAT denoted by Π1 has a solution with value k* if and only if the corresponding instance of MWTM denoted by Π2 has a solution with a value of k* + mn. This polynomial time reduction can also be used to establish that MWTM cannot have a polynomial-time approximation scheme (PTAS). Otherwise, we could use it to obtain a 7 / 8 + ε approximation algorithm for MAX-E3-SAT, and hence a contradiction. In order to see this, let us assume that MWTM has a 1 - δ approximation where δ∈(0,1]. For a given instance of MAX-E3-SAT, the corresponding instance of MWTM is first obtained in polynomial time using the transformation just depicted. Setting [formula], the approximation algorithm for MWTM is run next on the transformed instance to return k + mn  ≥  (1 - δ)(k* + mn) where k and k* are the number of clausal tasks in the approximate and optimal solutions respectively. We can then write the inequality (k* + mn) - (k + mn)  ≤  (k* + mn) - (1 - δ)(k* + mn). Arranging the left and the right hand sides, we obtain k* - k  ≤  δ(k* + mn). For sufficiently large values of m and n, the inequality can be rewritten as k* - k  ≤  δmnk* which is, in turn, arranged to give (1 - δmn)k*  ≤  k. Substituting the value for δ, (7 / 8 + ε)k*  ≤  k is readily obtained contradicting the fact that no such approximation is possible unless P = NP. A very trivial result can hence be stated as in the following corollary.

There exists no 1 - ε approximation algorithm for MWTM problem where ε∈(0,1] unless P = NP.

ILP Formulation of MWTM Problem

In an instance of MWTM, the number of nodes organized as a tree, T, and the number of tasks are given by n and m respectively. The weight of executing each task j by a node i is also denoted by wi,j where i∈[1..n] and j∈[1..m]. Let r designate the root of this tree, T. Let us denote by λ  ⊆  {1..n} the leaf nodes of T. Each unique path from the root r to a leaf node k∈λ is represented by a set of nodes on this path which is denoted by Πk. Integer Linear Programming (ILP) formulation of MWTM problem can thus be given as:

[formula]

The inequality in ([\ref=eqn:LP1]) simply means a node can be assigned to at most one task. The constraint in ([\ref=eqn:LP2]) is used to enforce that every task is executed by a single node. In order to enforce that on any path leading to a leaf node, at most one node can be assigned to a task, ([\ref=eqn:LP3]) is used. Finally, ([\ref=eqn:LP4]) is there to make sure that decision variables xi,j can take on the integer values 0 and 1 only. The given ILP formulation can readily be relaxed to an LP by removing the last constraint ([\ref=eqn:LP4]) which restricts xi,j values to either 0 or 1.

Bottom-Up Assignment Heuristic

In this section, a heuristic solution is developed in an effort to solve MWTM effectively. When ILP formulation is relaxed by removing the last constraint ([\ref=eqn:LP4]) to obtain an LP model, xi,j can take on fractional values in the range

[formula]

Experiments

In order to measure the performance of LP-relaxation based heuristic BOA in Algorithm [\ref=alg:bottom-up], several experiments have been performed for varying problem parameters. The parameters employed, and their values are as follows:

#Nodes: It represents the number of nodes in the tree in a given MWTM instance. In order to generate a variety of tree sizes, the following values are employed in the experiments: 16 (small tree), 32, 64, and 128 (large tree).

Average Degree: This parameter is defined to be the average degree of a node in the tree in a given instance of MWTM. It is tuned throughout the experiments to control the type of trees generated in a scale ranging from deep to shallow for fixed values of #Nodes parameter. The values used in the experiments are 1.5 (deep tree), 2.0, and 2.5 (shallow tree).

[formula]: It is defined to be the ratio of the number of the tasks to the number of the nodes in the tree associated with a given MWTM instance. This parameter is used to generate a range of MWTM instances changing from those with a very few tasks called sparse to those with a large number of tasks called dense in proportion to the tree size. The values used are 0.125 (sparse), 0.25, and 0.5 (dense). As this ratio increases, the flexibility to use non-leaf nodes for assignments decreases.

Weight Distribution: The weight of assigning a node to a task has a value chosen from the range

[formula]

For each combination of these four parameters, a total of 4 * 3 * 3 * 3  =  108 different test cases are formed. For each test case, 20 instances of the problem are then randomly generated, and their averages are taken in the experiments. We record the total number of LP calls made at line [\ref=boa:LP] in BOA for every instance. Corresponding to each instance, both the execution time and the solution obtained are also recorded once for the corresponding ILP formulation which gives the optimal solution, and once for BOA expected to return a suboptimal solution.

All the tests were run on a machine with a 4 GB of RAM and an Intel Core 2 Duo T9550 2.66 Ghz mobile processor. Microsoft Solver Foundation 3.0 was employed as LP/ILP solver library, and the code was developed in C  #  5.0.

The results of the experiments are presented through a series of seven tables in this section. These tables all share a common structure. As the topmost two rows are used to set the values for the parameters Average Degree and [formula], the leftmost two columns display the values for the parameters Weight Distribution and #Nodes. The last six tables, on the other hand, can be logically grouped into three each with two tables. While the first table in a group presents a comparison between the execution times of ILP and BOA, the second evaluates the quality of the solutions by BOA against the optimal. These three groups correspond to the three distinct values that the Average Degree parameter can take on, namely 2.5, 2.0, 1.5, and are presented in this order. Of the four parameters only one, namely the Average Degree, is fixed, and the average results are given for all combinations of the other three parameters in these groups of tables. Finally, an additional row labeled Method is inserted as the third from the top to allow us to specify either ILP or BOA in these tables. It should be noted that the cells at the same position in both tables in the same group correspond to the exact same combination of parameter values.

The colors yellow and green are used consistently to highlight the cells containing NaN and ∞   respectively in all the tables. The cells in yellow marked with NaN in a table mean that there exists no feasible solution. For some combinations of parameters no feasible solution was possible. Especially when the instances get dense, and the trees associated with them become deep, as would be expected, it becomes more difficult to find a feasible solution satisfying the hierarchy constraint. Such configurations are characterized with high [formula] values, and with the low values of the Average Degree parameter. The results in the tables to follow confirm this expectation. All such cases leading to infeasibility are shown in yellow. Moreover, when the weight distribution is such that it is decreasing from the root to the leaves, finding an optimal solution becomes even more difficult using ILP. Under these circumstances, the execution time for ILP grows very quickly after the number of nodes become larger than 16. We do not include these extremely large execution times in the tables, and indeed we have canceled those solutions without finding the optimal values. All such cells are displayed in green marked with an ∞   symbol. The existence of feasible solutions by BOA in the corresponding cells, on the other hand, is an evidence for the existence of the optimal solutions for those cases as well. In order to verify, therefore, the quality of a solution by BOA in these situations, we make use of the corresponding possibly fractional LP relaxation solution as a potential upper bound. A quick inspection of the relevant cells reveals that the difference is very small even in these cases which definitely guarantees an even smaller distance to the actual optimal. It is hence suspected that BOA might even have achieved it.

The table in Figure [\ref=fig:expNoOfLPCalls] displays the average number of LP invocations performed at line [\ref=boa:LP] in BOA in Algorithm [\ref=alg:bottom-up] for each of 108 different test cases. As the table clearly reflects, the number of times the call to the corresponding LP relaxation gets executed is very close to 1. The cells marked with NaN all correspond to the test cases for which no feasible solutions exist as explained above.

The two tables in Figure [\ref=fig:expTime_2.5] and Figure [\ref=fig:expGoal_2.5] display the execution times, and the solutions respectively when the parameter representing the average degree of a node in the tree is set to 2.5 which corresponds to shallow trees. There are only 3 out of 36 test cases where BOA is slightly slower in Figure [\ref=fig:expTime_2.5]. These correspond to the test cases where: i) [formula] = 1/8, Weight Distribution = random, #Nodes = 128, ii) [formula] = 1/4, Weight Distribution = increasing, #Nodes = 64, and iii) [formula] = 1/4, Weight Distribution = random, #Nodes = 64. BOA, on the other hand, achieves optimal or almost optimal solutions as seen in Figure [\ref=fig:expGoal_2.5] for these test cases. Also an examination of the cells corresponding to these test cases in the table in Figure [\ref=fig:expNoOfLPCalls] reveals that they all have the value one.

These execution time anomalies observed to occur when BOA finds an almost optimal solution in only one iteration can therefore be explained by the overhead introduced by BOA. When BOA obtains an almost optimal solution with a single LP call, it would be natural to also expect ILP itself to discover the optimal integer assignments quickly. As BOA has some additional computations, its running time for such cases would be slightly more than that of ILP.

Even when it takes forever to compute the optimal by ILP, the values in the corresponding blue cells in Figure [\ref=fig:expTime_2.5] are all available for BOA as an indication of its running time performance. In terms of solution quality, BOA always achieves optimal solutions when Weight Distribution is such that it is increasing from the root to the leaves. Otherwise, the solutions obtained as shown in Figure [\ref=fig:expGoal_2.5] are so close to the corresponding optimal values that it is easily seen to perform within 1% of even the upper bounds obtained via the corresponding LP relaxation solution.

The tables in Figure [\ref=fig:expTime_2.0] and Figure [\ref=fig:expGoal_2.0] display the execution times, and the solutions respectively when the Average Degree parameter is set to 2.0. There are this time 4 out of 36 test cases where BOA turns out to be slower than the ILP solver library, and these correspond to the cells in Figure [\ref=fig:expTime_2.0] characterized by: i) [formula] = 1/8, Weight Distribution = random, #Nodes = 64, ii) [formula] = 1/8, Weight Distribution = random, #Nodes = 128, iii) [formula] = 1/4, Weight Distribution = random, #Nodes = 64, and iv) [formula] = 1/2, Weight Distribution = increasing, #Nodes = 16. An inspection of the respective cells corresponding to these test cases in both Figure [\ref=fig:expNoOfLPCalls] and Figure [\ref=fig:expGoal_2.0] confirms once more that BOA finds solutions with optimal or almost optimal values in exactly one iteration making a single LP call. As a result, the previous analysis stating that ILP performs very fast for the instances whose LP formulations also return integer assignments still holds.

BOA always achieves optimal or very close to optimal solutions as shown in Figure [\ref=fig:expGoal_2.0]. For example, when [formula] for a 128-node tree, and the weights are randomly distributed among all nodes, the ILP produces the optimal goal value as 4047.9167 and BOA heuristic generates 4036.5. This is one of the cases with the largest difference between the optimal solution and our heuristic solution. Even in this case, the difference between the two solutions is much less than 1%. For some cases where we have used LP relaxation solutions as upper bounds instead of ILP, the differences are slightly higher. For example, when [formula] for a 128-node tree, and the weights are decreasing from the root to the leaves, the upper bound to the optimal is 20462.263, and BOA achieves 20355.1. Even for this upper bound the difference is very small. Potentially, BOA might even have the same solution as the actual optimal, or else would have definitely achieved a closer value to the actual optimal.

Figure [\ref=fig:expTime_1.5] and Figure [\ref=fig:expGoal_1.5] display the execution times, and the solutions respectively when the parameter representing the average degree of a tree node is set to 1.5 which corresponds to deep trees. In 4 out of the 36 test cases presented in Figure [\ref=fig:expTime_1.5], BOA executes longer in figuring out a solution. The first two of these correspond to the cases where the parameter #Nodes is set to either 64 or 128 when [formula] and Weight Distribution is random. The cells corresponding to these two test cases in Figure [\ref=fig:expNoOfLPCalls] have both the value 1.1. Furthermore, it is seen from the corresponding cells in Figure [\ref=fig:expGoal_1.5] that BOA finds solutions very close to optimal. The last two test cases correspond, however, to the combinations of parameters when #Nodes is set to either 16 or 64 when [formula] and Weight Distribution is such that it is increasing from the root to the leaves. A quick inspection of the corresponding cells for the last two test cases in the corresponding tables reveals that BOA found the optimal solutions after a single LP invocation. So the prior justification is still valid.

The results of the experiments show that for all cases BOA generates goal values very close to the optimal obtained by ILP. The results are either exactly the same, or there is a very small difference. Besides, in the latter case, the distance to the optimal is always much less than 1%. Moreover with Weight Distribution increasing from the root to the leaves, BOA always finds optimal solutions.

When the parameter Weight Distribution is such that it decreasing from the root to the leaves, it takes forever to compute the optimal by ILP as shown by the corresponding cells marked ∞   throughout the tables. Under the same setting, BOA, on the other hand, returns in polynomial time almost optimal solutions that are within 1% of even the upper bounds obtained via the corresponding LP relaxation solution.

In only 11 out of a total of 108 different test cases, ILP runs faster than ILP. All 11 of these execution time anomalies are seen occur when BOA discovers an almost optimal solution after at most 1 or 1.1 LP calls on the average. These test cases are therefore thought to correspond most probably to the instances that can be solved efficiently by ILP. In such a case ILP can essentially find a solution by making only a very few LP relaxation calls via a branch and bound algorithm. It is then easily anticipated that the additional overhead posed by BOA leaves it behind ILP.

Conclusion

In this paper we have introduced a new version of the assignment problem, called as MWTM problem. In MWTM, as is the case with the standard assignment problem, a one-to-one assignment is sought between a set of tasks and a set of agents (nodes) to maximize the total profit (weight) value. Moreover, there is an additional constraint in MWTM preventing some combinations of the assignments. Since agents are organized in a tree structure representing hierarchical (agent - sub-agent) relationships, when an agent is assigned to a task, none of its sub-agents or super-agents can be assigned to any other task. This problem is shown to be NP-hard. Therefore, we proposed an iterative LP-relaxation solution to it. Through experiments we have shown that our heuristic solution is very effective, and produces either the optimal solution, or a solution very close to the optimal in a very reasonable time performing only a few iterations. In most cases the solution is achieved within a single iteration.