A constraint on extensible quadrature rules

Introduction

For both Monte Carlo (MC) and quasi-Monte Carlo (QMC) sampling, [\cite=sobo:1998] recommends that the number n of sample points should be increased geometrically, not arithmetically. Specifically, he recommends using a sequence like n1, 2n1, 4n1, etc., instead of n1, 2n1, 3n1 and so on. For either MC or QMC, the estimate of an integral is a simple unweighted average of the integrand at n points.

In the case of Monte Carlo sampling with its slow n- 1 / 2 convergence rate, if n is too small to get a good answer then taking n + k sample points for k  ≪  n is unlikely to bring a meaningful improvement in accuracy. [\cite=sobo:1993:b] studies the correlations among Monte Carlo estimates along sample sizes nk  =  2k - 1n1.

In quasi-Monte Carlo sampling, much better convergence rates are sometimes possible, depending particularly on the smoothness and dimension of the problem space. [\cite=nova:wozn:2010] provide a comprehensive treatise on error rates for numerical integration. In favorable cases, a small change in n might make a meaningful reduction in the error bound. What we show here is that rate-optimal sample sizes are widely spaced in those favorable cases.

[\cite=sobo:1998] showed that a better rate than 1 / n cannot hold uniformly for all n. [\cite=hick:krit:kuo:nuye:2012] extended this finding to arithmetic sequences of sample sizes. They also showed that unequally weighted averages of function evaluations can attain a better than 1 / n rate at all values of n. Suppose for instance that the first n sample points are partitioned into blocks of nj points, for [formula] where the estimate from block j has error O(n-  αj). Then weighting those within block estimates proportionally to naj, with a  ≥  α, attains an error O((J / n)α). One can commonly arrange J = O( log n) and then the error is o(n-  α  +  ε) for any ε > 0.

It remains interesting to consider equal weight rules. For a complicated problem with weighted points from multiple spaces, keeping track of the weights becomes cumbersome. Also, there are quasi-Monte Carlo methods such as higher order digital nets [\citep=dick:2011], that are simultaneously rate optimal for more than one class of functions, each with its own rate. In such settings we might want to use the same weights for multiple integrands, but no single weighing might serve them all best. Finally, if the constraints we find here on equal weight rules are unpalatable for some given problem, it provides motivation to switch to an unequally weighted rule.

An outline of this note is as follows. Section [\ref=sec:sobols] presents the insight from the appendix of [\cite=sobo:1998] and the extension by [\cite=hick:krit:kuo:nuye:2012]. If a QMC rule has worst case error o(1 / n) holding for all n, then the points [formula] must have some very strange limit properties and the class of functions involved is odd enough that we could do very well using only one point [formula] for very large n. A generalization of that argument shows that an o(1 / n) rate along an arithmetic sequence of sample sizes raises similar problems. Section [\ref=sec:geoworst] shows that if quadrature error in a class F of functions has a worst case lower bound mn-  α for α > 1 and a specific sequence [formula] is rate optimal at sample sizes [formula], then necessarily nk + 1 / nk  ≥  ρ for some constant 1 < ρ < 2 depending on α and on how close the sequence comes to having the optimal constant. Section [\ref=sec:georms] considers root mean squared error for sequences of sample points incorporating some randomness. The same constraints hold in this setting as in the worst case setting.

Ruling out arithmetic sequences

It is not reasonable to expect a QMC rule to have errors of size o(1 / n) for all values of n  ≥  N0 for some N0 > 0. Intuitively, adding a single point makes a change of order 1 / n to the estimated integral. Therefore two consecutive values of the integral estimate are ordinarily an order of magnitude farther apart from each other than they could be by the triangle inequality, with the true integral value making the third corner of the triangle. This idea is made precise in the appendix of [\cite=sobo:1998], as we outline here.

Let [formula] and [formula]. The integral is over d and [formula] for i  ≥  1. Let

[formula]

and suppose that the points [formula] are chosen such

[formula]

where ε(n)  =  o(1 / n), and F is a class of integrands. Sobol' considered ε(n)  =  O(n-  α) for some α > 1. The classes F that we study are usually balls with respect to a seminorm, such as the standard deviation in Monte Carlo and the total variation in the sense of Hardy and Krause, for quasi-Monte Carlo.

Sobol' observed that

[formula]

as n  →    ∞  . As a result [formula] for all f∈F. The set F cannot be very rich in this case. As Sobol' noted, for d = 1, if F contains x it cannot also contain x2.

If we had such a sequence [formula] and a class F we might simply estimate μ by [formula] for one extremely large n, perhaps the largest one for which we can compute [formula]. Alternatively, when f∈F are all known to be integrable and anti-symmetric functions on d we can take [formula] and have a zero error. This is the most favorable case for antithetic sampling [\citep=hamm:mort:1956]. MC and QMC methods are ordinarily designed for more general purpose use, and so such special settings are of limited importance.

[\cite=hick:krit:kuo:nuye:2012] extend Sobol's argument to show that we should not expect ε(nk)  =  o(1 / n) as n  →    ∞   for any integer [formula]. We would then have a class of functions F with

[formula]

for all f∈F. That is a very limited class, and once again, we could solve the problem uniformly over that class simply by taking k points [formula] for some very large n.

Geometric spacing for the worst case setting

The class F of real-valued functions on d has a superlinear worst case lower bound if

[formula]

holds for some α > 1, m > 0, all [formula] and all [formula]. There is a uniformly rate optimal sequence for this class, if for some [formula] and a sequence of sample sizes [formula],

[formula]

holds, where m  ≤  M <   ∞  .

The proof of Theorem [\ref=thm:worstcaseextension] below makes use of some basic facts about fixed-point iterations. Let g(x) be a continuous function on the interval

[formula]

. Then g has at least one fixed point x*∈[a,b], with g(x*) = x*. If also, g has Lipschitz constant λ < 1 for all x∈[a,b], then the fixed point x* is unique. Now consider the fixed point iteration xn + 1 = g(xn). Under the Lipschitz condition, xn converges to x* from any x1∈[a,b]. These facts are consequences of [\citet=kelley1999iterative]. When g has derivative g' with 0 < g'(x) < 1 on

[formula]

g'(ρ) = (M)( 1+ρ) ρ

[formula]

into

[formula]

The root mean square error setting

The class F of real-valued functions on d has a superlinear root mean square lower bound if

[formula]

holds for some α > 1, m > 0, all [formula] and any random [formula]. Here [formula] denotes expectation with respect to the randomness in [formula]. The sequence of random [formula] is uniformly rate-optimal for this class if there is a sequence of sample sizes [formula], for which

[formula]

holds, where m  ≤  M <   ∞  .

The same theorem holds for the root mean square error case as holds for the worst case. It is not necessary to assume that any of [formula] are unbiased or to make any assumption about the correlation structure among the [formula]. We only need to square one of the identities in Theorem [\ref=thm:worstcaseextension] and then use standard moment inequalities from probability theory.

Let F have a root mean square lower bound given by [\eqref=eq:lowerboundrms] with α > 1 and 0 < m  ≤  M <   ∞  . Suppose that there also exists a uniformly rate optimal sequence of random [formula] satisfying [\eqref=eq:upperboundrms]. If ρ  =  ρk = nk + 1 / nk, then

[formula]

To shorten some expressions, let Δk  =  nk + 1 - nk and recall that [formula]. We begin with the identity,

[formula]

The expected square of the left side of [\eqref=eq:basicident] is no smaller than m2  /  Δ2αk. The expected square of the right side of [\eqref=eq:basicident] is

[formula]

As a result,

[formula]

Taking the square root of both sides of [\eqref=eq:squaredident] and rearranging, yields [\eqref=eq:keyineq], from which the theorem follows just as it did for the worst case analysis.

Discussion

We have found a constraint on the spacings of a rate-optimal equal-weight extensible MC or QMC sequence. The constraint only applies when the convergence is better than O(1 / n).

We can use that constraint in reverse as follows. Suppose that a rate-optimal sequence nk includes sample sizes with nk + 1 / nk  =  ρ∈(1,2) and attains the error rate O(n-  α) for α > 1. Then from [\eqref=eq:rhobound] we obtain

[formula]

As we approach an arithmetic progression by letting [formula], the inefficiency in the constant factor increases without bound.

The constraint only applies to rate-optimal sequences. In particular it does not apply to an extensible sequence that may be inefficient by a logarithmic factor.

Acknowledgments

I thank Alex Kreinin and Sergei Kucherenko for pointing out to me the elegant argument in the Appendix of [\cite=sobo:1998], and Erich Novak for sharing his slides from MCQMC 2014. This work was supported by grants DMS-0906056 and DMS-1407397 of the U.S. National Science Foundation.