A test for the equality of covariance operators

Introduction

In many applications, we study phenomena that are continuous in time or space and can be considered as smooth curves or functions. On the other hand, when working with more than one population, as in the finite dimensional case, the equality of the covariance operators associated with each population is often assumed. In the case of finite-dimensional data, tests for equality of covariance matrices have been extensively studied, see for example Seber (1984) and Gupta and Xu (2006). This problem has been considered even for high dimensional data, i.e., when the sample size is smaller than the number of variables under study; we refer among others to Ledoit and Wolf (2002) and Schott (2007).

For functional data, most of the literature on hypothesis testing deals with tests on the mean function including the functional linear model, see, for instance, Fan and Lin (1998), Cardot et al. (2003), Cuevas et al. (2004) and Shen and Faraway (2004). Tests on the covariance operators related to serial correlation were considered by Gabrys and Kokoszka (2007), Gabrys et al. (2010) and Horváth et al. (2010). On the other hand, Benko et al. (2009) proposed two-sample bootstrap tests for specific aspects of the spectrum of functional data, such as the equality of a subset of eigenfunctions while Ferraty et al. (2007) considered tests for comparison of groups of curves based on comparison of their covariances. The hypothesis tested by the later are that of equality, proportionality and others based on the spectral decomposition of the covariances. Their approach is high dimensional since they either approximate the curves over a grid of points, or use a projection approach. More recently, Panaretos et al. (2010) considered the problem of testing whether two samples of continuous zero mean i.i.d. Gaussian processes share or not the same covariance structure.

In this paper, we go one step further and consider the functional setting. Our goal is to provide a test statistic to test the hypothesis that the covariance operators of several independent samples are equal in a fully functional setting. To fix ideas, we will first describe the two sample situation. Let us assume that we have two independent populations with covariance operators 1 and 2. Denote by 1 and 2 consistent estimators of 1 and 2, respectively, such as the sample covariance estimators studied in Dauxois et al. (1982). It is clear that under the standard null hypothesis 1  =  2, the difference between the covariance operator estimators should be small. For that reason, a test statistic based on the norm of 1  -  2 may be helpful to study the hypothesis of equality.

The paper is organized as follows. Section [\ref=prel] introduce the notation and review some basic concepts which are used in later sections. Section [\ref=stest] introduces the test statistics for the two sample problem. Its asymptotic distribution under the null hypothesis is established in Section [\ref=test] while a bootstrap test is described in Section [\ref=boo]. An important issue is to describe the set of alternatives that the proposed statistic is able to detect. For that purpose, the asymptotic distribution under a set of contiguous alternatives based on the functional common principal component model is studied in Section [\ref=alt]. Finally, an extension to several populations is provided in Section [\ref=kpobla]. Proofs are relegated to the Appendix.

Preliminaries and notation

Let us consider independent random elements [formula] in a separable Hilbert space H (often L2(I)) with inner product 〈  ·  ,  ·  〉 and norm [formula] and assume that [formula]. Denote by μi∈H the mean of Xi, [formula] and by i:H  →  H the covariance operator of Xi. Let [formula] stand for the tensor product on H, e.g., for u,v∈H, the operator [formula] is defined as [formula]. With this notation, the covariance operator i can be written as [formula]. The operator i is linear, self-adjoint and continuous.

In particular, if H = L2(I) and [formula], the covariance operator is defined through the covariance function of Xi, γi(s,t)  =  (Xi(s),Xi(t)), s,t∈I as [formula]. It is usually assumed that [formula] hence, i is a Hilbert-Schmidt operator. Hilbert-Schmidt operators have a countable number of eigenvalues, all of them being real.

Let F denote the Hilbert space of Hilbert-Schmidt operators with inner product defined by [formula] and norm [formula], where [formula] is any orthonormal basis of H, while [formula], [formula] and [formula] are Hilbert-Schmidt operators, i.e., such that [formula]. Choosing an orthonormal basis [formula] of eigenfunctions of i related to the eigenvalues [formula] such that [formula], we get [formula]. In particular, if H = L2(I), we have [formula].

Our goal is to test whether the covariance operators i of several populations are equal or not. For that purpose, let us consider independent samples of each population, that is, let us assume that we have independent observations [formula], 1  ≤  i  ≤  k, with Xi,j  ~  Xi. A natural way to estimate the covariance operators i, for 1  ≤  i  ≤  k, is through their empirical versions. The sample covariance operator i is defined as

[formula]

where [formula]. Dauxois et al. (1982) obtained the asymptotic behaviour of i. In particular, they have shown that, when [formula], [formula] converges in distribution to a zero mean Gaussian random element of F, [formula], with covariance operator i given by

[formula]

where [formula] stands for the tensor product in F and, as mentioned above, [formula] is an orthonormal basis of eigenfunctions of i with associated eigenvalues [formula] such that [formula]. The coefficients sim are such that s2im  =  λi,m, while fim are the standardized coordinates of Xi  -  μi on the basis [formula], that is, [formula]. Note that [formula]. Using that [formula], we get that [formula], [formula] for m  ≠  s. In particular, the Karhunen-Loéve expansion leads to

[formula]

It is worth noticing that [formula] so, the sum of the eigenvalues of i is finite, implying that i is a linear operator over F which is Hilbert Schmidt. Thus, any linear combination of the operators i, [formula], with ai  ≥  0, will be a Hilbert Schmidt operator. Therefore, if [formula] stand for the eigenvalues of [formula] ordered in decreasing order, [formula] and [formula]. This property will be used later in Theorem [\ref=stest].1.

When H = L2(I), smooth estimators, i,h, of the covariance operators were studied in Boente and Fraiman (2000). The smoothed operator is the operator induced by the smooth covariance function

[formula]

where [formula] are the smoothed trajectories, Kh(  ·  ) = h- 1K(  ·   / h) is a nonnegative kernel function, and h a smoothing parameter. Boente and Fraiman (2000) have shown that, under mild conditions, the smooth estimators have the same asymptotic distribution that the empirical version.

Test statistics for two-sample problem

We first consider the problem of testing the hypothesis

[formula]

A natural approach is to consider i as the empirical covariance operators of each population and construct a statistic Tn based on the difference between the covariance operators estimators, i.e., to define [formula], where n = n1 + n2.

The null asymptotic distribution of the test statistic

The following result allows to study the asymptotic behaviour of [formula] when 1  =  2 and thus, to construct a test for the hypothesis ([\ref=test]) of equality of covariance operators.

Theorem [\ref=stest].1. Let [formula], for i = 1,2, be independent observations from two independent samples in H with mean μi and covariance operator i. Let n = n1 + n2 and assume also that ni / n  →  τi with τi∈(0,1). Let [formula], i = 1,2, be independent estimators of the i - th population covariance operator such that [formula], with [formula] a zero mean Gaussian random element with covariance operator i. Denote by [formula] the eigenvalues of the operator   =  τ1- 11  +  τ2- 12 with [formula]. Then,

[formula]

where [formula] are i.i.d. standard normal random variables. In particular, if 1  =  2 we have that [formula].

Remark [\ref=stest].1.

The results in Theorem [\ref=stest].1 apply in particular, when considering the sample covariance operator, i.e., when [formula]. Effectively, when [formula], [formula] converges in distribution to a zero mean Gaussian random element [formula] of F with covariance operator i given by ([\ref=varasintgamai]). As mentioned in the Introduction, the fact that [formula] entails that [formula].

It is worth noting that if qn is a sequence of integers such that qn  →    ∞  , the fact that [formula] implies that the sequence [formula] is Cauchy in L2 and therefore, the limit [formula] is well defined. In fact, analogous arguments to those considered in Neuhaus (1980) allow to show that the series converges almost surely. Moreover, since Z21  ~  χ21, U has a continuous distribution function FU and so FUn, the distribution functions of Un, converge to the FU uniformly, as shown in Lemma 2.11 in Van der Vaart (2000).

Remark [\ref=stest].2. Theorem [\ref=stest].1 implies that, under the null hypothesis H0:1  =  2, we have that [formula], hence an asymptotic test based on Tn rejecting for large values of Tn allows to test H0. To obtain the critical value, the distribution of U and thus, the eigenvalues of τ- 111  +  τ- 122 need to be estimated. As mentioned in Remark [\ref=stest].1 the distribution function of U can be uniformly approximated by that of Un and so, the critical values can be approximated by the (1 - α) - percentile of Un. Gupta and Xu (2006) provide an approximation for the distribution function of any finite mixture of χ21 independent random variables that can be used in the computation of the (1 - α) - percentile of [formula] where [formula] are estimators of [formula]. It is also, worth noticing that under H0:1  =  2, we have that for i = 1,2, i given in ([\ref=varasintgamai]) reduces to

[formula]

where for the sake of simplicity we have eliminated the subscript 1 and simply denote as sm  =  λ1 / 2m with λm the m - th largest eigenvalue of 1 and φm its corresponding eigenfunction. In particular, if all the populations have the same underlying distribution except for the mean and covariance operator, as it happens when comparing the covariance operators of Gaussian processes, the random function f2m has the same distribution as f1m and so, 1  =  2.

The previous comments motivate the use of the bootstrap methods, due the fact that the asymptotic distribution obtained in ([\ref=dist]) depends on the unknown eigenvalues [formula]. It is clear that when the underlying distribution of the process Xi is assumed to be known, for instance, if both samples correspond to Gaussian processes differing only on their mean and covariance operators, a parametric bootstrap can be implemented. Effectively, denote by [formula] the distribution of Xi where the parameters μi and i are explicit for later convenience. For each 1  ≤  i  ≤  k, generate bootstrap samples [formula], 1  ≤  j  ≤  ni, with distribution Gi,0,i. Note that the samples can be generated with mean 0 since our focus is on covariance operators. Besides, the sample covariance operator i is a finite range operator, hence the Karhunen-Loéve expansion ([\ref=kl]) allows to generate [formula] knowing the distribution of the random variables [formula], the eigenfunctions [formula] of i and its related eigenvalues [formula], [formula], that is, the estimators of the first principal components of the process. Define [formula] as the sample covariance operator of [formula], 1  ≤  j  ≤  ni and further, let [formula]. By replicating [formula] times, we obtain [formula] values [formula] that allow easily to construct a bootstrap test.

The drawback of the above described procedure, it that it assumes that the underlying distribution is known hence, it cannot be applied in many situations. For that reason, we will consider a bootstrap calibration for the distribution of the test that can be described as follows,

Step 1 Given a sample [formula], let i be consistent estimators of i for i = 1,2. Define   =     - 111  +     - 122 with i = ni / (n1  +  n2).

Step 2 For [formula] denote by [formula] the positive eigenvalues of [formula].

Step 3 Generate [formula] i.i.d. such that Z*i  ~  N(0,1) and let [formula].

Step 4 Repeat Step 3 [formula] times, to get [formula] values of U*nr for [formula].

The (1 - α) - quantile of the asymptotic distribution of Tn can be approximated by the (1 - α) - quantile of the empirical distribution of U*nr for [formula]. The p - value can be estimated by [formula] where s is the number of U*nr which are larger or equal than the observed value of Tn.

Remark [\ref=stest].3. Note that this procedure depends only on the asymptotic distribution of i. For the sample covariance estimator, the covariance operator i is given by ([\ref=varasintgamai]). Hence, for Gaussian samples, using that fij are independent and fij  ~  N(0,1), i can be estimated using as consistent estimators of the eigenvalues and eigenfunctions of i, the eigenvalues and eigenfunctions of the sample covariance. For non Gaussian samples, i can be estimated noticing that

[formula]

When considering other asymptotically normally estimators of i, such as the smoothed estimators [formula] for L2(I) trajectories, the estimators need to be adapted.

Validity of bootstrap procedure

The following theorem entails the validity of the bootstrap calibration method. It states that, under H0, the bootstrap distribution of U*n converges to the asymptotic null distribution of Tn. This fact ensures that the asymptotic significance level of the test based on the bootstrap critical value is indeed α.

Theorem [\ref=stest].2. Let qn such that [formula] and [formula]. Denote by [formula]. Then, under the assumptions of Theorem [\ref=stest].1, if [formula], we have that

[formula]

where FU denotes the distribution function of [formula], with [formula] independent of each other, and [formula] stands for the Kolmogorov distance between distribution functions F and G.

Behaviour under contiguous alternatives

In this section, we study the behaviour of the test statistic Tn under a set of contiguous alternatives. The contiguous alternatives to be considered consist in assuming that discrepancies from the null hypothesis arise only in the eigenvalues and not in the eigenfunctions of the covariance operators i, i.e., we assume that we are approximating the null hypothesis with alternatives satisfying a functional common principal model. In this sense, under those local alternatives, the processes Xi, i = 1,2, can be written as

[formula]

with [formula], [formula] at a given rate, while [formula] are random variables such that [formula], [formula], [formula] for [formula]. For simplicity, we have omitted the subscript 1 in [formula]. Hence, we are considering as alternatives a functional common principal component model which includes as a particular case, proportional alternatives of the form 2,n  =  ρn1, with ρn  →  1. For details on the functional principal component model, see for instance, Benko et al. (2009) and Boente et al. (2010).

Theorem [\ref=stest].3. Let [formula] for i = 1,2 be independent observations from two independent distributions in H, with mean μi and covariance operator i such that 2  =  2,n  =  1 + n- 1 / 2, with [formula]. Furthermore, assume that Xi,j  ~  Xi where Xi satisfy ([\ref=fcpc]) with [formula] and that [formula] for i = 1,2. Let n = n1 + n2 and assume also that ni / n  →  τi with τi∈(0,1). Let i be the sample covariance operator of the i - th population and denote by

[formula]

where sm  =  λ1 / 2m. Then, if [formula], [formula], [formula], [formula] and [formula], with [formula], we get that

[formula] with [formula] a zero mean Gaussian random element with covariance operator 2.

Denote by [formula] the eigenvalues of the operator   =  τ- 111  +  τ- 122. Moreover, let [formula] be an orthonormal basis of F such that [formula] is the eigenfunction of [formula] related to [formula] and consider the expansion [formula], with [formula]. Then,

[formula]

where [formula] are independent and [formula] .

Test statistics for k - populations

In this Section, we consider tests for the equality of the covariance operators of k populations. That is, if i denotes the covariance operator of the i - th population, we wish to test the null hypothesis

[formula]

Let [formula] and assume that ni / n  →  τi, 0 < τi < 1, [formula]. A natural generalization of the proposal given in Section [\ref=stest] is to consider the following test statistic

[formula]

where i stands for the sample covariance operator of i - th population. The following result states the asymptotic distribution of Tk,n, under the null hypothesis.

Theorem [\ref=kpobla].1. Let [formula], for 1  ≤  i  ≤  k, be independent observations from k independent distributions in H, with mean μi and covariance operator i such that [formula]. Let i be the sample covariance operator of the i - th population. Assume that ni / n  →  τi with τi∈(0,1) where [formula]. Denote [formula] the linear operator [formula] defined as

[formula]

where i are given in ([\ref=varasintgamai]). Let [formula] stand for the sequence of eigenvalues of [formula] ordered in decreasing order. Under [formula], we have

[formula]

where [formula] are independent.

As mentioned in the Introduction, the fact that [formula] entails that [formula].

Remark [\ref=kpobla].1. Note that Theorem [\ref=kpobla].1 is a natural extension of its analogous in the finite-dimensional case. To be more precisely, let [formula] with 1  ≤  i  ≤  k and 1  ≤  j  ≤  ni be independent random vectors and let i be their sample covariance matrix. Then, [formula] converges to a multivariate normal distribution with mean zero and covariance matrix Υi. Let

[formula]

where p stands for the identity matrix of order p. Then, straightforward calculations allow to show that [formula] where

[formula]

Therefore, under the null hypothesis of equality of the covariance matrices i, we have that [formula] where [formula] and [formula] are the eigenvalues of Υ. Note that the matrix Υ is the finite dimensional version of the covariance operator [formula].

Remark [\ref=kpobla].2. The conclusion of Theorem [\ref=kpobla].1 still holds if, instead of the sample covariance operator, one considers consistent and asymptotically normally distributed estimators [formula] of the covariance operator i such that [formula], where [formula] is zero mean Gaussian random element of F with Hilbert Schmidt covariance operator [formula]. For instance, the scatter estimators proposed by Locantore et al. (1999) and further developed by Gervini (2008) may be considered, if one suspects that outliers may be present in the sample. These estimators weight each observation according to their distance to the center of the sample. To be more precise, let us define the spatial median of the i - th population as the value ηi such that

[formula]

and the spatial covariance operator [formula] as

[formula]

with ηi being the spatial median. It is well known that, when second moment exist, [formula] is not equal to the covariance operator of the i - th sample, even if they share the same eigenfunctions when Xi has a finite Karhunen Loève expansion and the components [formula] in ([\ref=kl]) have a symmetric distribution, see Gervini (2008). Effectively, under symmetry of [formula], ηi  =  μi and we have that [formula] with

[formula]

The point to be noted here is that even if [formula] is not proportional to i, under the null hypothesis [formula], we also have that [formula] is true when the components [formula] are such that [formula] for 2  ≤  i  ≤  k, [formula] which means that all the populations have the same underlying distribution, except for the location parameter and the covariance operator. Thus, one can test [formula] through an statistic analogous to Tk,n defined in ([\ref=tkn]) but based on estimators of [formula].

Estimators of ηi and [formula] are defined through their empirical versions as follows. The estimator of the spatial median is the value i minimizing over μ the quantity [formula] while the spatial covariance operator estimator is defined as

[formula]

Gervini (2008) derived the consistency of these estimators and the asymptotic normality of i. Up to our knowledge, the asymptotic distribution of [formula] has not been given yet. However, we conjecture that, when the components [formula] in ([\ref=kl]) have a symmetric distribution, its asymptotic behaviour will be the same as that of

[formula]

since i is a root- n consistent estimator of ηi  =  μi. The asymptotic distribution of [formula] is beyond the scope of this paper while that of [formula] can be derived from the results in Dauxois et al. (1982) allowing us to apply the results in Theorem [\ref=kpobla].1 at least when the center of all the populations is assumed to be known.

Remark [\ref=kpobla].3. As in Section [\ref=stest], a bootstrap procedure can be considered. In order to estimate [formula], we can consider estimators of the operators i for 1  ≤  i  ≤  k and thus estimate [formula]. Therefore, if [formula] are the positive eigenvalues of [formula], a bootstrap procedure can defined using Steps 3 and 4 in Section [\ref=stest].

Acknowledgments

Appendix

{

Proof of Theorem [\ref=stest].1. Since ni / n  →  τi∈(0,1), the independence between the two estimated operators allows us to conclude that,

[formula]

where [formula] is a Gaussian random element of [formula] with covariance operator given by   =  τ1- 11  +  τ2- 12. Then, we easily get

[formula]

where [formula] are the eigenvalues associated to the operator [formula].

Proof of Theorem [\ref=stest].2. Let [formula], [formula] and [formula] with Zi  ~  N(0,1) independent. Define [formula], [formula] and [formula].

First note that [formula] for each [formula] (see Kato, 1966), which implies that

[formula]

On the other hand, we have

[formula]

which together with ([\ref=cota]), the fact that [formula], [formula] and [formula] implies that [formula].

We also have the following inequalities

[formula]

where Δε(t) =  sup |δ|  ≤  ε|FU(t + δ) - FU(t)|. Besides,

[formula]

Therefore,

[formula]

As we mentioned in Remark [\ref=stest].1, FU is a continuous distribution function on [formula] and so, uniformly continuous, hence [formula], which implies that [formula].

Proof of Theorem [\ref=stest].3. Using the Karhunen-Loéve representation, we can write

[formula]

where [formula] in ([\ref=fcpc]).

a) For 1  ≤  j  ≤  n2, let [formula], and

[formula]

Define the following operators that will be used in the sequel [formula] , [formula], [formula] and finally, [formula]. Using that X2,j  -  μ2 = Z0,j + Vj, we obtain the following expansion [formula].

The proof will be carried out in several steps, by showing that

[formula]

where [formula] is a zero mean Gaussian random element with covariance operator 2. Using that the covariance operator of Z0,j is 1, ([\ref=convgaz0]) follows from Dauxois et al. (1982).

We will derive ([\ref=reemplazo]). Note that X2,j  -  μ2 = Z0,j + Vj and [formula]. Then, it is enough to prove that [formula].

By the central limit theorem in Hilbert spaces, we get that [formula] converges in distribution, and so it is tight, i.e., [formula].

On the other hand, to derive that [formula], we will further show that [formula]. To do so, note that [formula]. Using the inequality [formula], for any a  ≥  0, we get that [formula], concluding the proof of ([\ref=reemplazo]).

To obtain ([\ref=convgav]), note that

[formula]

and

[formula]

where [formula]. Note that [formula] and so

[formula]

where the last bound follows from the Cauchy-Schwartz inequality and the fact that [formula] and [formula]. Thus, using the inequality [formula], we get that [formula] which together with ([\ref=cotaespU]) implies that

[formula]

and so, [formula] concluding the proof of ([\ref=convgav]).

Finally, to derive ([\ref=convergeA]) note that analogous arguments allow to show that

[formula]

while

[formula]

concluding the proof of ([\ref=convergeA]). The proof of a) follows easily combining ([\ref=reemplazo]) to ([\ref=convgaz0]).

b) From a), we have that [formula] where [formula] a zero mean Gaussian random element with covariance operator 2. On the other hand, the results in Dauxois et al. (1982) entail that [formula], where [formula] a zero mean Gaussian random element with covariance operator 1 and so, [formula]. The fact that the two populations are independent implies that [formula] and [formula] can be chosen to be independent so,

[formula]

where [formula] is a Gaussian random element of [formula] with covariance operator   =  τ1- 11  +  τ2- 12. Therefore, [formula].

To conclude the proof, we have to obtain the distribution of [formula]. Since [formula] is a zero mean Gaussian random element of [formula] with covariance operator [formula], we have that [formula] can be written as [formula] where [formula] are i.i.d. random variables such that [formula]. Hence, [formula] and so, [formula] concluding the proof.

Proof of Theorem [\ref=kpobla].1. Consider the process [formula]. The independence of the samples and among populations together with the results stated in Dauxois et al. (1982), allow to show that Vk,n converges in distribution to a zero mean Gaussian random element [formula] of Fk with covariance operator [formula]. More precisely, we get that

[formula]

where [formula] are independent random processes of F with covariance operators τi- 1i, respectively.

Let A:Fk  →  Fk - 1 be a linear operator given by [formula]. The continuous map Theorem guarantees that [formula], where W is a zero mean Gaussian random element of Fk - 1 with covariance operator [formula] where A* denote the adjoint operator of A. It is easy to see that the adjoint operator A*:Fk - 1  →  Fk is given by [formula]. Hence, as [formula] are independent, we conclude that

[formula]

Finally,

[formula]

where [formula] are i.i.d standard normal random variables and [formula] are the eigenvalues of the operator [formula].