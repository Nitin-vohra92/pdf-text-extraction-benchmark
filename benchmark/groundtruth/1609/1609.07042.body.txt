Temporal Selective Max Pooling Towards Practical Face Recognition

Department of Computer Science, Johns Hopkins University xxiang@cs.jhu.edu

Introduction

If we treat a frame of a video sequence as a random variable, we may assume the frames are identically distributed but not independent variables. When the task is to model the whole sequence such as using recurrent or recursive models, we may use the sample mean and variance to approximate the true data distribution which is assumed to be Gaussian. Despite that, temporal mean pooling is still conventional to represent a sequence such as mean encoding for face recognition [\cite=template16], feature averaging for action recognition [\cite=LRCN15] and mean pooling for video captioning [\cite=LSTM14NAACL].

In this report, we are interested in recognizing one source of variation among sequences. Temporal mean pooling is again straightforward while the sample mean is not a robust statistic. Convolutional Neural Networks (CNN) features describing identities are normally not robust to the pose variation. Pose variation is a primary challenge in uncontrolled environments. We argue that the frame-wise feature mean is unable to characterize the variation existing among frames. If we want the video feature to represent the subject identity, we had better preserve the overall pose diversity. Then, disregarding factors other than identity and pose, identity will be the only source of variation across videos since pose varies even within a single video. Following such an untangling variation idea, we propose a pose-robust face verification algorithm with each video represented as a bag of CNN features corresponding to selected key frames.

The impact of different spatial pooling methods in CNN has been discussed in [\cite=boureau10icml] [\cite=boureau10cvpr]. Instead of pooling from all the frames, the algorithm is highlighted at the key frame selection by pose quantization using pose distances to K-means centroids. It reduces the number of features from tens or hundreds to K while still preserving the overall diversity, which makes it possible to real-time process a video stream. Fig. [\ref=fig:pose-eg] shows an example sequence in the YouTube Face (YTF) dataset [\cite=ytf11]. This algorithm also serves as video sampling (to K images).

Now, if we select key frames, we still have many pairs of image between two videos. The metric to pool from many correlations normally are the mean or the max. Taking the max is essentially finding the nearest neighbor, which is a typical metric for measuring similarity or closeness of two point sets. There are existing works on measuring the similarity of two videos [\cite=shan15]. However, an extension of image-based correlation is preferred for its simplicity, such as temporal max or mean pooling [\cite=lionel15temporalpool]. In our work, the max correlation between two bags of frame-wise CNN features is employed to measure how likely two videos represent the same person.

In the end, a video is represented by a single frame's feature which induces nearest neighbors between two sets if we treat each frame as a data point. This is essentially a pairwise max pooling process. On the official 5000 video-pairs of YTF dataset [\cite=ytf11], our algorithm achieves a comparable performance with state-of-the-art that averages over deep features of all frames.

Related Works

In this deep learning era, face recognition on a number of benchmarks such as the Labeled Face in the Wild (LFW) dataset [\cite=lfw16] has been well solved by DeepFace [\cite=deepface14], DeepID [\cite=deepid14], FaceNet [\cite=facenet15] and so on. The Visual Geometry Group at the University of Oxford released their deep face model called VGG-Face Descriptor [\cite=vggface15] which also gives a comparable performance on LFW. In the following, we review possible choices in the specification of the problem of our interest.

First, there are two widely used tasks of face recognition - identification and verification. In identification, we gather information about a specific set of individuals to be recognized (i.e., the gallery). At test time, a new image or group of images is presented (i.e., the probe). Identification involves computing a one-to-many similarity, namely a ranked list of one-to-one similarity (i.e., verification). The cosine similarity or correlation both are well-defined similarity metrics. However, the correlation between two random image samples might characterize cues other than identity.

Second, there are many different application scenarios for face verifications. For Web-based applications, verification is conducted by comparing images to images. The images may be of the same person but were taken at different time or under different conditions. For online face verification, alive video rather than still images is used. More specifically, the existing video-based verification solutions assume that gallery face images are taken under controlled conditions [\cite=shan15]. However, gallery is often built uncontrolled. In practice, a camera could take a picture as well as capture a video. When there are more information describing identities in a video than an image, using a fully live video stream will require expensive computational resources. Normally we need video sampling or a temporal sliding window.

Third, there are two widely used data collection settings for face recognition: uncontrolled (the so-called in the wild) and controlled (the so-called lab setting). Considering the number of image parameters that were allowed to vary simultaneously, it is logical to consider a divide-and-conquer approach - studying each source of variation separately and keeping all other variations as constants in a control experiment. Such a separation of variables has been widely used in Physics and Biology for multivariate problems. In this data-driven machine learning era, it seems fine to remain all variations in realistic data, given the idea of letting the deep neural networks learn the variations existing in the enormous amount of data. For example, FaceNet [\cite=facenet15] trained using a private dataset of over 200M subjects is indeed robust to poses, while CNN features from conventional networks suach as DeepFace [\cite=deepface14] and VGG-Face [\cite=vggface15] are normally not. Moreover, the unconstrained data with fused variations may contain biases towards factors other than identity, since the feature might characterize a mixed information of identity, pose, illumination, expression, motion and background. For instance, pose similarities normally outweigh subject identity similarities, leading to matching based on pose rather than identity. As a result, it is critical to decouple pose and identity.

Deep Face with Selective Max Pooling

Pose normalization

We normalize the selected face templates to be near-frontal. There are two steps. First, we estimate the homography of the faces by face tracking, and align the faces to a rectangular reference. Second, we detect eyes and geometrically warp the face region so that eyes are fixed to a preset position. Very often only one eye can be detected due to head pose. To tackle the issue, we mirror the detected eye horizontally and do template matching around the mirrored region to find the other eye. However, profile faces will not be frontalized.

Deep face representation

The pre-trained VGG face model [\cite=vggface15] has benn pre-trained with face images of size 224 Ã—   224 with the average face image subtracted and then is used for our verification purpose without any re-training. However, such average face subtraction is unavailable and unnecessary given a new inputting image. As a result, we directly input the face image to VGG-Face network without any mean face subtraction. The VGG-Face network has 24 layers, including several stacked convolution-pooling layer, 2 fully-connected layer and one softmax layer. Since the model was trained for classification purpose only, we use the output of the second last fully-connected layer as the face feature, which is a 4096-dim vector for each input face.

Key face selection

Using a full live video stream will require many computational resources. As we observe from Fig. [\ref=fig:pose-pattern]. that patterns exist in pose distributions, we select K key frames out of a video according 3D poses. Intuitively we want to retain the key faces that have poses as different as possible. Practically the algorthms is ellaborated as followed.

Landmark based pose estimation - computing the rotation angles.

Clustering-based frame-wise pose quantization reduces the number of images required to represent the face from tens or hundreds to K (say, K = 9 for a K-means codebook), while preserving the overall diversity.

Selecting key frames using distances from each frame's estimated pose to pose centroids.

Correlation guided temporal max pooling

One usage of pooling is dimensionality reduction. In the temporal dimension, it also serves as further video sampling (from K images to 1 image). The proposed temporal pooling is a frame selection through the subsequent pari-wise correlation. Since the identity is normally consistent in a single video due to temporal coherence, we might claim two video represent the same person as long as one pair of images are highly correlated. The insight of not taking the mean is that a high correlation feature usually does not appear twice in a temporal sliding window. We term this max correlation based temporal frame selection as correlation guided max pooling.

Experiments

Implementation

Most programs in this work are developed in C++.

HOG based face detection using DLib.

The face detector is based on the cascaded boosting classifier. And once a face is detected, we will track the face.

Facial landmark detection: DLib.

Face alignment: centering eyes and mouth and applying affine warping using OpenCV.

Deep face representation : second last layer output (4096-dim) of VGG-Face [\cite=vggface15] using Caffe [\cite=jia2014caffe]. For your conveniece, you may consider using MatConvNet-VLFeat instead of Caffe.

Evaluation

The experiments are conducted on the YTF dataset [\cite=ytf11] and can be replicated by following a tutorial . YTF was built by using the 5,749 names of subjects included in the LFW dataset [\cite=lfw16] to search YouTube for videos of these same individuals. Then, a screening process reduced the original set of videos from the 18,899 of 3,345 subjects to 3,425 videos of 1,595 subjects.

In the same way with LFW, the creator of YTF provides an initial official list of 5,000 video pairs with ground truth (same person or not as shown in Fig. [\ref=fig:ytf-eg]). Fig. [\ref=fig:roc] presents the Receiver Operating Characteristic (ROC) curve obtained after we compute the 5,000 video-video similarity scores. One way to look at a ROC curve is to first fix the level of false positive rate that we can bear (say, 0.1) and then see how high is the true positive rate (say, roughly 0.9). Another way is to see how close the curve towards the top-left corner. In other words, we measure the Area Under the Curve (AUC) and hope it to be as large as possible. In this testing, the AUC is 0.9419. Note that we do run cross validations here as we do not have any training.

However later on, the creator of YTF sends a list of errors in the ground-truth label file and provides a corrected list of video pairs with updated ground-truth labels. As a result, we run again the proposed algorithm on the corrected 4,999 video pairs. Fig. [\ref=fig:roc-updated] updates the ROC curve with an AUC of 0.9418 which is identical with the result on the initial list.

Conclusion

In this work, we propose a video-based face verification algorithm achieving comparable performance with low computation. A simple metric such as correlation is fine to use as long as the face features are good. The algorithm is particularly designed to handle pose variation and video processing. However, a generic problem underneath is variable disentanglement in real-world problems.