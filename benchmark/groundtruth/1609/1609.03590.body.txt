>m

Survey and Taxonomy of Self-Aware and Self-Adaptive Autoscaling Systems in the Cloud

Introduction

From the literal meaning of the word "autoscaling", it is obvious that the process is dynamic and requires the system to adapt subject to the uncertain, changing state of the services being managed and the environment. In such a way, the cloud-based services can be expanded and shrink according to the environmental conditions at runtime. Given that it is almost impossible to access the low level details of cloud-based services (e.g., their codes and algorithms) at runtime, an autoscaling system often consist of two physical parts: a managing part containing the autoscaling logic and a manageable part including services and VMs running in the cloud. The two physical parts are seamlessly and transparently connected for realising the entire autoscaling process. This characteristic has made autoscaling systems well-suited to the broad category of self-adaptive systems [\cite=Chen:2015:computer], and the two parts structure of adaptation is known as the external adaptation [\cite=roadmap] [\cite=landscape].

While a number of autoscaling approaches have been proposed in the context of cluster, grid and web applications, cloud autoscaling is still in its infancy due to the unique characteristic of cloud, including scaling with cost in mind for better elasticity, considering dynamic changes of both software configuration and hardware resources and the effects of QoS interference etc. In particular, there has been no work that explicitly explore how self-awareness and the related algorithms can be used and what are the benefits for cloud autoscaling in a principled way.

In this paper, we present the brief background and history for autoscaling in the cloud and self-awareness, as well as self-adaptivity of a system. Subsequently, we conduct detailed survey and taxonomy of the key related work and identify the gaps in this area of research.

Background

Self-Adaptivity and Self-Awareness

The broad category of automatic and adaptive systems aim to deal with the dynamics that the system exhibited without human intervention; but this does not necessarily involve uncertainty, i.e., there are changes related to the system but it is easy to know when they would occur and the extent of these changes. Self-adaptivity, being a sub-category, is a particular capability of the system to handle both dynamics and uncertainty. Here, self-adaptive systems refer to the systems that are able to adjust their behaviours according to the perception of the uncertain environment and its own state. According to the adaptive behaviors, self-adaptivity can be regarded as the following four properties, each of which covers a specific set of goals, as explicitly discussed and categorized in many surveys, e.g., [\cite=landscape]:

Self-configuring The capability of reconfiguring automatically and dynamically in response to changes by installing, updating, integrating, and composing/decomposing software entities.

Self-healing This is the capability of discovering, diagnosing, and reacting to disruptions. It can also anticipate potential problems, and accordingly take proper actions to prevent a failure. Self-diagnosing refers to diagnosing errors, faults, and failures, while self-repairing focuses on recovery from them.

Self-optimizing This is also called self-tuning or self-adjusting, is the capability of managing performance and resource allocation in order to satisfy the requirements of different users. End-to-end response time, throughput, utilisation, and workload are examples of important concerns related to this property.

Self-protecting This is the capability of detecting security breaches and recovering from their effects. It has two aspects, namely defending the system against malicious attacks, and anticipating problems and taking actions to avoid them or to mitigate their effects.

Self-awareness, on the other hand, is concerned with the system's ability to acquire knowledge about its current state and the environment. Such knowledge permits better reasoning about the system's adaptive behaviours. Consequently, self-awareness is often seen as the lowest level of abstraction of self-adaptivity [\cite=landscape], and thus it can improve the perception and self-adaptivity of a system [\cite=2014_epics_handbook] [\cite=7185305] [\cite=epics_survey] [\cite=Chen2016:book]. Inspired from the psychology domain, Becker et al. [\cite=epics] have classified self-awareness of a computing system into the following general capabilities (they have used node to represent any conceptual part of a system being managed):

Stimulus-aware A node is stimulus-aware if it has knowledge of stimuli. The node is not able to distinguish between the sources of stimuli. It is a prerequisite for all other levels of self-awareness.

Interaction-aware A node is interaction-aware if it has knowledge that stimuli and its own actions form part of interactions with other nodes and the environment. It has knowledge via feedback loops that its actions can provoke, generate or cause specific reactions from the social or physical environment.

Time-aware A node is time-aware if it has knowledge of historical and/or likely future phenomena. Implementing time-awareness may involve the node possessing an explicit memory, capabilities of time series modelling and/or anticipation.

Goal-aware A node is goal-aware if it has knowledge of current goals, objectives, preferences and constraints. It is important to note that there is a difference between a goal existing implicitly in the design of a node, and the node having knowledge of that goal in such a way that it can reason about it. The former does not describe goal-awareness; the latter does.

Meta-self-aware A node is meta-self-aware if it has knowledge of its own capability(ies) of awareness and the degree of complexity with which the capability(ies) are exercised. Such awareness permits a node to reason about the benefits and costs of maintaining a certain capability of awareness (and degree of complexity with which it exercises this level).

The benefits that self-awareness introduces for computing systems, including better solution for runtime dynamics and uncertainty, heterogeneity, and trade-offs on objectives, have rendered it as a neat solution for the challenges of cloud autoscaling.

Autoscaling in Cloud

Depending on the given QoS attributes and the manageable cloud primitives, an autoscaling system can cover self-configuring, self-healing, self-optimising and self-protecting, or any combination of those, from the notion of self-adaptivity. A recent survey [\cite=epics_survey] has established the evidences that self-awareness can improve a system that requires self-adaptivity, and therefore achieving self-awareness is a promising way to enable better autoscaling systems in cloud.

The external adaptation of an autoscaling system is shown in Figure [\ref=ch1:simple_arch]. As we can see, the core of an autoscaling system in the cloud is the autoscaling logic, which can consist of multiple logical aspects. The simplest form of autoscaling system covers monitoring and scaling aspect in its autoscaling logic: the former gathers the service's or application's current state while the latter utilises the information to decide an action. However, such a simplified form of autoscaling system tends to be limited, since it cannot effectively handle the increasing runtime complexity of the cloud environment, including, e.g., dynamic and uncertainty caused by workload, the QoS performance and heterogeneity of cloud-based services. Given the shared infrastructure of cloud, the autoscaling process should be aware of and be able to handle QoS interference. This is because improving the QoS performance of a service may likely to downgrade that of its neighbouring services and VMs, which will negatively affect the overall quality of autoscaling and elasticity.

To improve the quality of adaptation, modern autoscaling systems often additionally cover other more sophisticated aspects, including modelling, determining granularity of control and decision making. The modelling is concerned with the model of QoS, environment conditions (e.g., workload) and demand of the control knobs (e.g., software configurations and hardware resources). The resulting models are a powerful tool to assist the autoscaling decision making process. Without loss of generality, in this paper, we term both control knobs and environment conditions in the cloud as cloud primitives. In such context, we further decompose the notion of primitives into two major domains: these are Control Primitive (CP) and Environmental Primitive (EP). Control Primitives are the internal control knobs and can be either software or hardware, which can be managed by the cloud providers to support QoS. Specifically, software control primitives are software tactics and the key configurations in cloud; such as the number of threads in the thread pool of a service/application, the buffer size and load balancing policies etc. Whereas, hardware control primitives are computational resources, such as CPU and memory. Software and hardware control primitives rely on the PaaS and IaaS layers respectively. In particular, it is non-trivial to consider software control primitives when autoscaling in the cloud as they have been shown to be important features for QoS [\cite=2013-JRAO-most-closest-work-2013] [\cite=software-RP-two-loops] [\cite=2014-decision-tree-software-CP-2014]. On the other hand, Environmental Primitives refer to the external stimuli that cause dynamics and uncertainties in the cloud. These, for example, can be the workload and unpredictable incoming data etc. If the cloud provider is able to control the presence of the stimulus, then these can be considered as control primitives. These models can often assist and improve the autoscaling decision making. It is worth noting that the examples of primitives listed above are not exhaustive, Ghanbari et al. [\cite=rule-control-elasticity-cloud] have provided a more completed and detailed list of the possible control primitives in cloud.

Determining granularity of control in the autoscaling logic is essential to ensure the benefit (e.g, QoS and cost objectives) for all cloud-based service. It is concerned with understanding whether certain objectives can be considered in isolation with some of the others. This is because objective-dependency (i.e., conflicted or harmonic objectives) often exist in the decisions making process, which implies that the overall quality of autoscaling can be significantly affected by the inclusion of conflicted or harmonic objectives in a decision making process, hence rendering it as a complex task. This is especially true for the shared infrastructure of cloud where objective-dependency exists for both intra- and inter-services. That is to say, objective-dependency is not only caused by the nature of objectives (intra- service), e.g, throughput and cost objective of a service; but also by the QoS interference (inter-services) due to the co-located services on a VM and co-hosted VMs on a PM [\cite=2013-JRAO-most-closest-work-2013] [\cite=software-RP-two-loops] [\cite=2014-decision-tree-software-CP-2014] [\cite=qcloud].

The final logical aspect in autoscaling logic is the dynamic decision making process that produces the optimal (or near-optimal) decision, which consists of the newly configured values of the related control primitives, for all the related objectives. In the presence of objective dependency, autoscaling decision making requires to resolve complex trade-offs, subject to the SLA and budget requirements. The trade-off decision can be then executed using either vertical scaling and/or horizontal scaling actions, which adapt the cloud-based services and/or VMs correspondingly.

In the following sections, we provide survey and taxonomy for the most influent and recent work that is related to this paper. Particularly, we present the review and discussions based on the key logical aspects for autoscaling in the cloud, which are architectural pattern, QoS modelling, granularity of control and decision making. We then position this paper by discussing how our work differ from those existing approaches.

Architectural Pattern

Autoscaling architecture is the most essential element of an autoscaling system in the cloud. It describes the structure of the autoscaling process, the interaction between components and the modularisation of the important logical aspects in autoscaling. In the following, we survey the key architectural patterns that have been applied for autoscaling in the cloud. In particular, we classify them into three categories based on their basic form; these are Feedback Loop Control [\cite=Brun:2009], Observe-Decide-Act [\cite=self-aware-ML-adaptive-control] and Monitor-Analysis-Plan-Execute [\cite=ibm], where the latter two are essentially detailed variations of the former one. The classification of those architectural patterns is the result obtained from literature review. As one of the key outputs, we have identified that those three architectural patterns are predominantly applied for autoscaling system in the cloud. The taxonomy has been illustrated in Figure [\ref=ch1:arch].

Feedback Loop Control

Feedback Loop Control is the most general architectural pattern for controlling self-adaptive systems, including the autoscaling systems. It is usually a closed-form loop made up of the managing system itself and the path transmitting its origin (e.g., a sensor) to its destination (e.g., an actuator). Here, we further divide the pattern in terms of whether single or multiple loops are used.

Single Loop Control

Single loop control is the simplest, yet the most commonly used pattern for autoscaling in the cloud due to its flexibility. The most common practice with single loop control is to build a feedback loop where the core is the decision making component and an optional QoS modelling component, e.g., Ferretti et al. [\cite=05557978] , CloudOpt [\cite=fine-grained-servce-LQM-MIP-power], SmartSLA [\cite=11icde_smartsla_full], Padala et al [\cite=HPL-2008-123R1-mimo], Kateb et al [\cite=2014-eplison-GA-weigh-h-scaling-2014], Jiang et al. [\cite=06119056], CLOUDFARM [\cite=2014-linear-centralized-decision-making-2014], Grandhi et al. [\cite=2014-kalman+rule-based-2014]. Some other work has included an additional component for workload or demand prediction based on either offline profiling, e.g., Jiang et al. [\cite=typical-navie-scaling-VM-number] and Fernandez et al. [\cite=2014-profiling-decision-tree-scaling-2014], or online learning, e.g., Kingfisher [\cite=2011-cost-aware-v-h-scaling-2011], Gambi et al. [\cite=kriging-controller], Chihi et al. [\cite=2013-self-organising-map-2013] and PRESS [\cite=signal-resource-trend-prediction].

Open feedback loop exists, as presented in Cloudine [\cite=2013-elasticity-primitives-2013], where the scaling actions are partially triggered by user requests. In particular, they use a centralised Resource and Execution Manager to handle all the scaling actions. Apart from the general autoscaling architecture, other efforts are particularly designed upon specific cloud providers [\cite=scale-rule-based], [\cite=MPC-price], [\cite=cache-static-ANN-bi-obj-2012], [\cite=sla-provision]. For example, Zhang et al. [\cite=MPC-price] and Kabir and Chiu [\cite=cache-static-ANN-bi-obj-2012] propose to use a simple feedback loop for architecting autoscaling system, which is heavily tied to the properties of Amazon EC2 and S3. Other applications of single loop control, which are worth mentioning, include: VScale [\cite=parallel-RL-vertical-QoS-2013] is a feedback based framework that particularly focus on vertical scaling of VM in the cloud. It is deployed in a decentralised manner where there is a dedicated instance running on each PM. iBoolean [\cite=MASCOTS11] is a feedback control approach for autoscaling hardware resource in the cloud. It is designed as a distributed management framework, in which each individual VM initialise its own management.

There are architectures using single loop control where the core is classical control theory [\cite=2014-fuzzy-compare-to-JRao-2014], [\cite=2013-software-CP-only-2013], [\cite=acdc09], [\cite=compare-to-Rao-fuzzy-2013]. Particularly, Anglano et al. [\cite=2014-fuzzy-compare-to-JRao-2014] present a fuzzilized feedback control for autoscaling in the cloud. It is a typical controller using fuzzy theory driven by application performance. Guo et al. [\cite=2013-software-CP-only-2013] also present a fuzzy logic based feedback control. However, it only intend to scale the software control primitives.

Multiple Loop Control

Unlike the single loop control approach, it is possible to use multiple loops and controllers for autoscaling in the cloud. Here, multiple feedback loops operate in different levels of the architecture, e.g., one operates at the cloud level while the others operate on each VM. The benefit is that multiple loops provide low coupling in the design of the loops. Notably, multiple loop control can be used to separate global and local controls [\cite=2014-kalman-pair-wised-coupling-on-tiers-2014], [\cite=2012-app-VM-mapping-2012], [\cite=EMA-CPU-memory-PD], [\cite=MIMO-fuzzy-hill-climbing-2013]. Among others, [\cite=2014-kalman-pair-wised-coupling-on-tiers-2014] apply a decentralised feedback control for autoscaling CPU in the cloud. Although it aims for individual applications, the controllers actually operate on each tier of an application. Different controllers do not need to interact with each others. ARUVE [\cite=2012-app-VM-mapping-2012] utilises a global controller in conjunction with the local controller to form multiple feedback loops. The local controllers are decentralised on each PM while the global controller is centralised.

Multiple loop control is also effective for isolating the logical aspects of autoscaling and management in the cloud [\cite=Arc-cover-all-controller], [\cite=ICDCS2011], [\cite=PCA-model-scaling-2013], [\cite=2014-2-SVM-workload-type-2014], [\cite=06032254], [\cite=fuzzy-2-loop-control], [\cite=MASCOTS11-bu-software-CP-full] [\cite=2013-JRAO-most-closest-work-2013], [\cite=TR-10-full-version]. For example, Emeakaroha et al. [\cite=06032254] has also used multiple feedback loops. In particular, they use a global feedback loop consisting of three local feedbacks, each of which operate on SaaS, PaaS and IaaS layer. Wang, Xu and Zhao [\cite=fuzzy-2-loop-control] propose a two layer feedback control for autoscaling in the cloud. The first layer, termed guest-to-host optimisation, controls the hardware resources, e.g., CPU and memory. Subsequently, the host-to-guest optimisation adapts the software configuration accordingly.

Overall, existing work adopts feedback loop control for its simplicity and flexibility. However, instead of designing autoscaling with a clear architectural blueprint beforehand, they utilise a bottom-up approach where the design of autoscaling system starts off from the underlying techniques and algorithms. Such design can limit the consideration of required knowledge for the autoscaling system to perform adaptations, or the consideration is rather simple and coarse-grained, as they do not express what level of knowledge is required at which logical aspect of the system, and how they can be beneficial for the adaptation.

Observe-Decide-Act

Observe-Decide-Act (ODA) loop [\cite=self-aware-ML-adaptive-control] is considered as an extended pattern of the feedback loop control, and it is concerned with the system monitoring itself and its environment, making decisions about how to adapt behaviour using a set of available actions. A unique Decide component separates it from the feedback loop control, as it explicitly requires to perform reasoning about the effects of adaptation on the system's goals and objectives. While an ODA loop is most commonly applied for self-adaptive systems in general, only few work (e.g., [\cite=HuBrKo2011-SEAMS-ResAlloc]) has included it for the design of autoscaling system in the cloud. This is because one important aspect of ODA is to define the effects of human activities on the adaptive behaviours, which is a difficult practice for autoscaling in the cloud.

SEEC [\cite=self-aware-ML-adaptive-control], being the very first work to introduce ODA, is a general framework for self-aware and self-adaptive systems. It applies ODA for decoupling the loops to different roles (i.e., application developer, system developer, and the SEEC runtime decision infrastructure) in the development life-cycle, each role focuses on one or more steps in ODA. [\cite=5694245] adopt an ODA loop to manage FPGA-based systems, where the decision and the its translation to actions are conducted by an incorporation of the Decide and Act steps. Bolchini et al. [\cite=6604228] have used ODA to realise the adaptation for self-adaptive systems because of its simplicity. It observes high-level and raw data from the Observe step, such data is then used by Decide to know which parts of the system to reason on, and finally, actions are taken depends on the characteristic of the system being managed. Huber et al. [\cite=HuBrKo2011-SEAMS-ResAlloc] also use ODA for self-aware autoscaling resources in the cloud. However, unlike traditional ODA loop, it has an additional Analysis step which is used to detect the type of problems that trigger adaptation.

Overall, although the knowledge that a system requirs is sometime discussed in the Decide step (e.g., [\cite=HuBrKo2011-SEAMS-ResAlloc]) in ODA, it cannot capture different levels of knowledge in a fine-grained representation as required by the system. This is because ODA is mainly designed for decoupling loops for different human activities, which allows application and systems programmers to separately specify observations and actions, according to their expertise.

Monitor-Analyze-Plan-Execute

Another pattern extended from the feedback loop control, namely Monitor-Analyze-Plan-Execute (MAPE), is firstly proposed by IBM for architecting self-adaptive systems. In such pattern, the Decide step in OAD is further divided into two substeps, these are Analyze and Plan, where the former is particularly designed to determine the causes for adaptations, e.g., SLA violation; the latter, on the other hand, is responsible for reasoning about the possible actions for adaptation. MAPE sometime can be extended by a Knowledge component (a.k.a. MAPE-K) which maintains historical data and knowledge used by the system for better adaptation.

MAPE (or MAPE-K) is widely applied for autoscaling in the cloud [\cite=li-opt-clouds-4], [\cite=cloud_computing_2010_5_20_50060-ext], [\cite=Compsac_2010_I_Brandic], [\cite=SEASS_2010_Michael_Maurer], [\cite=tse1], [\cite=fuzzy-vm-interference]. For example, the architecture of the FoSII ptoject [\cite=Compsac_2010_I_Brandic] [\cite=SEASS_2010_Michael_Maurer] leverages MAPE-K to realise the self- management interface, which is necessary to devise actions in order to prevent SLA violations in cloud. They also use the additional Knowledge (K) component to record cases and the related solutions, which can assist the autoscaling decision making. QoSMOS [\cite=tse1] is designed for service-based systems rather than for cloud specific autoscaling, however, it contains many aspects similar to that of autoscaling in the cloud. To achieve continuous adaptation, it applies MAPE with the focuses on analytical QoS modelling and optimisation of resource allocation. APPLEware [\cite=fuzzy-vm-interference] is an autoscaling framework which leverages MAPE. In their architecture, the Analyze component model the QoS while the Plan component conducts optimisation process for autoscaling.

Realising multiple MAPE loops is also possible. Zhang et al. [\cite=software-RP-two-loops] introduce an architecture for autoscaling using two nested MAPE loops. The first loop is responsible for adapting the software primitives while the other loop is used to change the hardware primitives. These two loops run sequentially upon autoscaling, that is, adapting the software primitives before changing the hardware primitives. Similarly, BRGA [\cite=BRGA-resource] utilises MAPE to realise a framework for autoscaling in the cloud. Such solution consists of both the local and global view of the cloud-based application. In particular, the Monitor and Execution phase maintain the global view whereas the Analyze and Plan phase manage the local view on each PM. The authors claim that such an approach can achieve good global quality with reasonable management overhead.

In conclusion, MAPE can be good for separation of concepts (e.g., Analyze and Plan) and for expressing the sequential interactions between those concepts. However, although the Knowledge component can be considered, there is still no fine-grained representation of the required knowledge for the system. Thus, it is not immediately intuitive that what level of the knowledge is required by each logical aspect of the autoscaling system.

QoS Modelling

QoS modelling, or performance modelling, is a fundamental research theme in cloud computing and it can serve as useful foundations for addressing many research problems in the cloud [\cite=autosclaing_survey], including autoscaling. The QoS models correlate the QoS attributes to various control primitives and environmental primitives. Clearly, these models are particularly important in cloud autoscaling, as they are a powerful tool that can assist the reasoning about the effects of adaptation on objectives in the autoscaling decision making process. Typically, QoS modelling consists of two phases, namely primitives selection and QoS function training. More precisely, the primitives selection phase determines which and when the primitives correlate with the QoS; while QoS function training phase identify how these primitives correlate with the QoS, i.e., their magnitudes in the correlation. The QoS models can be either static or dynamic, where the former refers to the models' expression and their structure (e.g., the number of inputs and their weights) do not change over time; while the latter permits such changes. Those models can be also applied as online at system runtime, or offline at design phase of the system. In the following, we survey the key work on QoS modelling in the cloud and classify them in terms of the algorithms they apply. The taxonomy has been illustrated in Figure [\ref=ch1:qos].

Analytical Modelling

Analytical modelling approaches rely on mathematical models that have a closed-form solution to model the cloud-based service. These models are often built offline based on theoretical principles and assumptions. Next, we further divide the analytical modelling approach into queuing theory, dependability models and black box models.

Queuing theory

Queuing model and queuing network are widely applied for QoS modelling in the cloud. They model the cloud-based services as a single queue or a collection of queues interacting through request arrivals and departures. Specifically, a single queue has been used to model the correlation of response time (or throughput) to CPU, number of VM and workload. For example, depending on the assumption of the distribution on arrival and service rate, the model can be built as M/G/c queue by Zhang et al. [\cite=MPC-price], M/G/m queue by Jiang et al. [\cite=06119056] , M/M/1 queue by E3-R [\cite=E3-R-extended] and JustSAT [\cite=queue-VM-group], and M/M/m queue by Jiang et al. [\cite=typical-navie-scaling-VM-number]. To create more detailed modelling with respect to the internal structure of cloud-based services, multiple queues can be used to create QoS models: Goudarzi and Pedram [\cite=multitier-resalloc-Cloud11] apply multiple queues to model the response time for cloud-based multi-tiered applications with respect to number of VM and workload. Their work calculates average response time for the queue in the forward direction throughout the tiers. In a similar way, Bi et al. [\cite=queue-prediction] use a queuing network composed of an M/M/c queue and multiple M/M/1 queues to estimate the correlation between response time and number of VMs and workload for cloud-based application. Li et al. [\cite=wosp10sla] apply a single queue to model the correlation between response time and CPU, workload and thread. In particular, the model contains finite capacity regions, which denote the place constraints on the maximum number of jobs circulating in a subnetwork of queues. This is because they are the simplest class of models that offer the features to describe performance scalability as a function of the software threading level and for the number of CPUs.

Unlike classical queuing model and queuing network, the Layer Queuing Network (LQN) additionally model the dependencies arising in a complex workflow of requests to cloud-based services and applications. Chi et al. [\cite=3-stages-game-theory] use LQN (i.e., based on M/M/n queue) to model the QoS of application, which is response time with respect to CPU and workload. CloudOpt [\cite=fine-grained-servce-LQM-MIP-power] relied on LQN as the aggregate QoS model for all the services contained by an application. It models only response time with respect to CPU and workload. Li et al. [\cite=li-opt-clouds-4] use LQN for model services in an application. Again, it only captures response time with respect to CPU and workload. Zhu et al. [\cite=sla-provision] have also used LQN where the authors employ a global M/M/c queue for the entire on-demand dispatcher and then a M/G/1 queue on each tier of an application. The former queue correlates the response time to number of VMs while the latter queue models the relationship between response time and CPU of the VM that contains the corresponding tier.

Dependability models

Dependability models are another widely used technique for QoS modelling in the cloud. This approach focuses on the modelling of stable states for QoS attributes. For example, Copil et al. [\cite=2013-rule-based-multi-elasiticity-2013] uses a graph representation to model the dependency between per-service QoS and the necessary primitives. Although the graph can be updated at runtime, the model is essentially analytical. In QoSMOS [\cite=tse1], the authors analytically solve the Markov Models (Discrete-Time Markov Chain and Markov Decision Process) to model the QoS for services in an application. The model correlates QoS attributes with hardware resources and workload. Huber et al. [\cite=HuBrKo2011-SEAMS-ResAlloc] uses Palladio Component Model (PCM) as architecture-level QoS model since it allows to explicitly model different usage profiles and resource allocations. Kateb et al. [\cite=2014-eplison-GA-weigh-h-scaling-2014] uses model@runtime to correlate QoS attributes with the number of VM. The modelling approach is essentially based on a domain specific language, which does not only able to reason about the system at design time, but is also able to assist decision making during runtime.

Black box models

Black-box models are also popular, in which the QoS is modelled based on empirical knowledge or statistical data of history [\cite=06032254], [\cite=Compsac_2010_I_Brandic], [\cite=2014-linear-centralized-decision-making-2014], [\cite=Emeakaroha_CloudComp2010], [\cite=HPCS_IWCMC_Vincent], [\cite=profit-cent-local-search], [\cite=cache-static-ANN-bi-obj-2012],[\cite=BRGA-resource]. Among others, CLOUDFARM [\cite=2014-linear-centralized-decision-making-2014] uses a empirical QoS model where the correlation between certain QoS values and the required resource is captured (i.e., CPU). In particular, the authors assumed that the magnitudes of resources to the QoS values is known, as specified by the cloud service or application provider. The FoSII project [\cite=Compsac_2010_I_Brandic] has also applied empirical QoS models such that the correlation between hardware resource (i.e., CPU, memory and bandwidth) and QoS is hard-coded using cases, each of which contains a set of particular values of resource and their resulted utility value. Another work from Emeakaroha et al. [\cite=Emeakaroha_CloudComp2010] propose an empirical model that maps the expected QoS values with CPU, memory, bandwidth and storage. The model relies heavily on the assumption of the system that being managed. Their extended work [\cite=HPCS_IWCMC_Vincent] is also based on a similar approach, where the authors correlate the QoS attributes to different CPU, memory, bandwidth and storage using manual and empirical mapping. The proposed mapping can be as simple as one QoS attribute to one primitives, or a complex form where multiple cloud primitives are associated with a QoS attribute.

Overall, analytical modelling has the advantage of simplicity and interoperability. In particular, such modelling is usually highly intuitive and has negligible overhead when applied for autoscaling in the cloud. However, analytical approaches often require in-depth knowledge about the likely behaviours of the system being modelled. Consequently, their effectiveness is restricted to the assumptions of service's internal operations; such static nature makes these approaches limited in coping with the dynamic and uncertainty at runtime. Finally, both primitives selection and QoS function training phases in analytical approaches are often static and offline. However, for some of the approaches (e.g., [\cite=tse1], [\cite=2014-eplison-GA-weigh-h-scaling-2014]), their QoS function training phase can be achieved in a dynamic and online manner.

Simulation Based Modelling

Various simulators exist for creating QoS models; here, conducting simulations is usually a complex and expensive process and thus they are used in an offline manner. In practice, simulation is required to be setup by the domain experts, who will often need to analyse, interpret and profile the data collected after simulation runs. Specifically, Fernandez et al [\cite=2014-profiling-decision-tree-scaling-2014] have relied on a profiling approach that builds the QoS model for each bundle of VM offline. The process is similar to a simulation modelling approach. CDOSim [\cite=full-simulation-model] is a framework that simulates the actual application in the cloud to restrict the search-space for autoscaling and to steer the exploration towards promising decisions. CloudSim [\cite=CloudSim] is a simulation toolkit that models QoS attributes (of VM) with respect to resource allocation. It supports both single cloud and multiple clouds scenarios. As an extension of CloudSim, CloudAnalysis [\cite=CloudAnalysis] allows the simulation of QoS attributes for the application deployed on geographically-distributed datacenters. Similarly, DCSim [\cite=DCSim] simulates the overall quality of resource autoscaling for the entire cloud.

Overall, simulation can produce good QoS models providing that the scenarios which have been simulated are similar to those that would occur at runtime. However, similar to the analytical approaches, simulation based modelling is also static and restricted by the assumptions made in the simulators, e.g., distribution of workload and the effects of QoS interference. In addition, it can be expensive to use as it often requires heavy human intervention. Commonly, simulation based modelling approach is an offline process, in particular, the QoS function training phases can be dynamic; while the primitives selection phase is static.

Machine Learning Based Modelling

The increasing complexity of managing services in the cloud makes the modelling difficulty far beyond the capability of human analysis. To this end, recent works have been leveraging the advances of machine learning algorithms. In the following, we survey the key work that applies machine learning approaches for QoS modelling in the cloud. In particular, we have classified them into two categories, these are: linear and nonlinear modelling.

Linear modelling

Learning algorithms based on linear models for QoS modelling in the cloud can handle linear correlation between a selected set of inputs (e.g, CPU, memory, number of VM, workload etc) and output (i.e., QoS attributes), and they are sometime very efficient. Diao et al. [\cite=noms2002_MIMO] propose a very early work on QoS modelling using Auto-Regressive and Moving-Average (ARMA) and Multi-Inputs-Multi-Output (MIMO) model on-the-fly. Their work is not cloud specific but it provides insight for many subsequent work on cloud based QoS modelling. Simple linear models most commonly rely on linear regression, where each primitive input is associated with a time-varying weight, e.g., Lim et al. [\cite=acdc09], Zhang et al. [\cite=software-RP-two-loops] and Collazo-Mojica et al. [\cite=linear-mapping-CP-bundles-2012]. More advanced forms exist, e.g., Padala et al. [\cite=HPL-2008-123R1-mimo] have used ARMA trained by Recursive Least Squares (RLS). The authors claim that the linear AMRA model is easy to be estimated online and can simplify the corresponding controller design problem. The authors found that the second-order ARMA model can predict the application performance with adequate accuracy. Kalivianaki et al. [\cite=2014-kalman-pair-wised-coupling-on-tiers-2014] uses Kalman filter to update the QoS model. The authors claim that the Kalman filter is optimal in the sum squared error sense under the assumptions that the system is described by a linear model, and the process and measurement noise are white and Gaussian.

Linear machine learning algorithms are also commonly used with analytical approaches to form QoS models. Specifically, Grandhi et al. [\cite=2014-kalman+rule-based-2014] and Zheng et al. [\cite=kalman-AR] have proposed hybrid model: to model the multi-tiered application, they have relied on a modified LQN where there are some time-varying coefficients. The authors then employ the Kalman filter as an online parameter estimator to continually estimate those coefficients. Ghanbari et al. [\cite=kalman-clustering] have also followed a similar approach, but through the use of k-mean clustering, they additionally cluster the model into multiple sub-models based on different types of workload. The approach proposed by Xiong et al. [\cite=ICDCS2011] has relied on a combined model, where a M/G/1 queue is used to model the correlation between response time and workload; while ARMA is used to model the relationship of response time and CPU.

There is limited work that attempts to capture the information of QoS interference in the linear QoS model and they only focus on the VM-level [\cite=fuzzy-vm-interference], [\cite=qcloud], [\cite=vm-fuzzy-MIMO], [\cite=2014-decision-tree-software-CP-2014]. As an example, Q-Cloud [\cite=qcloud] has explicitly considered QoS interference by using the hardware control primitives of all co-hosted VMs as inputs, rendering it in a MIMO manner. The model itself is a simple linear model and it can be easily trained by using Least Mean Square (LMS).

Nonlinear modelling

Learning algorithms based on nonlinear models for QoS modelling in the cloud is able to capture complex and nonlinear correlation, in addition to the linear one. However, it can also produce relatively large overhead than the linear modelling. Here, existing work often aim to model the correlation between hardware control primitives (e.g., CPU, memory and bandwidth) and QoS. The nonlinear modelling can be relied on kriging model [\cite=kriging-controller], Regression Tree (RT)[\cite=11icde_smartsla_full], Artificial Neural Network (ANN) [\cite=dynamic-model-comparison] [\cite=2014-navie-ANN-GA-2014] [\cite=JSS_Kousiouris], Support Vector Machine (SVM) [\cite=2014-2-SVM-workload-type-2014] [\cite=dynamic-model-comparison], change-point detection [\cite=change-point], and ensemble or bucket of multiple algorithms [\cite=Chen:2013:seams] [\cite=Chen:2014:ucc] [\cite=Chen:2015:tse-pending]. For example, Gambi et al. [\cite=kriging-controller] utilise kriging model, which is a spatial data interpolator akin to nonlinear and radial basis functions, and it extends traditional regression with stochastic Gaussian processes. SmartSLA [\cite=11icde_smartsla_full] employs Regression Tree (RT) and boosting to model the QoS. RT partitions the parameter space in a top-down fashion, and organises the regions into a tree style. The tree is then trained by M5P where the leaves are regression models. The work from Kunda et al. [\cite=dynamic-model-comparison] presents sub-modelling based on ANN and SVM for correlating QoS with hardware control primitives in the cloud. Instead of building a single model for a QoS attribute, they train n sub-models, whereby n is determined by performing k-mean clustering based on the similarity between data values of QoS. This is because they observe that large errors were mostly concentrated in a few sub-regions of the output value space, indicating a single model's inability to accurately characterise changes in application behaviour as it moves across critical resource allocation boundaries. The authors claim that the approach can be applied online.

Examples exist for cases where multiple linear and/or nonlinear machine learning algorithms are used together. Zhu and Agrawal [\cite=TR-10-full-version] use a variant of ARMA and SVM to model the correlation of QoS attributes to software and hardware control primitives. In particular, the ARMA variant, which is trained by SVM, is used to link QoS and software control primitives. Subsequently, another dedicated SVM is used to model the relationship between software control primitives and hardware control primitives (i.e., CPU and memory in the work). Another work from Kousiouris et al. [\cite=MOCS2014-full] correlates QoS attributes with various primitives using ANN. Additionally, it applies a time-series ANN to predict the workload in conjunction with the QoS models. This aims to provide more accurate information when making prediction online.

Dynamic primitives selection

All the aforementioned work regards the primitives selection as a manual and offline process, most commonly, they have relied on empirical knowledge and heavy human analysis to select the important primitives as the inputs of QoS models. Although not many, there is some work that explicitly considers dynamic process in primitives selection, which tends to be more accurate and can be easily applied [\cite=PCA-model-scaling-2013], [\cite=ISPASS07], [\cite=2013-single-learner-filter-wrapper-LR-2013]. As an example, vPerfGuard [\cite=2013-single-learner-filter-wrapper-LR-2013] is a framework that correlates QoS attributes with respect to software control primitives, hardware control primitives and environmental primitives. The authors achieve primitive selection based on both filter (relevance based correlation coefficient) and wrapper (i.e., hill-climbing comparison based on algorithms like k-nearest-neighbour and linear regression etc). Linear regression is used as default to train QoS based on the selected primitives.

Comparison of different learning algorithms

Given the various types of machine learning algorithms, it can be difficult to determine which one(s) are the appropriate algorithms for QoS modelling in the cloud, with respect to both accuracy and overhead. There are researches that have conducted detailed comparisons of different possible learning algorithms for QoS modelling in the cloud [\cite=2013-offline-profiling-2013] [\cite=determine-CP-compare-models] [\cite=tp38], for example, Lloyd et al. [\cite=determine-CP-compare-models] conduct an extensive experiment over various machine learning algorithms (i.e., MLR, MRS and ANN) in cloud. They select the primitives based on manual analysis. The results show that the relevant and useful primitives could be different depending on the characteristics of services and application; and that different machine learning algorithms achieve a variety of accuracy depending on the scenarios.

Overall, machine learning based modelling approaches have the advantage of requiring limited human intervention, and care able to continually evolve themselves at runtime in order to cope with dynamics and uncertainty. Nevertheless, depending on the learning algorithm, the resulting overhead can be high (e.g., the nonlinear ones) and the accuracy is sensitive to the given scenarios (e.g., fluctuation of the data trend). Generally, the machine learning approaches can be applied as offline, online or a mixture of the both. According to the existing work surveyed for QoS modelling in the cloud, we discover that the QoS function training phase is often dynamic; while there is very little work that intends to consider primitives selection phases as a dynamic process (online or offline), and the others have relied on offline and manual analysis. Therefore, we can conclude that majority of the approaches that apply machine learning for QoS modelling are semi-dynamic. In addition, we have also found that only a small amount of existing work intends to consider QoS interference in the modelling; and they only focus on VM-level interference.

Comparison of QoS Modelling to Workload and Demand Modelling

Despite the fact that QoS modelling is fundamentally helpful for cloud autoscaling, it can be difficult to achieve given the heterogeneity of possible primitives and the multi-dimensional input space of QoS modelling. Therefore, some existing work on autoscaling (e.g., [\cite=2011-cost-aware-v-h-scaling-2011] , [\cite=ILP-cost-only-scaling-2013], [\cite=JSS_Kousiouris]) have considered simpler alternatives, i.e., model the workload and demand for assisting autoscaling decision making. In those cases, the modelling is reduced to a single dimension, where the core is to model the trend of the workload or demand using its historical data. The models would be used to predict the likely value at the next interval. However, the single dimension in workload or demand models do not offer the ability to reason about the effects of autoscaling decisions and the possible trade-offs. It is important to note that trade-off decisions making is a critical logical aspect for autoscaling, and it is also one of the cores of this paper. Therefore, this paper has explicitly focused on QoS modelling.

Granularity of Control

The ultimate goal of autoscaling is to optimise the QoS and cost objectives, which are referred to as benefit, for all cloud-based services. To this end, the granularity of control in autoscaling plays an integral role, since it determines which and how many objectives should be considered in a decision making process of autoscaling. In the following, we classify existing cloud autoscaling approaches into different categories depending on what level of granularity they tend to operate at. The taxonomy has been illustrated in Figure [\ref=ch1:granularity].

Controlling at Service Level

Service level is the finest level of control in the cloud. It is worth noting that by service, we refer to any conceptual part of the system being managed. As a result, control granularity at the service level may refer to independently controlling/scaling an application, a tier of an application or a cloud-based service.

Specifically, most work has focused on controlling each cloud-based application. These approaches have relied on controlling the QoS and/or cost for each individual application in isolation, and therefore, they regard an application as a service. Examples of such include: [\cite=2011-cost-aware-v-h-scaling-2011], [\cite=queue-VM-group], [\cite=2014-profiling-decision-tree-scaling-2014], [\cite=linear-mapping-CP-bundles-2012], [\cite=TR-10-full-version], [\cite=wosp10sla], [\cite=06119056], [\cite=cloud_computing_2010_5_20_50060-ext], [\cite=signal-resource-trend-prediction] and [\cite=PCA-model-scaling-2013]. To describe some of them in detail, Lim et al. [\cite=acdc09] control the application and its required VM, in which case an application is regarded as a service. Sedaghat et al. [\cite=ILP-cost-only-scaling-2013] regard application as a service, and considered the required number of VMs and the fixed VM bundles for such service. Jiang et al. [\cite=typical-navie-scaling-VM-number] control each application, each of which is, again, regarded as a service and therefore it is essentially service-level control. In addition, the authors group VMs into different fixed bundles.

There is also existing work that controls cloud-based service in general, which can be regarded as any conceptual part of a cloud-based system. Copli et al. [\cite=2013-rule-based-multi-elasiticity-2013] control the QoS, cost and their elasticity for each service deployed in the cloud. Yang et al. [\cite=2014-ARMA-single-agg-objective-2014] control the cost of individual cloud-based services. The FoSII project [\cite=Compsac_2010_I_Brandic] controls individual cloud-based service, their QoS and cost. [\cite=kriging-controller] control at the service level, where the controller decides on the optimal autoscaling decision for cloud-based service in isolation. QoSMOS [\cite=tse1] explicitly focuses on each individual service, and adapting it in isolation. Kateb et al. [\cite=2014-eplison-GA-weigh-h-scaling-2014] consider many services are encapsulated in the application, and the authors focus on each service in isolation. The E3-R framework [\cite=E3-R-extended] and Frey et al. [\cite=GA-full-simulation] control each service, including their composition and autoscaling.

Controlling at Virtual Machine Level

VM level means that the control and decision making operate at each VM. In particular, certain work assumes a one-to-one mapping between application (or a tier) and VM and thus they can be categorised as either service level or VM level granularity. To better separate them from the pure service level granularity of control, these work are regarded as VM level granularity. Specifically, FC2Q [\cite=2014-fuzzy-compare-to-JRao-2014] regards application tier and VM interchangeably, therefore controlling each tier of an application is equivalent to control each individual VM. Similarly, Kalyvianaki et al. [\cite=2014-kalman-pair-wised-coupling-on-tiers-2014] control a tier of an application that resides on a VM, and the authors only focus on CPU allocation of a VM. Wang, Xu and Zhao [\cite=fuzzy-2-loop-control] control the cloud in a per-VM basis, and each VM is adapted in isolation. VScale [\cite=parallel-RL-vertical-QoS-2013] focuses on vertical scaling only and hence the control granularity is per VM. Zhang et al. [\cite=software-RP-two-loops] assume only one application per VM, and control the deployed VM in isolation correspondingly. Guo et al. [\cite=2013-software-CP-only-2013] control the application deployed on the VM and the authors assume one application per VM. Similarly, Matrix [\cite=2014-2-SVM-workload-type-2014] has used VM level control as there is only one application per VM.

Controlling at Physical Machine Level

Autoscaling decision making on each PM independently is referred to as PM level control. The primary intention of PM level control is to manage the QoS interference caused by co-hosted VMs. Among the others, Xu et al. [\cite=MASCOTS11] control the VMs collectively at the PM level, in this way, it tries to promote better management of QoS interference. The extended work from Xu et al. [\cite=MASCOTS11-bu-software-CP-full] consider QoS interference at the VM level, therefore the granularity of control is based on each PM. Similarly, Bu et al. [\cite=2013-JRAO-most-closest-work-2013] considers VM level QoS interference and thus the control is for each PM. Minarolli and Freisleben [\cite=2014-navie-ANN-GA-2014] consider all the co-hosted VM in conjunction with each others and thus its control granularity is at the PM level. Lama et al. [\cite=fuzzy-vm-interference] control at the PM level in order to handle QoS interference.

Controlling at Cloud Level

The most coarse level of control granularity is at the cloud level. The majority of the work achieves autoscaling at the cloud level by using a centralised and global controller, with an aim to manage utility ([\cite=3-stages-game-theory], [\cite=2012-app-VM-mapping-2012], [\cite=6008793], [\cite=2013-elasticity-primitives-2013], [\cite=profit-cent-local-search], [\cite=2014-linear-centralized-decision-making-2014]), profits ([\cite=li-opt-clouds-4], [\cite=sla-provision]) and availability ([\cite=06032254]). Among others, Ferretti et al. [\cite=05557978] control the QoS for all cloud-based services in a global manner. However, the actual deployment can be either centralised or decentralised. Similarly, CRAMP [\cite=EMA-CPU-memory-PD] uses a centralized and global controller, it controls the entire cloud for cost and QoS. CloudOpt [\cite=fine-grained-servce-LQM-MIP-power] also controls the entire cloud using centralised control, as the considered optimisation involves all the PM in the cloud. Zhang et al. [\cite=MPC-price] control the cost of the entire cloud with respect to how the VM instances of Amazon can be utilised. BRGA [\cite=BRGA-resource] maintains global view of the entire cloud, and thus it belongs to cloud level control. The FOSII project [\cite=self-sla-and-resource] also controls the entire cloud in a centralised manner, where the goal is to manage the entire cloud at infrastructure level.

Some of the developments have relied on a decentralised manner where a consensus protocol is employed for controlling at the cloud granularity. For example, Wuhib et al. [\cite=Arc-cover-all-controller] aim to control the entire cloud, and thus the QoS and the overall power consumption of cloud can be collectively managed. In the mean time, they have relied on decentralised deployment, which can reduce the overhead of cloud-level control.

Controlling at Multiple Levels

Some work operates at multiple levels, with an aim to better manage the overhead and global benefit [\cite=Chen:2013:iccs], [\cite=Chen:2014:seams], [\cite=Chen2014349]. For example, Minarolli and Freisleben [\cite=MIMO-fuzzy-hill-climbing-2013] combine both PM level and cloud level control, where the PM level is decentralised and the objective is to optimise the utility locally. Similarly, SmartSLA [\cite=11icde_smartsla_full] aims to control the resource allocation for all the cloud-based services, therefore it utilises a global, cloud-level control in addition to the decentralised local control on each VM.

In summary, the finer granularity of control implies that it is harder to achieve globally-optimal benefit but likely to generate smaller overhead. On the other hand, globally-optimal benefit can be easier reached with large overhead if the granularity of control is coarser. All of the approaches surveyed operate at static and fixed granularity of control, even for the hybrid ones. As a result, given the time-varying QoS sensitivity and interference in cloud, they can be inflexible for any runtime changes about the effects of control granularity to the global benefit.

Trade-off Decision Making

The final important logical aspect in cloud autoscaling is the challenging decision making process, with the goal to optimise QoS and cost objectives. It is even harder to handle the trade-off between possibly conflicting objectives. Such decision making process is essentially a combinatorial optimisation problem where the output is the optimal decision containing the newly configured values for all related control primitives. In the following, we survey the key work on the decision making for cloud autoscaling. In particular, we classify them into three categories, these are Rule Based Control, Control Theoretic Approach and Search Based Optimisation. The taxonomy has been illustrated in Figure [\ref=ch1:decision].

Rule Based Control

Rule-based control is the most classic approaches for making decision in cloud autoscaling. Commonly, one or more conditions are manually specified and mapped to a decision, e.g., increase CPU and memory by x if the throughput is lower than y. Therefore, the possible trade-off is often implicitly handled by the conditions and actions mapping. Specifically, Cloudline [\cite=2013-elasticity-primitives-2013] allows programmable elasticity rules to drive autoscaling decisions. It is also possible to modify these rules at runtime as required by the users. Copil et al. [\cite=2013-rule-based-multi-elasiticity-2013] handle the decision making process by specifying different condition-and-actions mapping for autoscaling in the cloud. In addition, the rules can be defined at different levels, e.g., PaaS and IaaS. Similarly, Ferretti et al. [\cite=05557978] allow to setup mapping between QoS expectation and actions using XML like notations. The autoscaling decision making in the work from Emeakaroha et al. [\cite=06032254] is also based on predefined rules, in addition, a simple heuristic algorithm is used to search for the best available VMs. Rule based control for autoscaling decision making can be also found in other work, e.g., Wuhib et al. [\cite=Arc-cover-all-controller], Manuer et al. [\cite=self-sla-and-resource], Han et al. [\cite=scale-rule-based] and Chazalet et al. [\cite=cloud_computing_2010_5_20_50060-ext].

Overall, we discovered that all the rule based autoscaling decision making approaches have considered both vertical and horizontal scaling as the final actuations in the cloud. Generally, rule based control is a highly intuitive approach for autoscaling decision making, and it also has negligible overhead. However, the static nature of the rules requires to assume all the possible conditions and the effects of those decisions that are mapped to the conditions. In addition, the fact that they heavily rely on human intervention and analysis can quickly become an issue. Consequently, they tend to be limited in dealing with the dynamics and uncertainties in cloud, especially when there are complex trade-offs.

Control Theoretic Approach

Advanced control theory is another widely investigated approach for autoscaling decision making in cloud because of its low latency and dynamic nature. However, it is difficult to explicitly reason about the effects of possible trade-off decisions in a control theoretic approach.

Among the others, classical controllers (e.g., Proportional-Derivative control [\cite=2012-app-VM-mapping-2012] [\cite=acdc09] [\cite=EMA-CPU-memory-PD], Kalman control [\cite=2014-kalman-pair-wised-coupling-on-tiers-2014] [\cite=2014-kalman+rule-based-2014] and Fuzzy control [\cite=2014-fuzzy-compare-to-JRao-2014] [\cite=compare-to-Rao-fuzzy-2013] [\cite=fuzzy-2-loop-control] ) are commonly designed as a sole approach to make autoscaling decisions in the cloud. Specifically, ARUVE [\cite=2012-app-VM-mapping-2012] and CRAMP [\cite=EMA-CPU-memory-PD] utilises a Proportional-Derivative (PD) controller, where the proportional and derivative factors do not depend on a QoS model of the application or the infrastructure dynamics, and support proactive resource allocation for the application server tier with dynamic scaling of web applications in a shared hosting environment. Anglano et al. [\cite=2014-fuzzy-compare-to-JRao-2014] and Albano et al. [\cite=compare-to-Rao-fuzzy-2013] apply fuzzy control that is updated by fuzzy rules at runtime. The aim is to optimise both QoS, cost and energy by autoscaling hardware resources. Although the authors claim they can cope with any hardware resources, the approach only focus on CPU allocation. They have also assumed that QoS interference rarely occurs. Kalyvianaki et al. [\cite=2014-kalman-pair-wised-coupling-on-tiers-2014] and Grandhi et al. [\cite=2014-kalman+rule-based-2014] use MIMO model and Kalman controller for making autoscaling decisions, and the authors aim at response time by autoscaling CPU on a VM.

Control theoretic approaches can be sometime used with other algorithms to better facilitate the autoscaling decision making [\cite=ICDCS2011], [\cite=MIMO-fuzzy-hill-climbing-2013], [\cite=2013-software-CP-only-2013], [\cite=IWQOS11], [\cite=TR-10-full-version], [\cite=fuzzy-vm-interference]. Particularly, the gains in the controllers can be further tuned by optimisation and/or machine learning algorithms, and this is especially useful for Model Predictive Control (MPC). Among others, Zhu and Agrawal [\cite=TR-10-full-version] utilise a Proportional-Integral-Derivative (PID) and reinforcement learning controller for decision making with respect to adapting software control primitives. Such result is then tuned in conjunction with the hardware control primitives using exhaustive search. The QoS attributes and cost are formulated as weighted-sum relation. The autoscaling decision making process in APPLEware [\cite=fuzzy-vm-interference] have relied on MPC, which involves optimising a cost function that expresses the local control objectives and resource constraints over a time interval. The current state of the local application and the control decisions made by neighbouring controllers of a VM are taken into account to perform the optimisation using quadratic programming solver.

In summary, we discovery that the majority of the control theoretic work have considered both vertical and horizontal scaling as the final actuations in the cloud. Overall, the control theoretic approaches are efficient for making autoscaling decisions. However, the major drawback of control theoretic approaches is that they require to make many actuations on the physical system, in order to collect the 'errors' for stabilising itself. This means that amateur decisions are very likely to be made. In addition, the trade-offs are only implicitly handled.

Search Based Optimisation

A large amount of existing work relies on search-based optimisation, in which the decisions and trade-offs are extensively reasoned in a finite, but possibly large search space. Depending on the algorithms, search-based optimisation for autoscaling decision making in the cloud can be either explicit or implicit— the former performs optimisation as guided by explicit system models; while this process is not required for the later.

Implicit search

As mentioned, the implicit and search-based optimisation approaches for autoscaling decision making do not use QoS models. Similar to the control theoretic approaches, the implicit search is also limited in reasoning about the possible trade-offs. For example, the work from Xu et al. [\cite=MASCOTS11] , [\cite=MASCOTS11-bu-software-CP-full] applies a model-free Reinforcement Learning (RL) approach for adapting thread, CPU and memory for QoS and cost. The approach is however implicit, providing that there is neither explicit system models nor explicit optimisation. The authors have considered QoS interference during autoscaling. Similarly, VScale [\cite=parallel-RL-vertical-QoS-2013] utilises RL for making autoscaling decisions, which are then achieved by vertical scaling. The RL is realised by using parallel learning, that is to say, the authors intend to speed up agent's learning process of approximated model by learning in parallel. Therefore, the agent does not have to visit every state-action pair in a given environment.

The approaches that rely on demand prediction (e.g., the Autoflex [\cite=2013-scaling-select-one-predictor-and-error-correlation-2013], PRESS [\cite=signal-resource-trend-prediction], [\cite=ILP-cost-only-scaling-2013], [\cite=2010-demand-pattern-match-2010], [\cite=2013-self-organising-map-2013] and [\cite=ensemble-prediction-VM-2014-full]) are also regarded as implicit search. This is because the autoscaling decision is essentially predicted by the demand models, without the needs of reasoning or optimisation process.

Explicit search

In search-based optimisation category, the explicit approaches for autoscaling decision making rely on the explicit QoS models to evaluate and guide the search process. Depending on the different formulations of the decision making problem for autoscaling in the cloud, explicit search can reason about the effects of decisions and the possible trade-offs in details. In this paper, we have surveyed the approaches that rely on three the most commonly used formulations, these are single objective optimisation, weighted-sum optimisation, and Pareto optimisation.

It is common to optimise only a single objective (e.g., cost or profit) for autoscaling in the cloud, providing that the requirements of the other objectives are satisfied (i.e., they are often regarded as constraints) [\cite=2014-ARMA-single-agg-objective-2014], [\cite=MPC-price], [\cite=PCA-model-scaling-2013], [\cite=queue-VM-group], [\cite=linear-mapping-CP-bundles-2012], [\cite=HuBrKo2011-SEAMS-ResAlloc], [\cite=2011-cost-aware-v-h-scaling-2011], [\cite=ILP-cost-only-scaling-2013], [\cite=fine-grained-servce-LQM-MIP-power], [\cite=2014-aco-app-to-VM-consolidation-2014], [\cite=2014-2-SVM-workload-type-2014], [\cite=2014-decision-tree-software-CP-2014]. For example, Kingfisher [\cite=2011-cost-aware-v-h-scaling-2011] and Sedaghat et al. [\cite=ILP-cost-only-scaling-2013] use Integer Linear Programming (ILP) to optimise the cost for scaling the CPU and memory for VMs of an application while regarding the demand for satisfying QoS as constraint. Sedaghat et al. [\cite=ILP-cost-only-scaling-2013] has additionally assumed fixed VM bundles. Similarly, CloudOpt [\cite=fine-grained-servce-LQM-MIP-power] models the decision making for autoscaling using a weighted-sum of different aspect of cost, which is still regarded as single objective optimisation, and the optimisation is then resolved by mixed integer programming approach. A recent extension of ARUVE [\cite=2014-aco-app-to-VM-consolidation-2014] formulates the decision making in autoscaling as a single optimisation on cost, which is resolved by using Ant Colony Optimisation (ACO).

To apply search based optimisation for autoscaling in the cloud, the most widely solution for handling the multi-objectivity is to aggregate all related objectives into a weighted (usually weighted-sum) formulation, which converts the decision making process into a single objective optimisation problem. The search based algorithm include: exhaustive search [\cite=3-stages-game-theory] [\cite=tse1] [\cite=kriging-controller] [\cite=typical-navie-scaling-VM-number], auxiliary network flow model [\cite=li-opt-clouds-4], force-directed search [\cite=multitier-resalloc-Cloud11], binary search [\cite=cache-static-ANN-bi-obj-2012]. For example, the FoSII project [\cite=Compsac_2010_I_Brandic] [\cite=SEASS_2010_Michael_Maurer] regards the autoscaling decision making as case based reasoning process, where the decision is made by looking for similar cases from the past and reusing the solutions of these cases to solve the current one. The case and solution pairs are linked to aggregative utility values based on the analytical model, thus the reasoning process is essentially an optimisation for the optimal decision using exhaustive search algorithm. Goudarzi and Pedram [\cite=multitier-resalloc-Cloud11] use a weighted-sum formulation containing QoS and cost for an application tier. The optimisation is resolved using force-directed search, in which an initial solution based on the solution given for the profit upper bound problem is generated. Next, distribution rates are fixed and resource sharing is improved by a local optimisation step.

Some work has relied on more advanced and nonlinear search algorithms, ranging from relatively simple ones: dynamic programming [\cite=2014-linear-centralized-decision-making-2014] and local-search strategy [\cite=profit-cent-local-search], to more complex forms: grid search [\cite=11icde_smartsla_full], decision tree search [\cite=2014-profiling-decision-tree-scaling-2014] [\cite=2014-decision-tree-software-CP-2014] and quadratic programming [\cite=HPL-2008-123R1-mimo]. For example, CLOUDFARM [\cite=2014-linear-centralized-decision-making-2014] addresses the decision making based on a weighted-sum utility function of all cloud-based application and services. The decision making process is formulated as a knapsack problem, which can be resolved by dynamic programming. In SmartSLA [\cite=11icde_smartsla_full], the decision making for autoscaling is aimed for optimising SLA penalty, which is essentially based on the aggregation of expected QoS values. The optimisation is resolved using a grid search algorithm. To optimise weighted-sum utilisation, Amiya et al. [\cite=2014-decision-tree-software-CP-2014] have explicitly aimed to mitigate QoS interference in the cloud using heuristic based decision tree search, however, they only intend to autoscale software control primitives.

Metaheuristic algorithms are also popular for autoscaling decision making in the cloud, because they can often efficiently address NP-hard problems with approximated results. The most common algorithms include: Tabu Search [\cite=sla-provision], Genetic Algorithm (GA) [\cite=software-RP-two-loops] [\cite=2014-navie-ANN-GA-2014] [\cite=BRGA-resource] , Particle Swarm Optimisation (PSO) [\cite=software-RP-two-loops]. As an example, Zhu et al. [\cite=sla-provision] formulate the autoscaling decision making as optimise a weighted-sum formulation of response time and cost. To optimise the objectives, the authors apply a hybrid Tabu Search, which, in every iteration, the current matrix is disturbed and a new decision is generated as initial solution of gradient descent. After reaching a particular fixed point, the variation of profit is calculated and the best configuration is returned.

Finally, Pareto relation can explicitly handle multi-objectivity for autoscaling in the cloud without the need to specify weights on the objectives [\cite=2014-eplison-GA-weigh-h-scaling-2014], [\cite=wosp10sla], [\cite=E3-R-extended], [\cite=GA-full-simulation], [\cite=Chen:2015:tsc-pending]. For example, Kateb et al. [\cite=2014-eplison-GA-weigh-h-scaling-2014] formulate the decision making process as Pareto-based multi-objective optimisation, which is solved by Multi-objective Genetic Algorithm (MOGA, e.g., NSGA-II). At each generation, MOGA identifies non-dominating solutions. Crowding distance is used to calculate the distance between an individual and its neighbours. In particular, each generation of the search is evaluated using epsilon dominance which is a relaxed form of the commonly used Pareto-dominance metric. In E3-R [\cite=E3-R-extended], the decision making problem is formulated as Pareto front, where it is resolved by using MOGA. In addition, the approach applies objective reduction technique with an aim to remove the objectives, which are not significantly conflicted with the others, from the decision making process. In this way, the author aims to reduce the overhead while not affecting the quality of decisions.

In conclusion, we discover that around two thirds of the surveyed search based approaches have considered both vertical and horizontal scaling as the final actions in the cloud. Overall, the nature of search-based optimisation permits certain level of reasoning during the decision making, and this presumably provides better assurance on the quality of the decisions before conducting the actual scaling actions. As we can see, there are small amount of the approaches surveyed belong to implicit search, which can be efficient as there is no need for QoS modelling. Nevertheless, the absence of QoS models also mean that they cannot explicitly handle the trade-offs. On the other hand, the other approaches make use of the explicit search, however, majority of those work formulate the problem as single objective or rely on weighted-sum of objectives, and hence their search of possible trade-offs decision tend to be limited in terms of both optimality and diversity. A limited amount of effort has considered Pareto relation, however, none of them have considered well-compromised trade-off, i.e., the decisions that have balanced improvements on the related objectives. Finally, QoS interference is often ignored in autoscaling decision making, even if it is considered, there is no explicit solution for handling the related trade-offs.

Conclusion

In this paper, we described the background and definitions for cloud autoscaling, self-aware and self-adaptive systems in general. Subsequently, we outlined the major requirements for the key logical aspects of autoscaling in the cloud, and discuss the key state-of-the-art developments proposed for each of the logical aspects, from which a taxonomy is provided. We hope that our systematic survey and taxonomy will motivate further research for more intelligent cloud autoscaling and its interaction with the other problems in the cloud.