Topic Modeling over Short Texts by Incorporating Word Embeddings

Introduction

Topic modeling has been proven to be useful for automatic topic discovery from a huge volume of texts. Topic model views texts as a mixture of probabilistic topics, where a topic is represented by a probability distribution over words. Based on the assumption that each text of a collection is modeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [\cite=blei2003latent] [\cite=griffiths2004finding] [\cite=wang2015extended]. With the rapid development of the World Wide Web, short text has been an important information source not only in traditional web site, e.g., web page title, text advertisement, and image caption, but in emerging social media, e.g., tweet, status message, and question in Q&A websites. Compared with long texts, such as news article and academic paper, topic discovery from short texts has the following three challenges: only very limited word co-occurrence information is available, the frequency of words plays a less discriminative role, and the limited contexts make it more difficult to identify the senses of ambiguous words [\cite=quan2015short]. Therefore, LDA cannot work very well on short texts [\cite=yin2014dirichlet] [\cite=cheng2014btm]. Finally, how to extract topics from short texts remains a challenging research problem [\cite=lau2012line] [\cite=wang2015exploring].

Two major heuristic strategies have been adopted to deal with how to discover the latent topics from short texts. One follows the simple assumption that each text is sampled from only one latent topic which is totally unsuited to long texts, but it can be suitable for short texts compared to the complex assumption that each text is modeled over a set of topics [\cite=yan2015probabilistic] [\cite=zhao2011comparing]. Therefore, many models for short texts were proposed based on this simple assumption [\cite=cheng2014btm] [\cite=yin2014dirichlet]. But, the problem of very limited word co-occurrence information in short texts has not been solved yet. The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [\cite=mehrotra2013improving] [\cite=quan2015short] [\cite=weng2010twitterrank]. However, these schemes are heuristic and highly dependent on the data, which is not fit for short texts such as news titles, advertisements or image captions. Figure 1 shows an example to explain the shortcomings of existing short text topic models. We can see s1 and s2 probably include two topics. 'Obama' and 'President' are likely to come from the same topic, and 'NBA' and 'Bulls' are from another topic. The simple assumption that each text is sampled from only one latent topic is unsuited to these texts. And if we directly aggregate the three short texts into two long pseudo-texts, it is very hard to decide how to aggregate these texts since they do not share the same words. But, it is very clear that s1 is more similar to s2 than s3.

To overcome these inherent weaknesses and keep the advantages of both strategies, we propose a novel method, Embedding-based Topic Model (ETM), to discover latent topics from short texts. Our method leverages recent results by word embeddings that obtain vector representations for words[\cite=mikolov2013distributed] [\cite=pennington2014glove]. The authors demonstrated that semantic relationships are often preserved in vector operations on word vectors, e.g., vec(King) - vec(man) + vec(woman) is close to vec(Queen), where vex(x) denotes the vector of word x. This suggests that distances between embedded word vectors are to some degree semantically meaningful. For example, all distances in Figure 1 are computed by word embedding model [\cite=pennington2014glove].

ETM has the following three steps. ETM firstly builds distributed word embeddings from a large corpus, and then aggregates short texts into long pseudo-texts by incorporating the semantic knowledge from word embeddings, thus alleviates the problem of very limited word co-occurrence information in short texts. Finally, ETM discovers latent topics from pseudo-texts based on the complex assumption that each text of a collection is modeled over a set of topics. Gaining insights from [\cite=xie2015incorporating], ETM adopts a Markov Random Field regularized model based on collapsed Gibbs sampling which utilizes word embeddings in a soft and topic-dependent manner to improve the coherence of topic modeling. Within a long pseudo-text, if two words are labeled as similar according to word embedding, a binary potential function is defined to encourage them to share the same latent topic. Through this way, ETM can effectively identify the senses of ambiguous words.

To measure the performance of ETM, we conduct experiments on two real-world short text datasets, Tweet 2011 and Google News. Experiments demonstrate that ETM can discover more prominent and coherent topics than the baselines. When applying the learned topic proportions of texts in clustering task, we also find that ETM can infer significantly better topic distribution than the baselines.

The remainder of the paper is organized as follows. Section 2 discusses related work. Section 3 presents the proposed framework for short text topic modeling. Section 4 reports experimental results on real-world datasets.

Related Work

In this section, we briefly describe the related work from the following two aspects: long text topic modeling and short text topic modeling.

Long Text Topic Modeling

Nigam et al. [\cite=nigam2000text] proposed a mixture of unigrams model based on the assumption that each document is generated by one topic. This simple assumption is often too limited to effectively model a large collection of long texts. The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [\cite=hofmann1999probabilistic] [\cite=blei2003latent] [\cite=griffiths2004finding]. In a sense, the complex assumption captures the possibility that a document may contain multiple topics. Based on this assumption, many topic models such as Probabilistic Latent Semantic Analysis (PLSA) [\cite=hofmann1999probabilistic] and Latent Dirichlet Allocation (LDA) [\cite=blei2003latent] have shown promising results.

In recent years, knowledge-based topic models have been proposed, which ask human users to provide some prior domain knowledge to guide the model to produce better topics instead of purely relying on how often words co-occur in different contexts. For example, Chen and Liu encode the Must-Links (meaning that two words should be in the same topic) and Cannot-Links (meaning that two words should not be in the same topic) between words over the topic-word multinomials [\cite=chen2014mining]. Besides, two recently proposed models, i.e., a quadratic regularized topic model based on semi-collapsed Gibbs sampler [\cite=newman2011improving] and a Markov Random Field regularized Latent Dirichlet Allocation model based on Variational Inference[\cite=xie2015incorporating], share the idea of incorporate the correlation between words. All these models only deal with long texts, and perform poorly on short texts.

Short Text Topic Modeling

The earliest works on short text topic models mainly focused on exploiting external knowledge to enrich the representation of short texts. For instance, Jin et al. [\cite=jin2011transferring] first found the related long texts for each short text, and learned topics over short texts and their related long texts using LDA. Phan et al. [\cite=phan2008learning] learned the topics on another large-scale dataset using a conventional topic model such as PLSA and LDA for short text classification. However, these models are only effective when the additional data are closely related to the original data. Furthermore, finding such additional data may be expensive or even impossible.

As a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [\cite=lin2010pet], content recommendation [\cite=phelan2009using], and influential users prediction [\cite=weng2010twitterrank]. Initially, due to the lack of specific topic models for short texts, some works directly applied long text topic models [\cite=ramage2010characterizing] [\cite=wang2012tm]. Since only very limited word co-occurrence information is available in short texts, some works took advantages of various heuristic ties among short texts to aggregate them into long pseudo-documents before topic inference [\cite=mehrotra2013improving] [\cite=quan2015short]. In a sense, each short text is considered to be generated from a long pseudo-document. The strategy can be regarded as an application of the author-topic model [\cite=steyvers2004probabilistic] to tweets, where each tweet (text) has a single author. For example, some models aggregated all the tweets of a user as a pseudo-text [\cite=weng2010twitterrank]. As these tweets with the same hashtag may come from a topic, Mehrotra et al. [\cite=mehrotra2013improving] aggregated all tweets into a pseudo-text based on hashtags. The other scheme directly aggregates short texts into long pseudo-texts through clustering methods [\cite=quan2015short], in which the clustering method will face this same problem of very limited word co-occurrence information. However, the above approaches cannot be readily applied to more general forms of short texts which provide hardly any such context information.

Recently, some works found that even through the assumption that each text is generated by one topic does not fit long texts, it can work well for short texts [\cite=quan2015short] [\cite=yin2014dirichlet]. Therefore, many topic models adopted this assumption to discover the latent topics in short texts. Zhao et al. [\cite=zhao2011comparing] empirically compared the data with traditional news media, and proposed a Twitter-LDA model by assuming that one tweet is generated from one topic. Yin and Wang [\cite=yin2014dirichlet] also adopted this assumption for topic inference based on Gibbs sampling. However, these models failed to solve the problem of very limited word co-occurrence information in short texts. Therefore, motivated by the results that prior domain knowledge is useful for long text topic models[\cite=newman2011improving] [\cite=xie2015incorporating], we will propose a novel method for short texts by incorporating the external word correlation knowledge provided by word embeddings to improve the quality of topic modeling.

Algorithm

In this section, we discuss our method, Embedding-based Topic Model (ETM), for identifying the key topics underlying a collection of short texts. Our model ETM includes three steps. First, we build distributed word embeddings for the vocabulary of the collection. Second, we aggregate short texts into long pseudo-texts by incorporating the semantic knowledge from word embeddings. We implement K-means using a new metric, Word Mover's Distance (WMD) [\cite=kusnerword], to compute the distance between two short texts. Third, we adopt a Markov Random Field regularized model which utilizes word embeddings in a soft and topic-dependent manner to improve the coherence of topic modeling. The framework of ETM is shown in Figure 2.

Word Embeddings

Mikolov et al. introduced Word2Vec, to learn a vector representation for each word using a shallow neural network architecture that consists of an input layer, a projection layer, and an output layer to predict nearby words [\cite=mikolov2013efficient] [\cite=mikolov2013distributed]. Word2Vec applies a standard technique such as skip-gram on the given corpus. The model avoids non-linear transformations and therefore makes training extremely efficient. This enables learning of embedded word vectors from huge datasets with billions of words, and millions of words in the vocabulary. Word embeddings can capture subtle semantic relationships between words, such as vec(Berlin) - vec(Germany) + vec(France) ≈   vec(Pairs) and vec(Einstein) - vec(scientist) + vec(Picasso) ≈   vec(painter), where vec(x) denotes the vector of word x [\cite=mikolov2013distributed].

Different from Word2Vec that only utilizes local context windows, Pennington et al. later introduced a new global log-bilinear regression model, Glob2Vec, which combines global word-word co-occurrence counts from a corpus, and local context windows based learning similar to Word2Vec to deliver an improved word vector representation.

Short Text Clustering

After obtaining word embeddings of each word, we use the typical cosine distance measure for the distance between words, i.e., for word vector vx and word vector vy, we define the distance

[formula]

Consider a collection of short texts, S = {s1, s2, ..., si, ..., sn}, for a vocabulary of V words, where si represents the ith text. We assume each text is represented as normalized bag-of-words (nBOW) vector, i ∈ [formula] is the vector of i, a V-dimension vector, ri,j=[formula] where ci,j denotes the occurrence times of the jth word of the vocabulary in text si. We can see that a nBOW vector is very sparse as only a few words appear in each text. For example, given three short texts in Figure 1, if we adopt these metrics (e.g., Euclidean distance, Manhattan distance [\cite=krause2012taxicab], Cosine Similarity ) to measure distance between two texts, it is hard to find their difference. Therefore, we introduce a new metric, called the Word Mover's Distance (WMD)[\cite=kusnerword], to compute the distance between texts. WMD computes the minimum cumulative cost that words from one text need to travel to match exactly the words of the other text as the distance of texts, in which the distance bewteen words is computed by word embeddings.

Let i and j be the nBOW representation of si and sj. Each word of i can be allowed to travel to the word of j. Let T [formula] be a flow matrix, where Tu,v represents how much of the weight of word u of i travels to word v of j. To transform all weights of i into j, we guarantee that the entire outgoing flow from vertex u equals to ri,u, namely [formula]. Correspondingly, the amount of incoming flow to vertex v must equal to rj,v, namely, [formula]. At last, we can define the distance of two texts as the minimum cumulative cost required to flow from all words of one text to the other text, namely, [formula]. Given the constraints, the distance between two texts can be solved using the following linear programming,

[formula]

The above optimization is a special case of the Earth Mover's Distance (EMD) [\cite=rubner1998metric] [\cite=wolsey2014integer], a well-known transportation problem for which specialized solvers have been developed [\cite=ling2007efficient] [\cite=pele2009fast]. The best average time complexity of solving the WMD problem is O(m3logm), where m is the number of unique words in the text. To speed up the optimization problem, we relax the WMD optimization problem and remove one of the two constraints. Consequently, the optimization becomes,

[formula]

The optimal solution is the probability of each word in one text is moved to the most similar word in the other text. The time complexity of WMD can be reduced to O(mlogm).

Once the distance between texts have been computed, we aggregate short texts into long pseudo-texts based on K-means clustering. Given the number of long pseudo-texts L, we compute a score for each short text by averaging the distance between this text and all short texts of each long pseudo-text, that is,

[formula]

Where d(si,su) is the distance between text si and su, |lj| represents the number of short texts in pseudo-text lj. In each iteration, for each short text, we choose the smallest score as its long pseudo-text. After a few iterations, we can obtain long pseudo-texts for all short texts.

Topic Inference

In this subsection, we present how to infer the topics from the long pseudo-texts using Markov Random Field Regularized (MRF) Model and parameter estimation based on collapsed Gibbs sampling.

Model Descritpion

We adopt the MRF model to learn the latent topics which can incorporate word distances into topic modeling for encouraging words labeled similarly to share the same topic assignment [\cite=xie2015incorporating]. Here, we continue to use word embeddings to compute the distance between words. We can see from Figure 3, MRF model extends the standard LDA model [\cite=blei2003latent] by imposing a Markov Random Field on the latent topic layer.

Suppose the corpus contains K topics and long pseudo-texts with L texts over V unique words in the vocabulary. Following the standard LDA, Φ is represented by a K  ×  V matrix where the kth row φk represents the distribution of words in topic k, Θ is represented by a L  ×  K where the lth row θl represents the topic distribution for the lth long pseudo-texts, α and β are hyperparameters, zli denotes the topic identities assigned to the ith word in the lth long pseudo-text.

The key idea is that if the distance between two words in one pseudo-text is smaller than a threshold, they are more likely to belong to the same topic. For example, in Figure 1, 'President' and 'Obama' ('Bulls' and 'NBA') are likely to belong to the same topic. Based on this idea, MRF model defines a Markov Random Field over the latent topic. Given a long pseudo-text l consisting of l words {wli}nli = 1. If the distance between any word pair (wli,wlj) in l is smaller than a threshold, MRF model creates an undirected edge between their topic assignments (zli,zlj). Finally, MRF creates an undirected graph l for the lth pseudo-text, where nodes are latent topic assignments {zli}nli = 1 and edges connect the topic assignments of correlated words. For example, in Figure 3, l is consisted of five nodes (zl1,zl2,zl3,zl4,zl5) and five edges {(zl1,zl2,), (zl1,zl3,), (zl2,zl4,), (zl2,zl5,), (zl3,zl5)} .

The same to LDA, MRF model uses the unary potential for zli as p(zli|θl). The difference is MRF model defines binary potential over each edge (zli,zlj) of l as exp {I(zli = zlj)}, which produces a large value if the two topic assignments are the same and generates a small value if the two topic assignments are different, where I(  ·  ) is the indicator function. Hence, similar words in one pseudo-text have a high probability to be put into the same topic. The joint probability of all topic assignments l  =  {zli}nli = 1 in MRF model can be calculated as

[formula]

where Pl represents all edges of l and |Pl| is the number of all edges in l. Here, λ is a user-specified parameter that controls the tradeoff between unary potential and binary potential. If λ=0, MRF model is reduced to LDA. Different from LDA that topic label zli is determined by topic distribution θl, zli in MRF model depends on both θl and the topic assignments of similar words in the lth pseudo-text.

Formally, the generative process of MRF model is described as follows.

1) Draw Θ  ~   Dirichlet(α)

2) For each topic k∈[1,K]

a) draw φk  ~   Dirichlet(β)

3) For each pseudo-text l in long pseudo-texts

a) draw topic assignments l for all words in pseudo-text l using Eq.(5)

b) draw wli  ~   Multinomial(φzli) for each word in lth pseudo-text

There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [\cite=hofmann1999probabilistic], to approximate inference methods like Variational Inference [\cite=blei2003latent] and Gibbs sampling [\cite=griffiths2004finding]. Variational Inference tends to approximate some of the parameters, such as Φ and Θ, not explicitly estimate them, may face the problem of local optimum. Therefore, different from this paper [\cite=xie2015incorporating] based on Variational Inference, we will use collapsed Gibbs sampling to estimate parameters under Dirichlet priors in this paper.

These parameters that need to be estimated include the topic assignments of z, the multinomial distribution parameters Φ and Θ. Using the technique of collapsed Gibbs sampling, we only need to sample the topic assignments of z by integrating out φ and θ according to the following condition distribution:

[formula]

where zli denotes the topic assignment for word wli in the lth pseudo-text, l, - li denotes the topic assignments for all words except wli in the lth pseudo-text, nkl, - li is the number of times assigned to topic k excluding wli in the lth pseudo-text, nwlik, - li is the number of times word wli assigned to topic k excluding wli, nk, - li is the number of occurrences of all words V that belongs to topic k excluding wli, Nli denotes the words that are labeled to be similar to wi in the lth pseudo-text, and |Nli| is the number of words in Nli.

Using the counts of the topic assignments of long pseudo-texts, we can estimate the topic-word distribution of φ and text-topic distribution θ as follows,

[formula]

[formula]

where nwk is the number of times word w assigned to topic k, nl,k is the number of times word wli assigned to topic k in the lth pseudo-text, and nl is the number of words in the lth pseudo-text.

For each short text s of long pseudo-texts, we can obtain its topic assignments as follows:

[formula]

where ns is the number of words in short text s, wj is the jth word of s.

Parameter Estimation

There are three types of variables (z, Φ and Θ) to be estimated for our model ETM. For the lth pseudo-text, the joint distribution of all known and hidden variables is given by the hyperparameters:

[formula]

We can obtain the likelihood of the lth pseudo-text l of the joint event of all words by integrating out φ and θ and summing over zli.

[formula]

Finally, the likelihood of all pseudo-texts   =  {l}Ll = 1 is determined by the product of the likelihood of the independent pseudo-texts:

[formula]

We try to formally derive the conditional distribution p(zli = k|l, - li,l, - li) used in our ETM algorithm as follows.

[formula]

From the graphical model of ETM, we can see

[formula]

The same to LDA, the target distribution p(|,β) is obtained by integrating over φ,

[formula]

where n(w)zli is the number of word w occurring in topic zli. Here, we adopt the Δ function in Heinrich (2009), and we can have [formula] and [formula], where Γ denotes the gamma function.

According to Equation (5), we can get

[formula]

Similarly, p(l|α,λ) can be obtained by integrating out Θ as

[formula]

where p(Θ|α) is a Dirichlet distribution, and l  =  {n(k)l}Kk = 1.

Finally, we put the joint distribution p(,|α,β,λ) into Equation (13), the conditional distribution in Equation (6) can be derived

[formula]

Experiments

In this section, we show the experimental results to demonstrate the effectiveness of our model by comparing it with five baselines on two real-world datasets.

Datasets Description and Setup

Datasets: We study the empirical performance of ETM on two short text datasets.

Tweet2011: Tweet2011 collection is a standard short text collection published on TREC 2011 microblog track, which includes approximately 16 million tweets sampled between January 23rd and February 8th, 2011.

GoogleNews: Similar to existing papers [\cite=yin2014dirichlet], we utilize Google news as a dataset to evaluate the performance of topic models. On Google news dataset, all news articles are grouped into clusters automatically. We took a snapshot of the Google news on April 27, 2015, and crawled the titles of 6,974 news articles belonging to 134 categories.

For each dataset, we conduct the following preprocessing: (1) Convert letters into lowercase; (2) Remove non-latin characters and stop words; (3) Remove words whose length are smaller than 3 or larger than 20; (4) Remove words with frequency less than 3.

Comparison Methods: We compare our model ETM with the following baselines:

Three short text topic models, Unigrams [\cite=nigam2000text], DMM [\cite=yin2014dirichlet], and BTM [\cite=cheng2014btm]. Unigrams and DMM use the simple assumption that each text is sampled from only one latent topic. BTM learns topics by directly modeling the generation of word co-occurrence patterns in the corpus.

Two Long text topic models, LDA [\cite=griffiths2004finding] and MRF-LDA [\cite=xie2015incorporating]. LDA is the most widely used topic model. MRF-LDA is one novel model designed to incorporate word knowledge into topic modeling.

For LDA, we use this package and the code for Unigrams, which are provided online. For BTM and MRF-LDA, we use the tools released by the authors. For DMM, we implement its code since the authors did not release the code.

Word Embeddings: Word2Vec [\cite=mikolov2013distributed] and Glob2Vec [\cite=pennington2014glove] are different word embeddings. As Glob2Vec has better performance than Word2Vec [\cite=pennington2014glove], the pre-trained embeddings by Glob2Vec based on Wikipedia is incorporated into our model and MRF-LDA.

Parameter Settings: For the baselines, we chooses the parameters according to their original papers. For LDA, Unigrams and BTM, both hyperparameters α and β are set to 50/K and 0.01. For DMM and ETM, both hyperparameters α and β are set to 0.1. For MRF-LDA, α=0.5 and λ=1. For ETM, λ is set to 1. For our model and MRF-LDA, words pairs with distance lower than 0.4 are labeled as correlated. The number of pseudo-texts is set as n/50, where n is the number of all short texts in the corpus.

Experimental Results

Qualitative Evaluation

On Tweet2011 dataset, there is no category information for each tweet. Manual labeling might be difficult due to the incomplete and informal content of tweets. Fortunately, some tweets are labeled by their authors with hashtags in the form of '#keyword' or '@keyword'. We manually choose 10 frequent hashtags as labels and collect documents with their hashtags. These hashtags are 'NBA', 'NASA', 'Art', 'Apple', 'Barackobama', 'Worldprayr', 'Starbucks', 'Job', 'Travel', 'Oscars', respectively.

Table 1 shows some topics learned by the six models on the Tweet2011 dataset. Each topic is visualized by the top ten words. Words that are noisy and lack of representativeness are highlighted in bold. These four topics are about 'NBA', 'NASA', 'Art' and 'Apple', respectively. From Tabel 1, our model ETM can learn more coherent topics with fewer noisy and meaningless words than all baseline models. Long text topic models (LDA and MRF-LDA) that model each text as a mixture of topics does not fit for short texts, as short text suffers from the sparsity of word co-occurrence patterns. Because short text only consists of a few words, MRF-LDA incorporating word correlation knowledge cannot improve the coherence of topic modeling. Consequently, noise words such as better, great, good, today which cannot effectively represent a topic due to their high frequency. Compared to long text, short text probably contains only one topic. Therefore, short text topic models (Unigrams and DMM) adopt a simple assumption that each text is generated by one topic work well on short texts compared to long text topic models. Similar to Unigrams and DMM, BTM posits that unordered word-pair co-occurring in a short text share the same topic drawn from a mixture of topics that can help improve the coherence of topic modeling. But, the existing short text topic models suffer from two problems. On one hand, the frequency of words in short text plays a less discriminative role than long text, making it hard to infer which words are more correlated in each text. On the other hand, these models bring in little additional word co-occurrence information and cannot alleviate the sparsity problem. As a consequence, the topics extracted from these three short text topic models are not satisfying. For example, Topic 3 learned by Unigrams contains less relevant words such as blog, good, and check. The Apple topic (Topic 4) by DMM and BTM consists of meaning-less words such as time, good, etc.

Our method ETM incorporates the word correlation knowledge provided by words embedding over the latent topic to cluster short texts to generate long pseudo-text. In this condition, the frequency of words in pseudo-text plays an important role to discover the topics based on this assumption each text is modeled as a mixture of topics. Simultaneously, our model ETM uses the word correlation knowledge over the latent topic to encourage correlated words to share the same topic label. Hence, although similar words may not have high co-occurrence in the corpus, they remain have a high probability to be put into the same topic. Consequently, from Table 1 we can see that the topics learned by our model are far better than those learned by the baselines. The learned topics have high coherence and contain fewer noisy and irrelevant words. Our model also can recognize the topic words that only have a few occurrences in the collection. For instance, the word flight from Topic 2, writer from topic 3, and tablet can only be recognized by our model ETM.

Table 2 shows some topics leaned from GoogleNews dataset. The four topics are events on April 27, 2015, which are "Nepal earthquake", "Iran nuclear", "Indonesia Bali", and "Yemen airstrikes". From this table, we observe that the topic learned by our method are not only better in coherence than those learned from long text topic models (LDA and MRF-LDA), but better than short text topic models (Unigrams, DMM, and BTM), which again demonstrates the effectiveness of our model.

Quantitative Evaluation

Similar to [\cite=xie2013integrating] [\cite=xie2015incorporating], we also evaluate our model in a quantitative manner based on the coherence measure (CM) to assess how coherent the learned topics are. For each topic, we choose the top 10 candidate words and ask human annotators to judge whether they are relevant to the corresponding topic. First, annotators need to judge whether a topic is interpretable or not. If not, the 10 words of the topic are labeled as irrelevant, or which words are identified by annotators as relevant words for this topic. Coherence measure (CM) is defined as the ratio of the number of relevant words to the total number of candidate words. In our experiments, four graduate students participated the labeling. For each dataset, we choose 10 topics for labeling.

Table 3 and Table 4 show the coherence measure of topics inferred on Tweet2011 and GoogleNews datasets, respectively. We can see our model ETM significantly outperforms the baseline models. On Tweet2011 dataset, ETM achieves an average coherence measure of 72.5%, which is larger than long text topic models (LDA and MRF-LDA) with a large margin. Compared to short text topic models, ETM still has a big improvement. In GoogleNews dataset, our model is also much better than the baselines. In conclusion, our model produces better results on both datasets compared to the baselines, which demonstrate the effectiveness of our model in exploring word correlation knowledge from words embeddings to improve the quality of topic modeling.

Short Text Clustering

We further compare the performance of all models in clustering on Tweet2011 and GoogleNews datasets. To provide alternative metrics, the normalized mutual information (NMI) is used to evaluate the quality of a clustering solution [\cite=huang2013dirichlet] [\cite=yin2014dirichlet]. NMI is an external clustering validation metric that effectively measures the amount of statistical information shared by the random variables representing the cluster assignments and the user-labeled class assignments of the data points. NMI value is always a number between 0 and 1, where 1 represents the best result and 0 means a random text partitioning. We run each model 20 times on each dataset and report the mean and standard deviation of their NMI values.

Table 5 shows the performance of all models on the two datasets. First, we can see that ETM performs significantly better than long text topic models (LDA and MRF-LDA). This is because long text topic models do not consider the problem that only very limited word co-occurrence information is available in short texts. Second, we can find that ETM outperforms short text topic models (Unigrams, DMM, and BTM). This is because topic models lack the mechanism to incorporate word correlation knowledge and generate the words independently. Therefore, we can conclude that our model fits for short texts compared to the baselines. Meanwhile, from the standard deviation of all results, we can see that the standard deviation of ETM is smaller than all other methods. This demonstrates that our assumption for short texts can reasonably simulate the generative process.

Conclusion

We propose a novel model, Embedding-based Topic Modeling (ETM), to discover the latent topics from short texts. ETM first aggregates short texts into long pseudo-texts by incorporating the semantic knowledge from word embeddings, then infers topics from long pseudo-texts using Markov Random Field regularized model, which encourages words labeled as similar to share the same topic assignment. Therefore, by incorporating the semantic knowledge ETM can alleviate the problem of very limited word co-occurrence information in short texts . Experimental results on two real-world short datasets corroborate its effectiveness both qualitatively and quantitatively over the state-of-the-art methods.