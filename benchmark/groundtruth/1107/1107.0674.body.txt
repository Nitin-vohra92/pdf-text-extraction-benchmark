"Memory foam" approach to unsupervised learning

The tasks being posed to, and solved by, the modern artificial "intelligent" (AI) devices are broad and include image and speech recognition, machine vision, language processing and medical diagnostics, to mention just a few [\cite=Arbib]. However, in spite of the word "intelligence" behind the AI abbreviation, in essence, these machines are only able to perform two tasks: classification and optimization, which include decision-making. Learning has been understood merely as acquiring the ability to perform these tasks.

The performance of modern AI devices is based on algorithms, i.e. while fulfilling their goal they perform a sequence of pre-defined commands. Even the later generation of AI devices, that are based on neural networks, employ algorithms at least at the stage of learning [\cite=Hertz]. Contrary to that, it seems that a biological brain does not naturally execute a sequence of commands, although it can be trained to do so (often with some effort, e.g. when solving routine mathematical problems). In particular, the brain does not seem to learn by an algorithm.

As can be expected from algorithm-based devices, the natural way of learning generally requires a teacher - i.e. a truly intelligent system - and can be fully supervised, semi-supervised [\cite=Semi-Supervised] or reinforcement [\cite=Dayan_reinforce]. The unsupervised learning defined within the AI field, is acquiring the ability to attribute a new entry to a certain class without any help from a teacher [\cite=Dayan_unsupervised].

In this Letter we propose an alternative approach to describe a learning process. Namely, we suggest that a thinking system should work as a machine, that adjusts its architecture in response both to sensory input, and to the processes inside itself in an analogue (i.e. non-algorithmic) way. We introduce a mathematical prototype of this machine - a dynamical system, that shapes its vector field in response to the external stimulus - i.e. we describe the first component of the thinking process. The model does not rely on any biological knowledge.

Let every (scalar or vector) value of the input at the given time moment represent a certain pattern, that can be of any origin: visual, auditory, tactile, olfactory, or their combination. It could be the color of the image, the pitch of the sound, etc. The implementation of a non-algorithmic classification (pattern recognition) was proposed in [\cite=Hopfield_attractors_86] by means of neural networks (NNs) - a collection of units, each with fixed architecture, which are flexibly coupled to each other. However, learning in a NN is algorithmic and consists in adjusting the strengths of couplings ("weights") in response to a training set of patterns. As a result, an energy profile is formed in the phase space of the NN [\cite=comment1], whose minima (attracting fixed points) represent the centres of classes, and the respective basins of attraction represent classes. When learning is over, the weights are fixed, the new input patterns are given by initial conditions, and classification occurs non-algorithmically as the NN evolves towards the nearest attractor [\cite=Hertz]. A series of technical problems can occur as a NN learns, including the formation of spurious attractors. Also, the most natural way of learning for a NN is supervised, while semi- or unsupervised learning require considerable complication of the algorithms.

Here, we propose the construction of a dynamical system, whose vector field is the gradient of the potential energy, which is shaped by the external stimulus non-algorithmically and without supervision. If the stimulus comes from a stationary and ergodic random process, this "energy" represents a negative multi-dimensional probability density distribution of the input, and each stable fixed point represents the most probable pattern from the input class. The system recognizes the new patterns just like a particle that is placed into a potential energy profile V(x), which moves towards the nearest minimum, possibly being affected by noise, according to [\cite=Malakhov_97]

[formula]

where x represents the location in N-dimensional space, and ξ(t) is noise.

Model. It is based on a loose analogy with the "memory foam", used in orthopedic mattresses, that takes the shape of the body pressed against it, but slowly returns to its original shape after the pressure is removed. Assume that initially we have a one-dimensional "foam" stretched in x direction, and that initially it is flat, i.e. its profile is U(x)  =  0 (Fig. [\ref=fig_foam_ill], t  =  0). If a stone drops onto the foam at position x  =  η, the foam profile is deformed: a dent appears, which is the deepest exactly at x  =  η, and gets shallower at larger distances from η (Fig. [\ref=fig_foam_ill], t  =  1). Also, assume that the foam is elastic with elasticity factor k, that models the capacity to forget. The deeper the dent at the position x is, the faster the foam tries to come back to U  =  0 (to forget). In other words, the foam will learn about the stone and its position. Now assume that we subject the foam to an external stimulus η(t), as if at any new time moment t a new stone drops at a new position x  =  η(t) (Fig. [\ref=fig_foam_ill], t  =  2), thus shaping the "foam" continuously. The signal η(t) can be of either deterministic, or stochastic nature, and can have arbitrary statistical properties. Next we derive an equation, that describes the evolution of the foam profile U(x,t) under the influence of η(t).

Consider how the foam profile changes over a small, but finite time interval Δt:

[formula]

where g(z) is some non-negative bell-shaped function, describing the shape of a single dent, e.g. a Gaussian function, [formula]. The natural initial conditions would be U(x,0)  =  0; however, as will be shown below, the limiting shape of the foam does not depend on the initial conditions if η(t) is ergodic and k  =  0.

In ([\ref=eq1]) move U(x,t) to the left-hand side, divide both parts of by Δt, and take the limit as Δt  →  0, to obtain

[formula]

It can be shown by numerical simulation with some arbitrary η(t), that the solution U(x,t) has a linear trend, i.e. it behaves as a linearly decaying function of t with superimposed fluctuations. We wish to eliminate this trend and see if we can achieve some sort of stationary behavior of U(x,t). Perform the change of variables

[formula]

and rewrite ([\ref=eq2]) as follows

[formula]

Evolution of the foam profile V(x,t) is illustrated in Fig. [\ref=fig_uncor_cor]: (a) in 3D, and (b) in its projection on the (x,t) plane, as the signal shown by filled circles in (b) is applied at each consecutive time moment t. Eq. ([\ref=eq3]) has the same form if the stimulus η is a vector of dimension N; then x is a vector, and V and g are functions of N variables.

Proof of shaping into the input density. Consider the evolution of V(x,t), where the N-dimensional input vector η(t) is a realization of a strict-sense stationary and ergodic random process H(t) with some arbitrary probability density distribution (PDD) [formula]. Due to stationarity, pHN does not change in time; due to ergodicity, any single realization η(t) contains all information about pHN, i.e. any statistical characteristic can be obtained from η(t) by averaging over time, rather than over the ensemble of realizations that would have been required for a non-ergodic process [\cite=Stratonovich_vol1]. Below we will show that with time, V takes the shape of pHN.

Assume that k  =  0, i.e. that the foam does not forget what it learnt. Multiply both parts of Eq. ([\ref=eq3]) by [formula] and integrate. A stationary behavior of V implies

[formula]

Consider the integral of the r.h.s. of Eq. ([\ref=eq3]) and its limit as t  →    ∞

[formula]

representing the (negative) time average 〈V  +  g(x  -  η)〉 of the expression under the integral. The term g(x  -  H) is a non-linear smooth function of an ergodic process H. As proved in [\cite=Wolf_ergodic_67], "zero-memory nonlinear operations on ergodic processes are ergodic" - therefore, g(x  -  H) is also an ergodic random process. Thus we can replace time average ([\ref=eq_time]) by statistical average,

[formula]

In the above, the integral with respect to η represents, for brevity, N integrals with respect to the components [formula] of vector η. Since V does not depend on η explicitly, the first term in the right-hand side of ([\ref=eq5]) is equal to V. The second term is the convolution of pHN(η) with the function g(η). If g(x  -  η)  =  δ(x  -  η), where δ(z) is Dirac delta-function of several variables, this term is equal to minus pHN(x), due to the sifting property of delta-function [\cite=Bracewell_86]. From ([\ref=eq3]) combined with ([\ref=eq4]) it follows that the expression ([\ref=eq5]) is equal to 0. We therefore proved that as time t goes to infinity, V(x,t) tends to -  pHN(x), provided that g(z) tends to Dirac delta-function.

In Fig. [\ref=fig_uncor_cor] the evolution of V(x,t) is illustrated, as two kinds of scalar stimuli are applied to the one-dimensional foam. Their PDDs are of similar two-peak shape (see solid lines at the front in (a,c)), but two consecutive values are non-correlated in (a,b), and correlated in (c,d) [\cite=comment2]. The actual signals applied are shown by filled circles in (b,d), and in g(z) we used [formula]. One can see that eventually both foams shape into the respective PDDs, but if the stimulus values are uncorrelated, the convergence is faster.

This shaping mechanism reminds one of kernel density estimation used in statistics [\cite=Scott_kernel_92], but is dynamical as opposed to algorithmic, and has no restriction of independent inputs to the system. If H(t) is not stationary, the foam evolves into a time-averaged density of the input.

Application to musical data. Next, we illustrate how the proposed foam discovers and memorises musical notes and phrases. A children's song "Mary had a little lamb" was performed with a flute by an amateur musician six times. The song involves three musical notes (A, B and G), consists of 32 beats and was chosen for its simplicity to illustrate the principle. The signal was recorded as a wave-file with sampling rate 8kHz. In agreement with what is usually done in speech recognition [\cite=Flanagan_77], the short-time Fourier Transform was applied [\cite=Allen_SFT] to the waveform with a sliding window of duration τ  =  0.75 sec, which was roughly the duration of each note. The highest spectral peak was extracted for each window, which corresponded to the main frequency f Hz of the given note. A sequence of frequencies f(t) was used to stimulate the foam. Note, that each value of f(t) was slightly different from the exact frequency of the respective note, because of the natural variability introduced by a human musician, and the signal f(t) was in fact random, as seen from Fig. [\ref=fig_flute_1d](b).

First, we illustrate how individual musical notes can be automatically identified. A one-dimensional foam received the signal η(t)  =  f(t), resampled to 8Hz to save computation time. Function f(t) can be seen as a realization of a 1st-order stationary and ergodic process F(t), consisting of infinitely many repetitions of the same song, which we observe during finite time. This process has a one-dimensional PDD pF1(f), which does not change in time. Gaussian kernel g(z) was used with [formula] Hz. As shown in Fig. [\ref=fig_flute_1d](a), the foam converges to some PDD shown by solid line. It automatically discovers the most probable frequencies as follows, figures in brackets showing the exact frequencies of the respective musical notes: 434Hz (440Hz) for A4, 490Hz (493.88Hz) for B4, and 388Hz (392Hz) for G4.

Second, we show how the foam can discover and memorize temporal patterns - musical phrases consisting of four beats. The 4D foam was used, and to each of its channels the same signal f(t) was applied, but with a phase shift. Namely, at each time t the foam received a vector stimulus ψ(t)  =  (f(t),f(t + τ),f(t + 2τ),f(t + 3τ)), τ  =  0.75 sec. For the purpose of this part, we can regard ψ(t) as a realization of a 4th-order stationary and ergodic vector random process Ψ(t) (which we observe during finite time) with 4D PDD pΨ4(f1,f2,f3,f4). We used a multivariate Gaussian kernel g with [formula] Hz in all of its four variables.

One cannot visualize evolution of a 4D foam in the same way as we did in Figs. [\ref=fig_uncor_cor]-[\ref=fig_flute_1d], and we use an alternative representation. We take four half-axes and make their origins coincide (Fig. [\ref=fig_flute_4d](a)). For each feasible input ψ  =  (f1,f2,f3,f4) we put 4 points with coordinates fi on each of half-axes, and connect them by lines. Thus, any feasible input pattern is represented by a polygon on a plane. (This can be done for any dimension of input vector.) The value of pΨ4 at each point can be represented by the color of the respective polygon (Fig. [\ref=fig_flute_4d](b)). The polygon, whose color is the darkest, is the most probable pattern. Unfortunately, when too many polygons overlap, it might be difficult to see the darkest ones. But they can be found using a particle in the 4D foam, that will go to the most probable pattern: five such patterns are given in smaller scale in Fig. [\ref=fig_flute_4d](c).

Recognition of musical phrases is also illustrated by the supplemented wave-files [\cite=URL].

Discussion. The memory foam approach presented here might pave the way to create a new generation of information processing machines. Unlike both digital computers and neural networks, these devices will be fully analogue and in this sense closer to biological brains. The proposed approach assumes naturally unsupervised learning, which is traditionally more challenging than other types of learning; however, supervision can be implemented at any stage, if required. Also, the "memory foam" can combine learning with pattern recognition, i.e. function in the "on-line learning" regime. The importance of being able to create hierarchies of patterns in AI devices cannot be overestimated (see e.g. [\cite=Hawkins_Intelligence]). With a musical example we demonstrated how hierarchies of patterns can be created in a dynamical way, by going from single notes to their combinations.

A famous major problem, arising in connection with AI performance, is the so-called "curse of dimensionality". As the problem becomes more complicated, the number of states of a traditional AI device grows very quickly, and becomes too large for the computer memory, or the connectivity of artificial NNs. The "curse" can be worked around [\cite=Powell], but there is always a price (e.g. the duration of calculations). The "memory foam" device would not require connectivity similar to that in NNs, and might provide a solution to the "curse" problem.

Acknowledgements. The authors are grateful to Alexander Balanov for a number of helpful critical comments on the draft of this paper, and to Victoria Marsh for playing the flute.