Lemma Corollary Proposition Definition Remark

Factorization of Matrices of Quaternions

The quaternionic condition

It is possible to prove many factorization results for matrices of quaterions by deriving them from their familiar complex counterparts. The amount of additional work is surprisingly small.

There are more abstract factorization theorems that apply to real C*-algebras, along the lines of the delightful paper by Pedersen on factorization in (complex) C*-algebras. We are dealing here with more basic questions, involving only finite-dimensional linear algebra. These are never (hardly ever?) addressed in basis linear algebra texts, creating the impression that linear algebra over the quaternions is more difficult than it really is.

There are serious hazards within linear algebra over the quaternions. One can be lured to difficult questions of determinants and handedness of the spectrum, leaving perhaps victorious, but with the impression that all of linear algebra over the quaternions is going to be difficult.

We mathematicians are well-advised, when upon such uneven ground, to seek guidance from physicists. Such guidance helped select the topics, and helped suggest notation.

One topic was included for reasons of elegance, the Jordan canonical form. For practical purposes, the Schur decomposition will generally suffice. We prove that as well.

It is assumed the reader is familiar with such things as the spectral theorem and the functional calculus for normal matrices. It is not assumed that the reader knows about C*-algebras or physics, but these topics are mentioned incidentally.

Let [formula] denote the algebra of quaternions

[formula]

This is an algebra over [formula]. The canonical embedding [formula] sending 1 to 1 and i to î does not make [formula] into an algebra over [formula]. The trouble is that the embedding is not central. An additional algebraic operation is the involution

[formula]

which satisfies several axioms, including the following.

[formula]

[formula]

[formula]

Turning to matrices, the algebra [formula] has the expected structure of a unital [formula]-algebra, plus the involution

[formula]

of conjugate-transpose. We would like to know

[formula]

and

[formula]

We will quickly prove these after we consider a representation of [formula] on [formula].

We have an obvious representation of [formula] on [formula] and quickly notice that since left and right scalar multiplication of [formula] disagree, we have both left-eigenvalues [formula] solving

[formula]

and right-eigenvalues [formula] solving

[formula]

A glance at the survey [\cite=zhang1997quaternions] reveals that many difficulties arise.

Ponder the situation for real matrices, in [formula]. To the chagrin of undergraduates, it is most natural to consider the representation of [formula] on [formula]. Given a real orthogonal matrix O we get the full picture of why it might not diagonalize in [formula] when we look at the complex eigenvalues. It is at this point that we are implicitly letting [formula] act on [formula].

We do well in the case of the quaternions to regard [formula] as represented on [formula], or in more modern terms that de-emphasizes the role of vectors, via a certain embedding

[formula]

This is a very old trick. For N > 1 it appears to have been noticed first by H. C. Lee [\cite=Lee_JordanFormQuaternions].

Given two complex N-by-N matrices A and B we set

[formula]

We generally study complex matrices in the image of χ and only at the end of our calculations do we draw conclusions about [formula]. This is in keeping with applications in quantum mechanics, where Hilbert space is always complex and time-reversal symmetry will often be incorporated in the conjugate linear operation T. We define [formula] by

[formula]

See [\cite=mehta2004random] for details on when T is relevant to time reversal symmetry.

A less generous description our our approach is we plan to study complex matrices with a certain symmetry that is useful in physics, and then sell the same results to pure mathematicians by re-branding them as theorems about matrices of quaternions.

The operator T is relatively well behaved, despite being only conjugate linear. It preserves orthogonality, and indeed

[formula]

Also

[formula]

which is another one-line calculation.

The mapping χ is well-defined, is an [formula]-algebra homomorphism, is one-to-one and satisfies

[formula]

Notice that every quaternion q can be written as α  +  β with α and β in [formula]. This makes the map well-defined. It is clearly one-to-one and [formula]-linear. Notice β  =   for complex number β and so [formula]. Therefore

[formula]

and

[formula]

We can describe the image of χ in several useful ways. We will need several unary operations on complex matrices. For starters we need the transpose [formula] and the pointwise conjugate [formula] and the conjugate-transpose, or adjoint, [formula]. Finally, we need a twisted transpose that is useful in physics. This is a "generalized involution" [formula] called the dual operation that is defined only for X in [formula].

For A, B, C and D all complex N-by-N matrices, define

[formula]

Alternatively, we set [formula] where

[formula]

Notice that Z could be replaced by a matrix with similar properties and define a variation on the dual operation. Indeed, the choice of Z is not standardized.

For X in [formula] the following are equivalent.

X is in the image of χ;

[formula];

[formula] meaning [formula] for every vector ξ in [formula] .

Assume

[formula]

Then

[formula]

Conversely [formula] translates into block form as

[formula]

so we have proven (1)[formula](2).

We compute [formula] for X again in block form, and find

[formula]

so the matrix for the linear operator [formula] is [formula]. Therefore (2)[formula](3).

Let us call

[formula]

the quaternionic condition and the matrix X we will call a quaternionic matrix.

We now dispose of the implications

[formula]

and

[formula]

for matrices of quaternions. These are true implications for complex matrices, and in particular for quaternionic matrices, and therefore true for matrices of quaternions.

We pause to note some axioms of the dual operation. It behaves a lot like the transpose. It is linear,

[formula]

which is true even for complex α. Here X and Y are any 2N-by-2N complex matrices. The dual reverses multiplication,

[formula]

It undoes itself,

[formula]

and commutes with the adjoint

[formula]

Every matrix G in [formula] can we expressed in a unique way as

[formula]

with X and Y being quaternionic matrices.

Given G we set

[formula]

and

[formula]

Kramers degeneracy and Schur factorization

Eigenvalue doubling is a key feature of self-dual, self-adjoint matrices. In physics this is called the Kramers degeneracy theorem, or the theory of Kramers pairs [\cite=kramers1930theorie] [\cite=wigner1932uber] [\cite=mehta2004random]. This generalizes in two ways, to give eigenvalue doubling given the symmetry [formula] and conjugate-pairing of eigenvalues given the symmetry [formula].

Such a collection of paired eigenvectors will, in good situations, form a unitary matrix. If U is unitary matrix that satisfies the quaternionic condition then it satisfies another symmetry making it symplectic, specifically [formula].

Suppose U is a unitary 2N-by-2N matrix. The following are equivalent:

U is symplectic;

[formula];

[formula];

[formula];

If [formula] is column j of U for j  ≤  N then column N + j of U is [formula].

This follows easily from Lemma [\ref=lem:QuaternionicCondition].

We need to know that the group of symplectic unitary acts transitively on [formula].

If [formula] is any unit vector in [formula] then there is a symplectic unitary with [formula].

Let [formula]. We use ([\ref=eq:TRoperatorPreservesOrtho]) and ([\ref=eq:TR-orthogonality]) to select in order vectors that are an orthonormal basis for [formula], but of the form

[formula]

If we reorder to

[formula]

we have the columns of the desired symplectic unitary.

The most basic result in this realm is an ugly lemma that says that T maps left eigenvectors of X to right eigenvectors of [formula], with the same eigenvalue. The second part of this lemma is more elegant, if less general. It specifices how every quaternionic matrix has a conjugate symmetry it its spectral decomposition.

Suppose X is in [formula].

If Xξ  =  λξ then

[formula]

If [formula] and Xξ  =  λξ then

[formula]

(1) Starting with

[formula]

and

[formula]

we find that Xξ  =  λξ translates to

[formula]

and [formula] translates to

[formula]

so these are equivalent conditions.

(2) follow from (1) by taking adjoints.

Now we are able to extend Kramers degeneracy to a variety of situations, starting with a block diagonalization for commuting quaternionic matrices. Part (2) of Theorem [\ref=thm:QuaternionicSchur] appeared in [\cite=wiegmannMatricesQuaternion].

Suppose [formula] in [formula] commute pairwise and [formula] for all j.

There is a single symplectic unitary U so that, for all j,

[formula]

with Tj upper-triangular and Sj strictly upper-triangular.

If, in addition, the Xj are normal then there is a single symplectic unitary U so that, for all j,

[formula]

with Dj diagonal.

Every symplectic unitary has determinant one.

(1) A finite set of commuting matrices will have a common eigenvector, so let [formula] be a unit vector so that [formula] We know then that so [formula] is an eigenvalue for Xj with eigenvector [formula] There is a symplectic unitary U1 so that [formula] and then [formula] by Lemma [\ref=lem:SymplecticUnitaryStructure]. Let Yj = U*1XjU1. Then

[formula]

and

[formula]

This means the column 1 and column N + 1 are all but zeroed-out,

[formula]

Up to a non-symplectic change of basis we are looking at block-upper triangular matrices that commute, so the lower-right corners in that basis commute. This means that the

[formula]

all commute, and each must satisfy [formula] By induction, we have proven the first claim.

For (2) we modify the proof just a little. Starting with the Xj normal, we find the Yj are also normal and so

[formula]

This, after an appropiate basis change, would be block-diagonal, and from this we can conclude that the matrix in ([\ref=eqn:smallerBlocks]) is normal. The induction proceeds as before, with the stronger conclusion that Bj  =  Cj = 0 and [formula] is diagonal.

(3) Applying (2) we find

[formula]

where D is a diagonal unitary. Therefore

[formula]

and since a unitary has determinant on the unit circle, we are done.

Every matrix in [formula] is unitarily equivalent to a upper-triangular matrix in [formula] that has complex numbers on the diagonal.

There is an algorithm [\cite=bunse1989quaternion] for the Schur decomposition of quaternionic matrices.

Every normal matrix in [formula] is unitarily equivalent to a diagonal matrix in [formula]. Every Hermitian matrix in [formula] is unitarily equivalent to a diagonal matrix in [formula].

Every Hermitian self-dual matrix X in [formula] is of the form

[formula]

for some symplectic unitary U and a diagonal real matrix Dj.

These corollaries cause us to reconsider the concept of left and right eigenvalues in [formula] and focus on just those that are in [formula]. For details on left eigenvalues and non-complex right eigenvalues, consult [\cite=FarenickSpectralThmquatern]. We put the complex right eigenvalues in a simple context with the following.

Suppose [formula] and [formula] and [formula]. Then

[formula]

if and only if

[formula]

Therefore [formula] is a right eigenvalue of [formula] if and only if λ is an eigenvalue of [formula].

This a short, direct calculation.

Jordan canonical form

Kramers degeneracy extends to the generalized eigenvectors used to find the Jordan canonical form.

Suppose [formula]. If

[formula]

then

[formula]

For any Y we have

[formula]

proving

[formula]

Since

[formula]

the result follows.

We see the general idea of a proof [\cite=zhang2001jordan] of the Jordan decomposition for a quaternionic matrix. When we build a Jordan basis we need to respect T in two ways. Whatever basis we pick for the subspace corresponding to λ with positive imaginary part, we apply T to get the basis for the subspace corresponding to [formula]. When λ is real we need to pick generalized eigenvectors in pairs.

The Jordan form of a quaternionic matrix is not so elegant, as each Jordan blocks larger than 2-by-2 gets spread around to all four quadrants of the matrix. We work directly with a Jordan basis and then make our final conclusion in terms of quaternions.

Suppose [formula]. There is a Jordan basis for X consisting of pairs of the form [formula].

Let Nλ denote the subspace of all generalized eigenvectors for λ toghether with the zero vector. Recall that just treating X as a complex matrix, the procedure to select a Jordan basis involves selecting, for each λ, a basis for Nλ with the following property: whenever [formula] is in this basis, then [formula] is either zero or back in this basis.

For λ in the spectrum with positive imaginary part we make such a choice and then apply T to get a set of vectors in [formula] that has the correct number of elements to be a basis of [formula]. It will also be a linearly independent set since Z and conjugation both preserve linear independence. Since

[formula]

this basis of [formula] has the desired property.

For λ in the spectrum that is real we need to modify the procedure for selecting the basis of Nλ. A common procedure selects a basis [formula] for

[formula]

and constructs for Nλ the Jordan basis

[formula]

Since

[formula]

we get the desired structure if the subspaces ([\ref=eq:endOfChains]) are T-invariant. This follows from the next two lemmas and the equality

[formula]

We can now assemble the bases of the various Nλ to get a Jordan basis built from the Kramers pairs.

If [formula] then ker (X) is T-invariant.

This is a restatement of the λ = 0 case of Lemma [\ref=lem:left-right-Lemma](2).

If a subspace H0 is T-invariant then [formula] is also T-invariant.

If [formula] is in [formula] then for every [formula] we have

[formula]

Suppose [formula]. There is an invertible matrix [formula] and a complex matrix J in Jordan Form such that X  =  S- 1JS.

Norms

There are two operator norms to consider on X in [formula], that induced by quaternionic Hilbert space and that induced by complex Hilbert space on [formula]. They end up identical.

Suppose X is in [formula]. Then using the norms

[formula]

on [formula] and

[formula]

on [formula] we have

[formula]

Utilizing also the norm on [formula] we calculate the four relevant norms:

[formula]

[formula]

[formula]

[formula]

The result follows.

Singular value decomposition

As in the complex case, a slick way to prove there is a singular value decomposition is to work out the polar decomposition and then use the spectral theorem on the positive part.

In [\cite=zhang1997quaternions] it is stated that "a little more work is needed for the singular case" when discussing the polar decomposition. The extra work involves padding out a quaternionic partial isometry to be a quaternionic unitary (so symplectic unitary.)

We remind the reader that U is a partial isometry when (U*U)2  =  U*U, or equivalently UU*U  =  U or U*UU*  =  U* or (UU*)2  =  UU*. If we restrict the domain and range of U we find it is an isometry from [formula] to [formula].

Suppose [formula]and U is a partial isometry in [formula]. There is a symplectic unitary W in [formula] so that Wξ = Uξ for all [formula].

If [formula] is in ker (U) then by Lemma [\ref=lem:left-right-Lemma], [formula] is also in ker (U). Since [formula] and [formula] are orthogonal, we can show that ker (U) has even dimension 2m and that is has a basis for the form

[formula]

We are working in finite dimensions so the dimension of ker (U*) is also 2m and we select for it a basis

[formula]

We can define W to agree with U on [formula] and to send [formula] to [formula] and [formula] to [formula] and so get a unitary that commutes with T, which means it is symplectic.

Suppose [formula] in [formula]. Then there is a unitary U and a positive semidefinite P with [formula]and [formula] and X = UP.

Let f be a continuous function on the positive reals with f(0) = 0 and [formula] for every nonzero eigenvalue of X*X. Then let

[formula]

The usual calculations in functional calculus tell us W is a partial isometry and that X = WP for [formula]. Working with monomials, polynomials and then taking limits, we can show

[formula]

for any positive operator Y and so

[formula]

Also [formula].

We just showed that the matrices in the minimal polar decomposition are quaternionic. We use Lemma [\ref=lem:IsometriesExtendQuaterionically] to finish the argument.

As expected, the polar decomposition leads to a singular value decomposition.

Suppose [formula] in [formula]. There are symplectic unitary matrices U and V and a diagonal matrix D with nonnegative real entries and [formula] so that X = UDV.

We take a quaternionic polar decomposition X = WP. Since P is positive, we apply Theorem [\ref=thm:QuaternionicSchur] to get symplectic unitary matrices Q and V and diagonal matrix D so that P = QDV. The eigenvalues of P are nonnegative, so the same is true for the diagonal elements of D and we have the needed factorization X = (WQ)DV.

QR factorization

It is easy to use Lemma [\ref=lem:transitivity] to get over [formula] a QR factorization theorem. Notice that upper triangular matrices are sent by χ to matrices that are block upper-triangular.

If [formula] in [formula] the there is a symplectic unitary Q and R of the form

[formula]

with A and B upper triangular. If [formula] then there is a untiary Q and upper triangular matrix R in [formula] so that X = QR.

(2) follows directly from (1), so we prove (1).

We apply Lemma  [\ref=lem:transitivity] to the first column of X and find X = Q1R1 where

[formula]

where A1 and B1 have zeros in their first columns, except perhaps in the top position. As we did earlier, we can proceed with a proof by induction.

Self-dual matrices

If we study matrices with [formula] then we are no longer working directly with quaternionic matrices, but as we discuss below, there is a connection. We discuss a Schur factorization and a structured polar decomposition for self-dual matrices. The latter is a bit tricky, so we warm up with a structured polar decomposition for symmetric complex matrices.

For dealing with a single self-dual matrix, there is the efficient Paige / Van Loan algorithm [\cite=HastLorTheoryPractice] [\cite=paigeVanLoan] to implement the following theorem.

Given [formula] in [formula] that commute pairwise and are self-dual, there is a single symplectic unitary U so that for all j,

[formula]

with Tj upper-triangular and the Cj skew-symmetric.

Let [formula] be a nonzero unit vector so that [formula] for all j. By Lemma [\ref=lem:left-right-Lemma],

[formula]

There is a symplectic unitary U1 so that [formula] and [formula] Let Yj = U*1XjU1. Then

[formula]

and

[formula]

Since [formula] is real, we take adjoint and discover

[formula]

This means the column 1 and row N + 1 are all but zeroed-out,

[formula]

Basic facts about block triangular matrices show that the

[formula]

are a commuting family of matrices, and since U1 was chosen to be symplectic, the Zj will be self-dual. As simple induction now finishes the proof.

A promising numerical technique for the joint diagonalization of two commuting self-dual self-adjoint matrices H and K would be to form the normal self-dual matrix X = A + iB and apply Paige / Van Loan to reduce to block diagonal form. Then apply ordinary Schur decomposition. This technique was used in [\cite=HastLorTheoryPractice] to diagonalize matrices that were exactly self-dual and approximately unitary. This idea is mentioned in [\cite=bunse1993numerical], section 6.5.

It is not hard to show that the minimal polar decomposition, the one that is unique and can involve a partial isometry, preserves in some way just about any symmetry thrown at it. This is because the functional calculus interacts well with the dual operation [\cite=LoringSorensenRealLins], as well as with the transpose. It is a bit harder to figure what happens for the maximal polar decomposition. By the minimal polar decomposition is meant the factorization that is unique and can involve a partial isometry. By the maximal polar decomposition is meant the factorization that involves a unitary.

We begin with the easier result about the polar decomposition of complex symmetric matrices.

If X in [formula] satisfies [formula] then there is a unitary U so that [formula] and [formula].

Again chose f with f(0) = 0 and [formula] for every nonzero eigenvalue of X*X and let

[formula]

As always, W is a partial isometry and X = WP for [formula]. Now we discover

[formula]

To create a unitary U with [formula] we must extend W to map ker (W) to [formula]. We can arrange [formula] as follows. Let [formula] be an orthonormal basis of ker (W). Then [formula] will be an orthonormal basis of [formula] and we define V to be zero on [formula] and [formula] Thus V* will be zero on [formula] and [formula], but the same can be said about [formula]. That means [formula] and so U = W + V will be the required symmetric unitary.

Notice there is no structure on [formula], but considered with [formula] we get the formula

[formula]

For the self-dual situation, we shall see that a similar construction works so long as we respect Kramers degeneracy.

If a partial isometry W in [formula] is self-dual, then the initial space of W will have even dimension and T will map the initial space isometrically onto the final space of W.

Applying Lemma [\ref=lem:left-right-Lemma] to the self-adjoint matrix W*W we find

[formula]

That is, when [formula] we have [formula] and [formula]. This is useful because

[formula]

which means [formula] and [formula] are orthogonal, and

[formula]

If we start with a unit vector [formula] in [formula] then we end up with an orthogonal pair of unit vectors [formula] and [formula] with [formula] and [formula].

If [formula] is a vector in [formula] that is orthogonal to both [formula] and [formula] then [formula] will be orthogonal to both [formula] and [formula] since T preserves orthogonality everywhere and W* preserves the orthogonality of vectors in [formula] Thus we can create a basis of [formula] out of pairs

[formula]

We need some examples of self-dual partial isometries. Treating vectors as 2N-by-1 matrices, if we set

[formula]

then this rank two (at most) matrix is self-dual since

[formula]

If we start with [formula] and [formula] orthogonal, then V will be the rank-two partial isometry taking [formula] to T and [formula] to T. We record this as a lemma that avoids the ugly notation.

Suppose [formula] and [formula] are orthogonal unit vectors. Then the partial isometry from [formula] to [formula] that sends [formula] to [formula] and [formula] to [formula] will be self-dual.

If X in [formula] satisfies [formula] then there is a unitary U so that [formula] and [formula].

Once more W = Xf(X*X) works to create a partial isometry W with X = WP for [formula] and this time we have [formula]. We need a partial isometry from ker (W) to [formula] that is self-dual. The dimension of ker (W) will be even, by Lemma [\ref=pro:self-dual-PartialIsomDouble]. Moreover if

[formula]

is an orthogonal basis for ker (W) then

[formula]

will be an orthogonal basis for [formula]. The needed self-dual partial isometry V will send [formula] to [formula] and [formula] to [formula] and the self-dual unitary we use will be U = W + V.

The odd particle causes even degeneracy

We hope not to scare the mathematical reader with more discussion of Kramers degeneracy. What Kramers discovered was that for a certain systems involving a odd number of electrons, the Hamiltonian always had all eigenvalues with even multiplicity.

A mathematical manifestation of this is that the tensor product of two dual operations is the transpose operation in disguise. In contrast to that, the tensor product of three dual operations is a large dual operation in disguise. Specifically if we implement an orthogonal change of basis, the dual operation becomes the transpose.

This is essentially the same as the facts that is more familiar to mathematicians, that [formula] and [formula].

In the following, we let ZN be as in ([\ref=eq:Define-Z]), the matrix that specified the dual operation.

Consider

[formula]

For all [formula] and [formula],

[formula]

Since [formula] we see [formula]. Also

[formula]

so U is a unitary and [formula].

Since [formula] we find

[formula]

We have refrained from discussing C*-algebras, but here they add clarity. What the lemma is showing is

[formula]

For all [formula] and [formula],

[formula]

where the [formula] on the right is taken with respect to [formula].

This is much simpler, as

[formula]

Together, these lemmas show us that a tensor product of three [formula] leads to something isomorphic to

[formula]

Acknowledgements

The author gratefully aknowleges the guidance and assistence received from mathematicians and physicists, in particular Vageli Coutsias, Matthew Hastings and Adam Sø rensen.

This work was partially supported by a grant from the Simons Foundation (208723 to Loring), and by the Efroymson fund at the University of New Mexico.