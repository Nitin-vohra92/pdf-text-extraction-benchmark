Corollary Lemma Proposition Assumption Conjecture Convention Definition Example Hypothesis Notation Remark

Upper bounds for the maximum of a random walk with negative drift

Introduction and statement of results

Let [formula] denote the random walk with increments Xi, that is,

[formula]

We shall assume that [formula] are independent copies of a random variable X with distribution function F and [formula]. The random walk Sn drifts to -    ∞   and the total maximum [formula] is finite almost surely. The random variable M plays a crucial role in a number of applications. For example, its distribution coincides with the stationary distribution of the queue-length in simple queueing systems. Another important application comes from the insurance mathematics: Under some special restrictions on X the quantity [formula] is equal to the ruin probability in the so-called Poisson model.

The tail-behaviour of M has been studied extensively in the literature. The first result goes back, apparently, to Cramer and Lundberg (see, for example, Asmussen [\cite=A00]): If

[formula]

and, in addition, [formula], then there exists a constant c0∈(0,1) such that

[formula]

The case [formula] has been considered recently by Korshunov [\cite=K05].

If ([\ref=Cramer]) is not fulfilled, then one assumes that the distribution of X is regular in some sense. To specify what regular means we recall some definitions and known properties. For their proofs we refer to Asmussen [\cite=A00]. Consider a distribution function B on [formula] and let [formula] be the right tail of B. A distribution function B with support [formula] is called subexponential, if [formula] for all x and

[formula]

for all n  ≥  2, where [formula] is the n-fold convolution of B with itself. For the subexponentiality it is sufficient to verify the equation ([\ref=subexponential]) in the case n = 2. All subexponential distributions are heavy-tailed, i.e. [formula] for all ε > 0, hence subexponential distributions do not satisfy ([\ref=Cramer]). If ([\ref=Cramer]) is not fulfilled, the most classical result for the asymptotics of M is due to Veraverbeke [\cite=V77], who showed that if the integrated tail [formula] is subexponential, then

[formula]

In many situations one needs non-asymptotic properties of the distribution of M. Since the exact form of that distribution is known in some special cases only, good estimates are required. Under condition ([\ref=Cramer]) one has for all x > 0 the inequality

[formula]

In the case when ([\ref=Cramer]) is not fulfilled, upper bounds for [formula] have been derived by Kalashnikov [\cite=K99] and by Richards [\cite=R09]. The approach in these papers is based on the representation of M as a geometric sum of independent random variables:

[formula]

where {χ+k} are independent random variables and [formula]. The main difficulty in this approach is the fact that one has to know the distribution of χ+k and the parameter q. In some special cases this information can be obtained from the initial data. But in general one has to obtain appropriate estimates for q and [formula].

The main purpose of the present paper is to derive upper bounds for [formula] assuming the existence of power moments of X only. Thereby we want to avoid the representation via geometric sums and use a supermartingale-construction instead.

As it is usual for deriving upper bounds, we are going to truncate summands and to use inequalities, which are based on truncated exponential moments. But the problem is that we have infinitely many Xi's. So we can not truncate all of them at the same level. Thus, we have to split the time axis into intervals of finite length and then choose a level of truncation on each of these intervals. One can take, for example, a deterministic strictly increasing sequence kn with k0 = 0 and consider the intervals In: = (kn,kn + 1]:

[formula]

Now, one can apply the Fuk-Nagaev inequalities, see [\cite=N79], to every probability in the last line. It is clear that replacing sup k∈In(Sk - ka) by sup k  ≤  kn + 1(Sk - ka) is not too rough if and only if kn + 1 and kn + 1 - kn are comparable. Thus, one has to take kn exponentially growing. Using this approach with kn = x2n, Borovkov [\cite=B76] obtained a version of the Markov inequality for M.

Our strategy is quite different and consists in splitting

[formula]

(X>x))/(|X|>x)→ p∈(0,1) x→∞.

[formula]

[formula]

lim=

[formula]

c := ,   c := .

[formula]

lim sup ≤ θ.

[formula]

→   z→ ∞ ,

[formula]

lim sup ≤ .

[formula]

(M > x)~(-1)(χ > x) x→∞.

[formula]

lim inf > σ.

[formula]

Proofs

Proofs of Theorems [\ref=T1] and [\ref=T2]

We set for brevity τ  =  τz.

For all h satisfying

[formula]

we have the inequality

[formula]

Our strategy is to truncate the random variables Xi in the level y:

[formula]

From the Wald identity follows

[formula]

To examine the first term on the right-hand-side of ([\ref=eq2.1]) we introduce the process {Wk} defined by

[formula]

It is clear that if h satisfies ([\ref=h-cond]), {Wk} is a positive supermartingale. Define

[formula]

Applying the Optional Stopping Theorem to the supermartingale [formula] leads us

[formula]

We analyse the two terms on the right-hand-side separately:

[formula]

and

[formula]

Consequently,

[formula]

and hence by applying ([\ref=L1.1]),

[formula]

It is easy to see that

[formula]

and as a result we have

[formula]

Applying ([\ref=L1.1]) and ([\ref=L1.2]) to the summands in ([\ref=eq2.1]) finishes the proof.

To prove Theorems [\ref=T1] and [\ref=T2] we need to choose a specific h for which ([\ref=h-cond]) holds. The optimal choice would be the positive solution of the equation [formula], which is in the spirit of the Cramer-Lundberg condition. But it is not clear how to solve this equation. For this reason we replace [formula] by the equation φ(h,y) = 1, where φ(h,y) is an appropriate upper bound for [formula].

If At  <    ∞  , we may use a bound from the proof of Theorem 2 from [\cite=FN71], which says

[formula]

Using the Markov inequality we also obtain

[formula]

and therefore

[formula]

Put [formula]. It is easy to see that

[formula]

for all y such that yt - 1  ≥  (e - 1)At / a and this implies that h0 satisfies ([\ref=h-cond]). Using ([\ref=L1.0]) with h = h0 and applying the inequality

[formula]

we obtain

[formula]

Thus, the proof of Theorem [\ref=T1] is complete.

In order to show that one can replace [formula] and At by the corresponding truncated moments, see Remark [\ref=R1], we first note that analogously to ([\ref=T1.2]) and by additionally using ex - 1  ≤  xex,

[formula]

If [formula], then

[formula]

is strictly positive and solves

[formula]

Therefore, we may use Lemma [\ref=L1] with h = h0 and get an inequality with truncated moments.

To bound [formula] under the conditions of Theorem [\ref=T2] we proceed similar to the proof of Theorem 3 from [\cite=N79] and get

[formula]

Following further the method from the proof of this Theorem, we split this upper bound into two parts:

[formula]

[formula]

We consider f1 and f2 separately. It is clear that

[formula]

is the positive solution of the equation f1(h) = 0. Moreover, f1(h) < 0 for all h∈(0,h1).

Furthermore, it is easy to see that f2 takes it's unique minimum in

[formula]

Since f2 is convex, one has

[formula]

The assumption in Theorem [\ref=T2](i) means that h1  ≤  h2. In this case, taking into account ([\ref=ineq]), we obtain

[formula]

From the latter inequality we conclude that h1 satisfies ([\ref=h-cond]) and by applying ([\ref=L1.0]) with h = h1 we obtain ([\ref=T2.1]).

Under the conditions of Theorem [\ref=T2] (ii) we have h2  ≤  h1. By the same arguments we get

[formula]

Then, applying ([\ref=L1.0]) with h = h2 and using the inequality (1 + u)x / y  ≥  ux / y, we obtain ([\ref=T2.2]).

Proof of Proposition [\ref=P1]

We want to use Theorem 2.1 from [\cite=BF98]. If we put F: = F- X the conditions (G1)-(G3) of this Theorem are fulfilled in our setting. Hence we get

[formula]

where

[formula]

with [formula] and ε∈(0,1) arbitrary. By the Theorem of Fubini we obtain

[formula]

Changing the order of integration gives us

[formula]

As you can easily see,

[formula]

therefore by ([\ref=c])

[formula]

and by minimisation over ε∈(0,1)

[formula]

Finally, combining ([\ref=eq2.2]), ([\ref=eq2.3]), ([\ref=eq2.4]) and ([\ref=c2]) gives us the desired result.

Proof of Theorem [\ref=T3]

We prove ([\ref=T3.1]) only. The proof of the second bound goes along the same line.

Using Theorem [\ref=T1] with y = θ(x + jz), we obtain

[formula]

and in view of ([\ref=eq1.1]),

[formula]

where

[formula]

and

[formula]

Define

[formula]

The summands in this sum are strictly decreasing, so we conclude by the integral criteria for sums:

[formula]

and further by integration by parts,

[formula]

Therefore, for all x sufficing xt - 1  ≥  θ1 - t(eθ - 1)Ata- 1 and x  ≥  z(t - 1)θ- 1,

[formula]

Furthermore, it is easy to see that

[formula]

and Theorem [\ref=T3] is proved.

Proof of Theorem [\ref=T4]

Foss, Korshunov and Zachary have shown, see Theorem 5.1 in [\cite=FKZ], that for any random walk with the drift - a and xa with xa  →    ∞   as a  →  0 one has the following lower bound:

[formula]

It follows from the regular variation of [formula], that

[formula]

therefore

[formula]

Thus, we only have to derive an upper bound. During this proof we assume a to be sufficiently small in every inequality. We want to apply Theorem [\ref=T3] (ii) with t < r. It is clear that

[formula]

therefore A(a)t, + is finite for t < r and

[formula]

Further, we have to show that ([\ref=T2.2.1]) is fulfilled for y = θx under our assumptions. Since the function y- 1 log (1 + βayt - 1 / A(a)t, +) is decreasing for y  ≫  a1 / (t - 1), we have the following bound for xa  ≥  ca- 1 log a- 1:

[formula]

This implies that if c > er(r - 2)σ2 / 2 and θ = (t - 2) / (r - 2), we can choose α < 1 so close to 1 that xa satisfies ([\ref=T2.2.1]).

We take z = za satisfying a- 1  ≪  z  ≪  xa. Then, combining ([\ref=tau]) and ([\ref=lorden]), we get

[formula]

Since (t - 1) / θ = (t - 1)(r - 2) / (t - 2) > r - 1,

[formula]

Further, it follows from the condition z = o(x) and the regular variation of [formula] that

[formula]

Combining ([\ref=eq2.5]) with ([\ref=eq2.6.1]) and ([\ref=eq2.4.2]), we obtain

[formula]

and plugging ([\ref=eq2.6]) and ([\ref=eq2.7]) into ([\ref=T3.2]) gives us

[formula]

To complete the proof it suffices to note, that we can choose θ arbitrary close to 1 by choosing t close to r. This implies that the previous inequality is valid even with θ = 1.

Proof of Theorem [\ref=T5]

We again need an upper bound only. Let a be sufficiently small during this proof.

It follows from the assumptions in the theorem that S(0)n / cn converges weakly to a stable law of index r. The sequence cn can be taken from the equation c- rnL(cn) = 1 / n. It is known that the function g(a) in the heavy-traffic approximation can be defined by the relations

[formula]

The latter can be rewritten as

[formula]

From this we infer that ([\ref=T5.1]) is equivalent to

[formula]

We want to apply Theorem [\ref=T1] with [formula] and A2(y) instead of a and A2 respectively with y = θx. According to Remark [\ref=R1] we have to show that [formula] is negative. Using the Markov inequality, we have

[formula]

In view of ([\ref=g]), axa  →    ∞  . Therefore,

[formula]

Furthermore,

[formula]

and consequently by [formula],

[formula]

with k1 and k2 appropriate. Then, ([\ref=g]) implies that

[formula]

Further,

[formula]

with k3 suitable and hence by ([\ref=g]),

[formula]

Then, combining ([\ref=eq2.8]), ([\ref=eq2.9]) and ([\ref=eq2.10]), Theorem [\ref=T1] with t = 2 and y = θxa gives us

[formula]

where θ∈(0,1) is arbitrary. Hence by θ  →  1,

[formula]

By the summation formula ([\ref=eq1.1]) we get a bound for the total maximum:

[formula]

Combining ([\ref=eq2.4.1]) and ([\ref=eq2.4.2]) with a- 1  ≪  z  ≪  xa gives us

[formula]

and regarding ([\ref=eq2.5]) completes the proof.

Acknowledgement. We are grateful to Denis Denisov for useful references.