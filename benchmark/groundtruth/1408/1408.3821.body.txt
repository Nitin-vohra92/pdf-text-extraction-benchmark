Limits on Fundamental Limits to Computation

Introduction

Emerging technologies for computing promise to outperform conventional integrated circuits in computation bandwidth or speed, power consumption, manufacturing cost, or form factor [\cite=Cavin12] [\cite=Chien13]. However, razor-sharp focus on any one nascent technology and its benefits sometimes neglects serious limitations or discounts ongoing improvements in established approaches. To foster a richer context for evaluating emerging technologies, we review limiting factors and salient trends in computing that determine what is achievable in principle and in practice. Several fundamental limits remain substantially loose, possibly indicating viable opportunities for emerging technologies. To clarify this uncertainty, we study limits on fundamental limits.

Universal and general-purpose computers. Viewing clocks and watches as early computers, it is easy to see the importance of long-running calculations that can be repeated with high accuracy by mass-produced devices. The significance of programmable digital computers became clear at least 200 years ago, as illustrated by Jacquard looms in textile manufacturing. However, the existence of universal computers that can efficiently simulate (almost) all other computing devices -- analog or digital -- was only articulated in the 1930s by Church and Turing (Turing excluded quantum physics when considering universality) [\cite=Herken13]. Efficiency was studied from a theoretical perspective at first, but strong demand in military applications in the 1940s lead Turing and von Neumann to develop detailed hardware architectures for universal computers -- Turing's design (Pilot ACE) was more efficient, but von Neumann's was easier to program. The stored-program architecture made universal computers practical in the sense that a single computer design could be effective in many diverse applications. Such practical universality thrives (i) in economies of scale in computer hardware and (ii) among extensive software stacks. Not surprisingly, the most sophisticated and commercially successful computer designs and components, such as Intel and IBM CPUs, were based on the von Neumann's paradigm. The numerous uses and large markets of general-purpose chips, as well as the exact reproducibility of their results, justify the enormous capital investment in the design, verification and manufacturing of leading-edge integrated circuits. Today general-purpose CPUs power cloud server-farms and displace specialized (but still universal) mainframe processors in many supercomputers. Emerging universal computers based on field-programmable gate-arrays (FPGAs) and general-purpose graphics processing units (GPGPUs) outperform CPUs in some cases, but their efficiencies remain complementary to those of CPUs. The success of deterministic general-purpose computing manifests in the convergence of diverse functionalities in portable inexpensive smartphones. After steady improvement, general-purpose computing displaced entire industries (newspapers, photography, etc) and launched new applications (video conferencing, GPS navigation, online shopping, networked entertainment, etc) [\cite=Andreesen11]. Application-specific integrated circuits (ASICs) streamline input-output and networking, or optimize functionalities previously performed by general-purpose hardware. They speed up biomolecular simulation 100-fold [\cite=Padua11] [\cite=Shaw13] and improve the efficiency of video decoding 500-fold [\cite=Hameed11], but require design effort with keen understanding of specific computations, impose high costs and financial risks, need markets where general-purpose computers lag behind, and often cannot adapt to new algorithms. Recent techniques for customizable domain-specific computing [\cite=Cong11] offer better tradeoffs, while many applications favor the combination of general-purpose hardware and domain-specific software, including specialized programming languages [\cite=Mernik05] [\cite=Olukotun14] such as Erlang used in Whatsapp.

Limits as aids to evaluating emerging technologies. Without sufficient history, we cannot extrapolate scaling laws for emerging technologies, yet expectations run high. For example, new proposals for analog processors appear frequently (as illustrated by adiabatic quantum computers), but fail to address concerns about analog computing, such as its limitations on scale, reliability, and long-running error-free computation. General-purpose computers meet these requirements with digital integrated circuits (IC) and now command the electronics market. In comparison, quantum computers -- both digital and analog -- hold promise only in niche applications and do not offer faster general-purpose computing as they are no faster for sorting and other specific tasks [\cite=Aaronson04] [\cite=Jain10] [\cite=Nielsen11]. In exaggerating the engineering impact of quantum computers, popular press missed this nuance. But in scientific research, building quantum computers may help simulating quantum-chemical phenomena and reveal new fundamental limits. Sections [\ref=sec:space] and [\ref=sec:conc] discuss limits on emerging technologies.

Technology extrapolation versus fundamental limits. The scaling of commercial computing hardware regularly runs into formidable obstacles [\cite=Cavin12], but near-term technological advances often circumvent them. The International Technology Roadmap for Semiconductors (ITRS) [\cite=ITRS] keeps track of such obstacles and possible solutions with a focus on frequently-revised consensus estimates. For example, consensus estimates initially predicted 10 GHz CPUs for the 45 nm technology node [\cite=Sylvester00], versus the 3-4 GHz range seen in practice. In 2004, the unrelated Quantum Information Science and Technology Roadmap [\cite=Hughes04] forecast 50 physical qubits by 2012. Such optimism arose by assuming technological solutions long before they were developed and validated, and by overlooking important limits. The authors of [\cite=Meindl95] [\cite=Davis01] classify limits to device and interconnect as fundamental, material, device, circuit, and system limits. These categories define the rows of Table [\ref=tab:limits], and the columns reflect sections of this paper where specific limits are examined for tightness.

Engineering obstacles

Engineering obstacles limit specific technologies and choices. For example, a key bottleneck today is IC manufacture, which packs billions of transistors and wires in several cm2 of silicon with astronomically low defect rates. Layers of material are deposited on silicon and patterned with lasers, fabricating all circuit components simultaneously. Precision optics and photochemical processes ensure accuracy.

Limits on manufacturing. No account of limits to computing is complete without the Abbe diffraction limit: light with wavelength λ, traversing a medium with refractive index η, and converging to a spot with angle θ (perhaps, focused by a lens) creates a spot with diameter d = λ / NA, where NA = η sin θ is the numerical aperture. NA reaches 1.4 for modern optics, so it would seem that semiconductor manufacturing is limited to feature sizes λ / 2.8, hence ArF lasers with 193nm wavelength should not support photolithographic manufacturing of transistors with 65nm features. Yet, they supported sub-wavelength lithography for the 45nm-22nm technology nodes using asymmetric illumination and computational lithography [\cite=MaA11]. Here one starts with optical masks that look like the intended image, but when the image gets blurry, alter masks by gently shifting edges to improve the image, possibly giving up the semblance between the two. Clearly, some limits are formulated to be broken! Ten years ago, researchers demonstrated patterning of nanomaterials by live viruses [\cite=Mazzola03]. Known virions exceed 20nm in diameter, whereas subwavelength lithography with 193nm-wavelength ArF laser recently extended to 14nm semiconductor manufacturing [\cite=ITRS]. Hence, viruses and microorganisms are no longer at the forefront of semiconductor manufacturing. Extreme ultra-violet (X-ray) lasers have been energy-limited, but are improving. Their use requires changing refractive optics to reflective. Additional progress in multiple patterning and directed self-assembly promises to support photolithography beyond the 10nm technology node.

Limits on individual interconnects. Despite the doubling of transistor density with Moore's law [\cite=Moore65], semiconductor integrated circuits (ICs) would not work without fast and dense interconnects. Metallic wires can be either fast or dense, but not both at the same time -- smaller cross-section increases electrical resistance, while greater height or width increase parasitic capacitance with neighboring wires (wire delay grows with RC). In 1995, an Intel researcher pointed out that on-chip interconnect scaling is the real limiter of high-performance ICs [\cite=Bohr95]. The scaling of interconnect is also moderated by electron scattering against rough edges of metallic wires [\cite=Davis01] [\cite=Naeemi14], inevitable with atomic-scale wires. Hence, IC interconnect stacks have evolved [\cite=Sylvester00] [\cite=Shelar13] from four equal-pitch layers in 2000 to 16 layers with pitches varying by 32 times, including a large amount of dense (thin) wiring and fast (thick) wires used for global on-chip communication (Figure [\ref=fig:wires]). Aluminum and copper remain unrivaled for conventional interconnects and can be combined in short wires [\cite=Naeemi14]; carbon-nanotube and spintronic interconnects are also evaluated in [\cite=Naeemi14]. Photonic waveguides and RF links offer alternative IC interconnect [\cite=Almeida04] [\cite=Chang02], but obey fundamental limits derived from Maxwell's equations, such as the maximum propagation speed of EM waves [\cite=Davis01]. I/O links are limited by the perimeter or surface area of a chip, whereas chip capacity grows with area or volume, respectively.

Limits on conventional transistors. Transistors are limited by their tiniest feature -- the width of the gate dielectric, -- which recently reached the size of several atoms (Figure [\ref=fig:xtor-atoms]), creating problems: (i) a few missing atoms could alter transistor performance, (ii) manufacturing variation makes all transistors slightly different (Figure [\ref=fig:xtor-field]), (iii) electric current tends to leak through thin narrow dielectrics [\cite=Meindl95]. Instead of a thinner dielectric, transistors can be redesigned with wider dielectric layers [\cite=Hisamoto02] that surround a fin shape (Figure [\ref=fig:xtor-fin]). Such configurations improve the control of electric field, reduce current densities and leakage, and diminish process variations. Each transistor can use several fins, extending transistor scaling by several generations. Semiconductor manufacturers adopted FinFETs for upcoming technology nodes. One step further, in tunneling transistors [\cite=Seabaugh13] a gate wraps around the channel to control tunnelling rate.

Limits on design effort.

In the 1980s, Mead and Conway formalized IC design using a regular grid, enabling automated layout through algorithms. But resulting optimization problems remain hard, and heuristics are only good enough for practical use. Besides frequent algorithmic improvements, each technology generation alters circuit physics and requires new CAD software. The cost of design has doubled in a few years, becoming prohibitive for ICs with limited market penetration [\cite=ITRS]. Emerging technologies, such as FinFETs and high-K dielectrics, circumvent known obstacles using forms of design optimization. Therefore, reasonably tight limits should account for potential future optimizations. Low-level technology enhancements, no matter how powerful, are often viewed as one-off improvements, in contrast to architectural redesigns that affect many processor generations. Between technology enhancements and architectural redesigns are global and local optimizations that alter "the texture" of IC design, such as logic restructuring, gate sizing and device parameter selection. Moore's law promises higher transistor densities, but some transistors are designed to be 32 times larger than others. Large gates consume greater power to drive long interconnects at acceptable speed and satisfy performance constraints. Minimizing circuit area and power, subject to timing constraints (by configuring each logic gate to a certain size, threshold voltage, etc), is a hard but increasingly important optimization with a large parameter space. A recent convex optimization method [\cite=Ozdal12] saved 30% power in Intel chips, and the impact of such improvements grows with circuit size. Many aspects of IC design are being improved, continually raising the bar for technologies that compete with CMOS.

Completing new IC designs, optimizing and verifying them requires great effort and continuing innovation, e.g., the lack of scalable design automation is a limiting factor for analog ICs [\cite=Rutenbar06] [\cite=Rutenbar10]. In 1999, bottom-up analysis of digital IC technologies [\cite=Ho99] [\cite=Sylvester00] outlined design scaling up to self-contained modules with 50K standard cells (each cell contains 1-3 logic gates), but further scaling was limited by global interconnect. In 2010, physical separation of modules became less critical, as large-scale placement optimizations assumed greater responsibility for IC layout and learned to blend nearby modules [\cite=Markov12] [\cite=Puri13]. In a general trend, powerful design automation [\cite=Lavagno06] frees circuit engineers to focus on microarchitecture [\cite=Puri13], but increasingly relies on algorithmic optimization. Until recently, this strategy suffered significant losses in performance [\cite=Chinnery04] and power [\cite=Chinnery07] compared to ideal designs, but has now became both successful and indispensable due to rapidly increasing complexity of digital and mixed-signal electronic systems. Hardware and software must now be co-designed and co-verified, with software efforts increasing at a faster rate. Platform-based design combines high-level design abstractions with effective reuse of components and functionalities in engineered systems [\cite=Vincentelli04]. Customizable domain-specific computing [\cite=Cong11] and domain-specific programming languages [\cite=Mernik05] [\cite=Olukotun14] offload specialization to software running on reusable hardware platforms.

Energy-time limits

In predicting the main obstacles to improving modern electronics, the International Technology Roadmap for Semiconductors (ITRS) highlights the management of system power and energy as the dominant Grand Challenge [\cite=ITRS]. The faster the computation, the more energy it consumes, but actual power-performance tradeoffs depend on the physical scale. While the ITRS, by its charter, focuses on near-term projections and IC design techniques, fundamental limits reflect available energy resources, properties of the physical space, power-dissipation constraints, and energy waste.

Reversibility

A 1961 result by Landauer [\cite=Landauer61] shows that erasing one bit of information entails an energy loss ≥  kT ln 2 ( thermodynamic threshold), where k is the Boltzmann constant and T is the temperature in Kelvin. This principle was validated empirically in 2012 [\cite=Berut12] and seems to motivate reversible computing [\cite=Bennett85], where all input information is preserved, incurring additional costs. Formally speaking, zero-energy computation is prohibited by the energy-time form of the Heisenberg uncertainty principle (ΔtΔE  ≥  ): faster computation requires greater energy [\cite=Aharonov61] [\cite=Lloyd00]. However, recent work in applied superconductivity [\cite=Ren11] demonstrates "highly exotic" physically-reversible circuits operating at 4[formula]K with energy dissipation below the thermodynamic threshold. They apparently fail to scale to large sizes, run into other limits, and remain no more practical than "mainstream" superconducting circuits and refrigerated low-power CMOS circuits. Technologies that implement quantum circuits [\cite=Monroe14] can approximate reversible Boolean computing, but currently do not scale to large sizes, are energy-inefficient at the system level, rely on fragile components, and require heavy fault-tolerance overhead [\cite=Nielsen11]. Conventional ICs also do not help obtaining energy savings from reversible computing because they dissipate 30-60% of all energy in (reversible) wires and repeaters [\cite=Shelar13]. At room temperature, Landauer's limit amounts to 2.85×  10- 21 J -- a very small fraction of the total, given that modern ICs dissipate 0.1-100 Watts and contain < 109 logic gates. With the increasing dominance of interconnect (Section [\ref=sec:space]), more energy is spent on communication than on computation. Logically-reversible computing is important for reasons other than energy -- in cryptography, quantum information processing, etc [\cite=Saeedi13].

Power constraints and CPUs

The end of CPU frequency scaling. In 2004, Intel Corp. abruptly cancelled a 4GHz CPU project because high power density required awkward cooling technologies. Other CPU manufacturers kept clock-frequencies in the 1-6GHz range, but also resorted to multicore CPUs [\cite=Borkar07]. Since dynamic circuit power grows with clock frequency and supply voltage squared [\cite=Rabaey04], energy can be saved by distributing work among slower, lower-voltage parallel CPU cores if parallelization overhead is small.

Dark, darker, dim, gray silicon. A companion trend to Moore's law -- the Dennard scaling theory [\cite=Bohr07] -- shows how to keep power consumption of semiconductor ICs constant while increasing their density. But Dennard scaling broke down ten years ago [\cite=Bohr07]. Extrapolation of semiconductor scaling trends for CMOS -- the dominant semiconductor technology for 20 years -- shows that the power consumption of transistors available in modern ICs reduces more slowly than their size (which is subject to Moore's law) [\cite=Taylor12] [\cite=Esmaielzadeh13]. To ensure the performance envelope of transistors, chip power density must be limited, and a fraction of transistors must be kept dark at any given time. Modern CPUs have not been able to use all their circuits at once, but this asymptotic effect -- termed the utilization wall [\cite=Taylor12] -- will soon black out 99% of the chip, hence the term dark silicon and a reasoned reference to the apocalypse [\cite=Taylor12]. Saving power by slowing CPU cores down is termed dim silicon. Detailed studies of dark silicon [\cite=Esmaielzadeh13] show similar results. To this end, executives from Microsoft and IBM have recently proclaimed an end to the era of multicore microprocessors [\cite=IBM13]. Two related trends appeared earlier: (i) increasingly large IC regions remain transistor-free to aid routing and physical synthesis, to accommodate power-ground networks, etc [\cite=Caldwell03] [\cite=Adya06] -- we call them darker silicon, (ii) increasingly many gates do not perform useful computation but reinforce long, weak interconnects [\cite=Saxena04] or slow down wires that are too short -- call them gray silicon. Today, 50-80% of all gates in high-performance ICs are repeaters.

Limits for power supply and cooling. Data centers in the US consumed 2.2% of total U.S. electricity in 2011. As powerplants take time to build, we cannot sustain past trends of doubled power consumption per year. It is possible to improve the efficiency of transmission lines (using high-temperature superconductors [\cite=Oestergaard01]) and power conversion in datacenters, but the efficiency of on-chip power-networks may soon reach 80-90%. Modern IC power management includes clock and power gating [\cite=Borkar07], per-core voltage scaling [\cite=Pinckney13], charge recovery [\cite=Kim05] and, in recent processors, a CPU core dedicated to power scheduling. IC power consumption depends quadratically on supply voltage, which has decreased steadily for many years, but recently stabilized at 0.5-2V [\cite=Rabaey04]. Supply voltage typically exceeds the threshold voltage of field-effect transistors by a safety margin that ensures circuit reliability, fast operation and low leakage. Threshold voltage depends on the thickness of gate dielectric, which reached a practical limit of several atoms (Section [\ref=sec:eng]). Supply voltage is limited by around 200mV [\cite=Meindl95] -- five times below current practice -- and simple circuits reach this limit. With slower operation, near- and sub-threshold circuits may consume 100 times less energy [\cite=Dreslinski10].

Cooling technologies can improve too, but fundamental quantum limits bound the efficiency of heat removal [\cite=Pendry83] [\cite=Blencowe00] [\cite=Whitney14].

Broader limits

The study in [\cite=ZhirnovCHB03] explores a general binary-logic switch model with binary states represented by two quantum wells separated by a potential barrier. Representing information by electric charge requires energy for binary switching and thus limits the logic-switching density, if a significant fraction of the chip can switch simultaneously. To circumvent this limit, one can encode information in spin-states, photon polarizations, super-conducting currents, or magnetic flux, noting that these carriers have already been in commercial use. Spin-states are particularly attractive because they promise high-density nonvolatile storage [\cite=Wolf01] and scalable interconnects [\cite=Naeemi14]. More powerful limits are based on the amount of material in Earth's crust (where silicon is the second most common element after oxygen), on atomic spacing (Section [\ref=sec:eng]), radii and energies, bandgaps, as well as the wavelength of the electron. We are currently using only a tiny fraction of Earth's mass for computing, and yet various limits could be circumvented if new particles are discovered. Beyond atomic physics, some limits rely on basic constants: the speed of light, the gravitational constant, the quantum (Planck) scale, the Boltzmann constant, etc. Lloyd [\cite=Lloyd00], as well as Kraus [\cite=Krauss04] extend well-known bounds by Bremermann and Bekenstein, and give Moore's law 150 and 600 years, respectively. These results are too loose to obstruct the performance of practical computers. In contrast, current consensus estimates from the ITRS [\cite=ITRS] give Moore's law only 10-20 years, due to both technological and economic considerations [\cite=Chien13].

Asymptotic space-time limits

Engineering limits for deployed technologies can often be circumvented, while first-principles limits on energy and power are very loose. Reasonably tight limits are rare.

Limits to parallelism. Suppose we wish to compare a parallel and sequential computer built from the same units, to argue that a new parallel algorithm is many times faster than the best sequential algorithm (the same reasoning applies to logic gates on an IC). Given N parallel units and an algorithm that runs K times faster on sufficiently large inputs, one can simulate the parallel system on the sequential system by dividing its time between N computational slices. Since this simulation is roughly N times slower, it runs K / N times faster than the original sequential algorithm. If this algorithm was best possible, we have K  ≤  N. The bound is reasonably tight in practice for small N and can be violated slightly since N CPUs include more CPU cache, but such violations do not justify parallel algorithms -- one could instead buy/build one CPU with a larger cache. Such linear speedup is optimistically assumed for the parallelizable component in the 1988 Gustafson's law that suggests scaling the number of processors with input size (as illustrated by instantaneous Web search queries over massive data sets) [\cite=Padua11]. Also in 1988, Fisher [\cite=Fisher88] employed asymptotic runtime estimates instead of numerical limits and avoided the breakdown into parallel and sequential runtime components, assumed in Amdahl's [\cite=Amdahl13] and Gustafson's laws [\cite=Padua11]. Asymptotic estimates neglect leading constants and offer a powerful way to capture nonlinear phenomena occurring at large scale.

Fisher [\cite=Fisher88] assumes a sequential computation with T(n) elementary steps for input of size n, and limits the performance of its parallel variants that can use an unbounded d-dimensional grid of finite-size computing units (electrical switches on a semiconductor chip, logic gates, CPU cores, etc) communicating at a finite speed, say, bounded by the speed of light. We highlight only one aspect of this four-page work -- the parallel computation requires [formula] steps. This result undermines the N-fold speedup assumed in Gustafson's law for N processors on appropriately sized input data [\cite=Padua11]. A more realistic speedup from ~  nk to ~   log n can be achieved in an abstract model of computation for matrix multiplication and fast Fourier transforms. But not in physical space [\cite=Fisher88]. Surprising as it may seem, after reviewing many loose limits to computation, we have identified a reasonably tight limit (the impact of I/O -- a major bottleneck today -- is also covered in[\cite=Fisher88]). Indeed, many parallel computations today (excluding multimedia processing and Web search) are limited by several forms of communication and synchronization, including network and storage access. The billions of logic gates and memory elements in modern ICs are linked by up to 16 levels of wires (Figure [\ref=fig:wires]), longer wires are segmented by repeaters. Most of the physical volume and circuit delay are attributed to interconnect [\cite=Shelar13]. This is relatively new, as gate delays were dominant until 2000 [\cite=ITRS], but wires get slower relative to gates at each new technology node. This uneven scaling has compounded in ways that would surprise Turing and von Neumann -- a single clock cycle is now far too short for a signal to cross the entire chip, and even the distance covered in 200 ps (5 GHz) at light-speed is close to chip size. Yet, most electrical engineers and computer scientists continue to focus on gates.

Implications to 3D ICs and other emerging technologies. The promise of 3D integration for improving IC performance can be contrasted with technical obstructions to its industry adoption. To derive limits on possible improvement, we use the result from [\cite=Fisher88] sensitive to the dimension of the physical space: a sequential computation with T(n) steps requires [formula] steps in 2D and [formula] in 3D. Letting [formula], shows that 3D integration asymptotically reduces t to t3 / 4 -- a significant but not dramatic speedup. This speedup requires an unbounded number of 2D device layers, otherwise there is no asymptotic speedup [\cite=Mak12]. For 3D ICs with 2-3 layers, the main benefits of 3D IC integration today are in improving manufacturing yield, improving I/O bandwidth, and combining 2D ICs that are optimized for random logic, dense memory, FPGA, analog, MEMS, etc. Ultra-high density CMOS logic ICs with monolithic 3D integration [\cite=Lee12] suffer higher routing congestion than traditional 2D ICs. Emerging technologies promise to improve device parameters, but often remain limited by scale, faults, and interconnect, e.g., quantum dots enable Terahertz switching but hamper nonlocal communication [\cite=Sherwin99]. CNT-FETs [\cite=Shulaker13] leverage extraordinary carrier mobility in semiconducting carbon nanotubes to use interconnect more efficiently by improving drive strength, while reducing supply voltage. Emerging interconnects include silicon photonics, shown by Intel in 2013 [\cite=Simonite13] as a 100Gb/s replacement of copper cables connecting adjacent chips. It promises to reduce power consumption and form factor. Quantum physics alters the nature of communication with Einstein's "spooky action at a distance" facilitated by entanglement [\cite=Nielsen11]. However, the flows of information and entropy are subject to quantum limits [\cite=Pendry83] [\cite=Blencowe00]. Several quantum algorithms run asymptotically faster than best conventional algorithms [\cite=Nielsen11], but fault-tolerance overhead offsets their potential benefits in practice, and empirical evidence of quantum speedups has not been compelling so far [\cite=Ronnow14] [\cite=Shin14]. Several stages in the development of quantum information processing remain challenging [\cite=Devoret13], and the surprising difficulty of scaling up reliable quantum computation could stem from limits on communication and entropy [\cite=Pendry83] [\cite=Blencowe00] [\cite=Nielsen11]. In contrast, Lloyd [\cite=Lloyd00] notes that individual quantum devices now approach energy limits for switching, whereas nonquantum devices remain orders of magnitude away. This suggests an obstacle to simulating quantum physics on conventional computers (abstract models aside). In terms of computational complexity though, quantum computers cannot attain significant advantage for many problem types [\cite=Aaronson04] [\cite=Jain10] [\cite=Nielsen11]. Such lack of consistent general-purpose speedup limits the benefits of several emerging technologies in mature applications with diverse algorithmic steps, e.g., computer-aided design and Web search. Accelerating one step usually does not greatly speed up the entire application, as noted by Amdahl in 1967 [\cite=Amdahl13]. Figuratively speaking, the most successful computers are designed for the decathlon, rather than for sprint only.

Complexity-theoretic limits

Section [\ref=sec:space] enabled tighter limits by neglecting energy and using asymptotic rather than numeric bounds -- a more abstract model focuses on the impact of scale, and recurring trends quickly overtake one-off device-specific effects.

Next, we neglect spatial effects and focus on the nature of computation in an abstract model (used by software engineers) that represents computation by elementary steps with input-independent runtimes. Such limits survive many improvements in computer technologies, and are often stronger for specific problems. For example, the best-known algorithms for multiplying large numbers are only slightly slower than reading the input (an obvious speed limit), but only in the asymptotic sense -- for numbers with <  1000 bits, those algorithms lag behind simpler algorithms in actual performance.

To focus on what matters, we now do not just track asymptotic worst-case complexity of best algorithms for a given problem, but merely distinguish polynomial asymptotic growth from exponential. Limits formulated in such crude terms (unsolvability in polynomial time on any computer) are powerful [\cite=Sipser12]: the hardness of number-factoring underpins Internet commerce, while the P≠  NP conjecture explains the lack of satisfactory, scalable solutions to important algorithmic problems, e.g., in optimization and verification of IC designs [\cite=Fortnow09]. A similar conjecture P≠  NC seeks to explain why many algorithmic problems that can be solved efficiently have not parallelized efficiently [\cite=Markov13]. Most of these limits have not been proven. Some can be circumvented by using radically different physics, e.g., quantum computers solve number factoring in polynomial time (in theory). But quantum computation does not affect P≠  NP [\cite=Aaronson05]. The lack of proofs, despite heavy empirical evidence, requires faith and is an important limitation of many nonphysical limits to computing. This faith is not universally shared -- Donald Knuth argues that P=NP would not contradict anything we know today. A rare proven result by Turing (also invulnerable to quantum physics) states that checking if a given program ever halts is undecidable: no algorithm solves this problem in all cases regardless of runtime. Yet, software developers solve this problem during peer code reviews, and computer science teachers -- when grading exams in programming courses. Worst-case analysis is another limitation of nonphysical limits to computing, but suggests potential gains through approximation and specialization. For some NP-hard optimization problems, such as the Euclidean Travelling Salesman Problem (EucTSP), polynomial-time approximations exist, but in other cases, such as the maximum clique problem, accurate approximation is as hard as finding optimal solutions [\cite=Vazirani02]. For some important problems and algorithms, such as the Simplex algorithm for linear programming, few inputs lead to exponential runtime, and minute perturbations reduce runtime to polynomial [\cite=Spielman01].

Conclusions

The death march of Moore's law [\cite=Cavin12] [\cite=Chien13] invites discussions of fundamental limits and alternatives to silicon semiconductors [\cite=Shulaker13]. Near-term constraints invariably tie to costs and capital, but are explained away by new markets for electronics, increasing Earth population, and growing world economy [\cite=Chien13]. Such economic pressures emphasize the value of computational universality and broad applicability of IC architectures to solve multiple tasks under conventional environmental conditions. In a likely scenario, only CPUs, GPUs, FPGAs and dense memory ICs will remain viable at the end of Moore's law, while specialized circuits will be manufactured with less advanced technologies. Indeed, memory chips have lead Moore scaling by leveraging their simpler structure, modest interconnect, and more controllable manufacturing, but their scaling is slowing down [\cite=Chien13]. The decelerated scaling of CMOS ICs still outperforms the scaling of the most viable emerging technologies. Empirical scaling laws describing the evolution of computing are well-known [\cite=Getov13]. In addition to Moore's law, Dennard scaling, as well as Amdahl's and Gustafson's laws reviewed earlier, Metcalfe's law [\cite=Metcalfe13] states that the value of a computer network, such as the Internet or Facebook, scales as the number of user-to-user connections that can be formed. Grosch's law [\cite=Ryan13] ties N-fold improvements in computer performance to N2-fold cost increases (in equivalent units). Applying it in reverse, we can estimate acceptable performance of cheaper computers. But such laws only capture ongoing scaling and will break down in the future.

The roadmapping process represented by the International Technology Roadmap for Semiconductors (ITRS) [\cite=ITRS] relies on consensus estimates and works around engineering obstacles. It tracks improvements in materials and tools, collects best practices and outlines promising design strategies. As suggested in [\cite=Meindl95] [\cite=Davis01], it can be enriched by analysis of limits. We additionally focus on how closely such limits can be approached. Aside from historical "wrong turns" recalled in Sections [\ref=sec:eng] and [\ref=sec:energy], we find interesting effects when examining the tightness of individual limits. While energy-time limits are most critical in computer design [\cite=ITRS] [\cite=Wenisch12], space-time limits appear tighter [\cite=Fisher88] and capture bottlenecks formed by interconnect and communication. They suggest optimizing gate locations and sizes, and placing gates in three dimensions. One can also adapt algorithms to spatial embeddings [\cite=Bachrach07] [\cite=Rosenbaum12] and seek space-time limits. But the gap between current technologies and energy-time limits hints at greater rewards. Charge recovery [\cite=Kim05], power management [\cite=Borkar07], voltage scaling [\cite=Pinckney13], and near-threshold computing [\cite=Dreslinski10] reduce energy waste. Optimizing algorithms and circuits simultaneously for energy and spatial embedding [\cite=Patil07] gives biological systems an edge (from the 1D worm C. elegans with 302 neurons to the 3D human brain with 86 billion neurons) [\cite=Cavin12]. Yet, using mass-energy to compute can be a veritable nuclear option. In a 1959 talk, which predated Moore's law, Richard Feynman suggested that there was "plenty of room at the bottom," forecasting the miniaturization of electronics. Today, with relatively little physical room left, there is plenty of energy at the bottom. If this energy is tapped for computing, how can resulting heat be removed? Recycling heat into mass or electricity seems ruled out by limits to energy conversion and the acceptable thermal envelope.

Technology-specific limits for modern computers tend to express tradeoffs, especially for systems with conflicting performance parameters and properties [\cite=CAP12]. Little is known about limits on design technologies. Given that large-scale complex systems are often designed and implemented hierarchically [\cite=Caldwell03] with multiple levels of abstraction, it would be valuable to capture losses incurred at abstraction boundaries and between levels of design hierarchies. It is common to estimate resources required for a subsystem and then implement the subsystem to satisfy resource budgets. Underestimation is avoided because it leads to failures, but overestimation results in overdesign. Inaccuracies in estimation and physical modeling also lead to losses during optimization, especially in the presence of uncertainty. Clarifying engineering limits gives hope to circumvent them.

Technology-agnostic limits look simple and have had significant impact in practice, for example Aaronson explains why NP-hardness is unlikely to be circumvented by through physics [\cite=Aaronson05]. Limits to parallel computation became prominent after CPU speed levelled off ten years ago. They suggest using faster interconnect [\cite=Davis01], local computation that reduces communication [\cite=Demmel13], time-division multiplexing of logic [\cite=Hatfill10], architectural and algorithmic techniques [\cite=Dror11], solving larger problem instances, and altering applications to embrace parallelism [\cite=Padua11]. John Gustafson advocates a natural selection: the survival of applications fittest for parallelism. In another twist, the performance and power consumption of industry-scale distributed systems is often described by probability distributions, rather than single numbers [\cite=Dean13] [\cite=Barroso13], making it harder to even formulate appropriate limits. We also cannot yet formulate fundamental limits related to the complexity of the software-development effort, the efficiency of CPU caches [\cite=Jouppi11], and computational requirements of incremental functional verification, but we have noticed that many known limits are either loose or can be circumvented, leading to secondary limits. To wit, the P  ≠  NP limit is worded in terms of worst-case rather than average-case performance, and has not been proven despite heavy evidence. Researchers have ruled out entire categories of proof techniques as insufficient to complete such a proof [\cite=Fortnow09] [\cite=Aaronson09]. While esoteric, such tertiary limits can be effective in practice -- in August 2010, they helped researchers quickly invalidate Vinay Deolalikar's highly-technical attempt at proving P  ≠  NP. On the other hand, the correctness of lengthy proofs for some key results could not be established with acceptable level of certainty by reviewers, prompting efforts in verifying mathematics by computation [\cite=Avigad14].

In summary, we have reviewed what is known about limits to computation, including existential challenges arising in the sciences, design and optimization challenges arising in engineering, as well as current state of the art. These categories are closely linked due to the rapid pace of technology development. When a specific limit is approached and obstructs progress, understanding its assumptions is a key to circumventing it. Some limits are hopelessly loose and can be ignored, while other limits remain conjectured based on empirical evidence and may be very difficult to establish rigorously. Such limits on limits to computation deserve further study.

Acknowledgments. This work was supported in part by the Semiconductor Research Corporation (SRC) Task 2264.001 (funded by Intel and IBM), US Airforce Research Laboratory Award FA8750-11-2-0043, and US National Science Foundation (NSF) Award 1162087.