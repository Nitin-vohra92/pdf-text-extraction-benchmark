On the Use of Deep Learning for Blind Image Quality Assessment

Introduction

Digital pictures may have a low perceived visual quality. Capture settings, such as lighting, exposure, aperture, sensitivity to noise, and lens limitations, if not properly handled could cause annoying image artifacts that lead to an unsatisfactory perceived visual quality. Being able to automatically predict the quality of digital pictures can help to handle low quality images or to correct their quality during the capture process [\cite=bovik2013automatic]. An automatic Image Quality Assessment (IQA) algorithm, given an input image, tries to predict its perceptual quality. The perceptual quality of an image is usually defined as the mean of the individual ratings of perceived quality assigned by human subjects (Mean Opinion Score - MOS).

In recent years, many IQA approaches have been proposed. They can be divided into three groups, depending on the additional information needed: Full-Reference Image Quality Assessment (FR-IQA) algorithms e.g. [\cite=eckert1998perceptual] [\cite=winkler1999issues] [\cite=pappas2000perceptual] [\cite=wang2003objective] [\cite=wang2004image] [\cite=bianco2009image], Reduced-Reference Image Quality Assessment (RR-IQA) algorithms, and No-reference/Blind Image Quality Assessment (NR-IQA) algorithms e.g. [\cite=moorthy2011blind] [\cite=mittal2012no] [\cite=mittal2013making]. NR-IQA algorithms can be further classified into two main sub-groups: to the first group belong those targeted to estimate the presence of a specific image artifact (i.e. blur, blocking, grain, etc.) [\cite=ciancio2011no] [\cite=corchs2014no]; to the second group the ones that estimate the overall image quality and thus are distortion generic [\cite=mittal2015no] [\cite=seshadrinathan2011automatic] [\cite=bovik2013automatic] [\cite=ciocca2014assess]. In this work we focus on distortion-generic NR-IQA.

Most of the distortion-generic methods estimate the image quality by measuring deviations from Natural Scene Statistic (NSS) models [\cite=bovik2013automatic] that capture the statistical "naturalness" of non-distorted images. These models are based on the two following principles: i) good quality real-world photographic images obey certain perceptually relevant statistical laws; ii) common image distortions alter such statistical laws. The Natural Image Quality Evaluator (NIQE) [\cite=mittal2013making] is based on the construction of a quality aware collection of statistical features based on a space domain NSS model. The Distortion Identification-based Image Verity and INtegrity Evaluation (DIIVINE) index [\cite=moorthy2011blind] is based on a two-stage framework for estimating quality based on NSS models, involving distortion identification and distortion-specific quality assessment. The core of the method uses a Gaussian scale mixture to model neighboring wavelet coefficients. C-DIIVINE [\cite=zhang2014c] is an extension of the DIIVINE algorithm in the complex domain, and blindly assesses image quality based on the complex Gaussian scale mixture model corresponding to the complex version of the steerable pyramid wavelet transform. The BLIINDS-II [\cite=saad2012blind] method, given an input image, computes a set of features and then uses a Bayesian approach to predict quality scores. Such features are obtained by transforming the model parameters of a generalized NSS-based model of local Discrete Cosine Transform coefficients into a vector of features.

The Blind/Referenceless Image Spatial QUality Evaluator (BRISQUE) [\cite=mittal2012no] operates in the spatial domain and is also based on a NSS model. The algorithm quantifies possible losses of naturalness in the image due to the presence of distortions.

The use of a database of images along with their subjective scores, such as the LIVE In the Wild Image Quality Challenge Database [\cite=Ghadiyaram2016], is fundamental for both the design and the evaluation of IQA algorithms. Recent approaches to the blind image quality assessment problem use human provided quality scores with machine learning to learn a quality measure. The Feature maps based Referenceless Image QUality Evaluation Engine (FRIQUEE) [\cite=Ghadiyaram2014] [\cite=Ghadiyaram2016] combines a deep belief net and a SVM to predict image quality. Tang et al. [\cite=tang2014blind] define a simple radial basis function on the output of a deep belief network to predict the perceived image quality. They first pre-train the network in an unsupervised manner and then fine-tune it with labeled data. Finally they model the quality of images exploiting a Gaussian Process regression. Hou et al. [\cite=hou2015blind] propose to represent images by NSS features and to train a discriminative deep model to classify the features into five grades (i.e. excellent, good, fair, poor, and bad). Quality pooling is then applied to convert the qualitative labels into scores. In [\cite=lv2015difference] a model is proposed which uses local normalized multi-scale Difference of Gaussian (DoG) response as feature vectors. Then, a three-steps framework based deep neural network is designed and employed as pooling strategy. Ye et al. [\cite=ye2013real] presented a supervised filter learning based algorithm that uses a small set of supervised learned filters and operates directly on raw image patches. Later they extended their work using a shallow convolutional neural network [\cite=kang2014convolutional].

This paper investigates the use of deep learning for distortion generic NR-IQA. More precisely, we use CNN as feature extractor on top of which we exploit a Support Vector Regression (SVR) machine [\cite=vapnik1998statistical] [\cite=cristianini2000introduction] to learn the mapping function from the image features to the perceived quality scores. We evaluate the effect of several design choices: i) the use of different CNNs that are pre-trained on different image classification tasks; ii) the use of a number of different image sub-regions (opposed to the use of the whole image) as well as the use of different strategies for feature and score predictions pooling; iii) the use of a CNN that is fine-tuned for image quality assessment.

The experiments are conducted on the LIVE In the Wild Image Quality Challenge Database which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices [\cite=ghadiyaram2014crowdsourced]. The result of this study is a CNN suitably adapted to the blind quality assessment task that accurately predicts the quality of images with a high agreement with respect to human subjective scores.

The rest of the paper is organized as follows: Section II introduces the proposed approach and the different design choices considered; Section III describes the data, evaluation metrics, and the experimental setup. Section IV analyzes the experimental results. Finally, Section V presents our final considerations.

Deep Learning for blind image quality assessment

Deep Convolutional Neural Networks (CNNs) are a class of learnable architectures used in many image domains [\cite=lecun2015deep] [\cite=razavian2014cnn] such as recognition, annotation, retrieval, object detection, etc. CNNs are usually composed of several layers of processing, each involving linear as well as non-linear operators that are jointly learned in an end-to-end manner to solve a particular task.

A typical CNN architecture consists of a set of stacked layers: convolutional layers to extract local features; point-wise non-linear mappings; pooling layers, which aggregates the statistics of the features at nearby locations; and fully connected layers. The result of the last fully connected layer is the CNN output. CNN architectures vary in the number of layers, the number of outputs per layer, the size of the convolutional filters, and the size and type of spatial pooling. CNNs are usually supervisedly trained by means of standard back-propagation [\cite=lecun2012efficient].

In practice, very few people train an entire CNN from scratch, because it is relatively rare to have a dataset of sufficient size. Instead, it is common to take a CNN that is pre-trained on a different large dataset (e.g. ImageNet [\cite=deng2012imagenet]), and then to use it either as a feature extractor or as an initialization for a further learning process (i.e. transfer learning, known also as fine-tuning [\cite=yosinski2014transferable] [\cite=bengio2012deep]). In this work, we use the Caffe network architecture [\cite=jia2014caffe] (inspired by the AlexNet [\cite=krizhevsky2012imagenet]) as a feature extractor on top of which we exploit a Support Vector Regression (SVR) machine [\cite=vapnik1998statistical] [\cite=cristianini2000introduction] with a linear kernel to learn a mapping function from the CNN features to the perceived quality scores (i.e. MOS).

Given an input image, the CNN performs all the multilayered operations and the corresponding feature vector is obtained by removing the final softmax nonlinearity and the last fully-connected layer. The length of the feature vector is 4096. A graphical representation of the described approach is reported in Figure [\ref=Exp1_pipeline].

In this work we evaluate the effect of several design choices for feature extraction, such as: i) the use of different CNNs that are pre-trained on different image classification tasks; ii) the use of a number of different image sub-regions (opposed to the use of the whole image) as well as the use of different strategies for feature and score prediction pooling; iii) the use of a CNN that is fine-tuned for category-based image quality assessment.

Image description using pre-trained CNNs

Razavian et al. [\cite=razavian2014cnn] showed that the generic descriptors extracted from convolutional neural networks are very powerful and their use outperforms hand crafted, state-of-the-art systems in many visual classification tasks. Within the approach depicted in Figure [\ref=Exp1_pipeline], our baseline consists in the use of off-the-shelf CNNs as feature extractors. Features are computed by feeding the CNN with the whole image, that must be resized to fit its predefined input size (see Figure [\ref=Exp2_ROIs].a).

We experiment the use of three CNNs that have been pre-trained on three different image classification tasks:

ImageNet-CNN, which has been trained on 1.2 million images of ImageNet (ILSVRC 2012) for object recognition belonging to 978 categories;

Places-CNN, which has been trained on  2.5 million images of the Places Database for scene recognition belonging to 205 categories;

Hybrid-CNN [\cite=zhou2014learning], which has been trained using 3.5 million images from 1,183 categories, obtained by merging the scene categories from Places Database and some object categories from ImageNet.

Feature and prediction pooling strategies

In the previous design choice, we resized the image to match the predefined CNN input size. Since the resizing operation can mask some image artifacts, we consider here a different design choice in which CNN features are computed on multiple sub-regions (i.e. crops) of the input image. Crops dimensions are chosen to be equal to the CNN input size so that no scaling operation is involved (see Figure [\ref=Exp2_ROIs].b).

We experiment the use of a different number randomly selected sub-regions, ranging from 5 to 50. The information coming from the multiple crops has to be fused to predict a single quality score for the whole image. Different fusion strategies are experimented:

feature pooling: information fusion is performed element by element on the sub-region feature vectors to generate a single feature vector for each image (see Figure [\ref=Exp2_Pooling].a). Minimum, average, and maximum feature pooling are considered.

feature concatenation: information fusion is performed by concatenating the sub-region feature vectors in a single longer feature vector (see Figure [\ref=Exp2_Pooling].b).

prediction pooling: information fusion is performed on the predicted quality scores. The SVR predicts a quality score for each image crop, and these scores are then fused using a minimum, average, or maximum pooling operator (see Figure [\ref=Exp2_Pooling].c).

Image description using a fine-tuned CNN

Convolutional neural networks usually require millions of training samples in order to avoid overfitting. Since in the blind image quality assessment domain the amount of data available is not so large, we investigate the fine-tuning of a pre-trained CNN exploiting the available NR-IQA data. When the amount of data is small, it is likely best to keep some of the earlier layers fixed and only fine-tune some higher-level portion of the network. This procedure, which is also called transfer learning [\cite=yosinski2014transferable] [\cite=bengio2012deep], is feasible since the first layers of CNNs learn features similar to Gabor filters and color blobs that appear not to be specific to a particular image domain; while the following layers of CNNs become progressively more specific to the given domain [\cite=yosinski2014transferable] [\cite=bengio2012deep].

We start the fine-tuning procedure to the image quality assessment task, by substituting the last fully connected layer of a pre-trained CNN with a new one initialized with random values. The new layer is trained from scratch using the back-propagation algorithm [\cite=lecun2012efficient] with the available data for image quality assessment. In this work, image quality data are a set of images having human average quality scores (i.e. MOS).

The CNN is discriminatively fine-tuned to classify image sub-regions into five classes corresponding to five image quality grades. The five classes are obtained by a crisp partition of the MOS: bad (score ∈[0,20]), poor (score ∈]20,40]), fair (score ∈]40,60]), good (score ∈]60,80]), and excellent (score ∈]80,100]). Once the CNN is trained, it is used for feature extraction within the approach depicted in Figure [\ref=Exp1_pipeline], just like one of the pre-trained CNNs.

Image Database and evaluation criterions

We evaluate the different design choices within the proposed approach on the LIVE In the Wild Image Quality Challenge Database [\cite=ghadiyaram2014crowdsourced] [\cite=Ghadiyaram2016]. It contains 1,162 images with resolution equal to [formula] pixels affected by diverse authentic distortions and genuine artifacts such as low-light noise and blur, motion-induced blur, over and underexposure, compression errors, etc. Figure [\ref=LBIQCdb_samples] shows some database samples. Database images have been rated by many thousands of subjects via an online crowdsourcing system designed for subjective quality assessment. About 280,000 opinion scores from over 5,000 subjects have been gathered. The Mean Opinion Score (MOS) of each image is computed by averaging the individual ratings across subjects. The MOS is used as ground truth quality score.

We compared the different design choices within the proposed approach with a number of leading blind IQA algorithms. Since most of these algorithms are machine learning-based training procedures, following [\cite=Ghadiyaram2016] in all the experiments we randomly split the data into 80% training and 20% testing sets, using the training data to learn the model and validating its performance on the test data. To mitigate any bias due to the division of data, the random split of the dataset is repeated 10 times. For each repetition we compute the Pearson’s Linear Correlation Coefficient (LCC) and the Spearman’s Rank Ordered Correlation Coefficient (SROCC) between the predicted and the ground truth quality scores, reporting the median of these correlation coefficients across the 10 splits. In all the experiments we use the Caffe open-source framework [\cite=jia2014caffe] for CNN training and feature extraction, and the LIBLINEAR library [\cite=fan2008liblinear] for SVR training.

Experimental results

In this section we evaluate the performance of each design choice introduced in Section II.

Experiment I: Image description using pre-trained CNNs

We extract the 4096-dimensional features from the fc7 layer of the pre-trained ImageNet-CNN, Places-CNN and Hybrid-CNN. Since these CNNs require an input with a dimensionality equal to [formula] pixels, we rescale the original [formula] images to [formula] keeping aspect ratio, and then we crop out the central [formula] sub-region from the resulting image. All the images are pre-processed by subtracting the mean image. The median LCC and SROCC over the 10 train-test splits are reported in Table [\ref=Exp1_results]. From the results it is possible to see that Hybrid-CNN outperforms both Imagenet-CNN and Places-CNN, with Places-CNN giving the worst performance.

Experiment II: Feature and prediction pooling strategies

In the previous experiment the resize operation could have reduced the effect of some artifacts, e.g. noise. In order to keep unchanged the distortion level we evaluate the performances of features extracted from a variable number of randomly cropped [formula] sub-regions from the original image. Given the results of the previous experiment, the only features considered here are those extracted using the Hybrid-CNN.

We evaluate three different fusion schemes for combining the information generated by the multiple sub-regions to obtain a single score prediction for the whole image. The first scheme is feature pooling, where information fusion is performed element-wise on the feature vectors: this can be seen as an early fusion approach, in which a single feature vector is generated and given as input to the SVR to predict a single quality score for the whole image. Three different pooling operators are considered: minimum, average, and maximum.

The second scheme is feature concatenation, where information fusion is achieved by concatenating the multiple feature vectors into a single feature vector with higher dimensionality.

The third scheme is prediction pooling, where information fusion is performed on the predicted quality scores: this can be seen as a late fusion approach, in which a score is predicted from each crop. Three different pooling operators are considered to combine the multiple scores: minimum, average, and maximum pooling.

In all the experiments the number of random crops is varied between 5 and 50 in steps of 5. The numerical values for the best configurations of each fusion scheme (across pooling operators and number of crops) are reported in Table [\ref=Exp2_table_results], while Figure [\ref=Exp2_results] shows their plots for LCC and SROCC with respect to the number of crops considered. From the plots it is possible to see that feature pooling conveys the best results. Prediction pooling is able to give comparable results with those of feature pooling only when a small number of crops is considered. Finally, feature concatenation gives the worst results, giving comparable results with those of prediction pooling only when a large number of crops is considered.

Experiment III: Image description using a fine-tuned CNN

In all previous experiments we use pre-trained CNNs for feature extraction. In this experiment instead, we fine-tune the Hybrid-CNN for the NR-IQA task. The CNN is discriminatively fine-tuned to classify image crops into five distortion classes (i.e. bad, poor, fair, good, and excellent) obtained by crisp partitioning the MOS into five disjoint sets. Since the number of images belonging to the five sets is uneven (see Figure [\ref=LBIQCdb_samplesdistr]), during training we give larger weights to images belonging to less represented distortion classes [\cite=ting2002instance] [\cite=zhou2006training]. Weights are computed as the ratio between the frequency of the most represented class and the frequency of the class to which the image belongs. On the NR-IQA task this weighting scheme gives better results compared to batch-balancing (i.e. assuring that in each batch all the classes are evenly sampled) since it guarantees more heterogeneous batches.

Given the results of the previous experiments, we only evaluate the performance of the fine-tuned CNN with feature pooling and prediction pooling with average operator. The numerical values for the best configurations are reported in Table [\ref=Exp4_table_results], while Figure [\ref=Exp4_results] shows their plots for LCC and SROCC with respect to the number of crops considered. From the plots it is possible to notice that prediction pooling conveys the best results for whatever is the number of crops considered.

Discussion and conclusions

In Table [\ref=results] we compare the results of the different instances of the proposed approach, that we name DeepBIQ, with those of some NR-IQA algorithms in the state of the art. From the results it is possible to see that the use of a pre-trained CNN on the whole image is able to give slightly better results than those of the state of the art. The use of multiple crops with average-pooled features is able to improve LCC and SROCC with respect to the best method in the state of the art by 0.08 and 0.11 respectively. Finally the use of the fine-tuned CNN with multiple image crops and average-pooled predictions is able to improve LCC and SROCC by 0.20 and 0.21 respectively.

Error statistics may not give an intuitive idea of how well a NR-IQA algorithm performs. On the other hand, individual human scores can be rather noisy. Taking into account that the LIVE In the Wild Image Quality Challenge Database gives for each image the MOS as well as the standard deviation of the human subjective scores, to have an intuitive assessment of DeepBIQ performance we proceed as follows: we divide the absolute prediction error of each image by the standard deviation of the subjective scores for that particular image. We then build a cumulative histogram and collect statistics at one, two, and three standard deviations. Results indicate that 97.2% of our predictions are below σ, 99.4% below 2σ and 99.8% below 3σ. Assuming a normal error distribution, this means that in many cases the image quality predictions made by DeepBIQ are closer to the average observer than those of a generic human observer.