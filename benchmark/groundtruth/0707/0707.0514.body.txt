=1

Phase space methods and psychoacoustic models in lossy transform coding

Introduction

In lossy transform coding, a signal is transformed before re-quantization, and then partially recovered by applying the inverse transformation. In perceptual codecs, the goal is to make the necessarily introduced noise imperceptible. Mathematically, let ψ be the original signal, [formula] (for "key") be a linear transformation, and L̂  =  - 1 (for "lock") be its inverse. Then

[formula]

is the recovered signal. Here, [formula] is a quantization operator. It is not a linear operator, but we can often model [formula] as introducing noise:

[formula]

where X is a time series of uniformly distributed independent random variables on the interval ( - 1 / 2,1 / 2), and the constant aQ is determined by the quantization scale. Hence, for the reconstituted signal as above, the introduced noise is

[formula]

The noise is no longer white, but rather shaped by the operator [formula].

A good psychoacoustic model will determine whether this noise is masked by ψ, and a good lossy encoding algorithm will choose [formula] so that it minimizes the combined storage requirements of the key [formula] and the encoded signal - 1ψ, subject to the constraint that the introduced noise cannot be heard.

In this paper, I extend the types of transformations to include pseudo-differential operators. In language of signal processing, a pseudo-differential operator on a sampled signal is a matrix with limited extent off its diagonal and a limited rate of change along the diagonal; one could also call it a slowly evolving filter. As far as I can tell, the community has not used pseudo-differential operators in transform codecs, because, I would guess, they are not diagonal in any standard basis and are therefore more difficult to invert. The phase space theory of these operators, below, resolves the presumed difficulties and brings pseudodifferential operators into the realm of practical transforms.

The Weyl symbol

A symbol correspondence is a bijection between operators (here, on signals) and functions on the corresponding classical phase space (here, the time-frequency plane). The canonical symbol correspondence is the Weyl [\cite=Weyl] symbol. It enjoys many properties that entitle it to be called "the" phase space representation of an operator, and is defined as follows. If Â is an operator with t-space matrix elements 〈t1|Â|t2〉, its Weyl symbol (sÂ) is a function of t and f defined by

[formula]

That is, the Weyl symbol is the Fourier transform of the matrix in its difference variable.

Some examples and properties of s:

If Î is the identity operator, then (sÎ)(t,f) = 1.

If Â is diagonal in the t-representation with 〈t|Â|t〉 = a(t), then (sÂ)(t,f) = a(t). If Â is diagonal in the f-representation, with diagonal elements b(f), then (sÂ)(t,f) = b(f). This parallel is part of a larger metaplectic covariance of the formalism.

If ψ is a signal, then we can form the one dimensional projection operator Â  =  |ψ〉〈ψ|. The Weyl symbol of this operator is called the Wigner [\cite=Wigner] function, which, in signal processing, is a type of spectrogram. It is typically a rapidly varying function on phase space. Generally speaking, the Wigner function contains too much information to be of use for our purposes. However, various smoothings of it are valuable.

The Weyl symbol allows us to regard operators as functions on phase space. However, in order to use these functions, we need to know what happens to operator multiplication. It becomes the star product, defined by

[formula]

The star product acts generally through an integral kernel Δ, called the trikernel:

[formula]

where z = (t,f) etc, and Δ, not needed elsewhere, is an exponential of the area spanned by the triangle with vertices (z,z1,z2). The trikernal formula is useful in some contexts, such as when B̂ is a Wigner function, but is often too complicated to be of value. However, there is a class of functions we will call slowly varying, for which a simpler expression holds. (These functions are pseudo-differential operators.)

If A and B are slowly varying symbols, then their star product can be expanded as series of bidifferential operators, called the Moyal [\cite=Moyal] star product:

[formula]

where the "Janus" operator

[formula]

Here, derivatives topped with left (right) arrows act to the left (right, resp.) Conversely, if the Moyal series converges for [formula], then they are slowly varying.

In the case of extremely slowly varying functions, the star product is well-approximated by its leading term, the ordinary product. This is important: it means for the right operators, the Weyl transform maps complicated operator multiplication to simple ordinary multiplication.

How do we know a priori whether a function is slowly varying? One way is to consider sets of functions having rigorous bounds on the ratio of higher terms in the Moyal series to the leading term. On such set is the set of bounded variation. We say A(t,f) is of bounded variation, with length scales (at,af), if

[formula]

With this definition, it is easy to prove that if A and B are of bounded variation and 2πataf  ≫  1, then the Moyal series converges. In other words, the area of the characteristic scale of variation must be much larger than a Planck cell. This is a direct consequence of time-frequency uncertainty.

We can create functions of bounded variation by using a [formula] kernel: if

[formula]

and A(t,f) is a positive-valued function, then the convolution [formula] is of bounded variation, with scales (at,af). Note the convolution is on the whole phase space, rather than just in t or f.

Finally, we introduce an important formula for the symbol of a function of an operator (the sofoo formula), i.e., sf(Â). For example, given Â or its symbol A = sÂ, we will need to know sÂ1 / 2 and sÂ- 1. Fortunately, the subject was considered at length by Gracia-Saz [\cite=graciasaz]. The general formula is quite complicated and expressed in terms of a series of diagrams. For our purposes, the important facts are these: first, which follows directly from the Moyal product, that

[formula]

and the higher order terms at right involve successively more derivatives of sÂ; second, that those derivative terms contain t and f derivatives in pairs. Thus, if A is of bounded variation, then the series for sf(Â) is well-approximated by its first term. However, even A is not of bounded variation, then the series might still converge; in particular, we might imagine varying over phase space the smoothing scales (at,af) while maintaining a constant product.

The first correction term to Eq. ([\ref=sfa]) is

[formula]

The diagrams in [\cite=graciasaz] and [\cite=cargo_thesis] help express this result more economically. This formula may find application below when the first term alone is not accurate enough.

Phase space description of noise.

Fully understanding a times series of random variables Y(t), requires knowing its entire joint probability distribution. However, for most purposes, it suffices to study only the two point correlation function, that is, the expectation of a product at two times:

[formula]

assuming the variables have individual means of zero. This expectation allow us to define a Hermitian noise operator N̂Y, whose t space matrix elements are as above, i.e., 〈t1|N̂Y|t2〉 = E(Y(t1)Y(t2)*).

We will characterize a given noise operator by its Weyl symbol. For example, the noise operator for white noise X(t) with unit variance is the identity operator, and hence its symbol is unity--which makes sense. The random variable series for time localized noise is Y = WX, where W is a window. It follows that the Weyl symbol for this noise is just W(t)2. Colored noise, on the other hand, is defined by its Fourier transform: FY = WX, and its Weyl symbol is W(f)2.

The converse problem is to produce noise with a given (Hermitian) noise operator N̂, and the solution is simple: defining Y = N̂1 / 2X gives the desired noise, since

[formula]

Here we use for white noise that E(|X〉〈X|) = Î. As a practical matter, finding the square root of a given operator may not be so easy. However, if the operator has a slowly varying Weyl symbol N, the matter is straightforward: N̂1 / 2 = s- 1sN̂1 / 2  ≈  s- 1N1 / 2; that is, we simply take the square root of the noise operator's symbol, and convert it back to an operator using the inverse symbol. Note that this procedure does require that N is a positive function.

As a psychoacoustical matter, can we always describe noise by a slowly varying operator? Consider, for example, noise created from white noise by applying rapidly varying operators. In particular, the one-dimensional projector |ψ〉〈ψ| onto a signal ψ is definitely not a slowly varying operator, and, when applied to white noise, gives a signal proportional to ψ itself--the only randomness left is in the overall norm of the signal. Roughly speaking, the more rapidly an operator Â varies, the more structure it imparts to a white noise signal X, and the less noisy the resulting signal sounds.

Phase space setting for masking of noise.

Psychoacoustical experiments of signals masking noise are consistent with the hypothesis that the maximal noise masked by a given signal is a slowly varying noise operator. Masking experiments, except for those done informally in the testing of compression algorithms, are typically done in time or frequency, but not both. The classic paper by Ehmer [\cite=ehmer] shows masking curves of noise by pure tones. The curves typically peak at the tone frequency and fall off at a scale proportional to the frequency itself, but faster toward decreasing frequency. Temporal masking experiments show pre-masking rising to a certain threshold under the signal, and decaying afterward.

We can generalize these results to a broader hypothesis: For a given signal ψ, there exists a noise operator ψ, such that ÂX is fully masked by ψ whenever s(Â2) is strictly less than Mψ = sψ. In other words, ψ generates a phase space profile for the maximum allowed noise.

This phase space profile must be related to the phase space profile of ψ itself--but how? We have already mentioned that the Wigner function is typically not useful for phase space analysis. To begin, it is not slowly varying. Moreover, it sometimes falls below zero. We can guess, however, that the masking noise profile of ψ might be related to a smoothing of the Wigner function over phase space. One particular smoothing of the Wigner function gives another well-known phase space distribution--the coherent state representation.

The coherent state representation Cψ(t0,f0) is defined as follows. Using the moving gaussian window wt0 with width a defined by

[formula]

we define

[formula]

where F is the Fourier transform. The coherent state representation depends on the parameter a, making it less canonical than the Wigner function; on the other hand, the finite width of the window makes it much easier to calculate. The coherent state representation, regarded as a symbol of an operator, is not slowly-varying, but it does vary more slowly than the Wigner function, and it is never negative.

Our hypothesis, then, is that sψ is related to a smoothing of Cψ (which is itself a smoothing of s(|ψ〉〈ψ|)) with normalization and width parameters determined by listening tests. The future full theory will take into account different masking widths at different frequencies as well as the statistical properties of Cψ, in order to account for the known assymmetry between the masking of noise by tones and the masking of noise by noise.

In the meantime, I have explored simplified theories that, though yielding sub-maximal phase space noise thresholds Mψ, nevertheless condemn to obscurity noise operators whose symbols fall below them. I will call such noise operators noise-confining operators; the goal for more sophisticated psycho-acoustical models will be an algorithm for generating the maximal noise-confining operator--however, as we shall see, a sub-maximal noise-confining operator can still be useful.

Finding a noise-confining operator is straightforward. For a signal ψ, I smoothed Cψ by convolving it with the [formula] kernels of Eq. ([\ref=sech_kernel]) in order to produce an easily manipulated function Sψ of bounded variation. I used width parameters suggested by masking experiments. To test the theory, I took s- 1(S1 / 2ψ), and, as explained in section [\ref=section_noise], applied this operator to a noisy signal x (a realization of the uniformly distributed random variable series X). I then listened to

[formula]

and increased α to the threshold at which the above began to sound different from ψ. By repeating this for different signals, and choosing the smallest α, I became confident that

[formula]

did indeed describe a noise-confining operator.

Phase space codec

In the previous section, I introduced an explictly phase phase space setting for the signal dependent threshold of noise, and we will now use it to design a lossy transform codec. First, a note about normalization. I will assume that the original signal ψ and the encoded signal ψencoded are both quantized on a unit scale. The quantization noise X present in ψencoded may, under certain conditions, be described as uniformly distributed on the interval ( - 1 / 2,1 / 2) with variance 1 / 12. We have, for the encoded and restored signals, that

[formula]

Comparing Eqs. ([\ref=codec]) and ([\ref=psymodel]), we set

[formula]

so that the noise introduced into ψrestored is just at the threshold measured by the psychoacoustical model.

I now present the argument showing how [formula] reduces the average bit content of ψencoded. I use an empirical observation that the values taken by ψencoded are uniformly distributed over its range, but the argument does generalize easily to more general distributions. If this assumption is true, we can estimate the average size of ψencoded from its variance:

[formula]

Here [formula] takes the trace of its operand, and, in the last line we have used the traciality property of the Weyl symbol, namely, that

[formula]

On the right, it is important to note the ordinary product appears rather than the star product. The traciality property converts the mean over t space into a mean over phase space. In the last of Eq. ([\ref=mean_calc]), we have divided by the phase space volume as a formal way to avoid worrying about the normalization factor in Eq. ([\ref=traciality]). Now, in the numerator integrand, the slowly varying function M- 1ψ appears next to the rapidly varying Wigner function s(|ψ〉〈ψ|). To a good approximation, then, we may replace the Wigner function by its average value within the variation scale of M- 1ψ. This average is, of course, Sψ. Thus, if we are working with the simplified model where Mψ  =  α2Sφ, we find the expectation

[formula]

Using the information theoretic definition of entropy we can convert this into a bit rate. Since we have not yet used that ψencoded is uniformly distributed, we can afford to make a more general argument in which ψencoded, before quantization, takes its values from a probability density p(ψ)dψ. Quantization casts its values into bins i of width q( = 1), and the probability that φ falls within the i'th bin is Pi, where

[formula]

where we have used q = 1. The entropy per sample is

[formula]

Thus, if p(φ) is uniformly distributed with standard deviation σ

[formula]

Or, if p(φ) is gaussian,

[formula]

Now, σ itself is obtained from φ, leading to

[formula]

where At denotes a phase space average over a time scale equal large enough to quell rapid variations in the result. This formula, when plugged into either Eq. ([\ref=uniform_sigma]) or Eq. ([\ref=gaussian_sigma]), as appropriate, gives an expression for the time-dependent number of bits consumed by ψencoded. This formula evidently defines a phase space measure of perceptual entropy.

When, as in our simple model, σ  ≈  1 / α, we find

[formula]

so that the lossy stage of this encoding scheme takes no more than 5 bits per sample.

As for the coding, we may again employ the considerable power of the sofoo formula and approximate

[formula]

That is, we simply invert the masking threshold Mψ, take its square root, and apply the inverse Weyl symbol. This procedure ignores higher order terms in the exact expression for [formula]'s inverse. If this is not accurate enough, we can always write the operator more accurately by using the higher order terms in the sofoo formula. (And this is okay, since time is the luxury of the coder.)

Summary of the codec so far

Through listening tests, we refine a phase space theory for the signal dependent threshold of noise. The outcome is a mapping from ψ to a noise operator ψ. We define a key operator   =  s- 1M1 / 2ψ and send it off to a bit packing (entropy coding) algorithm for further compression. Using the symbol of a function of an operator formula, we define the lock operator L̂  =  s- 1M- 1 / 2ψ, apply it to ψ, quantize the result, and deliver it also for bit packing. This is the coding. As for decoding, we unpack the key and the encoded signal and then apply the key operator to it.

Practical issues and modifications

In this section, we introduce two modifications which would have cluttered the earlier presentation.

Existing perceptual codecs, in addition to exploiting masking phenomena, also use that much of the high frequency content is irrelevant because we cannot hear it anyway. This fact is easy to put into the phase space framework. Let Ĥ be the noise operator for the frequency dependent threshold of human hearing, i.e., the loudest colored noise that cannot be heard in any circumstances. We can then add Ĥ to ψ in Eq.( [\ref=psymodel]) without changing how ψ sounds. This suggests we take [formula] However, examining the formula for the ψrestored, we see that this key introduces noise that, though inaudible, is independent of the signal itself, meaning that it carries no information. I have found that it works well to keep the Ĥ term in the lock, but drop it from the key. Two choices that work well for the lock are

[formula]

If we use these locks, then even in the not-quantized case, the restored signal is different from the original. In the second lock above, it becomes

[formula]

This expression bears similarity to a Wiener filter in [\cite=kirschauer].

Using this type of lock can significantly increase the subsequent lossless compressibility of ψencoded. However, the improvement is not the same for all signals; those with significant high frequency content retain their original compressiblity.

This brings us to the second deviation from the prior setup. So far, we have always written ψencoded in the time domain, but this is a problem because ψencoded is not as compressible there. Even lossless compressors designed for the time domain, such as ones using linear prediction coefficients, do not perform as well as LZ or Huffman encoding in the frequency domain. I have therefore found it better to package the encoded signal in DFT'd chunks. To avoid the errors caused by quantizing twice, I delay the quantization of the encoded signal until after it has been Fourier transformed. This is valid because white noise is white in both the time and frequency domain. However, one must be careful to use a suitably large FFT. If the chunks are too small, frequency localization in the DFTs can cause the quantization noise assumptions to break down and introduce a noticeable warble to the decoded signal. I find that chunks of 512 or 1024 samples work best. In this format, standard compression programs (like gzip) reduce monophonic samples at 44kHz from 4 to 12 percent of their original size. However, this does not include the storing the key.

Storing the key

The key spectrogram in this method takes the place of scale factor side information in standard lossy codecs. Naïve lossless compression of a sampled key spectrogram yields disappointing results. Even though it is a smooth object with variations on the order of 40 times the minimum uncertainty scale ΔfΔt  =  1, there is simply too much overhead in storing values at every point, or differences between them--I have tried just about everything. In compression of monophonic samples, no such lossless method made the key file take less than 10 percent of the sample size. I have realized that, in order to make this method competitive, we must regard as only a suggestion that the key noise operator should be equal to the measured masking operator. Of course, if we make the key bigger than that operator, we will no longer be in the noise confined regime. Conversely, smaller keys sacrifice some of the available entropy. However, I have found that the key can be stored at a fractional accuracy of 10 percent without substantially introducing audible noise or degrading the compressibility.

This lattitude allows us to store the key as an interpolated object where the value at each knot is specified with only one byte. Specifically, I have used an adaptive grid by allowing for variable time steps and then, for each selected time, sampling the slice at a time-specific frequency step size. The step sizes for the adaptive grid are chosen as large as possible subject to the constraint that linear spline over it differs fractionally from the original key by no more than 10 percent. The step size information, together with the values at the spline knots, comprise a much smaller object: they reduce the overhead to less than 1 percent. One might think, given my earlier emphasis on using functions with bounded variation so that the sofoo formula applies, that the obvious discontinuities introduced by this method would cause the whole framework to fall apart. However, as is often the case in semi-classical analysis, we get more than we deserve using the final results of naïve formal calculations: the method seems to work fine even with only piecewise smooth keys. If, however, in the future, these are found to introduce artifacts, more sophisticated curve fits, such as cubic splines, could be developed, without, I think, sacrificing compressibility. An alternative would be to store an interpolated key with the understanding that it would be smoothed in a standard way after it is reconstituted; the practicality of such an approach would depend on the spare computational overhead in the decode routine.

Conclusion

It is clear that we perceive sound in a time frequency plane, simply because we hear pitch and rhythm. Thus, any psychoacoustic theory should achieve its most natural form in phase space. If I am correct that the maximal noise masked by a given signal is always characterized by a slowly varying (pseudo-differential) noise operator, then this codec can exploit any valid psychoacoustical model. This makes it an attractive framework for directly translating advances in the phenomenology of masking into better lossy data compression. It also offers an interesting perspective on perceptual entropy.

The main practical concern is the processing load of the main decoding loop. In early, fairly unoptimized code, the decode runs faster than real time by a factor of two. The decode loop is O(N), but the coefficient is rather large--on the order of 500. Whether this loop can be implemented in real time on a portable device is beyond my expertise.

I have not presented any suggestions for how this method develops in the stereophonic case. It presents many new and interesting issues, including psychoacoustical modeling of binaural masking effects and matrix-valued spectrograms. I leave these matters to a future paper. In the meantime, I can report that my early attempts at stereophonic compression-- in which I seperately calculate left and right smoothed spectrograms, use them to transform the left and right channels, and then send the transformed mid and side channels to lossless compression--are transparent (informally) at 6 to 13 percent overall compression ratios. It also works to form a single key from the mid channel and use it to encode both the mid and side channels.

On the whole, I am encouraged by the performance at this early stage. The method is quite young, and it clearly has many refinements and tweaks ahead of it. Beyond that, the formalism emphasizes the value of phase space methods in the treatment of noise, masking phenomena, and the measurement of perceptual entropy.

Entropy of phase space noise

I am not sure how the following argument fits in with the earlier entropy result of Eq. ([\ref=bit_rate]), but it is yet another interesting application for the sofoo formula. The entropy S of a series of random variables Yi is

[formula]

where [formula] is the joint probability density. If Y = X, and X is uncorrelated white noise, then we can use that P(Y) transforms as a one form to conclude that

[formula]

The log 2 term being constant, we can integrate out the p.d.f for X and resume as

[formula]

where we have used the traciality property and, in the last, assumed that [formula] is slowly varying. In this context, that ψ can tolerate the addition of noise s- 11 / 2ψX means that it belongs to an ensemble of identical sounding signals, and its information content goes down by the above result. This result differs from the previous in that the original ψ does not appear, and the log 2 is inside the integrand.

Encoding already noisy signals

Suppose ψ already sounds noisy. Then, if the statistics of ψencoded are correct, then the noisy part ψencoded may suffice for a realistic sounding reconstitution of ψ. The argument goes as follows. As usual,

[formula]

We set

[formula]

and use that ψ sounds noisy to approximate

[formula]

leading to

[formula]

Inspection of this equation leads us to guess that

[formula]

so that

[formula]

This one equation does not determine these two proportionalities. We fix this by requiring that [formula] be as large as possible, so that ψencoded be as small as possible. This implies αl  =  0. Thus, when a signal is already noisy, we can take L̂  =  0 and s2  ≈  Sψ. In this extreme case, the entire signal information is contained in the key. Of course, with L̂ = 0, ψencoded  =  0, and, in order that the reconstituted signal sound at all, we need to dither white noise into ψencoded. Real signals will contain a fraction of noise and purer tones, so this extreme case will rarely actually occur; nevertheless, the argument shows that noise can help us increase the overall key scale, and hence the compression ratio for ψencoded. The argument also shows us another case where the lock is not the key's inverse.