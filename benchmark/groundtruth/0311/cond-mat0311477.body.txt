Qualms concerning Tsallis's condition of Pseudo-Additivity as a Definition of Non-Extensivity

Tsallis [\cite=Tsallis] has used the pseudo-additivity condition,

[formula]

where we work in energy units in which k = 1,

[formula]

is the Tsallis entropy [formula], and the probability distribution [formula] is complete. Supposedly [\cite=Tsallis],

A and B are two independent systems in the sense that the probabilities of A + B factorize into those of A and B (i.e., pij(A + B) = pi(A)pj(B)). We immediately see that, since in all cases [formula] (nonnegativity property), α < 1, α = 1 and α > 1 respectively correspond to superadditivity (superextensivity), additivity (extensivity) and subadditivity (subextensivity).

Yet, it appears odd that criteria of super-and sub-additivity can be obtained through a functional equation rather than as a geometric inequality as are the criteria for convexity and concavity. The subadditive property is the triangle inequality  [\cite=BB], whereas the geometrical interpretation of a concave function is one that never rises above its tangent plane at any point. There are classes of functions which are defined by inequalities that are weaker than convexity (concavity) and stronger than superadditivity (subadditivity) [\cite=Bruckner].

To prove that ([\ref=eq:S-Tsallis]) is always subadditive, consider the sum [formula]. Companions to Minkowski's inequalities are [\cite=Hardy]

[formula]

for α > 1, and

[formula]

for 0 < α < 1, where [formula] is another complete distribution. Inequality ([\ref=eq:super]) is the condition for superadditivity; the negative of which is subadditive. Hence, for α > 1 the Tsallis entropy is subadditive. Inequality ([\ref=eq:sub]) is the criterion for subadditivity, and, hence, the Tsallis entropy is subadditive for 0 < α < 1. This is in flagrant contradiction with the above conclusion that the entropy ([\ref=eq:S-Tsallis]) is superadditive for 0 < α < 1 on the basis of ([\ref=eq:p-add]).

The pseudo-additivity relation ([\ref=eq:p-add]) is actually expressed in terms of the complete probability distributions

[formula]

The product of the two sums on the left-hand side does not imply additivity of two systems. Rather, ([\ref=eq:p-add-T]) is a functional equation which determines the form of the entropy. Additive entropies of degree-1 and degree-α can be derived from the first- order homogeneous property of the weighted means [\cite=Hardy]

[formula]

where x stands for a set of discrete variables, [formula]. Since φ is defined up to a constant, we may set

[formula]

Now ([\ref=eq:homo]) stands for

[formula]

On account of the translational invariancy of the weighted mean

[formula]

where [formula] and b are functions of the parameter λ. Since it is apparent from ([\ref=eq:mean-bis]) that ψ(x) = φ(λx), we get

[formula]

However, according to ([\ref=eq:boundary]) we must set b(λ)  =  φ(λ). Letting λ become another positive variable, y, ([\ref=eq:linear]) becomes the functional equation

[formula]

By the law of associativity

[formula]

Setting the right hand sides of ([\ref=eq:f1]) and ([\ref=eq:f2]) equal to one another results in

[formula]

where c is a separation constant. Solving ([\ref=eq:c]) for a(y) and substituting it into ([\ref=eq:f1]) gives the functional equation

[formula]

which is known in information science as the additivity property of degree-α.

If c = 0, it reduces to the classical equation

[formula]

whose most general solution is φ = A log x.

If x stands for the probability p, then the best code length for an input symbol of probability p is

[formula]

where A =  - 1 and ni is the length of sequence i. Then the average length, [formula] corresponds to the average entropy [\cite=Campbell]

[formula]

which is the Shannon entropy, or additive entropy of degree-1.

With [formula], we insert cφ(x) + 1 = g(x) into ([\ref=eq:add]), thereby reducing it to

[formula]

Since the general solution to the functional equation ([\ref=eq:classical] is g = xr, the random entropy function is

[formula]

Since entropies must be positive, we must have c < 0. Moreover, we want ([\ref=eq:cost-bis]) to reduce to ([\ref=eq:cost]) in the limit as - c  →   + 0 so that r =  - c. Hence ([\ref=eq:cost-bis] becomes

[formula]

where we have set c = 1 - α < 0. In the limit [formula] it reduces to ([\ref=eq:cost]). The average of the entropy ([\ref=eq:cost-tris]) is the nonadditive entropy of degree-α [\cite=Havrda] [\cite=Daroczy] [\cite=Vajda] [\cite=Aggarwal] [\cite=Forte] [\cite=Aczel] [\cite=Mathai] [\cite=Tsallis88] ([\ref=eq:S-Tsallis]) which reduces to the additive entropy ([\ref=eq:Shannon]) in the limit as [formula].

There is nothing unique about ([\ref=eq:cost-tris]). If our only concern is that it reduce to ([\ref=eq:cost]) in the limit [formula] then an equally likely candidate is

[formula]

If we average ([\ref=eq:cost-4]) with respect to Q, then we must choose the P in terms of Q such that the new P are normalized. In terms of a variational problem we require

[formula]

where the constraint has been introduced using the Lagrange multiplier, μ. Performing the variation we get pi = (qi  /  μ)1 / α, and in order that P be normalized

[formula]

Introducing these so-called escort probabilities [\cite=Beck] back into ([\ref=eq:cost-4]) and averaging give

[formula]

where we have set α = 1 / β. The first-order homogeneous entropy ([\ref=eq:S-Arimoto]) appears to have been first introduced by Arimoto [\cite=Arimoto], and subsequently given a complete characterization by Boekee and van der Lubbe [\cite=Lubbe].

Just as the Tsallis entropy ([\ref=eq:S-Tsallis]) obeys the pseudo-additivity relation ([\ref=eq:p-add-T]), the Arimoto entropy ([\ref=eq:S-Arimoto])satisfies

[formula]

Following Tsallis' line of reasoning we would conclude from ([\ref=eq:p-add-A]) that ([\ref=eq:S-Arimoto]) is subadditive for β > 1 and superadditive for β < 1. However, according to Minkowski's inequalities [\cite=Hardy]

[formula]

for β > 1, and

[formula]

for β < 1. Thus, [formula] is subadditive for β > 1 and superadditive for β < 1. Since the negative of the former and positive of the latter is used to construct the Arimoto entropy, we conclude that it is always superadditive.

Moreover, concavity follows easily by setting pi  =  λai and qi = (1 - λ)bi, where the sets of numbers ai and bi are nonnegative, and [formula]. Then inequality ([\ref=eq:sub-M]) becomes the criterion for the convexity of [formula],

[formula]

for β > 1, and

[formula]

for β < 1 is the condition for its concavity. Since the Arimoto entropy takes the negative of the former and the positive of the latter it is everywhere concave [\cite=Lubbe].

Likewise, the concavity of the Tsallis entropy ([\ref=eq:S-Tsallis]) can be established from the observations that [formula] is convex for α > 1 and concave for 0 < α < 1. Since the Tsallis entropy is formed from the negative of the former and the positive of the latter, it is concave for all α > 0.

Furthermore, since the entropy is defined to within a constant, classically, the Arimoto entropy ([\ref=eq:S-Arimoto]) is a first-order homogeneous function, and, yet, it obeys the pseudo-additive relation ([\ref=eq:p-add-A]), just like the Tsallis entropy ([\ref=eq:S-Tsallis]) obeys the pseudo-additive relation ([\ref=eq:p-add-T]). Hence, we can conclude that this relation has nothing whatsoever to do with the extensivity properties, or lack thereof, of the nonadditive entropies.