Scalable Bayesian nonparametric regression via a Plackett-Luce model for conditional ranks

, Chris C. Holmes

, Fran cois Caron

Introduction

Bayesian nonparametric regression offers a flexible and robust way of modeling the dependence between covariates x∈X and a response variable [formula] by using models with larger support than their parametric counterparts. Nonparametric statistical models are motivated by robustness and their ability to capture effects such as outliers, strong nonlinearities or multimodalities, while providing probabilistic measures of predictive uncertainty. Bayesian nonparametric regression methods are largely underpinned by one of two random probability measures namely, Dirichlet process mixtures [\citep=Ferguson1973] [\citep=Lo1984] and Pólya trees [\citep=Lavine1992] [\citep=Lavine1994]. These approaches, widely applied to density estimation problems [\citep=Hjort2010], have been used as building blocks of various nonparametric regression models through a number of different approaches.

One approach, called the conditional approach, considers the covariates as fixed, and models directly the conditional distribution f(y|x) of the response given the covariate . This conditional distribution may be constructed in a semiparametric or fully nonparametric way. The semiparametric conditional approach typically assumes that

[formula]

where η is some unknown flexible mean function and ε is the residual. Regression models (priors) have been proposed for the mean function η such as Gaussian processes [\citep=Rasmussen2006], basis function representations such as splines or kernels [\citep=Denison2002] [\citep=Muller2004] or Bayesian regression trees [\citep=Chipman2010]. More generally, [\cite=Kottas2001] and [\cite=Lavine1995] proposed to use Dirichlet process mixtures for the distribution of the residuals, while [\cite=Pati2014] jointly model the mean function and residual distribution using Gaussian processes and probit stick-breaking processes [\citep=Chung2009]. The fully nonparametric conditional approach considers that [formula] takes the form of a mixture model with unknown mixing distribution Px for θ. A prior is set on the family of probability distributions (Px)x∈X. In particular, following the seminal work of [\citet=MacEachern1999], various dependent Dirichlet process models have been proposed in the literature [\citep=Gelfand2003] [\citep=GriffinSteel2006] [\citep=DunsonPillaiPark2007] [\citep=Caron2007] [\citep=Caron2008] [\citep=Dunson2008]. Similarly, [\cite=Trippa2011] define a class of dependent random probability distributions using Pólya trees.

An alternative to the conditional approach is to treat the covariates as random variables and to build a joint statistical model for (X,Y). In this way, one can cast the regression problem as a density estimation one. For example, [\cite=Mueller96] proposed to use Dirichlet process mixtures for the joint distribution of (X,Y). This approach was later extended by [\cite=Shahbaba2009], [\cite=Hannah2011] and [\cite=Wade2014].

A major drawback of current Bayesian methods for semi or nonparametric regression is that many methods do not scale well with the number of samples and/or with the dimensionality of the covariates. In this paper, we propose a novel joint Bayesian nonparametric regression model FX,Y that affords an approximation which can scale easily to large data applications. The model is parameterized in terms of the marginal distributions of the response FY and covariates FX, and then a conditional regression model that utilises the two marginal distributions,

[formula]

where [formula] and [formula] are some nonparametric prior over probability distributions, [formula] is some parametric regression function of the covariates, and Cλβ plays a role similar to a copula in that it takes marginal distributions as inputs and characterises the dependence between them using the function λβ. In particular we consider a Plackett-Luce model for ranks for the regression structure. This construction, detailed in Section [\ref=sec:model], builds on the original Plackett-Luce model [\citep=luce1959] [\citep=plackett1975] for ranking. The positive function λβ tunes the stochastic ordering of the responses given the covariates, the ratio [formula] representing the conditional probability, Pr(Yi  <  Yj|Xi,Xj), that response Yi is less than response Yj given knowledge of {Xi,Xj}. There is thus a natural interpretation of the parameters: λβ tunes the relative ordering of the responses at different covariate values, and FY sets the marginal distribution of the responses. This strong interpretability is an important feature as it provides a good vehicle for specifying prior beliefs.

For inference we propose to use a marginal composite likelihood approach, which we show allows the model to scale tractably to large data applications and allows for the use of standard, existing, software from Bayesian nonparametric density estimation and Plackett-Luce ranking estimation to be applied. As an illustration, we show an application of our approach to a US Census dataset, with over 1,300,000 data points and more than 100 covariates.

The paper is organized as follows. Section [\ref=sec:densityestimation] provides background on Dirichlet process mixtures and Pólya trees for density estimation. Section [\ref=sec:model] describes the Plackett-Luce copula model. The marginal composite likelihood approach for scalable inference is presented in Section [\ref=sec:inference]. Section [\ref=sec:appli] presents some results of our approach on simulated data and on the US Census dataset.

Bayesian nonparametric density estimation

The appeal of Bayesian nonparametric models is the large support and probabilistic inference provided by such priors. This both safeguards against model misspecification and enables highly flexible estimation of distributions. This has lead to particular popularity of Bayesian nonparametric priors in density estimation.

In the simple case of density estimation for a real valued random variable many nonparametric priors exist - see [\cite=Hjort2010] for a recent review. A popular class of model is the Dirichlet Process Mixture ([\cite=Lo1984]), whereby a Dirichlet process prior is placed on the distribution of the parameters of a parametric family. The result is an "infinite mixture model". Precisely:

[formula]

where K is the density of the chosen parametric family, c > 0 is a scale parameter and P0 is a base measure. Since draws from a Dirichlet Process are almost surely atomic measures, there is positive probability of observations sharing a parameter value given the random measure P. The result is an effect of clustering within a sample, with a random, limitless number of clusters. This has proved to be an extremely popular model as it models heterogeneity within a sample well, and provides a highly flexible support. Efficient MCMC schemes ([\cite=Escobar1995] [\cite=MaceachernMuller1998] [\cite=neal2000]) have lead to the widespread use of the Dirichlet Process Mixture (DPM) in density estimation.

Pólya trees provide another flexible nonparametric prior for density estimation ([\cite=Ferguson1974] [\cite=Lavine1992] [\cite=Lavine1994] [\cite=Mauldin1992]). They are defined as follows: Let [formula], and define a sequence of embedded partitions of [formula] to be [formula], where the [formula] are defined recursively, such that [formula]. Now let [formula], the set of all countable sequences of zeros and ones, and let [formula] be a set of nonnegative real numbers. Then, a random probability measure P is a Pólya tree process with respect to [formula] and A if [formula], independently for all [formula]. There are two properties of the Pólya tree process that are appealing for density estimation: Pólya trees are conjugate, meaning that both the prior and the posterior have the same functional form, and, for certain choices of A, realizations are absolutely continuous probability distributions, almost surely. It is worth pointing out that empirically the model can depend heavily on the defined sequence of partitions Γ, although a mixture of Pólya trees proposed by [\cite=Lavine1992] can smooth out this dependence over multiple partitions. In what follows we make use of these nonparametric models to specify priors for the marginal distributions of covariates and response variables.

The statistical model

Let (Xi,Yi), [formula] be the covariates and responses and regression function [formula]. To build the dependence we introduce a latent random variable Zi that is used to capture the underlying relative level of the response via,

[formula]

where [formula] denotes the standard exponential distribution of rate a. The latent variable Zi may be interpreted as an "arrival time" of individual i. The arrival times then define a conditional ranking of the predicted response variables [formula].

The model can be summarized as follows, for [formula]

[formula]

where

[formula]

Figure ([\ref=fig:tree]) shows the correspondence between the conditional exponential random variables, Z|X, shown in [\ref=fig:tree](a) for differing covariate values, and the resulting predictive distributions in [\ref=fig:tree](b), where the marginal FY is a Gaussian mixture model shown as the black line. We can see visually that the distributions in [\ref=fig:tree](b) are stochastically ordered under the model. The coloured points shown in (a) are mapped to the points shown in (b), where again ordering is preserved.

As FY and FZ are cumulative density functions, [formula] is a monotonically increasing function and

[formula]

This clarifies the role of the regression function. More generally, given an ordering [formula] (a permutation of [formula]), we have

[formula]

The above model is the Plackett-Luce model [\citep=luce1959] [\citep=plackett1975], popular in the ranking literature, and also corresponds to the partial likelihood used for Cox proportional hazards models [\citep=Cox1972].

By construction FZ(Zi) is marginally uniformly distributed on

[formula]

F(x,y)=C(F(x),F(y)).

[formula]

Approximations for posterior inference and prediction

Assume that both FX and FY admit a density with respect to Lebesgue measure, noted fX and fY. The unknown quantities for our regression model are therefore (fY,β,fX). Given data (x1:n,y1:n), where [formula] and [formula], we have the following likelihood:

[formula]

Inference could proceed using numerical methods such as MCMC but for large datasets this is cumbersome. Hence we consider here a Bayesian composite marginal likelihood approach [\citep=Lindsay1988] [\citep=Cox2004] [\citep=Varin2011] [\citep=Pauli2011] [\citep=Ribatet2012] that we show offers computational tractability and the use of standard Bayesian methods. Define y*1:n to be y1:n ordered from lowest to highest, and let [formula] be a vector representing the order of y1:n, so that y*i  =  yνi. Then we can re-write our data {y1:n,x1:n} equivalently as {y*1:n,ν1:n,x1:n}. Now let LC denote the composite marginal likelihood based on {y*1:n} and {ν1:n,x1:n}. That is the product of the likelihood terms associated with each of these terms:

[formula]

We can see that this composite likelihood approach factors the likelihood into separate terms involving fY,β and fX, leading to the following pseudo posterior distribution

[formula]

Inference over the parameters fY,β,fX can thus be carried out independently under the composite likelihood approach. Standard software for Bayesian nonparametric univariate density estimation can be used for fY and fX, and software for fitting Plackett-Luce/Cox proportional hazard can be used for fitting β. Overall the advantages of the approximate composite likelihood approach include computational tractability and scalable inference using standard software, hence good numerical reproducibility, and high interpretability as the components in the composite likelihood have explicit form and meaning. This latter point aids in prior elicitation as it allows the analyst to separate out and represent their beliefs on the marginal distributions, which are simpler to specify than the full conditionals, and then consider the dependence given the marginals.

The Bayesian composite likelihood approach has attracted some attention over recent years [\citep=Pauli2011] [\citep=Varin2011] [\citep=Ribatet2012]. In particular, [\cite=Ribatet2012] considered two adjustements to the marginal likelihood approach in order to retain some of the desirable properties of the usual likelihood. However, their adjustments apply to a specific form of composite likelihood, where it factorizes as a product of composite likelihoods for each observation: [formula] where Lc(yi|θ) is the composite likelihood for observation i. Our composite likelihood approach does not fit in this framework, as we do not have this product form over the observations, and we cannot therefore apply the adjustments suggested by [\cite=Ribatet2012]. Extending the adjustment of [\cite=Ribatet2012] to our framework is an interesting direction, but beyond the scope of this article.

Asymptotics for the marginal composite posteriors

Consider first the pseudo-posterior for fY:

[formula]

So our pseudo-posterior is exactly the posterior based on the i.i.d sample {y1:n}, where y1:n  ~  FY. This is the standard setting for posterior inference, so we can apply consistency results from Bayesian nonparametric inference for FY, see for example [\cite=ghosal2013fundamentals]. The same is true for fX. Now consider the log-linear form for λ: λ(x)  =   exp ( - βx). Then, we have the pseudo-posterior:

[formula]

This is exactly the posterior considered by [\citet=kim2006] in a different setting where a Bernstein-Von Mises theorem is proven, which can be applied here.

Posterior predictive

We can use simulation methods such as MCMC to easily generate samples {F(j)Y,β(j)}mj = 1 from the pseudo-posterior [\eqref=eq:pseudopost]; the predictive distribution can then be approximated by

[formula]

To simulate from this distribution, we can use the forward generating process of our model, given X = x':

[formula]

In many applications, modeling FX might be cumbersome, and not the primary object of interest. In this case we propose to use an empirical Bayes approach by setting FX  =  X at the empirical CDF. So, to generate a posterior predictive sample, given a posterior sample {F(j)Y,β(j)}mj = 1, Eq. [\eqref=eq:YgivenZ] becomes:

[formula]

where we note that Z'(j) is conditional on X' = x', and the CDF inversion is tractable, depending on the form of FY. Alternately one can use Monte Carlo to draw samples from the predictive, which is trivial when FY can be sampled from. Some particular examples are discussed in Appendix A.

Illustrations

In this Section we apply our method to two examples. The first is a simulation example where we generate from a multi-modal conditional and explore the ability of our method to fit the data. The second is a large real-world application in the regression analysis of US Census data.

Simulation example

In this section we apply the model to a dataset simulated from our model to consider how well we can recover known dependence. The marginal distribution of Y is set to a mixture of three Gaussian distributions, with means 3, 9 and 15, standard deviations of 2, 0.5 and 1 with mixture weights of 0.5, 0.2 and 0.3 respectively. β is set to 0.25, with λβ(x) =  exp (βx). [formula] and n = 500. The data is shown in Figure [\ref=fig:sim_data](a).

Clearly any type of linear or non-linear regression with a parametric noise distribution will be inappropriate here. The conditional distribution of Y given x is multi-modal, rendering many popular regression models inappropriate.

We compared our approach to a linear dependent Dirichlet process mixture of normals (LDDPM) [\citep=DeIorio2004], using the R package DPpackage [\citep=dppackage] [\citep=jara07]. This model specifies that

[formula]

where [formula] and

with [formula] and [formula].

We apply our model, modeling the marginal as a Dirichlet Process mixture of Gaussian distributions using α = 1 and a normal-inverted-Wishart distribution for the base measure. That is, our base measure [formula], where μ1  =  9,κ1 = 0.5,ν1 = 4 and ψ1 = 1. A Gaussian prior centered at 0 with unit variance is used for β.

In Figure [\ref=fig:sim_data](b) the simulated data is shown, with the 80% highest posterior density (HPD) intervals of the predictive distribution at each value of x. Qualitatively we see that the model can capture the nonlinearities in the data and demonstrates the flexibility to model the multi-modal conditional response. In Figure [\ref=fig:marg] we show the predictive marginal, Y and the posterior distribution for β. Clearly the marginal distribution for Y is very well recovered from the data. This parameterization of the model in terms of the marginal distribution for the response allows this to be estimated from the complete dataset, without reliance on other aspects of the model. The strength of information available is apparent in the quality of the fit to the sampling distribution. The posterior for the parameter β shows reasonable support around the true value, being slightly pulled towards 0 by the prior.

We can further inspect how these come together in the posterior predictive conditional distribution for Y given x. Consider this distribution for x = 5 and x = 12, for both our model and the linear DDP mixture model, as shown in Figure [\ref=fig:pred]. Again, our model provides a reasonable fit. The predictive distribution is not as accurate as the marginal distribution for Y, but this is to be expected, since the conditional distribution is a product of the whole model, compounding uncertainties from both β and the marginal distribution for y. Nonetheless, the fit is good and noticeably better than the flexible linear DDP mixture, as you would expect, given that the sampling distribution is within the support of our model. Concretely, the L1-distance between the estimated conditional and the true conditional distribution can be calculated in each case. When x = 5 the distance to our prediction is 0.00869, whereas the distance to the linear DDP is 0.0214, and when x = 12 the distance to our prediction is 0.0127 and the distance to the linear DDP is 0.0146.

A point of note is that these posterior predictive plots are smoothed kernel density estimates of MCMC samples. Therefore, Gaussian shapes are slightly exaggerated. Whilst not entirely clear from the plot, both our predictive and the sampling distribution comprise of slightly skewed Gaussian distributions, since the conditional distribution is the marginal distribution for Y weighted across the quantiles.

To illustrate that the model is capable of modeling a range of distributions, we consider data sampled from a Gaussian linear model. The covariates are simulated uniformly on

[formula]

US Census application

We apply the methodology to a regression task using US census data for personal annual income.

We use the American Community Survey data from 2013, which comprises of responses to questions on the survey given to a 1% sample of the US population. Since we are interested in income, the subset of 1,371,401 employed civilians over the age of 16 is used. We have used a relevant, linearly independent subset of the data as covariates, excluding highly informative questions such as occupation, which would almost completely explain the response. This leaves 15 explanatory variables, 10 of which are categorical variables, some of which have many levels. The result is a 1,371,401  ×  114 design matrix.

The covariates are: US state (Texas as a baseline), weight, age, class of worker (employee of private for-profit company as a baseline), travel time to work, means of transportation to work (works from home as a baseline), language other than English spoken at home (no as a baseline), marital status (married as a baseline), educational attainment (regular high school diploma as a baseline), gender (male as a baseline), hours worked a week, weeks worked last year, disability status (without a disability as a baseline), quarter of birth (first quarter as a baseline), and world area of birth (United States of America as a baseline).

The levels of annual income shown in Figure ([\ref=fig:census]) can be seen to be heavy tailed, which requires a flexible model to capture. Another noticeable feature of the data is that the income levels are discontinuous, with large spikes in frequency at particular income levels. This could in part be due to standardized salary structures resulting in certain salary levels becoming common. This motivates the use of a nonparametric approach as it is difficult to imagine how a parametric density could conditionally capture the features shown in Figure ([\ref=fig:census]). However, standard Bayesian nonparametric models simply cannot be applied to a problem of this scale. Attempting to apply existing methods in this literature, such as the linear dependent Dirichlet process mixture, failed to run due to the dimensionality and scale of the data.

For the analysis, we consider both the empirical distribution function and a Pólya tree prior for the marginal distribution of y. The partition of the Pólya tree is set on the quantiles of a Gaussian distribution with mean 35,000 and standard deviation 20,000, and [formula]. We use a log-linear regression function λ(x)  =   exp (βx) and place independent Gaussian priors with mean 0 and unit variance on the coefficients in β.

Predictive performance

We compare the out-of-sample predictive performance of our model with three competing non-Bayesian approaches namely, a standard linear regression model, a median regression model and a LASSO. For our model we investigated three distinct priors for the marginal distribution of the response: a Pólya tree centred on a Gaussian, a Pólya tree centred on a Laplace, and an empirical Bayes approach using the empirical CDF. To compare methods we use repeated random subsets of 1000 test samples and train each model on the remaining data, with 10 repeats. Predictive accuracy is judged by mean squared-error (MSE), mean absolute error (MAE) and qualitatively via a qq-plot. To create the qq-plots we compute the predictive distribution function F(y|x) evaluated at the observed value for each of these test samples. Under the assumption that we have a perfect predictive distribution, these values should be independent uniform random variables. A deviation from this distribution implies a mis-match of the posterior predictive and the actual distribution. We are unable to apply this approach to the median regression model, as it does not provide a predictive distribution and would require fitting the model for a large number of quantiles. In the case of the linear model we used maximum likelihood estimates for prediction, rather than a fully Bayesian approach. With such a large dataset the strength of any reasonable default prior would be significantly diminished, so this should mimic a Bayesian approach well.

Summary statistics of predictive fit are shown in Table [\ref=tab:mse]. Perhaps unsurprisingly on such a large data set the linear model targeting the conditional mean does best on MSE but this is at the expense of the median under MAE. In addition, studying the predictive qq-plot in Figure([\ref=fig:qqplot]b) shows the inadequacy of the linear model to provide calibrated predictions. The LASSO performs relatively poorly suggesting most covariates are influential for prediction, whereas the median regression whilst, as expected, provides relative accuracy on the MAE does so at the expense of MSE and as mentioned above suffers from the lack of a fully predictive model. The Bayesian nonparametric methods perform relatively well on both summary measures, with perhaps that based on the Laplace marginal showing greatest accuracy. In Figure([\ref=fig:qqplot]a) we show the predictive qq-plot from this model, demonstrating that the full predictive distribution is captured well.

These diagnostics suggest that even non-linear regression models with parametric noise would not provide a satisfactory fit for the data, since the unusual conditional distribution of the response cannot be captured by such models. This highlights the benefit of our nonparametric approach.

We next consider inference for covariate effects. In order to gain a measure of the relevance of each covariate we quantified the concentration of the posterior probability measure away from the prior "null" centring of βj = 0. To do this we estimated the Bayesian sign-probability from the posterior marginal for each covariate as,

[formula]

where π(βj|  ·  ) is the posterior marginal for βj. This measures the relative tail area in the posterior marginal laying to the left or right of 0. A large value of PrSign suggests there is strong evidence against βj = 0. In certain respects this is akin to a Bayesian marginal version of a p-value, and is trivially calculated from MCMC output, or from normal approximations to the posterior distribution. Table [\ref=tab:psign] shows the most relevant covariates as ranked by this measure.

Unsurprisingly, hours worked a week and weeks worked last year show high certainly of a positive effect on income. After these, educational achievement measured via degrees unsurprisingly imply higher earnings compared to the regular high school diploma. Since these are part of the same variable it is simple to compare the effects due to these degrees. Despite Bachelor's degree providing the most certainty of a positive effect, a further Professional degree beyond bachelor's has the highest posterior mean effect. The ranking in Table [\ref=tab:psign] reflects the greater evidence in the data for a non-zero Bachelor effect, due to a much higher number of observations of those with Bachelor's degrees, and hence lower variance in the effect size compared with those with a higher degree. There is also strong evidence for Female workers earning less than their male counterparts, as well as increasing income with age and even travel time to work.

Finally, we show it is simple to provide the full posterior predictive distribution of annual income of somebody in the test sample, using the Pólya tree model. We choose as a hypothetical person a 57 year old female from North Carolina, who is self employed, married, 140 lbs bodyweight, who works from home, speaks English at home, went to college but for less than a year, who usually works 30 hours a week, for 43.5 weeks last year, was born in the first quarter of the year in the USA. The structure and shape of the posterior predictive, represented in Figure [\ref=fig:lin_qq], match that of the marginal distribution for Y in the data, just on a narrower range.

Discussion

We introduced a new Bayesian semiparametric regression model that is designed to scale to large data applications. In doing so we make use of an interpretable model for ranks, via a Plackett-Luce copula method, and nonparametric density models for the marginals. We used a composite marginal likelihood approximation that leads to a number of advantages. It affords computationally tractability, aids in the interpretation of the model, and makes prior specification explicit on known objects.

The key to the scalability of the method is the use of the composite likelihood approximation, which splits the inference into two simpler tasks. The use of the Laplace approximation for the covariate effect and the Pólya tree for the marginal response allow for fast posterior inference, without requiring any MCMC sampling methods. In fact, sampling methods are only used for prediction, which is by far the slowest part of the inference procedure.

Going forward, it would be interesting to see if theoretical bounds on the approximation error as a function of sample size could be derived. It may also be possible to apply results such as those found in [\citet=kim2006] to provide further guarantees of asymptotic behavior such as properties of the predictive distribution. In addition, it would be interesting to explore non-linear models for the regression function λ(x), such as those based on a random forests methodology. In fact, random forests applied to the US Census dataset (with restricted node size to enable application to this scale) gives a highly competitive MSE to our tested models. This might be because random forests is able to capture interaction terms between covariates, which seem highly plausible in this particular dataset. It will be interesting to incorporate such flexibility into a Bayesian nonparametric approach using Plackett-Luce regression functions.

Details on simulating from the predictive

In this section we provide additional on the simulation from the predictive distribution depending on the choice of the prior on FY.

Empirical CDF

It might be the case that n is so large that simply using the empirical CDF is a reasonable approximation. In this case, the inversion of the cdf becomes trivial, and MCMC is only required for the β posterior sample.

Simulate β(j) from the partial posterior

[formula]

Calculate [formula]

Set Y'(j)  =  y(⌈nu(j)⌉)

Bayesian Bootstrap

An alternative approach might be to use a Bayesian Bootstrap on FY. This works out similarly to using the empirical CDF, but we must simulate the Dirichlet weights for each of the atoms. The sampling scheme becomes:

Simulate β(j) from the partial posterior

Sample [formula]

Calculate [formula]

Simulate [formula]

Set [formula]

Pólya Trees

Under our composite likelihood scheme, the posterior for FY is also a Pólya tree, due to conjugacy of the Pólya tree prior. Simulation then proceeds as follows:

Simulate β(j) from the partial posterior

[formula]

Calculate [formula]

Then all we need to calculate is F- 1(j)Y(u(j)). Pólya trees make this easy. A sample from a Pólya tree distribution is a random probability measure, which is constructed by assigning random masses to each branch in a partition tree of the space. So, given the first partition point in the tree, we can simply generate the random mass either side of this point, and trivially deduce which branch F- 1(j)Y(u(j)) lies in. We repeat this process down the tree until we reach the truncation point often used when using Pólya trees.

Formally, given a Pólya tree truncated at level M, let [formula] and for k from 1 to M:

[formula]

if u∈[ak,θk(bk  -  1   -   ak  -  1)   +   ak  -  1]

εk  =  εk - 10

ak  =  ak - 1

bk  =  ak - 1  +  θk(bk - 1  -  ak - 1)

Otherwise

εk  =  εk - 11

ak  =  ak - 1  +  θk(bk - 1  -  ak - 1)

bk  =  ak - 1

Dirichlet Process Mixture models

The difficulty here becomes the inversion of FY, since this has no closed form. A simple Monte Carlo approximation can be used to approximate this inversion for each posterior sample.

Simulate β(j) from the partial posterior

Sample [formula]

Simulate F(j)Y from the partial posterior

Simulate Y'(j)k  ~  F(j)Y for [formula]

Calculate [formula]

Set Y'(j)  =  Y'(j)(⌈Nu(j)⌉)