Corollary Lemma Remark Example Definition

Proposition

Contamination Estimation via Convex Relaxations

Index terms: contamination estimation, anomaly detection, entropy minimization, discrete goodness-of-fit testing.

Introduction

Anomalies in datasets are typically associated with unexpected or unwanted characteristics such as contamination, noise or outliers that deviate significantly from expectations. The ability to detect anomalies and accurately estimate contamination in datasets is important in a wide variety of domains including healthcare, astronomy, environmental and materials sciences. The context that motivates our work is detecting anomalies and estimating contamination in datasets collected from communication and computer systems. Specific applications of anomaly detection in these datasets include network management and Internet security broadly defined. Communication and Internet measurement datasets have several distinguishing characteristics including the potential for extreme scale and high dimensionality.

The standard framework for anomaly detection is based on establishing a baseline for normal ( e.g., in a distributional sense) and then setting a threshold which if exceeded identifies an anomaly. The goal in establishing norms and thresholds is to identify anomalies with low false alarm rates. There is an extensive literature on methods for anomaly detection (see related work in Section [\ref=sec:related]).

In this paper we describe a new method for anomaly detection which is based on estimating the level of contamination in a dataset. An anomaly is declared if a dataset has an elevated level of contaminate. We consider the contamination-free ( i.e., normal) condition of a dataset to be specified by a model comprised of a set of distributions. We then compare the model to the distributional profile of a target dataset collected over a specified period. A standard method for comparing datasets in this way is goodness of fit (GoF) testing [\cite=d1986goodness]. To the best of our knowledge, this paper is the first to address the problem of contamination estimation using GoF testing based on entropy minimization, as we define in Section [\ref=sec:probstate].

The approach we develop is based on answering the following question. Given a model consisting of a family of distributions, a specified p-value, and an empirical dataset, what is the minimum number of data points that must be discarded so that the empirical distribution of the data matches a member model distribution (in terms of GoF for a specified p-value)? This is akin to finding the largest subset of the original dataset which has an empirical distribution close to the model. We show that this question can be efficiently answered by solving a series of convex optimizations. Solving the optimizations results in a lower bound on the minimum number of data points that are attributed to a contaminate. In the simplest case, each convex optimization is an inequality constrained entropy minimization problem (whose dual is a constrained geometric program) which can be solved in real time and at scale for many applications. More generally, the approach can be applied to any setting in which the model consists of a convex set of distributions. Two specific instances which we discuss are 1) models defined by any number of distributions with arbitrary mixture proportions, and 2) models defined by the set distributions with small Kullback-Leibler (KL) divergence to a specified distribution, which arises when the model itself is generated from a finite amount of data. Lastly, we show the lower bound output by the optimization converges to an upper bound known as the separation distance at a rate of [formula], where p is the number of data points.

Quantifying Contamination

Notation

Let [formula] and [formula] denote probability mass functions over n categories, with elements Pi, [formula] and Qi, [formula]. Throughout, P denotes the distribution under test, Q denotes a member distribution of the model, Q0 denotes the 'true' unknown model distribution, and Qj indexes multiple distributions. The empirical distribution of a sequence of random variables [formula] is the relative proportion of occurrences of each element of X in X. Specifically, let [formula] and define [formula] for [formula]. Then [formula] [formula] denotes probability measure with respect to distribution Q. For simplicity of notation, we write [formula] as short hand for [formula]. The Kullback-Leibler divergence between two distributions is defined in the usual manner,

[formula]

D(P||Q) is a jointly convex function in P and Q. The minimum entropy set, [formula], is a convex set (for a fixed Q, ε). Lastly, let [formula] denote the probability simplex:

[formula]

Quantifying Contamination

Consider a set of model distributions Q whose elements are supported over a finite number of categories X with |X| = n. For example, Q could be set of minimum entropy distributions, or a mixture distribution, [formula], where [formula] are unknown (Q is the set of all such mixture distributions). Let X∈Xp denote a collection of samples. An unknown subset of the samples consists of i.i.d. draws from an unknown distribution Q∈Q. The remaining samples, C  ⊂  [p], are generated by some other means, and correspond to contaminated samples. This paper is concerned with lower bounding the size of the contaminating set C given the set of model distributions Q, a specified significance level (a p-value), and the observed samples [formula].

Intuitively, if the empirical distribution of a sequence of random variables is close to the model distribution in terms of GoF, we conclude the sequence is not contaminated. To quantify this intuition, we define a set of typical empirical distributions based on statistical significance; we note this definition is distinct from the usual definitions of strongly and weakly typical, and making this connection is a contribution herein.

. Let [formula] be an ordering on all empirical distributions (of p samples and n categories) such that [formula]. A sequence of random variables X with [formula] is typical at significance level ε with respect to Q iff

[formula]

for any such ordering.

The definition implies a sequence of random variables X is typical if the probability of the empirical distribution of X or any less likely empirical distribution is more than a specified significance level. Note ε is interpreted as a p-value; as ε approaches zero, all sequences become typical (requiring stronger evidence to reject the null hypothesis). As ε increases, fewer sequences are typical.

We say X is contaminated iff X is not typical (with respect to Q and with significance ε). Likewise, an empirical distribution P̂(X) is contaminated iff X is not typical.

In this paper we study the following question. Let [formula] be a dataset, and let [formula] be any subset of of the original dataset. What is the smallest set   ⊂  [p] such that [formula] is not contaminated? Specifically, let

[formula]

How and under what conditions can one compute c* efficiently? Our main focus and insight will be on the continuous approximation to c* / p, denoted α*:

[formula]

where P(X,α) is the set of all distributions that can be created by discarding a fraction α of the mass of P̂(X) (see Sec. [\ref=sec:ConvRel]):

[formula]

Throughout, α is a key parameter that represents the fraction of the dataset attributed to contamination; α* represents the smallest α such that there exists a subset of the original data of size p(1 - α) that is not contaminated. If α*  =  0, the original dataset is not contaminated; if α*  =  1, the entire dataset must be attributed to contamination.

Separation Distance

We assume [formula] for all [formula]. For Xi, i∈C, no assumption is made. This agnostic approach has inherent limitations. In the extreme case the distribution of the contaminated data could exactly follow that of the model. Here, the distribution of the full dataset should closely match the model, and be indistinguishable from the setting where C is empty. No contamination should be reported to within the significance level (in m realizations of Xp, we expect c*  ≠  0 fewer than mε times).

A more interesting scenario is when the empirical distribution of the full dataset converges to a distinct distribution i.e., P̂(Xp)  →  P  ≠  Q0. In the case that [formula], a consistent estimator will report non-zero contamination for large p. P can be written as a mixture distribution, and we are interested in reporting the smallest κ such that (1 - κ)Q0  +  κF  =  P for any distribution F. F represents the contaminating distribution, and κ the proportion of the samples which are drawn from F. This minimum value of κ is known as the separation distance [\cite=aldous1987strong] between P and Q0, written succinctly as

[formula]

In this way, the separation distance between the empirical distribution of the data and model distribution plays an important role in the behavior of c* and α* as the sample size grows. We show as a corollary to later results that α* is both upper bounded by and converges to κ(P̂(X)||Q0) as p grows (see Proposition [\ref=prop:12324] and Theorem [\ref=thm:largep]).

Convex Relaxations

With the exception of problems involving data over only two categories (n = 2), directly checking if a sample is contaminated is computationally prohibitive, even in the setting where the model consists of a single distribution (when Q  =  {Q0}). Alternatively, using large deviations results, bounds can be derived. The bound presented below can confirm if a particular dataset is contaminated. The theorem involves the KL divergence between the empirical distribution and a member of Q. In the case where Q  =  {Q0}, the bound provides a simple way to check if a sample is contaminated at a particular significance level ε; in the more general case, if Q is a convex set, numerical optimization techniques can efficiently check the condition.

(Outer Bound). If

[formula]

then X is contaminated at significance level ε.

See Appendix A.

Theorem [\ref=thm:outer] is an outer bound; any empirical distribution with KL distance greater than the stated quantity (from all elements in Q) is contaminated. Theorem [\ref=thm:outer] can be used to bound the size of the smallest set C  ⊂  [p] such that [formula] is not contaminated. This is simplified if Q consists of a single model distribution; we first discuss this scenario. In principle, given a dataset X∈Xp and a model distribution Q0, one could first check if X is contaminated by evaluating ([\ref=eqn:Qouter]). If ([\ref=eqn:Qouter]) holds, X is contaminated, and an immediate question follows - how many and which data points must be excluded so that ([\ref=eqn:Qouter]) no longer holds? A exhaustive approach to answer this question would be the following. For each xi∈X, discard a single data point that takes the value xi, and recalculate the empirical distribution with the data point removed. Of the n new empirical distributions, check if the one with minimum KL divergence to the model distribution still satisfies ([\ref=eqn:Qouter]).

If ([\ref=eqn:Qouter]) still holds for all possible empirical distributions with one data point removed, check all distinct empirical distributions that can be created by discarding 2 data points (roughly n2 possibilities, provided each xi appears at least twice in the data). Continuing in this manner, one would check each of the ~  nm possible empirical distributions that can be created by discarding m data points. When ([\ref=eqn:Qouter]) is first violated, m lower bounds the minimum number of data points that must be excluded to match the model. We can interpret this as a series of integer programs. For [formula] define D*m as the solution to

[formula]

where pi is the number of times xi appears in the original dataset X. The optimization variables, mi, represent the number of samples to discard corresponding to a particular xi. Note that the objective is the KL divergence between the new empirical distribution (with m samples removed) and the known distribution Q0. The value of D*m can be checked in Theorem [\ref=thm:outer], providing conditions under which one can find a set |C| = m such that [formula] is not contaminated. This gives a bound on c*. Specifically,

[formula]

Note that the condition in Theorem [\ref=thm:outer] will always be met for some m; in particular, for m = p, by convention D*0 = 0, implying that the empty set, X{}, is not contaminated.

The optimization in ([\ref=eqn:int]) is an integer program over a subset of [formula]. To efficiently solve the optimization, we can translate the integer valued variables to their continuous counterparts; specifically, let P̂i  =  pi / p, be the original empirical distribution, and α  =  m / p represent the fraction the total samples discarded. Making these substitutions results in a convex entropy minimization problem:

[formula]

where α∈[0,1] represents the fraction of samples removed.

More generally, Q is a set of distributions. The same continuous approximation results in a joint optimization over the model space Q and the space of empirical distributions, P(X,α) defined in ([\ref=eqn:pram]). Formally, let D*α be given as

[formula]

If Q is a convex set, the above optimization can be efficiently solved in many settings (see Sec. [\ref=sec:minentro]).

To answer our original question and bound α*, one can conduct a line search over α∈[0,1], repeatedly solving the above optimization, and checking the output value of D*α against Theorem [\ref=thm:outer]. This is captured in the following proposition.

Let

[formula]

then [formula].

The proof follows directly from Theorem [\ref=thm:outer]. For any α such that the condition on D*α in ([\ref=eqn:alp_ub]) holds, by Theorem [\ref=thm:outer], any distribution in P(X,α) is contaminated. We note that [formula] always exists by monotone properties of D*α and the right hand side of the conditional in ([\ref=eqn:alp_ub]). See Appendix B, Theorem [\ref=thm:largep] for details.

Fig. [\ref=fig:geo] shows a geometric interpretation of Proposition [\ref=prop:12324] and the optimization in ([\ref=eqn:contop1]). See the caption for details.

The lower bound obtained by solving the series of optimization problems converges to the separation distance, captured by the following theorem.

Let Q  =  {Q0}. Fix P̂(X). Then

[formula]

See Appendix B.

Theorem [\ref=thm:conv] is stated for a fixed P̂(X), although one would in general assume P̂(X) to be an implicit function of p. The reason for fixing P̂(X) is both generality and simplicity. The assumption decouples randomness from the convergence rate of the upper bound and the lower bound produced the optimization; without this assumption, the upper and lower bounds would be random variables, and necessitate a probabilistic statement. We also note that a precise limit statement can be readily extracted from the proof.

Discussion

In practice, it is often the case that the precise model distribution is not known; instead, it may be known that the model distribution comes from some family of distributions. This arises in anomaly detection when normal events are known to correspond to unknown proportions of samples from a finite set of distributions. This is the case of the mixture model i.e., Q is the set of all distributions that can be represented as [formula] for any mixture proportions πj. As the set of mixture distributions with unknown mixture components is a convex set, we can directly address this setting using the developments of Sec. [\ref=sec:ConvRel]. Jointly optimizing over the mixture weights and the mixture distribution, the optimization takes the form

[formula]

We note that the above optimization can be solved at scale in real time for many applications; see discussions of numerical experiments below for details.

For many applications, model distributions are generated using a finite amount of data from known good sources (i.e., sources that are known to have no contamination). Let [formula] be an empirical distribution generated from p' samples of an i.i.d. population, and consider the set

[formula]

Here, Q is the set of all distributions that have [formula] as a typical empirical distribution. As before, determining membership in Q is intractable for large p' and more than two categories. Let

[formula]

[formula] satisfies two important properties. First, Q  ⊆   by Theorem [\ref=thm:outer] and second, [formula] is a convex set.

Solving the optimization in ([\ref=eqn:contop]) with Q  =   provides a powerful result which we state in the following proposition.

Consider two empirical distributions P̂ and [formula]. Let Q  =  , defined above, and let D*0 be the solution to the optimization in ([\ref=eqn:contop]) with α  =  0. If

[formula]

there is no Q that simultaneously satisfies 1) [formula] is typical with respect to Q and 2) P̂ is typical with respect to Q.

Satisfying proposition [\ref=prob:last] implies that observing a [formula] and a P̂ generated by the same underlying distribution by chance can occur at most a fraction ε of the time; in this sense, P̂ must be contaminated. With a single parameter search over α∈[0,1], the lower bound applies: [formula]. We note that the formulation does not require the empirical model and the distribution under test to have joint support.

Numerical experiments were conducted to highlight the utility of Proposition [\ref=prop:12324]; results are shown in Fig. [\ref=fig:varyp_normed]. In contrast to the deterministic experiments in Fig. [\ref=fig:varyp_normed], experiments with random samples from various model and test distributions as input were run, showing similar convergence behavior. An experiment with with Q being a set of 10 mixture distributions with n = 50 was also conducted. The line search over α was completed using a bisecting search to an accuracy of 2- 28 (the optimization was solved 27 times for each experiment). Averaged over 50 trials, the total time to compute [formula] was 0.4 seconds. Experiments were implemented using CVXOPT [\cite=cvxopt] and results visualized with matplotlib [\cite=Hunter:2007].

Related Work

Related work can be broadly classified into traditional work in goodness of fit (GoF) testing, and more recent work in anomaly detection. GoF testing has an extensive literature. When the data are binary valued, and the model distribution Bernoulli, quantifying contamination using GoF tests can be addressed by evaluating binomial probabilities (a technique known as Fisher's Exact method [\cite=mehta1984exact]). When the data take on more than two values, exact solutions for the level of contamination become intractable.

A customary approach to GoF testing for categorical data is Pearson's χ2 test [\cite=agresti2014categorical]. This approach to GoF testing can be quite powerful, but suffers from limitations. χ2 tests are approximations, and are known to be invalid under certain conditions. In particular, the test is invalid when pi  =  0 for one or more categories. Nonetheless, employing the χ2 test, one can deduce another optimization (much as we do in Sec. [\ref=sec:ConvRel]) to answer the aforementioned question; we note the resulting optimization is a separable quadratic program with linear equality constraints which has an analytic solution [\cite=bay2010analytic], and would be an interesting starting point for future work. Since Pearson's χ2 test hinges on a normal approximation, this approach would not result in strict contamination bounds. More specific to the contamination estimation problem presented here, recent work includes decontamination with multiclass label noise [\cite=blanchard2014decontamination] [\cite=scott2013classification], which focuses on recovering proportions of a set of mixture distributions present in dataset.

There is an extensive literature on the related topics of anomaly detection and outlier detection including work employing entropy based techniques, in particular [\cite=hero2006geometric] and [\cite=gu2005detecting]; we note the formulations here are distinct in that the level of contamination is not estimated. Lastly, we briefly discuss related work in anomaly detection the areas of computer networks, systems and security as this is the motivation for our developments. Early work on identifying anomalous or unexpected behaviors such faults ( e.g., due to outages or failures) or spikes ( e.g., associated with DoS attacks or flash crowds) in computer network traffic was based on the application of graph models, time series and multi-resolution methods e.g., [\cite=Feather93] [\cite=Katzela95] [\cite=Brutlag00] [\cite=Barford02], and Principle Components Analysis (PCA) [\cite=Lakina04] [\cite=Lakina04a] [\cite=Lakina05]. There are significant difficulties in tuning these methods to provide low false alarm rates in practice [\cite=Ringberg07], necessitating methods based on statistical significance, as presented here.

Appendix A

Proof of Theorem [\ref=thm:outer] requires two ingredients, both relying on results from large deviations theory. The first ingredient is Sanov's Theorem, which we state below.

(Sanov's Theorem) [\cite=cover2012elements] (Theorem 11.4.1). Let S be a set of empirical distributions (with p samples over n categories). Then

[formula]

The second ingredient is also readily derived from results in large deviations theory.

Let S be a set of empirical distributions such that [formula] for all P̂∈S. Then,

[formula]

The following inequalities hold [\cite=cover2012elements] (Theorem 11.1.4):

[formula]

Thus, for any [formula],

[formula]

which implies the result, completing the proof of Theorem [\ref=thm:bnd1].

Combining Theorems [\ref=thm:sanov] and [\ref=thm:bnd1], we have

[formula]

provided [formula]. This provides a simple way to confirm if a sample is contaminated at a particular significance level ε. In particular, assume [formula]. If

[formula]

or equivalently

[formula]

then X is not typical; X satisfies

[formula]

and is contaminated with significance ε. If ([\ref=eqn:4454e]) holds for all Q∈Q, in other words, if

[formula]

we conclude then X is not typical with respect to (Q,ε), implying the result.

Appendix B

Proof of Theorem [\ref=thm:conv]. The proof requires three main steps. The first step is to show that when α is sufficiently close to κ(P̂||Q0), the solution to ([\ref=eqn:contop]) can be written in closed form. The second step is to show a number of properties regarding the asymptotic behavior of [formula] as p grows; specifically, [formula] is monotone increasing in p, and converges to the separation distance; these properties imply that for large p, the closed form solution is valid. Lastly, we can bound the difference between κ(P̂||Q0) and [formula] using the closed form solution.

Step 1: For α close to the separation distance (equivalently, for large p, as we show next in Theorem [\ref=thm:largep]), the optimization has a closed form. This is captured in the following Theorem. Note the theorem assumes there is a unique largest [formula]; in the degenerate case when this is not true, the theorem can be restated introducing at most a factor of n, which does not affect the final result.

Let [formula] be ordered such that [formula]. For [formula]

[formula]

is the unique solution to ([\ref=eqn:contop]).

The result can be shown by verifying the conditions KKT conditions with

[formula]

and

[formula]

where the Lagrangian [\cite=boyd2009convex] is given as

[formula]

These primal and dual optimal points are derived using methods similar to [\cite=boyd2009convex] (p. 228, 248); in what follows, we simply verify the KKT conditions which suffice to complete the proof. First, we confirm that the solution is a stationary point:

[formula]

which holds for all i. The complementary slackness condition is readily verified:

[formula]

It remains to show conditions under which the solution is primal and dual feasible. First, [formula] provided

[formula]

After arranging terms, the above holds when [formula]. The primal equality constraint, [formula], is readily verified. Lastly, we check the primal inequality constraints. [formula] is trivially feasible. For [formula], we require

[formula]

which holds when

[formula]

Since [formula] for all [formula], the solution is feasible if

[formula]

We conclude that the KKT conditions are satisfied for the range α specified in the statement of the theorem. Since the objective is strictly convex the solution is unique, which completes the proof.

Step 2: We show that as p approaches infinity, α approaches the separation distance. More specifically, we have the following theorem.

[formula]

We begin the proof by examining the behavior of D*α and [formula]. Note that D*α (the minimizer of ([\ref=eqn:contop])) is monotone non-increasing in α, as increasing α relaxes the constraints. For α  =  κ(P̂||Q0), D*α  =  0 as the constraints allow Pi = Qi for all i (as KL divergence is minimized if and only if Pi = Qi for all i). Define

[formula]

for α∈[0,1], p > 0. We can write ([\ref=eqn:alp_ub]) as

[formula]

For fixed p, [formula] is strictly increasing in α for α∈[0,1]. This (and since D*α is monotone non-decreasing in α) implies existence and uniqueness of [formula] for fixed p. Next, for fixed α, [formula] is strictly decreasing in p. Since D*α is not a function of p, we conclude that [formula] is non-decreasing in p.

Lastly, to prove the limit statement, we require D*α be left continuous at α  =  κ(P̂||Q0); for any ε > 0, there exists some δ > 0 such that D*κ(P̂||Q0) - δ  <  ε. This follows as the objective is continuous in the optimization variables, and constraints are continuous in α; an arbitrarily small increase in the objective can be realized by sufficiently reducing α.

Step 3: Bound [formula] using the closed form solution.

The value of KL divergence at P* from ([\ref=eqn:soon]) is

[formula]

where the inequality follows since log (x)  ≤  x  -  1. We are ready to bound the difference between [formula] and the separation distance. Recall the definition of [formula]; [formula] must satisfy

[formula]

and by ([\ref=eqn:ineq44])

[formula]

which implies the result

[formula]