pc

Spectral thresholds in the bipartite stochastic block model

Introduction

The stochastic block model is a widely studied model of community detection in random graphs, introduced by [\cite=hll]. A simple description of the model is as follows: we start with n vertices, divided into two or more communities, then add edges independently at random, with probabilities depending on which communities the endpoints belong to. The algorithmic task is then to infer the communities from the graph structure.

A different class of models of random computational problems with planted solutions is that of planted satisfiability problems: we start with an assignment σ to n boolean variables and then choose clauses independently at random that are satisfied by σ. The task is to recover σ given the random formula. A closely related problem is that of recovering the planted assignment in [\cite=goldreich2000candidate]'s one-way function, see Section [\ref=sec:goldreich].

A priori, the stochastic block model and planted satisfiability may seem only tangentially related. Nevertheless, two observations reveal a strong connection:

Planted satisfiability can be viewed as a k-uniform hypergraph stochastic block model, with the set of 2n booleans literals partitioned into two communities of true and false literals under the planted assignment, and clauses represented as hyperedges.

[\cite=feldman2014algorithm] gave a general algorithm for a unified model of planted satisfiability problems which reduces a random formula with a planted assignment to a bipartite stochastic block model with planted partitions in each of the two parts.

The bipartite stochastic block model in [\cite=feldman2014algorithm] has the distinctive feature that the two sides of the bipartition are extremely unbalanced; in reducing from a planted k-satisfiability problem on n variables, one side is of size Θ(n) while the other can be as large as Θ(nk - 1).

We study this bipartite block model in detail, first locating a sharp threshold for detection and then studying the performance of spectral algorithms.

Our main contributions are the following:

When the ratio of the sizes of the two parts diverge, we locate a sharp threshold below which detection is impossible and above which an efficient algorithm succeeds (Theorems [\ref=thm:detection] and [\ref=thm:noDetect]). The proof of impossibility follows that of [\cite=mossel2012stochastic] in the stochastic block model, with the change that we couple the graph to a broadcast model on a two-type Poisson Galton-Watson tree. The algorithm we propose involves a reduction to the stochastic block model and the algorithms of [\cite=massoulie2014community] [\cite=mossel2013proof].

We next consider spectral algorithms and show that computing the singular value decomposition (SVD) of the biadjacency matrix M of the model can succeed in recovering the planted partition even when the norm of the 'signal', [formula], is much smaller than the norm of the 'noise', [formula] (Theorem [\ref=thm:main]).

We show that at a sparser density, the SVD fails due to a localization phenomenon in the singular vectors: almost all of the weight of the top singular vectors is concentrated on a vanishing fraction of coordinates (Theorem [\ref=thm:SVDfail]).

We propose a modification of the SVD algorithm, Diagonal Deletion SVD, that succeeds at a sparser density still, far below the failure of the SVD (Theorem [\ref=thm:main]).

We apply the first algorithm to planted hypergraph partition and planted satisfiability problems to find the best known general bounds on the density at which the planted partition or assignment can be recovered efficiently (Theorem [\ref=thm:ksat]).

The model and main results

The bipartite stochastic block model

Fix parameters δ∈[0,2], n1  ≤  n2, and p∈[0,1  /  2]. Then we define the bipartite stochastic block model as follows:

Take two vertex sets V1,V2, with |V1|  =  n1, |V2| = n2.

Assign labels '+' and '-' independently with probability 1 / 2 to each vertex in V1 and V2. Let σ∈{  ±  1}n1 denote the labels of the vertices in V1 and τ∈{  ±  1}n2 denote the labels of V2.

Add edges independently at random between V1 and V2 as follows: for u∈V1,v∈V2 with σ(u)  =  τ(v), add the edge (u,v) with probability δp; for σ(u)  ≠  τ(v), add (u,v) with probability (2 - δ)p.

Algorithmic task: Determine the labels of the vertices given the bipartite graph, and do so with an efficient algorithm at the smallest possible edge density p.

Preliminaries and assumptions

In the application to planted satisfiability, it suffices to recover σ, the partition of the smaller vertex set, V1, and so we focus on that task here; we will accomplish that task even when the number of edges is much smaller than the size of V2. For a planted k-SAT problem or k-uniform hypergraph partitioning problem on n variables or vertices, the reduction gives vertex sets of size n1  =  Θ(n),n2  =  Θ(nk - 1), and so the relevant cases are extremely unbalanced.

We will say that an algorithm detects the partition if for some fixed ε > 0, independent of n1, whp it returns an ε-correlated partition, i.e. a partition that agrees with σ on a (1 / 2  +  ε)-fraction of vertices in V1 (again, up to the sign of σ).

We will say an algorithm recovers the partition of V1 if whp the algorithm returns a partition that agrees with σ on 1 - o(1) fraction of vertices in V1. Note that agreement is up to sign as σ and -  σ give the same partition.

Optimal algorithms for detection

On the basis of heuristic analysis of the belief propagation algorithm, [\cite=decelle2011asymptotic] made the striking conjecture that in the two part stochastic block model, with interior edge probability a / n, crossing edge probability b / n, there is a sharp threshold for detection: for (a - b)2  >  2(a + b) detection can be achieved with an efficient algorithm, while for (a - b)2  ≤  2(a + b), detection is impossible for any algorithm. This conjecture was proved by [\cite=mossel2012stochastic] [\cite=mossel2013proof] and [\cite=massoulie2014community].

Our first result is an analogous sharp threshold for detection in the bipartite stochastic block model at p  =  (δ - 1)- 2(n1n2)- 1 / 2, with an algorithm based on a reduction to the SBM, and a lower bound based on a connection with the non-reconstruction of a broadcast process on a tree associated to a two-type Galton Watson branching process (analogous to the proof for the SBM [\cite=mossel2012stochastic] which used a single-type Galton Watson process).

Let [formula] be fixed and n2  =  ω(n1). Then there is a polynomial-time algorithm that detects the partition [formula] whp if

[formula]

for any fixed ε  >  0.

On the other hand, if n2  ≥  n1 and

[formula]

then no algorithm can detect the partition whp.

Note that for [formula] it is clear that detection is impossible: whp there is no giant component in the graph. The content of Theorem [\ref=thm:noDetect] is finding the sharp dependence on δ.

Spectral algorithms

One common approach to graph partitioning is spectral: compute eigenvectors or singular vectors of an appropriate matrix and round the vector(s) to partition the vertex set. In our setting, we can take the n1  ×  n2 rectangular biadjacency matrix M, with rows and columns indexed by the vertices of V1 and V2 respectively, with a 1 in the entry (u,v) if the edge (u,v) is present, and a 0 otherwise. The matrix M has independent entries that are 1 with probability δp or (2  -  δ)p depending on the label of u and v and 0 otherwise.

A typical analysis of spectral algorithms requires that the second largest eigenvalue or singular value of the expectation matrix [formula] is much larger than the spectral norm of the noise matrix, [formula]. But here we have [formula], which is in fact much larger than [formula] when p  =  o(n- 11). Does this doom the spectral approach at lower densities?

For what values of p  =  p(n1,n2) is the singular value decomposition (SVD) of M correlated with the vector σ indicating the partition of V1?

In particular, this question was asked by [\cite=feldman2014algorithm]. We show that there are two thresholds, both well below p  =  n- 11: at p  =  (n- 2 / 31n- 1 / 32) the second singular vector of M is correlated with the partition of V1, but below this density, it is uncorrelated with the partition, and in fact localized. Nevertheless, we give a simple spectral algorithm based on modifications of M that matches the bound p  =  Õ((n1n2)- 1 / 2) achieved with subsampling by [\cite=feldman2014algorithm]. In the case of very unbalanced sizes, in particular in the applications noted above, these thresholds can differ by a polynomial factor in n1.

Our results locate two different thresholds for spectral algorithms for the bipartite block model: while the usual SVD is only effective with p  =  (n- 2 / 31n- 1 / 32), the modified diagonal deletion algorithm is effective already at p  =  (n- 1 / 21n- 1 / 22), which is optimal up to logarithmic factors. In particular, when n1 = n,n2 = nk - 1 for some k  ≥  3, as in the application above, these thresholds are separated by a polynomial factor in n.

First we give positive results for recovery using the two spectral algorithms.

Let n2  ≥  n1 log 4n1, with n1  →    ∞  . Let [formula] be fixed with respect to n1,n2. Then there exists a universal constant C > 0 so that

If p  =  C(n1n2)- 1 / 2 log n1, then whp the diagonal deletion SVD algorithm recovers the partition [formula].

If p  =  Cn- 2 / 31n- 1 / 32 log n1, then whp the unmodified SVD algorithm recovers the partition.

Next we show that below the recovery threshold for the SVD, the top left singular vectors are in fact localized: they have nearly all of their mass on a vanishingly small fraction of coordinates.

Let n2  ≥  n1 log 4n1. For any constant c > 0, let p = cn- 2 / 31n- 1 / 32, t  ≤  n1 / 31, and r = n1  /   log n1. Let [formula], and [formula] be the top t left unit-norm singular vectors of M.

Then, whp, there exists a set [formula] of coordinates, |S|  ≤  r, so that for all 1  ≤  i  ≤  t, there exists a unit vector ui supported on S so that

[formula]

That is, each of the first t singular vectors has nearly all of its weight on the coordinates in S. In particular, this implies that for all 1  ≤  i  ≤  t, vi is asymptotically uncorrelated with the planted partition:

[formula]

One point of interest in Theorem [\ref=thm:SVDfail] is that in this case of a random biadjacency matrix of unbalanced dimension, the localization and delocalization of the singular vectors can be understood and analyzed in a simple manner, in contrast to the more delicate phenomenon for random square adjacency matrices.

Our techniques use bounds on the norms of random matrices and eigenvector perturbation theorems, applied to carefully chosen decompositions of the matrices of interest. In particular, our proof technique suggested the Diagonal Deletion SVD, which proved much more effective than the usual SVD algorithm on these unbalanced bipartite block models, and has the advantage over more sophisticated approaches of being extremely simple to describe and implement. We believe it may prove effective in many other settings.

Under what conditions might we expect the Diagonal Deletion SVD outperform the usual SVD? The SVD is a central algorithm in statistics, machine learning, and computer science, and so any general improvement would be useful. The bipartite block model addressed here has two distinctive characteristics: the dimensions of the matrix M are extremely unbalanced, and the entries are very sparse Bernoulli random variables, a distribution whose fourth moment is much larger than the square of its second moment. These two facts together lead to the phenomenon of multiple spectral thresholds and the outperformance of the SVD by the Diagonal Deletion SVD. Under both of these conditions we expect dramatic improvement by using diagonal deletion, while under one or the other condition, we expect mild improvement. We expect diagonal deletion will be effective in the more general setting of recovering a low-rank matrix in the presence of random noise, beyond our setting of adjacency matrices of graphs.

Planted k-SAT and hypergraph partitioning

[\cite=feldman2014algorithm] reduce three planted problems to solving the bipartite block model: planted hypergraph partitioning, planted random k-SAT, and Goldreich's planted CSP. We describe the reduction here and calculate the density at which our algorithm can detect the planted solution by solving the resulting bipartite block model.

We state the general model in terms of hypergraph partitioning first.

Planted hypergraph partitioning

Fix a function Q:{  ±  1}k  →  [0,1] so that [formula]. Fix parameters n and p∈(0,1) so that max xQ(x)2kp  ≤  1. Then we define the planted k-uniform hypergraph partitioning model as follows:

Take a vertex set V of size n.

Assign labels '+' and '-' independently with probability 1 / 2 to each vertex in V. Let σ∈{  ±  1}n denote the labels of the vertices.

Add (ordered) k-uniform hyperedges independently at random according to the distribution

[formula]

where σ(e) is the evaluation of σ on the vertices in e.

Algorithmic task: Determine the labels of the vertices given the hypergraph, and do so with an efficient algorithm at the smallest possible edge density p.

Usually Q will be symmetric in the sense that Q(x) depends only on the number of + 1's in the vector x, and in this case we can view hyperedges as unordered. We assume that Q is not identically 2- k as this distribution would simply be uniform and the planted partition would not be evident.

Planted k-satisfiability is defined similarly: we fix an assignment σ to n boolean variables which induces a partition of the set of 2n literals (boolean variables and their negations) into true and false literals. Then we add k-clauses independently at random, with probability proportional to the evaluation of Q on the k literals of the clause.

Planting distributions for the above problems are classified by their distribution complexity, [formula], where (S) is the discrete Fourier coefficient of Q corresponding to the subset S  ⊆  [k]. This is an integer between 1 and k, where k is the uniformity of the hyperedges or clauses.

A consequence of Theorem [\ref=thm:detection] is the following:

There is an efficient algorithm to detect the planted partition in the random k-uniform hypergraph partitioning problem, with planting function Q, when

[formula]

for any fixed ε > 0. Similarly, in the planted k-satisfiability model with planting function Q, there is an efficient algorithm to detect the planted assignment when

[formula]

In both cases, if the distribution complexity of Q is at least 3, we can achieve full recovery at the given density.

Suppose Q has distribution complexity r. Fix a set S  ⊆  [k] with (S)  ≠  0, and |S| = r. The first step of the reduction of [\cite=feldman2014algorithm] transforms each k-uniform hyperedge into an r-uniform hyperedge by selecting the vertices indicated by the set S. Then a bipartite block model is constructed on vertex sets V1,V2, with V1 the set of all vertices in the hypergraph (or literals in the formula), and V2 the set of all (r - 1)-tuples of vertices or literals. An edge is added by taking each r-uniform edge and splitting it randomly into sets of size 1 and r - 1 and joining the associated vertices in V1 and V2. The parameters in our model are n1 = n and n2  ~  nr - 1 (considering ordered (r - 1)-tuples of vertices or literals).

These edges appear with probabilities that depend on the parity of the number of vertices on one side of the original partition in the joined sets, exactly the bipartite block model addressed in this paper; the parameter δ in the model is given by δ  =  1 + 2k(S) (see Lemma 1 of [\cite=feldman2014algorithm]). Theorems [\ref=thm:detection] then states that detection in the resulting block model exhibits a sharp threshold at edge density p* , with [formula]. The difference in bounds in Theorem [\ref=thm:ksat] is due to the two models having n vertices and 2n literals respectively.

To go from an ε-correlated partition to full recovery, if r  ≥  3, we can appeal to Theorem 2 of [\cite=bogdanov2009security] and achieve full recovery using only a linear number of additional hyperedges or clauses, which is lower order than the Θ(nr / 2) used by our algorithm.

Note that Theorem [\ref=thm:noDetect] says that no further improvement can be gained by analyzing this particular reduction to a bipartite stochastic block model.

There is some evidence that up to constant factors in the clause or hyperedge density, there may be no better efficient algorithms [\cite=ow] [\cite=feldman2013complexity], unless the constraints induce a consistent system of linear equations. But in the spirit of [\cite=decelle2011asymptotic], we can ask if there is in fact a sharp threshold for detection of planted solutions in these models. In one special case, such sharp thresholds have been conjectured: [\cite=krzakala2014reweighted] have conjectured threshold densities based on fixed points of belief propagation equations. The planted k-SAT distributions covered, however, are only those with distribution complexity r = 2: those that are known to be solvable with a linear number of clauses. We ask if there are sharp thresholds for detection in the general case, and in particular for those distributions with distribution complexity r  ≥  3 that cannot be solved by Gaussian elimination. In particular, in the case of the parity distribution we conjecture that there is a sharp threshold for detection.

Partition a set of n vertices at random into sets A,B. Add k-uniform hyperedges independently at random with probability δp if the number of vertices in the edge from A is even and (2 - δ)p if the number of vertices from A is odd. Then for any δ∈(0,2) there is a constant cδ so that [formula] is a sharp threshold for detection of the planted partition by an efficient algorithm. That is, if [formula], then there is a polynomial-time algorithm that detects the partition whp, and if [formula] then no polynomial-time algorithm can detect the partition whp.

This is a generalization to hypergraphs of the SBM conjecture of [\cite=decelle2011asymptotic]; the k = 2 parity distribution is that of the stochastic block model. We do not venture a guess as to the precise constant [formula], but even a heuristic as to what the constant might be would be very interesting.

Relation to Goldreich's generator

[\cite=goldreich2000candidate]'s pseudorandom generator or one-way function can be viewed as a variant of planted satisfiability. Fix an assignment σ to n boolean variables, and fix a predicate P:{  ±  1}k  →  {0,1}. Now choose m k-tuples of variables uniformly at random, and label the k-tuple with the evaluation of P on the tuple with the boolean values given by σ. In essence this generates a uniformly random k-uniform hypergraph with labels that depend on the planted assignment and the fixed predicate P. The task is to recover σ given this labeled hypergraph. The algorithm we describe above will work in this setting by simply discarding all hyperedges labeled 0 and working with the remaining hypergraph.

Related work

The stochastic block model has been a source of considerable recent interest. There are many algorithmic approaches to the problem, including algorithms based on maximum-likelihood methods [\cite=sn], belief propagation [\cite=decelle2011asymptotic], spectral methods [\cite=mcsherry2001spectral], modularity maximization [\cite=bc], and combinatorial methods [\cite=bcls], [\cite=df], [\cite=js], [\cite=ck]. [\cite=coja2010graph] gave the first algorithm to detect partitions in the sparse, constant average degree regime. [\cite=decelle2011asymptotic] conjectured the precise achievable constant and subsequent algorithms [\cite=massoulie2014community] [\cite=mossel2013proof] [\cite=bordenave2015non] [\cite=as] achieved this bound. Sharp thresholds for full recovery (as opposed to detection) have been found by [\cite=mossel2015consistency] [\cite=abh] [\cite=hajek2015achieving].

[\cite=bogdanov2009security] used ideas for reconstructing assignments to random 3-SAT formulas in the planted 3-SAT model to show that Goldreich's construction of a one-way function in [\cite=goldreich2000candidate] is not secure when the predicate correlates with either one or two of its inputs. For more on Goldreich's PRG from a cryptographic perspective see the survey of [\cite=applebaum2013cryptographic].

[\cite=feldman2014algorithm] gave an algorithm to recover the partition of V1 in the bipartite stochastic block model to solve instances of planted random k-SAT and planted hypergraph partitioning using subsampled power iteration.

A key part of our analysis relies on looking at an auxiliary graph on V1 with edges between vertices which share a common neighbor; this is known as the one-mode projection of a bipartite graph: [\cite=zhou2007bipartite] give an approach to recommendation systems using a weighted version of the one-mode projection. One-mode projections are implicitly used in studying collaboration networks, for example in [\cite=newman2001scientific]'s analysis of scientific collaboration networks. [\cite=larremore2014efficiently] defined a general model of bipartite block models, and propose a community detection algorithm that does not use one-mode projection.

The behavior of the singular vectors of a low rank rectangular matrix plus a noise matrix was studied by [\cite=benaych2012singular]. The setting there is different: the ratio between n1 and n2 converges, and the entries of the noise matrix are mean 0 variance 1.

[\cite=butucea2015sharp] and [\cite=hajek2015submatrix] both consider the case of recovering a planted submatrix with elevated mean in a random rectangular Gaussian matrix.

Notation

All asymptotics are as n1  →    ∞  , so for example, 'E occurs whp' means [formula]. We write f(n1)  =  Õ(g(n1)) and f(n1)  =  (g(n1)) if there exist constants C,c so that f(n1)  ≤  C log c(n1)  ·  g(n1) and f(n1)  ≥  g(n1) / (C log c(n1)) respectively. For a vector, [formula] denotes the l2 norm. For a matrix, [formula] denotes the spectral norm, i.e. the largest singular value (or largest eigenvalue in absolute value for a square matrix). For ease of reading, C will always denote an absolute constant, but the value may change during the course of the proofs.

Proof of Theorem [\ref=thm:detection]: detection

In this section we prove Theorem [\ref=thm:detection], giving an optimal algorithm for detection in the bipartite stochastic block model when n2  =  ω(n1). The main idea of the proof is that almost all of the information in the bipartite block model is in the subgraph induced by V1 and the vertices of degree two in V2. From this induced subgraph of the bipartite graph we form a graph [formula] on V1 by replacing each path of length two from V1 to V2 back to V1 with a single edge between the two endpoints in V1. We then apply an algorithm from [\cite=massoulie2014community] [\cite=mossel2013proof], or [\cite=bordenave2015non] to detect the partition.

Fix ε > 0. Given an instance G of the bipartite block model with

[formula]

we reduce to a graph [formula] on V1 as follows:

Sort V2 according to degrees and remove any vertices (with their accompanying edges) which are not of degree 2.

We now have a union of 2-edge paths from vertices in V1 to vertices in V2 and back to vertices in V1. Create a multi-set of edges E on V1 by replacing each 2-path u  -  v - w by the edge (u,w).

Choose N from the distribution [formula].

If N  >  |E|, then stop and output 'failure'. Otherwise, select N edges uniformly at random from E to form the graph [formula] on V1, replacing any edge of multiplicity greater than one with a single edge.

Apply an SBM algorithm to [formula] to partition V1.

We now determine the distribution of [formula] conditioned on σ. Let β1 be the bias of + 1 labels in σ, [formula]. Conditioned on β1, the degrees [formula] of the vertices of V2 are independent, identically distributed random variables. Let Yi = d(vi). Under the high probability event that β1  =  o(n- 1 / 31), we can compute

[formula]

and so whp, [formula], and N  <  |E|. Note that it is only at this step that we require the assumption that n2  =  ω(n1).

Conditioned on σ, the edges in E are independent and identically distributed, with distribution of a given edge e = (u,v) as

[formula]

When β1  =  o(n- 1 / 31),

[formula]

By Poisson thinning this means that the number of times each +  +  or -  -  edge appears in the subsampled collection of edges is a Poisson of mean [formula], and each +  -  edge according to a Poisson of mean [formula], and all of these edge counts are independent.

Now define a so that [formula], and b so that [formula].

From the construction above, conditioned on σ the distribution of [formula] is that of the stochastic block model on V1 with partition σ: each edge interior to the partition is present with probability a / n1, each crossing edge with probability b / n1, and all edges are independent.

For σ such that β1  =  o(n- 1 / 3), we have

[formula]

For these values of a and b the condition for detection in the SBM, (a - b)2  ≥  (1 + ε)2(a + b) is satisfied and so whp the algorithms from [\cite=massoulie2014community] [\cite=mossel2013proof] [\cite=bordenave2015non] will find a partition that agrees with σ on [formula] fraction of vertices.

Proof of Theorem [\ref=thm:noDetect]: impossibility

The proof of impossibility below the threshold (a - b)2  =  2(a + b) in [\cite=mossel2012stochastic] proceeds by showing that the log n depth neighborhood of a vertex ρ, along with the accompanying labels, can be coupled to a binary symmetric broadcast model on a Poisson Galton-Watson tree. In this model, it was shown by [\cite=evans2000broadcasting] that reconstruction, recovering the label of the root given the labels at depth R of the tree, is impossible as R  →    ∞  , for the corresponding parameter values (the critical case was shown by [\cite=pemantle2010critical]).

In the binary symmetric broadcast model, the root of a tree is labeled with a uniformly random label + 1 or - 1, and then each child takes its parent's label with probability 1 - η and the opposite label with probability η, independently over all of the parent's children. The process continues in each successive generation of the tree.

The criteria for non-reconstruction can be stated as (1 - 2η)2B  ≤  1, where B is the branching number of the tree T. The branching number is B  =  pc(T)- 1, where pc is the critical probability for bond percolation on T (see [\cite=lyons1990random] for more on the branching number).

Assume first that n2  ~  cn1 for some constant c, and that p  =  d / n1. Then there is a natural multitype Poisson branching process that we can associate to the bipartite block model: nodes of type 1, corresponding to vertices in V1, have a [formula] number of children of type 2; nodes of type 2, corresponding to vertices in V2, have a [formula] number of children of type 1. The branching number of this distribution on trees is [formula], an easy calculation by reducing to a one-type Galton Watson process by combining two generations into one. Transferring the block model labeling to the branching process gives η  =  δ / 2, and so the threshold for reconstruction is given by

[formula]

or in other words,

[formula]

exactly the threshold in Theorem [\ref=thm:noDetect]. In fact, in this case the proof from [\cite=mossel2012stochastic] can be carried out in essentially the exact same way in our setting.

Now take n2  =  ω(n1). A complication arises: the distribution of the number of neighbors of a node of type 1 does not converge (its mean is n2p  →    ∞  ), and the distribution of the number of neighbors of a node of type 2 converges to a delta mass at 0. But this can be fixed by ignoring the vertices in V2 of degree 0 and 1. Now we explore from a vertex ρ∈V1, but discard any vertices from V2 that do not have a second neighbor. We denote by Ĝ the subgraph of G induced by V1 and the vertices of V2 of degree at least 2. Let T be the branching process associated to this modified graph: nodes of type 1 have [formula] neighbors of type 2, and nodes of type 2 have exactly 1 neighbor of type 1, where here [formula]. The branching number of this process is d, and the reconstruction threshold is (δ - 1)2d  ≤  1, again giving the threshold [formula], as required.

As in [\cite=mossel2012stochastic], the proof of impossibility will show the stronger statement that conditioned on the label of a fixed vertex w∈V1 and the graph G, the variance of the label of another fixed vertex ρ tends to 1 as n1  →    ∞  . The proof of this fact has two main ingredients: showing that the depth R neighborhood of a vertex ρ in the bipartite block model (with vertices of degree 0 and 1 in V2 removed) can be coupled with the branching process described above, and showing that conditioned on the labels on the boundary of the neighborhood, the label of ρ is asymptotically independent of the rest of the graph and the labels outside of the neighborhood. We will use the notation from Section 4 of [\cite=mossel2012stochastic] and indicate the places in which our proof must differ; the most significant is that we must show that the vertices of degree 0 and 1 in V2 give essentially no information about the label of ρ.

First note that in Proposition 4.2 from [\cite=mossel2012stochastic], R  =  Θ( log n), but for the proof of Theorem 2.1 all that is required is R  =  ω(1). We choose

[formula]

Let T be the branching process described above, starting with a root of type 1. We will denote the labeling functions of nodes of type 1 and type 2 in T by σ̂ and τ̂ respectively. We will consider two steps of the exploration process at once, so the depth 0 neighborhood is ρ itself, the depth 1 neighborhood is ρ, its neighbors (of degree at least 2), and the neighbors of these neighbors. The depth r neighborhood then includes those vertices in V1 at distance 2r from ρ. Let Ĝr be this depth r neighborhood in Ĝ, and σr,τr be the labelings of V1 and V2 restricted to the vertices in Ĝr. Define TR,σ̂,τ̂ as the same objects for the tree process. Let ∂1Ĝr,∂1Tr be the set of vertices from V1, nodes of type 1, in the last layer, and ∂2Ĝr,∂2Tr the vertices from V2 and nodes of type 2 in the last layer. Let [formula]. We will show:

With R as above, there is a coupling so that (ĜR,σR,τR)  =  (TR,σ̂R,τ̂R) whp.

T can be constructed by three sequences of independent random variables [formula], [formula], for u of type 1 in T, and [formula] for v of type 2 in T. To create the branching process, we start with the root ρ of type 1, and assign it + 1 or - 1 label at random. We then assign it Yaρ type-2 children of the same label and Ybρ type-2 children of the opposite label. All together the number of children has a [formula] distribution, and the labels are selected independently to agree with ρ with probability δ / 2 and to disagree with probability 1 - δ / 2. Now each child v of type 2 has exactly one child of its own, whose label agrees if Xv = 1 and disagrees otherwise. Then the process continues inductively.

Now consider exploring the depth R neighborhood of ρ in Ĝ. We index the vertices from V1 in the order in which we encounter them in this breadth-first exploration. We explore two layers of the neighborhood at once: the active vertex ui will always be from V1. To explore from ui, we reveal all edges from ui to unexplored vertices in V2; call these neighbors N(ui). We then set all v∈N(ui) to be explored, and query all edges from N(ui) to unexplored vertices in V1; call these vertices N2(ui), as they are all connected by a path of length 2 to ui in Ĝ. Set all vertices in N2(ui) to explored, and place them in a FIFO queue of vertices. Then set ui to dead, and take the next vertex ui + 1 from the queue, set to active, and repeat.

Let Naa(ui),Nab(ui),Nba(ui),Nbb(ui) be the number of paths of length 2 from ui to an unexplored vertex in V1, with the subscripts denoting whether the labels along the path agree or disagree with the label of ui; e.g. if σ(ui) =  + 1, then Nba(ui) is the number of paths that go through a vertex v∈V2 with label - 1 and then to an unexplored vertex w∈V1 with label + 1. Let V+1(i),V-1(i) be the number of unexplored vertices in V1 with the respective labels at the moment ui becomes active, and likewise for V+2(i),V-2(i). If we condition on the bias of σ and τ, [formula] and [formula], then at each step of the exploration, the distribution of the N*  *'s depends only on the V*i's.

As in [\cite=mossel2012stochastic], let Ar be the event that no vertex in [formula] has more than one neighbor in Ĝr - 1; let Br be the event that no vertex in ∂2Ĝr has more than one neighbor in ∂1Ĝr, also define the event [formula]. Then analogously to Lemma 4.3 in [\cite=mossel2012stochastic], we have

If

(Ĝr - 1,σr - 1,τr - 1)  =  (Tr - 1,σ̂r - 1,τ̂r - 1).

For every u∈∂1Ĝr - 1, Naa(u)  +  Nab(u) = Yau  =  (d2δ / 2); Nba(u)  +  Nbb(u) = Ybu  =  (d2(2 - δ) / 2).

For every u∈∂1Ĝr - 1,

[formula]

[formula]

Ar,Br,Dr hold.

Then (Ĝr,σr,τr)  =  (Tr,σ̂r,τ̂r).

Next we define [formula].Then,

Whp, Ar,Br,Cr, and Dr hold for all 1  ≤  r  ≤  R and |ĜR|  =  O( min {n1 / 81,(n2 / n1)1 / 8}).

As in Lemma 4.4 from [\cite=mossel2012stochastic], stochastic domination and a Chernoff bound show that Ar,Br,Cr hold whp: the distribution of N2(ui) is dominated by a [formula]. If v∈V2 is revealed to be a neighbor of ui, then the probability it has at least 2 additional neighbors in V1 is bounded by O(n21p2)  =  O(n1 / n2). Given that |ĜR|  =  O( min {n1 / 81,(n2 / n1)1 / 8}), a union bound gives that Dr holds for all 1  ≤  r  ≤  R whp.

Finally, we complete the proof of Lemma [\ref=lem:Rdepth]. Condition on the event that β1  =  O(n- 1 / 31) and β2  =  O(n- 1 / 32), which occurs with probability ≥  1 -  exp ( - Θ(n1 / 31)). Condition also on the event that the number of edges incident to all explored vertices in V1 is at most 2n2p min {n1 / 81,(n2 / n1)1 / 8}. Under these two events we have |V+1(i)|,|V-1(i)|  =  n1 / 2(1 + O(n- 1 / 31)) and |V+2(i)|,|V-2(i)|  =  n2 / 2(1 + O(n- 1 / 32)) for all 1  ≤  i  ≤  R.

For ui∈V1, the distribution of the number of its unexplored neighbors of degree 2 and label + 1 is [formula], and the distribution of the number of its unexplored neighbors of degree 2 and label - 1 is [formula] where

[formula]

and likewise

[formula]

From Lemma 4.6 in [\cite=mossel2012stochastic], we then have that for σ(u)∈{  ±  1},

[formula]

Since we have |ĜR|  ≤  n1 / 81 whp, a union bound over [formula] shows that there is a coupling so that whp for all r  ≤  R and every u∈∂1Ĝr - 1, Naa(u)  +  Nab(u) = Yau; Nba(u)  +  Nbb(u) = Ybu.

Next, we show that the probability the second neighbor of a vertex of degree 2 in V2 has the same label is close to δ / 2. Let ui be the current active vertex, v a neighbor of ui of degree 2, and w the second unexplored neighbor of v. Then

[formula]

This shows that the coupling can be extended to the Xv's, and that whp under this coupling (ĜR,σR,τR)  =  (TR,σ̂R,τ̂R).

Let V(0)2 and V(1)2 be the subsets of V2 of degree 0 and 1 respectively. Let V(  ≥  2)2 be the vertices of V2 of degree at least 2. Recall that Ĝ is the subgraph of G induced by V1 and V(  ≥  2)2. From Ĝ we can determine the set [formula] but not the two sets individually.

The following is an analogue of Lemma 4.7 in [\cite=mossel2012stochastic]. It says that conditioned on the labels at depth R in Ĝ from the root ρ, neither the graph outside the R-neighborhood, nor the vertices in V(  ≤  1)2 contain significant information about the label of ρ.

Let A,B,C be a partition of [formula] so that B separates A and C in Ĝ. Assume [formula]. Then

[formula]

whp over G,σ and τ.

We delay the proof of Lemma [\ref=lem:47] to the Appendix.

Now, we can finish the proof of Theorem [\ref=thm:noDetect]. By the monotonicity of conditional variance,

[formula]

Then whp w∉ĜR, and so by Lemma [\ref=lem:47],

[formula]

(since σ(ρ) and σ(w) are independent given G,σ∂1ĜR,τ∂2ĜR). By Lemma [\ref=lem:Rdepth],

[formula]

From the results of [\cite=evans2000broadcasting] and the condition [formula],

[formula]

Thus, whp

[formula]

as well. This implies that the labels of ρ and w are asymptotically independent and in particular proves Theorem [\ref=thm:noDetect].

Proof of Theorem [\ref=thm:main]: Recovery

We will follow a similar framework to prove both parts of Theorem [\ref=thm:main]. Recalling M to be the adjacency matrix, let [formula] and [formula].

A simple computation shows that the second eigenvector of [formula] is the vector σ that we wish to recover; we will consider the different perturbations of [formula] that arise with the three spectral algorithms and show that at the respective thresholds, the second eigenvector of the resulting matrix is close to σ. To analyze the diagonal deletion SVD, we must show that the second eigenvector of B is highly correlated with σ (the addition of a constant multiple of the identity matrix does not change the eigenvectors). The main step is to bound the spectral norm [formula]. Since the entries of B are not independent, we will decompose B into a sequence of matrices based on subgraphs induced by vertices of a given degree in V2. This (Lemma [\ref=lem:norms]) is the most technical part of the work.

To analyze the unmodified SVD, we write [formula]. The left singular vectors of M are the eigenvectors of MMT. [formula] has σ as its second eigenvector and [formula] is a multiples of the identity matrix and so adding it does not change the eigenvectors. As above we bound [formula] and what remains is showing that the difference of the matrix DV with its expectation has small spectral norms at the respective thresholds; this involves simple bounds on the fluctuations of independent random variables.

We will assume that σ and τ assign + 1 and - 1 labels to an equal number of vertices; this allows for a clearer presentation, but is not necessary to the argument. We will treat σ and τ as unknown but fixed, and so expectations and probabilities will all be conditioned on the labelings.

The main technical lemma is the following:

Define B,DV as above. Assume n1,n2, and p are as in Theorem [\ref=thm:main]. Then there exists an absolute constant C so that

[formula], with λ1  =  n1n2p2 and λ2  =  (δ - 1)2n1n2p2, where J is the all ones n1  ×  n1 matrix.

For p  ≥  n- 1 / 21n- 1 / 22 log n1, [formula] whp.

[formula] is a multiple of the identity matrix.

For p  ≥  n- 2 / 31n- 1 / 32 log n1, [formula] whp.

This is proved in Appendix [\ref=sec:proveBBound].

We also will use the following lemma from [\cite=lelarge2013reconstruction] to round a unit vector with high correlation with σ to a ±  1 vector that denotes a partition:

For any x∈{ - 1, + 1}n and [formula] with [formula] we have

[formula]

where d represents the Hamming distance.

The next lemma is a classic eigenvector perturbation theorem. Denote by PA(S) the orthogonal projection onto the subspace spanned by the eigenvectors of A corresponding to those of its eigenvalues that lie in S.

Let A be an n  ×  n symmetric matrix with [formula], with |λk|  -  |λk + 1|  ≥  2δ. Let B be a symmetric matrix with [formula]. Let Ak and (A + B)k be the spaces spanned by the top k eigenvectors of the respective matrices. Then

[formula]

In particular, If |λ1|  -  |λ2|  ≥  2δ, |λ2|  -  |λ3|  ≥  2δ, [formula], and e2(A), e2(A + B) are the second (unit) eigenvectors of A and A + B, respectively, satisfying e2(A)  ·  e2(A + B)  ≥  0, then [formula].

In the particular case, let u1,u2 be the first two eigenvectors of A, and v1,v2 the first two eigenvectors of A + B, with signs chosen so that u1  ·  v1,u2  ·  v2  ≥  0. Let [formula]. First we apply the lemma with k = 1 to get [formula]. Applying this to v1, we get [formula], and so [formula]. The triangle inequality gives [formula]. Now apply the lemma with k = 2 to get [formula]. Apply this to v2 and use the triangle inequality again to get [formula].

Now using Lemmas [\ref=lem:norms], [\ref=lem:lelarge2], and [\ref=lem:daviskahan] we prove parts 1 and 2 of Theorem [\ref=thm:main].

Diagonal deletion SVD

Let p  ≥  n- 1 / 21n- 1 / 22 log n1. Part 1 of Lemma [\ref=lem:norms] shows that if we had access to the second eigenvector of [formula], we would recover σ exactly. (The addition of a multiple of the identity matrix does not change the eigenvectors). Instead we have access to [formula], a noisy version of the matrix we want. We use a matrix perturbation inequality to show that the top eigenvectors of the noisy version are not too far from the original eigenvectors.

Let y1 and y2 be the top two eigenvectors of B, and B̂ be the space spanned by y1 and y2, and [formula] the space spanned by the top two eigenvectors of [formula]. Then Lemma [\ref=lem:daviskahan] gives

[formula]

where the inequality holds whp by Lemma [\ref=lem:norms]. Assuming δ∈(0,2), we use the particular case of Lemma [\ref=lem:daviskahan] to show that [formula]. We round y2 by signs to get z, and then apply Lemma [\ref=lem:lelarge2] to show that whp the algorithm recovers 1 - o(1) fraction of the coordinates of σ. (If δ = 0 or 2, then instead of taking the second eigenvector, we take the component of B̂ perpendicular to the all ones vector and get the same result).

The SVD

Let p  ≥  n- 2 / 31n- 1 / 32 log n1. Let y1 and y2 be the top two left singular vectors of M, and M2 be the space spanned by y1 and y2. y1 and y2 are the top two eigenvectors of MMT  =  B  +  DV. Again Lemma [\ref=lem:daviskahan] gives that whp,

[formula]

This gives [formula], and shows that the SVD algorithm recovers σ whp. Note that in this case [formula]. It is these fluctuations on the diagonal that explain the poor performance of the SVD and its need for a higher edge density for success.

Proof of Theorem [\ref=thm:SVDfail]: Failure of the vanilla SVD

Here we again use a matrix perturbation lemma, but in the opposite way: we will show that the 'noise matrix' [formula] has a large spectral norm (and an eigenvalue gap), and thus adding the 'signal matrix' approximately preserves the space spanned by the top eigenvalues. This shows that the top t eigenvectors of B + DV have almost all their weight on a small number of coordinates and is enough to conclude that they cannot be close to the planted vector σ.

The perturbation lemma we use is a generalization of the Davis-Kahan theorem found in [\cite=bhatia1997matrix].

Let A and B be n  ×  n symmetric matrices with the eigenvalues of A ordered [formula]. Suppose r > k, λk  -  λr  >  2δ, and [formula]. Let Ar denote the subspace spanned by the first r eigenvectors of A and likewise for (A + B)k. Then

[formula]

In particular, if vk is the kth unit eigenvector of (A + B), then there is some unit vector u∈Ar so that

[formula]

This lemma is a special case of Theorem VII.3.1 from [\cite=bhatia1997matrix], itself a generalization of the Davis-Kahan theorem. In the particular case, write [formula] where v(r)k∈Ar and [formula]. Let [formula]. Then, by multiplying we get

[formula]

We see that [formula], and thus [formula]. Take [formula] and use the triangle inequality to complete the lemma: u∈Ar and [formula].

We also need to analyze the degrees of the vertices in V1. The following lemma gives some basic information about the degree sequence:

Let [formula] be the sequence of degrees of vertices in V1. Then there exist constants c1,c2,c3 so that

The di's are independent and identically distributed, with distribution [formula].

[formula].

Whp, [formula].

Whp, [formula].

Whp, [formula].

The lemma follows from basic Chernoff bounds and the first- and second-moment methods. Now, we can finish the proof of Theorem [\ref=thm:SVDfail].

Let p  =  cn- 2 / 31n- 1 / 32. The left singular vectors of M are the eigenvectors of B + DV. Recall that DV is a diagonal matrix with the ith entry the degree of the ith vertex of V1. [formula] is therefore a multiple of the identity matrix, and so subtracting [formula] from B + DV does not change its eigenvectors. The standard basis vectors form an orthonormal set of eigenvectors of [formula].

For the constants c2,c3 in Lemma [\ref=lem:degrees], let [formula] and [formula]. Order the eigenvalues of [formula] as [formula] and let r be the smallest integer such that λr  <  η2. Then we have [formula] for all 1  ≤  i  ≤  t. From Lemma [\ref=lem:degrees], r  ≤  n1  /   log n1.

We now bound

[formula]

Now Lemma [\ref=lem:perturb] says that if vi is the ith eigenvector of [formula], then there is a vector u in the span of the first r eigenvectors of [formula] so that

[formula]

The span of the first r eigenvectors of [formula] is supported on only r coordinates, so u is far from [formula]:

[formula]

By the triangle inequality, vi must also be far from [formula]: [formula]. This proves Theorem [\ref=thm:SVDfail].

Acknowledgements

We thank the Institute for Mathematics and its Applications (IMA) in Minneapolis, where part of this work was done, for its support and hospitality.

Proof of Lemma [\ref=lem:47].

[formula]

We now show that the last factor is 1 + o(1) whp over σ,τ, and G.

Let U  ⊆  V1, and σU the restriction of σ to U. Then

[formula]

whp over the choices of σ,G.

We leave the proof of Lemma [\ref=lem:degree01] to Appendix B.

To prove Lemma [\ref=lem:47], it remains to show

[formula]

whp over σ,τ,G. Now that we have removed vertices of degree 0 and 1 from V2, the proof proceeds along the same lines as the proof of Lemma 4.7 of [\cite=mossel2012stochastic].

For u∈V1,v∈V2, define

[formula]

Define QU1,U2 to be the product of ψu,v(Ĝ,σ,τ) over all u∈U1,v∈U2. Define

[formula]

We denote by η and φ labelings of V1 and V2 respectively. ηA refers to the restriction of η to the set [formula], and so on. We write σAB instead of [formula] for cleaner notation. Equation ([\ref=47eq]) is equivalent to

[formula]

We rewrite the LHS of ([\ref=eq:47.2]) as

[formula]

This is a similar expression as encountered in the proof of Lemma 4.7 in [\cite=mossel2012stochastic] apart from the factors involving Q-. To address these factors we use the following Lemma:

Let [formula] so that [formula]. Then for any η,φ so that ηU  =  σU, φU  =  τU,

[formula]

with probability 1 - o(1) over the choice of σ,τ,Ĝ.

The proof is similar to that of Lemma [\ref=lem:degree01], where we use the fact that Uc is small to show that β1 is essentially determined on U.

With Lemma [\ref=lem:qm], equation ([\ref=eq47.3]) becomes

[formula]

with probability 1 - o(1). Now using (2) from [\cite=mossel2012stochastic] gives

[formula]

with probability 1 - o(1). Now since QA,AB does not depend on σC,τC, we have

[formula]

Now we can proceed similarly with the RHS of ([\ref=eq:47.2]):

[formula]

where we have again used Lemma [\ref=lem:qm] and (2) from [\cite=mossel2012stochastic]. Now ([\ref=eq:47.6]) matches ([\ref=eq:47.4]) and so

[formula]

with probability 1 - o(1) over σ,τ,Ĝ, completing the proof of Lemma [\ref=lem:47].

Proof of Lemma [\ref=lem:degree01].

Note that conditioned on Ĝ and β1, the distribution of G is that of independently choosing degree 1 or 0 for each v∈V2 of degree less than 2, with probability that depends only on β1. Let v∈V2. We can condition on β1 and compute

[formula]

Similarly,

[formula]

where [formula] and [formula].

Then we have

[formula]

and

[formula]

[formula]

Given σ, β1 is determined, and whp over the choice of σ, β41  ≤  n- 9 / 51. Similarly, whp over the choice of σU, the conditional expectation of β41 is ≤  n- 9 / 51. All together this gives that whp,

[formula]

This proves Lemma [\ref=lem:degree01].

Proof of Lemma [\ref=lem:norms]

We use another auxiliary lemma, a high probability bound on the norm of a random matrix with mean 0 independent entries. Such a lemma is proved for Bernoulli random entries in [\cite=vu2014simple] [\cite=crv], here we extend it to Poisson entries.

Let E be an n  ×  n symmetric random matrix with zeros on the diagonal and independent entries eij above the diagonal which take the values Xij  -  λij where each Xij is a Poisson random variable with mean λij. Then there is a constant C > 0, so that if σ2: =  max ijλij, and σ2  ≥  C log n / n,

[formula]

for any T  ≥  1.

If [formula], then we can apply Theorem 1.5 from [\cite=vu2005spectral] (the failure probability can be made as small as n- T with the additional factor T in the bound).

For [formula], we truncate each Xij by writing

[formula]

where X̂ij  =   min {Xij,1} and ij  =  Xij  -  X̂ij. We then define two matrices Ê and Ẽ with [formula] and [formula]. Thus E  =  Ê  +  Ẽ, and so we will bound [formula].

Note that [formula], and each X̂ij is a Bernoulli random variable, and so we can apply Lemma 3.4 from [\cite=vu2014simple] to get [formula] with probability ≥  1 - n- T (again inspecting the details of the proof in [\cite=vu2014simple] gives a failure probability of n- T at the expense of the extra factor T in the bound).

To bound [formula], consider one row sum, [formula]. For [formula], the random variables [formula] are independent, mean 0 random variable with variance O(σ4) with Poisson tails, and so a Chernoff bound gives

[formula]

and so with probability at least 1 - n- T all row sums (and thus [formula]) are at most [formula].

With this we prove Lemma [\ref=lem:norms].

First, note that under the conditions of Theorem [\ref=thm:main], n2  ≥  n1 log 4n1, so n1p  <  1 /  log n1. (In fact, cases in which the density is much higher than this can be dealt with by the standard method of bounding [formula]).

(1): We can compute [formula]: this is the expected number of paths of length 2 from i to j in G. Say σ(i)  =  σ(j), i  ≠  j. Then,

[formula]

If σ(i)  ≠  σ(j), then

[formula]

The diagonal entries of [formula] are 0 by construction. So [formula] is a rank 2 matrix, [formula], with λ1  =  n1n2p2 and λ2  =  (δ - 1)2n1n2p2.

(2): The matrix [formula] is symmetric with mean zero entries, but the entries are not quite independent, and so we cannot directly apply a bound like Lemma [\ref=lem:binomialNorm]. Instead, we will first decompose the matrix into the sum of a sequence of adjacency matrices of subgraphs induced by vertices of a given degree in V2. We will couple each matrix in the sum to a matrix with independent entries, apply Lemma [\ref=lem:binomialNorm] to each, then take the sum of these bounds as our upper bound on [formula].

We decompose the graph G by sorting the vertices of V2 by degree. Let V(i)2, [formula] be the set of vertices in V2 of degree i.

Let Mi be the adjacency matrix of G induced by V(i)2. The main idea of the decomposition is that M1 does not contribute to B, as its edges only appear on the diagonal of MMT, and that nearly all of the remaining edges in the graph are in M2. We write

[formula]

and

[formula]

The cross terms disappear since the matrices are supported on disjoint sets of columns.

Recall [formula], and [formula]. We have

[formula]

since M1MT1 is a diagonal matrix.

A vertex v∈V(i)2 has exactly i neighbors, and its contribution to Bi is 1 in each entry (u,w) where u  ≠  w are neighbors of v. Call Mv the adjacency matrix induced by v, and [formula]. We have [formula]. Bv has [formula] 1's above the diagonal. Now for each i  ≥  3, we randomly split each Bv, v∈V(i)2, into [formula] symmetric matrices [formula] by randomly assigning each of the 1's above the diagonal to a unique matrix, along with the symmetric 1 below the diagonal. Then we combine the matrices as

[formula]

Each Bi is the adjacency matrix of a random graph formed by adding a given number of random i-cliques to an empty graph. Clearly there are correlations between edges in such a graph, and so the purpose of this decomposition is to split the graph into [formula] graphs, G(j)i, [formula], with independent edges. This gives a sequence of adjacency matrices [formula] with identical distributions, but not independent across the different matrices.

All together, we write

[formula]

and

[formula]

where

[formula]

What remains is to bound the norms of the mean zero random matrices in the decomposition above: [formula], and the [formula]'s.

Let L(i)+,L(i)- be the number of vertices of V2 with degree i and label +   or -   respectively. As a first step, we calculate the expectations of these random variables:

[formula]

where we use the total variation distance bound on a Poisson approximation of a binomial, along with our assumption n1p = o(1). Let i0 be the smallest i so that [formula]. Considering the sum [formula], we see that the expected row sums of B≥  i0 are bounded by O( log n1), and so from a Chernoff bound, with probability at least 1 - n- 21, all row sums are O( log n1). This gives

[formula]

with probability ≥  1 - n- 21.

Now consider i  <  i0. Let N(j)i( +  + ),N(j)i( -  - ),N(j)i( +  - ) be the number of edges between vertices with the respective labels in the graph G(j)i corresponding to B(j)i. Conditioned on N(j)i( +  + ),N(j)i( -  - ),N(j)i( +  - ) the edges are distributed uniformly (with replacement) in the respective categories. Alternatively, consider the adjacency matrices 2,(j)i, where each edge (u,v) appears with multiplicity according to an independent Poisson random variable of mean [formula] (one of two values depending on whether u and v have the same or opposite label). Again in this setting, conditioned on the number of edges of each type, Ñ(j)i( +  + ),Ñ(j)i( -  - ),Ñ(j)i( +  - ), the edges are distributed uniformly with replacement in the respective categories.

Since [formula] and [formula] by construction, we can apply Lemma [\ref=lem:binomialNorm] and choose C large enough to get

[formula]

with probability at least 1 - n- 81, as each entry has variance bounded by [formula], and similarly

[formula]

with probability at least 1  -  n- 6 - i1. Note that from ([\ref=eq:Lpbound]), the means [formula] decrease with i faster than 1 / i! and so summing () over 3  ≤  i  ≤  i0, and all j, gives a bound of [formula].

To transfer these bounds, we couple the Poisson matrices with B2 and the B(j)i's. If the means of N(j)i( +  + ),N(j)i( -  - ),N(j)i( +  - ) are small enough, we can couple the matrices to be equal whp. If the means are large, we couple so that [formula] is small. Take N(j)i( +  + ). Its distribution is a [formula] for q that depends on n1,p,i, and δ. The corresponding random variable, Ñ(j)i( +  + ) is a [formula]. Say q  =  o(n1 / 22). In this case the total variation distance between the two is O(n2q2) and so we can couple the corresponding matrices to be equal whp. q is decreasing like 1 / i!, and so we can sum the deviation probabilities over all i and j. When q  =  Ω(n- 1 / 22), we write N(j)i( +  + ) as the sum of n2 independent [formula] random variables and Ñ(j)i( +  + ) as the sum of n2 independent [formula] random variables, and term by term in each sum couple by an optimal coupling with respect to total variation distance. Then the difference N(j)i( +  + )  -  Ñ(j)i( +  + ) is the sum of n2 mean 0 random variables of variance O(q2), and so whp the difference is bounded by O(qn2 / 32). We can couple the matrices so that their difference has non-zero entries distributed uniformly in entries corresponding to + ,  +   labels. Then, as above, a Chernoff bound shows that the row sums (and thus the matrix norm) of the difference matrix are all bounded by O(qn- 11n2 / 32 log n1). Since q  <  n21p2, this gives a bound on the norm of O(n1n2 / 32p2)  =  o(n1 / 21n1 / 22p).

All together the bound ([\ref=eq:boundhighi]) and the transferred bounds give

with probability at least 1 - n- 11, which completes the proof of part 2 of Lemma [\ref=lem:norms].

Parts 3 and 4 of Lemma [\ref=lem:norms] follow from the observation that the ith diagonal entries of DV are the degrees of the ith vertex of V1. Since the degrees of vertices of V1 have identical distributions, the expectation matrices are multiples of the identity. For part 4 we use Chernoff bounds. We have [formula] whp.