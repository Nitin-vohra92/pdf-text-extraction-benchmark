Stochastic modeling of regional archeomagnetic series

, N. Gillet1, C. Bouligand1, D. Jault1 1 Univ. Grenoble Alpes, CNRS, ISTerre, F-38 041 Grenoble, France (gabrielle.hellio@ujf-grenoble.fr)

Introduction

From 1840 onward, continuous records from ground-based observatories are available and make it possible to characterize the time derivative of the main field, or secular variation, as a function of length-scale [\citep=holme11]. The spectral properties of magnetic series obtained from these instrumental records can be transposed into prior information on core processes in the framework of stochastic processes [\citep=gillet13].

Before the first direct measurements, direction and intensity of the magnetic field can be inferred from remanent magnetization of sediments, volcanic deposits or archeological artifacts. The sparse repartition of archeomagnetic data in space and time and their associated large measurement and dating uncertainties limit our ability to recover the spatio-temporal variations of the geomagnetic field over the past few millennia. Strong efforts have nevertheless been made to calculate time-dependent global models of the archeomagnetic field from these data [\citep=korte03] [\citep=korte09] [\citep=licht13]. To take advantage of the large amount of data and the relatively dense temporal coverage available in some areas, for instance in Western Europe [\citep=donadini09], archeomagnetic data are also used to construct regional curves (so-called master curves) that describe the temporal behavior of the magnetic field [\citep=legoff02] [\citep=lanos05] [\citep=thebault10]. Beyond information on processes occurring in the core, master curves provide useful tools for archeomagnetic dating.

In this study, we focus on the construction of regional archeomagnetic models describing the time evolution of the declination, inclination and intensity of the Earth's magnetic field over the past 6000 years. To compensate for the uneven repartition of data and their large uncertainties and to reduce the non-uniqueness, the construction of such models usually incorporates a regularization in time that consists in penalizing the second time derivative of the field [\citep=bloxham92]. Such regularizations, however, arbitrarily smooth the reconstructed time fluctuations. Instead, we rely here on a Gaussian process regression method based on prior information extrapolated from the statistical properties of models obtained from satellite and observatory data.

Furthermore, dating uncertainties in archeomagnetic data are an important source of errors in the construction of master curves and most inversion methods do not directly account for them. Indeed, dating errors are often converted into equivalent measurement errors [\citep=korte05]; alternatively, they are estimated using bootstrap or jack-knife methods, which consist in investigating the variability of models obtained from an ensemble of randomly noised and/or sub-sampled datasets [\citep=korte09]. Here, we use Markov Chain Monte Carlo (MCMC) methods for the dates at which observations have been obtained, based on the probability inherent to the Gaussian process method.

This paper is divided into 5 sections. We present in the next section the Gaussian process regression framework, our choice of prior information for the model parameters, the use of Markov Chain Monte Carlo on observation dates and a robust measure of data errors in order to decrease the effect of outliers. The method is tested using synthetic observations (section 3), before being applied to datasets from France and the Middle East (section 4). A discussion of our results and conclusions are presented in section 5.

Method

We consider geomagnetic series as the realization of a stochastic process sampled through observations. We use the Gaussian Process Regression (section 2.1) to couple the information contained in measurements with that from the a priori time covariance function of the process. To account for dating errors, we integrate the regression method into a Markov Chain Monte Carlo algorithm, as described in section 2.2. We define, in section 2.3, the a priori information on which relies the Gaussian process framework. Finally, in section 2.4, we show how to incorporate a robust measure of data errors in order to decrease the effect of outliers that appears when using a standard L2-measure with geophysical series.

Gaussian process regression

Let us consider a Gaussian, stationary stochastic process [formula], defined by its average value [formula], the perturbation φ'(t) from this mean value and its covariance function:

[formula]

with σ2 the variance and ρ the autocorrelation function of the process that will contain the a priori information on the model parameters ; the notation [formula] stands for the statistical expectation. The continuous process φ is sampled with data stored at discrete times into a vector [formula], and estimated as a sequence of parameters stored into a vector [formula], with [formula] the background model and [formula] the model perturbation. In our context we consider that the parameters in [formula] are homogeneous to the observations in [formula] (they are images of the same quantity). Vectors [formula] and [formula] contain respectively the epochs at which the data and the model are sampled.

The estimate of the model [formula], given the data [formula] and the measurement errors [formula], is characterized by the a posteriori expectation model

[formula]

and the a posteriori covariance matrix [formula]:

[formula]

[\citep=rasmussen06]. Here [formula] is the prediction from the background model [formula] at times [formula], [formula] is the data error covariance matrix. Matrices [formula], [formula] and [formula] are derived from the autocorrelation function using expression ([\ref=covar_GP]):

[formula]

Note that the above estimate ([\ref=expect]) in term of Gaussian process comes down to calculating the BLUE (Best Linear Unbiased Estimator).

Accounting for dating uncertainties with Markov Chain Monte Carlo

Expression ([\ref=Cmd]) assumes that each datum yi is representative of an epoch tyi. Because dating uncertainties are prominent in archeomagnetic databases, we should consider the probability density function (pdf) for the date of the datum yi. This distribution depends on the dating method: it is generally considered Gaussian for 14C dating [\citep=aguilar2013], uniform when the date is estimated from historical or archeological constraints [\citep=genevey03], or more complex in the case of calibrated 14C dates [\citep=reimer09].

To consider these dating uncertainties, we build several sets of dates [formula], illustrated in Figure [\ref=pdf]a. We associate at each record a date drawn inside its dating error bar. We estimate for each draw a model [formula] defined by a mean model [formula] and its covariances [formula] (equations ([\ref=expect]) and ([\ref=cpost])) at times [formula]. We then evaluate the joint probability of the draw after , see also [\citep=pavon11]:

[formula]

The integration of the probability density function over all possible values of [formula] gives the posterior probabilities of the dates [formula].

[formula]

In practice, we first evaluate these probabilities for each record at time tyi. To this end we multiply the Gaussian posterior probability density function N(yi,σyi) of the model at time tyi (red curves in Figure [\ref=pdf]b and c), by the Gaussian prior probability density function N(yi,ei) of measurement yi (blue curves in Figure [\ref=pdf]b and c). The notation N(μ,σ) stands for Gaussian distribution with mean μ and standard deviation σ. The standard deviation σyi of the model at time tyi is obtained from the posterior covariance matrix [formula]. By this multiplication, we obtain the joint probability density function (green curve in Figure [\ref=pdf]c). We then integrate the obtained probability density function over all possible values of yi to get the probability of the date tyi. We finally multiply the posterior probabilities of all dates to obtain the probability of draw k, noted [formula].

A natural way to proceed following is to weigh each mean model given the probability of the corresponding draw. However, few draws have very high probabilities compared to all others and numerous iterations provide very few representative mean models. To overcome this problem, we use Markov Chain Monte Carlo to explore the possible dates of observations and to select draws with the highest probabilities. We remind here the main steps, (see for more details):

1- To explore the possible dates, we generate the [formula] draw from the previous one as a random draw inside the proposal distribution [formula]. For uniform probability distribution, it comes down to a random walk restricted to the a priori time interval.

2- We define an acceptance ratio [formula], with [formula].

3- We keep the [formula] draw if α > u, u being a random value obtained from a uniform distribution between 0 and 1, and reject it if not.

We stop the chain after N iterations depending on the dataset studied and perform several chains to better explore the space of possible dates [\citep=gilks1996introducing]. The number of chains is determined by the evolution of the posterior distribution of the dates. The number of accepted draws in each chain depends on [formula]. We adjust the latter parameter so the number of kept draws is between 20 and 60% of all draws. All these informations are summarized in Appendix B. Each draw k selected by the above Markov rules consists in a set of dates [formula], associated to records [formula] and measurement errors [formula]. From equations ([\ref=expect]) and ([\ref=cpost]), we obtain for each dataset a mean model [formula] and its associated covariances [formula] at the times [formula].

Expectations and a posteriori covariance matrices are used to build ensembles of models consistent with both the observations and the a priori information assumed for model parameters. To this end, we use the Choleski decomposition [formula] of the a posteriori covariance matrix, [formula], from which we compute an ensemble of model realizations [formula], with [formula] a random Gaussian vector with zero mean and unit variance. Each draw selected by the Markov chain is used to build an ensemble of realizations. We put together all these ensembles to build our final estimate of the probability density function. Note that this distribution is not necessarily Gaussian.

A priori covariances on geomagnetic series

We detail below how we derive our covariances on geomagnetic series (intensity F, inclination I, declination D) from a priori covariances on the Gauss spherical harmonic coefficients. These are chosen to be compatible with the temporal power spectral densities recorded in ground-based observatories [\citep=gillet13].

We assume that all Gauss coefficients (gmn,hmn), with n and m the spherical harmonic degrees and orders, result from an auto-regressive (AR) process of order 2, with correlation function

[formula]

Covariances for Gauss coefficients are then:

[formula]

with a similar notation for hmn coefficients. The time τc and the variance σ2g are functions of the degree n only. We assume that there is no cross-correlations between Gauss coefficients of different degrees and orders, and between g and h as well. Note that this correlation function is solution of the stochastic differential equation [\citep=yaglom04]:

[formula]

where ζ(t) is the Brownian motion (or Wiener process).

Variances σ2g(n) for the non-dipole Gauss coefficients are obtained from the variance of the Gauss coefficients estimated in satellite field models, as in the models COV-OBS [\citep=gillet13]:

[formula]

Using a similar definition for [formula], equation ([\ref=stoch]) imposes the value of the correlation time:

[formula]

The background model is composed of the axial dipole value [formula]T, and the variance for the dipole coefficients is chosen as σ2g(1) = 5μT2, the value typically found for the past 4000 years [\citep=korte11]. Since [formula] is not affected by the presence of a stationary background, we find a correlation time of about 200 years for all coefficients of degree one.

We have propagated this a priori information on Gauss coefficients to geomagnetic series of declination D, inclination I and intensity F recorded at the Earth's surface. Our approach requires that these quantities have a Gaussian distribution. It has been shown that the intensity distribution was close to a Gaussian distribution in the limit of small relative dispersion [\citep=love03]. This is indeed the case for archeomagnetic data, since on centennial to millennial time-scales the standard deviation in the axial dipole is small compared to the average value. Assuming that Gauss coefficients are the result of a random stationary process and that they have a zero mean except for the axial dipole g01, we show in Appendix A how to obtain the mean, covariance and cross-covariance of geomagnetic series of D, I and F (equations ([\ref=covar_DIF_app]) and ([\ref=cross-covar_DIF_app])). Covariances depend on the colatitude θ of the sampled site, on [formula] and on sums over degree n of the correlation function defined in equation ([\ref=corr_func]). Note in particular that we find non-zero covariances between F and I.

Studies carried out on magnetic series from paleomagnetic to archeomagnetic records suggest a continuous spectrum of the Virtual Axial Dipole Moment [\citep=constable05] [\citep=ziegler11], with slope decreasing from about zero on the longest periods towards about -2 at millennial periods. The analysis of models of Holocene lake sediment magnetic records [\citep=panovska2013observed] has shown that temporal power spectra for declination, inclination and relative paleointensity from lake sediments data follow a power law with a slope [formula] for periods between 300 and 4000 years. These findings are in good agreement with recent results obtained for the dipole moment from geodynamo numerical simulations [\citep=olson12], which also display steeper slopes at higher periods.

The a priori information discussed above presents the advantage to require only a single parameter per degree (τc). The slope of the temporal power spectrum for a process defined by equation ([\ref=stoch]) is by construction -4 at periods τ  ≪  τc [\citep=gillet13], which agrees with that obtained for observatory series [\citep=desantis03]. We illustrate in Figure [\ref=norm_spec] that we retrieve the -4 slope for spectra of the auto-correlation functions for F, D and I, obtained with equation (A.15) - the square of the power spectrum for a series φ(t) is the power spectrum of its covariance function [formula]. The choice of a priori information in the present study is particularly important for periods shorter than a few hundred of years. Indeed, archeomagnetic data being sparse in time, it is towards high frequencies that we need to buttress the evidence from observations with prior information.

Dealing with outliers using robust measures of the data errors

The methodology developed hitherto relies on a L2-norm to account for measurement errors, which makes the approach vulnerable to large errors. Outliers to the L2-norm are unfortunately a common feature of archeomagnetic data analyses [\citep=suttie11]. To decrease the effect of these outliers when using L2-norms, assigned to all data a minimum value for the measurement errors (5μT for intensity data and 4.3� for directional data). We can instead modify the measure of the misfit to observations and replace the L2-norm with the Huber norm, which distribution is defined as: (see ):

[formula]

with N = 2.6046 for c = 1.5 in this study and r, the normalized data misfit residuals. To implement the Huber norm with the previous method, we use the iteratively re-weighting least-squares algorithm where the matrix [formula] is constructed from the residual of the data i, [formula], as

[formula]

The Huber norm impacts also the joint probability (equation ([\ref=proba])). Instead of multiplying the Gaussian posterior probability density function of the model by the Gaussian prior probability density function of the measurements, we multiply it by the Huber probability density function defined in equation ([\ref=huber]). Few iterations are needed to obtain convergency. The use of the Huber norm rescales the weight in [formula] associated with outliers. We present in the following section synthetic tests for which there is no need to use this norm since there are no outliers. In section 4 however, we apply the Huber norm to all geophysical datasets. We compare it with the L2-norm for the Syrian series to show how it reduces the effect of outliers.

Synthetic tests

In order to test the Gaussian process regression on observations presenting dating errors (accounted for with the MCMC method), we build synthetic datasets of D, I and F that are consistent with an AR process of order 2 as defined in section [\ref=sec:_prior], and that display similar characteristics to real archeomagnetic datasets in terms of temporal distribution and errors. To this end, we first construct series for the period 3000 BC to 2000 AD, sampled every 10 years, using the covariance functions defined in equations ([\ref=covar_DIF_app]) and ([\ref=cross-covar_DIF_app]). In these covariance functions, the functions Kn(τ) are defined using the variances and correlation times defined in equations ([\ref=gnm_var]) and ([\ref=tau_n]), and the sums are performed with a spherical harmonic truncation degree N = 14. We observe that the model is not modified when increasing further this truncation degree, and that it is already converged with N = 4. We then randomly sub-sample the series and add random measurement and dating errors to each data. These errors are built using a Gaussian law for measurement errors and a uniform law for dating errors, to mimic the dating uncertainties from historical constraints. We present for comparison the results considering Gaussian dating errors. Finally, the measurement and dating errors used in the modeling phase correspond to the standard deviation and the half-width of the law used to build them. We report in Appendix B, the parameters used for MCMC method for all studied series. We use two different datasets consisting of 20 and 50 records respectively with randomly assigned dating and measurement errors. Dating errors are generated from a uniform distribution with a half-width of 25 years, and measurement errors from a Gaussian distribution with a standard deviation of 1μT.

We report in figures [\ref=F1050](a) and [\ref=F1050](b) the obtained pdf of the intensity. We first notice that the distribution always encompasses the true series (black curve). In the case where 20 data only are available, the sharp changes present in the true series are not closely recovered by the pdf due to the lack of data, and the range of estimates is wide except during the few time intervals that are well sampled. Increasing the quantity of synthetic observations dramatically improves the fit of the pdf to the true series and narrows the distribution (see figure [\ref=F_50]).

In figure [\ref=F_more], we invert the same dataset as in figure [\ref=F_10], here again noised following uniform and Gaussian laws with respectively 25 years half-width and 1μT standard deviation. However, following the strategy used by , we assign in the inversion a minimum threshold value for measurement errors, that replaces error estimates lower than this minimal value, chosen to be 5 μT for intensities. The distribution is significantly affected by this process, the dispersion happens to be strongly increased particularly when data are available. We conclude here that this way of handling small measurement errors penalizes accurate data and leads to lose information. In figure [\ref=F_10times], we invert the same dataset as in figure [\ref=F_10] but after multiplying dating errors by a factor of ten. The dispersion is then a lot wider for the whole studied period.

For all the precedent cases, the dating errors are supposed uniform what is mostly the case for archeomagnetic objects. However, some of them are dated by radiocarbon methods, which can lead to Gaussian or more complicated error distributions. Figure [\ref=F_gauss] presents the obtained pdf when dating errors are assumed Gaussian for the inversion of the same dataset as in figure [\ref=F_10times]. We see that the resulting pdf are rather similar, although Gaussian dating errors slightly increase the pdf when observations are available. These tests illustrate the importance of assigning realistic error bars for both dating and measurement errors. Furthermore, it shows that our method, where the posterior covariance matrix is used to estimate the model error, is capable of accounting for a realistic measure of the information contained into geomagnetic observations, and thus avoids reducing the importance of relatively more accurate records.

We have evaluated the importance of considering covariances between intensity and inclination within synthetic tests but have not seen significant differences while inverting jointly or separately these observations. Further on, covariances between F and I are considered.

Application to data sets from Syria and France

In this section, we present results for Mari (Syria) and Paris (France), obtained from intensity data in Syria for epochs between 4000 BC and 0 and from directional and intensity data in France for epochs between 0 and 1900 AD. For directions, we have converted the 95% cone of confidence (α95) onto declination σD and inclination σI errors [\citep=piper89]:

[formula]

Archeointensity data from the Middle-East

The dataset used here comprises 39 intensity values for Syria [\citep=genevey03] [\citep=gallet06] [\citep=gallet08] [\citep=gallet10]. All data are reduced to Mari in Syria using the geomagnetic axial dipole (GAD) hypothesis. The error caused by the reduction is small compared to measurement errors. A priori information is built from a magnetic field model truncated at spherical harmonic degree N = 4.

Results are displayed in figure [\ref=syr-ME](a) for the Syrian dataset. The distribution is narrow between 2700 and 1600 BC due to the numerous data present during this period. Local maxima appear resolved in 2500, 2250, 1450 and 650 BC. The distribution prevents us from concluding about extrema value around 3200 BC. Next, we have augmented the dataset with new archeointensity data from Syria [\citep=gallet2006high] [\citep=gallet2014archaeological] [\citep=gallet2014archaeomagnetism], and data from the southern Levantine region and Iran [\citep=benyosef08] [\citep=benyosef09] [\citep=ertepinar12] [\citep=shaar11]. The new data are plotted in blue. Note that the dataset used here comprises more data than the expanded dataset used by . Study of the distribution obtained from this expanded dataset confirms the maxima inferred in 2500 and 2250 BC, figure [\ref=syr-ME](b). Two sharp maxima appear in 1000 and 650 BC. On figure [\ref=syr-ME]a), we remark a wide dispersion around 3200 BC. The few data added between 3500 and 3000 BC, despite very large uncertainties, point to a maximum in 3400 BC followed by a local minimum in 3200 BC although the distribution is still wide. Increasing the number of observations refines the distribution. We see that a mean model in Figure [\ref=syr-ME](a), would not predict the behavior observed with the expanded dataset, the reason why we use probability density functions to represent the results.

Our modeling strategy differs from the iterative inverse method previously developed by . The latter consists in a projection onto cubic B-splines, penalizing the second time derivative, together with a bootstrap strategy to handle dating and measurement errors. Our results for the restricted dataset (figure [\ref=syr-ME](a)) present more rapid variations. Particularly for the two maxima of 2250 and 2500 BC which are well defined in our study and confirmed by the recent observations, whereas the master curve in is flat for this period. In comparison, our distribution presents also a wider dispersion, particularly when data are sparse.

We show in Figure [\ref=syr-ME](c) the results for the expanded dataset when using the L2-norm. Sharp local maxima appear now around 800, 1700, 2800 BC which are not apparent in figure [\ref=syr-ME](b). This behavior illustrates a common issue in archeomagnetic modeling. Even when the method accounts for all uncertainties present in the dataset, some incompatibilities within the dataset cannot be handled. One record appearing in 1785 BC has small dating and measurement errors so it forces alone a sharp variation of the model. A rejection criterium has been used in to tackle this issue. We see that the Huber norm alleviates also this difficulty still allowing these data to possibly keep some influence through the MCMC sampling. Here, we show the importance of assigning realistic measurement and dating errors to all data.

Note also that the posterior distribution is not necessarily Gaussian. Figure [\ref=cross] shows three pdf of the intensity estimated in 3700, 1500 and 50 BC. We see that the distributions can be similar to Laplacian distribution ([\ref=cross]a), Gaussian distribution ([\ref=cross]c) or multi-modal distributions ([\ref=cross]b). This finding makes awkward the definition of a mean model, the reason why we only consider pdf and not master curves.

Finally, an important result of our method is the posterior probability on dates. These distributions are very different from their a priori uniform distribution. We focus on five data of the extended dataset (see colored error bars in Figure [\ref=data](a)) and show histograms of the dates preferentially selected in the Markov chains (Figure [\ref=data](b-f)). The distribution of dates in figure [\ref=data](d) is very different from a uniform distribution: very few dates appear before 1680 BC and the highest probability for this date is for epochs younger than 1650 BC. Figure [\ref=data](f) displays a multi-modal distribution that makes unlikely epochs around 1450 BC. This methodology can be used to refine the pdf of record dates.

Direction and intensity of the magnetic field in Paris

We use in this section directional data collated by , and some of the intensity data presented in for France. We adopt the quality criteria of and keep only data with age uncertainties lower than 100 years, acquired using the Thellier and Thellier method with pTRM-check and with a minimum of three results per site. The dataset finally contains 119 directional values and 104 intensity measurements. All of them have been reduced to Paris using virtual geomagnetic poles derived from the GAD hypothesis. Again, the error caused by the reduction is small compared to measurement errors. MCMC parameters are summarized in Appendix B. We need less chains than for the Syrian study due to the smaller dating errors. We display in figure [\ref=paris] the pdf for D, I and F. The intensity series present a general decrease from 850 to 1800 AD, with a local maximum in 1350 AD. Data coverage is particularly sparse between 500 and 700 AD, which implies a wide dispersion during this period. A maximum close to 80[formula] appears clearly defined in 850 AD. Our results present similar features in comparison with those of , except for the local maximum around 1600 AD that does not exist in our study.

Predictions from the ARCH3k global model [\citep=korte11] and from the A-FM global model [\citep=licht13] are superimposed in figure [\ref=paris] for comparison, in blue and green respectively. The models are in good agreement for declination series except for periods between 600 and 850 AD. For inclination however, the high values found at the end of the IX[formula] century are not accounted for by the ARCH3k and the A-FM models. The intensity minimum found in our study around 1700 AD is not accounted for by the global models. The intensity maximum appears in both models but is slightly sharper in our model and delayed towards recent epochs. This can be due to the penalization of second-time derivatives in the ARCH3k and A-FM models, which may filter out locally well documented rapid variations in order to avoid spurious oscillations elsewhere or to the fact that this model incorporates globally distributed data.

The a priori information on the model clearly emerges at epoch for which no data are available. In this study, it particularly appears at the end of the studied time interval for declination and inclination. There, the model pdf is controlled by the a priori correlation function which ensures the continuity of the first time derivative through the AR-2 process.

Conclusion

In this study, we have developed a new method for the construction of archeomagnetic pdf from inclination, declination, and intensity data. Our method is based on Gaussian process regression and it incorporates a priori information consistent with the statistics obtained from historical geomagnetic data. Markov Chain Monte Carlo applied on the dates of observations selects random distribution of dates with the highest probabilities. The Huber norm is applied to deal efficiently with outliers. This new method has several advantages: first it avoids the use of arbitrary regularization, and any unspecified filtering introduced by the projection onto support functions such as cubic B-splines; it furthermore allows to account for dating errors in a probabilistic framework.

We first try our method on synthetic datasets constructed from AR-2 process series. Our tests illustrate the importance of using correct estimates of the dating and measurement errors in the inversion in order to optimally recover the a posteriori errors on model parameters. They also show that our method is capable of accounting for data displaying disparate accuracies, without losing information contained into the highest quality records. The application of this newly developed method to European datasets provides pdf that display rapid fluctuations. These are less smooth than changes obtained from regularized global [\citep=korte09] or regional [\citep=thebault10] models. The pdf together with the posterior probability of the record dates may be useful for a purpose of archeomagnetic dating.

We find particularly interesting the use of the MCMC method in order to efficiently explore the space of possible record dates, as we observe that naive random sampling yields largely disparate probabilities for the different sets of dates. We now plan to extend our method to global models. In this context, efficient sampling is crucial.

In the present study we employ the simplest AR-2 stochastic process that mimics well high frequency variations of the field. Over longer periods, a -2 slope temporal power spectral density has been put forward [\citep=panovska2013observed]. Such a slope is consistent with the identification of archeomagnetic jerks [\citep=gallet03]. It has motivated the introduction of AR-1 stochastic process in the modeling of long period changes of the magnetic field [\citep=brendel07] [\citep=buffett13]. Alternative AR-2 processes may be employed to represent the two behaviors on short (5-100 years) and long (300-10,000 years) periods. Consider for instance the damped oscillator process [\citep=yaglom04], governed by stochastic equations depending on two parameters and of the general form:

[formula]

The Mat�rn AR-2 process used in this study corresponds to the case α  =  ω. Using instead 2α  >  ω2 one can mimic both the -2 slope temporal power spectral density found for the dipole moment at periods up to approximately 105 yrs from the analysis of geomagnetic records [\citep=constable05], and retrieved in geodynamo simulations [\citep=olson12], and the -4 slope observed at shorter periods. This could be an interesting alternative given the cyclic behavior found for the dipole tilt at millennial periods [\citep=nilsson11].

D, I and F covariances

In this Appendix, we derive the statistical properties (i.e. mean values and covariances) of the inclination I, declination D, and intensity F of the magnetic field at a location of longitude φ and colatitude θ at the surface of the Earth. We assume that the Gauss coefficients describing the magnetic field are the result of a random stationary process, are characterized by a null mean value (except for the axial dipole, whose mean value is noted 01), are independent from each other, and have a covariance function that depends only on degree n:

[formula]

Such assumptions amount to impose that the statistical properties of the deviation of the magnetic field from an axial dipole are invariant over the surface of the Earth (as demonstrated in [\citep=Hulot:2005xy]).

We first derive the statistical properties of the north X, east Y, and downward Z components of the magnetic field. Their expressions (for a truncation degree N) at the surface of the Earth are [\citep=Langel:1987ys]:

[formula]

Because only g01 has a non-zero mean value, the mean values of X, Y, and Z are simply

[formula]

Because of the independence of Gauss coefficients, and because their covariance function depends only on the spherical harmonic degree n, covariances on X, Y and Z simplify into :

[formula]

Such expressions can be further simplified using the following relations for Schmidt normalized associated Legendre functions [\citep=winch1995derivatives]:

[formula]

We therefore deduce that :

[formula]

and that the series of X, Y, and Z recorded at a same location are independent from each other :

[formula]

The declination D, inclination I and intensity F of the magnetic field are not linearly related to the components X, Y, and Z:

[formula]

Let us denote [formula] and [formula]. If the vector [formula] does not depart much from its mean value [formula] (corresponding to the mean axial dipole), the above non-linear relations, noted [formula], can be approximated using a first-order Taylor expansion :

[formula]

The mean value of [formula] is therefore approximated by:

[formula]

Combining equations ([\ref=meanb]), ([\ref=meancomponent]), and ([\ref=dif]), we obtain the expression for the mean value of D, I, and F:

[formula]

and the covariance matrix for [formula] is approximated by:

[formula]

Because the series of X, Y, and Z are independent of each other, this expression can be simplified into:

[formula]

This expression involves the partial derivative of D, I, and F with respect to X, Y, and Z evaluated at [formula] :

[formula]

Finally, combining equations ([\ref=covb]), ([\ref=cov_XYZ]), and ([\ref=partial_DIF]), we obtain the following approximated expressions for the covariances of D, I, and F :

[formula]

and the cross-covariances within the different quantities:

[formula]

Parameters used in the MCMC method. Number N of iterations per chain, [formula] and number [formula] of draws selected by the Markov rules. The number of lines corresponds to the number of chains used for each figure.