|barta,| |nagyg,| |kazi,| |henk

GEFCOM 2014 - Probabilistic Electricity Price Forecasting

Introduction

Forecasting electricity prices is a difficult task as they reflect the actions of various participants both inside and outside the market. Both producers and consumers use day-ahead price forecasts to derive their unique strategies and make informed decisions in their respective businesses and on the electricity market. High precision short-term price forecasting models are beneficial in maximizing their profits and conducting cost-efficient business. Day-ahead market forecasts also help system operators to match the bids of both generating companies and consumers and to allocate significant energy amounts ahead of time.

The methodology of the current research paper originates from the GEFCOM 2014 forecasting contest. In last year's contest our team achieved a high ranking position by ensembling multiple regressors using the Gradient Boosted Regression Trees paradigm. Promising results encouraged us to further explore potential of the initial approach and establish a framework to compare results with one of the most popular forecasting methods; ARMAX.

Global Energy Forecasting Competition is a well-established competition first announced in 2012 [\cite=hong2012] with worldwide success. The 2014 edition [\cite=hong2014] put focus on renewal energy sources and probabilistic forecasting. The GEFCOM 2014 Probabilistic Electricity Price Forecasting Track offered a unique approach to forecasting energy price outputs, since competition participants needed to forecast not a single value but a probability distribution of the forecasted variables. This methodological difference offers more information to stakeholders in the industry to incorporate into their daily work. As a side effect new methods had to be used to produce probabilistic forecasts.

The report contains five sections:

Methods show the underlying models in detail with references.

Data description provides some statistics and description about the target variables and the features used in research.

Experiment Methodology summarizes the training and testing environment and evaluation scheme the research was conducted on.

Results are presented in a the corresponding section.

Conclusions are drawn at the end.

Methods

Previous experience showed us that oftentimes multiple regressors are better than one[\cite=rokach]. Therefore we used an ensemble method that was successful in various other competitions: Gradient Boosted Regression Trees[\cite=aggarwal] [\cite=mcmahan] [\cite=graepel]. Experimental results were benchmarked using ARMAX; a model widely used for time series regression. GBR implementation was provided by Python's Scikit-learn[\cite=skl] library and ARMAX by Statsmodels[\cite=stm].

ARMAX

We used ARMAX to benchmark our methods because it is a widely applied methodology for time series regression [\cite=tan2010] [\cite=feng2010] [\cite=hou2010] [\cite=yang2009] [\cite=hong1996]. This method expands the ARMA model with (a linear combination of) exogenic inputs (X). ARMA is an abbreviation of auto-regression (AR) and moving-average (MA). ARMA models were originally designed to describe stationary stochastic processes in terms of AR and MA to support hypothesis testing in time series analysis [\cite=whittle51]. As the forecasting task in question has exogenic inputs by specification therefore ARMAX is a reasonable candidate to be used as a modeler.

Using the ARMAX model (considering a linear model wrt. the exogenous input) the following relation is assumed and modeled in terms of Xt which is the variable in question at the time denoted by t. According to this the value of Xt is a combination of (p) (auto-regression of order p), (q) (moving average of order q) and a linear combination of the exogenic input.

[formula]

The symbol εt in the formula above represents an error term (generally regarded as Gaussian noise around zero). [formula] represents the autoregression submodel with the order of p: φi is the i-th parameter to weight a previous value. The elements of the sum [formula] are the weighted error terms of the moving average submodel with the order of q. The last part of the formula is the linear combination of exogenic input dt.

Usually p and q are chosen to be as small as they can with an acceptable error. After choosing the values of p and q the ARMAX model can be trained using least squares regression to find a suitable parameter setting which minimizes the error.

Gradient Boosting Decision Trees

Gradient boosting is another ensemble method responsible for combining weak learners for higher model accuracy, as suggested by Friedman in 2000 [\cite=friedman2001]. The predictor generated in gradient boosting is a linear combination of weak learners, again we use tree models for this purpose. We iteratively build a sequence of models, and our final predictor will be the weighted average of these predictors. Boosting generally results in an additive prediction function:

[formula]

In each turn of the iteration the ensemble calculates two set of weights:

one for the current tree in the ensemble

one for each observation in the training dataset

The rows in the training set are iteratively reweighted by upweighting previously misclassified observations.

The general idea is to compute a sequence of simple trees, where each successive tree is built for the prediction residuals of the preceding tree. Each new base-learner is chosen to be maximally correlated with the negative gradient of the loss function, associated with the whole ensemble. This way the subsequent stages will work harder on fitting these examples and the resulting predictor is a linear combination of weak learners.

Utilizing boosting has many beneficial properties; various risk functions are applicable, intrinsic variable selection is carried out, also resolves multicollinearity issues, and works well with large number of features without overfitting.

Data description

The original competition goal was to predict hourly electricity prices for every hour on a given day. The provided dataset contained information about the prices on hourly resolution for a roughly 3 year long period between 2011 and 2013 for an unknown zone. Beside the prices two additional variables were in the dataset. One was the Forecasted Zonal Load ('z') and the other was the Forecasted Total Load ('t'). The first attribute is a forecasted electricity load value for the same zone where the price data came from. The second attribute contains the forecasted total electricity load in the provider network. The unit of measurement for these variables remain unknown, as is the precision of the forecasted values. Also, no additional data sources were allowed to be used for this competition.

In Table [\ref=tab:inpstat] we can see the descriptive statistic values for the original variables and the target. The histogram of the target variable (Figure [\ref=fig:p_hist]) is a bit skewed to the left with a long tail on the right and some unusual high values. Due to this characteristic we decided to take the natural log value of the target and build models on that value. The model performance was better indeed when they were trained on this transformed target.

The distribution of the other two descriptive variables are far from normal as we can see on Figure [\ref=fig:tz_hist]. As we can see the shapes are very similar for these variables with the peak, the left plateau and the tail on the right. They are also highly correlated with a correlation value of 0.97, but not so much with the target itself (0.5-0.58).

Beside the variables of Table [\ref=tab:inpstat] we also calculated additional attributes based on them: several variables derived from the two exogenous variable 'z' and 't', also date and time related attributes were extracted from the timestamps (see Table [\ref=tab:attrs] for details).

During the analysis we observed from the autocorrelation plots that some variables value have stronger correlation with its +/- 1 hour value, so we also calculated these values for every row. Figure [\ref=fig:autocorr] shows 3 selected variables to be shifted as the autocorrelation values are extremely high when a lagging window of less than 2 hours is used.

In Figure [\ref=fig:autocorr2] figure we can see an autocorrelation plot of price values in specific hours and they are shifted in days (24 hours). It is clearly seen that the autocorrelation values for the early and late hours are much higher than for the afternoon hours. That means it is worth to include shifted variables in the models as we did. Not surprisingly the errors at the early and late hours were much lower than midday and afternoon.

Gradient Boosting Regression Trees also provided intrinsic variable importance measures. Table [\ref=tab:attrimportance] shows that (apart from the original input variables) the calculated differences were found to be important. The relatively high importance of the hour of day suggests strong within-day periodicity.

Experiment Methodology

In our research framework we abandoned the idea of probabilistic forecasting as this is a fairly new approach and our goal was to gain comparable results with well-established conventional forecasting methods; ARMAX in this case.

We used all data from 2013 as a validation set in our research methodology (unlike in the competition where specific dates were marked for evaluation in each task). To be on a par with ARMAX we decided to use a rolling window of 30 days to train GBR. This means much less training data (a substantial drawback for the GBR model), but yields comparable results between the two methods.

The target variable is known until 2013-12-17, leaving us with 350 days for testing. For each day the training set consisted of the previous 1 month period, and the subsequent day was used for testing the 24 hourly forecasts. On some days the ARMAX model did not converge leaving us with 347 days in total to be used to assess model performance. The forecasts are compared to the known target variable, we provide 2 metrics to compare the two methods: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Gradient Boosting and ARMAX optimizes Mean Squared Error directly meaning that one should focus more on RMSE than MAE.

Results

Figure [\ref=fig:rmse_plot] compares the model outputs with actual prices for a single day. While Table [\ref=tab:errors] shows the descriptive statistics of the error metrics: mae_p_armax, rmse_p_armax, mae_p_gbr and rmse_p_gbr are the Mean Absolute Errors and Root Mean Squared Errors of ARMAX and GBR models respectively. The average of the 24 forecasted observations are used for each day, and the average of daily means are depicted for all the 347 days. In terms of both RMSE and MAE the average and median error is significantly lower for the GBR model; surpassing ARMAX by approx. 20% on average.

During the evaluation we came across several days that had very big error measures, filtering out these outliers represented by the top and bottom 5% of the observed errors we have taken a t-test to confirm that the difference between the two models is indeed significant (t = 2.3187, p = 0.0208 for RMSE).

Conclusions and future work

The GEFCOM competition offered a novel way of forecasting; probabilistic forecasts offer more information to stakeholders and is an approach worth investigating in energy price forecasting. Our efforts in the contest were focused on developing accurate forecasts with the help of well-established estimators in the literature used in a fairly different context. This approach was capable of achieving roughly 10 place in the GEFCOM 2014 competition Price Track and performs surprisingly well when compared to the conventional and widespread benchmarking method ARMAX overperforming it by roughly 20%.

The methodology used in this paper can be easily applied in other domains of forecasting as well. Applying the framework and observing model performance on a wider range of datasets yields more robust results and shall be covered in future work.

During the competition we filtered the GBR training set to better represent the characteristics of the day to be forecasted, which greatly improved model performance. Automating this process is also a promising and chief goal of ongoing research.