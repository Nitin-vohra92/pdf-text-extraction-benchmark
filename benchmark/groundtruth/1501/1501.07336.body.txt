=2500

Generalized Simplified Variable-Scaled Min Sum LDPC decoder for irregular LDPC Codes

Introduction

Low-density parity check codes (LDPC) were introduced by Gallager [\cite=gallager62low] in the early 1960s. Decoding of LDPC codes, by log-likelihood ratio sum-product algorithms (LLR-SPA), are proven to achieve excellent capacity performance, by approaching the Shannon bound [\cite=mackay1999good]. However, the drawbacks of LLR-SPA, namely, the high complexity and sensitivity to linear scaling, are solved by the Min-Sum algorithm [\cite=fossorier1999reduced]. Scaled Min-Sum [\cite=chen2002density] is a modification of Min-Sum algorithm, where a scaling factor is used to decrease the error introduced by using the approximate minimum operation. Scaled Min-Sum (with constant scaling factor) is suitable for regular LDPC codes. However, irregular LDPC codes require different scaling technique [\cite=zhang2005improved] [\cite=lechner2006improved] [\cite=ahmed14CCNC]. In [\cite=zhang2005improved], a two-dimensional (2D) correction of the min-sum was proposed. In this algorithm, different scaling factors are required for different check node degrees and variable node degrees. Consequently, the algorithm requires the calculation of two scaling factor vectors α and β with length equal maximum check node's degree and variable node's degree respectively. These vectors are optimized by parallel differential optimization of Density Evolution (DE). In [\cite=lechner2006improved], different scaling factor per iteration is proposed for irregular LDPC codes. Different scaling factor per iteration technique has good performance for irregular LDPC codes. However, adding these scaling factors requires complex calculation steps in designing stage, and requires extra storage to store the scaling factor value of each iteration. In [\cite=ahmed14CCNC], we proposed simplified variable-scaled min-sum (SVS-min-sum) decoding technique. This algorithm uses simply implemented heuristic technique to update the scaling factor with iterations. It is simpler than both variable scaling factor [\cite=lechner2006improved] and 2D correction Min-Sum [\cite=zhang2005improved] in implementing and designing. Simulation results show that SVS-min-sum has lower Bit Error Rate (BER) than constant scaling factor for many LDPC codes [\cite=ahmed14CCNC]. SVS-min-sum algorithm starts the scaling factor sequence with a constant value equals 0.5. This restriction decreases its performance and makes it unsuitable for some codes. In this paper, we introduce a generalization of the SVS-min-sum algorithm by removing the restriction of starting the scaling sequence from 0.5. This generalization leads to better performance than constant scaling for all codes. In fact, constant scaling can be seen as a special sub-optimized version of the proposed algorithm as shown in section IV. We apply Nelder-Mead optimization [\cite=nelder1965simplex] on DE to jointly optimize the initial scaling factor and updating step of the scaling sequence. Simulation results illustrate the improvement of the proposed algorithm in both BER performance and decoding latency over other scaling strategies. The rest of the paper is organized as follows: Section II presents the necessary background on the SPA, Min-Sum, Scaled Min-Sum, Variable Scaled Min-Sum and SVS-min-sum algorithms. Section III presents the generalized SVS-min-sum algorithm. The simulation results are displayed and discussed in Section IV. Finally, the paper is concluded in section V.

REVIEW OF THE SPA AND MIN-SUM based ALGORITHMS

An (n,k) LDPC code is a binary code characterized by a sparse parity check matrix [formula] where m = n - k. It can be represented by a Tanner graph which contains variable nodes j∈{1..n} and check nodes i∈{1..m} . We denote the set of variable nodes connected to a certain check node i as V{i}. Furthermore, the set V{i} / j denotes the set of variable nodes connected to check node i excluding j. Similarly, the set of check nodes connected to a certain variable node j is denoted by C{j}. C{j} / i denotes the set of check nodes connected to the variable node j excluding i. LDPC codes are efficiently decoded by message passing decoding algorithms. The main idea behind all message passing algorithms is processing the received symbols iteratively in concatenated steps that can be seen over the Tanner graph as horizontal step followed by vertical step. In this section, we review some message passing decoding algorithms that are either used for comparision or as the starting point of our modified algorithm.

Sum-Product algorithm (SPA)

One iteration of the tanh-based SPA is described in the following steps:-

Initialization step: The LLR of bit number j is initialized with its channel LLR (Uchj). These initial values are used as vj  →  i , messages from variable node j to check nodes i [formula] i∈C{j}.

Horizontal step: At each check node i, messages vj  →  i (which come from variable nodes V{i}) are used to calculate the reply messages Ui  →  j for all j∈V{i} by [\eqref=eq:1].

[formula]

Vertical step: At each variable node j, messages Ui  →  j are used to calculate the reply messages vj  →  i for all i∈C{j} by [\eqref=eq:2].

Decision step For each variable node j, its LLR is updated by [\eqref=eq:3]

The LLR values are applied to the hard decision to decide on the bit value to be 1 if LLRj < 0 and zero otherwise. The syndrome is calculated and checked; if it is all-zero vector, this word is successfully decoded, otherwise, if the syndrome condition is not satisfied, the decoder proceeds to the next iteration. This process continues till either the code word is successfully decoded or the maximum iterations are exhausted.

Min-Sum algorithm

The Min-Sum algorithm follows the same steps as SPA. It only approximates the horizontal step calculation by minimum operation as shown in [\eqref=eq:4] [\cite=fossorier1999reduced] Min-Sum is easier to implement, as it gets rid of the tanh(.) calculation. However, the approximation of the tanh(.) to the min(.) leads to some loss of performance compared to the tanh-based SPA algorithm. This loss of performance is partially recovered by Scaled Min-Sum algorithm.

Scaled Min-Sum algorithm

In order to decrease the gap between the min-sum and the tanh-based SPA algorithms, a constant scaling factor (α  <  1) is applied to the check node updating equation [\eqref=eq:4]. In other words, converts the Horizontal step to [\eqref=eq:5] This scaling factor is optimized to maximize the performance of Scaled Min-Sum algorithm.

SVS-min-sum algorithm

Changing the scaling factor with iteration for irregular LDPC codes is used in [\cite=lechner2006improved]. Despite of performance enhancement of this variable scaled min-sum algorithm, it requires extra storage because we need different scaling factor value per iteration, associated with different mutual information into passed messages per iteration [\cite=lechner2006improved]. The general fractional values (taken by the scaling factors) make the multiplication operation complex to implement. We proposed in [\cite=ahmed14CCNC] an SVS-min-sum algorithm addresses the particular point of simply per-iteration updated scaling rule. As stated in [\cite=lechner2006improved], the scaling factor should increase exponentially with iterations and its final value is 1. So we approximate the scaling factor sequence to a stair sequence which is updated every S iterations, increase exponentially and easy to implement. The variable scaling factor can be calculated as:

[formula]

Where ⌈i / S⌉ is the first integer greater than or equal to i / S . i is the iteration index which takes values {1,2,3...}. By using [\eqref=eq:6], the scaling factor of each iteration can be calculated as shown in table [\ref=table_I]. This sequence is:-

Easy to design, because it requires a single parameter S.

Does not need to store a specific scaling sequence for each code rate. It only requires to store the optimal updating step size S of each code rate.

Easy to implement, because it only requires shifting right by ⌈i / S⌉ then subtraction. Number of required shifts ⌈i / S⌉ can be stored in a register and increased by 1 every S iterations.

As shown in Fig. [\ref=SVS_block], the SVS scaling strategy is implemented by two sub-circuits. The first sub-circuit calculates the number of shifts required in each iteration ⌈i / S⌉, this number of shifts is calculated by dividing the iteration clock by S (updating step) to generate the updating clock of the scaling factor, then this updating clock is used to count the required number of shifts ⌈i / S⌉ using a counter starts by 1. The other sub-circuit multiplies the minimum operation output U' by α specified in [\eqref=eq:6] by using a Barrel shifter to shift U' right by ⌈i / S⌉ then subtract.

Generalized SVS-min-sum (GSVS-min-sum) algorithm

As shown in table [\ref=table_I], SVS-min-sum sequence starts with 0.5 and increases exponentially with iterations. The limitation of starting with a fixed value of 0.5 restricts the performance to be sub-optimal. As a solution, we propose a new GSVS-min-sum algorithm where the scaling factors sequence is calculated by [\eqref=eq:7]:

[formula]

Where α0 is the initial scaling factor. By using [\eqref=eq:7], scaling factor of each iteration is calculated as shown in table [\ref=table_II], where scaling factor values start with α0 and increase exponentially to unity for large value of iteration index i. Circuit representation of GSVS scaling is shown in Fig. [\ref=GSVS_block]. It is similar to SVS scaling circuit, but with two main differences: the first difference is that U' is multiplied by (1 - α0) before shifting right, this is added to generalize the initial scaling factor. (1 - α0) is chosen to be simply implemented. We use (1 - α0) to be in the form of 2- i or 2- j + 2- k, where i, j and k are integer numbers; for example, if i  =  2   →   (1  -  α0)  =  0.25, i  =  3   →   (1  -  α0)  =  0.125 and if j  =  2,k  =  3   →   (1  -  α0)  =  0.375 . The second difference is that the counter of required shifts starts with 0 instead of 1 because GSVS-min-sum requires (⌈i / S⌉ - 1) shifts not ⌈i / S⌉ as in SVS-min-sum. In SVS-min-sum, we only need to optimize the updating step size S, however, in GSVS-min-sum we also need to optimize the initial scaling factor α0. To calculate the optimal (α0,S)opt., we use Nelder-Mead optimization Method [\cite=nelder1965simplex] (summarized in [\ref=sub:NM]) to minimize (Eb / N0)min, where (Eb / N0)min is the minimum Eb / N0 required to achieve pre-specified BER threshold. (Eb / N0)min of given (α0,S) is calculated by DE.

Density Evolution (DE) of Min-Sum based algorithms

DE checks the ability of an LDPC decoder to correctly decode messages with specific noise variance. This is done by tracing the Probability Density Function (PDF) of messages passed between check and variable nodes (using all-zero code-word). Using of all-zero code-word is valid in Binary Phase Shift Keying (BPSK) because the LLR of both 1 and 0 has a similar PDF shape, but this is not valid in Quadrature Amplitude Modulation (QAM) signaling [\cite=tullberg2005serial]. DE is used in [\cite=mackay1999good] to obtain the optimal weight distribution of irregular LDPC codes, and is used in [\cite=chen2002density] [\cite=zhang2005improved] to calculate the optimal scaling factor(s) of LDPC decoder. We use the same DE as in [\cite=zhang2005improved] after changing the PDF of channel LLR so that we can use it with QAM signaling.

Channel LLR's PDF of BPSK over an AWGN channel

In BPSK, all-zero code-word's bits Vk = 0 are modulated to xk = 1 - 2Vk = 1. Then xk is transmitted over an Additive White Gaussian Noise (AWGN) channel with noise variance σ2, so the received sequence is yk = 1 + nk where nk is normally distributed random variable with mean=0 and variance=σ2 . Therefore, the channel LLR (Uchk = 2  ×  yk  /  σ2) is normally distributed random variable with mean=2 / σ2 and variance=4 / σ2. The PDF of Uchk is used as the initial PDF of variable nodes' messages to check nodes.

Channel LLR's PDF of higher order QAM constellation over an AWGN channel

As shown in [\cite=tullberg2005serial], for higher order constellations, we cannot assume that all-zero code-word was transmitted. Therefore, we used a similar procedure to [\cite=tullberg2005serial], where authors modified the definition of bit's LLR to be: "LLR of receiving the same bit value as was transmitted" instead of "LLR of receiving 0". This is equivalent to replacing Uch by U+ch, where U+chk = Uchk  ×  (1 - 2Vk). So U+chk will be positive if and only if Uchk has the same sign as given by Vk . Firstly, for any constellation point W, we calculate the PDF of Uch for bit number l into W given that W is transmitted fUch(ul / W) [\cite=benjillali2006probability]. Then we calculate the average PDF of U+ch by [\eqref=eq:8].

[formula]

Where l is the bit position index. Only half of bit positions were used, because of symmetry between real and imaginary axes of QAM signaling. Zl is the set of W where bit position l contains 0. Ol is the set of W where bit position l contains 1. η is PDF correction factor used to ensure that area under the PDF=1. In other words, calculate the average PDF of Uch for zeros and - Uch for ones. Then use this PDF as the initial PDF of variable nodes' messages.

Nelder-Mead (NM) optimization method

NM optimization [\cite=nelder1965simplex] of (α0,S) is based on constructing a simplex (polygon) of 2+1=3 random solution points {Xi = (α0,S)i|i = 1,2 or 3} for our 2-dimension problem. After calculating (Eb / N0)min of these three points, the worst point is replaced by a better point as described in the flow chart in Fig.[\ref=NM_block]. This procedure is repeated until the simplex shrink enough to the optimal solution. We use Nelder-Mead method for many reasons: first, it does not need the mathematical derivative of the cost function (which is not available because our cost function is (Eb / N0)min which comes from DE). Second, Nelder-Mead is faster in convergence than other heuristic methods. Finally, our cost function has only one minimum, so the algorithm does not get trapped in a local minimum. To prove the last claim that (Eb / N0)min has only one minimum, we calculated it for all possible combinations of (α0,S) for short length LDPC code with rate 0.5 specified in DVB-T2. Fig.[\ref=single_min] shows that (Eb / N0)min has only one minimum. Similar results are obtained for all tested codes. Optimization of (α0,S) is calculated offline for each LDPC code rate, then the optimal value is used to implement the decoding circuit.

Simulation environment and results

For simulation, we used (16200, 7200) eIRA LDPC code specified in DVB-T2 standard [\cite=bluebooka122] [\cite=bluebooka133]. Data are produced as binary bits modulated using the challenging 256-QAM modulation scheme and sent over an AWGN channel. The simulations are performed using MATLAB platform. Maximum number of iterations is set to 40 iterations.

Fig.[\ref=WER_05_Short] shows the WER of LLR-SPA, SVS-min-sum with S = 10 [\cite=ahmed14CCNC], Scaled Min-Sum with α  =  15 / 16 (optimized by DE and the same as in [\cite=ahmed14CCNC]), GSVS-min-sum with (α0 = 0.75 and S = 9) (optimized by DE with Nelder-Mead method) and 2D correction min-sum; where the output of the check nodes with degree 4,5,6 and 7 is multiplied by 0.94, 0.92, 0.88 and 0.86 respectively, and the output of the variable nodes with degree 1,2,3 and 8 is multiplied by 1.00, 1.00, 0.91 and 0.83 respectively [\cite=zhang2005improved]. Results in Fig.[\ref=WER_05_Short] show that:

GSVS-min-sum has better performance than SVS-min-sum by 0.5 dB at WER= 10- 3, this indicates the importance of the proposed algorithm, which jointly optimizes the initial scaling factor α0 with the updating step size S.

Although 2D correction min-sum has an excellent performance after many iterations (200 iterations) [\cite=zhang2005improved], it has higher WER than GSVS-min-sum algorithm after 40 iterations for the whole simulation range and higher than scaled-min-sum for high Eb / N0 range. The gap between GSVS-min-sum and 2D correction is 0.3 dB at WER= 10- 3.

There is a small gap between GSVS-min-sum and LLR-SPA performances (0.1 dB for low Eb / N0 and nearly disappears at high Eb / N0), with much lower implementation complexity.

For the scaled min-sum algorithm, Optimizing the scaling factor of each code rate increases its performance specially for high Eb / N0. This concept is illustrated in [\cite=ahmed14CCNC] by showing that each code rate of DVB-T2 LDPC codes has different optimal scaling factor.

Fig.[\ref=Itr_05_Short] shows clearly that GSVS-min-sum not only has lower WER than other min-sum based algorithms, but also it has the lowest average number of iterations which leads to lower latency and higher average throughput. For more results, we used three different rates of the LDPC codes specified in DVB-T2 standard with BPSK, these codes are (16200,7200) short code with nominal rate = 0.5, (16200,11880) short code with nominal rate = 0.75 and (64800,48600) normal code with rate = 0.75. WER of these codes with constant scaled-min-sum, SVS-min-sum and GSVS-min-sum decoding algorithm are shown in Fig. [\ref=WER_BPSK]. Simulation results show that GSVS-min-sum has the lowest WER among the three decoding algorithms, even though scaled min-sum has lower WER than SVS-min-sum or not. Note that: constant scaling (in scaled min-sum) and SVS-min-sum are special cases of the GSVS-min-sum where S= number of iterations for scaled min-sum and α0 = 0.5 for SVS-min-sum. So GSVS-min-sum, which has optimized values for both α0 and S, has the best performance between them as shown in Fig. [\ref=WER_BPSK]. The parameters of the three decoding algorithms are shown in table [\ref=table_III]. For (16200,11880) code, GSVS-min-sum has the same performance as constant scaling factor and better performance than SVS-min-sum. The poor performance of SVS-min-sum comes from the limitation of starting by 0.5 which is away from the optimal scaling sequence. For the other codes, GSVS-min-sum has better performance than both SVS-min-sum and constant scaled min-sum.

CONCLUSION AND FUTURE WORK

In this paper, we generalized the SVS-min-sum decoder by allowing it to start with any initial scaling factor α0. Simulation results indicated the superior performance and lower latency of GSVS-min-sum decoder to other min-sum based algorithms. In addition, GSVS-min-sum algorithm performance is very close to the LLR-SPA with much lower complexity. Moreover, the proposed algorthim is still simpler to implement than both the variable scaling factor in [\cite=lechner2006improved] and the 2D correction Min-Sum in [\cite=zhang2005improved]. As future work, we will apply our GSVS-min-sum decoding algorithm to layered LDPC codes implementation.

Acknowledgment

This work is supported by E-JUST and Minister of High Education (MoHE). And is supported by NTRA as part of the project "Design and implementation of DVB-T/T2 solution".