The use of a common location measure in the invariant coordinate selection and projection pursuit

Keywords: Cluster analysis; Invariant coordinate selection; Projection pursuit; Robust scatter matrices; Location measures; Multivariate mixture model.

Introduction

Consider a multivariate dataset, given as an n  ×  p data matrix X, and suppose we want to explore the existence of any clusters. One way to detect clusters is by projecting the data onto a lower dimensional subspace for which the data are maximally non-normal. Hence, methods that are sensitive to non-normality can be used to detect clusters.

One set of methods based on this principle is invariant coordinate selection (ICS), introduced by [\citet=Tyler], together with a one-dimensional variant called projection pursuit (PP), introduced by [\citet=friedman1974]. ICS involves the use of two scatter matrices, S1  =  S1(X) and S2  =  S2(X) with S2 chosen to be more robust than S1. An eigen-decomposition of S- 12S1 is carried out. If the data can be partitioned into two clusters, then typically the eigenvector corresponding to the smallest eigenvalue is a good estimate of the clustering direction. The main choice for the user when carrying out ICS is the choice of the two scatter matrices.

However, in numerical experiments based on a simple mixture of two bivariate normal distributions, some strange behaviour was noticed. In certain circumstances, ICS, and its variant PP, badly failed to pick out the right clustering direction. Eventually, it was discovered that the cause was the use of different location measures in the two scatter matrices. The purpose of this paper is to explore the reasons for this strange behaviour in detail and to demonstrate the benefits of using common location measures.

Section [\ref=sec:sec2] gives some examples of scatter matrices and reviews the use of ICS and PP as clustering methods. Section [\ref=sec:sec3] sets out the multivariate normal mixture model with two useful standardizations of the coordinate system. Section [\ref=sec:sec4] demonstrates in the population setting an ideal situation where ICS and PP work as expected and where an analytic solution is available -- the two-group normal mixture model where the two scatter matrices are given by the covariance matrix and a kurtosis-based matrix. Some examples with other robust estimators are given in Sections [\ref=sec:sec5]-[\ref=sec:sec6], which show how ICS and PP can go wrong when different location measures are used and how the problem is fixed by using a common location measure.

Notation. Univariate random variables, and their realizations, are denoted by lowercase letters, x, say. Multivariate random vectors, and their realizations, are denoted by lowercase bold letters, [formula], say. A capital letter, X, say is used for n  ×  p data matrix containing p variables or measurements on n observations; X can be written in terms of its rows as

[formula]

with ith row [formula].

Background

Scatter matrices

A scatter matrix S(X), as a function of an n  ×  p data matrix X is a p  ×  p affine equivariant positive definite matrix. Following [\citet=Tyler], it is convenient to classify scatter matrices into three classes depending on their robustness.

Class I: is the class of non-robust scatter matrices with zero breakdown point and unbounded influence function. Examples include the covariance matrix defined below in [\eqref=eq:var] and the kurtosis-based matrix in [\eqref=eq:kmat].

Class II: is the class of scatter matrices that are locally robust, in the sense that they have bounded influence function and positive breakdown points not greater than [formula]. An example from this class is the class of multivariate M-estimators, such as the M-estimate for the t-distribution [\citep=kc94a] [\citep=arslan1995convergence].

Class III: is the class of scatter matrices with high breakdown points such as the Stahel-Donoho estimate, the minimum volume ellipsoid (mve) [\citep=van2009minimum] and the constrained M-estimates, [\citep=kent1996constrained].

Each scatter matrix has an implicit location measure. Let us look at the main examples in more detail, and note what happens in p = 1 dimension. The labels in parentheses are used as part of the notation later in the paper.

The sample covariance matrix (var) is defined by

[formula]

where for convenience here a divisor of 1 / n is used, and where [formula] is the sample mean vector. The implicit measure of location is just the sample mean.

The kurtosis-based matrix (kmat) is defined by

[formula]

Note that outlying observations are given higher weight than for the covariance matrix, so that K is less robust than S. Again the implicit measure of location is just the sample mean. When p = 1, the scatter matrix S- 1K reduces to 3 plus the usual univariate kurtosis.

The M-estimator of scatter based on the multivariate tν-distribution for fixed ν is the maximum likelihood estimate obtained by maximizing the likelihood jointly over scatter matrix Σ and location vector [formula]. If both parameters are unknown and ν  ≥  1, then under mild conditions on the data, the mle of [formula], is is the unique stationary point of the likelihood. Similarly, if ν  ≥  0 and [formula] is known, the mle of Σ, is is the unique stationary point of the likelihood [\citep=kc94a]. In either case, an iterative numerical algorithm is needed. Note that when [formula] is to be estimated as well as Σ, the mle of μ is the implicit measure of location for this scatter matrix. For this paper we limit attention to the choice ν = 2 (and label it below by t2).

The minimum volume ellipsoid (mve) estimate of scatter [formula], introduced by [\citet=rousseeuw1985], is the ellipsoid that has the minimum volume among all ellipsoids containing at least half of observations, and its implicit estimate of location, [formula], say, is the centre of that ellipsoid. Calculating the exact mve requires extensive computation. In practice, it is calculated approximately by considering only a subset of all subsamples that contain 50% of the observations, [\citep=van2009minimum] [\citep=maronna2006]. If the location vector is specified, the search is limited to ellipsoids centred at this location measure.

When p = 1, the mve reduces to the lshorth, defined as the length of the shortest interval that contains at least half of observations. The corresponding estimate of location, [formula], say, is the midpoint of this interval. Calculating the lshorth around a known measure of location is trivial; just find the length of the interval that contains half of observations centered at this location measure. The lshorth was introduced by [\cite=grubel1988length], building on earlier suggestion of [\citet=andrews1972robust] to use [formula], which they called the shorth, as a location measure.

The minimum covariance determinant estimate of scatter (mcd), [formula] is defined as the covariance matrix of half of observations with the smallest determinant. The mcd location measure, [formula], say, is the sample mean of those observations. The mcd can be calculated approximately by considering only a subset of all subsamples that contain at least half of observations, [\citep=rousseeuw1999fast]. The mcd estimate of scatter with respect to a known location measure [formula] is defined as the covariance matrix about [formula] of half of observations with the smallest determinant. Recall that the covariance matrix about [formula] for a dataset is given by [formula], where S and [formula] are the sample covariance matrix and mean vector of the dataset.

When p = 1, the mcd reduces to a truncated variance, [formula], say, defined as the smallest variance of half the observations. Its implicit measure of location, [formula], say, is the sample mean of that interval. Also, a modified definition of [formula] using a known location measure is trivial and does not require any search; just find the interval that contains half of observations centered at the given location measure and calculate the variance.

Routines are available in R [\citep=R] to compute (at least approximately) these robust covariance matrices and their implicit location measures, in particular, tM from the package ICS [\citep=nordhausen2008] for the multivariate t-distribution, cov.rob from the package MASS [\citep=venables2010package] for mve, and CovMcd from the package rrcov [\citep=todorov2008package] for mcd. Modified versions of these routines have been written by us to deal with the case of known location measures.

Invariant coordinate selection and projection pursuit

Given an n  ×  p data matrix X, the ICS objective function is given by the ratio of quadratic forms

[formula]

where S1 = S1(X) and S2 = S2(X) are two scatter matrices. By convention, S2 is chosen to be more robust than S1. For exploratory statistical analysis, attention is focused on the choices for [formula] maximizing or minimizing [formula]. These values can be calculated analytically as the eigenvectors of S- 12S1 corresponding to the maximum/minimum eigenvalues.

The original ICS method did not make a strong distinction between the largest and the smallest eigenvalues. However for clustering purposes between two groups, when the mixing proportion is not too far from 1 / 2, it is the minimum eigenvalue which is of interest; see Section [\ref=sec:sec4].

The method of PP can be regarded as a one-dimensional version of ICS. It looks for a linear projection [formula] to maximize or minimize the criterion,

[formula]

where [formula] and [formula] are two one-dimensional measures of spread. In general, optimizing [formula] must be carried out numerically. Searching for a global optimum is computationally expensive, and the complexity of the search increases as the dimension p increases. Alternatively, we can search for a local optimum starting from a sensible initial solution, such as the ICS optimum direction.

Both ICS and PP are equivariant under affine transformations. That is, if X is transformed to [formula], where Q(p  ×  p) is nonsingular and [formula] is a translation vector in [formula], then for either ICS or PP the new optimal vector [formula], say, for U is related to the corresponding optimal vector [formula] for X by

[formula]

For numerical work it is convenient to have an explicit notation for the different choices in ICS and PP. If Scat1 and Scat2 are the names of two types of multivariate scatter matrix, each computed with its own implicit location measure, then the corresponding versions of ICS and PP will be denoted

[formula]

Note that PP is based on the univariate versions of Scat1 and Scat2. For example, ICS based on the covariance matrix and the minimum volume ellipsoid will be denoted by ICS:var:mve. Other choices for scatter matrices have been summarized in Section [\ref=sec:sec2].

When a common location measure is imposed on Scat1 and Scat2, then this restriction will be indicated by the augmented notation

[formula]

and similarly for PP. In this paper the only choice used for the location measure is the sample mean (mean). For example, ICS based on the covariance matrix and the minimum volume ellipsoid, both computed with respect to the mean vector, is denoted

[formula]

The two-group multivariate normal mixture model

The simple model used to demonstrate the main points of this paper is the two group multivariate normal mixture model, with density

[formula]

where φp is the multivariate normal density, [formula] and [formula] are two mean vectors, Ω is a common covariance matrix, and 0 < q < 1 is the mixing proportion. Even in this simple case, major problems with ICS and PP can arise.

Since ICS and PP are affine equivariant, we may without loss of generality choose the coordinate system so that

[formula]

where [formula] is a unit vector along the first coordinate axis, and α > 0. That is, [formula] and [formula] lie equally spaced about the origin along the first coordinate axis, and the covariance matrix of each component equals the identity matrix.

A random vector [formula] from the mixture model can also be given a stochastic representation,

[formula]

where [formula] independently of an indicator variable s,

[formula]

Moments under the mixture model are calculated most simply in terms of this stochastic representation. In particular,

[formula]

so that the covariance matrix is

[formula]

For practical work it is also convenient to consider a standardization for which the overall covariance matrix is the identity matrix. That is, define a new random vector

[formula]

where [formula], where c1  =  {1 + 4q(1 - q)α2}1 / 2, and [formula]. Then [formula] has a stochastic representation

[formula]

where

[formula]

and

[formula]

where the first diagonal term σ2η has two equivalent formulas,

[formula]

The first two moments of [formula] are

[formula]

A population example: PP based on the kurtosis and ICS based on the kurtosis-based matrix and the covariance matrix

In this section we look at ICS:kmat:var and PP:kmat:var in the population case. In this setting it is possible to derive analytic results. Note that since kmat is based on fourth moments it is less robust than the variance matrix; hence kmat is listed first.

Recall the kurtosis of a univariate random variable u, say, with mean μu, is defined by

[formula]

The univariate kurtosis is zero when the random variable has normal distribution. For non-normal distributions the kurtosis lies in the interval

[formula]

(s) = - 6 + 4/σ.

[formula]

(δ u + δ u ) = δ (u) + δ (u).

[formula]

(p+2+, p+2, , p+2).

[formula]

b C a,

[formula]

tan φ = c tan θ,

[formula]

The effect of using a common location measure on ICS and PP

As we mentioned earlier in Section [\ref=sec:sec22], the ICS and PP criteria are expected to have similar behaviour to the kurtosis-based criteria in Section [\ref=sec:sec4]. Namely, they are expected to be minimized in the clustering direction when the mixing proportion is not too far from 1/2.

However, when applying ICS with at least one robust estimate of scatter (mainly from Class III), some peculiar behaviour was observed. In particular, the ICS criterion was often maximized in the clustering direction rather than minimized.

Here is an explanation. Under the two-group mixture model with one group slightly bigger than the other, a class III scatter matrix will typically home in on the larger group, with its corresponding location measure at the center of this group and its estimate of the scatter matrix capturing the spread of this group. The other scatter matrix (Class I or II) will measure the overall scatter of the data with its corresponding location measure at the overall center of the data. The result is erratic behaviour in [formula] and [formula].

Imposing a common location measure on the two scatter matrices fixes this problem. Here is an example in p = 2 dimensions to illustrate the issues in greater detail.

In this example we look at ICS:var:mve for the population bivariate normal mixture model in Section [\ref=sec:sec3], with q = 1 / 2 and any value of α > 0, i.e. 0  ≤  δ  ≤  1, where δ is given in [\eqref=eq:delta]. Standardize the coordinate system so that the overall covariance matrix is the identity, Σy = I2. Let [formula] denote the population minimum volume ellipsoid scatter matrix.

Then it turns out that [formula] is the within-group covariance matrix for (either) one of the groups,

[formula]

where 0  ≤  δ  ≤  1 is given in ([\ref=eq:delta]). The implicit estimate of the center of the data will be given by the center of either group, [formula]; both values fit equally well.

Figure [\ref=fig:fig3] shows that the clustering direction estimated by the ICS:var:mve method is the direction that minimizes [formula] (the eigenvector of the smallest eigenvalue of [formula]), namely (0,1)T, i.e. φ  =    ±  π / 2. However, the true direction of group separation direction is (1,0)T, i.e. φ = 0.

Next consider ICS:var:mve:mean, i.e. the common mean version of the previous example. The overall mean of the data is at the origin. When [formula] is constrained to have its location measure at the origin, then the ICS criterion now picks out the true clustering direction. In order to give an analytic proof of this result, we restrict attention to the the limiting case of the balanced mixture model, i.e when δ = 1, q = 1 / 2. Hence, the group components will lie on two parallel vertical lines with means

[formula]

and within-group covariance matrix

[formula]

In this setting, it can be shown that the population version of the MVE matrix, [formula], say, takes the form

[formula]

where d  =  Φ- 1(.75)  =  0.674, the 75th quantile of the standard normal distribution (see the Appendix). Hence the dominant eigenvector is [formula]. The ellipse of [formula] is plotted in Figure [\ref=fig:mve]. Figure [\ref=fig:fig4] shows that the criterion of ICS:var:mve:mean, [formula] picks out the correct clustering direction [formula].

Like ICS, PP can fail to detect the clustering direction if applied using different location measures. The reason for that is the projection direction that separates the data into two groups with one slightly bigger than the other, the more robust measure of spread will be located at the larger group. In Section [\ref=sec:sec6], we give a detailed numerical example of the problem arising from using two different location measures in PP:var:mcd, and how the problem is fixed by using a common location measure.

Examples

Overview

In this section, we give numerical examples that demonstrate different ways in which ICS and/or PP can go wrong. We also show the effect of using common location measures in these examples. We use one simulated data set and apply different ICS and PP methods, with and without imposing a common location measure (the mean).

A two-dimensional data set of size n = 500 is generated from the balanced mixture model, defined in Section [\ref=sec:sec3], with q = 1 / 2, and α = 3, so that δ = 0.95. Thus the two groups are well-separated and no sensible statistical method should have any problem finding the two clusters. All calculations are done after standardization with respect to the "total" coordinates. That is, the data matrix Y(500  ×  2) is standardized to have sample mean [formula] and sample covariance matrix I2.

The ICS and PP methods used are:

(PP,ICS):var:t2 with corresponding criteria [formula], and [formula].

(PP,ICS):var:mcd with corresponding criteria [formula], and [formula].

(PP,ICS):var:mve with corresponding criteria [formula], and [formula].

(PP,ICS):t2:mcd with corresponding criteria [formula], and [formula].

(PP,ICS):t2:mve with corresponding criteria [formula], and [formula].

When imposing the mean as the common location measure, the ICS and PP criteria will be denoted by [formula] and [formula], where [formula].

To understand the behaviour of the ICS and PP, their criteria are plotted against -  π / 2  ≤  φ  ≤  π / 2. The plots are shown in Figure [\ref=fig:fig5].

From the panels in Figure [\ref=fig:fig5], we make the following remarks based on the simulated data set:

Panel (a) shows that ICS:var:t2 and PP:var:t2 work well since [formula] and [formula] are approximately equal. Hence, imposing a common location measure has little effect, as shown in (b).

Panels (c), (e), (g), (i) show examples when ICS and/or PP go wrong because of the difference in the location measures.

Using a common location measure fixes the problem in panel (d) for (PP, ICS):var:mcd, panel (f) for (PP, ICS):var:mve, and panel(h) for (PP, ICS):t2:mcd.

From panel (j), using a common location measure in PP:t2:mve:mean does not seem to work well. The reason might be due to the unstable behaviour of the mve and lshorth.

The plots generally suggest that PP will be more accurate than ICS, since the PP plots are narrower at the clustering direction than the ICS plot. This property has been confirmed empirically in [\citet=Fatimah] for certain multivariate normal mixture models and choices of scatter matrix.

Similar patterns are seen with most simulated data sets from this model.

Behaviour of ICS:var:mcd

To gain a deeper understanding of the behaviour of ICS:var:mcd in panel [\ref=fig:fig5] (c) and the effect of forcing a common location measure on mcd in panel (d), we plot the ellipses of [formula] ( with and without imposing a common location meaure) and superimpose it on the data points of our example. The plots are shown in panels [\ref=fig:fig6] (a) and (b). The behaviour in this example agrees with the interpretation given for the population example in Section [\ref=sec:sec5].

Behaviour of PP:var:mcd

The objective function for PP:var:mcd, has a similar problem to ICS; it is maximized rather than minimized near the correct clustering direction.

To understand this behaviour in more detail, we plot in Figure [\ref=fig:fig8] one-dimensional histograms after projections by the following choices for the angle φ: [formula], [formula], [formula], and [formula]. For each histogram, we plot the 50% of the data that has the smallest variance, and the corresponding location measure [formula]. The plots are repeated where the location measure is constrained at the sample mean x̄ = 0.

The shape of the histograms depends on of the projection directions. Note that as [formula] gets smaller, the PP criterion [formula] gets larger.

The [formula] projection produces two widely separated groups with one group is slightly bigger than the other. In this case, [formula] is at the larger group and [formula] is essentially the variance of this group. Hence [formula] takes its smallest value and [formula] is largest.

The [formula] projection produces two slightly separated groups with within-group variance is larger than in the [formula] projection. The value of [formula] is larger than for [formula].

The [formula] projection produces one group, with a pseudo-uniform distribution. The value of [formula] is larger than for [formula].

The [formula] projection produces one normally distributed group. The value for [formula] becomes small again.

Constraining the mean to be at the origin fixes the problem. The value of [formula] steadily decreases from [formula] to [formula].

Appendix

Consider the limiting balanced bivariate normal mixture model,

[formula]

where s  =    ±  1, each with probability 1/2, independent of z  ~  N(0,1), and [formula], [formula]. This model is standardized with respect to the "total" coordinates; i.e. [formula] and [formula]. The model can also be described in terms of a mixture of two normal distributions, concentrated on the vertical lines y1 = 1 and y1  =   - 1.

In this appendix we shall show that the population version of the mve, constrained to be centred at at the origin, is given by

[formula]

where d  =  Φ- 1(.75) in terms of the cumulative distribution function of the N(0,1) distribution.

First let u1  <  u2 be two possible values for y2 and consider and ellipse based on a matrix Σ, with inverse Σ- 1  =  Ω,

[formula]

which intersects the vertical lines at these points,

[formula]

By symmetry the ellipse also intersects the points ( - 1, - u1)T and ( - 1, - u2)T. Note that Σ will be a candidate for the mve matrix if the interior of the ellipse covers 50% of the probability mass, that is,

[formula]

If u1 and u2 are finite, then necessarily u1  <  0 and u2 > 0.

The proof will proceed in two stages. First, for fixed u1, u2 satisfying [\eqref=A:coverage], we choose Σ to minimize det (Σ) (or equivalently maximize det (Ω)). Secondly, we optimize over the choice of u1, u2.

Thus, start with a fixed pair of values u1, u2 satisfying [\eqref=A:coverage]. If [formula] represents a point on one of the vertical lines, then the intersection with the ellipse [\eqref=A:ellipse] can be written

[formula]

or equivalently as the quadratic equation in u,

[formula]

where A  =  ω22, B  =  2ω12, C  =  ω11 - 1. If this ellipse passes through (1,u1)T and (1,u2)T, then then u1, u2 are roots of the quadratic equation, so

[formula]

In particular, setting M  =  (u1 + u2) / 2 to be the mean of the roots, and P  =  u1u2 to be the product of the roots, we have

[formula]

Let us try to maximize det (Ω) subject to the ellipse satisfying [\eqref=A:ellipse-lines]. Start with an arbitrary ω22 > 0. Then [\eqref=A:roots] determines the remaining elements of Ω,

[formula]

Hence

[formula]

where

[formula]

Maximizing det (Ω) with respect to the choice of ω22 leads to ω22  =  1 / (2Q) and

[formula]

The remaining task is to choose u1 < 0 (which determines u2 > 0 by [\eqref=A:coverage]) to maximize det (Ω), or equivalently, to minimize Q in [\eqref=A:Q].

Recall a basic result from calculus. If t = f(u) and u  =  g(t) are monotone functions which are inverse to one another, then g(f(u))  =  u. Differentiating two times yields the relation between the derivatives,

[formula]

In particular, consider f(u)  =  Φ(u), with derivatives f'(u)  =  φ(u) and f''(u)  =   - uφ(u), where φ(u) is the probability density function of N(0,1). Then g(t)  =  Φ- 1(t) with derivatives g'(t)  =  1 / φ(u) and g''(t)  =  u / {φ(u)}2, where u  =  Φ- 1(t).

With this notation, write u1  =  g(t) for 0  <  t  <  1 / 2. Then u2  =  g(t + 1 / 2). Write φ1  =  φ(u1), φ2  =  φ(u2). The quantity Q in [\eqref=A:Q], treated as a function of t, has derivatives

[formula]

If u1 =  - d, then u2 = d and φ1  =  φ2 so that the first derivative vanishes. For all (0  <  t < 1 / 2), the second derivative is positive, so the function is convex. Hence Q is minimized for u1 =  - d, u2 = d. Then M = 0,Q =  - P  =  d2 and the optimal Σ becomes

[formula]

as required.