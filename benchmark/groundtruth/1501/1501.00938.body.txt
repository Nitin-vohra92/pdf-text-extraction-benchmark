Schwarz Iterative Methods: Infinite Space Splittings

Introduction

The aim of this paper is to extend convergence results for greedy and randomized versions of multiplicative Schwarz methods for solving elliptic variational problems in Hilbert spaces from the case of finite space splittings [\cite=GrOs2011] [\cite=OsZh2014] to the case of infinite space splittings. Let V be a separable real or complex Hilbert space with scalar product (  ·  ,  ·  ), let a(  ·  ,  ·  ) be a continuous and coercive Hermitian form on V, and let F be a bounded linear functional on V. Note that a(  ·  ,  ·  ) induces a spectrally equivalent scalar product on V, in the sequel we write Va to indicate that we consider V with this new scalar product, and use the notation [formula] for the induced norm. Then the variational problem

Find u∈V such that

[formula]

possesses a unique solution, and is equivalent to the quadratic minimization problem

Find the minimizer u∈V of the quadratic functional

[formula]

We treat the problem in the form (A), by turning it into an infinite linear system using space splittings as described next. The equivalent formulation (B) provides the link to convex optimization, where algorithms similar to the ones considered here are known and investigated under the name block-coordinate descent methods.

For the separable Hilbert space Va, we consider space splittings generated by families of Hilbert spaces Vai (with scalar product ai(  ·  ,  ·  ) and norm [formula]) and bounded linear operators Ri:  Vai  →  Va, i∈I, such that the span of the subspaces RiVai  ⊂  Va is dense in Va. Here, the index set I can be finite ([formula]), or countable ([formula]). These conditions on a space splitting are silently assumed throughout this paper. We call a space splitting stable, and write

[formula]

if

[formula]

where

[formula]

Not every infinite space splitting is stable, in particular, in ([\ref=SSS]) it is assumed that every element in Va possesses at least one converging series expansion with respect to {RiVai} which follows from ([\ref=SSS1]) and the assumed density of span({RiVai}) in Va. The constants λmin and λmax are called lower and upper stability constants, and κ: = B / A is called the condition of the stable space splitting ([\ref=SSS]), respectively. A prominent case of stable space splittings are frames and fusion frames, see [\cite=Ch] [\cite=CaKu] [\cite=PO] [\cite=PO3]. In all these definitions, we allow for redundancy, i.e., [formula] is not required for i  ≠  j, and we do not assume that the Vai are closed subspaces of Va.

For the setup of Schwarz iterative methods we need to define operators Ti:  Va  →  Vai via the variational problems

[formula]

to be solved for given v∈Va on the spaces Vai, i∈I. Evaluating Tiv is equivalent to solving a variational problem in Vai, and it is silently assumed that this is easier than solving the original problem (A). This stems from the fact that Vai has typically much smaller dimension than Va and/or the Hermitian form ai(  ·  ,  ·  ) leads to a linear system with better spectral properties or simpler structure. If the underlying space splitting is finite then, using these Ti, analogs of the classical Jacobi-Richardson and Gauss-Seidel-SOR iterations, called additive and multiplicative Schwarz methods with respect to (stable) space splittings can be defined and investigated, pretty much along the lines of the standard methods, see [\cite=Xu] [\cite=Gr] [\cite=PO] [\cite=GrOs] [\cite=PO4].

Here we formulate a generic version of the multiplicative (also called sequential or asynchronous) Schwarz method with relaxation suitable for the case of infinite space splittings. Choose an initial approximation u(0), and repeat the following steps for [formula] until a stopping criterion is met:

Subproblem pick and solution: Given the current u(m) and an index set Im  ⊂  I, choose an index i = im∈Im (according to some rule to be specified), and compute the partial residual r(m)i: = Tie(m), where e(m): = u - u(m). Although u is unknown, this can be done since the right-hand side in the corresponding subproblem ([\ref=subproblem]) reads

[formula]

and does not depend on knowledge about u.

Linear update: Determine relaxation parameters αm  ≥  0 and ωm (according to some rule to be specified), and set

[formula]

So far, this is a theoretical algorithm since executing Steps 1 and 2 is not feasible without further specification and assumptions. In the case of finite splittings, the Schwarz iterative method figures also under the name alternating directions method (ADM), see [\cite=Ga2004] for references.

As to Step 1, we need to specify the rule for picking the next index i = im. There are at least three standard versions to be considered:

Deterministic orderings. In this case, we choose an index sequence {im} beforehand. In the case of finite splittings, the default orderings are cyclic (irN + k - 1 = k) or symmetric-cyclic (i2rN + k - 1 = k,  i2rN + N + k - 1 = N + 1 - k) for [formula], r = 0,1) which corresponds to the classical SOR and SSOR methods, respectively. A naive deterministic ordering for infinite space splittings would be to choose [formula]. For finite splittings, convergence is known for cyclic orderings from the ADM theory (compare, e.g., [\cite=Ga2004]), see also the convergence rate estimates for Schwarz iterative methods in [\cite=GrOs] and, more recently, for coordinate descent methods and convex optimization problems [\cite=BeTe2013].

Greedy orderings. The idea goes back to Gauss and Seidel, and was popularized by Southwell [\cite=So] [\cite=So1] (the corresponding algorithms for finite splittings are often called Gauss-Southwell methods). Here the decision for the next index im depends on the current iterate u(m), and aims at maximizing the error reduction in the next step. For instance, we can require im∈Im to satisfy

[formula]

where βm∈(0,1] is called weakness parameter. This approach is expensive, as it involves the computation of multiple partial residuals, at least approximately, just to pick the next index. Most of the research on quantitative convergence results for greedy orderings and infinite splittings (see [\cite=Te2011] for an overview) is devoted to the case [formula], where finding an im that satisfies ([\ref=greedySSS]) in a numerically feasible way can be guaranteed only under additional assumptions. In practice, one would prefer working with dynamically growing but finite index sets Im  ⊂  I. In the case of finite splittings, algorithms with greedy orderings have been analyzed in the more general setting of convex optimization methods, see e.g. [\cite=Ts] [\cite=LuTs] [\cite=Ts2001] for early results in this direction, for a short proof in the case of problem (A), see [\cite=GrOs2011].

Random orderings. Choose a sequence of discrete probability distributions

[formula]

and pick i = im∈I randomly according to π(m), [formula]. Even in the case of finite splittings, a theoretical analysis of such algorithms has been started only recently but it revealed that they are competitive with the best (often unknown) deterministic orderings, and numerically much cheaper than greedy orderings. We refer to [\cite=LeLe] [\cite=GrOs2011] [\cite=OsZh2014] [\cite=StVe] for the setting of the present paper (quadratic minimization as in (B)), and to [\cite=Ma2013] [\cite=Ne2012] [\cite=RiTa2012] [\cite=TaRiGo2013] for recent convergence results on block coordinate descent methods for large-scale convex optimization problems.

Certainly, there are many more variants to explore. In this paper, we concentrate on greedy and random orderings for infinite splittings ([formula]).

As to Step 2, many options have been discussed in the literature, especially in connection with greedy orderings, see [\cite=Te] [\cite=Te2011] for an overview and references.

The simplest algorithms result if we fix both parameters αm = 1, ωm  =  ω independently of m, and assume some normalization condition for the Ri. Then we arrive at analogs of the algorithms discussed in the theory of greedy methods under the names weak (WGA) and weak relaxed (WRGA) greedy algorithms. Their counterparts for β = 1 are called pure (PGA) and relaxed (RGA) greedy algorithms, respectively.

More generally, one can try to find αm  ≥  0,ωm simultaneously by minimizing the new error term

[formula]

with respect to α and ω. This is equivalent to solving a two-dimensional quadratic minimization problem. Various restrictions have been proposed, e.g., α  +  ω = 1 is a popular choice, see the early work [\cite=Jo] [\cite=Ba] on relaxed greedy algorithms, and [\cite=Te2012a] [\cite=DeTe2014] in a slightly more general setting.

In this paper, we consider a variant with fixed parameter sequence

[formula]

and with ωm determined by minimizing the error term ([\ref=errornext]) with respect to ω. More explicitly,

[formula]

where m: = (1 - αm) = (m + 2)- 1, m  ≥  0. In the theory of greedy algorithms, this version is labeled GAWR, see [\cite=BCDD] [\cite=Te2].

Further modifications are listed and investigated in [\cite=Te2] [\cite=Te2011], for extensions to convex optimization problems on Hilbert and Banach spaces see the recent papers [\cite=Zh2003] [\cite=Te2012a] [\cite=DeTe2014] [\cite=NgPe2014]. We also mention that instead of finding an optimal approximation u(m + 1) only from span({Rir(m)i,u(m)}) as done in ([\ref=errornext]), one could include earlier approximations u(k), k < m, into the local search in Step 2. Orthogonal matching pursuit is a relatively expensive extension of this type. A less expensive version motivated by the conjugate gradient method would be to find u(m + 1) from span({Rir(m)i,u(m) - u(m - 1)}).

The main contributions of this paper can be summarized as follows. For the greedy case, we restrict ourselves in ([\ref=greedySSS]) to

[formula]

and give a convergence proof for the above specified theoretical algorithm. This proof is, in the case of the Hilbert space setting, a modification of the approach used in [\cite=BCDD] [\cite=Te2]. In particular, we show the convergence rate

[formula]

for u from the class A1 which will be defined below. This class appears naturally in all investigations on greedy algorithms, and the exponent 1 / 2 in the convergence rate estimate is known to be optimal in our considered situation of general space splittings [\cite=DeTe].

For the case of random picks with a fixed probability distribution π(m)  =  π > 0, we prove a similar estimate for the expected error decay,

[formula]

for u from a smaller class Aπ∞  ⊂  A1. To the best of our knowledge, this is the first general convergence result for randomized Schwarz iterative methods in the case of infinite splittings. Using an approximation and density argument, we show convergence in expectation (without guaranteed rate) also for arbitrary u∈Va and for sequences π(m) that converge to a fixed probability distribution π > 0 in [formula].

Although mathematically not difficult, we emphasize that our approach via space splittings (rather than expansions with respect to dictionaries D  ⊂  V and updates along one-dimensional search directions) covers block-iterative methods, auxiliary space techniques, and outer approximation schemes which may lead to a broader applicability of our theoretical findings.

The remainder of this paper is organized as follows: In Section [\ref=sec2] we present our convergence results. To this end, we first introduce approximation spaces Aγq associated with a given infinite space splitting, and give some preparatory lemmata. Then, we prove the main theorems on convergence estimates for greedy and randomized Schwarz iterations. In Section [\ref=sec3] we discuss some further results and consequences.

Convergence Results

Approximation spaces related to space splittings

Throughout this section, set [formula], and fix the families [formula] of auxiliary Hilbert spaces and [formula] of bounded linear operators. Furthermore, let them be such that the span of the linear subspaces RiVai is dense in Va. We assume uniform boundedness of the operators Ri, i.e., there exists a constant Λ such that

[formula]

In theory, this can always be achieved by rescaling either Ri or the auxiliary Hermitian forms ai. For the practical application this is however irrelevant as the minimization with respect to ω in the update Step 2 of the algorithms automatically takes care of this. Unless stated otherwise, we will not assume that ([\ref=SSS]) is a stable space splitting for Va.

For any non-negative weight sequence [formula], and 0 < q  ≤    ∞  , introduce approximation spaces [formula] as follows: For [formula], define

[formula]

and introduce Aγq as the completion with respect to this (quasi-)semi-norm. For the weight sequence γi = 1, we drop the superscript γ from the notation. The cases we are most interested in are

[formula]

where π  ≥  0 is any given discrete probability distribution, i.e., πi  ≥  0 and [formula]. The embeddings are continuous. If ([\ref=SSS]) is a stable splitting, then obviously A2 = Va, and all spaces Aπ∞ and Aq, q < 2, are subspaces of Va. They are also dense in Va (for Aπ∞ under the assumption that [formula]). For the case of one-dimensional subspaces of Va generated by a countable dictionary, these definitions are standard and have been instrumental for setting up a quantitative convergence theory for greedy algorithms with infinite dictionaries, see [\cite=Te] [\cite=Te1].

The following technical lemma is crucial for our convergence proofs below.

For the underlying space splitting, assume ([\ref=RBound]). For any e∈Va, denote ri = Tie∈Vai, [formula], [formula]. a) If [formula] is such that [formula] for some 0 < β  ≤  1, then, for any nontrivial h∈A1, we have

[formula]

b) If π is any discrete probability distribution, then, for any nontrivial h∈Aπ∞, we have

[formula]

Proof. Since ri = Tie is the unique minimizer of the associated quadratic minimization problem, i.e.,

[formula]

for any index i, one concludes for any vi∈Vai with [formula] that

[formula]

If ri  ≠  0, after dividing by [formula], this yields

[formula]

This inequality holds for ri = 0 as well, since in this case 0 = ai(Tie,vi) = a(e,Rivi) for all vi∈Vai. Since by

[formula]

convergence in A1 obviously implies convergence in Va, we can assume that for any ε > 0, there exists a finitely representable

[formula]

(with finite [formula] and [formula]) such that

[formula]

For Part a), we thus have

[formula]

and letting ε  →  0 implies ([\ref=Agreedy]).

Similarly, in Part b) we can choose h̃ of the form ([\ref=hhref]) such that

[formula]

Then, by the same reasoning

[formula]

With ε  →  0, we get ([\ref=Arandom]) which finishes the proof of Lemma 1. [formula]

Below, we will apply this lemma with e = e(m): = u - u(m), i.e. the error after m steps of the algorithm, and with β  =  βm, i.e. the weakness parameter in the case of greedy orderings, while π coincides with the probability distribution used to create random orderings.

As another preparation, we formulate an auxiliary result for approximation in Aπ∞ spaces that will allow us to work with variable probability distributions (see also Remark 4 in Section [\ref=sec3]).

Assume that π > 0 is a fixed probability distribution with support [formula], and assume that π(m)  ≥  0 is a sequence of probability distributions that converges to π in the [formula] norm. For the underlying space splitting, assume ([\ref=RBound]). Then, for any given h∈Aπ∞, there exists a sequence of finitely representable

[formula]

such that for m  ≥  0

[formula]

Proof. Since h∈Aπ∞, for given m  ≥  0 and δ > 0 there is a finite I' and a

[formula]

such that

[formula]

In the definition of h(m), set [formula] with a constant c to be fixed below. Then, obviously, we have

[formula]

But for [formula] we have (1 - c)πi  ≤  πi  -  π(m)i, thus

[formula]

On the other hand, by the definition of [formula] and h(m)

[formula]

Choosing δ = c = 1 / 2 gives then the statement of Lemma [\ref=lem2]. [formula]

Convergence Estimates

As in the case of finite space splittings [\cite=GrOs2011], our convergence proof of the Schwarz iterative method for infinite space splittings is based on the same error representation for both greedy and random orderings. We therefore state the core estimates together in one place.

Proof. We derive a recursion for the (expected) squared error. Suppose that u(m) is determined, and that the i-th subproblem solution r(m)i: = Tie(m) is used for the update to u(m + 1) according to ([\ref=updateLin]). Thus, we can write

[formula]

where m = 1 - αm, and, in agreement with the previous subsection, [formula]. The parameter ξi,m  =  θi,m  /  m is found by solving the minimization problem

[formula]

Thus, for any ξ and any chosen i, we have

[formula]

Using the triangle inequality and ([\ref=RBound]), the norm in the last term can be bounded, independently of i, by

[formula]

For dealing with the term a(e(m),u) - ξa(e(m),w(m)i), we invoke Lemma [\ref=lem1]. In the case of greedy orderings, we take h = u∈A1 in Part a) of Lemma [\ref=lem1] and choose [formula]. We then arrive at

[formula]

Denote [formula] and [formula]. Substituting the concrete values of αm = 1 - (m + 2)- 1 and m = (m + 2)- 1 into ([\ref=Egreedy]), we obtain

[formula]

Since [formula], this implies cm < M for all m  ≥  0, and proves the result in Part a) of Theorem [\ref=theo1].

In the case of random orderings, we now use Lemma [\ref=lem1] b) for the given discrete probability distribution π with h = u∈Aπ ∞ . This yields the following estimate for the conditional expectation of [formula] with respect to given u(m) valid for any ξ  ≥  0:

[formula]

Thus, fixing [formula] and taking the expectations with respect to u(m), we get

[formula]

This gives Part b) of Theorem [\ref=theo1] if we argue as before. [formula]

Using a density argument as in [\cite=BCDD] [\cite=Te2], one can extend the estimate of Theorem [\ref=theo1], and show convergence for all u∈Va.

Proof. We start with the greedy case, take any h∈A1. Repeat the proof of Part a) of Theorem [\ref=theo1]. When invoking Lemma [\ref=lem1] a), use it with h instead of u, and set [formula]. Then

[formula]

and ([\ref=Egreedy]) can be replaced by

[formula]

If [formula], then

[formula]

Substituting the previous estimate of [formula], this yields

[formula]

where [formula]. If, alternatively, [formula], then ([\ref=Egreedy1]) holds trivially. The inequality ([\ref=Egreedy1]) is complemented by

[formula]

In the random case, we proceed similarly. For any h∈Aπ ∞ , we apply Lemma 1 b) with [formula]. This shows

[formula]

and, instead of ([\ref=Erandom]), we obtain

[formula]

where [formula]. Using the notation [formula] and the obvious inequality [formula], by the same reasoning as above, we arrive at the following replacement for ([\ref=Erandom]):

[formula]

To obtain a complementary estimate analogous to ([\ref=Egreedy2]), by definition of u(m + 1) we can write

[formula]

Then we take expectations on both sides, use again [formula], and obtain

[formula]

Up to different constants M and [formula], the recursive inequalites ([\ref=Egreedy1]-[\ref=Egreedy2]) for the sequence {εm} and ([\ref=Erandom1]-[\ref=Erandom2]) for {ε̃m} are identical. Therefore, it is enough to consider the random case. Set [formula], m  ≥  0. Then, a quick calculation shows that ([\ref=Erandom2]) turns into

[formula]

where [formula], while under the assumption bm > 0 the inequality ([\ref=Erandom1]) implies

[formula]

A similar recursive system of inequalities has been considered in [\cite=BCDD] [\cite=Te2]. Lemma [\ref=lem3] stated below implies bm  ≤  2 for all m  ≥  0 (note that [formula]). This yields

[formula]

and proves ([\ref=Hrandom]). The estimate ([\ref=Hgreedy]) is derived in complete analogy.

To show convergence for arbitrary u∈Va, for given ε > 0, choose h∈A1 by the density of A1 in Va such that [formula]. Then, with this h fixed, the second term in the right-hand side of ([\ref=Hgreedy]) will become <  ε / 3 for all large enough m as well. This proves convergence for the greedy version. An analogous argument shows [formula] as m  →    ∞   for the random case. Theorem [\ref=theo2] is fully established. [formula]

For the convenience of the reader, we conclude this section with the short proof of the boundedness of sequences {bm}m  ≥  0 satisfying the recursion ([\ref=E2]-[\ref=E1]) used in the proof of Theorem [\ref=theo2].

Suppose, a sequence {bm}m  ≥  0 satisfies the inequalities ([\ref=E2]) and ([\ref=E1]), where αm = (m + 1) / (m + 2) and B > 0. Fix a constant [formula]. Then b0  ≤  A implies bm  ≤  A for all m  ≥  1. In particular, if [formula] one can choose A = 2.

Proof. We use induction in m. Assume bm  ≤  A. For a value t = tm > 0 to be fixed below, we consider two cases. If bm  ≤  t, by ([\ref=E2]) we have

[formula]

if

[formula]

On the other hand, if t  ≤  bm  ≤  A we use ([\ref=E1]) which gives

[formula]

if

[formula]

It is easy to see that the choice t = tm: = 2α- 1 / 2m / A > 0 satisfies both ([\ref=C2]) and ([\ref=C1]) if [formula]. The latter follows from the assumption on A which implies [formula]. This finishes the induction step, and proves Lemma [\ref=lem3]. [formula]

Further Results and Discussion

Remark 1. Our results for the expected error decay for random orderings imply immediately estimates in probability. Using the Markov-Chebyshev inequality, under the assumptions of Theorem [\ref=theo1] b), we get

[formula]

for any error threshold ε > 0, or, equivalently,

[formula]

for any confidence level δ. An investigation of the variance or other higher-order moments of the squared error that could lead to improved estimates has not been undertaken yet. Numerical experiments with randomized Schwarz iterations for finite splittings [\cite=GrOs2011] [\cite=OsZh2014] suggest that the variance is reasonably small in practice.

Remark 2. If in addition to our assumptions on space splittings we assume that ([\ref=SSS]) is a stable space splitting of Va, i.e., if A2 = Va holds, then, for u∈Aq, 1 < q < 2 and the greedy version of the Schwarz iterative method specified in Part a) of Theorem [\ref=theo1], we have the error decay rate

[formula]

where [formula] is some absolute constant depending on β and the upper stability constant λmax only. This can be established by an interpolation argument along the lines of [\cite=BCDD] [\cite=Te2], where the authors consider the special case of splittings into one-dimensional subspaces [formula] of Va induced by a dictionary D  =  {ψi} of unit norm elements ψi∈Va such that its span is dense in Va. For this case, the estimate ([\ref=Aqerror]) may be replaced by a similar statement for u∈Bq, where Bq, 1 < q < 2, is obtained by real interpolation for the pair (A1,Va). In the special case, when D is a frame in Va and thus Va  =  A2, the scales Aq and Bq coincide for 1  ≤  q < 2, in general, they are different.

Remark 3. In [\cite=Te], weaker error estimates for other greedy algorithms such as PGA and WGA can be found. We believe that their proofs carry over to the setting based on space splittings without difficulties. E.g., for the PGA with αm  =  β = 1, we expect

[formula]

to hold, see [\cite=DeTe] for the PGA in the dictionary case. Whether the exponent 1 / 6 can be increased to 1 / 2 under the assumption that ([\ref=SSS]) is a stable space splitting is an open problem, even when the space splitting comes from a frame. Slightly better exponents are possible for PGA and WGA, see [\cite=Te] [\cite=Te2011].

Remark 4. Theorems [\ref=theo1] and [\ref=theo2] provide convergence guarantees under theoretical assumptions that look still questionable from a practical point of view: The question of rounding errors is not addressed, for the greedy version the condition ([\ref=greedySSS]) needs to be checked for an infinite index set [formula], while in the random Schwarz iterative method drawing the next index i = im according to a (rather general) discrete probability distribution π defined on [formula] seems inconvenient as well. For greedy algorithms based on dictionaries, there are partial results in this direction [\cite=Te2005] [\cite=BCDD] [\cite=DeTe2014] which can be adapted to the case of space splittings considered here.

We concentrate on the randomized version. When combined with Lemma [\ref=lem2], the estimation techniques leading to Theorem [\ref=theo2] give the following result which, in particular, allows us to work with finitely supported probability distributions π(m) that converge to a desired π > 0 sufficiently fast, without sacrificing convergence speed.

Proof. We repeat the same steps that lead to ([\ref=Erandom1]), with the following changes: The conditional expectation [formula] is now computed with respect to π(m). For estimating the difference [formula], we use the h(m)∈Aπ(m) ∞  whose existence is guaranteed by Lemma [\ref=lem2], and set [formula]. This gives

[formula]

where we have substituted ([\ref=Hm]) and ([\ref=PiError]). Using as before the notation [formula] and the obvious inequality [formula], we arrive at the following replacement for ([\ref=Erandom1]):

[formula]

where as before [formula]. The constants are [formula], and

[formula]

see ([\ref=Hm]). This gives a recursion for

[formula]

similar to ([\ref=E1]) but with a new term induced by the additional term 2C0(m + 1)- 3 / 2 in ([\ref=Erandom3]):

[formula]

This relation is again complemented by the inequality ([\ref=E2]), this time with the constant [formula]. Since repeating the proof of Lemma [\ref=lem3] with the additional term in the right-hand side of ([\ref=Erandom4]) does not represent any difficulty, we leave it to the reader to show that bm  ≤  A, m  ≥  0, holds for some new constant A depending on B and [formula]. This shows ([\ref=NewRandom]) with C = AC1 / 21, and finishes our sketch of the proof of Proposition [\ref=pro3]. [formula]

Remark 5. In the generality considered here, the obtained convergence rates for the error [formula] are not very impressive but unfortunately cannot be improved much. For greedy orderings, this issue has been addressed in [\cite=DeTe] [\cite=Te] [\cite=Te1]. We add some comments for random orderings. Consider the very special situation of a one-dimensional subspace splitting induced by a complete orthonormal system D in V with a(  ·  ,  ·  ) = (  ·  ,  ·  ), and the problem of incremental approximation of a given u∈V by linear combinations of elements from D. I.e., if

[formula]

is the unique orthogonal decomposition of u with respect to D, then we have RiTiu = ciψi. Fix the discrete probability distribution π > 0, and consider the associated randomized Schwarz iterative method with updates of the form ([\ref=updateLin]). It is easy to find that, due to the orthogonality of the splitting, the best expected convergence rate is achieved for αm = 1. In that case, we have

[formula]

with probability [formula], where {ik}k  ≥  0 is the random index sequence, and I(m) is the set of the first m such indices (I(m) may have cardinality ≤  m, repetitions are possible). We leave it to the reader to verify the identity

[formula]

In the particular case considered, the formula confirms the statement of Theorem [\ref=theo2] b): Since πi > 0 for all [formula], we have [formula] as m  →    ∞  , i.e., the expected error converges to 0 for any u∈V and any probability distribution π > 0.

On the other hand, when inspecting the statement of Theorem [\ref=theo1] b) for our case, we see that u∈Aπ ∞  is equivalent to the inequality

[formula]

Thus, for u∈Aπ∞ we have

[formula]

which is sharp in the sense that equality holds (simultaneously for all m  ≥  1) if we set ci = Cπi. Since φ(t) = t2(1 - t)m  ≤  c / (m + 1)2 for t∈[0,1] for some absolute constant c and all m  ≥  1, we get

[formula]

Indeed, the first sum can be estimated according to

[formula]

and the second is a finite sum with ≤  m terms. This result is in line with the bound of Theorem [\ref=theo1].

No substantial improvement of the decay rate O((m + 1)- 1) can be expected for general probability distributions: For each fixed m, taking π sufficiently close to the uniform distribution on [formula] provides a lower bound of c' / (m + 1) for the right-hand side in ([\ref=PCONS]) while choosing π > 0 according to

[formula]

shows that, for some u∈Aπ ∞ , lower bounds of the form [formula] may hold for all m  ≥  0 simultaneously, with any α > 1. However, for sequences π of the form

[formula]

with s  >  0, slight improvements are possible. Note that for specific complete orthonormal systems (such as the trigonometric system in [formula]) the classes Aπ ∞  for such π have natural interpretations as L2-Besov-Lipschitz spaces (in the case of our example this would be Bs2,  ∞, and s corresponds to a smoothness parameter). Better rates can also be concluded if we assume that u belongs to a smaller class of this type with parameter s' > s.

Although the validity of these observations heavily relies on the assumed orthogonality of the splitting, we believe that especially the randomized versions should be investigated further. In particular, improved convergence rates for special classes of space splittings (e.g., induced by multilevel frames and sparse grid spaces) are desirable, and the potential of randomization techniques for the development of new adaptive algorithms needs to be further evaluated.

Acknowledgement

M. Griebel was partially supported by the project EXAHD of the DFG priority program 1648 Software for Exascale Computing" (SPPEXA) and by the Sonderforschungsbereich 1060 The Mathematics of Emergent Effects funded by the Deutsche Forschungsgemeinschaft.