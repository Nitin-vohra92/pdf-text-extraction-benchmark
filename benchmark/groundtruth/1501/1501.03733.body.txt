Total cost of operating an information engine

Introduction

In 1867, J.C. Maxwell created a thought experiment to demonstrate a possible violation of the second law of thermodynamics: A thermally isolated container with a gas is divided into two parts and a fictitious demon opens or closes a door between the two parts depending on the velocity of approaching particles, creating an increase of the temperature in one of the compartments [\cite=HistoricalReview].

Smoluchowski was the first to provide an explanation why Maxwell's demon does not work. To this end, he modeled the demon mechanically by a trapdoor combined with a gentle spring. The trapdoor acts as a valve in such a way that fast particles coming from one side can open the door while slow ones cannot, leading to a pressure difference. However, taking the full dynamics of the apparatus into account, Smoluchowski could demonstrate that the energy of the spring system itself equilibrates at such a high energy that it opens and closes essentially randomly, leading to the same pressure difference as if the trapdoor was always open.

In 1928, L. Szilárd refined the concept of Maxwell's demon, suggesting what is known today as the Szilárd engine [\cite=Szilard]. Starting point is a box that contains only one particle (see Fig. [\ref=fig:szilard]). If a wall is inserted in the middle, the particle will be in one of the two parts. Expanding the volume isothermally to its original size by moving the shutter into the empty half of the box one can extract the work W = kBT ln 2. However, this requires to know in which compartment the particle actually is, demonstrating that the possession of information can be converted into physical work. Thus, in order to keep a Szilárd engine running, a closed loop of measurement and feedback is needed. Very recently, such a feedback scheme could be realized experimentally for the first time [\cite=SzilardExperiment].

Since the feedback mechanism in the Szilárd engine was perceived as information processing at zero cost, it seemed to produce usable work from nothing, violating the second law. However, as shown by Landauer [\cite=Landauer] in 1961, any irreversible logical operation requires to apply a well-defined minimum of work. In case of the Szilárd engine, the demon has to store the information about the particle position in a single bit. According to Landauer's principle, resetting this bit requires to convert a work of kBT ln 2 into heat. As shown by Bennett [\cite=Bennett], who realized measurement and feedback as reversible processes, this extra work for resetting restores the second law.

Recently Maxwell's demon attracted renewed attention as it was shown that a system in a feedback loop obeys an integral fluctuation theorem (IFT) of the form

[formula]

implying β〈Wex〉  ≤  〈ΔI〉, where Wex is the extracted work during feedback, β is the inverse temperature, and ΔI is the gained mutual information between system and demon during the measurement [\cite=SagawaUeda1] [\cite=SagawaUeda2] [\cite=SagawaUeda3]. This fluctuation theorem implies that in each cycle of the engine the extracted work is limited by the gained mutual information, - a highly plausible result that nicely demonstrates the equivalence of thermodynamic work and information.

Subsequently this remarkable result was made more specific in various ways. For example, it was shown that one can construct feedback schemes that satisfy the inequality sharply [\cite=SagawaUeda4] [\cite=Horowitz]. Moreover, the IFT was generalized to schemes with finite-time relaxation [\cite=Abreu] [\cite=Bauer] and continuous feedback schemes [\cite=Sandber]. Very recently, the generalized IFT could also be confirmed experimentally [\cite=Koski].

The arising problem with these generalized Jarzynski equalities is that the tightness of the bound seems to depend on the specific feedback scheme such as discrete and continuous feedback as well as memory tape models. This led Barato et al. to look for a unifying master IFT [\cite=Barato1] [\cite=Barato2] [\cite=Barato3] [\cite=Barato4]. To achieve this, they duplicated the configuration space, modeling bit flips of the memory by transitions between the two replicas. Doing so they followed Smoluchowski's original idea of modeling the whole feedback loop as a physical device, defined as a stochastic Markov process.

In this paper, we follow these lines of thought, being interested in the total cost of operating an information engine as described above. Instead of duplicating the configuration space, we devise physically realizable stochastic processes of the joint system not only for relaxation, but also for the measurement process. We show that the generalized IFT for the relaxation of system is always accompanied by an opposite IFT during the measurement carried out by the demon, restoring the second law for the joint system. This implies that a certain minimal amount of work has to be done on the memory, in accordance with Landauer's principle. The calculation of the total cost enables us to derive the efficiency of the information engine as a function of its cycle time. We also discuss the optimal cycle time and corresponding efficiency, at which the extracted power is maximized.

Definition of a minimal model

In what follows we consider a system with two different energy levels separated by ΔE. The system is coupled to a heat bath with inverse temperature β. The controller (demon) is implemented as a 1-bit memory. We devise a discrete feedback scheme evolving in three steps (see Fig. [\ref=fig:engine]). First the system relaxes thermally without external influence. Then the actual energy level, denoted by 0 and 1, is copied to the memory of the controller. Finally, if the memory bit is 1, the controller induces a flip of the system 1  →  0, extracting the energy ΔE, otherwise it does nothing.

Following Barato and Seifert [\cite=Barato4], we are aiming to model these steps by stochastic Markov processes, including both the system and the memory. This defines a four-dimensional configuration space in which each step can be represented by a simple 4  ×  4 matrix. In the following, we discuss each of the three steps in detail:

1. Relaxation

During relaxation, the system flips randomly according to the rules

[formula]

For convenience, we express these rates in terms of

[formula]

with [formula]. After infinite time, the system eventually reaches an equilibrium state with the stationary probability distribution P1stat = 1 - P0stat = q. This means that the energy difference between the two levels is given by

[formula]

Let us now describe the relaxation process in the composite configuration space of system and memory. Throughout this paper, we will use a canonical configuration basis ordered by

[formula]

Since the memory is inactive during relaxation, the time evolution operator LR for relaxation represented in this basis reads

[formula]

After the relaxation time tR, the corresponding transition matrix is given by

[formula]

In the infinite-time relaxation limit (tR  →    ∞  ), this transition matrix reduces to

[formula]

2. Measurement

A perfect measurement would faithfully copy the system state to the memory, i.e., if m = 0,1 denotes the previous memory state and s = 0,1 the actual system state, it would simply copy (s,m)  ↦  (s,s). However, it is well-known that such a perfect measurement is irreversible, leading to a diverging entropy production [\cite=Barato4] [\cite=Esposito]. Therefore, one usually considers imperfect measurements

[formula]

with a small error probability 0 < ε < 1 / 2. Using the basis ([\ref=basis]) this corresponds to the transition matrix

[formula]

Remarkably, this imperfect measurement process can be implemented by a stochastic Markov process as well, since T2M  =  TM. The corresponding time evolution operator reads

[formula]

with a rate k' and it is easy to show that the transition matrix TM in Eq. ([\ref=TMI]) is retrieved in the limit of infinite measurement time:

[formula]

Thus, we succeeded to implement the second step as a stochastic Markov process as well.

If the memory is considered as being in contact with some heat bath of inverse temperature β during the stochastic measurement process, the time evolution defined above implies that the incorrectly measured state (s,1 - s) has a higher energy than the correctly measured state (s,s), and that the corresponding energy difference between the two composite states is given by

[formula]

3. Feedback

The purpose of the feedback is to use the information stored in the memory in order to extract energy from the system. If the preceding measurement was faithful, this would mean to perform the transitions

[formula]

These transitions alone would be again irreversible, causing an infinite entropy production. However, if we add symmetric transitions in the (unlikely) case of erroneous measurements, namely,

[formula]

we obtain the feedback transition matrix

[formula]

Thus the feedback process is carried out in such a way that the system state is flipped (s  ↦  1 - s) for m = 1 while it remains unchanged (s  ↦  s) for m = 0. It is assumed that the feedback transition occurs instantaneously so that the total time τ of a complete cycle TR  →  TM  →  TF is given by τ  =  tR  +  tM.

Since [formula], the feedback is fully reversible, hence it does not produce entropy in the environment. Moreover, it is easy to see that it simply exchanges the second and the fourth component of a vector, and therefore it does not change the joint entropy of system and memory. However, as will be shown below, it generally changes the entropy of the subsystems.

Due to its reversible nature, the feedback as defined above cannot be implemented as a stochastic Markov process. However, we would like to point out that it is even possible to implement the feedback physically so that the entire chain of steps is represented cleanly as a sequence of stochastic processes. This can be done by replacing two subsequent cycles TR  →  TM  →  TF  →  TR  →  TM  →  TF  →  TR equivalently by TR  →  TM  →  TFTRTF  →  TFTMTF  →  TR. Since [formula] and [formula] satisfy the stochasticity condition ([formula], [formula]), the whole sequence of steps can be implemented by stochastic processes, providing a safe ground for the calculation of entropy production, mutual information, work, and heat. Having verified that this description is fully equivalent, we nevertheless keep the explicit feedback for simplicity in the original form.

Since the rates k and k' simply rescale tR and tM, we will set

[formula]

throughout the paper. Thus, apart from tR and tM, the model is controlled by only two parameters, namely, the relaxation parameter q and the error probability ε.

Stationary state

If the information engine runs repeatedly through many cycles, the probability distributions between the three steps will become stationary. The corresponding stationary probability distributions |P0〉, |P1〉, and |P2〉 are represented as four-component vectors

[formula]

are determined by the equations

[formula]

with |P1〉  =  TR|P0〉 and |P2〉  =  TM|P1〉 (see Fig. [\ref=fig:cycle]).

The reduced stationary probability vectors of the system (s) and the memory (m) are given, respectively, as

[formula]

Clearly, the measurement does not modify the system state, meaning that |P(s)1〉  =  |P(s)2〉. Similarly, both the relaxation and feedback do not affect the memory, hence |P(m)0〉  =  |P(m)1〉 and |P(m)2〉  =  |P(m)0〉. As a result, in a stationary situation, the information acquired during the measurement is statistically the same in each cycle, implying that |P(m)1〉  =  |P(m)2〉.

As a simple example, first consider the case of infinite-time relaxation and measurement (tR,tM  →    ∞  ), where the matrices TR,TM, and TF are given by Eqs. ([\ref=TRI]), ([\ref=TMI]), and ([\ref=TFI]). In this case the normalized stationary probability vectors turn out to be given by

[formula]

The corresponding reduced vectors for the system and the memory read

[formula]

Entropy and entropy production

1. Shannon entropy

Given the stationary probability distributions |Pk〉, it is straightforward to compute the entropies of the system (s), the memory (m), and the joint system (sm) between the steps in a stationary cycle, using the definition of the Shannon entropy

[formula]

where the sum runs over the vector components. Because of the aforementioned coincidence of various probability vectors we have H(s)1 = H(s)2 and H(m)1 = H(m)2 = H(m)3. Furthermore, the reversibility of the feedback process guarantees that H(sm)2 = H(sm)0.

For tR,tM  →    ∞  , the expressions for the Shannon entropies reduce to

[formula]

where we used the notation h(p): =  - p ln p.

During the relaxation process, where the system tries to restore the equilibrium distribution from the overpopulated ground state after energy extraction, the system entropy H(s) is expected to increase, provided that the error probability ε is sufficiently small (ε  <  q). The same applies to the composite entropy H(sm).

To summarize, the entropy changes during relaxation (R), measurement (M), and feedback (F) are given by

[formula]

2. Mutual information

With these expressions, it is straightforward to compute the mutual information

[formula]

which is a measure for the correlation between system and memory. This correlation is expected to build up during the measurement and then to decrease during feedback and relaxation, implying the inequalities

[formula]

It is interesting to note that the change of the composite entropy is purely given by amount of mutual information acquired during the measurement, i.e.

[formula]

For tR,tM  →    ∞  , we have

[formula]

Note that the result I1  =  0 is true only if the relaxation time is infinite, which obviously destroys all correlations between system and memory.

3. Entropy production

Let us now turn to the entropy production. According to Schnakenberg [\cite=Schnakenberg] [\cite=Gaspard] [\cite=Seifert05], whenever the system or the memory jumps spontaneously from the configuration c to another configuration c', the amount of entropy

[formula]

is generated in the environment. Here, wc  →  c'(t) denotes the transition rate at time t. Therefore, the mean entropy production rate is given by

[formula]

In an arbitrary nonequilibrium system, one would have to solve the master equation, plug the solution into the equation above, and integrate the resulting expression over a certain window of time. However, in the present case this is not necessary since the model is so simple that the rates happen to obey detailed balance, defined as Pcstatwc  →  c' = Pc'statwc'  →  c in the stationary equilibrium state. In this case, it is therefore straightforward to rewrite the equation given above as

[formula]

allowing us to compute the average entropy production in each step directly without integration by means of

[formula]

Here Pcinit and Pcfinal denote the initial and the final probabilities for a finite time span while Pcstat is the stationary probability distribution that would emerge after infinite long time. For example, during relaxation, Pcstat can be obtained by taking tR  →    ∞   in the expression for Pc1 in Eq. ([\ref=P1stat]). Similarly, during measurement, Pcstat  =   lim tM  →    ∞Pc2 with Pc2 given in Eq. ([\ref=P2stat]).

With the above formula, we obtain the following expressions for the entropy production in each process:

[formula]

Note that these expressions hold for any finite tR and tM. The last result is obvious since the feedback with T2F  =   is a reversible operation.

For tR,tM  →    ∞  , by inserting Eqs. ([\ref=P0stat])-([\ref=P2stat]) we get explicit expressions

[formula]

which are plotted for various error probabilities in Fig. [\ref=fig:entropyproduction]. As one can see, for ε < q < 1 / 2 the entropy production during relaxation ΔHenvR is negative, meaning that the engine imports entropy (heat) from the environment rather than producing it. Obviously, this is the regime of interest where we would like to operate our information engine. However, as can be seen, the negative entropy production during relaxation is always overcompensated by a positive one during the measurement, which is consistent with the second law of thermodynamics. Notice that the entropy production during measurement is always positive since ε < 1 / 2.

Work extraction and supply

By virtue of Clausius' law dQ  =  TdH, the produced entropy can be translated directly into an amount of heat. In most studies, it is usually assumed that the temperatures of the system and the memory are identical. However, for the sake of generality, let us allow the temperatures to be different, assigning βR = 1 / TR during relaxation and βM = 1 / TM during measurement, as sketched in Fig. [\ref=fig:totalscheme]. Thus the respective heat contributions averaged over all possible stochastic trajectories are given by

[formula]

Here we use the sign convention that heat flowing away from the engine into the environment has a positive sign, i.e. we expect 〈QR〉 to be negative and 〈QM〉 to be positive.

In order to maintain stationarity of the system after one engine cycle, the average work 〈Wex〉 extracted during feedback should exactly balance the average heat 〈QR〉 during relaxation, i.e.

[formula]

Consequently, the measurement process does not change the energy of the system. This requires an additional influx of energy in the form of extra work 〈Wsup〉 into the memory which is necessary to compensate the loss of heat 〈QM〉 flowing away to the environment during the measurement process:

[formula]

The average net work performed by the machine, defined as the difference of extracted and supplied work, is therefore given by

[formula]

Note that the net work can change its sign depending on the choice of the parameters q, ε, tR, and tM.

Let us first compute the extracted work. According to Sect. II.3 〈Wex〉 is given by (P112 - P012)ΔE, where [formula], see ([\ref=DeltaE]). Using |P(s)1〉  =  |P(s)2〉 (no system change during measurement) together with the feedback identities P012 = P110 and P102 = P100 (flip s only when m = 1), it is easy to show explicitly that

[formula]

The additional work 〈Wsup〉 supplied during the measurement process can be interpreted as the energy needed to operate the measurement device. Technically this contribution comes from the fact that the energy levels of the joint system are different during relaxation and measurement so that extra energy is needed to move them around. For example, this could be done by applying an external potential in order to make the energy level of the erroneous composite state (s,1 - s) higher than that of the correctly measured state (s,s) by the amount of [formula]. When the external potential is turned on just before the measurement, the average energy of the composite of system and memory increases by 〈Ein〉 and similarly it looses the energy 〈Eout〉 when the potential is turned off at the end of the measurement:

[formula]

Comparing the difference 〈Wsup〉  =  〈Ein〉  -  〈Eout〉 with Eq. ([\ref=Henv]) one can see immediately that

[formula]

In the limit tR,tM  →    ∞  , the average work contributions read

[formula]

Thermodynamic second laws and fluctuation theorems

The total entropy production of the whole setup during relaxation (R) and measurement (M) is given by

[formula]

According to Eq. ([\ref=DHI]), the entropy differences in the joint system are given solely in terms of the mutual information difference

[formula]

while the entropy differences in the environment are given in terms of the transferred heat by ΔHenvR,M  =  βR,M〈QR,M〉. Using Eqs. ([\ref=Wex1]) and ([\ref=Wex2]) we arrive at

[formula]

For the total system including the environment the second law of thermodynamics should be satisfied for each process, i.e.,

[formula]

or equivalently

[formula]

Most existing studies are only interested in the first inequality during relaxation, while the other one during measurement is ignored. The purpose of this work is to point out that there is a second inequality for the measurement process as well, and that the two inequalities are complementary with respect to each other. More specifically, if βR  =  βM, the extracted work is bounded from above by the mutual information while the work required to operate the memory is bounded from below by the same threshold. This means that the setup cannot be used to gain work out of nothing, 〈Wnet〉  ≤  0, as expected by the second law. However, if the two reservoir temperatures were different (βR  <  βM), the extracted work could be larger than the supplied one, in which case the system operates like a conventional heat engine (see Fig. [\ref=fig:IFT_infinite]).

It is almost trivial to construct the integral fluctuation theorems (IFT's) through the standard approach of stochastic thermodynamics [\cite=Seifert05] by considering the heat along all possible trajectories in the composite configurational state space. With an appropriate definition of the Shannon entropy for a given trajectory [\cite=Seifert05], one can easily get the fluctuation theorems for the total entropy production for each process as

[formula]

The second laws in Eq. ([\ref=Thermod_lawsH]) are simple consequences of the IFT's with ΔHtotR,M  =  〈ΔHtotR,M(traj.)〉. It is rather tricky to find the IFT's in terms of work and mutual information, because it requires an equilibrium state as an initial condition. This is the case only when tR becomes infinite so that the system is in equilibrium at the start of the measurement as well as at the beginning of the feedback. However, note that the bounds for works in Eq. ([\ref=Thermod_laws]) are valid even if tR is finite.

Finite-Time relaxation and measurement

In practice, an engine is only useful if the cycle time τ is finite. Thus, it is obviously of interest to derive all physical quantities as function of the cycle time. This allows one to find the optimum for maximal power generation, as will be discussed in the next section.

It is straightforward to obtain the transition matrices for relaxation and measurement for finite time spans tR and tM:

[formula]

[formula]

where

[formula]

Note that the transition probability from (s,1 - s) to (s,s) during measurement, [formula], is smaller than [formula], which means that the measurement for finite tM is less accurate than in the limit of infinite time.

Solving Eq. ([\ref=SS]), one can find explicit but complicated expressions for all stationary distributions such as |P0〉, |P1〉, and |P2〉 for finite tR and tM, which are not shown here explicitly. The heat dissipation during the finite-time relaxation and measurement can be obtained from Eq. ([\ref=Henv]) while the extracted work and the supplied work are given by Eqs. ([\ref=WW1]) and ([\ref=WW2]).

Using the relation |P2〉  =  TM|P1〉 we find that

[formula]

Here [formula] is explicitly given by

[formula]

with α  =  R  +  (  -  q)(  -  ε). In a similar manner, we obtain 〈Wex〉 as the function of E:

[formula]

In the limit of tR,tM  →    ∞   (R,M  →  0), we consistently recover Eq. ([\ref=WW_inf]).

Note that the finite-time works in ([\ref=eq:finite_Ws]) and ([\ref=eq:finite_Wex]) decrease monotonously with R and M (see Fig. [\ref=fig:IFT_finite]). Moreover, since the correlation between system and memory builds up continuously during the measurement process, it is obvious that ΔIM decreases with M, remaining positive by definition. The positivity of ΔIM guarantees that 〈Wsup〉 is also positive. On the other hand, 〈Wex〉 can be negative for short-time measurement and relaxation, as its upper bound ΔIM approaches zero for tM  →  0.

The monotonous dependence shown in Fig. [\ref=fig:IFT_finite] suggests that 〈Wex〉 becomes maximal for maximal measurement accuracy (tM  →    ∞  ) and full relaxation (tR  →    ∞  ) in order to redistribute and pump the overpopulated ground state s = 0 back to the energetically excited state s = 1. Therefore, both limit tM,tR  →    ∞   have to be carried out simultaneously. To establish this combined limit conveniently, let us from now on set

[formula]

meaning that R  =  M  =  e-  τ / 2. With this convention we expect 〈Wex〉 to be maximal in the limit of infinite cycle time (τ  →    ∞  ). Moreover, as τ decreases, we expect 〈Wex〉 to decrease and eventually to become negative.

If 〈Wex〉  >  〈Wsup〉, the system operates like a conventional heat engine. For infinite τ the net work 〈Wnet〉 is maximal, but the power (net work per unit time) vanishes. For finite but sufficiently large τ and properly chosen parameters the system still produces a positive net work, hence the power is positive. However, as can be seen in Fig. [\ref=fig:IFT_finite], the curves for 〈Wex〉 and 〈Wsup〉 cross each other at some finite cycle time τ  =  τs. At this point we do no longer obtain any net work from the engine, hence the power vanishes again. Consequently, there will be a particular cycle time in between, at which the power of the engine is maximal. In the next section, we will discuss this aspect in more detail.

Efficiency

Let us now assume that the information engine operates in a regime where the net work is positive. In this case the whole setup can be interpreted as a conventional heat engine, as sketched in Fig. [\ref=fig:totalscheme2]. As βR  <  βM, the upper reservoir for the relaxation process plays a role of a high-temperature heat source, while the lower reservoir in contact with the memory device acts as a heat sink. The efficiency of this heat engine in a single cycle is defined in the usual way as

[formula]

Using Eqs. ([\ref=eq:finite_Ws]) and ([\ref=eq:finite_Wex]) the efficiency can be rewritten as

[formula]

where

[formula]

Note that the relaxation and measurement processes are not quasi-static so that even in the limit τ  →    ∞   the engine never reaches the Carnot efficiency [formula]. Instead, we find that the efficiency is limited by a different upper bound ηmax which can be computed as follows. According to the monotonicity arguments discussed in the preceding section, η(τ) is expected to become maximal in the limit τ  →    ∞  . This suggests that

[formula]

where

[formula]

Fig. [\ref=fig:eff_infinite] shows 1 / λ∞ as function of q for several values of ε. It is obvious that 1 / λ∞ is positive for ε  ≤  q  ≤  1 / 2 with a unimodal shape due to the similar behavior of 〈Wex〉 demonstrated in Fig. [\ref=fig:IFT_infinite]. As ε  →  0, 1 / λ∞ approaches zero except for q  ≈  0. In the limit of both q  →  0 and ε  →  0, λ∞ approaches a constant bounded from below by λ∞  ≥  2. Consequently, in order to obtain a positive work gain, the difference of temperatures should be at least βM  /  βR  ≥  2 for the infinite-time process. In short, we find that the efficiency of the information engine is bounded by

[formula]

In conventional heat engines operating with a finite cycle time, thermodynamic processes are no longer quasi-static, leading to an efficiency below the Carnot bound ηc. In our case, we also find that η(τ) becomes maximal in the limit τ  →    ∞  . However, in contrast to the Carnot limit, where the entropy production vanishes, the entropy production per cycle in our model ΔHtotR  +  ΔHtotM becomes also maximal at τ  =    ∞  . This is one of the crucial features of our information engine which is totally distinct from conventional heat engines. Nevertheless, the entropy production rate (per unit time) decreases with increasing τ and finally vanishes at τ  =    ∞  . Hence, one may also say that the maximum efficiency is found at the minimum entropy production rate.

Similarly, the average power gain, [formula] in fact vanishes for τ  →    ∞   because 〈Wnet〉 remains finite in this limit. For a realistic engine, we usually want to optimize the power gain, trading off the efficiency against the cycle time. As expected, 〈P〉 is maximized at some finite time, τop between τs and ∞   as shown in Fig. [\ref=fig:eff_finite] (a).

The efficiency of heat engines at maximal power has been studied previously in Refs. [\cite=eff_CA] [\cite=eff_Broeck] [\cite=eff_Esposito]. Especially, for the Curzon-Ahlborn (CA) endoreversible model [\cite=eff_CA], it is well-known that the efficiency ηCA at the optimal power is given by [formula]. In Fig. [\ref=fig:eff_maxp], we plot the efficiency at optimal power ηop  =  η(τ  =  τop) according to Eq. ([\ref=eq:eff2]) as a function of ηmax instead of ηc. It turns out that the functional behavior of ηop is completely different from ηCA.

In more general situations, it has been found that the efficiency at the maximum power obeys a universal form, ηop  =  ηc / 2  +  O(η2c) for small ηc, when the engine and heat baths are strongly coupled [\cite=eff_Broeck] [\cite=eff_Esposito]. Our information engine exhibits a completely distinct behavior even for small ηmax. As seen in Fig. [\ref=fig:eff_maxp], ηop is more or less the same as ηmax in this regime.

In order to investigate this unusual behavior in more detail, we now examine ηop analytically for small ηmax. As the stall time τs and thus τop (>  τs) becomes large, a small R expansion may be valid for small ηmax. Expanding η(τ) in Eq. ([\ref=eq:eff2]) up to the linear order of R = e-  τ / 2, we get

[formula]

where we have used E  ≈  E0  +  E1R with

[formula]

We also checked that the linear coefficient inside the parentheses in Eq. ([\ref=eq:expand1]) is always numerically positive in the interval ε < q < 1 / 2.

As the stall time τs is defined by η(τs) = 0, Eq. ([\ref=eq:expand1]) immediately gives us

[formula]

where the coefficient As is given by

[formula]

It turns out that the scaling behavior in Eq. ([\ref=eq:tau_s]) extends quite well to finite ηmax, as can be seen in Fig. [\ref=fig:eff_finite] (b).

The optimal time τop should satisfy

[formula]

which is rewritten as

[formula]

where Rop  =  e-  τop / 2. As τop  >  τs, Rop should be also small. This allows us to expand the above equation for small Rop, yielding

[formula]

where E0 and E1 are the same as in Eq. ([\ref=eq:E01]), and λop is given by

[formula]

Plugging the above expression for λop into Eq. ([\ref=eq:dpdt_R]), we get

[formula]

which yields

[formula]

where

[formula]

Finally, inserting Eq. ([\ref=eq:tau_op]) into Eq. ([\ref=eq:expand1]), it is straightforward to find an approximate expression for ηop:

[formula]

Therefore, in the limit of ηmax  →  0, one can see ηop  ≈  ηmax, not [formula], and that the next correction is logarithmic and therefore quite slow. This calculation confirms that the linear irreversible thermodynamics slightly out of equilibrium in [\cite=eff_Broeck] should not be applicable in our case, simply because our processes are far from equilibrium. Indeed, in our case, the entropy production is maximal in the limit of ηmax  →  0. In future studies, the validity of ηop  ≈  ηmax should be addressed in the context of universality for general information engines showing the maximum efficiency at the same point where the entropy production is maximal.

Conclusions

In this work, we have studied a simple example of an information engine which can be realized physically in terms of stochastic Markov processes. In agreement with previous studies, we find that the information feedback allows one to extract work in a situation where this would be thermodynamically impossible without feedback. Moreover, we confirm that total entropy production during relaxation obeys a fluctuation theorem, implying that the extracted work is bounded from above by the mutual information gain between memory and system.

Providing a physical realization of the memory and the feedback loop, we have shown that the measurement process exhibits similar properties which are opposite in character. In particular, the entropy production during measurement is found to obey a fluctuation theorem as well. This implies that the measurement process itself costs energy, and that this additional energy supply is bounded from below by the same mutual information gain. Putting these pieces together, it is no surprise that the total setup consisting of system and memory satisfies the conventional second law of thermodynamics. Thus, we have shown that the thermodynamic second law, which is required to hold for the entire system during any finite process, leads to a duality in the properties of system and memory in this kind of information engines.

For simplicity, we have presented most of our analytic results in the limit of infinite measurement- and relaxation time. However, the extension to finite times is straightforward. At the end of the paper, we have explicitly described some numerical results for finite-time measurement and relaxation. As in conventional heat engines the efficiency of the information engine is maximized when the cycle time becomes infinite. However, in contrast to conventional heat engines, the entropy production is also maximal in this limit. On the other hand, we have demonstrated that the power gain acquires its maximum at a finite cycle time. We have also discussed the relation between the maximal efficiency and the efficiency at the operating point of maximal power.

The striking differences between our model and conventional reversible heat engines can be traced back to the fact that our setup operates under non-equilibrium conditions. It would be interesting to investigate to what extent our observations can be explained in a universal framework.

Acknowledgments

This research was supported by the NRF Grant No.2013R1A6A3A03028463 (J.U.), 2013R1A1A2011079 (C.K.), and 2013R1A1A2A10009722 (H.P.). We thank the Galileo Galilei Institute for Theoretical Physics for the hospitality and the INFN for partial support during the completion of this work.