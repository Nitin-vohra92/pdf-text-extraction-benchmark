=10000 = 10000

The Anatomy of Relevance

Introduction

For decades the main evaluation paradigm for search engines was the Cranfield methodology [\cite=cleverdon1966aslib]. In a typical setting of a TREC conference, the documents are evaluated by human raters who assign relevance labels based on their judgement about the relevance of the document to the user's topic of interest, expressed as a query. A graded relevance scale is typically used with topical relevance labels ranging from 0 to 4 or from irrelevant to highly relevant.

These relevance labels can be obtained either from trained experts or using a crowdsourcing approach. Either way, cases of disagreement have to be addressed, and those are usually treated as raters' mistakes, but may also arise from different interpretations of the user intent or the notion of relevance. In a traditional evaluation approach a single relevance label is chosen for each document-topic pair. These labels are then aggregated to SERP-level quality measures such as DCG [\cite=jarvelin2002cumulated] or ERR [\cite=chapelle2009expected]. By using additional inputs from raters, we can

Related Work

The idea to separate perceived and topical relevance was suggested by [\cite=chapelle2009dynamic] while designing the DBN click model. Unlike earlier click models, it suggests that the likelihood of a user clicking a document depends not on the topical relevance of the document, but rather on its perceived relevance, since the user can only judge based on the result snippet. This idea was later picked up by [\cite=turpin2009including] who showed that while topical and perceived relevance are correlated, there is a noticeable discrepancy between them. They performed a simulated experiment by modeling the user click probability and showed that taking it into account would lead to substantially different ordering of the systems participating in a TREC Web Track.

The idea to separate out snippet relevance appears after the introduction of good abandonment [\cite=li2009good]: cases when users abandon a search result page without clicking any results and yet they are satisfied. This may be due to the SERP being rich with instant answers [\cite=chilton2011addressing], e.g., a weather widget or a dictionary box, or due to the fact that a query has a precise informational need, that can easily be answered in a result snippet [\cite=chuklin2012good]. In fact, as was shown by [\cite=stamou2010interpreting] a big portion of abandoned searches was due to a pre-determined behaviors: users came to a search engine with a prior intention to find an answer directly on a SERP. This is especially true when considering mobile search where the internet connection can be slow or the user interface is less convenient to use. We complement these works by arguing that good and relevant snippet does not necessarily lead to a complete good abandonment, but rather represents an aspect of utility gained by the user that is currently ignored.

Application to Evaluation

As was suggested by [\cite=carterette2011system], many evaluation metrics, including DCG and ERR may be viewed as based on a click model. This was further refined by [\cite=chuklin2013click] where a recipe of converting any click model into a metric was presented:

[formula]

where Rk is the (topical) relevance of the k-th document in the ranking, and P(Ck  =  1) is the probability that the user will click on that document. Depending on the user model, the click probability may depend on attractiveness parameters. This is where we can use perceived relevance labels Ak (attractiveness). For example, for a metric based on the DCM model [\cite=guo2009efficient] we have:

[formula]

where a(A) is a list of parameters, one for each possible value of perceived relevance label A; si is another list of parameters, one for each value of the document position i.

Further, if we want to use snippet relevance labels Sk, we introduce a metric of the utility gained from the SERP itself similar to [\eqref=eq:umetric]:

[formula]

where P(Ek  =  1) is the probability that the user examines the k-th document. Again, for DCM that would lead us to:

[formula]

To summarize, we showed that by collecting perceived, topical and snippet relevance we can refine system quality measures (eq. [\eqref=eq:umetric], [\eqref=eq:umetricS]). To estimate the effect of this refinement one can compute correlations with online click metrics similar to [\cite=chuklin2013click] or with side-by-side comparison judgements collected using independent set of raters.

Gathering Judgements

Now that we have argued that perceived, topical and snippet relevance are potentially valuable dimensions of assessing system quality, how do we gather the required judgements? Firstly, we believe, that the topical relevance definition used by TREC raters is time-tested and hence can be used without modification. Secondly, snippet relevance can be treated as document topical relevance with document replaced by its snippet. We also need additional messaging for the raters explaining to them why the "documents" are so short to avoid undervalued scores. In order to prevent the raters from confusing this task with perceived relevance judgement, we may hide the fact that they are judging clickable snippets and just refer to them as short summaries. Similar ratings were collected by [\cite=chuklin2012good], where three possible answers were offered to the raters: the snippet "answers the user question," "answers the question only partially," "does not answer the question." Third and finally, perceived relevance is a new task that has to be formulated by explaining to the rater the story of a web search and asking her if she would click this link in order to find the relevant information in the document. The snippet has to be shown without the context of the other snippets and without its placement on a SERP to avoid position and presentation biases.

These, then, are the challenges of gathering relevance judgements in the multi-aspect setting that we are proposing:

Conclusion

This paper advocates for the need to review the notion of relevance in to order improve evaluation as well as understand the anatomy of relevance. We believe that after performing initial experiments and collecting feedback from the raters, we can address the challenges outlined above and derive a judgement procedure that will allow us to collect all three aspects of relevance, refine system performance evaluation and get deeper insights into the foundation of relevance.