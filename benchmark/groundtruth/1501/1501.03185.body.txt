Post-Selection and Post-Regularization Inference in Linear Models with Many Controls and Instruments

Abstract. In this note, we offer an approach to estimating structural parameters in the presence of many instruments and controls based on methods for estimating sparse high-dimensional models. We use these high-dimensional methods to select both which instruments and which control variables to use. The approach we take extends , which covers selection of instruments for IV models with a small number of controls, and extends , which covers selection of controls in models where the variable of interest is exogenous conditional on observables, to accommodate both a large number of controls and a large number of instruments. We illustrate the approach with a simulation and an empirical example. Technical supporting material is available in a supplementary appendix.

Publication: American Economic Review 2015, Papers and Proceedings.

Online Appendix: Post-Selection and Post-Regularization Inference: An Elementary, General Approach.

Model and Estimation Approach

Consider the linear IV model

[formula]

with [(zi',xi')'εi]  =  [(zi',xi')'ui]  =  0. di is the scalar endogenous variable and α the coefficient of interest, xi is a pxn-vector of exogenous control variables, zi is a pzn-vector of instruments, n is the sample size, and pxn  ≫  n and pzn  ≫  n are allowed. Extension to the case where di is a vector is straightforward and omitted for simplicity. We may have that zi and xi are correlated so that zi are only valid instruments after controlling for xi; specifically, we let zi  =  Πxi  +  ζi, for Π a pzn  ×  pxn matrix and ζi a pzn-vector of unobservables with [xiζi']  =  0. Substituting this expression for zi as a function of xi into ([\ref=dzx]) and then further substituting into ([\ref=ystructure]) gives a system for yi and di that depends only on xi:

[formula]

with [xiρyi]  =  0 and [xiρdi]  =  0. This model includes the many instruments and small number of controls case by setting pxn  ≪  n and can accommodate the exogenous case by setting pzn  =  0 and imposing the additional condition [diεi]  =  0.

Because the dimension of η0  =  (θ'0,ϑ0',γ0',δ0')' may be larger than n, informative estimation and inference about α0 is impossible without imposing restrictions on η0. For simplicity, we provide discussion under the assumption of exact sparsity and present a generalization to approximate sparsity in the supplemental material. Specifically, we assume that

[formula]

where [formula] denotes the number of non-zero elements of η0. That is, sparsity requires that, among the pxn  +  pzn observed variables, the number of variables with non-zero coefficients is small relative to the sample size. This assumption then reduces the problem of estimating α to a problem of finding which instruments and controls to use in equations ([\ref=ystructure]) and ([\ref=dzx]).

The problem that arises is that variable selection techniques are not perfect and are prone to making selection mistakes. There are two kinds of selection mistakes: A variable may be deemed relevant when in fact it has a zero coefficient and thus has no true explanatory power, or a variable may be dropped from the model despite having a non-zero coefficient. Both types of mistakes may detrimentally affect post-model-selection estimators and inference for α. When irrelevant variables are spuriously included after being deemed predictive from looking at the data, overfitting occurs and importantly the spuriously included variables are those most correlated to the noise in the sample due to data-snooping which introduces a type of "endogeneity" bias. When relevant x variables are excluded, one is left with standard omitted variables bias. When relevant z variables are excluded, one loses identification power. This last concern can be dealt with through appropriate use of weak identification robust inference as in .

The first type of mistake, the spurious inclusion of irrelevant variables, can be avoided through the use of modern, principled data-mining methods. For example, we use the Lasso with tuning parameters chosen as in , and many other options are available. These methods differ from the unprincipled data-snooping that many economists associate with the term data-mining. Specifically, modern data-mining denotes a principled search for �true� predictive power that guards against false discovery and overfitting, does not erroneously equate in-sample fit to out-of-sample predictive ability, and accurately accounts for using the same data to examine many different hypotheses or models.

Of course, guarding against the first type of error comes at the cost of needing to acknowledge that the exclusion of relevant variables is likely to occur. While sensible approaches such as Lasso will accurately find strong predictors, one can show that such procedures have non-negligible probability of missing predictors with small but non-zero coefficients. Exclusion of such predictors can have substantive impacts on inference for parameters of interest such as α in our model; see, for example, . To overcome this difficulty, one needs to base estimation and inference on procedures that are robust to this type of model selection mistake. One such approach relies on using estimating equations that are locally insensitive to this type of mistake, termed orthogonal moment functions in .

In the IV model with many instruments and controls, such a moment condition is given by

[formula]

where ψi(α,η)  =  (yi  -  diα̃)ṽi for η: = (θ',ϑ',γ',δ')', yi: = yi  -  xi'θ, di: = di - xi'ϑ, and ṽi: = xi'γ  +  zi'δ  -  xi'ϑ. When we set   =  η0, we have yi  =  ρyi  =  yi  -  xi'θ0, di  =  ρdi  =  di - xi'ϑ0, and ṽi  =  vi: = xi'γ0  +  zi'δ0  -  xi'ϑ0  =  ζi'δ0.

We can see that small selection errors will have relatively little impact on estimation of α0 by noting that the following orthogonality condition holds:

[formula]

In other words, missing the true value η0 by a small amount does not invalidate the moment condition. Thus, estimators [formula] of α0 based on the empirical analog of ([\ref=moment]),

[formula]

with [formula] can be shown to be "immunized" against small selection mistakes. See for a general formulation of orthogonal moment funtions for use in sparse high-dimenionsal models and a number of estimation and inference results.

Note that operationally using the empirical version of ([\ref=moment]) to estimate α0 is equivalent to using the usual IV regression of ρy on ρd using v as instruments. Based on this argument, we suggest the following algorithm for estimating α0 based on the "double-selection" strategy of .

Algorithm 1. (1) Do Lasso or Post-Lasso Regression of di on xi,zi to obtain [formula] and [formula]. (2) Do Lasso or Post-Lasso Regression of yi on xi to get [formula]. (3) Do Lasso or Post-Lasso Regression of i  =  xi'  +  zi' on xi to get [formula]. (4) Let yi: = yi  -  xi', di: = di  -  xi', and i: = xi'  +  zi'  -  xi'. Get estimator [formula] from ([\ref=emp_moment]) by using standard IV regression of yi on di with i as the instrument. Perform inference on α0 using [formula] or the associated score statistic and conventional heteroscedasticity robust standard errors.

The following result summarizes the properties of [formula] obtained from Algorithm 1.

Proposition 1. Under the stated sparsity and other regularity conditions, the estimator [formula] defined in Algorithm 1 satisfies [formula] where V  =  [v2i]- 2[ψi(α0,η0)2]. The score statistic [formula] satisfies [formula]. Confidence intervals based on these two results are uniformly valid for inference about α0 over a large class of models.

The supplementary material provides a precise statement and proof. The theoretical results do not depend on whether the Lasso estimator or the Post-Lasso estimator of is used. In the results reported in this paper, we use the Post-Lasso estimator. Note that there are other algorithms that would yield similar asymptotic properties. For example, one could follow the double-selection strategy more closely by running Lasso regression of di on xi and zi, Lasso regression of di on xi, Lasso regression of yi on xi, and then forming a 2SLS estimator using instruments selected in the first step and controlling for the union of controls selected in the three Lasso steps.

Simulation Example

To illustrate the preceding discussion, we report results from a small simulation experiment. Data were generated from the model given in Section 2 with n  =  200, pxn  =  300, and pzn  =  150. Other parameter values were chosen so that the infeasible, optimal instruments are "strong", perfect model selection is impossible, and the sparse model provides a good approximation. Further details are available in the supplementary material.

We provide results for four different estimators - an infeasible Oracle estimator that knows the nuisance parameters η (Oracle), two naive estimators, and the "Double-Selection" estimator. The first naive estimator follows Algorithm 1 but replaces Lasso/Post-Lasso with stepwise regression with p-value for entry of .05 and p-value for removal of .10 (Naive 1). It is well-known that this procedure fails to control model selection mistakes in which irrelevant variables are included. The second naive estimator estimates the high-dimensional nuisance functions using Post-Lasso but uses the moment condition [formula] (Naive 2). This moment condition does not satisfy the orthogonality condition described above, though estimation and inference about α0 using this condition will be valid when perfect model selection for the regression of y on x and d on x is possible.

We report the median bias (Bias), median absolute deviation (MAD), and size of 5% level tests (Size) obtained from 1000 simulation replications for each procedure. For the Oracle, we have Bias of .006, MAD of .095, and Size of .043. For Naive 1, Bias, MAD, and Size are .160, .227, and .302 respectively; and Bias, MAD, and Size are respectively .035, .103, and .095 for Naive 2. Finally, the Double-Selection approach gives Bias of .021, MAD of .099, and Size of .054.

These results correspond to the discussion in Section I. The first naive, unprincipled procedure fails to control spurious inclusion of irrelevant variables and performs quite poorly relative to the other three approaches. The second naive procedure can be shown to be formally valid when perfect model selection is possible and performs relatively well in terms of MAD. However, the asymptotic approximation under perfect model selection provides a misleading approximation to the true sampling distribution as evidenced by the size distortion. Finally, we see that basing estimation and inference on a principled variable selection procedure and moment conditions that are immunized against small model selection mistakes produces an estimator that performs well relative to the infeasible Oracle in terms of both estimation and inference performance as measured by MAD and Size.

Empirical Example

We conclude with a brief empirical example where we estimate the coefficients in a simple model of demand for automobiles. We use the data and basic strategy of . For simplicity, we consider the most basic specification

[formula]

where sit is the market share of product i in market t with product 0 denoting the outside option, pit is price and treated as endogenous, xit are observed included product characteristics, and zit are instruments. One could also consider allowing random coefficients and adapting the variable selection procedures to this setting; see .

In their basic results, use five variables in xit: a constant, an air conditioning dummy, horsepower divided by weight, miles per dollar, and vehicle size. They argue that characteristics of other products provide valid instruments for price and choose 10 instruments for pit based on intuition and an exchangeability argument. The first five instruments are formed by deleting product i and then summing each characteristic in x across all remaining products produced by product i's firm. The other five instruments are similarly constructed by deleting all products from product i's firm and then summing each characteristic in x across all remaining products. Using these controls and instruments, the 2SLS estimate of α is -.142 with an estimated standard error of .012. One might compare this to the OLS estimate obtained treating price as exogenous given the five controls listed above which is -.089 with estimated standard of .004.

It is interesting to note that state, "The choice of which attributes to include in the utility function is, of course, ad hoc" (p. 872). They similarly note that one could have considered additional instruments such as higher order terms [\cite=BLP]. The high-dimensional methods outlined in this paper offer one strategy to help address these concerns which complements the well-founded economic intuition motivating the authors' choices. We apply our outlined methods in two scenarios. In the first, we apply the method using just the original five controls and 10 instruments. In the second, we augment the set of potential controls with a time trend, quadratics, and cubics in all continuous variables, and all first order interactions and then use sums of these characteristics as potential instruments following the original strategy. These additions give a total of 24 x-variables and 48 potential instruments. We include the intercept in all models and select over the remaining variables.

In both cases, the results suggest demand is more elastic than indicated in the baseline results. After selection using only the original variables, we estimate the price coefficient to be -.185 with an estimated standard error of .014. In this case, all five controls are selected in the log-share on controls regression, all five controls but only four instruments are selected in the price on controls and instruments regression, and four of the controls are selected for the price on controls relationship. The difference between the baseline results is thus largely driven by the difference in instrument sets. The change in the estimated coefficient is consistent with the wisdom from the many-instrument literature that inclusion of irrelevant instruments biases 2SLS toward OLS.

With the larger set of variables, our post-model-selection estimator of the price coefficient is -.221 with an estimated standard error .015. Here, we see some evidence that the original set of controls may have been overly parsimonious. In the log-share on controls regression, we have that eight control variables are selected; and we have seven controls and only four instruments selected in the price on controls and instrument regression. We also have that 13 variables are selected for the price on controls relationship. The selection of these additional variables suggests that there is important nonlinearity missed by the baseline set of variables.

Finally, we note that in terms of own-price elasticities, the results become more plausible as we move from the baseline results to the results based on variable selection with a large number of controls. Recall that facing inelastic demand is inconsistent with profit maximizing price choice within the present context, so theory would predict that demand should be elastic for all products. However, the baseline point estimates imply inelastic demand for 670 products. Using the variable selection results provides results closer to the theoretical prediction. The point estimates based on selection from only the baseline variables imply inelastic demand for 139 products, and we estimate inelastic demand for only 12 products using the results based on selection from the larger set of variables. Thus, the new methods provide the most reasonable estimates of own-price elasticities. Of course, the simple specification above suffers from the usual drawbacks of the logit demand model, but the example illustrates how the application of the methods outlined in this note may be used in estimation of structural parameters in economics and add to the plausibility of the resulting estimates.

Conclusion

A great deal of empirical economic research aiming to estimate causal or structural effects depends on using the right set of controls and instruments. The need for formal methods that perform this model selection and inference procedures that remain valid following model selection is likely to increase in importance as data sets become richer. We have outlined one simple approach that can be used in an instrumental variables model with many instruments and controls that extends and . The approach relies on an approximate sparsity assumption and the use of high-quality variable selection procedures coupled with the use of appropriate moment functions. These ideas follow from the general framework considered in . For more applications of similar ideas in economics, see also , ; ; ; and and references therein.