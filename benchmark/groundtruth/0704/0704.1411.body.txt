Trellis-Coded Quantization Based on Maximum-Hamming-Distance Binary Codes

Introduction

well known that trellis source coding asymptotically achieves the bounds of Shannon theory at the price of constructing (and storing) huge codebooks of random reproduction values [\cite=viterbi_TrellisCoding]. More feasible trellis codes are instead obtained by reducing and structuring the codebooks. Trellis-coded quantization (TCQ) represents a successful attempt in this direction [\cite=marcellin_TCQ].

TCQ has first appeared as a natural counterpart, in the source coding framework, of trellis-coded modulation [\cite=ungerboeck_ChCodWithMultilevelSignals]. The basic TCQ scheme uses an extended codebook of 2R + 1 reproduction values to code a source at a rate of R bit/sample. This codebook is divided into four sub-codebooks that are assigned to the branches of a trellis diagram by means of a (2,1) binary convolutional code, and the Viterbi algorithm is used to find the minimum mean-squared-error path through the trellis. Among the others, TCQ and TCQ-like coding ([\cite=fisher_TCVQ] [\cite=wang_TCVQ]) have found application in image coding [\cite=taubman_JPEG2000] [\cite=bilgin_ProgressiveTCQ], in scalable coding [\cite=wang_succRef-tcq] and in multiple description coding [\cite=wang_md-tcq].

Since the choice of the actual reproduction values is responsible for the achievable performance [\cite=pearlman_codingUsingQuantRepLevels], several algorithms have been proposed to find the best codebook. For instance, in the original paper [\cite=marcellin_TCQ] an iterative algorithm is given for alphabet optimization that improves on the uniform distribution. In other approaches, in an attempt to achieve some of the boundary gain too [\cite=eyuboglu_LatticeTrellisQuant], the occurrences of each reconstruction level are taken into account during the optimization (and the coding) procedure. Examples are given in [\cite=yang_trellisMemoryless] (that refines the work in [\cite=laroia_tb-svq]) and in [\cite=marcellin_OnECTCQ], where the concept is more elegantly approached via Lagrangian optimization.

Observing that the size of the reproduction alphabet puts (at rates close to log 2|A|, with |A| the size of the alphabet) a non-negligible limit on the achievable rate-distortion performance [\cite=finamore_EncodingFiniteOutput], recent approaches [\cite=vanDerVleuten_TCQconst] [\cite=eriksson_lc-tsc-tcom] have investigated the opportunity to use larger codebooks for improved performance. In particular, in [\cite=eriksson_lc-tsc-tcom] a linear congruential recursion is used to randomize with a very small computational burden the labels associated with the trellis branches.

The performance improvement over the basic TCQ systems was eventually obtained by taking into account for the rate (i.e. by entropy coding) or by increasing the reproduction set, but essentially keeping the trellis structure of a convolutional binary code for codeword randomization. As observed in [\cite=eriksson_lc-tsc-tcom], in fact, there seems to be no performance gain from changing the very simple shift-register transitions.

Despite the fact that eventually TCQ heavily relies on convolutional codes, the current know-how about the algebraic properties of the convolutional codes has not been considered for the design of TCQ systems, which are instead based on ad-hoc convolutional codes (such as the ones in [\cite=ungerboeck_ChCodWithMultilevelSignals]). In this letter a TCQ design technique based on maximum-Hamming-distance convolutional codes is investigated for high rate TCQ. The obtained codes are shown to be competitive with the ones in the literature for the same computational complexity.

Maximum-Hamming-Distance-Based Trellis-Coded Quantizer Design

Suppose that the source to be quantized produces sequences of independent and identically distributed samples. According to the generally accepted Gersho's conjecture [\cite=gersho_AsymptOptBlockQuant], the Voronoi cells of the best vector quantizer are all asymptotically congruent to the same polytope, with the possible exception of the ones touching the boundary of the typical set over which the source outcomes are uniformly distributed. At high rates, i.e. when most Voronoi cells lie in the interior of the typical set, it is then reasonable that all granular gain [\cite=eyuboglu_LatticeTrellisQuant] can be achieved by means of a geometrically uniform code [\cite=forney_GeomUnifCodes]. Successively, entropy coding achieves all boundary gain [\cite=eyuboglu_LatticeTrellisQuant], independently of the source distribution.

If the extended codebook [formula] is a geometrically uniform set that can be partitioned into the cosets of [formula], and S / S' is isomorphic to the group [formula], then TCQ defines a geometrically uniform code [\cite=forney_GeomUnifCodes]. In particular, it is the generalized coset code [formula], where [formula] are the codewords of maximum degree L - 1 of the used (2,1) convolutional code [\cite=mceliece_ConvCodes_inbook]. The rate-distortion performance of [formula] depends on the reciprocal position of the coset representatives of the partition [formula] which are the closest to the null element 0LS' of S'L. The aim is hence to maximize the reciprocal distance of these coset representatives.

Distance Preserving Labelings

Since the representatives of the different cosets are related to the different paths on the trellis, and then to different codewords of the underlying convolutional code, it is worth to investigate if there exist systems that associate more distant coset representatives (in Euclidean signal space) to more distant codewords (i.e. with large Hamming-distance). In such a case, distance-optimal binary convolutional codes would lead to generalized coset codes with good distance properties.

Any coset representative of the partition [formula] is simply an L-tuple [formula] of points of S such that sn∈μ(an), [formula], where μ is the isometric labeling [\cite=forney_GeomUnifCodes] of the partition S / S' and [formula] is the codeword of the underlying convolutional code which specifies the considered coset. Since the (squared) Euclidean-distance is additive, among the possible coset representatives of a coset the closest to the point 0LS' is the one for which sn∈μ(an) is the closest to 0S', for each [formula]. Let us denote with S̄ the subset of S which exactly contains all the coset representatives of the partition S / S' which are the closest to 0S'.

Like (squared) Euclidean-distance, Hamming-distance is additive. To assign more distant coset representatives to more distant codewords it is then sufficient for μ to label the elements of S̄ (and hence of S) with binary tuples such that the closer two points are in N-dimensional Euclidean space, the smaller is the Hamming-distance between the corresponding labels. A binary isometric labeling satisfying this property has been called a distance preserving labeling. In the following common cases such a labeling exists.

One-Dimensional Partition [formula]. Consider, for N = 1, the partition [formula], that corresponds to the (scaled and) infinitely extended alphabet case obtained from the scalar example in [\cite=marcellin_TCQ]. Usually, the partition is labeled as in Fig. [\ref=f:isolabel1] and ad-hoc (1,2) binary convolutional codes are used. A possible distance preserving labeling is shown in Fig. [\ref=f:isolabel2]: labels differing by one or two bits are respectively given to cosets whose minimum (squared) Euclidean-distance equals one or four.

Two-Dimensional Partition [formula]. For N = 2, consider the partition [formula], that again corresponds to the (scaled and) infinitely extended alphabet case obtained from the two-dimensional example in [\cite=marcellin_TCQ]. A distance preserving labeling is shown in Fig. [\ref=f:isolabel2_2d]: labels differing by one or two bits are respectively given to cosets whose minimum (squared) Euclidean-distance equals one or two.

Simulation Results

At high rates, and assuming that the symbols output by the trellis-coded quantizer are entropy-coded down to their entropy, the asymptotic performance of geometrically uniform TCQ is measured by the asymptotic normalized second moment (per dimension) of the Voronoi cell of [formula], independently from both the rate and the probability distribution of the source [\cite=eyuboglu_LatticeTrellisQuant]. Equivalently, the performance can be expressed in terms of coding gain w.r.t. entropy-coded uniform quantization by the asymptotic value of the granular gain, γg, which simply represents the factor of reduction of the normalized second moment obtained by shaping an hypercube into the shape of the Voronoi cell of the trellis-coded quantizer.

In the following, hence, the granular gain of the traditional TCQ systems found in the literature with the granular gain of maximum-Hamming-distance-based systems having the same number of states (i.e., exactly the same computational complexity) are compared.

Experimental Setup

The approximated second moment of a Voronoi cell can be evaluated only by simulation. If an LN-dimensional region containing several cells (along each coordinate) is uniformly populated by Nv random vectors (sequences), the average per dimension energy of the corresponding quantization errors approximates the second moment (per dimension) P(C) itself. Hence, the quantity

[formula]

can be evaluated, where xi,j is the j-th realization of the i-th random sequence (and i,j is its reconstruction). The fidelity of this approximation can be measured as follows. Note that for high values of LN, one can appeal to the central limit theorem and hence assume that the expression in square brackets in ([\ref=e:tildeP]) is a Gaussian r.v. belonging to N(P(C),σ2). Consequently, P̃(C)∈N(P(C),σ2 / Nv) and in the 95% of the cases

[formula]

where the best unbiased estimate of the standard deviation given the Nv measured distortion samples can be used as [formula].

In the experiments, the granular gain is evaluated with Nv = 5000 sequences of length LN = 1000, randomly distributed in the hypercube with sides equal to 2R  ·  2 and to [formula] respectively for the case [formula] and the case [formula], with R = 8.

Results at High Rates

The top-half of Table [\ref=t:1d] compares the granular gains of the [formula]-TCQ systems in [\cite=marcellin_TCQ] with the ones of the proposed systems that are based on the labeling of Fig. [\ref=f:isolabel2]. For each number of states, the codes are shown in octal form. The codes shown in the fourth column of Table [\ref=t:1d] are distance-optimal (2,1) convolutional codes taken from [\cite=mceliece_ConvCodes_inbook]. The values in the left hand side of Table [\ref=t:1d] confirm the results at high rates of [\cite=marcellin_TCQ] and [\cite=eyuboglu_LatticeTrellisQuant].

For most number of states, the proposed codes exactly achieve the same granular gain. In particular, the codes with 4 and 8 states lead to systems exactly equivalent to the corresponding ones based on Ungerboeck codes. A measurable, yet almost negligible improvement is instead measured for codes with 16, 64, and 256 states. The 1024-states code

[formula]

Results at Low Rates

Since the asymptotic gains describe only the high rate performance, it is interesting to investigate if the proposed codes are competitive with the traditional ones even at low rates. By simply limiting the extended codebook to only 2R + 1 consecutive elements of [formula] (assuming a symmetric source distribution with respect to zero) it would be possible to achieve any (integer) desired limited bit rate. However, the performance increases if such a finite alphabet is optimized according to some training data set, while the encoding complexity remains essentially the same.

The comparison, relative to the optimized finite alphabets with R equal to 1, 2, and 3 bit/sample, is shown in Table [\ref=t:finite_rate] for both the uniform and the Gaussian distribution.

The results show a similar improvement with respect to the TCQ systems in [\cite=marcellin_TCQ] for the codes with 16, 64, and 256 states. Distance-optimal convolutional codes are hence able to distribute the reconstruction values in a maximal-distance fashion even in non-regular arrays. If the burden of randomizing the labels of the trellis and increasing the codebook size is not wanted (see [\cite=vanDerVleuten_TCQconst] [\cite=eriksson_lc-tsc-tcom]), then the proposed systems are a feasible alternative to traditional trellis-coded quantizers.

Conclusion

In this letter an approach to TCQ design has been presented that is based on coupling a distance preserving labeling of the extended codebook with maximum-Hamming-distance convolutional codes. In this way, the relation between the minimum distance of the feasible reconstructed sequences and the Hamming-distance of the underlying convolutional code is exposed.

Results show that it is possible and somewhat advantageous to conduct the search for good trellis codes into the binary domain, i.e. to look for good convolutional codes. In fact, the codes found are competitive with or somewhat better than the ones in literature. In the [formula] case better codes have been found for the 16, 64, and 256 states case, while in the [formula] case better codes have been found for the 16, 32, 64, and 256 states cases. In addition, a 1024 states code is given that, according to the authors' knowledge, was never published before in the context of TCQ.