Kahane-Khinchin type Averages

Introduction

omerfrie@post.tau.ac.il; address: School of Mathematical Sciences, Tel Aviv University, Ramat Aviv, Tel Aviv 69978, Israel.

The classical Kahane's inequality (cf. Kahane [\cite=ka]) states that for any 1  ≤  p <   ∞   there exists a constant Kp > 0 such that

[formula]

holds true for every n and arbitrary choice of vectors [formula], where X is a normed space with the norm [formula], and ri is a Rademacher function that is given by [formula], [formula] (for more information about Rademacher functions, see, e.g. Milman and Schechtman [\cite=ms], section 5.5). Kwapie [\cite=kw] proved that [formula] is a constant depending only on p.

Note that, in view of the definition of ri(t), these integrals in ([\ref=kahane]) can also be represented as averages (for [formula])

[formula]

Bourgain, Lindenstrauss and Milman [\cite=blm] proved that, if [formula] are unit vectors in a normed space [formula] then, for any ε > 0 there exists a constant C(ε) > 0, such that N = C(ε)n random sign vectors [formula] satisfy with probability greater than 1 - e- cn that

[formula]

for every [formula], where [formula].

Obviously, the norm |||  ·  ||| is an unconditional norm, i.e., it is invariant under the change of signs of the coordinates. As ([\ref=blm]) shows, it is sufficient to average O(n) terms in ([\ref=average]) rather than 2n, in order to obtain a norm that is isometric to |||  ·  |||, and in particular symmetrize our original norm [formula] with selected vectors {vi}ni = 1 to become almost unconditional.

In this paper, we change the settings given in ([\ref=blm]), in two ways:

First, random sign vectors are replaced by random vectors [formula] with an arbitrary log-concave probability measure μ.

Second, we are now interested in using a small number N = (1 + δ)n of random vectors, at the cost of an isomorphic, instead of an almost isometric, comparison of [formula] with the random norm [formula].

Let [formula] be unit vectors in a normed space [formula]. Define a norm |||  ·  ||| on [formula]: [formula], where ai is the ith coordinate of the vector a and μ is a log-concave probability measure μ on [formula].

Proof of Theorem

Before we proceed, we give a short description of Chernoff's method. The following lemma, which is a version of Chernoff's bounds, gives estimates for the probability that at least βN trials out of N succeed, when the probability of success in one trial is p (cf. Hagerup and R�ub [\cite=hr]).

Let [formula] be independent Bernoulli random variables with mean 0 < p < 1, that is, Zi takes value 1 with probability p and value 0 with probability (1 - p). Then we have

1)     [formula]     when β < p  ,

2)     [formula]          when β > p  ,

where [formula].

In the questions above, essentially, we are looking for upper and lower bounds of [formula]. Upper bounds are relatively easy to obtain, and quite often do not require new methods, but only the use of large deviation inequalities like Bernstein's inequality and some net argument. Obtaining lower bounds is different, usually one needs small ball probabilities, which are hard to get, and some extra delicate arguments which are closely related to the context of the question at hand. Here comes Chernoff's method, if one has a small ball probability for one trial, using Chernoff's bounds, the estimate of the average of many trials can be amplified. For more detailed description of this method, see Artstein-Avidan, Friedland and Milman [\cite=afm1], [\cite=afm2].

Finally, let us analyze the function

[formula]

where we denoted u(β)  =  β ln β  +  (1 - β) ln (1  -  β). The term u(β) is a negative, convex function which approaches 0 as β  →  0 and as β  →  1, and is symmetric about 1 / 2 where it has a minima equal to -   ln 2. Thus the whole exponent in Lemma [\ref=chernoff] is of the form

[formula]

Proof of Theorem. We estimate the following probability

[formula]

where [formula] and [formula].

It is clear that the probability above is greater than the following one

[formula]

Upper bound: We begin by estimating the first term

[formula]

This is relatively easy, and does not require a new method; we do it in a similar way to the one in [\cite=blm]: let N  =  {y(i)}mi  =  1 be a [formula]-net with respect to |||  ·  ||| on Sn - 1|||  ·  |||, it is known that such a net exists with m  ≤  5n. For each 1  ≤  i  ≤  m we consider the random variables {Xi,j}Nj = 1 defined by

[formula]

where [formula] and y(i)k is the kth coordinate of the vector y(i). Clearly, for each i and j, Xi,j has mean 0 and [formula] for some absolute constant b > 0 (see Milman and Schechtman [\cite=ms], App. III). Now, using the well-known Bernstein's inequality which we shall use in the form of ψ1 estimate (see Vaart and Wellner [\cite=vw]):

Let [formula] be independent random variables with mean 0 such that for some b > 0 and every i, [formula]. Then, for any t > 0, where c > 0 is an absolute constant.

we deduce that for any t > 0 and every 1  ≤  i  ≤  m, we have

[formula]

which implies that for a point y(i)∈N and for any t > 1 we have

[formula]

The obvious way to make this probability small enough to handle a large net is to increase t, and obviously we shall get a worse upper bound constant. So, we choose t such that

[formula]

for example [formula]. Then, with probability at least 1 - e- n, for every 1  ≤  i  ≤  m we have

[formula]

We thus have an upper bound for a net on the sphere. It is standard to transform this to an upper bound estimate on all the sphere (this is an important difference between lower and upper bounds). One uses a consecutive approximation of a point on the sphere by points from the net to get that |||x|||N  ≤  2t  =  2t for every x∈Sn - 1|||  ·  |||. This completes the proof of the upper bound, where [formula] is a universal constant.

Lower bound: We now turn to estimate the second term

[formula]

Note that when estimating this term, we know in advance that the (random) norm |||  ·  |||N is bounded from above on the sphere Sn - 1|||  ·  ||| (i.e. [formula] we have |||y|||N  ≤  C, where C comes from the upper bound). This is crucial to transform a lower bound on a net on the sphere to a lower bound on the whole sphere. For the lower bound we use Chernoff's method, as described above, to estimate the probability in ([\ref=lower]).

Let us denote by p the probability that for a random vector [formula] we have [formula], where α > 0 and x is some point on Sn - 1|||  ·  |||:

[formula]

If "doing an experiment" means checking whether [formula] (where [formula] is a random vector) then for |||x|||N to be greater than some c, it is enough that (c / α)N of the experiments succeed.

Of course, we will eventually not want to do this on all points x on the sphere, but just on some dense enough set. So, first we estimate the probability p ~ :

There exists a universal constant γ > 0 such that for any [formula], we have

[formula]

Proof of Lemma [\ref=simple]. Let us define [formula] for [formula], then Ax is convex and symmetric set. We take γx > 0 to be the number such that [formula]. Applying Borell's lemma (see Milman and Schechtman [\cite=ms], App. III.3) we get, for all t > 1,

[formula]

and consequently,

[formula]

for some universal constant c4 > 0 which doesn't depend on x. Therefore, we get that [formula]. Now, we take [formula]. For this γ > 0 and any [formula] we have [formula] [formula]

Now, we can use the following lemma of Latała with the set

[formula]

for which we proved above that [formula].

For each b < 1 there exists a constant cb > 0 such that for every log-concave probability measure μ and every measurable convex, symmetric set C with μ(C)  ≤  b we have

[formula]

Notice that, for any point x, we can make the probability as small as we like by reducing t. This allows us to use a simple net: take θ-net N in Sn - 1|||  ·  |||, with less than [formula] points.

For every [formula] there is a vector y∈N such that |||x - y|||  ≤  θ, and we have |||y|||N  ≤  |||x|||N + |||x - y|||N  ≤  c + Cθ (where c = c(δ) and C comes from the upper bound). Therefore, we bound ([\ref=lower]) by

[formula]

By Lemma [\ref=latala], for a given y∈N we have for any 0 < t < 1 that

[formula]

where [formula].

We return to our scheme, in order to estimate the probability in ([\ref=cont_net]), assume that βtγ  ≥  c + Cθ, where β shall be the portion of good trials out of N, and t another constant that we choose later such that p > β. So, we know that for β < 1 - c1t (which is hardly a restriction, t will be very small and so will β), from Lemma [\ref=chernoff] for a given y∈N we have

[formula]

where [formula].

We choose β so that [formula], hence [formula]. We choose θ  =  βtγ / 2C, where C comes from the upper bound. To make sure that the probability above holds for all points in the net we ask that

[formula]

[formula]

For the first inequality we use ([\ref=analyse]), we choose [formula], for some universal constant c2 > 0, and get the lower bound for each point of the net N.

Now using the upper bound, for every [formula] there is a vector y∈N such that |||x - y|||  ≤  θ, therefore we have

[formula]

[formula], where c3 > 0 is an absolute constant. Thus the proof of the lower bound, and of the Theorem, is completed. [formula]