=4

Manifold Matching using Shortest-Path Distance and Joint Neighborhood Selection

Introduction

In the modern world it is becoming increasingly important to deal effectively with large amounts of high-dimensional data. For the purpose of data analysis, it is imperative to consider dimension reduction and embed the data into a low-dimensional space for subsequent analysis. Traditional linear embedding techniques have solid theoretical foundations and are widely used, e.g., principal component analysis (PCA) [\cite=JolliffePCABook], [\cite=BishopTipping1999] and multi-dimensional scaling (MDS) [\cite=TorgersonBook], [\cite=BorgBook], [\cite=CoxBook] for a single data set, and canonical correlation analysis (CCA) [\cite=Hotelling1936], [\cite=BachJordan2005] for multiple data sets.

However, real data may exhibit nonlinear geometry, and unfolding the non-linearity can be beneficial for subsequent inference. Many manifold learning algorithms have been proposed to learn the intrinsic low-dimensional structure of nonlinear data, including Isomap [\cite=TenenbaumSilvaLangford2000], [\cite=SilvaTenenbaum2003], locally linear embedding (LLE) [\cite=SaulRoweis2000], [\cite=RoweisSaul2003], Hessian LLE [\cite=DonohoGrimes2003], Laplacian eigenmaps [\cite=BelkinNiyogi2003], [\cite=HeEtAl2005], local tangent space alignment (LTSA) [\cite=ZhangZha2004], [\cite=ZhangWangZha2012], among many others. Most algorithms start with the assumption that the data are locally linear, and explore the local geometry via the nearest-neighbor graph of the sample data: transformation of the data is carried out based on the neighborhood graph, and the low-dimensional manifold is learned by optimizing some objective function. These nonlinear embedding algorithms usually serve as a preliminary feature extraction step enabling subsequent inference, and have achieved many practical successes in object recognition, image processing, etc.

In this paper, we consider the manifold matching task for two or more data sets from disparate sources. Classical methods to identify the relationship among multiple random variables are still very popular in theory and practice, such as canonical correlation [\cite=Hotelling1936], [\cite=Kettenring1971], [\cite=Hardoon2004] and Procrustes transformation [\cite=Sibson1978], [\cite=Sibson1979], [\cite=GoldbergRitov2009], [\cite=GowerProcrustesBook]. But it is becoming a much more challenging task in practice to match disparate data collected from different sources, such as the same document in different languages, an image and its descriptions, networks of the same actors on different social websites, etc. There are many recent endeavors in data fusion and manifold matching [\cite=LafonKellerCoifman2006], [\cite=WangMahadevan2008], [\cite=WangMahadevan2012], [\cite=SharmaKumar2012], [\cite=PriebeMarchette2012]; and similar to dimension reduction of a single data set, manifold matching usually serves as a feature extraction step to explore multiple data sets, and has also been shown to help subsequent inference in object recognition, information retrieval and transfer learning [\cite=KimKittlerCipolla2007], [\cite=PanYang2010], [\cite=SunPriebe2012], [\cite=SunPriebeTang2013], [\cite=ShenSunTangPriebe2014], [\cite=LyzinskiFishkindPriebe2014].

Due to the success of nonlinear embedding algorithms for a single data set, it seems intuitive that they can be combined with proper matching methods to achieve better feature extraction for multiple data sets. The simplest procedure is to pick one nonlinear algorithm, apply it to each data set separately, then match the transformed data sets together. But there exists three questions for this simple procedure: Firstly, how to assess whether the nonlinear algorithm improves the matching task? Secondly, among so many nonlinear embedding algorithms, each has its pros and cons; which algorithm is most suitable for the matching task? Thirdly, can we optimize the procedure and achieve better performance for disparate data matching, comparing to the separate embed and match strategy? To that end, we use distance correlation and hypothesis testing power to evaluate the matching quality, and propose a nonlinear manifold matching algorithm using shortest-path distance and joint neighborhood selection. The algorithm turns out to significantly improve the matching quality for disparate data matching (e.g., one data set has nonlinear geometry while the other does not), and also achieves robust performance against model selection and noisy data.

The paper is organized as follows: In Section [\ref=review] we review the basic setting, three common matching methods, followed by the Isomap algorithm that constructs the shortest-path distance. In Section [\ref=main] we present our nonlinear manifold matching algorithm, the evaluation criteria using distance correlation and hypothesis testing, and discuss various algorithmic issues. In Section [\ref=numer] we illustrate the advantages of our methodology via numerical simulations and real data experiments, using the simulated Swiss roll data and the Wikipedia document data with text and graph features. Finally in Section [\ref=conclu] we summarize and provide concluding remarks. The code and data are available on our website .

Reviews

The Matching Framework

We first introduce a formal setting for multiple matched data sets, and then briefly review the three matching methods discussed in [\cite=PriebeMarchette2012].

Suppose n objects are measured under two different sources. Then we have available Xl  =  {xil}∈Ξl for l = 1,2, [formula], with xi1  ~  xi2 for each i (~   denotes the matched data points). We assume [formula], or equivalently [formula]. Note that in practice matched data of disparate sources may have different or even unknown dimensions, say an image and its description, in which case it is more appropriate to assume that each space Ξl is endowed with a distance measure. But as the data can always be embedded into a proper ambient dimension first, for ease of presentation we assume the ambient space is [formula] for all data sources. The setting is also extendable to more than two data sets, but for convenience we assume l = 2 for most of the paper.

Since the ambient dimension m is usually large in modern applications, dimension reduction is often required to achieve a meaningful matching. The matching and embedding of multiple data sets are formulated as finding two mappings [formula] based on the given data Xl, i.e., ρl embed and match the two data sets in the common low-dimensional space [formula]. Here d should satisfy 1  ≤  d  ≤  m, and we denote X̂l  =  {ρl(xil)} as the mapped data in [formula]. There are many ways to assess the matching quality, but intuitively we would like the matched pairs (x̂i1, x̂i2) to be as close as possible for all i, while unmatched data are not close.

We presented three matching methods in [\cite=PriebeMarchette2012] to derive ρl based on different objective functions, namely MDS followed by the Procrustes matching, CCA matching, and joint MDS. In what follows, Xl represents an m  ×  n data matrix properly centered for each l, and the final output X̂l is a d  ×  n matrix.

The Procrustes method first projects the data separately by MDS or PCA into [formula], then minimizes the Procrustes fit [formula] by finding a proper rotation P. Here Ul is the d  ×  m PCA projection for each Xl, P is the d  ×  d Procrustes transformation, and [formula] is the Frobenius norm. Thus the two mappings are ρ1  =  PU1 and ρ2  =  U2.

The CCA method finds two d  ×  m CCA transformations Cl to maximize the correlation between X̂1 = C1X1 and X̂2 = C2X2, subject to the constraints that the sample covariance matrix of ClXl is identity for each l. Thus the two mappings are ρ1  =  C1 and ρ2  =  C2.

The joint MDS method constructs a 2n  ×  2n distance matrix using Euclidean distance within each Xl, and then applies MDS to directly project the data into [formula]. Note that the off-diagonal distance, i.e., the distance between X1 and X2, is usually unavailable and needs to be properly imputed; details are in Section [\ref=main].

We have investigated the property of these matching methods [\cite=PriebeMarchette2012], [\cite=FishkindShenPriebe2013], but it is not our purpose here to discuss which one is better for matching. They are all intuitive to use and easy to implement, and serve as a platform for introducing nonlinear embedding into the matching framework, because different nonlinear embedding algorithms may work better with certain matching method than others.

Shortest-Path Distance and Isomap

As we use shortest-path distance for matching, Isomap is the main nonlinear algorithm that applies shortest-path distance to achieve nonlinear embedding. Thus we review the Isomap algorithm here followed by some discussions. In this context we use X1 to denote the original data, X̂1 to denote the embedded data by Isomap, and d as the embedding dimension.

The algorithm works as follows: First it constructs a nearest-neighbor graph G based on the Euclidean distance matrix [formula], by k-nearest-neighbor (kNN) method or ε-ball method. Then this graph is used to iteratively calculate the shortest-path distance matrix ΔG. Finally X̂1 is obtained by projecting ΔG into dimension d by MDS.

The other steps in the Isomap algorithm being routine, the shortest-path distance construction, i.e., the calculation of ΔG, is the key step: For each i,j, initiate ΔG(i,j) = Δ1(i,j) if xi1 and xj1 are adjacent in G, ΔG(i,j) =   ∞   otherwise. Then iterate through [formula] and replace the entry ΔG(i,j) by min {ΔG(i,j),ΔG(i,q) + ΔG(q,j)}. The final matrix ΔG becomes the shortest-path distance matrix for X1. This can be effectively implemented by Floyd's algorithm or Dijkstra's algorithm.

Therefore, Isomap is essentially the same as MDS except it constructs the shortest-path distance matrix for MDS application rather than the original distance matrix. It has been shown in [\cite=BernsteinEtAl2000], [\cite=SilvaTenenbaum2003] that the shortest-path distance can recover the geodesic distance of isometric manifolds with high probability under certain sampling condition, and can also recover the geodesic distance of certain curved manifolds when using a slightly different version called conformal Isomap [\cite=SilvaTenenbaum2002].

Other than the fact that Isomap cannot recover all types of nonlinear geometry, the main downside of Isomap is the running time for large n. But its computation can be sped up by landmark Isomap or out-of-sample MDS, see [\cite=SilvaTenenbaum2003], [\cite=BengioEtal2003], [\cite=TrossetPriebe2008]. The idea is to pick a subset of landmark points to do usual Isomap, then compute the shortest-path distance of all other points with respect to the landmark points only, followed by out-of-sample MDS embedding. In this paper we do not use this technique for speed purpose, because we do not use large n in the experiments. But the out-of-sample technique is applied in hypothesis testing as described in the next section.

Manifold Matching Framework

In this section we first present the nonlinear manifold matching algorithm using shortest-path distance and joint neighborhood selection, then propose two evaluation criteria (distance correlation and hypothesis testing), followed by discussions on various implementation issues.

Main Algorithm

Our algorithm can be decomposed into three steps, where the first step applies joint neighborhood selection, the second step constructs the shortest-path distance, and the last step embeds and matches the data based on the constructed distances.

Step 1: Jointly select the neighbors and construct a single nearest-neighbor graph G for all data {Xl,l = 1,2}. This can be achieved by using the sum of distance to derive the nearest neighbors, i.e., whether xil is adjacent to xjl in G is determined by [formula] instead of Δl(i,j), and we always use k-nearest-neighbor for neighborhood selection.

Note that in order to achieve a meaningful joint neighborhood selection, it is necessary to pre-scale the data so the distance matrices are on the same scale. Alternatively, one may use a weighted sum of distance or rank-based method to derive a joint neighborhood.

Step 2: Given the graph G, calculate the shortest-path distance matrices ΔGl for each l using the same procedure as Isomap.

Step 3: Derive the mappings ρl and low-dimensional mapped data X̂l using any of the three matching methods on {ΔGl}.

Specifically, for the Procrustes method, we separately embed the two shortest-path distance matrices into [formula] by MDS, followed by Procrustes transformation.

For the CCA method, the same separate MDS embeddings are matched by CCA transformations in [formula] to maximize the correlation.

For the joint MDS method, we concatenate an omnibus matrix

[formula]

with OG = (ΔG1  +  ΔG1) / 2, and apply MDS (either classical MDS or raw-stress with proper weights, see [\cite=PriebeMarchette2012]) directly on MG to yield the embeddings {X̂l,l = 1,2} in [formula].

Note that if we only use step 3 of the algorithm without step 1 and step 2, it is equivalent to match the original distance without any nonlinear algorithm.

Evaluation Criteria

To assess the quality of the nonlinear manifold matching algorithm, we use distance correlation and hypothesis testing power as the evaluation criteria.

The distance correlation proposed in [\cite=SzekelyRizzoBakirov2007], [\cite=SzekelyRizzo2009] measures the correlation between data by distance, and has the nice property of being 0 if and only if two random variables are independent. The notion of distance correlation is particularly suitable for evaluating the shortest-path distance constructed in step 2 of our algorithm. If the distance correlation between the jointly constructed shortest-path distances is significantly larger than others (such as the distance correlation between the original distances, or between the shortest-path distances without joint neighborhood, or between the Euclidean distances by other algorithms like LLE), it indicates that shortest-path distance and joint neighborhood are better for matching.

We also consider the following hypothesis test used in [\cite=PriebeMarchette2012]. First we randomly split the sample data into training data Xl of matched pairs, testing data Yl  =  {yil} containing both matched pairs and unmatched pairs, and consider the matching test H0:yi1  ~  yi2. Then we learn the mappings ρl based on the training data Xl only, apply the mappings to the testing data, and use the Euclidean distance T = d(ŷi1,ŷi2) in the embedded space as the test statistic. We can construct the power curve by calculating the empirical distributions of the test statistic T under the null and the alternative, for which a higher matching power indicates a better manifold matching algorithm.

Those two criteria are complementary: distance correlation is fast to compute and independent of the embedding dimension and the matching method, but it is possible that the improvement of distance correlation can be offset by proper matching or may be due to over-fitting; hypothesis testing is a more complete evaluation of the manifold matching algorithm, but the testing power depends on the dimension choice in step 3. Thus we use both criteria in this paper. Note that there exists many other possible criteria: for example, one may test the usual correlation or the Procrustes statistic on the embedded data; and if the sample data has labels, one may test the classification error after matching, etc.

Discussions

In this subsection we discuss some implementation details and potential extensions of the nonlinear manifold matching algorithm, as well as providing explanations for using joint neighborhood selection and shortest-path distance.

On the scaling and centering of the data: To obtain a meaningful matching, proper scaling is usually required. This can be achieved by scaling the original data Xl, or the shortest-path distance matrices ΔGl, or the embedded data X̂l. And many algorithms such as conformal Isomap, LLE, and LTSA do implicit scaling in their algorithms. In order to facilitate the joint neighborhood step, we always pre-scale the original data to have the same Frobenius norm.

In addition to scaling, centering the original data Xl or the embedded data X̂l to have the same mean is also necessary for matching, which is implicitly handled by all embedding algorithms used in the paper.

On the joint neighborhood selection: Comparing to the usual separate neighborhood selection, it is more intuitive to consider joint neighborhood selection for a better manifold matching: in the ideal matching case, if xi1 is adjacent to xj1 in the first data set, so should xi2 and xj2 in the second data set; and in case of noisy data, separate graphs may yield larger discrepancy for later nonlinear embedding. Thus the two shortest-path distances should be more similar to each other when using joint neighborhood. Note that for hypothesis testing, we do not use joint neighborhood for the testing data because the testing pairs may be unmatched.

On the neighborhood size and dimension choice: The model selection problem is important for any algorithm involving nearest-neighbor graph or dimension reduction, which is also intrinsic to our manifold matching task. It has been argued that the neighborhood size k should neither be too small nor too large in order to recover the local geometry for nonlinear embedding; and there exists extensive discussions and adaptive methods on choosing the neighborhood size in [\cite=RoweisSaul2003], [\cite=MekuzTsotsos2006], [\cite=ZhangWangZha2012]. As for the dimension choice d, it affects the embedding step and later inference based on the embedding; and there exists automatic procedures using profile likelihood or Bayesian model selection from [\cite=Minka2001], [\cite=ZhuGhodsi2006], [\cite=Hoyle2008]. Note that k is required to be larger than d in most nonlinear embedding algorithms except Isomap.

In the numerical experiments we simply choose k as 10 or 20, and we choose the embedding dimension d based on the scree plot of the data. Although we do not delve into the complexity of the model selection problem, we provide a numerical example in Figure [\ref=fig7] to show that our manifold matching algorithm is fairly robust against the choice of k and d, due to the nature of the matching task and the evaluation criteria.

On the out-of-sample technique for testing: As already mentioned in Isomap, during the hypothesis testing of the proposed manifold matching algorithm, we keep the training data Xl as the landmark points, and use the out-of-sample technique to embed the testing data. The reason is similar to why we do not use joint neighborhood in testing: the training pairs are always matched while the testing pairs may be unmatched, and we do not wish the unmatched data to affect the distance calculation of the matched training data or the other way around. Note that the out-of-sample technique is widely available to many nonlinear algorithms like LLE, Laplacian eigenmaps, kernel PCA, see in [\cite=ScholkopfSmolaMuller1998], [\cite=BengioEtal2003], [\cite=Plaat2005], [\cite=MaatenEtAl2009]. But we do not apply out-of-sample embedding other than using Isomap or joint neighborhood, because it does not help the testing power for non-distance based algorithms and separate neighborhood.

On using other embedding algorithms: The manifold matching algorithm can be extended to most other nonlinear embedding algorithms not limited to shortest-path distance and Isomap. Taking LLE as the example, we may keep the joint neighborhood in step 1, and use LLE in step 2 instead of the shortest-path distance. This means the output of step 2 is two Euclidean data sets of dimension d rather than two distance matrices, and the matching step can be applied directly, i.e., in case of Procrustes or CCA matching we can directly match the data without doing separate MDS, and in case of joint MDS we form two Euclidean distance matrices to concatenate the omnibus matrix. In the numerical section, we will use LLE, LTSA, and Laplacian eigenmaps to compare with our proposed manifold matching algorithm using shortest-path distance and joint neighborhood.

On using the shortest-path distance: No nonlinear algorithm can always recover the nonlinear geometry; and shortest-path distance does not always approximate the geodesic distance either. But the matching task and the evaluation criteria ask that the matched data are close to each other, rather than that the geometry is fully recovered: the shortest-path distance is always no smaller than the original distance, and is able to enlarge the distance that is not in the local neighborhood. This often improves the distance correlation and the testing power.

Furthermore, Isomap is usually able to preserve the local geometry more faithfully than others. Many other nonlinear algorithms involve a normalization step, which only preserves the local geometry up to some affine transformation [\cite=GoldbergRitov2008]. This may cause trouble in matching disparate data if the affine transformations of each data set are significantly different. Indeed, in the numerical section we will observe that normalization-based algorithms like LLE, LTSA, and Laplacian eigenmaps may not perform stably for matching noisy data or data from disparate sources, and shortest-path distance usually prevails. But it does not mean that other nonlinear algorithms should not be used; they may still be valuable for certain data type or other evaluation criteria.

On matching more than two data sets: The manifold matching algorithm is readily extendable to match more than two data sets, which will appear in the numerical section. In this case, other than the minimal change of joint neighborhood in step 1, our algorithm needs proper modifications for the matching part in step 3: for the Procrustes method, we consider minimizing the square sum of Procrustes fit [formula] using two Procrustes transformation matrices; for the CCA method, generalized CCA [\cite=Kettenring1971], [\cite=TenenhausTenenhaus2011], [\cite=ShenSunTangPriebe2014] is the standard extension, which finds three transformations Cl to maximize the sum of pair-wise correlations subject to proper constraints; for the joint MDS method, the omnibus matrix can be constructed by three distance matrices followed by proper imputation and MDS. As to the evaluation criteria, we consider the test statistic T = d(ŷi1,ŷi2) + d(ŷi1,ŷi3) + d(ŷi2,ŷi3) for the hypothesis test H0:yi1  ~  yi2  ~  yi3, and the distance correlation is changed to the distance correlation sum (i.e., the sum of all pairwise distance correlation). The above extensions can be generalized to match any number of data sets.

Numerical Experiments

In this section we demonstrate the numerical advantages of our nonlinear manifold matching algorithm. Overall, we observe that the proposed algorithm can improve both the distance correlation and testing power, comparing to without using shortest-path distance or without using joint neighborhood.

Swiss Roll Simulation

The Swiss roll data from [\cite=TenenbaumSilvaLangford2000] is a 3D data set representing a nonlinear manifold, but intrinsically generated by points on a 2D linear manifold. Figure [\ref=fig1] shows the 3D Swiss roll data with 5000 points in colors, along with the embedded 2D data by MDS, Isomap and LLE at neighborhood size k = 10. Clearly MDS fails to recognize the nonlinear geometry while both Isomap and LLE succeed. The Isomap embedding looks similar to the original 2D linear manifold, but the geometry recovered by LLE is quite different from both Isomap and the original 2D data.

We carry out the first matching task as follows: for each Monte-Carlo run, we randomly pick n = 1000 training points from the original 2D linear manifold as the first data set X1, and take the corresponding points on the 3D Swiss roll as the second data set X2. Thus X1 and X2 are matched training data with distinct geometry, on which we can carry out our manifold matching algorithm and calculate the distance correlation. The parameters are set at k = 10, d = 2, and we use 100 matched testing pairs and 100 unmatched testing pairs to do testing.

After repeating 100 times, we present the mean distance correlation for the training data in Table [\ref=table:swiss], which shows that jointly constructed shortest-path distance has the best distance correlation. This advantage is reflected in the mean testing power with respect to different type 1 error levels in Figure [\ref=fig2], for which we present the matching performance for the Procrustes matching method combined with various nonlinear algorithms. We do not show joint MDS matching and CCA matching here, because their power curves have the same interpretation except CCA matching is a little inferior to the other two in the actual testing power. We should point out that we purposely set the sample size to be 1000, because in this example the testing power for all nonlinear algorithms will converge to 1 as n increases.

Note that in the captions of all following tables and figures, original distance means that we apply the matching methods without any nonlinear embedding, joint Isomap means that we apply our proposed manifold matching algorithm using shortest-path distance and joint neighborhood selection, separate Isomap means that we separately embed the data by Isomap and then do matching, joint LLE means that we apply our algorithm using LLE and joint neighborhood selection, and separate LLE means that we separately embed the data by LLE and then do matching. Moreover, the LLE version is implemented based on the distance version presented in [\cite=RoweisSaul2003] and uses out-of-sample technique for testing, in order to compare more fairly with our proposed manifold matching algorithm. When appropriate, we also add LTSA and Laplacian eigenmaps for benchmark purpose, which are not distance based and only use separate neighborhood.

Next we check the robustness of the manifold matching algorithm against noise. We do so by adding white noise to the first data set X1, with the noise being independently identically distributed as N(0,εI2  ×  2). We carry out the exact same procedure as before, and plot the mean Procrustes matching power at the fixed type 1 error level 0.05 with respect to increasing noise [formula] in Figure [\ref=fig3]. LTSA and Laplacian eigenmaps are also included in this simulation, with their mean CCA matching power plotted in the same figure (because CCA works better than Procrustes and joint MDS for them). Clearly joint Isomap is almost always superior; and even though joint LLE performs optimally in the previous example, its sensitivity to noise degrades the matching performance for noisy data; and both Laplacian eigenmaps and LTSA are significantly inferior to joint Isomap until ε = 9,10, but they are better than LLE and separate Isomap.

At last we check the performance of our algorithm for matching all linear data. We still use the original 2D linear manifold as the first data set X1, but replace X2 by the LLE-embedded 2D data set. Thus both data are linear with some differences, and we plot the mean Procrustes matching power with respect to different type 1 error levels in Figure [\ref=fig4]. In this case joint Isomap performs a little better than separate Isomap, which almost coincides with matching the original distance; and LLE performs significantly worse. This indicates that shortest-path distance and joint neighborhood are robust in matching data of similar geometry.

Wikipedia Articles Experiment

In this experiment we apply the manifold matching algorithm to match Wikipedia article features from disparate sources. The data contains n = 1382 pairs of articles from Wikipedia English and its corresponding French translations, within the 2-neighborhood of the English article "Algebraic Geometry". On Wikipedia, the same articles of different languages are almost never the exact translations of each other, because they are very likely written by different people and their contents may differ in many ways.

For both English articles and French articles, we construct a text feature matrix and a network adjacency matrix for each language: for the text feature, we consider the latent semantic indexing (LSI) features [\cite=DeerwesterDumais1990] followed by cosine dissimilarity to construct two text dissimilarity matrices TE and TF (standing for English text and French text); for the network, we directly construct two shortest-path distance matrices GE and GF (standing for English graph and French graph) based on the Internet hyperlinks under each language setting, and impute any path distance larger than 4 to be 6 to avoid infinite distances and scaling issues.

Thus we have four distinct matrices to describe the same article, making TE,TF,GE,GF matched in the context but of disparate sources. Furthermore, as the text matrices are derived by cosine similarity while the graph matrices are based on shortest-path distance, the former probably have nonlinear geometry while the latter should be close to linear.

As the first trial, we randomly pick n = 500 pairs of training points, 100 pairs of testing matched points and 100 pairs of testing unmatched points, set k = 20, d = 10, and carry out the manifold matching algorithm for every possible two data sets combination. After 100 Monte-Carlo runs, we present the mean distance correlation of the training data in Table [\ref=table:wikiDCor], and the mean testing power in Table [\ref=table:wiki] at type 1 error level 0.05 showing only the highest matching power among Procrustes, CCA and joint MDS.

We also provide joint MDS matching power in Figure [\ref=fig5] and Figure [\ref=fig6] for matching (TE,TF) and (TE,GE) with respect to different type 1 error levels, and include LTSA and Laplacian eigenmaps in the figures for comparison; we do not show Procrustes and CCA matching here, as they have the same behavior with slightly inferior matching power.

Clearly joint Isomap achieves the best performance for all combinations, and LLE does not work well for either distance correlation or testing power whenever any graph matrix is involved in matching. Furthermore, from Table [\ref=table:wikiDCor] and Table [\ref=table:wiki] we observe that our manifold matching algorithm helps the most for matching data of distinct geometry, but less significant for both linear data using graph matrices. This phenomenon is the same as the Swiss roll simulation.

As to LTSA and Laplacian eigenmaps, they cannot work with distance matrices directly. Thus for the Wikipedia data, we first project the original distance matrices into an ambient space [formula] with m = 50, then proceed to apply LTSA and Laplacian eigenmaps into [formula] followed by matching. We observe in Figure [\ref=fig5] and Figure [\ref=fig6] that they are significantly worse than joint Isomap but perform similar to LLE or original distance, which is the case for all other data combinations. Therefore we do not show their performance later on. Note that it is possible that adjusting the parameter m may improve the performance, but they are not significantly better for all dimensions we tried from m = 10 to m = 300; and it is possible that no suitable ambient dimension exists for LTSA and Laplacian eigenmaps to work well for matching the Wikipedia data.

We should also point out that the actual matching power depends on the parameters k and d, and the power of certain algorithm can be significantly improved by changing the parameters. For example, the matching power is only 0.45 for matching (TE,TF) using original distance at d = 10, but it can be improved to around 0.7 at a different d; and even though joint Isomap has the best power at 0.84, it can be further increased to around 0.9 by changing d and k. Because the model selection issue for hypothesis testing does not seem to affect the interpretation, we use the same parameters for different methods and data combinations here; also distance correlation offers an alternative performance measure independent of the embedding dimension and the matching method (except distance correlation on LLE still depends on d).

Next we repeat the same procedure to test three data sets matching and four data sets matching. After 100 Monte-Carlo runs, we present the mean distance correlation sum in Table [\ref=table:wiki2DCorr] and the mean testing power in Table [\ref=table:wiki2] at type 1 error level 0.05, showing only the highest matching power among Procrustes, CCA and joint MDS. The interpretation is similar to the two data sets matching example, which always favors our manifold matching algorithm. Note that the distance correlation sum for three data sets ranges from 0 to 3, and it ranges from 0 to 6 for four data sets matching.

We also observe that given a specific embedding algorithm, matching (TE,TF) always yields the highest distance correlation and matching power in two data sets matching, compared to other combinations of data; but adding additional graph matrix like GE and GF significantly degrades the matching power in three or four data sets matching, for all nonlinear algorithms except joint Isomap. This is probably because the network information is less reliable than the text feature; and the excellent matching performance achieved by the proposed manifold matching algorithm indicates its robustness against disparate and fallible data sources.

At last, for the model selection issue, we show two power surface plots in Figure [\ref=fig7] for matching (TE,GE) using Procrustes matching with joint Isomap and separate Isomap respectively. The mean power is calculated at type 1 error level 0.05 with respect to different neighborhood sizes from k = 10 to 30 and different dimension choices from d = 2 to 30, for 100 Monte-Carlo runs. Note that choosing k and d for the Swiss roll data is quite easy, because the neighborhood size has been validated to perform well and the embedding dimension equals the intrinsic true dimension (and the scree plot has a clear cut-off at d = 2); but for the real data, it is much more difficult to determine the optimal parameters without cross validation.

Nevertheless, Figure [\ref=fig7] shows that our approach is robust against model selection: the matching power of joint Isomap is quite stable with respect to the neighborhood size and the dimension choice, and we observe that joint neighborhood selection is consistently better than separate neighborhood selection. This phenomenon holds for joint MDS and CCA matching too, although the optimal parameters are not the same for different matching methods and different data combinations. Note that the optimal parameters for matching (TE,GE) using Procrustes matching and joint Isomap are roughly k = 15 with d > 15, which cannot be achieved by LLE/LTSA/Laplacian eigenmaps because they all require k > d in the nonlinear algorithm.

Concluding Remarks

In summary, we propose a nonlinear manifold matching algorithm using shortest-path distance and joint neighborhood. The algorithm is concise and intuitive, yet achieves superior and robust performance throughout the numerical experiments: it is able to significantly improve both the distance correlation and the testing power when matching data of distinct geometry, and does not perform worse than its linear counterpart when all involved data have similar geometry; it is robust against model selection such as dimension choice and neighborhood size, robust against noise, and easily extendable to match more than two data sets. Our arguments and experiments indicate that shortest-path distance and joint neighborhood selection are two main reasons behind the excellent matching performance.

Acknowledgment

This work was partially supported by National Security Science and Engineering Faculty Fellowship (NSSEFF), Johns Hopkins University Human Language Technology Center of Excellence (JHU HLT COE), and the XDATA program of the Defense Advanced Research Projects Agency (DARPA) administered through Air Force Research Laboratory contract FA8750-12-2-0303.

The authors would like to thank Y. Park and J. Vogelstein for their valuable comments and discussions in improving this paper.