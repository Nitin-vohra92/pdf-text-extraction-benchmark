A simple method for the evaluation of the information content and complexity in atoms. A proposal for scalability.

Introduction

Information-theoretic methods have been used extensively to study various systems in communications [\cite=ShannonArticle], physics [\cite=BBM], chemistry [\cite=SearsPHD], [\cite=PaperKP], biology [\cite=AdamiPLR], e.t.c. Applications in quantum systems, e.g. atoms ([\cite=GadreSearsEtAlPaper], [\cite=SenEdBook] and references therein), have been rewarding and applied with considerable success. Also Shannon information entropy has been connected with experiment, e.g. the ionization potential and dipole polarizability [\cite=PaperSPCM]. Related research in atoms employs Hartree-Fock wavefunctions, obtained numerically, which can be used as an input for the calculation of Shannon information entropies in position- and momentum- spaces according to the definitions:

[formula]

where ρ() and n() stand for the electron probability densities in position- and momentum- spaces respectively. Entropic Uncertainty Relations (EUR) [\cite=BBM] hold of the form

[formula]

(n = 3 for a 3-dimensional system). EUR are considered as an improvement compared with the usual Heisenberg uncertainty relations, in the following sense: First, one can derive Heisenberg's relations from EUR, whereas the inverse is not true and, second, the right-hand side of EUR does not depend on the state of the system, whereas Heisenberg's do depend. Another important issue is that the entropic sum S  =  Sr  +  Sk is scale-invariant i.e. it does not depend on the units chosen to measure position [formula] and momentum [formula]. Thus, S is an important dimensionless quantity that represents the information content of a quantum system in bits, if the base of the logarithm in the definition of Sr and Sk is 2, nats, if the base is e, and Hartleys, for base 10. Here, we use the natural logarithm.

An interesting universal property is: S = a + b ln N, where N is the number of particles of a quantum system. So far, it has been shown empirically to hold approximately for various quantum systems [\cite=GadreSearsEtAlPaper], [\cite=PaperMMP], [\cite=PaperMP2], such as atoms, nuclei, atomic clusters and bosons in a trap. These are systems with various numbers of particles N, ranging from a few to millions, and have various sizes, with constituent particles obeying different interactions and different statistics such as fermions and bosons. The parameters a, b depend on the system under consideration. S is connected with the kinetic energy [\cite=PaperMP]. Another important finding is the discovery that the application of external electric and magnetic fields to an atom influences the information content of electrons [\cite=FerezDehesa.Paper]. Complexity in atoms has been quantified, starting in [\cite=PaperCMP] and extending in [\cite=PaperPCMK]. Thus, in [\cite=PaperCMP], for the first time, complexity was evaluated as an index characterizing a quantum system, specifically an atom. The SDL (Shiner, Davison, Landsberg) [\cite=SDL.paper] and LMC (Lopez-Ruiz, Mancini, Calbet) [\cite=LMC.paper] statistical measures of complexity were employed, defined in such a way as to overcome the fact that sometimes entropy is not a proper measure of disorder and complexity. They are called statistical, because they are based on information entropy, calculated from probability densities. The question arises, whether one can predict that a system has the ability to organize itself, i.e. increase its complexity, as the number of particles N increases or another parameter changes, or it needs some external factor or agent towards self-organization (see discussion in [\cite=PaperPCMK]).

A difficult scientific and philosophical issue is to analyze a system in terms of interacting components (reductionism) or inversely, to derive the properties of the initial system from those of the constituents. The aim is to investigate quantitatively whether reductionism is correct or if, by moving to higher levels in the hierarchy of systems (bottom-up), new laws appear, which cannot be explained in terms of components lying lower in the hierarchy. For example, the detailed calculation of electronic properties of molecules is cumbersome or intractable. A real progress, in that case, became feasible in the past, when an effective hypothesis was made, i.e. the chemical bonding, resulting in an immense and practical understanding of molecules, with infinite applications, without the need to know or calculate all the details of the underlying electronic structure. An example of quantification of the order of the chemical bonding using Shannon information is [\cite=PaperKP]. Specifically, an information-theoretic comparison of Fermi and Coulomb electron pairs was carried out, employing a simplified probabilistic procedure.

Researchers could desire to understand the following hierarchy: Nucleons →   Nuclei →   Atoms →   Molecules →   Larger Structures (e.g. DNA). In each level of description, information is processed in different ways, with various degrees of freedom and constants being relevant. One could not be sure that he/she will ever be capable of crossing the border from one level to the next, even in principle. Pragmatically speaking, detailed numerical calculations are difficult for the above transition and probably useless. One could hope to find simple methods to do this, without compromising the important characteristics of systems. In this paper, we propose such a simple method for atoms, reproducing the main results obtained from more sophisticated and accurate models, leading to the same qualitative conclusions for self-organized complexity. The next molecular level might be reached if, for example, the probabilities of electrons to occupy basins in various molecules become available and used in a simplified probabilistic treatment of information and complexity, instead of more complicated calculations. At the moment, the electronic structure of molecules is obtained ab initio or from approximate calculations, which are quite involved and mostly carried out for a specific molecule. A simplified systematic approach is called for. As we proceed to higher levels in the hierarchy of systems, we should check whether the basic features, related to information and complexity, are preserved by a simplified approach trying to achieve a controlled scalability.

An approach in a spirit similar to our present work is the treatment of information and complexity by Bonchev in Ch. 4 of [\cite=BonchevBook]. In fact, our starting step to employ fractional occupation probabilities of atomic orbitals in order to evaluate the Shannon entropy in atoms is analogous with [\cite=BoncevKamenskaArticle], but, here, we elaborate further, by extending to SDL and LMC measures of complexity and Tsallis entropy. Another point of view is a survey of several molecular indices, based on evaluating graph complexity [\cite=BonchevBook]. We also mention that calculations of Shannon entropy for molecules have already been carried out using molecular wavefunctions [\cite=MinhhuyPaper], [\cite=MinhhuyPaper2]. A very recent calculation of information and complexity measures for H2  +   may serve as a promising starting point [\cite=KalidasH2]. The aim of [\cite=KalidasH2] is to clarify the nature of molecular bonding, employing information-theoretic and complexity concepts. Finally, one should not omit the comprehensive information-theoretic treatment of molecular systems in [\cite=NalewajskiBook] and references therein.

The outline of the paper is the following: In Section [\ref=sec:Definitions], we give various definitions of Information and Complexity measures, while in Section [\ref=sec:Methodology] we present our method and numerical results. Specifically, subsection [\ref=subsec:SDLcoeff] contains a new definition of a Pair of Order-Disorder Indices (PODI), characterizing quantum many-body systems [\cite=ChatzPanosPODI]. Section [\ref=sec:Discussion] contains a discussion and finally, in Section [\ref=sec:Conclusions], we display our main conclusions.

Definitions of Information and Complexity Measures

Let us consider a discrete probability distribution [formula]. The corresponding Shannon information entropy [\cite=ShannonArticle] is defined as:

[formula]

where [formula]. In the case of atoms, pi might be the occupation probability of a specific electron orbital. The maximum and minimum values of S, in this case, will be:

[formula]

where ν is the number of occupied orbitals, while Smin = 0 holds only if one of the pi's equals 1 and all the rest equal 0. We note that for continuous density distributions e.g. in atoms or other quantum many-body systems i.e. nuclei, atomic clusters and bosons in a trap, Smin and Smax obey the much more complicated inequalities of Gadre [\cite=GadreSearsEtAlPaper], [\cite=SenEdBook] instead of ([\ref=eq:ShannonDefMaxDiscrete]). These inequalities were verified in [\cite=PaperMMP]. S is a global measure of information in the sense that, by changing the order of the values [formula], there is no effect on the value of the sum in relation ([\ref=eq:ShannonDefinitionDiscrete]). On the contrary, Fisher's definition of information [\cite=FisherArticle], [\cite=FriedenBook]

[formula]

is called a local measure of information, because it contains the derivative of the continuous density distribution ρ(). The same argument can be extended for a discrete distribution, where the corresponding Fisher information is:

[formula]

where we put pν + 1 = 0. Here, one should use a specific sequence of the probabilities {pi} and the obvious choice is the ordering of atomic orbitals in electron configurations, dictated by nature. Recently in [\cite=PaperCMP], [\cite=PaperPCMK], the information entropy S = Sr + Sk for atoms, derived probabilistically, was used to calculate quantitatively complexity measures. First, the SDL measure [\cite=SDL.paper] was calculated as a function of Z, defined as:

[formula]

where Δ is the disorder of the system (Landsberg) [\cite=LandsbergBook], [\cite=LandsbergArticle]:

[formula]

and Ω is the order:

[formula]

In other words, Δ and Ω are the normalized measures of disorder and order respectively, with 0 < Δ < 1 and 0 < Ω < 1.

The SDL measure of complexity obeys the desired property: Γα,β  =  0 for both extreme cases of an ideal gas in complete disorder, S = Smax and a perfect crystal in complete order, S = 0. The interesting part is between complete order and complete disorder, which is intuitively satisfactory, i.e.

[formula]

The values α, strength of disorder, and β, strength of order, are still to be specified and play a crucial role to observe an increasing, decreasing or convex trend for Γα,β as a function of Δ or Ω or the number of particles N [\cite=PaperPCMK], [\cite=SDL.paper].

The particular values of α and β in ([\ref=eq:SDLdefinition]) might be chosen by imposing the condition that the values of the two complexity measures Γα,β and C are approximately the same. This comparison has been suggested in [\cite=PaperPCMK], where we obtained roughly α≃0 and β≃4 and has been observed that complexity increases with Z based on the trend of closed shells. However, maybe the solution is not unique and it would be better to find various regions of pairs (α,β) in the α  -  β plane, each one characterized by an index giving a different behavior (increasing, decreasing or convex) for complexity, as a function of N. A more detailed search for the proper values of (α,β) is being carried out in a paper in preparation [\cite=ChatzPanosPODI], employing more accurate continuous densities in position and momentum spaces applied in atoms and other quantum many-body systems as well. This procedure may lead to a quantitative answer, whether the system can show self-organization without the intervention of external factors. The effect of external electric and magnetic fields on the information content of some excited states of hydrogen was studied in [\cite=FerezDehesa.Paper]. A first step to evaluate the effect of the environment is to study confined atoms [\cite=SenPaper].

Another measure of complexity is the LMC measure [\cite=LMC.paper]:

[formula]

where S is the usual information entropy, while D is the disequilibrium, i.e. the distance of the specific non-equiprobable distribution pi from a uniform distribution [formula], where ν is the number of occupied orbitals of the atom. Thus, for the discrete case:

[formula]

The definition of the Onicescu information energy for a continuous distribution is [formula] and for a discrete one is:

[formula]

In the literature, the quantity E is called Onicescu's information energy [\cite=OnicescuBook], [\cite=OnicescuStefanescuBook], although it does not have the dimension of energy. This name has been given by analogy with thermodynamics, because E attains a minimum for a uniform distribution of equal probabilities -total disorder. It can be employed as a finer measure of information content [\cite=PaperCMP]. We also note that the continuous generalization of D is [formula], the same with E. D is an experimentally measurable quantity [\cite=HymanPaper], known also as quantum self-similarity [\cite=BorgooPaper2].

The definition of information content given above can be extended employing the Tsallis entropy [\cite=Tsallis.paper]:

[formula]

which, for q  →  1 goes to the Shannon information entropy and for q = 2 to 1 minus the Onicescu information energy.

We choose the specific value of the entropy index q for atoms, using the Tsallis prescription [\cite=PrivTsallisPanos]. Specifically, Tq is plotted as a function of ln Z and the best value of q is found when the closest trend to a linear relation of Tq with ln Z is obtained.

Methodology and Numerical Results

We construct a fractional occupation probability distribution {[formula] of electrons in atomic orbitals for each neutral atom and the corresponding value of Z, in a way similar to our previous work for atomic nuclei ([\cite=PaperCP]). For example, the electron configuration for Ca (Z = 20) is

[formula]

We obtain the corresponding normalized probability distribution pi by dividing the superscripts (number of electrons occupying the particular orbitals) by Z, i.e. [formula], [formula], [formula], e.t.c. Then, the values {pi}, summing up to 1, are inserted for the fixed value of Z = 20 into previous relations (1) to (10) and the results are shown in Table 1, where Si =  - pi ln pi, [formula], Ei = p2i, [formula] and are summed up at the last row of the Table. This Table is presented for pedagogical reasons, so that anybody can understand the simplicity of the method described here and reproduce our results for all values of Z.

Fractional occupation probabilities {pi} for Z=1 to 105, obtained as described above, are employed as an input into the formulas of section [\ref=sec:Definitions]. Thus, we obtain the functions S(Z), Smax(Z) (Fig. [\ref=fig:graphSh]), I(Z) (Fig. [\ref=fig:graphFiSh]), Δ(Z), Ω(Z) (Fig. [\ref=fig:graphOrderDis]), Γα,β(Z), C(Z) (Fig. [\ref=fig:FigabCalc]), D(Z) (Fig. [\ref=fig:graphD]), E(Z) (Fig. [\ref=fig:graphOnSh]) and Tq(Z) (Fig. [\ref=fig:graphTsallis]). A detailed table with numerical values for the above functions is available in an online version of the present paper.

Shannon Information Entropy

Employing the maximization of R2 (correlation coefficient) criterion, we calculate the best fit of the form S(Z) = aS  +  bS ln Z to our numerical results for S(Z). The resulting linear relation is Sln(Z) =  - 0.1726 + 0.6034 ln Z, plotted together with S(Z) and Smax(Z) in Fig. [\ref=fig:graphSh]. The two latter curves correspond to our present approach employing fractional occupation probabilities and are compared in Fig. [\ref=fig:ScSd], with S(Z) obtained previously [\cite=PaperCMP] with H-F densities (continuous) in position- and momentum- spaces.

Tsallis Information Entropy

Special attention is devoted to the calculation of Tsallis entropy ([\ref=eq:TsallisDefinition]) with the entropy index q as a parameter. The plots of Tq(Z) for q=0.5, 0.75, 1, 1.25, and 1.5 are shown in Fig. [\ref=fig:graphTsallis].

We assume that Tq(Z) is a linear function of ln Z of the form Tq(Z)  =  aT  +  bT ln Z. Thus we follow the Tsallis prescription [\cite=PrivTsallisPanos] described in Section [\ref=sec:Definitions]. We calculate numerically the value of q, which gives the greatest correlation value R2, for the linear fit of Tq(Z). The best fit value is q = 1.031 (Fig. [\ref=fig:graphTsallisRegr] and Fig. [\ref=fig:graphR2vsQ], where R2 is plotted versus q) and the fitted expression is Tln  =   - 0.13147  +  0.57229 ln Z. This value of q is close to q = 1, corresponding to the Shannon information entropy. We can conclude that atoms are extensive systems, in the sense that deviation of q from 1 indicates non-extensivity [\cite=Tsallis.paper]. This "optimal" value of q is obtained easily, employing our simple method with discrete {pi}, to be contrasted with the use of H-F densities, which would involve integrals of the continuous quantity pq().

The SDL complexity coefficients (α,β) defined as a Pair of Order-Disorder Indices (PODI)

As mentioned in Sec. [\ref=sec:Definitions], we may calculate the particular values of α and β by imposing the condition that the data sets of Γα,β and C are approximately the same, in the sense that the sum of squared errors Σε2 between these two data sets, by varying the α and β, coefficients is minimum. The values calculated are α  =  0.085 and β  =  1.015 and the corresponding curves for Γα,β(Z) and C(Z) are plotted together in Fig. [\ref=fig:FigabCalc]. The similarity of the two patterns is obvious. These values can be compared with our rough guess (α≃0, β≃4) in [\cite=PaperPCMK] which, however, were obtained with continuous H-F atomic densities, jointly, in position- and momentum-spaces. Our prescription for the proper values of (α,β) enables us to define them as a Pair of Order-Disorder Indices (PODI), which quantifies the contribution of order versus disorder to the complexity of any quantum system [\cite=ChatzPanosPODI].

In Fig. [\ref=fig:SDL4] we plot Γα,β(Z) for four typical pairs (α,β), i.e. (1,1), (1, [formula]), ([formula],0) and (0,4) specified in [\cite=SDL.paper] and compare with the corresponding curves of [\cite=PaperCMP]. It is seen that both, continuous and discrete cases, lead on the average to the same conclusion, that the atom cannot grow in complexity as Z increases, by observing all the values of Z. The same trend has been observed in [\cite=PaperCMP], in contrast to [\cite=PaperPCMK], where an increasing trend was obtained. This difference can be resolved by stating that, if one observes the closed shells, as in [\cite=PaperPCMK], the trend is increasing, while by inspecting all atoms, the trend is, on the average, not increasing, as in [\cite=PaperCMP]. In [\cite=BorgooPaper] the authors used improved electron densities in position-space by introducing relativistic corrections and state that complexity increases with Z for position-space. An analogous work leading to the same conclusion is [\cite=SanudoPaper]. However, we may note that such results cannot be considered complete or conclusive, because a proper treatment of this issue should involve both, position- and momentum- spaces. We mention, for example, the well known seminal research of Gadre et al [\cite=GadreSearsEtAlPaper], [\cite=SenEdBook] and further more recent work [\cite=PaperMP2], [\cite=PaperCMP], where both spaces are taken into account for S. We stress that the resulting momentum-space integrals should be treated carefully for numerical accuracy.

The final pair (α = 0.085,β = 1.015) characterizes the strength of order of atoms β versus the strength of disorder α throughout the periodic table and can serve as a Pair of Order-Disorder Indices (PODI) for any quantum many body-system. We observe that β  ≫  α, which indicates that atoms are "ordered" systems, or more accurately, they are more "ordered" than "disordered". This fits well with the fact that we can visualize the creation of atoms by putting electrons one-by-one in well defined orbitals. Our final conclusion is: Atoms are "ordered" systems, which do not grow in complexity with Z, at least in the context of the present work. Perhaps, future research might check and/or modify our present conclusion, if more accurate densities are employed and treated properly. The merit of our definition for the PODI pair (α,β) is under investigation, by a comparative study of its application in several quantum systems [\cite=ChatzPanosPODI].

Discussion

It is seen in Fig. [\ref=fig:ScSd] that all local minima of S(Z) obtained in the present work occur at the same values of Z, as the corresponding discontinuities of the slope of S(Z) found in [\cite=PaperCMP] with the more accurate continuous Roothaan-Hartree-Fock (RHF) wavefunctions [\cite=RHF_AWF_Values.paper]. This fits well with our expectations for atoms with closed shells of electrons, which can be considered as more compact than neighboring atoms and are expected to display smaller values of information content. Specifically, in the present work, we obtain local maxima for S(Z) for Z=6, 15, 23, 25, 35, 40, 43, 58, 62, 64, 77, 93, 96, and 105 and local minima for Z=10, 18, 24, 29, 36, 41, 46, 59, 63, 70, 79, 94, and 102.

We can also observe in Fig. [\ref=fig:graphShDiPo] and Fig. [\ref=fig:graphShIoPo] that S(Z) obtained with our simplified method correlates well with experimental data for atomic dipole polarizability αD [\cite=DipPolBook] and the inverse of the first ionization potential of atoms I.P. [\cite=IonPotSite]. It is seen that, for closed shells atoms, the local minima of αD (and, accordingly, the local maxima of I.P.) coincide either with the local minima of S for Z = 10, 18 and 36 or with discontinuities in the slope of S for Z = 54 and 86. In Fig. [\ref=fig:fisherpol] and Fig. [\ref=fig:fisherip], we also display the correlation of Fisher information I(Z) with αD and I.P. respectively. In order to appreciate the significance of those Figures, one should take into account the reciprocal behavior of S(Z) and I(Z).

In Fig. [\ref=fig:graphOnSh] we see the expected reciprocal behavior of S(Z) and Onicescu information E(Z), while in Fig. [\ref=fig:graphFiSh] a local measure of information, Fisher information I(Z), is compared with a global one, S(Z). Here, one verifies that Fisher Information is a more sensitive measure than S(Z), providing more detailed structure, as a function of Z. It is obvious from our Figures , that I(Z) shows local maxima in the form of abrupt increases for values of Z, corresponding to just mild discontinuities of the slope of S(Z) (continuous) and local minima of S(Z) (discrete).

Finally, the Disequilibrium D(Z) is shown in Fig. [\ref=fig:graphD]. An additional bonus of that figure is that D(Z) shows the same pattern (obvious by simple inspection) as Γα,β(Z) and C(Z) of Fig. [\ref=fig:FigabCalc]. Thus, a third measure of complexity emerges.

Our present approximate approach is promising as a first information-theoretic step to accomplish the transition from one level of nature to the next, e.g. from atoms to molecules, indicating an analogous treatment for molecules. Also, our proposal is pragmatic and contributes towards a unification of physical systems, from the point of view of self-organization (organized complexity) using information theory based on a probabilistic description with minimal computational cost.

The number of particles of atoms, seen as systems of electrons, is mostly Z≃102. One might try, for example, to evaluate, by simplifying, S and C for systems with electrons up to Z = 106 in ultra-large-scale electronic structure theory [\cite=TakeoHoshiPaper].

A possible quibble might be the following. Here, we consider fractional occupation probabilities of electronic configurations in atomic orbitals. The standard, maybe ideal, method would be, first, to diagonalize the density matrix ρ(,') of electrons, in order to obtain the corresponding natural occupation numbers and natural orbitals, and, second construct [formula] to be inserted into the formulas. Thus, the effect of electron correlation could be included in the density matrix. Again, we may argue that our alternative approach is much more feasible and is, in this sense, a better candidate for scalability for larger systems. Here, by "larger" we mean systems with more constituent particles or entities. It may also serve as a change of our way of thinking about complexity: one must not hesitate to carry out calculations for complex systems, but, instead, has to try to find working effective approximations in order to quantify complexity.

We note that both definitions of complexity, Γα,β (SDL) and C (LMC) are rather not final, but in view of the inability to give an ideal or single definition, we might say that both capture basic and desirable features, which a good measure of complexity should show. This is usually the case in physics, in order to start thinking about a new concept. Furthermore, we calculate the optimal pair (α,β) (disorder versus order parameter), by requiring that two different and reasonable functions Γα,β(Z) and C(Z) exhibit approximately the same pattern. The fact that the obtained patterns are extremely similar, enhances the reliability of our approach.

Conclusions

We propose a simplified method to quantify the information content and complexity in atoms. It is validated by obtaining significant similarities between several basic features (experimental and theoretical) in comparison with more sophisticated approaches. This procedure may be tested for scalability, by applying it to more complicated systems than the atom, i.e. with more components. We find the entropic index q = 1.031, indicating that atoms are extensive systems. We also present a prescription to find the proper values of the strength of disorder α and order β, defined as a Pair of Order-Disorder Indices (PODI) characterizing any quantum many-body system [\cite=ChatzPanosPODI]. It is seen that for atoms β  ≫  α. Finally, we conclude that, at least in the context of a non-relativistic treatment of atoms, taking into account both, position- and momentum- spaces: "Atoms are "ordered" systems, which do not grow in complexity as Z increases".

We note that the above conclusion has been tested for neutral atoms in non-excited states, employing two measures of complexity: SDL and LMC and a third one, the Disequilibrium D, emerging from this work.

Acknowledgments

K. Ch. Chatzisavvas is supported by a Post-Doctoral Research Fellowship of the Hellenic State Institute of Scholarships (IKY).

Figures

Tables