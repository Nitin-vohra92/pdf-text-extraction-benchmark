A Novel Clustering Algorithm Based Upon Games on Evolving Network

Introduction

Cluster analysis is an important branch of Pattern Recognition, which is widely used in many fields such as pattern analysis, data mining, information retrieval and image segmentation. For the past thirty years, many excellent clustering algorithms have been presented, say, K-means [\cite=MacQueen1967], C4.5 [\cite=Quinlan1993], support vector clustering (SVC) [\cite=Ben-Hur2001], spectral clustering [\cite=Ng2002], etc., in which the data points for clustering are fixed, and various functions are designed to find separating hyperplanes. In recent years, however, a significant change has been made. Some researchers thought about that why not those data points could move by themselves, just like agents or something, and collect together automatically. Therefore, following their ideas, they created a few exciting algorithms [\cite=Rhouma2001] [\cite=Folino2002] [\cite=Merwe2003] [\cite=Labroche2003] [\cite=Cui2006], in which data points move in space according to certain simple local rules preset in advance.

Game theory came into being with the book named "Theory of Games and Economic Behavior" by John von Neumann and Oskar Morgenstern [\cite=Neumann1944] in 1940. In this period, Cooperative Game was widely studied. Till 1950's, John Nash published two well-known papers to present the theory of non-cooperative game, in which he proposed the concept of Nash equilibrium, and proved the existence of equilibrium in a finite non-cooperative game [\cite=Nash1950] [\cite=Nash1951]. Although non-cooperative game was established on the rigorous mathematics, it required that players in a game must be perfect rational or even hyper-rational. If this assumption could not hold, the Nash equilibrium might not be reached sometimes. On the other hand, evolutionary game theory [\cite=Smith1976] stems from the researches in biology which are to analyze the conflict and cooperation between animals or plants. It differs from classical game theory by focusing on the dynamics of strategy change more than the properties of strategy equilibria, and does not require perfect rational players. Besides, an important concept, evolutionarily stable strategy [\cite=Smith1976] [\cite=Smith1973], in evolutionary game theory was defined and introduced by John Maynard Smith and George R. Price in 1973, which was often used to explain the evolution of social behavior in animals.

To the best of our knowledge, the problem of data clustering has not been investigated based on evolutionary game theory. So, if data points in a dataset are considered as players in games, could clusters be formed automatically by playing games among them? This is the question that we attempt to answer. In our clustering algorithm, each player hopes to maximize his own payoff, so he constantly adjusts his strategies by observing neighbors' payoffs. In the course of strategies evolving, some strategies are spread in the network of players. Finally, some parts will be formed automatically in each of which the same strategy is used. According to different strategies played, data points in the dataset can be naturally collected as several different clusters. The remainder of this paper is organized as follows: Section 2 introduces some basic concepts and methods about the evolutionary game theory and evolutionary game on graph. In Section 3, the model based upon games on evolving network is proposed and described specifically. Section 4 gives three algorithms based on this model, and the algorithms are elaborated and analyzed in detail. Section 5 introduces those datasets used in the experiments briefly, and then demonstrates experimental results of the algorithms. Further, the relationship between the number of clusters and the number of nearest neighbors is discussed, and three edge-removing-and-rewiring (ERR) functions employed in the clustering algorithms are compared. The conclusion is given in Section 6.

Related work

Cooperation is commonly observed in genomes, cells, multi-cellular organisms, social insects, and human society, but Darwin's Theory of Evolution implies fierce competition for existence among selfish and unrelated individuals. In past decades, many efforts have been devoted to understanding the mechanisms behind the emergence and maintenance of cooperation in the context of evolutionary game theory.

Evolutionary game theory, which combines the traditional game theory with the idea of evolution, is based on the assumption of bounded rationality. On the contrary, in classical game theory players are supposed to be perfectly rational or hyper-rational, and always choose optimal strategies in complex environments. Finite information and cognitive limitations, however, often make rational decisions inaccessible. Besides, perfect rationality may cause the so-called backward induction paradox [\cite=Pettit1989] in finitely repeated games. On the other hand, as the relaxation of perfect rationality in classical game theory, bounded rationality means people in games need only part rationality [\cite=Simon1996], which explains why in many cases people respond or play instinctively according to heuristic rules and social norms rather than adopting the strategies indicated by rational game theory [\cite=Szabo2007]. So, various dynamic rules can be defined to characterize the boundedly rational behavior of players in evolutionary game theory.

Evolutionary stability is a central concept in evolutionary game theory. In biological situations the evolutionary stability provides a robust criterion for strategies against natural selection. Furthermore, it also means that any small group of individuals who tries some alternative strategies gets lower payoffs than those who stick to the original strategy [\cite=Weibull1995]. Suppose that individuals in an infinite and homogenous population who play symmetric games with equal probability are randomly matched and all employ the same strategy A. Nevertheless, if a small group of mutants with population share ε∈(0,1) who plays some other strategy appear in the whole group of individuals, they will receive lower payoffs. Therefore, the strategy A is said to be evolutionary stable for any mutant strategy B, if and only if the inequality, E(A,(1 - ε)A + εB) > E(B,(1 - ε)A + εB), holds, where the function E(  ·  ,  ·  ) denotes the payoff for playing strategy A against strategy B [\cite=Broom2000].

In addition, the cooperation mechanism and spatial-temporal dynamics related to it have long been investigated within the framework of evolutionary game theory based on the prisoner's dilemma (PD) game or snowdrift game which models interactions between a pair of players. In early days, the iterated PD game was widely studied, in which a player interacted with all other players. By round robin interactions among players, strategies in the population began to evolve according to their payoffs. As a result, the strategy of unconditional defection was always evolutionary stable [\cite=Hofbauer1998] while pure cooperators could not survive. Nevertheless, the Tit-for-Tat strategy is evolutionary stable as well, which promotes cooperation based on reciprocity [\cite=Axelrod1981].

Recently, evolutionary dynamics in structured populations has attracted much attention, where the structured population denotes an infinite and well-mixed population which simplifies the analytical description of the evolution process. In real populations, individuals are more likely affected by their neighbors than those who are far away, but the spatial structure of population is omitted in the iterated PD game. To study the spatial effects upon strategy frequencies in the population, Nowak and May [\cite=Nowak1993] have introduced the spatial PD game, in which players are located on the vertices of a two-dimensional lattice, whose edges represent connections among the corresponding players. Instead of playing with all other contestants, each player only interacts with his neighbors. Without any strategic complexity the stable coexistence of cooperators and defectors can be achieved. However, the model presented in [\cite=Nowak1993] assumes a noise free environment. To characterize the effect of noise, Szabó and Toke [\cite=Szabo1998] have presented a stochastic update rule that permits irrational choice. Besides, Perc and Szolnoki [\cite=Perc2008] account for social diversity by stochastic variables that determine the mapping of game payoffs to individual fitness. Furthermore, many other works centered on the lattice structure have also been done. For example, Vukov and Szabó [\cite=Vukov2005] have presented a hierarchical lattice and shown that for different hierarchical levels the highest frequency of cooperators may occur at the top or middle layer. For more details about evolutionary games on graphs, see [\cite=Szabo2007] [\cite=Doebeli2005] [\cite=Nowak2006] and references therein.

Yet, as imitations of real social networks, the evolutionary game on lattices assumes that there is a fixed neighborhood for each player. Nevertheless, this assumption does not always hold for most of real social networks. Unlike models mentioned above, the relationships among players (data points) in our model are represented by a weighted and directed network, which means that players are not located on a regular lattice any more. And the network will evolve over time because each player is allowed to apply an edges-removing-and-rewiring (ERR) function to change his connections between him and his neighbors. Furthermore, the payoff matrix of any two players in the proposed model is also time-varying instead of a constant payoff matrix, for instance, the payoff matrix in PD game. As a consequence, when the evolutionarily stable strategies emerge in the network, it will be observed that only a few players (data points) receive considerable connections, while most of them have only one connection. Naturally, players (data points) are divided into several parts (clusters) according to their evolutionarily stable strategies.

Proposed model

Assume a set [formula] with N players, [formula], which are distributed in a m-dimensional metric space. In this metric space, there is a distance function [formula], which satisfies the condition that the closer any two players are, the smaller the output is. Based on the distance function a distance matrix is computed whose entries are distances between any two players. Next, a weighted and directed k-nearest neighbor (knn) network, G0(,E0,d), is formed by adding k edges directed toward its k nearest neighbors for each player, which represents the initial relationships among all players.

If there is a set [formula] with N players, [formula], the initial weighted and directed knn network, G0(,E0,d), is created as below.

[formula]

Here, players in the set [formula] correspond to vertexes in the network G0(,E0,d); directed edges in the network represent certain relationships established among players and the distances denote the weights over edges; the function, argmink(  ·  ), is to find k nearest neighbors of a player, which construct a neighbor set, Γ0(i).

It is worth noting that the distance between a player i and himself, d(i,i), is zero according to the defined distance function, which means that at the beginning he is one of his k nearest neighbors. So there is an edge between the player i and himself, namely a self-loop. In practice, the distance is set by d(i,i) = 1.

When the initial network G0 is established, we can define a evolutionary game, Ω  =  {,G0,S0,U0}, on it further.

An evolutionary game Ω  =  {,G0,S0,U0} on a network G0 is a 4-tuple: [formula] is a set of players; G0 represents the initial relationships among players; [formula] represents a set of players' strategies; [formula] represents a set of players' payoffs. In each round, players choose theirs strategies simultaneously, and each player can only observe its neighbors' payoffs, but does not know the strategy profile of anyone of all other players in [formula]. Finally, all players update their strategy profiles synchronously.

In the proposed model, assume each player in [formula] sets up a group, and hopes to maximize the payoff of his own group in order to attract more players to join. At the same time, he also joins k groups set up by other players, so the initial strategy set s0(i)∈S0 of a player i is defined as his neighbor set, s0(i) = Γ0(i). However, it is worth noting that his preference to join each group is changeable, whose initial value is given below,

[formula]

where P0(i) is the preference set, and the symbol |  ·  | denotes the cardinality of a set. Thus, a player's payoff may be defined as follows.

After a player i chooses his strategies and corresponding preferences, he receives a payoff u0(i),

[formula]

where Deg0(j) represents the degree of a player j in the neighbor set, and the degree is a sum of the indegree and outdegree.

When all players have received their payoffs, each one will check his neighbors' payoffs, and apply an ERR function Bi(  ·  ) to change his connections and update his neighbor set.

The ERR function Bi(  ·  ) is a function of payoffs, whose output is a set with k elements, i.e., an updated neighbor set Γ1(i) of a player i.

[formula]

where θ0(i) is a payoff threshold, Υ0(i) is called an extended neighbor set, and the function argmaxk(  ·  ) is to find k neighbors with the first to the k-th largest payoffs in the union [formula].

The ERR function Bi(  ·  ) expands the view of a player i, and makes him able to observe payoffs of players in the extended neighbor set, which provides a chance to find players with higher payoffs around him. If no players with higher payoffs are found in the extended neighbor set, i.e., min({u0(j),j∈Γ0(i)})  ≥  max({u0(h),h∈Υ0(i)}), then the output of the ERR function is Γ1(i) = Bi(û0(i)) = Γ0(i). Otherwise, a neighbor with the minimal payoff will be removed together with the corresponding edge from the neighbor set and the edge set, and replaced by a found player with larger payoff. This process is repeated till the payoffs of unconnected players in the extended neighbor set are no larger than those of connected neighbors. Since the connections among players, namely the edge set E0 in the network G0(,E0,d), are changed by the ERR function, the network Gt(,Et,d) will begin to evolve over time, when [formula]. As such, after the ERR function is applied, the new preference set Pt(i) of a player i needs to be adjusted.

The new preference set of a player i∈ is formed by means of the below formulation.

[formula]

Then, the player adjusts his preference set Pt(i) as follows. First, he identifies the neighbor m with maximal payoff in his neighbor set,

[formula]

Next, each element in the preference set is taken its square root and the preference pt(i,m) of joining the group built by the neighbor m becomes negative,

[formula]

Further, let [formula], thus, the updated preference set is,

[formula]

After the preference set of each player has been adjusted, an iteration of the model is completed. In conclusion, when [formula], the network representing relationships among players begins to evolve over time, which also makes a player's strategy set and payoff set become time-varying. Therefore, the game on evolving network Gt(,Et,d) is rewritten as Ω  =  {,Gt,St,Ut}.

[formula]

As the model is iterated, some strategies are spread in the evolving network, which are played by a great number of players. In other words, a certain strategy or several strategies in the strategy set st(i)∈St will be always played by the player i with the maximal preference.

If a player i∈ always or periodically chooses a strategy t(i,j)∈st(i) with the maximal preference max({pt(i,j),j∈Γt(i)}),

[formula]

then the strategy t(i,j)∈st(i) is called the evolutionarily stable strategy (ESS) of the player i∈. Here, the variable T is a constant period.

As a consequence, each player in the network will choose one of evolutionarily stable strategies as his strategy and he is not willing to change his strategy unilaterally during the iterations.

Algorithm and analysis

In this section, at first three different ERR functions (B1i(  ·  ),B2i(  ·  ),B3i(  ·  )) are designed, and then three clustering algorithms (EG1, EG2, EG3) based on them are established. Finally, the clustering algorithm is elaborated and analyzed in detail.

Clustering algorithms

Assume an unlabeled dataset [formula], in which each instance consists of m features. In the clustering algorithms, the relationships among all data points are represented by a weighted and directed network Gt(,Et,d), and each data point in the dataset is considered as a player in the proposed model, who adjusts his strategy profile in order to maximize his own payoff by observing other players' payoffs in the union [formula].

According to the proposed model, after a distance function [formula] is selected, the initial connections of data points, G0(,E0,d), are constructed by means of Definition 1. Then, the initial payoff set U0 of data points is computed step by step. Finally, an ERR function is applied to explore in a neighborhood of each data point, which changes neighbors in the neighbor set of the data point. Thus, a network Gt(,Et,d) will be evolving when [formula].

However, different ERR functions provide different exploring capacities for data points, i.e., the observable areas of data points depend on an ERR function. As such, there is no doubt that the obtained results vary when different ERR functions are employed. Here, three ERR functions (B1i(  ·  ),B2i(  ·  ),B3i(  ·  )) are designed, and three clustering algorithms based on them are constructed respectively.

Algorithm EG1:

In Algorithm EG1, an ERR function B1i(  ·  ) that is realized most easily is used. This function always observes an extended neighbor set formed by ⌈η  ×  |Γt - 1(i)|⌉ neighbors of a data point i, where the symbol ⌈  ·  ⌉ is to take an integer part of a number satisfying the integer part is no larger than the number, and the variable η is called a ratio of exploration, η∈[0,1]. According to Definition 4, a payoff threshold θ1t - 1(i) is set by θ1t - 1(i) = findα({ut - 1(j),j∈Γt - 1(i)}),α  =  ⌈(1 - η)  ×  |Γt - 1(i)|⌉ firstly, where the function findα(  ·  ) is to find the α-th largest payoff in the neighbor set Γt - 1(i). As such, the set Γt - 1(i) is divided into two sets naturally:

[formula]

Then, based on the set Γ+t - 1(i), the extended neighbor set is built, [formula]. Further, the observable payoff set of the data point is written as,

[formula]

At last, the ERR function B1i(  ·  ) is applied, which means that the edges connecting to the neighbors with small payoffs are removed and new edges are created between the data point and found players with larger payoffs. Hence, his new neighbor set is

[formula]

Algorithm EG2:

Unlike Algorithm EG1, the ERR function B2i(  ·  ) in Algorithm EG2 adjusts the number of neighbors dynamically to form an extended neighbor set instead of the constant number of neighbors in Algorithm EG1. Furthermore, the payoff threshold θ2t - 1(i) is set by the average of neighbors' payoffs, [formula]. Next, the set Γ+t - 1(i) is formed, Γ+t - 1(i) = {j|ut - 1(j)  ≥  θ2t - 1(i),j∈Γt - 1(i)}, and then the new neighbor set is achieved by means of the ERR function B2i(  ·  ). In the case, when the payoffs of all neighbors are equal to the payoff threshold θ2t - 1(i), the output of the ERR function is Γt(i) = B2i(ût - 1(i)) = Γt - 1(i). This may be viewed as self-protective behavior for avoiding a payoff loss due to no enough information acquired.

Algorithm EG3:

The ERR function B3i(  ·  ) used in Algorithm EG3 provides more strongly exploring capacities for the data points with small payoffs than that for those with larger payoffs. Generally speaking, for maximizing their payoffs, players with small payoffs often seems radical and show stronger desire for exploration, because this is the only way to improve their payoffs. On the other hand, those players with large payoffs look conservative for protecting their payoff gotten.

Formally, the ratio of exploration γ(i) of a data point i∈ is given as below:

[formula]

Thus, the data point i∈ can observe an extended neighbor set that is formed by ⌈γ(i)  ×  |Γt - 1(i)|⌉ neighbors. Further, the payoff threshold is set by θ3t - 1(i) = findβ(i)({ut - 1(j),j∈Γt - 1(i)}),β(i) = ⌈(1 - γ(i))  ×  |Γt - 1(i)|⌉ and then the new neighbor set Γt(i) is built according to the ERR function B3i(  ·  ).

The ERR function brings about changes of the connections among data points, so that the preferences need to be adjusted in terms of Definition 5. When the evolutionarily stable strategies appear in the network, the clustering algorithm exits. In the end, the data points using the same evolutionarily stable strategy are gathered together as a cluster, and the number of evolutionarily stable strategies indicates the number of clusters.

Analysis of algorithm

The process of data clustering in the proposed algorithm can be viewed an explanation about the group formation in society. Initially, each data point (a player) in the dataset establishes a group which corresponds to an initial cluster at the same time that he joins other groups built by k other players. As such, the preference p(i,j) may be explained as the level of participation; Deg(i) represents the total number of players in a group; 1 / d(i,j) denotes the position that the player occupies in a group in order to identify a player is a president or an average member, and a player i usually occupies the highest position in his own group; the total payoff u(i) of a player i is viewed as the attraction of a group. According to Definition 3, the reward of a player i is associated directly with the preference, the total number of players and his position in a group. If a player i who occupies an important position joins a group with maximal preference, and the number of players in the group is considerable, then the reward R(i,j) that the player receives is also large. On the other hand, for a player h who is an average member in the same group, i.e., 1 / d(i,j) > 1 / d(h,j), although his preference to join this group is as same as that of the player i, p(i,j) = p(h,j), his reward acquired from this group is smaller than that of the player i by means of Eq. 3. This seems unfair, but it is consistent with the phenomena observed in society. In addition, the total payoff ut(i) of the player i is the sum of his all rewards, [formula].

Certainly, each player is willing to join a group with large attraction, and quit a group with little attraction, as is done by an ERR function. Next, the player finds the group with the largest attraction and increases the level of participation, namely the preference pt(i,j) at the same time that other preferences are decreased. To adjust the preference set Pt(i) of a player i, the Grover iteration G in the quantum search algorithm [\cite=Grover1997], a well-known algorithm in quantum computation[\cite=Nielsen2000], is employed, which is a way to adjust the probability amplitude of each term in a superposition state. By adjustment, the probability amplitude of the wanted is increased, while the others are reduced. This whole process may be regarded as the inversion about average operation [\cite=Grover1997]. For our case, each element in the preference set needs to be taken its square root first, and then the average Avet(i) of all square roots is obtained. Finally, all values are inverted about the average. There are three main reasons that we select the modified Grover iteration as the updating method of preferences: (a) the sum of preferences updated retains one, [formula], (b) a certain preference updated in a player's preference set is much larger than the others, [formula], and (c) it helps players' payoffs to be a power law distribution, in which only a few players' payoffs are far larger than others' after iterations. Moreover, this is consistent with our observations in society, i.e., a player will change the level of participation obviously after he takes part in activities held by neighbors' groups. In other words, for the group with large attraction he shows higher level of participation, whereas he hardly joins those groups with little attraction.

After the model is iterated several times, a player i∈ will find the most attractive group for him, and join this group with the largest preference, max({pt(i,j),j∈Γt(i)}). Hence, only a few groups are so attractive that almost all players join them. On the other hand, most of groups are closed due to a lack of players. Those lucky survivals not only attract many players but also those players in the groups show the highest level of participation. Furthermore, these surviving groups are the clusters formed by players (data points) automatically, where the players (data points) in a cluster play the same evolutionarily stable strategies, and the number of surviving groups is also the number of clusters.

Experiments and Discussions

To evaluate these three clustering algorithms, five datasets are selected from UCI repository [\cite=Blake1998], which are Soybean, Iris, Wine, Ionosphere and Breast cancer Wisconsin datasets, and experiments are performed on them.

Experiments

In this subsection, firstly these datasets are introduced briefly, and then the experimental results are demonstrated. The original data points in above datasets all are scattered in high dimensional spaces spanned by their features, where the description of all datasets is summarized in Table [\ref=tab:2]. As for Breast dataset, some lost features are replaced by random numbers, and the Wine dataset is standardized. Finally, this algorithm is coded in Matlab 6.5.

Throughout all experiments, data points in a dataset are considered as players in games whose initial positions are taken from the dataset. Next, the network representing initial relationships among data points are created according to Definition 1, after a distance function is selected. This distance function only needs to satisfy the condition that the more similar data points are, the smaller the output of the function is. In the experiments, the distance function applied is as following:

[formula]

where the symbol [formula] represents L2-norm. The advantage of this function is that it not only satisfies above requirements, but also overcomes the drawbacks of Euclidean distance. For instance, when two points are very close, the output of Euclidean distance function approaches zero, as may make the computation of payoff fail due to the payoff approaching infinite. Nevertheless, when Eq.(15) is selected as the distance function, it is more convenient to compute the players' payoffs, since its minimum is one and the reciprocals of its output are between zero and one, 1 / d(i,j)∈[0,1]. In addition, the parameter σ in Eq.(15) takes one and the distance between a data point and itself is set by d(i,i) = 1. As is analyzed in 4.2, a data point i occupies the highest position in the group established by himself.

Three clustering algorithms are applied on the five datasets respectively. Because the capacity of exploration of an ERR function depends in part on the number k of nearest neighbors, the algorithms are run on every dataset at different numbers of nearest neighbors. Those clustering results obtained by three algorithms are compared in Fig. [\ref=fig:5], in which each point represents a clustering accuracy. The clustering accuracy is defined as below:

csti is the label which is assigned to a data point i in a dataset by the algorithm, and ci is the actual label of the data point i in the dataset. So the clustering accuracy is [\cite=Erkan2006]:

[formula]

where the mapping function map(  ·  ) maps the label got by the algorithm to the actual label.

As is shown in Fig. [\ref=fig:5], the clustering results obtained by Algorithm EG1 and EG2 are similar at different numbers of nearest neighbors. As a whole, the results of Algorithm EG1 are a bit better than that of Algorithm EG2 owing to the stronger capacity of exploration. However, for the same dataset the best result is achieved by Algorithm EG3, which shows the strongest capacity of exploration. As analyzed above, both the strongest capacity of exploration and the stable results cannot be achieved at the same time, so a trade-off is needed between them.

We compare our results to those results obtained by other clustering algorithms, Kmeans [\cite=Ding2007], PCA-Kmeans [\cite=Ding2007], LDA-Km [\cite=Ding2007], on the same dataset. The comparison is summarized in Table [\ref=tab:3].

Discussions

In the subsection, firstly, we discuss how the number of clusters is affected by the number k of nearest neighbors changing. Then, for the Algorithm EG1, the relationships between the ratio of exploration and the clustering results is investigated, which provides a way to select the ratio of exploration η. Finally, the rates of convergence in three clustering algorithms are compared.

Number of nearest neighbors vs. number of clusters

The number k of nearest neighbors represents the number of neighbors to which a data point i∈ connects. For a dataset, the number k of nearest neighbors determines the number of clusters in part. Generally speaking, the number of clusters decreases inversely with the number k of nearest neighbors. If the number k of nearest neighbors is small, which indicates a data point i connects to a few neighbors, in this case the area that the ERR function may explore is also small, i.e., the elements in the union of the extended neighbor set Υt - 1(i) and the neighbor set Γt - 1(i) are only a few. Therefore, when the network evolves over time, strategies are spread only in a small area. Finally many small clusters with evolutionarily stable strategies appear in the network. On the other hand, a big number k of nearest neighbors provides more neighbors for a data point, which implies that the cardinality of the union is larger than that when a small k is employed. This also means that a larger area can be observed and explored by the ERR function, so that big clusters containing more data points are formed because evolutionarily stable strategies are spread in larger areas.

For a dataset, the clustering results in different number k of nearest neighbors have been illustrated in Fig. [\ref=fig:1], in which each data point only connects to the neighbor with the largest preference, and clusters are represented by different signs. As is shown in Fig. [\ref=fig:1], we can find that only a few data points receive considerable connections, whereas most of data points have only one connection. This indicates that when the evolution of network is ended, the network formed is characterized by the scale-free network [\cite=Barabasi2003], i.e., winner takes all. Besides, in Fig. [\ref=fig:1](a), six clusters are obtained by the clustering algorithm, when k = 8. As the number k of nearest neighbors rises, five clusters are obtained when k = 10, three clusters when k = 15. So, if the exact number of clusters is not known in advance, different number of clusters may be achieved by adjusting the number of nearest neighbors in practice.

Effect of the ratio of exploration in Algorithm EG1

For the ERR function B1i(  ·  ) used in Algorithm EG1, its capacity of exploration may be adjusted by setting different ratio of exploration η∈[0,1]. If the ratio of exploration is η = 0, then the extended neighbor set formed will be empty. Hence, the network does not evolve over time, since the ERR function does not explore. On the other hand, when the ratio of exploration takes the maximum η = 1, the ERR function is with the strongest capacity of exploration, because an extended neighbor set formed by all neighbors can be observed. Then we may ask naturally: what should the ratio of exploration be taken? To answer this question, we compare those clustering results at different η shown in Fig. [\ref=fig:2], in which the results are represented by the clustering accuracies.

From Fig. [\ref=fig:2], we can see that when the ratio of exploration is greater than 0.5, the clustering results obtained by Algorithms EG1 fluctuates strongly, as seems over exploration. On the contrary, in the case when [formula], the clustering results are stable relatively. As a whole, when the ratio of exploration η = 0.5, the best results are achieved in Algorithm EG1. In conclusion, a big ratio of exploration is not too good, because it may cause the ERR function B1i(  ·  ) over exploration. However, if the ratio of exploration is too small, good results are not obtained because of a lack of exploration. Therefore, in the later discussion, the ratio of exploration in Algorithm EG1 takes η = 0.5.

Three ERR functions vs. rate of convergence

As for Algorithm EG1, when η = 0.5, the ERR function B1i(  ·  ) may observe the extended neighbor set formed by half of neighbors. In this case, the payoff threshold is θ1t - 1(i) = median({ut - 1(j),j∈Γt - 1(i)}). In Algorithm EG2, however, the ERR function B2i(  ·  ) only can observe the extended neighbor set formed by neighbors whose payoffs are greater than the average θ2t - 1(i). Generally speaking, for the same k, the median of payoffs is smaller than or equal to the mean, i.e., θ1t - 1(i)  ≤  θ2t - 1(i). So the exploring area of the ERR function B1i(  ·  ) is larger than that of the ERR function B2i(  ·  ), that is, the number of edges rewired in Algorithm EG1 is more than that in Algorithm EG2. In addition, the number of edges rewired in Algorithm EG3 is largest, since in Algorithm EG3 the ERR function B3i(  ·  ) provides stronger capacity of exploration for players with small payoffs, which makes more edges are removed and rewired. The comparison of number of edges rewired in three algorithms is illustrated in Fig. [\ref=fig:3]. As is shown in Fig. [\ref=fig:3], at different number of nearest neighbors, the number of edges rewired in Algorithm EG3 is larger than that in other two, and the number of edges rewired in Algorithm EG1 is larger than Algorithm EG2. For each one of three algorithms, the number of edges rewired increases rapidly with the number of nearest neighbors.

Besides, the number of iterations indicates the rate of convergence of an algorithm. From Fig. [\ref=fig:3], we can see that the rates of convergence in Algorithm EG1 and EG2 are almost the same, and the rate of Algorithm EG3 is slower slightly than the other two because the ERR function B3i(  ·  ) in Algorithm EG3 explores larger areas than that in Algorithm EG1 and EG2. For each one of three algorithms, as the number of nearest neighbors rises, the number of iterations also rises slightly.

When the algorithm converges, the evolutionarily stable strategies appear in the network at the same time. The changes of the strategies with the largest preference for each data point are shown in Fig. [\ref=fig:4], where the straight lines in the right side of figures represent the evolutionarily stable strategies, and the number of straight lines is the number of clusters. As is shown in Fig. [\ref=fig:4], the evolutionarily stable strategies appear in the network after only a few iterations, as indicates the rates of convergence of algorithms are fast enough.

Conclusion

A model based upon games on an evolving network has been established, which may be used to explain the formation of groups in society partly. Following this model, three clustering algorithms (EG1, EG2 and EG3) using three different ERR functions are constructed, in which each data point in a dataset is regarded as a player in a game. When a distance function is selected, the initial network is created among data points according to Definition 1. Then, by applying an ERR function, the network will evolve over time due to edges removed and rewired. Hence, the preference set of a player needs to be adjusted in terms of Definition 5, and payoffs of players are recomputed too. During the network evolving, certain strategies are spread in the network. Finally, the evolutionarily stable strategies emerge in the network. According to evolutionarily stable strategies played by players, those data points with the same evolutionarily stable strategies are collected as a cluster. As such, the clustering results are obtained, where the number of evolutionarily stable strategies corresponds to the number of clusters.

The ERR functions (B1i(  ·  ),B2i(  ·  ),B3i(  ·  )) employed in three clustering algorithms provide different capacities of exploration for these clustering algorithms, i.e., the sizes of areas which they can observe are various. So, the clustering results of three algorithms are different. For the ERR function B1i(  ·  ), it is with a constant ratio of exploration η = 0.5 because over exploration may occur when η > 0.5, and can observe an extend neighbor set formed by half of neighbors. The ERR function B2i(  ·  ), however, can observe an extend neighbor set formed by neighbors whose payoffs are larger than the average, while the ERR function B3i(  ·  ) provides stronger capacity of exploration for data points with small payoffs. Besides, the clustering results of Algorithm EG1 and EG2 are more stable than that of Algorithm EG3, but the best results are achieved by Algorithm EG3 due to the strongest capacity of exploration among three algorithms.

In the case when the exact number of clusters is unknown in advance, one can adjust the number k of nearest neighbors to control the number of clusters, where the number of clusters decreases inversely with the number k of nearest neighbors. We evaluate the clustering algorithms on five real datasets, experimental results have demonstrated that data points in a dataset are clustered reasonably and efficiently, and the rates of convergence of three algorithms are fast enough.