Numerical analysis of decoy state quantum key distribution protocols

Introduction

Quantum key distribution (QKD) is a cryptographic protocol that allows two remote parties (Alice and Bob) to generate a random key (a string of bits) so that only Alice and Bob have any information about the key. The security of the system is guaranteed by the laws of quantum mechanics unlike other key distribution schemes, such as public key cryptography, that rely on assumed computational limitations of the attacker. The classic QKD system is BB84 [\cite=bb84], which consists of a sender (Alice) sending a receiver (Bob) a series of bit values encoded on qubits in one of two mutually unbiased bases. They only keep the bit values where Bob measured in the same basis that Alice prepared the state in. Provided that any eavesdropper (Eve) does not know the basis at the time of transmission, she cannot gain any knowledge of the key Alice sent without introducing a disturbance rate into the stream that Bob measures. Alice and Bob can then reveal a portion of their bits to determine with high probability (asymptotically 1) that Eve was not present and thus are assured of unconditional security. Of course, in non-ideal (real world) situations, noise has to be accounted for, whether it is due to Eve or the physical apparatus. It has been shown [\cite=shor-preskill] [\cite=gllp] [\cite=ilm] that Alice and Bob can still be assured of unconditional security in the presence of noise by implementing error correction and privacy amplification by way of classical messages over an authenticated public channel.

There are also forms of attack that are based on the specifics of the physical protocol and how they differ from the ideal BB84 protocol. One common physical difference is the use of weak coherent light pulses for the encoded bits that Alice sends to Bob, instead of using a qubit space. Since it is currently hard to create a steady deterministic source of single-photon signals (SPS), phase-randomized coherent states with their Poisson distribution of photon number are often used instead. The photon-number-splitting (PNS) attack takes advantage of an inherent vulnerability in these types of signals, specifically the fact that sometimes multiple photons are simultaneously prepared and sent with the same encoded information. In a PNS attack, Eve performs a non-demolition measurement on the photon number of each signal that Alice sends to Bob. When the number is greater than one, Eve splits a photon off of the signal and stores it in quantum memory. After Bob has measured the signal and revealed which basis he measured in, Eve can then measure in the same basis and acquire full information on those signals without introducing any disturbance.

In order to protect against this type of attack, Alice and Bob must place a bound on the number of SPS that Bob received. Then, they must privacy amplify away all other signals. If Alice prepares all of the signals from a single mean photon number (μ), conservative analysis leads to a very loose bound and hence a significant reduction in secret bit rate and maximum secure distance. Decoy states were proposed as a solution [\cite=hwang03] [\cite=lo05] [\cite=wang05] [\cite=harrington05] to boost performance and have since been implemented experimentally [\cite=decoyExpA] [\cite=decoyExpB] [\cite=decoyExpC] [\cite=decoyExpD] [\cite=decoyExpE] [\cite=decoyExpF] [\cite=decoyExpG] [\cite=decoyExpH]. This is where Alice creates multiple signal intensities (μj) and randomly varies between these intensities when sending each signal to Bob. By looking at the number of signals of each level that reach Bob, one can characterize a slice of how a channel behaves for a signal intensity μj. Since each signal level gives a different slice, the decoy states better characterize the channel. This means that they can place a much tighter bound on the SPS and thus acheive significantly greater rates and maximum distances.

The rate of key generation is affected by several protocol parameters, namely the number of decoy states, the intensity of each decoy state (μj), and the proportion of each decoy state among the total signals sent. It is thus useful to form heuristics for determining their optimal setting. These optimal parameters are in turn affected by the system parameters: dark (and background) count rate (y0), signal visibility (V), the session length in number of signals that Alice sent (N), the security parameter (ε), and the channel characteristics. By building on the formalism introduced in [\cite=harrington05], this paper will present results from numerical analysis of the effects of system parameters on the optimal decoy state protocols and their associated rates. In Section II, we present our secret key generation rate formula and discuss the decoy state optimization programs. In Section III, we show how we perform the global search over system parameters and find the optimal decoy state scheme. Numerical results detailing how the different system parameters affect the rate are presented in Section IV. We then demonstrate how to carry out decoy state analysis either when the intensities are not known exactly (Section V) or when the levels of the protocol are partially distinguishable by an adversary (Section VI). Finally, we summarize the heuristics found in this work in Section VII. Appendix A contains an argument concerning the validity of the decoy state analysis regardless of whether the quantum channel is stationary or time-varying.

Calculating the rate

For any given protocol, there is a simple linear program that can be used along with some statistics from Bob's data to place a tight bound on the number of SPS. There is also a quadratic program to find a bound on the bit error rate (BER) of the SPS. In both cases we consider the intensities μj to be known exactly; see Section V for a relaxation of this assumption. The length of the secret key generated is the bound on the SPS plus the dark counts [\cite=nothing] minus the amount of information revealed during the error-correction process minus the amount of required privacy amplification. The following secret key rate formula has a very similar form to that of GLLP [\cite=gllp], but its (composable) security is actually a finite statistics derivation of Koashi's security proof [\cite=koashi05] [\cite=koashi06], which will be published elsewhere. For the numerical studies reported in this work we use:

[formula]

where the summation is only of the j's that label signals that encode secret key bits, K is the length of the key, R is the key rate, N is the session length measured in the number of signals sent, Sj is the lower bound on the SPS and Dj is the lower bound on the dark counts, Cj is the number of total signals that Bob received from decoy state j, H2(  ·  ) is the binary Shannon entropy function and b max1 is an upper bound on the bit error rate of only the SPS. Efficiency factors fe.c. and fp.a. respectively relate how close error correction and privacy amplification are to the Shannon limit. The simulations here use fEC  =  1.2 and [formula], where [formula] is the bound on the total number of SPS contributing to the secret key. (The expression for fPA is a rough numerical fit we found for calculating the number of typical strings needed to describe the output of a binary symmetric channel with high confidence, as needed for computing privacy amplification in Koashi's approach [\cite=koashi05].) Not every level needs to encode key bits; there can be levels acting purely as decoys, which will contribute by bounding the SPS. In most of our simulations, only one of the (usually three) levels is chosen to encode information for generating secret key.

Prior to calculating the amount of required privacy amplfication, Alice and Bob will share information on: the number of signals at each level sent by Alice ([formula]), the number of signals at each level successfully received by Bob (Cj), and the number of errors on the sifted bits at each level (Ej). From this data, they must find the lower bound S and the upper bound b max1.

Finding yields

Since Eve cannot distinguish between the decoy states except by means of the photon number, the channel parameters that she can adjust can be varied independently for each photon number, k. Define yk to be the probability that Bob gets a click at his detector given that the click came from a k signal that Alice sent. y0 will encompass all dark and background counts. If Eve does not apply a stationary channel the yk's will be averaged probabilities. See Appendix [\ref=stationaryArgument] for further details. To bound S, first we can bound the probability, P, that Bob's click came from a SPS or a dark count given that Alice sent a signal of intensity μj. These probabilities are

[formula]

Since μj is fixed before the protocol begins and Eve is free to pick the values of all the yk's, the minimum probability, Pj, is found by ranging over optimization variables [formula]'s to find the minimum subject to the statistics the Bob measured. Each signal, regardless of whether it encodes information, adds a constraint:

[formula]

where Y± are upper and lower bounds on the overall yield of each decoy state. The maximum likelihood of the overall yield for state j is [formula]. The bounds are found such that with probability greater that (1  -  ε) the true probability is greater than Y- and with the same probability it is less than Y+. They can be calculated by using a standard function in statistics, namely the inverse incomplete beta function.

If there are three decoy states, then when calculating each Pj, all three constraints are imposed for each minimization. This is because Eve cannot change the channel based on which decoy state Alice sent so the yk's must be the same for all decoy states. The variable bounds are [formula] since the yk's are probabilities. The bar indicates a program variable. Putting everything together gives two linear programs:

[formula]

and

[formula]

Because it is unwieldy to have k range from 0 to ∞   and thus have an infinite number of optimization variables, we can choose a cutoff value of K max and then set any yk  ≥  K max to be either 0 or 1 for the upper or lower bounds, respectively. In this paper, K max was set to 9, although numerically there was little difference for K max  ≥  5.

Finding error rates

The next step necessary for calculating the rate is to determine the bit error rate on only the single-photon signals. This can be done in a similar manner. Let bk be the bit error rate that Eve (or anything else) introduces on signals with k photons present. She can vary her attacks independently for each photon number here as well. The number of errors that Bob measured on signals from state j is Ej. Construct bounds, B±, on the probability that Bob measures an error given that Alice sent a signal. Note that this is not the same as the probability of an error given that Bob received a click. The constraints are

[formula]

Additionally, the constraints on the yields still hold. The total program becomes

[formula]

This is a quadratically-constrained program, due to the [formula] terms, with a linear objective. With the double-sided nature of the constraints, this computation takes a bit longer than the simple linear program described above. After calculating [formula], where the * indicates the optimal value returned from the program, the rate formula (Eq. [\ref=rate]) can be computed and the length of the secret key and rate of key generation can be determined for any set of measurements generated in a QKD session. Details of this calculation and the protocol used to create this key rate (Eq. [\ref=rate]) can be found in a forthcoming paper.

Global search over protocol parameters given system parameters

In order to find the optimal protocol parameters, a simulation of the particular protocol is done with a set of system parameters. Some of the parameters are relatively straightforward. The security parameter was set at 10- 7 except when testing the effects of its change. The dark count probability y0 included both detector dark counts and background counts. The values used were from 10- 5 to 10- 8. This is the probability of a dark count being detected in the place of one signal. For example, if the combined dark and background rate was 100 Hz and the system operated at 10 MHz with no windowing the dark count would be y0  =  10- 5. The session length is also expressed in terms of number of signals. The number of signals sent ranged from 107 up to close to 1013. At a rate of 10 MHz, the sessions could last anywhere from one second to a few days. Most simulations used a session length of 1010, which is about 15 minutes at 10 MHz. The error rate for dark counts was set at 50%. For all other numbers of photons in the signal the bit error rate was a fixed value, ranging from 0% to 5%. That is to say that the true values (as opposed to the bounds found in (Eq. [\ref=bmin])) are b0  =  .5 and [formula], where V is the visibility. We also need to define the channel transmission efficiency. The simulations used a simple beamsplitter channel with transmission efficiency η. This assumption is only for simulating the channel; Bob does not make any assumptions on the channel when performing the decoy state analysis.

Once these parameters are set, many simulations are done that range over the protocol parameters. The bulk of the simulations used three signal states; of which one was the vacuum state (μ  =  0). The remainder used four states. There are thus four parameters, the signal intensity for the two non-vacuum states and two parameters for the three probabilities of occurrence. Using sample system parameters, many simulations of the rate of key growth were done to search the protocol parameter space. The result was that the rate function appeared convex. The only exception was with a different physical system that will be discussed below; that system had a bimodal rate function. The remaining searches to find optimum protocols used standard convex optimization techniques (the objective function used contains linear and quadratically-constrained programs).

For example, with system parameters: ε  =  10- 7, N  =  1010, V = 98%, y0  =  2  ×  10- 6, and η  =  10- 3, the optimal protocol is found to have the three signal intensity values (0,0.063,0.655) occurring with probabilities (1%,2.75%,96.25%). The rate of this protocol is 9.99621  ×  10- 5 which means that the length of the secret key generated is 999,621.

Some basic features of this example are general for all optimal protocols. The first to note is that one of the signal intensities is 0. The vacuum state is always an optimal state to use (with three or more levels). It typically has a small probability less than 5%. The next state also has a low probability and a weak signal intensity. The last state has a high probability and a high intensity. This high state does the main work of transmitting the secret key, whereas the lower signal intensities characterize the channel. In fact, such a large share of the key comes from the high signal intensity that any key generated from the weak signal intensity can be ignored (with only slight adjustment in the optimal protocol and secret bit rate). For the rest of the simulations the high signal state will be assumed to be the only key generating signal.

Effects of system parameters on the optimal protocol

With an algorithm for finding the optimal protocol parameters and the associated rate for a given set of system parameters, the effect of the system parameters on the optimal rate can be examined.

Session length

Increasing the session length effects the rate in two ways. Most clearly it gives Bob additional chances to detect signals and lengthens the raw key and so lengthens the final key. It also increases the rate of final key production per signal sent by Alice. This is due to a tightening of the confidence bounds used in the optimizations. This rate increase effects all protocols and so it shows in an increase of the optimal rate. The main effect on the optimal protocol is to increase the probability of the highest intensity signal when the session length starts out large. The effects of the session length on the rate and probability are displayed in graphs (Figs. [\ref=graph_N1], [\ref=graph_N2]). Generally, the product Nη needs to at least 105 to have enough statistics for successfully producing key. The increase in the rate and probability of sending the highest intensity signal tends to asymptote around N  ≈  1013 (Fig. [\ref=graph_N3]).

Security parameter

The security parameter effects the rate in a similar manner. As the security parameter is decreased, the confidence bounds tighten and the optimal rate increases. Fig. [\ref=graph_epsilon1] shows the small effect of the security parameter on the rate. Perhaps most interesting is the relative stability over nine orders of magnitude.

Dark Counts

The dark count rate is the parameter that most determines how far you can conduct QKD. In other words, fixing all other parameters except the transmission coefficient (η) the value of y0 is the most important parameter to finding the smallest η with a positive rate. As the transmission parameter decreases, the number of counts due to signals from Alice decreases but the number due to dark counts does not change. So the proportion of counts due to dark counts increases. Since dark counts have an error rate of 50%, this increases the error rate of the raw key significantly. Even with infinite statistics, the secret key rate vanishes when the error rate reaches about 11%. In simulations with reasonable parameters, the smallest η was consistently between two and three orders of magnitude larger than the dark count rate. The effect of various dark count rates are shown in graph (Fig. [\ref=graph_y0-1], [\ref=graph_y0-2]). The dark counts have some effect on the protocol. The lower signal state has to slightly increase intensity so that the dark count noise won't wash out the difference between the low state and the vacuum state. However, most of the effect is indirectly due to increasing the effective error rate (Fig. [\ref=graph_y0-3]).

System visibility

The error rate has the largest effect on which protocol is the optimal protocol, particularly the value of the high state intensity. The secret key rate is also effected by the error rate because the amount of error correction is determined by the error rate (as long as the transmission probability is not within two orders of magnitude as the dark counts). The part of the privacy amplification due to errors is also mainly determined by the error rate. See Fig. [\ref=graph_ber2] for the optimal rate and the optimal high intensity state. Fitting to the curve gives an approximate formula μhigh  ≈  e- 15(1 - V). For larger visibility errors, the ability to bound b1 with the quadratic program is more important. Without the optimization, one must use the worst-case assumption that all the errors observed were errors on SPS. See Fig. [\ref=graph_ber3] for a comparison of the two methods.

Channel transmission efficiency

The transmission coefficient has an effect on the rate but not much on the protocol. See Figs. [\ref=graph_eta1] and [\ref=graph_eta2]. The rate scales linearly with η, [formula]. This is the main point in favor of decoy states. Without decoy states, the optimal signal intensity must scale with the transmission efficiency to yield a positive secret bit rate [\cite=lutkenhaus]. With [formula], then [formula]. With decoy states, the signal intensity (of the high state) shouldn't scale with η, ([formula]). In practice there is still a slight logarithmic dependence on the transmission efficiency. See Fig. [\ref=graph_eta2].

Detectors

We also did simulations detailing the effects of different detectors in a system. The simulations treated the detectors as a plug-n-play portion of the system. The system was assumed to have an optical loss of 7 dB and a visibility of 97.68%, which are measured values for a fiber QKD system developed at LANL. The system was simulated for 15 minutes at 10 MHz, even if some detectors are capable of much faster operating rates. The three detectors that were simulated were superconducing nanowire single-photon detectors (SNSPDs), transition-edge sensor superconducting single-photon detectors (TES) and commercially available InGaAs avalanche photodiodes (APDs). The SNSPDs had a combined dark count probability per signal of 1.44  ×  10- 8 and a detector efficiency of 2%. The TES detectors had a background count probability per signal of 4  ×  10- 6 (set by blackbody radiation transmitted down the fiber) and a detector efficiency of 50% after applying a filter. The APDs had a combined dark count probability per clock cycle of 1.5  ×  10- 5 and a detector efficiency of 10%. The fiber was modeled with a loss of .2 dB/km. The simulations are shown in Fig. [\ref=graph_detectors]. The longest distance acheivable is by the SNSPDs. This is mainly due to their low number of dark counts per signal. The low detector efficiency of SNSPDs however, leads to a lower rate for shorter distances. The APDs could not go out as far as either of the other detectors. The TES detectors have a high detector efficiency and so have a higher rate for most values of channel distance, but they do not reach quite as far as the SNSPDs.

Number of levels

All results described before were for three-level decoy state protocols. The number of decoy levels that can be implemented is only limited by practical considerations and the fact that increasing the number of levels necessarily decreases the amount of statistics available for each level. By looking at the effects of the session length, it can be shown that this is not a big concern until the number of states is an extreme case of tens or hundreds. The rate increase of using four states instead of three states is shown in Fig. [\ref=graph_4-1] and is practically nonexistent. The improvement is small with less than a percent difference. The form of the optimal four-level protocol is similar to the three-level one as well. Like the previous protocols, there is a vacuum state and a state with low intensity, both with low probabilities. The three-level protocol's high intensity state splits into two high states in the four-level protocol. The two high states have intensities that straddle that of the three-level protocol (one intensity is higher and one is slightly lower). Generally, the highest state gets most of the probability.

For both the three- and the four- level protocols, there was an advantage to only sending information with the high intensity levels and allowing the low intensity and vacuum levels to be pure decoys. This was not the case with the other two results shown on Fig. [\ref=graph_4-1]. With only one level, that level must send information, and in the two-level protocol both levels are sending information. As shown in the figure, adding even one decoy level dramatically protects against the photon-number splitting attacks and significantly improves the maximum tolerable loss. Adding another level yields the three-level protocol that is the focus of this paper and an improvement over the two-level protocol. After that, it seems that the asymptotic limit is reached very quickly with very little noticeable improvement with a four-level protocol.

Uncertainty in intensities

Here we consider how to incorporate uncertainty into the values of the intensities selected for each level in a decoy state protocol. In practical terms, the mean photon number of a laser output will never be known exactly, and either the provider or the users of a quantum key distribution system will have to determine how well-calibrated and stable the intensities are. We model here uncertainty for the low and high intensity levels but do not consider perturbations of the vacuum level. We first simulate Alice sending the signals through the channel and Bob receiving them with the true intensity values. Then, for each value U of the uncertainty, we minimize according to the program in Eq. [\ref=linearEQ] four separate times using a value of μj  =  (1  ±  U)μj, and taking the worst case of the four results. We previously ran simulations by modifying both μ values by a fine grain across this range of uncertainty, and we always found the worst case occurred at the endpoints of the ranges. Intuitively, this can be seen because only one side of any single inequality from ([\ref=linearEQ]) may be tight. Using a non-extreme value for μ would simply restrict the tight side and thus increase the minimum value. On the other hand, relaxing the loose side of the inequality will not change the minimum value. See Fig. [\ref=graph_uncertainty] for details on how uncertainty in intensities impacts the secret key rate. In short, a positive rate exists even with relatively large uncertainties, except when the channel loss is close to the maximum tolerated.

Partially distinguishable levels

The validity of decoy states and of equations ([\ref=linearEQ]) and ([\ref=bmin]) is contingent on Eve being unable to guess which signal level Alice sent other than the expected photon number statistics. If she could distinguish the levels further (due to information leaking into another signal property, such as polarization), then the variables {yk} could be modified by Eve to be different values for each level. We must then expand variables {yk} to {yj,k} where j is a label for each distinguishable level. The linear program in ([\ref=linearEQ]) becomes

[formula]

Now the decoy states contribute no constraints to the optimization. In this case of completely distinguishable decoy states, the protocol does no better than without decoy states. But one could imagine a situation where Eve could only probabilistically distinguish among the decoy states. For example, in a system where the differently encoded bits are created by four different lasers with fixed optics one might want to add a decoy level with a single additional laser. This low intensity laser would need to produce a maximally mixed state (in polarization or phase) to match the average of the four BB84 states. For signals generated with either zero or one photons, the high and the low intensity states are indistinguishable. When more photons are generated, these two sources are partially distinguishable. In this example, when Eve cannot distinguish between the states, then a common set of {yk}'s are associated with the signal state. Let us define the probability of indistinguishability as Qj,k. This creates additional constraints and the linear program becomes

[formula]

For the example above, Qj,k  =  1 when j is the high intensity signal state or the vacuum state. When j is the low signal intensity state Qj,k becomes:

[formula]

Since the low state isn't a key bit carrying signal, it can't have an error rate. This means that the quadratically-constrained program (Eq. [\ref=bmin]) cannot be used. An upper bound on b1 can be found that is still lower than the worst-case situation. The dark count error rate should be [formula] with statistical fluctuations. By putting a lower bound on y0 with the linear program, one can put a lower bound with probability (1 - ε) that so many errors are due to dark counts. That reduces the number of errors that could be due to single-photon signals. Fig. [\ref=graph_free1] shows how these limitations effect the rate compared to the standard three-level protocols for the example Q's.

Discussion

These simulations shed some light on what system parameters are most important to improve rates and maximum distances. We find that on the order of 105 received detections are sufficient for a positive secret bit rate with our finite statistics approach, provided that noise is sufficiently low. The intensities for 3-level decoy state protocols are generally optimized with μ0  =  0 (vacuum) and μ1  ~  0.1, while the optimal value for the key-generating signal intensity is dominated by the noise, with a rough fit of μ2  ~  e- 15(BER). Modifying the total loss by an order of magnitude results in only minor modifications to these values. Increasing the security parameter by many orders of magnitude also leads to very minor effects, so there is little cost to boosting confidence levels (provided that there are sufficient statistics collected to bound the single photons and their bit error rate). As expected, dark counts are the limiting factor for maximizing the distances (or tolerable loss) for a positive secret bit rate. It is worth noting, however, that an order of magnitude suppression in dark and background count rates generally adds less than 10 dB to the tolerable loss, unless the session length is also increased to maintain count statistics. Various imperfections can also be incorporated into the decoy state analysis, such as not knowing the intensities exactly or potentially leaking partial information about the levels to an adversary, and all of the bounds described in this work can be implemented in software to run in real-time.

Why bounds derived for stationary channels are valid for non-stationary channels

The bounds created by the linearly- and quadratically-constrained programs were obtained with the assumption that Eve provides a stationary channel characterized by the parameters {yk,bk}. This assumption is not necessary. We will show that the bounds are valid in the situation that for each signal Alice sends, Eve chooses one of L channels to apply (for arbitrary L). The L channels are parameterized by {yk,l,bk,l} and Eve applies them with frequencies fl, with [formula].

Yield bounds

In the stationary case, the total number of single-photon signals (SPS) is (p1 N y1) and the total number of clicks is [formula]. The inequality for each signal intensity appearing in the linear program becomes

[formula]

where Y± are the bounds on the yield [formula], [formula] are the variables over which the program is optimized, and pk is the probability that a signal contains k-photons. Let the functions that calculate the yield bounds be g-(s,t,ε) and g+(s,t,ε). Specifically, they provide lower and upper bounds on the success probability of a binomial trial, given that for t trials the observed frequency was [formula]. The statement that the true success probability lies between both bounds has a confidence of at least (1 - ε)2.

Now, when Eve uses the L channels the true value of y1 becomes [formula], the total number of the SPS becomes [formula] and the total number of clicks become [formula]. The inequalities are:

[formula]

Defining new variables [formula], g- ,l, g+ ,l such that [formula], [formula], and [formula], we can create an equivalent linear program

[formula]

Now we can create L fictional linear programs [formula] subject to:

[formula]

Since the inequalities in ([\ref=fictional]) are stronger that the inequality created by their sum found in the original program, the polytope of allowable solutions created by the intersection of the polytope (over all variables) for each program is contained in the polytope of allowable solutions to ([\ref=original]). This means that the original program provides a lower bound on the sum of the solutions to the fictional programs.

[formula]

The fictional programs provide bounds for the L stationary channels. So we have

[formula]

Putting ([\ref=bound1]) and ([\ref=bound2]) together completes the argument

[formula]

Disturbance bounds

In the stationary case, the true value of b1 is simply b1. That can be rewritten to [formula] for positive y1. Defining ck  =  bk yk as the total probability that a k-photon signal sent from Alice will result in an erroneous click at Bob, b1 becomes [formula]. The total number of errors is [formula]. The inequalities of the program are

[formula]

where B± are bounds on each signal intensity's bit error rate.

Now, when Eve uses the L channels, the true value of b1 is still [formula], but now [formula]. An upper bound on b1 is

[formula]

This means that we can find an upper bound on b1, if we can find an upper bound on c1.

We can transform ([\ref=ber_basic]) into an equivalent quadratically-constrained program with the following inequalities

[formula]

The objective function is still b1, but note that any assignment of variables that maximizes b1 will also maximize c1 since b1 only appears next to y1 in the inequalities. This can be proved by contradiction. Assume an assignment of variables that maximizes b1. This implies that at least one constraint that involves b1 is tight. c1 can only be increased by either increasing b1 or y1. If b1 can be increased, then it wasn't maximized. If y1 can be increased, all inequalities that involve y1 are slack. Since b1 is only in inequalities that involve y1 this is a contradiction. Making a trade-off such as first decreasing one variable in order to increase the other can't work either, since any such trade-off keeps c1 constant.

Now, we can play the same trick as before. Again, the intersection of the allowable solutions to the L fictional programs is contained in the region (no longer necessarily a polytope) allowed by ([\ref=ber]). So the maximum of the original program is an upper bound on the sum of the fictional maximums. [formula]. Solving the fictional programs yields

[formula]

Combining with the above means that the original program provides an upper bound on c1 and thus an upper bound on b1.