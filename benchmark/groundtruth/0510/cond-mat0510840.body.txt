1.0cm

The computational complexity of Kauffman nets and the P versus NP problem

Computational complexity theory addresses the question of how fast the resources required to solve a given problem grow with the size of the input needed to specify the problem.[\cite=papadimitriou94] The close relationships between the physics of random systems and computational complexity theory have been explored for nearly two decades. [\cite=fu86] [\cite=mezard87]

Whether or not P, the complexity class of problems that can be solved in a time that grows polynomially with the size of the problem specification ("polynomial time"), and NP, the class of problems for which a solution can be verified in polynomial time, are distinct is a central unanswered question in computational complexity theory.[\cite=claymath] The class of NP-complete problems are equivalent in that being able to solve any one of them in polynomial time implies that any problem in NP can be solved in polynomial time.[\cite=cook71] [\cite=levin86] [\cite=garey79] An intuitive picture believed to be appropriate for NP-complete problems is that the presence of conflicting constraints, or "frustration,"[\cite=toulouse77] causes each problem to have an "energy landscape" with many local minima, and finding the global minimum is difficult because typical algorithms must explore an extremely large number of local minima to find the global one.[\cite=mezard87]

Here it is shown that the energy landscapes of NP-complete problems have important differences in their local properties. Specifically, it is demonstrated that the parameters of a particular problem in NP can be adjusted so that the solution is extremely sensitive to small changes in the problem, and it is argued that exploiting this sensitivity may lead to new strategies for resolving the P versus NP question.

We consider here a specific system called a Kauffman net, or N-K model.[\cite=kauffman69] It has N Boolean elements σi, i=1, 2,..., N, and the value of the ith element at time t+1 is determined by the value of its K inputs j1(i), j2(i), ..., jK(i) at time t via

[formula]

where each fi is a randomly chosen Boolean function that depends on K arguments. The K inputs for each element and the Boolean functions fi are all chosen randomly before beginning and then fixed throughout the computation. Specifying a Kauffman net requires time and space polynomial in N so long as K grows no faster than log(N): this follows because specifying the K inputs for each of the N elements takes a number of bits proportional to NK log(N), and specifying the fi takes N2K bits (one per output for each of the 2K possible inputs for each element). For the predecessor problem, a natural choice for the energy of a given configuration {} is the number of bits in the successor configuration {}=f({}) that differ from the corresponding bit in the target configuration.[\cite=baskaran87]

Here we consider the problem of determining whether a given configuration of a Kauffman net [formula] has a predecessor. When K = N, although specifying the model requires space that grows exponentially with N, one can still ask how many evaluations of the Boolean functions are required to determine whether such a predecessor exists. A candidate solution can be verified with a single evaluation of each Kauffman net function, but because in this case each configuration is a truly random function of its predecessor[\cite=derrida86] [\cite=derrida87], the only way to determine whether a predecessor exists is to check all of the exponentially many candidates. [\cite=bennett81]

When K = A log 2N with A a constant, the problem of determining whether a given configuration has a predecessor is in NP. The problem is potentially hard because one must determine whether many different, potentially conflicting constraints can be satisfied simultaneously. Goldreich[\cite=goldreich00] has speculated that finding a predecessor requires time exponential in N, and the question of whether a given configuration of a given Kauffman net has a predecessor can easily be rewritten as an instance of satisfiability, a classic NP-complete problem.[\cite=garey79] The instances of satisfiability that correspond to randomly chosen Kauffman nets are expected to require exponential time for any K3.[\cite=mezard02]

For any K > 2, local search algorithms that work by decreasing the number of wrong bits by changing a small number of bits in the current "guess" of the predecessor typically find only a local minimum and not the global one. A new ingredient that enters for K = A log 2N is that finding the global minimum is hard even if one starts off with a configuration whose successor has only one bit in error. In contrast, if one has found a configuration whose successor differs from the target by one bit for a random problem instance with K=3, the error can be corrected with very high probability via a small number of single bit flips. In addition, when Klog2N with A large enough, if the target configuration with the single bit flip has a predecessor, the number of bits in the predecessor configuration that must be changed grows as a power of N, while when K=3 a change in a single bit in the target results in a change of only O(1) bits of the predecessor.

The sensitivity of the predecessor configuration to single-bit changes in the target constrains the computational strategies that could solve the predecessor problem in polynomial time. First, the algorithm must yield the exact answer, since the local search algorithm cannot correct even single-bit errors. Second, the algorithm must explicitly depend on the specification of every bit of every input function as well as every bit of the target configuration. This is because if one realization of the functions yields the target output, then a second function realization that differs from the first by a single bit change could yield a configuration that differs from the target by one bit, and the arguments given above then demonstrate that the predecessor for the second function realization, if it exists, has a large number of bits that are different than for the first function realization.

The extreme sensitivity of the predecessor configuration to small changes in the target suggests that a strategy for providing a lower bound on the difficulty of solving the predecessor problem could be to investigate the complexity of the polynomial describing the predecessor function. For example, one could define a function [formula] to be zero if the configuration {σ} has a single predecessor {τ} with i=0, one if {} has a single predecessor {} with i=1, two if the configuration {} has no predecessor, and three if {} has more than one predecessor. The degree of this polynomial is bounded below by the number of its zeros, and since changes of a single bit in the target change many bits of the predecessor, the number of zeros of this polynomial grows approximately as fast as the number of target configurations that have a single predecessor. Moreover, the sensitive dependence of the polynomial's coefficients on the choice of the Kauffman net functions constrain the possible compact algorithms for computing them. Implementing this strategy is nontrivial, since one must prove that the number of target configurations with a single predecessor grows faster than polynomially with N as well as that the large number of different possible polynomials precludes efficient computation of the coefficients. Nonetheless, considering random networks with [formula] is useful in this context because the sensitivity of the predecessor to single bit changes in the successor enables one to relate the number of sign changes of the function to the number of configurations with a single predecessor.

To demonstrate the increasing sensitivity of the solution of the predecessor problem to single-bit changes in the target as K is increased, one assumes that a one is given a configuration {} such that f({}){} differs from the target configuration {σ} by exactly one bit. One then attempts to find the state {} such that f({})={} as follows: (1) For each of the K inputs of the wrong element i, find the configuration that results when a given input is flipped, and (2) flip the input of i that minimizes the number of wrong elements in the output. This "downhill" algorithm succeeds if single-element changes of {} yield no errors in the output instead of one error.

This algorithm is characterized here using methods similar to those in [\cite=bastolla96] [\cite=bastolla98]. Starting with a configuration that yields a successor that differs from the target by one bit, the probability that flipping one given input of the wrong bit causes the output result to have k errors is

[formula]

Eq. ([\ref=eq:P_K]) follows because a given input affects K elements, and the output for each is correct with probability 1/2.

Now one gets to pick the input that yields the fewest incorrect outputs. Defining QK(j) as the probability that the best output configuration differs from the target in j bits, one finds

Eqs. ([\ref=eq:Q_K]) follow because if the smallest number of errors yielded by this process is i, no trials can yield a number of errors less than i, at least one trial must yield i errors, and [formula] is the probability that only more than i errors are obtained.

This procedure fixes the error if the resulting configuration has no wrong bits. If the number of wrong bits is one, then it is likely that a different element is now wrong, and one can repeat the procedure and perhaps find a correct solution in the next iteration. If the number of wrong bits is more than one, then the process has yielded a configuration that is farther from a solution.

One way to estimate whether the process is getting closer to or farther from a solution is to calculate after one step of the procedure the expectation value of the number of wrong bits, [formula],

[formula]

Now [formula] increases monotonically with K, and [formula] and [formula], so the number of wrong bits is reduced by the procedure when K=3 but not when K=4. Thus, it appears that the procedure corrects the one-bit error with high probability for some values of K strictly greater than 3, but not when K is larger than 4.

The energy landscape of the model becomes increasingly "rugged" as K is increased. [\cite=kauffman93], and the probability that the downhill algorithm fixes the error decreases monotonically as K increases. When K = A log 2N, the number of wrong bits after one step is strongly peaked at K / 2 = (A / 2) log 2N, and the probability that the operation yields either zero errors or one error [formula], and the probability of being able to correct any one-bit error vanishes as N  →    ∞  .

Now it is shown that if the target configuration that is changed by one bit has a predecessor, for large enough A the new predecessor configuration differs from the original one by a number of bits that grows at least as fast as N[formula], with x strictly greater than zero. This follows because (1) changing a single input bit is extremely likely to change many output bits, and (2) if one chooses M random numbers out of N possibilities, when M is much smaller than [formula], the probability of duplication is exponentially small in the ratio [formula].[\cite=diaconis89]

Here, we consider Kauffman nets in which each element has exactly K inputs and K outputs, a restriction that does not affect any of the results but simplifies the analysis. One begins with a configuration {} such that f({}) is the target configuration {}, and now considers a new target configuration {'} that is identical to {} except for a single bit flip. One then asks how many bits one must flip in the configuration {} to obtain a configuration {'} such that f({'})={'}. To see that {'} differs from {} by many bits, consider all configurations that differ from the original predecessor {} by S bits, where S < Nx, with x<1/2. The number of different ways that these S bits can be chosen is [formula], which can be written [formula] when N and S are large and S<< [formula]. Next we define Q to be the number of elements with at least one input that has been changed. When S<< [formula] and K is large, Q>SK/2. This is because the number of affected outputs would be SK if none were duplicated, and while one can obtain individual duplications of an output by choosing inputs that have a common output, the probability of obtaining additional duplications when the connections are chosen randomly is negligible so long as SK<< [formula]. Now we define R to be the number of elements whose output value has changed. When Q is large, given a realization with Q outputs that have at least one perturbed input, the probability distribution governing the number of output bits that are different from the original target is a normal distribution with mean Q/2 and standard deviation [formula].[\cite=feller68] Therefore, the probability that R<Q/4 for such a realization must be less than [formula] [formula].

So now finally we show that when A is large enough the probability that a choice of the S inputs exists for which R<SK/8 is smaller than 1/N as N. This follows because the probability that Q>SK/2 is essentially unity and that the probability that R<QK/4 is very small. Specifically, the conclusion follows if

[formula]

For Q>SK/2 and K=Alog2N, one has

[formula]

The expression ([\ref=eq:condition2]) becomes arbitrarily small as N for any S<N[formula] with x<1/2 when A/64 ln 2 >1.

Thus, we have shown that no configuration that differs from the unperturbed predecessor by a number of bits that is smaller than SN[formula] with x<1/2 can be the predecessor of a new target configuration that differs from the original target by O(1) bits.

To summarize, we have shown that the problem of finding a predecessor configuration of a random Kauffman net in which each element has a number of connections that grows logarithmically with the number of elements is extremely sensitive to single-bit changes in the target configuration. We are optimistic that this sensitivity to small changes can be exploited to develop new strategies to attack the difficult question of whether the complexity P and NP are distinct.

The author gratefully acknowledges financial support from grants NSF-DMR 0209630 and NSF-EMT 0523680, and useful conversations with Eric Bach, Robert Joynt, and Dieter von Melkebeek. The hospitality of the Aspen Center for Physics, where some of this work was done, is greatly appreciated.