Stochastic dynamics simulations in a new generalized ensemble

Ulrich H.E. Hansmann,a, Frank Eisenmenger,b, and Yuko Okamotoa,

a Department of Theoretical Studies, Institute for Molecular Science

Okazaki, Aichi 444-8585, Japan

bInstitute for Biochemistry, Medical Faculty of the Humboldt University Berlin

10115 Berlin, Germany

ABSTRACT

We develop a formulation for molecular dynamics, Langevin, and hybrid Monte Carlo algorithms in the recently proposed generalized ensemble that is based on a physically motivated realisation of Tsallis weights. The effectiveness of the methods are tested with an energy function for a protein system. Simulations in this generalized ensemble by the three methods are performed for a penta peptide, Met-enkephalin. For each algorithm, it is shown that from only one simulation run one can not only find the global-minimum-energy conformation but also obtain probability distributions in canonical ensemble at any temperature, which allows the calculation of any thermodynamic quantity as a function of temperature.

1.  INTRODUCTION For many important physical systems like biological macromolecules it is very difficult to obtain the accurate canonical distribution at low temperatures by conventional simulation methods. This is because the energy function has many local minima, separated by high energy barriers, and at low temperatures simulations will necessarily get trapped in the configurations corresponding to one of these local minima. In order to overcome this multiple-minima problem, many methods have been proposed. For instance, the generalized-ensemble algorithms, most well-known of which is the multicanonical approach, [\cite=MU] [\cite=MU3] are powerful ones and were first introduced to the protein-folding problem in Ref. [\cite=HO]. Simulations in the multicanonical ensemble perform 1D random walk in energy space. They can thus avoid getting trapped in states of energy local minima. Besides multicanonical algorithms, simulated tempering [\cite=ST1] [\cite=ST2] and 1/k-sampling [\cite=HS] have been shown to be equally effective generalized-ensemble methods in the protein folding problem.[\cite=HO96b] The simulations are usually performed with Monte Carlo scheme, but recently molecular dynamics version of multicanonical algorithm was also developed.[\cite=HEO96] [\cite=NNK]

The generalized-ensemble approach is based on non-Boltzmann probability weight factors, and in the above three methods the determination of the weight factors is non-trivial. We have recently shown that a particular choice of the weight factor of Tsallis statistical mechanics,[\cite=Tsa] which is a nonextensive generalization of Boltzmann-Gibbs statistical mechanics, can be used for a generalized-ensemble Monte Carlo simulation.[\cite=HO96d] The advantage of this ensemble is that it greatly simplifies the determination of the weight factor.

The purpose of the present work is to generalize this Monte Carlo approach to other simulation techniques. Here, we consider three commonly used algorithms: molecular dynamics, [\cite=MD] Langevin,[\cite=Lang] and hybrid Monte Carlo.[\cite=HMC] The performances of the algorithms are tested with the system of an oligopeptide, Met-enkephalin.

2.  METHODS 2.1.  Monte Carlo in the new ensemble In the canonical ensemble at temperature T each state with potential energy E is weighted by the Boltzmann factor:

[formula]

where the inverse temperature β is defined by β  =  1 / kBT with Boltzmann constant kB. This weight factor gives the usual bell-shaped canonical probability distribution of energy:

[formula]

where n(E) is the density of states. For systems with many degrees of freedom, it is usually very difficult to generate a canonical distribution at low temperatures. This is because there are many local minima in the energy function, and simulations will get trapped in states of energy local minima.

Generalized-ensemble algorithms are the methods that overcome this difficulty by performing random walks in energy space, allowing simulations to escape from any state of energy local minimum. Here, we discuss one of the latest examples of simulation techniques in generalized ensemble.[\cite=HO96d] The probability weight factor of this method is given by

[formula]

where T0  =  1 / kBβ0 is a low temperature, nF is the number of degrees of freedom, and EGS is the global-minimum potential energy (when EGS is not known, we use its best estimate). Note that this weight is a special case of the weights used in Tsallis generalized statistical mechanics, [\cite=Tsa] where the Tsallis parameter q is chosen as

[formula]

Note also that through the substraction of EGS it is ensured that the weights will always be positive definite.

The above choice of q was motivated by the following reasoning. [\cite=HO96d] We are interested in an ensemble where not only the low-energy region can be sampled efficiently but also the high-energy states can be visited with finite probability. In this way the simulation can overcome energy barriers and escape from local minima. The probability distribution of energy should resemble that of an ideal low-temperature canonical distribution, but with a tail to higher energies. The Tsallis weight at low temperature

[formula]

has the required properties when the parameter q is carefully chosen. Namely, for suitable q  >  1 it is a good approximation of the Boltzmann weight WB(E,T0)  =   exp ( - β0(E - EGS)) for (q  -  1)β0(E  -  EGS)  ≪  1 ~ , while at high energies it is no longer exponentially suppressed but only according to a power law with the exponent 1 / (q  -  1). To ensure that simulations are able to escape from energy local minima, the weight should start deviating from the exponentially damped Boltzmann weight at energies near its mean value. This is because at low temperatures there are only small fluctuations of energy around its mean (< E > T0). In Eq. ([\ref=tsw2]) we may thus set

[formula]

The mean value at low temperatures is given by the harmonic approximation:

[formula]

Substituting this value into Eq. ([\ref=eqnf]), we obtain the optimal Tsallis parameter in Eq. ([\ref=eqtsq]).

We remark that the calculation of the weight factor is much easier than in other generalized-ensemble techniques, since it requires one to find only an estimator for the ground-state energy EGS, which can be done, for instance, by a procedure described in Ref. [\cite=HO96d].

As in the case of other generalized ensembles, we can use the reweighting techniques [\cite=FS] to construct canonical distributions at various temperatures T. This is because the simulation by the present algorithm samples a large range of energies. The thermodynamic average of any physical quantity A can be calculated over a wide temperature range by

[formula]

where x stands for configurations.

In the following subsections, we describe how to implement Langevin, molecular dynamics, and hybrid Monte Carlo algorithms in the new ensemble defined by the weight of Eq. ([\ref=eqopwe]). We remark that Langevin and molecular dynamics algorithms for Tsallis statistical mechanics were also developed in Refs. [\cite=Star] and [\cite=StrMD], respectively.

2.2.  Langevin algorithm The Langevin algorithm[\cite=Lang] is used to integrate the following differential equation:

[formula]

where [formula] is the (generalized) coordinates of the system, E is the potential energy, fi is the "force" acting on the particle at qi, and ηi is a set of independent Gaussian distributed random variables with a variance:

[formula]

Here (and hereafter), we set all the masses mi equal to unity for simplicity. It can be shown that the dynamics based on the Langevin algorithm yields a canonical distribution [formula]. In order to generalize this technique to simulations in the new ensemble, we rewrite the weight factor in Eq. ([\ref=eqopwe]) as

[formula]

Defining now an effective potential energy by [\cite=Star] [\cite=StrMD]

[formula]

we see that Langevin simulations in the new ensemble can be performed by replacing E in Eq. ([\ref=eq10]) by Eeff(E):

[formula]

Note that the procedure that led to the above equations is exactly the same as the one we followed when we developed molecular dynamics and related algorithms in another generalized ensemble, i.e., multicanonical ensemble.[\cite=HEO96]

For numerical work one has to integrate the above equation by discretizing the time with step Δt and therefore for actual simulations we use the following difference equation:

[formula]

Using the above equation we will sample in the Langevin simulation the same ensemble as in a Monte Carlo simulation with the weight of Eq. ([\ref=eqopwe]). Hence, we can again use the re-weighting techniques and calculate thermodynamic averages according to Eq. ([\ref=eqrw]).

2.3.  Molecular dynamics and hybrid Monte Carlo algorithms Once the formulation of Langevin algorithm for the new ensemble is given, the implementation of molecular dynamics algorithm is straightforward.

The classical molecular dynamics algorithm is based on a Hamiltonian

[formula]

where πi are the conjugate momenta corresponding to the coordinates qi. Hamilton's equations of motion are then given by

[formula]

and they are used to generate representative ensembles of configurations. For numerical work the time is discretized with step Δt and the equations are integrated according to the leapfrog (or other time reversible integration) scheme:

[formula]

The initial momenta [formula] for the iteration are prepared by

[formula]

with appropriately chosen qi(0) and πi(0) (πi(0) is from a Gaussian distribution).

In order to generalize this widely used technique to simulations in our case, we again propose to replace E by Eeff (of Eq. ([\ref=equs])) in Eq. ([\ref=eq8]). A new set of Hamilton's equations of motion are now given by

[formula]

This is the set of equations we adopt for MD simulation in our new ensemble. For numerical work the time is again discretized with step Δt and the equations are integrated according to the leapfrog scheme.

The hybrid Monte Carlo algorithm[\cite=HMC] is based on the combination of molecular dynamics and Metropolis Monte Carlo algorithms [\cite=Metro]. Namely, each proposal for the Monte Carlo update is prepared by a short MD run starting from the actual configuration. In this sense, the algorithm is based on a global update, while in the conventional Metropolis method one is usually restricted to a local update. Furthermore, the Metropolis step ensures that the sampled configurations are distributed according to the chosen ensemble, while convential molecular dynamics simulations are hampered by difficult-to-control systematic errors due to finite step size in the integration of the equations of motion.

Given the set of coordinates {qi} of the previous configuration and choosing the corresponding momenta {πi} from a Gaussian distribution, a certain number of MD steps are performed to obtain a candidate configuration [formula]. This candidate is accepted according to the Metropolis Monte Carlo criterion with probability

[formula]

where H is the Hamiltonian in Eq. ([\ref=eq7]). The time reversibility of the leapfrog integration scheme ensures detailed balance and therefore convergence to the correct distribution. The whole process is repeated for a desired number of times (Monte Carlo steps). The number of integration (leapfrog) steps NLF and the size of the time step Δt are free parameters in the hybrid Monte Carlo algorithm, which have to be tuned carefully. A choice of too small NLF and Δt means that the sampled configurations are too much correlated, while too large NLF (or Δt) yields high rejection rates. In both cases the algorithm becomes inefficient. The generalization of this technique to simulations for our ensemble can again be made by replacing the potential energy E by Eeff (of Eq. ([\ref=equs])) in the Hamiltonian of Eq. ([\ref=eq7]).

3.  RESULTS AND DISCUSSION The effectiveness of the algorithms presented in the previous section is tested for the system of an oligopeptide, Met-enkephalin. This peptide has the amino-acid sequence Tyr-Gly-Gly-Phe-Met. The potential energy function that we used is given by the sum of electrostatic term, Lennard-Jones term, and hydrogen-bond term for all pairs of atoms in the peptide together with the torsion term for all torsion angles. The parameters for the energy function were adopted from ECEPP/2.[\cite=EC1]-[\cite=EC3] The computer code SMC [\cite=SMC] was modified to accomodate the algorithms.

For the generalized coordinates {qi} we used the dihedral angles. The peptide-bond dihedral angles ω were fixed to be 180[formula] for simplicity. This leaves 19 dihedral angles as generalized coordinates (nF  =  19). The global-minimum potential energy EGS in this case was obtained previously and we have EGS  =   - 10.7 kcal/mol.[\cite=EH96d] As for the temperature, we set T0  =  50 K (or, β0  =  10.1

[formula]

. Our definition guarantees that we have

[formula]

with the limiting values

[formula]

Only the result from the hybrid Monte Carlo simulation is given in Fig. 2, since the other two simulations give similar results. Note that there is a clear anti-correlation between the potential energy E and overlap O (compare Figs. 1c and 2), indicating that the lower the potential energy is, the larger the overlap is (closer to the ground state).

Simulations in generalized ensemble can not only find the energy global minimum but also any thermodynamic quantity as a function of temperature from a single simulation run. As an example, we show in Fig. 3 the average potential energy as a function of temperature calculated from independent runs of the three algorithms together with that from Monte Carlo results of Ref. [\cite=EH96d] which rely on a multicanonical Monte Carlo simulation. The results all agree within error bars.

Another example of such a calculation is the average overlap as a function of temperature. The results are essentially the same for the three algorithms. That from the MD algorithm is shown in Fig. 4. We see that the average overlap approaches 1 in the limit the temperature going to zero, as it should (see Eq. ([\ref=eqordp])). We remark that the average overlap approaches the other limiting value, zero (see Eq. ([\ref=eqordp])), only very slowly as the temperature increases. This is because < O > T ~  = 0 corresponds to a completely random distribution of dihedral angles which is energetically highly unfavorable because of the steric hindrance of both main and side chains.

CONCLUSIONS In this article we have shown that the generalized-ensemble algorithm based on a special realisation of Tsallis weights is not restricted to Monte Carlo simulations, but can also be used in combination with other simulation methods such as molecular dynamics, Langevin, and hybrid Monte Carlo algorithms. We have tested the performances of the above three methods in the generalized ensemble for a simple peptide, Met-enkephalin. The results were comparable to those of the original Monte Carlo version [\cite=HO96d] in that the rate of convergence to the generalized ensemble is of the same order and that the thermodynamic quantities calculated as functions of temperature all agree with each other. We believe that there is a wide range of applications for the generalized-ensemble versions of molecular dynamics and related algorithms. For instance, the generalized-ensemble MD simulations may prove to be a valuable tool for refinement of protein structures inferred from X-ray and/or NMR experiments.

Acknowledgements: Our simulations were performed on the computers of the Computer Center at the Institute for Molecular Science, Okazaki, Japan. This work is supported, in part, by a Grant-in-Aid for Scientific Research from the Japanese Ministry of Education, Science, Sports and Culture.

Figure Captions

FIG. 1. (a) Time series of the total potential energy E (kcal/mol) from a Langevin simulation in the new generalized ensemble. The simulation consists of 800,000  ×  19 time steps with step size Δt  =  0.000028. (b) Time series of E from a molecular dynamics simulation in the new generalized ensemble. The simulation consists of 800,000  ×  19 time steps with step size Δt  =  0.0075. (c) Time series of E from a hybrid Monte Carlo simulation in the new generalized ensemble. The simulation consists of 400,000 MC steps. For each MC step an MD run of 19 time steps was made with step size Δt  =  0.01375.

FIG. 2. Time series of overlap function O (as defined in the text) from the hybrid Monte Carlo simulation in the new generalized ensemble. The simulation consists of 400,000 MC steps. For each MC step an MD run of 19 time steps was made with step size Δt  =  0.01375.

Fig. 3. The average potential energy < E > T (kcal/mol) as a function of temperature T (K) obtained from independent runs of the three algorithms and a multicanonical Monte Carlo simulation of Ref. [\cite=EH96d]

FIG. 4: The average overlap function < O > T (defined in the text) as a function of temperature T (K) obtained from the molecular dynamics simulation in the new generalized ensemble.