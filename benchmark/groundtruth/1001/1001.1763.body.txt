Infinite-message Interactive Function Computation in Collocated Networks[formula]

Introduction

This material is based upon work supported by the US National Science Foundation (NSF) under award (CAREER) CCF-0546598 and CCF-0915389. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

In this paper, we study, using a distributed block source coding framework, an interactive function computation problem in a collocated network where nodes take turns to broadcast messages over multiple rounds. Consider a network consisting of m source nodes and a sink node. Each source node observes a discrete memoryless stationary source. The sources at different nodes are independent. The sink does not observe any source and needs to compute a samplewise function of all the sources. To achieve this objective, the nodes take turns to broadcast t messages in total. Nodes are collocated, meaning that every message is recovered at every node without error. After all the message broadcasts, the sink computes the samplewise function. The communication is said to be interactive if t > m.

For all finite t, a single-letter characterization of the set of all feasible coding rates (the rate region) and the minimum sum-rate was provided in [\cite=ISIT09] using traditional information-theoretic techniques. This, however, does not lead to a satisfactory characterization of the infinite-message limit of the minimum sum-rate as the number of messages t tends to infinity. The objective of this paper is to provide a "limit-free" characterization of the infinite-message minimum sum-rate, i.e., it does not involve taking a limit as t  →    ∞  , and also an iterative algorithm to evaluate it. This result is similar to that provided in [\cite=Allerton09], where a two-terminal interactive function computation problem was studied. The infinite-message minimum sum-rate is the fundamental limit of cooperative function computation, where potentially an infinite number of infinitesimal-rate messages can be used. While the asymptotics of blocklength, rate, quantizer step-size, and network size have been explored in the distributed source coding literature, asymptotics involving an infinite number of messages has not, to the best of our knowledge, been studied and is not well understood.

In this paper, we view the infinite-message minimum sum-rate as a functional of the joint source pmf. The main result is the characterization this functional as the least element in a partially ordered family of functionals having certain convex-geometric properties. This characterization does not involve taking a limit as the number of messages goes to infinity. The proof of this main result suggests an iterative algorithm for evaluating the infinite-message minimum sum-rate functional. We demonstrate this algorithm through an example of computing the minimum function of three sources.

Related interactive computation problems in various networks have been studied in [\cite=Kumar2005] [\cite=GuptaISIT] [\cite=MassimoAllerton] [\cite=KumarCDC09] using the framework of communication complexity [\cite=Yao1979] [\cite=CommComplexity], where computation is required to be error-free. A function computation problem in a collocated network is studied in [\cite=Prabha] within a distributed block source coding framework, under the assumption that conditioned on the desired function, the observations of source nodes are independent. Multiround (interactive) function computation in a two-terminal network is studied in [\cite=OrlitskyRoche] [\cite=ISIT08] [\cite=Allerton09] within a distributed block source coding framework. The impact of transmission noise on function computation is considered in [\cite=Gallager88] [\cite=Srikant] [\cite=GastparMAC] but without a block coding rate.

The rest of this paper is organized as follows. In Sec. [\ref=sec:problem], we setup the problem and recap previous results. In Sec. [\ref=sec:characterization] we provide the main result, a "limit-free" characterization of the infinite-message minimum sum-rate. In Sec. [\ref=sec:itera] we present an iterative algorithm for evaluating the minimum sum-rate functional and demonstrate it through an example.

Interactive Computation in Collocated Networks

Problem formulation

Consider a network consisting of m source nodes numbered [formula], and an un-numbered sink (node). Each source node observes a discrete memoryless stationary source taking values in a finite alphabet. The sink has no source samples. For each [formula], let [formula] denote the n source samples which are available at node-j. In this paper, we assume sources are independent, i.e., for [formula], [formula] are iid pXm∈PXm where [formula] is the set of all product pmfs on [formula]. We adopt this assumption for two reasons: (1) to isolate the impact of the structure of the desired function on the efficiency of computation, (2) to obtain an exact characterization of the optimal efficiency. The general problem where the sources are dependent across nodes is open. Let [formula] be the function of interest at the sink and let [formula]. The tuple [formula], which denotes n samples of the samplewise function, needs be computed at the sink.

The communication is initiated by node-k. The nodes take turns to broadcast messages in t steps. In the i-th step, node-j, where j = (k + i - 1 mod m), generates a message as a function of the source samples [formula] and all the previous messages and broadcasts it. Nodes are collocated, meaning that every broadcasted message is recovered without error at every node. After t message broadcasts, the sink computes the samplewise function based on all the messages. If t > m, the communication is multi-round and will be called interactive.

A t-message distributed block source code for function computation initiated by node-k in a collocated network with parameters [formula] is the tuple [formula] consisting of t block encoding functions [formula] and a block decoding functions g, of block-length n, where for every [formula], j = (k + i - 1   mod m), The output of ei, denoted by Mi, is called the i-th message. The output of g is denoted by [formula]. For each i, (1 / n) log 2|Mi| is called the i-th block-coding rate (in bits per sample).

(i) Each message Mi could be a null message (|Mi| = 1). By incorporating null messages, the coding scheme described above subsumes all orders of messages transfers from m source nodes, and a t-round coding scheme subsumes a t'-round coding scheme if t' < t. (ii) Since the information available to the sink is also available to all source nodes, there is no advantage in terms of sum-rate to allow the sink to send any message. (iii) Although the problem studied in [\cite=ISIT09] is a special case with k = 1 and t = mr, where [formula] is the number of rounds, the characterizations for the rate region and the minimum sum-rate in [\cite=ISIT09] naturally extend to the general problem described above.

A rate tuple [formula] is admissible for t-message function computation initiated by node-k if, [formula], [formula] such that [formula], there exists a t-message distributed block source code with parameters [formula] satisfying

The set of all admissible rate tuples, denoted by Rkt, is called the operational rate region for t-message function computation initiated by node-k. The minimum sum-rate Rksum,t is given by [formula]. The focus of this paper is on the minimum sum-rate rather than the rate region.

(i) We allow the number of messages t to be equal to 0 and abbreviate Rksum,0 to Rsum,0 because there is no message transfer and the initial-node is irrelevant. (ii) For t < m, function computation may be infeasible, i.e., Rkt may be empty. If so, we define Rksum,t: =  +   ∞  . For special pXm and f, however, computation may be feasible even with t < m; in that case, Rksum,t would be finite. (iii) For all [formula], Rksum,t  ≥  Rksum,t + τ  ≥  0 holds, because the last τ messages could be null. Hence the limit lim t  →    ∞Rksum,t = :Rksum,  ∞ exists and is finite. (iv) For all [formula], Rksum,t  ≥  R(k - τ  mod m)sum,t + τ holds, because the first τ messages could be null. It follows that Rksum,  ∞ is independent of k and we abbreviate it to Rsum,  ∞. For all finite t, however, we keep the superscript in Rksum,t because this notation is convenient in the proof of Theorem [\ref=thm:functioncomp].

For all finite t, a single-letter characterization of Rkt and Rksum,t was provided in Theorem 1 and Corollary 1 of [\cite=ISIT09]. This, however, does not directly lead to a satisfactory characterization of the infinite-message limit Rsum,  ∞, which is a new dimension for asymptotic-analysis involving potentially an infinite number of infinitesimal-rate messages. The main contribution of this paper is a novel convex-geometric characterization of Rsum,  ∞.

Characterization of Rksum,t for finite t

[formula]

where Pkt(pXm) is the set of all pUt|Xm such that (i) H(f(Xm)|Ut) = 0, (ii) [formula], and (iii) the cardinalities of the alphabets of the auxiliary random variables Ut are upper-bounded by functions of [formula] and t.

The Markov chain conditions in Fact [\ref=fact:minsumrate] are equivalent to the following factorization of pUt|Xm:

[formula]

The cardinality bounds in Fact [\ref=fact:minsumrate] which can be derived using the Carathéodory theorem are omitted here for clarity. Although the exact expressions of the cardinality bounds are unimportant for our discussion, a key property that needs to be highlighted is that the bound on the alphabet of Ut increases exponentially with respect to (w.r.t.) t. Therefore the dimension of the optimization problem in [\ref=eqn:minsumrate] explodes as t increases.

Using Fact [\ref=fact:minsumrate], we could compute Rksum,t for a large t to approximate Rsum,  ∞. This is impractical because (i) the dimension of the optimization problem is large, (ii) the characterization of Rksum,t does not inform us how close Rksum,t is to Rsum,  ∞. Alternatively, we could compute Rksum,t for increasing values of t until |Rksum,t - 1 - Rksum,t| falls below a threshold. However, the dimensionality of the optimization problem grows exponentially with increasing values of t and there is no obvious way to reuse the computations done for evaluating Rksum,t - 1 when evaluating Rksum,t. Finally, if we need to evaluate Rsum,  ∞ for a different pXm, we need to repeat the entire process.

In Sec. [\ref=sec:characterization], we take a new fundamentally different approach. We first view Rsum,  ∞ as a functional of pXm for a fixed f. Then we develop a convex-geometric blocklength-free characterization of the entire functional Rsum,  ∞(pXm) which does not involve taking a limit as t  →    ∞  . This leads to a simple test for checking if a given achievable sum-rate functional of pXm coincides with Rsum,  ∞(pXm). It also provides a whole new family of lower bounds for Rsum,  ∞. In Sec. [\ref=sec:itera], we use the new characterization to develop an iterative algorithm for computing the functional Rsum,  ∞(pXm) and Rksum,t(pXm) (for any finite t) in which, crudely speaking, the complexity of computation in each iteration does not grow with iteration number, and results from the previous iteration are reused in the following one. We demonstrate the iterative algorithm through an example.

Characterization of Rsum,  ∞(pXm)

The rate reduction functional ρkt(pXm)

If the goal is to losslessly reproduce the sources, the minimum sum-rate is equal to [formula] because the sources are independent. The minimum sum-rate for function computation cannot be larger than that for lossless source reproduction. The reduction in the minimum sum-rate for function computation in comparison to source reproduction is given by

[formula]

A quantity which plays a key role in the characterization of Rsum,  ∞ is ρ0 - the "rate reduction" for zero messages (there are no auxiliary random variables in this case). Let Error-free computations can be performed without any message transfers if, and only if, pXm∈  Pf. Thus,

[formula]

If f(xm) is not constant, for all pXm∈Pf, we have [formula]. Such pXm can only lie on the boundary of PXm.

Evaluating Rksum,t is equivalent to evaluating the rate reduction ρkt. It turns out, however, that ρ∞: =  lim t  →    ∞ρkt = H(Xm) - Rsum,  ∞ is easier to characterize than Rsum,  ∞ (see Remark [\ref=rem:proofcomments2]). The rate reduction functional is the key to the characterization.

Main result

Generally speaking, ρkt, ρ0, and ρ∞ depend on pXm and f. We will fix f and view ρkt(pXm), ρ0(pXm), and ρ∞(pXm) as functionals of pXm to emphasize the dependence of pXm. Instead of evaluating ρ∞(pXm) for one particular pXm as it is done in the numerical evaluation of single-terminal and Wyner-Ziv rate-distortion functions, our approach is to characterize and evaluate the functional ρ∞(pXm) for the entire set of product distributions PXm rather than for one particular pXm. To describe the characterization of the functional ρ∞(pXm), it is convenient to define the following family of functionals.

The set of marginal-distributions-concave, ρ0-majorizing family of functionals F is the set of all the functionals [formula] satisfying the following conditions:

ρ0-majorization: [formula], ρ(pXm)  ≥  ρ0(pXm).

Concavity w.r.t. marginal distributions: For all [formula], with pXj held fixed for all j  ≠  k, [formula] is a concave function of pXk.

Since ρ0(pXm) =   -    ∞   for all pXm∉Pf, condition 1) of Definition [\ref=def:F] is trivially satisfied for all [formula] (we use the convention that [formula], a  >    -    ∞  ). Thus the statement that ρ majorizes ρ0 on the set PXm is equivalent to the statement that ρ majorizes H(Xm) on the set Pf.

Condition 2) does not imply that ρ(pXm) is concave w.r.t. the joint pmf pXm. In fact, PXm is not convex.

We now state and prove the main result of this paper.

(i) ρ∞∈  F. (ii) For all ρ∈  F, and all pXm∈  PXm, we have ρ∞(pXm)  ≤  ρ(pXm).

The set F is partially ordered w.r.t. majorization. Theorem [\ref=thm:functioncomp] says that ρ∞ is the least element of F. Note that there is no parameter t which needs to be sent to infinity in this characterization of ρ∞.

To prove Theorem [\ref=thm:functioncomp] we will establish a connection between the t-message interactive coding problem and a (t - 1)-message subproblem. Intuitively, to construct a t-message interactive code with initial-node k and [formula], we need to begin by choosing the first message which corresponds to choosing the auxiliary random variable U1. Then for each realization U1  =  u1, constructing the remaining part of the code becomes a (t - 1)-message subproblem with initial-node k+: = (k + 1 mod m) with the same desired function, but with a different joint source pmf [formula], where for all i  ≠  k, p'Xi = pXi and p'Xk = pXk|U1. We can repeat this procedure recursively to construct a (t - 1)-message interactive code. After t steps of recursion, we will be left with the trivial 0-message problem.

(i) We need to verify that ρ∞ satisfies the two conditions in Definition [\ref=def:F]:

1) Since [formula], Rsum,  ∞(pXm)  ≤  Rsum,0(pXm), we have ρ∞(pXm)  ≥  ρ0(pXm).

2) For any [formula], consider two arbitrary distributions pXk,0 and pXk,1, and arbitrary distributions pXj for all j  ≠  k. For u1 = 0,1, let [formula]. For λ∈(0,1), let pXm,λ: = λpXm,1  +  (1 - λ)pXm,0. We will show that ρ∞(pXm,λ)  ≥  λ  ρ∞(pXm,1) + (1 - λ)  ρ∞(pXm,0). Let U*1  ~   Ber(λ) and (Xm,U*1)  ~  pXm,u1pU*1(u1), which imply pXm  =  pXm,λ∈PXm and pXm|U*1(  ·  |u1)  =  pXm,u1∈PXm. For all [formula] we have,

[formula]

In step (a) we replaced pU1|Xk with the particular pU*1|Xk defined above. Step (b) follows from the "law of total conditional entropy" with the additional observations that conditioned on U*1  =  u1, pXm|U*1(  ·  |u1)  =  pXm,u1 and H(Xm|Ut2,U*1 = u1) only depends on pUt2|XmU*1(  ·  |  ·  ,u1). Step (c) is due to the observation that for a fixed pU*1|Xk, conditioned on U*1 = u1, pU*1|XkpUt2|XmU*1∈  Pkt(pXm,u1) iff pUt2|XmU*1∈  Pk+t - 1(pXm,u1). Now send t to infinity in both the left and right sides of ([\ref=eqn:convexify1]). Since lim t  →    ∞ρkt  =   lim t  →    ∞ρk+t  =  ρ∞, we have ρ∞(pXm,λ)  ≥  λ  ρ∞(pXm,1) + (1 - λ)  ρ∞(pXm,0). Therefore, ρ∞ satisfies condition 2) in Definition [\ref=def:F]. Thus, ρ∞∈  F.

(ii) It is sufficient to show that: [formula], [formula], [formula], [formula], ρkt(pXm)  ≤  ρ(pXm). We prove this by induction on t. For t  =  0, the result is true by condition 1) in Definition [\ref=def:F]. Assume that for an arbitrary [formula], ρkt - 1(pXm)  ≤  ρ(pXm) holds. We will show that ρkt(pXm)  ≤  ρ(pXm) holds.

[formula]

[formula]

The reasoning for steps (d) and (e) are similar to those for steps (b) and (c) respectively in the proof of part (i) (see equation array ([\ref=eqn:convexify1])). In step (e) we need to use the fact that pXm|U1(  ·  |u1)∈PXm, which is due to ([\ref=eqn:mc]) and the assumption that pXm∈PXm. Step (f) is due to the induction hypothesis ρkt - 1(pXm)  ≤  ρ(pXm) for all k. Step (g) is due to the Markov chain U1  -  Xk  -  (Xk - 1Xmk + 1) and because Xk and (Xk - 1Xmk + 1) are independent. Step (h) is Jensen's inequality applied to ρ(pXk  ·  pXk - 1Xmk + 1) which is concave w.r.t. pXk.

Since every ρ∈  F gives an upper bound for ρ∞, (H(Xm) - ρ) gives a lower bound for Rsum,  ∞. This fact provides a method for testing if an achievable sum-rate functional is optimal. If R*(pXm) is an achievable sum-rate functional then [formula], R*(pXm)  ≥  Rsum,  ∞(pXm). If it can be verified that ρ*: = (H(Xm)  -  R*)∈F, then by Theorem [\ref=thm:functioncomp], R*  =  Rsum,  ∞.

Iterative algorithm

Although Theorem [\ref=thm:functioncomp] provides a characterization of ρ∞ and Rsum,  ∞ that is not obtained by taking a limit, it does not directly provide an algorithm to evaluate Rsum,  ∞. To efficiently represent and search for the least element of F is nontrivial because each element is a functional; not a scalar. The proof of Theorem [\ref=thm:functioncomp], however, inspires an iterative algorithm for evaluating Rksum,t and Rsum,  ∞.

Equation ([\ref=eqn:convexify]) states that ρkt(pXm) is the maximum value of [formula] such that [formula] is a finite convex combination of {(pXm|U1(  ·  |u1), [formula], where pXm(  ·  ) and pXm|U1(  ·  |u1) have the same marginal distributions pXj for all j  ≠  k and differ only on pXk. Now we fix the marginal distributions pXj for all j  ≠  k, and consider the hypograph of ρk+t - 1 w.r.t. pXk: [formula]. Due to ([\ref=eqn:convexify]), the convex hull of pXkρk+t - 1 is pXkρkt. This relation enables us to evaluate ρkt from ρk+t - 1: fixing pXj for all j  ≠  k, ρkt is the least concave functional w.r.t. pXk that majorizes ρk+t - 1. In the convex optimization literature, ( - ρkt) is called the double Legendre-Fenchel transform or convex biconjugate of ( - ρk+t - 1) [\cite=ConvexAnalysis]. We have the following iterative algorithm.

Algorithm to evaluate Rksum,t

Initialization: For all [formula], define ρk0(pXm)  =  ρ0(pXm) by equation ([\ref=eqn:rho0]) for all pXm in [formula].

Loop: For τ = 1 through t do the following. For every [formula] do the following. For every set of marginal distributions {pXj}mj = 1,j  ≠  k do the following.

Construct pXkρk+τ - 1.

Let ρkτ be the upper boundary of the convex hull of pXkρk+τ - 1.

Output: Rksum,t(pXm) = H(Xm) - ρkt(pXm).

To make numerical computation feasible, PXm has to be discretized. Once discretized, however, in each iteration, the amount of computation is the same and is fixed by the discretization step-size. Also note that results from each iteration are reused in the following one. Therefore, for large t, the complexity to compute Rksum,t grows linearly w.r.t. t.

Rsum,  ∞ can also be evaluated to any precision, in principle, by running this iterative algorithm for [formula], until some stopping criterion is met, e.g., the maximum difference between ρkt - 1 and ρkt on PXm falls below some threshold. Developing stopping criteria with precision guarantees requires some knowledge of the rate of convergence which is not established in this paper and will be explored in future work. When the objective is to evaluate Rsum,  ∞(pXm) for all pmfs in PXm, this iterative algorithm is much more efficient than using ([\ref=eqn:minsumrate]) to solve for Rksum,t for each pXm for [formula], an approach which follows the definition of Rsum,  ∞ literally as the limit of Rksum,t as t  →    ∞  . Our iterative algorithm is based on Theorem [\ref=thm:functioncomp] which is a characterization of Rsum,  ∞ without taking a limit involving t.

Example: (MIN function) Take m = 3 nodes. Xi  ~   Ber(pi). f(x3) =  min i = 1,2,3xi. The joint pmf pX3 is parameterized by [formula]. It is easy to see that where [formula] or p1p2p3 = 0}.

Now let us fix pX1 and pX2 and apply the convex biconjugate operation on ρ0 w.r.t. pX3 to obtain ρ31. Specifically, for every fixed (p1,p2), we focus on ρ0 on the line segment {p1}  ×  {p2}  ×  [0,1] and convexify p3ρ0  =  {(p3,ρ):ρ  ≤  ρ0(p1,p2,p3)} to obtain p3ρ31. Then we repeat this procedure but applying the convexification operation w.r.t. pX2, pX1, etc to obtain ρ22, ρ13, etc. In numerical computation, [formula] takes values on a discrete grid where p1,p2,p3 are multiples of a finite step size Δ. The convexification operation involves finding a convex hull of a finite number of points in a plane.

As we decrease Δ and increase t, ρt approximates ρ∞. Fig. [\ref=fig:stepsize] shows ρt(1 / 2,1 / 2,1 / 2) for different t and Δ. For each Δ, ρt converges as t increases. For a small enough Δ (fine enough discretization), the limit represents the actual value of ρ∞(1 / 2,1 / 2,1 / 2). Notice that for a small enough Δ, ρt keeps increasing as t grows, which means there is always an improvement for using more messages.

Fig. [\ref=fig:layer] shows the plots of the rate reduction function with t = 40 for four values of p3.