The adaptive and the thresholded Lasso

for potentially misspecified models

July, 2010

Sara van de Geer*, Peter Bühlmann*, and Shuheng Zhou**

*Seminar for Statistics, ETH Zürich

**Department of Statistics, University of Michigan

Abstract We revisit the adaptive Lasso as well as the thresholded Lasso with refitting, in a high-dimensional linear model, and study prediction error, [formula]-error (q∈{1,2}), and number of false positive selections. Our theoretical results for the two methods are, at a rather fine scale, comparable. The differences only show up in terms of the (minimal) restricted and sparse eigenvalues, favoring thresholding over the adaptive Lasso. As regards prediction and estimation, the difference is virtually negligible, but our bound for the number of false positives is larger for the adaptive Lasso than for thresholding. Moreover, both these two-stage methods add value to the one-stage Lasso in the sense that, under appropriate restricted and sparse eigenvalue conditions, they have similar prediction and estimation error as the one-stage Lasso, but substantially less false positives.

Keywords: adaptive Lasso, estimation, prediction, restricted eigenvalue, thresholding, variable selection

Running Head: Adaptive and thresholded Lasso

Introduction

Consider the linear model

[formula]

where [formula] is a vector of coefficients, [formula] is an (n  ×  p)-design matrix, and [formula] is an n-vector of noisy observations, ε being the noise term. We examine the case p  ≥  n, i.e., a high-dimensional situation. The design matrix [formula] is treated as fixed, and the Gram matrix is denoted by [formula]. Throughout, we assume the normalization j,j  =  1 for all [formula].

This paper presents a theoretical comparison between the thresholded Lasso with refitting and the adaptive Lasso. Both methods are very popular in practical applications for reducing the number of active variables.

We emphasize here and describe later that we allow for model misspecification where the true regression function may be non-linear in the covariates. For such cases, we can consider the projection onto the linear span of the covariates. The (projected or true) linear model does not need to be sparse nor do we require that the non-zero regression coefficients (from a sparse approximation) are "sufficiently large". As for the latter, we will show in Lemma [\ref=min.lemma] how this can be invoked to improve the result. Furthermore, we also do not require the stringent irrepresentable conditions or incoherence assumptions on the design matrix [formula] but only some weaker restricted or sparse eigenvalue conditions.

Regularized estimation with the [formula]-norm penalty, also known as the Lasso ([\cite=Tib96]), refers to the following convex optimization problem:

[formula]

where λ > 0 is a penalization parameter.

Regularization with [formula]-penalization in high-dimensional scenarios has become extremely popular. The methods are easy to use, due to recent progress in specifically tailored convex optimization ([\cite=mevdgpb08], [\cite=fht08]).

A two-stage version of the Lasso is the so-called adaptive Lasso

[formula]

Here, init is the one-stage Lasso defined in ([\ref=eq.init]), with initial tuning parameter λ  =  λinit, and λadap > 0 is the tuning parameter for the second stage. Note that when |j,init| = 0, we exclude variable j in the second stage. The adaptive Lasso was originally proposed by [\cite=Zou06].

Another possibility is the thresholded Lasso with refitting. Define

[formula]

which is the set of variables having estimated coefficients larger than some given threshold λthres. The refitting is then done by ordinary least squares:

[formula]

where, for a set [formula], βS has coefficients different from zero at the components in S only.

We will present bounds for the prediction error, its [formula]-error (q∈{1,2}), and the number of false positives. The bounds for the two methods are qualitatively the same. A difference is that our variable selection properties results for the adaptive Lasso depend on its prediction error, whereas for the thresholded Lasso, variable selection can be studied without reference to its prediction error. In our analysis this leads to a bound for the number of false positives of the thresholded Lasso that is smaller than the one for the adaptive Lasso, when restricted or sparse minimal eigenvalues are small and/or sparse maximal eigenvalues are large.

Of course, such comparisons depend on how the tuning parameters are chosen. Choosing these by cross validation is in our view the most appropriate, but it is beyond the scope of this paper to present a mathematically rigorous theory for the cross validation scheme for the adaptive and/or thresholded Lasso (see [\cite=arlot2010survey] for a recent survey on cross validation).

Related work

Consistency results for the prediction error of the Lasso can be found in [\cite=GR04]. The prediction error is asymptotically oracle optimal under certain conditions on the design matrix [formula], see e.g. [\cite=Bunea:06] [\cite=Bunea:07a] [\cite=Bunea:07b], [\cite=vandeG08], [\cite=bickel2009sal], [\cite=koltch09a] [\cite=koltch09b], where also estimation in terms of the [formula]- or [formula]-loss is considered. The "restricted eigenvalue condition" of [\cite=bickel2009sal] (see also [\cite=koltch09a] [\cite=koltch09b]) plays a key role here. Restricted eigenvalue conditions are implied by, but generally much weaker than, "incoherence" conditions, which exclude high correlations between co-variables. Also [\cite=CP09] allow for a major relaxation of incoherence conditions, using assumptions on the set of true coefficients.

There is however a bias problem with [formula]-penalization, due to the shrinking of the estimates which correspond to true signal variables. A discussion can be found in [\cite=Zou06], and [\cite=ME07]. Moreover, for consistent variable selection with the Lasso, it is known that the so-called "neighborhood stability condition" ([\cite=MB06]) for the design matrix, which has been re-formulated in a nicer form as the "irrepresentable condition" ([\cite=ZY07]), is sufficient and essentially necessary. [\cite=Wai07] [\cite=Wai08] analyzes the smallest sample size needed to recover a sparse signal under certain incoherence conditions, Because irrepresentable or incoherence conditions are restrictive and much stronger than restricted eigenvalue conditions (see [\cite=vdG:2009] for a comparison), we conclude that the Lasso for exact variable selection only works in a rather narrow range of problems, excluding for example some cases where the design exhibits strong (empirical) correlations.

Regularization with the [formula]-"norm" with q < 1 will mitigate some of the bias problems, see [\cite=zhang2010nearly]. Related are multi-step procedures where each of the steps involves a convex optimization only. A prime example is the adaptive Lasso which is a two-step algorithm and whose repeated application corresponds in some "loose" sense to a non-convex penalization scheme ([\cite=zouli2008]). [\cite=Zou06] analyzed the adaptive Lasso in an asymptotic setup for the case where p is fixed. Further progress in the high-dimensional scenario has been achieved by [\cite=HMZ08]. Under a rather strong mutual incoherence condition between every pair of relevant and irrelevant covariables, they prove that the adaptive Lasso recovers the correct model and has an oracle property. As we will explain in Subsection [\ref=irrepresentable.section], the adaptive Lasso indeed essentially needs a - still quite restrictive - weighted version of the irrepresentable condition in order to be able to correctly estimate the support of the coefficients.

[\cite=MY08] examine the thresholding procedure, assuming all non-zero components are large enough, an assumption we will avoid. Thresholding and multistage procedures are also considered in [\cite=candes2006stable], [\cite=candes14enhancing]. In [\cite=Zhou09] [\cite=Zhou10], it is shown that a multi-step thresholding procedure can accurately estimate a sparse vector [formula] under the restricted eigenvalue condition of [\cite=bickel2009sal]. The two-stage procedure in [\cite=Zhang09] applies "selective penalization" in the second stage. This procedure is studied assuming incoherence conditions. A more general framework for multi-stage variable selection was studied by [\cite=WR08]. Their approach controls the probability of false positives (type I error) but pays a price in terms of false negatives (type II error). The main contribution of this paper is that we provide bounds for the adaptive Lasso that are comparable to the bounds for the Lasso followed by a thresholding procedure. Because the true regression itself, or its linear projection, is perhaps not sparse, we moreover consider a sparse approximation of the truth, somewhat in the spirit of [\cite=zhang2008sparsity].

Organization of the paper

The next section introduces the sparse oracle approximation, with which we compare the initial and adaptive Lasso. In Section [\ref=preview.section], we present the main results. Eigenvalues and their restricted and sparse counterparts are defined in Section [\ref=notations.section]. Some conclusions are presented in Section [\ref=conclusion.section].

The rest of the paper presents intermediate results and complements for establishing the main results of Section [\ref=preview.section]. In Section [\ref=noiseless.section], we consider the noiseless case, i.e., the case where ε  =  0. The reason is that many of the theoretical issues involved concern the approximation properties of the two stage procedure, and not so much the fact that there is noise. By studying the noiseless case first, we separate the approximation problem from the stochastic problem.

Both initial and adaptive Lasso are special cases of a weighted Lasso. We discuss prediction error, [formula]-error (q∈{1,2}) and variable selection with the weighted Lasso in Subsection [\ref=weight.section]. Theorem [\ref=weight] in this section is the core of the present work, as regards prediction and estimation. Lemma [\ref=weightselect] in this section is the main result as regards variable selection. The behavior of the noiseless initial and adaptive Lasso are simple corollaries of Theorem [\ref=weight] and Lemma [\ref=weightselect]. We give in Subsection [\ref=init.section] the resulting bounds for the initial Lasso and discuss in Section [\ref=thres.section] its thresholded version. In Subsection [\ref=adap.section] we derive results for the adaptive Lasso by comparing it with a thresholded initial Lasso. Moreover, Subsection [\ref=irrepresentable.section] briefly discusses the weighted irrepresentable condition, to show that even the adaptive Lasso needs strong conditions on the design for exact variable selection. This subsection is linked to Corollary [\ref=min.corollary], where it is proved that the false positives of the adaptive Lasso vanish if the coefficients of the oracle are sufficiently large.

Section [\ref=noise.section] studies the noisy case. It is an easy extension of the results of Sections [\ref=weight.section], [\ref=init.section], [\ref=thres.section] and [\ref=adap.section]. We do however need to further specify the choice of the tuning parameters λinit and λadap. After explaining the notation, we present the bounds for the prediction error, estimation error and for the number of false positives, of the weighted Lasso. This then provides us with the tools to prove the main results.

All proofs are in Section [\ref=proofs.section]. Here, we also present explicit constants in the bounds to highlight the non-asymptotic character of the results.

Model misspecification, weak variables and the oracle

Let

[formula]

where [formula] is the regression function. First, we note that without loss of generality, we can assume that [formula] is linear. If [formula] is non-linear in the covariates, we consider its projection [formula] onto the linear space [formula], i.e.,

[formula]

It is not difficult to see that all our results still hold if [formula] is replaced by its projection [formula]. The statistical implication is very relevant. The mathematical argument is the orthogonality

[formula]

For ease of notation, we therefore assume from now on that [formula] is indeed linear:

[formula]

Nevertheless, βtrue itself may not be sparse. Denote the active set of βtrue by

[formula]

which has cardinality strue: = |Strue|. It may well be that strue is quite large, but that there are many weak variables, that is, many very small non-zero coefficients in βtrue. Therefore, the sparse object we aim to recover may not be the "true" unknown parameter [formula] of the linear regression, but rather a sparse approximation. We believe that an extension to the case where [formula] is only "approximately" sparse, better reflects the true state of nature. We emphasize however that throughout the paper, it is allowed to replace the oracle approximation b0 given below by βtrue. This would simplify the theory. However, we have chosen not to follow this route because it generally leads to a large price to pay in the bounds.

The sparse approximation of [formula] that we consider is defined as follows. For a set of indices [formula] and for [formula], we let

[formula]

Given a set S, the best approximation of [formula] using only variables in S is

[formula]

Thus, fS is the projection of [formula] on the linear span of the variables in S. Our target is now the projection fS0, where

[formula]

Here, |S| denotes the size of S. Moreover, φ2(6,S) is a "restricted eigenvalue" (see Section [\ref=notations.section] for its definition), which depends on the Gram matrix [formula] and on the set S. The constants are chosen in relation with the oracle result (see Corollary [\ref=noisyinit.corollary]). In other words, fS0 is the optimal [formula]-penalized approximation, albeit that it is discounted by the restricted eigenvalue φ2(6,S0). To facilitate the interpretation, we require S0 to be a subset of Strue, so that the oracle is not allowed to trade irrelevant coefficients against restricted eigenvalues. With S0  ⊂  Strue, any false positive selection with respect to Strue is also a false positive for S0.

We refer to fS0 as the "oracle". The set S0 is called the oracle active set, and b0  =  bS0 are the oracle coefficients, i.e.,

[formula]

We write s0  =  |S0|.

Inferring the sparsity pattern, i.e. variable selection, refers to the task of estimating the set of non-zero coefficients, that is, to have a limited number of false positives (type I errors) and false negatives (type II errors). It can be verified that under reasonable conditions with suitably chosen tuning parameter λ, the "ideal" estimator

[formula]

has O(λ2s0) prediction error and O(s0) false positives (see for instance [\cite=Barr:Birg:Mass:1999] and [\cite=vdG:2001]). With this in mind, we generally aim at O(s0) false positives (see also [\cite=Zhou10]), yet keeping the prediction error as small as possible (see Corollary [\ref=compare.corollary]).

As regards false negative selections, we refer to Subsection [\ref=falsenegatives.section], where we derive bounds based on the [formula]-error.

Main results

Main conditions

The behavior of the thresholded Lasso and adaptive Lasso depends on the tuning parameters, on the design, as well as on the true [formula], and actually on the interplay between these quantities. To keep the exposition clear, we will use order symbols. Our expressions are functions of n, p, [formula], and [formula], and also of the tuning parameters λinit, λthres, and λadap. For positive functions g and h, we say that g = O(h) if [formula] is bounded, and [formula] if in addition [formula] is bounded. Moreover, we say that g  =  Osuff(h) if [formula] is not larger than a suitably chosen sufficiently small constant, and [formula] if in addition [formula] is bounded.

Our results depend on restricted eigenvalues φ(L,S,N), minimal restricted eigenvalues φ min(L,S,N), and minimal sparse eigenvalues φsparse(S,N) (which we generally think of as being not too small), as well on maximal sparse eigenvalues Λsparse(s) (which we generally think of being not too large). The exact definition of these constants is given in Section [\ref=notations.section].

To simplify the expressions, we assume throughout that

[formula]

(where φ(6,S0)  =  φ(6,S0,s0)), which roughly says that the oracle "squared bias" term is not substantially larger than the oracle "variance" term. For example, in the case of orthogonal design, this condition holds if the small non-zero coefficients are small enough, or if there are not too many of them, i.e., if

[formula]

We stress that ([\ref=biasvariance]) is merely to write order bounds for the oracle, bounds with which we compare the ones for the various Lasso versions. If actually the "squared bias" term is the dominating term, this mathematically does not alter the theory but makes the result more difficult to interpret.

We will furthermore discuss the results on the set

[formula]

where [formula] is the j-th column of the matrix [formula]. For an appropriate choice of λinit, depending on the distribution of ε, the set T has large probability. Typically, λinit can be taken of order

[formula]

The next lemma serves as an example, but the results can clearly be extended to other distributions.

Suppose that ε  ~  N(0,σ2I). Take for a given t > 0,

[formula]

Then

[formula]

The following conditions play an important role. Conditions A and AA for thresholding are similar to those in [\cite=Zhou10] (Theorems 1.2, 1.3 and 1.4).

Condition A For the thresholded Lasso, the threshold level λthres is chosen sufficiently large, in such a way that

[formula]

Condition AA For the thresholded Lasso, the threshold level λthres is chosen sufficiently large, but such that

[formula]

Condition B For the adaptive Lasso, the tuning parameter λadap is chosen sufficiently large, in such a way that

[formula]

Condition BB For the adaptive Lasso, the tuning parameter λadap is chosen sufficiently large, but such that

[formula]

The above conditions can be considered with a zoomed-out look, neglecting the expressions in the square brackets (

[formula]

The results

The next three theorems contain the main ingredients of the present work. Theorem [\ref=noisyinit.theorem] is not new (see e.g. [\cite=Bunea:06] [\cite=Bunea:07a] [\cite=Bunea:07b], [\cite=bickel2009sal], [\cite=koltch09a]), albeit that we replace the perhaps non-sparse βtrue by the sparser b0 (see also [\cite=vandeG08]). Recall that the latter replacement is done because it yields generally an improvement of the bounds.

For the initial Lasso init  =   defined in ([\ref=eq.init]), we have on T,

[formula]

and

[formula]

and

[formula]

The next theorem discusses thresholding. The results correspond to those in [\cite=Zhou10], and will be invoked to prove similar bounds for the adaptive Lasso, as presented in Theorem [\ref=noisyadap.theorem].

Suppose Condition A holds. Then on T,

[formula]

and

[formula]

and

[formula]

and

[formula]

Suppose Condition B holds. Then on T,

[formula]

and

[formula]

and

[formula]

and

[formula]

We did not present a bound for the number of false positives of the initial Lasso: it can be quite large depending on further conditions as given in Lemma [\ref=refineselect.lemma]. A rough bound is presented in Lemma [\ref=Lambdamax.lemma].

Theorem [\ref=noisythres.theorem] and [\ref=noisyadap.theorem] show how the results depend on the choice of the tuning parameters λthres and λadap. The following corollary takes the choices of Conditions AA and BB, as these choices give the smallest prediction and estimation error.

Suppose we are on T. Then, under Condition AA,

[formula]

and

[formula]

and

[formula]

and

[formula]

Similarly, under Condition BB,

[formula]

and

[formula]

and

[formula]

and

[formula]

Note that our conditions on λthres and λadap depend on the φ's and Λ's, which are unknown. Indeed, our study is of theoretical nature, revealing common features of thresholding and the adaptive Lasso. Furthermore, it is possible to remove the dependence of the φ's and Λ's, when one imposes stronger sparse eigenvalue conditions, along the lines of [\cite=zhang2008sparsity]. In practice, the tuning parameters are generally chosen by cross validation.

Comparison with the Lasso

At the zoomed-out level, where all φ's and Λ's are neglected, we see that the thresholded Lasso (under Condition AA) and the adaptive Lasso (under Condition BB) achieve the same order of magnitude for the prediction error as the initial, one-stage Lasso discussed in Theorem [\ref=noisyinit.theorem]. The same is true for their estimation errors. Zooming in on the φ's and the Λ's, their error bounds are generally larger than for the initial Lasso.

For comparison in terms of false positives, we need a corresponding bound for the initial Lasso. In the paper of [\cite=zhang2008sparsity], one can find results that ensure that also for the initial Lasso, modulo φ's and Λ's, the number of false positives is of order s0. However, this result requires rather involved conditions which also improve the bounds for the adaptive and thresholded Lasso. We briefly address this refinement in Subsection [\ref=refine.section], imposing a condition of similar nature as the one used in [\cite=zhang2008sparsity]. Also under these stronger conditions, the general message remains that thresholding and the adaptive Lasso can have similar prediction and estimation error as the initial Lasso, and are often far better as regards variable selection

In this section, we confine ourselves to the following lemma. Here, Λ2 max is the largest eigenvalue of [formula], which can generally be quite large.

On T,

[formula]

Comparison between adaptive and thresholded Lasso

When zooming-out, we see that the adaptive and thresholded Lasso have bounds of the same order of magnitude, for prediction, estimation and variable selection.

At the zoomed-in level, the adaptive and thresholded Lasso also have very similar bounds for the prediction error (compare ([\ref=threspredict]) with ([\ref=adappredict])) in terms of the φ's and Λ's. A similar conclusion holds for their estimation error. We remark that our choice of Conditions AA and BB for the tuning parameters is motivated by the fact that according to our theory, these give the smallest prediction and estimation errors. It then turns out that the "optimal" errors of the two methods match at a quite detailed level. However, if we zoom-in even further and look at the definition of φsparse, φ, and φ min in Section [\ref=notations.section], it will show up that the bounds for the adaptive Lasso prediction and estimation error are (slightly) larger.

Regarding variable selection, at zoomed-out level the results are also comparable (see ([\ref=thresselect]) and ([\ref=adapselect])). Zooming-in on the the φ's and Λ's, the adaptive Lasso may have more false positives than the thresholded version.

A conclusion is that at the zoomed-in level, the adaptive Lasso has less favorable bounds as the refitted thresholded Lasso. However, these are still only bounds, which are based on focussing on a direct comparison between the two methods, and we may have lost the finer properties of the adaptive Lasso. Indeed, the non-explicitness of the adaptive Lasso makes its analysis a non-trivial task. The adaptive Lasso is a quite popular practical method, and we certainly do not advocate that it should always be replaced by thresholding and refitting.

Bounds for the number of false negatives

The [formula]-error has immediate consequences for the number of false negatives: if for some estimator [formula], some target b0, and some constant δupperq one has

[formula]

then the number of undetected yet large coefficients cannot be very large, in the sense that

[formula]

Therefore, on T, for example

[formula]

Similar bounds hold for the thresholded and the adaptive Lasso (considering now, in terms of the φ's and Λ's, somewhat larger |b0j|).

One may argue that one should not aim at detecting variables that the oracle considers as irrelevant. Nevertheless, given an estimator [formula], it is straightforward to bound [formula] in terms of [formula]: apply the triangle inequality

[formula]

Moreover, for q = 2, one has the inequality

[formula]

where Λ min(S) is the smallest eigenvalue of the Gram matrix corresponding to the variables in S. One may verify that φ(6,Strue)  ≤  Λ min(Strue). In other words, by choosing βtrue as target instead of b0, does in our approach not lead to an improvement in the bounds for [formula].

Having large coefficients

Let us have a closer look at what conditions on the size of the coefficients can bring us. We only discuss the adaptive Lasso (thresholding again giving similar results, see also [\cite=Zhou10]).

We define

[formula]

Moreover, we let

[formula]

be the harmonic mean of the squared coefficients.

Condition C For the adaptive Lasso, take λadap sufficiently large, such that

[formula]

Condition CC For the adaptive Lasso, take λadap sufficiently large, but such that

[formula]

Suppose that for some constant δupper∞, on T,

[formula]

Assume in addition that

[formula]

Then under Condition C,

[formula]

and

[formula]

and

[formula]

and

[formula]

It is clear that by Theorem [\ref=noisyinit.theorem],

[formula]

This can be improved under coherence conditions on the Gram matrix. To simplify the exposition, we will not discuss such improvements in detail (see [\cite=Lou08]).

Under Condition CC, the bound for the prediction error and estimation error is again the smallest. We moreover have the following corollary for the number of false positives.

Assume the conditions of Lemma [\ref=min.lemma] and

[formula]

Then on T,

[formula]

By assuming that |b0|harm is sufficiently large, that is,

[formula]

one can bring [formula] down to zero, i.e., no false positives. One may verify that this boils down to a situation where the weighted irrepresentable condition holds: see Example [\ref=separate.example] in Subsection [\ref=irrepresentable.section].

As discussed in Section [\ref=falsenegatives.section], large non-zero coefficients also lead to a small number or eventually zero false negative selections. Therefore, the adaptive and thresholded Lasso are recovering the support of S0 if all of its non-zero coefficients are sufficiently large (in absolute value), assuming much weaker conditions on the design than the (unweighted) irrepresentable condition, which is necessary for the Lasso.

Notation and definition of generalized eigenvalues

We reformulate the problem in L2(Q), where Q is a generic probability measure on some space X. (This is somewhat more natural in the noiseless case, which we will consider in Section [\ref=noiseless.section].) Let {ψj}pj = 1  ⊂  L2(Q) be a given dictionary. For [formula], the function ψj will play the role of the j-th co-variable. The Gram matrix is

[formula]

We assume that Σ is normalized, i.e., that [formula] for all j. In our final results, we will actually take Σ  =  , the (empirical) Gram matrix corresponding to fixed design.

Write a linear function of the ψj with coefficients [formula] as

[formula]

The L2(Q)-norm is denoted by [formula], so that

[formula]

Recall that for an arbitrary [formula], and an arbitrary index set S, we use the notation

[formula]

We now present our notation for eigenvalues. We also introduce restricted eigenvalues and sparse eigenvalues.

Eigenvalues

The largest eigenvalue of Σ is denoted by Λ2 max, i.e.,

[formula]

We will also need the largest eigenvalue of a submatrix containing the inner products of variables in S:

[formula]

Its minimal eigenvalue is

[formula]

Restricted eigenvalues

A restricted eigenvalue is of similar nature as the minimal eigenvalue of Σ, but with the coefficients β restricted to certain subsets of [formula]. The restricted eigenvalue condition we impose corresponds to the so-called adaptive version as introduced in [\cite=vdG:2009]. It differs from the restricted eigenvalue condition in [\cite=bickel2009sal] or [\cite=koltch09a] [\cite=koltch09b]. This is due to the fact that we want to mimic the oracle fS0, that is, do not choose [formula] as target, so that we have to deal with a bias term [formula]. For a given S, our restricted eigenvalue condition is stronger than the one in [\cite=bickel2009sal] or [\cite=koltch09a] [\cite=koltch09b]. On the other hand, we apply it to the smaller set S0 instead of to Strue.

Define for an index set [formula], and for a set N  ⊃  S and constant L  >  0, the sets of restrictions

[formula]

Definition: Restricted eigenvalue. For N  ≥  |S|, we call

[formula]

the (L,S,N)-restricted eigenvalue. The (L,S,N)-restricted eigenvalue condition holds if φ(L,S,N) > 0. For the case N = |S|, we write φ(L,S): = φ(L,S,|S|). The minimal (L,S,N)-restricted eigenvalue is

[formula]

It is easy to see that φ min(L,S,N)  ≤  φ(L,S,N)  ≤  φ(L,S)  ≤  Λ min(S) for all L  >  0. It can moreover be shown that

[formula]

Sparse eigenvalues

The fact that we also need sparse eigenvalues is in line with the sparse Riesz condition occurring in [\cite=zhang2008sparsity].

Definition: Sparse eigenvalues. For [formula], the maximal sparse eigenvalue is

[formula]

For an index set [formula] with |S|  ≤  N, the minimal sparse eigenvalue is

[formula]

One easily verifies that for any set N with |N| = ks, [formula],

[formula]

Moreover, for all L  ≥  0,

[formula]

Conclusions

We present some comparable bounds for the adaptive Lasso and the thresholded Lasso with refitting and we also compared them to the ordinary Lasso. The framework of our analysis allows for misspecified linear models whose best linear projection is not necessarily sparse and with possibly small non-zero regression coefficients, i.e., many weak variables. This setting is much more realistic than the usual high-dimensional framework where the model is true with only a few but strong variables.

Estimating the support S0 of the non-zero coefficients is a hard statistical problem. The irrepresentable condition, which is essentially a necessary condition for exact recovery of the non-zero coefficients by the one-step Lasso, is much too restrictive in many cases. In this paper, our main focus is on having O(s0) false positives while achieving good prediction and estimation. This is inspired by the behavior of the "ideal" [formula]-penalized estimator.

We have examined thresholding the Lasso with least squares refitting and the adaptive Lasso. Our main conclusion is that both methods can have about the same prediction and estimation error as the one-stage ordinary Lasso, and that both gain over the one-stage Lasso in the sense of having less false positives. Moreover, according to our theory (and not exploiting the fact that the adaptive Lasso mimics thresholding and refitting using an "oracle" threshold), thresholding with least squares refitting and the adaptive Lasso perform equally well, even when considered at a rather fine scale. Our bounds for the adaptive Lasso are more sensitive to small (minimal) restricted eigenvalues or small minimal sparse eigenvalues, or large sparse maximal eigenvalues. Both thresholded and adaptive Lasso benefit from a situation with large non-zero coefficients of the oracle.

We do not give an account of the tightness of our bounds. The thresholded Lasso allows a rather direct analysis, and we believe there is little room for improvement of the bounds for this method. The analysis of the adaptive Lasso more involved. Our comparison to thresholding might not do justice to the adaptive Lasso. Indeed, we have not fully exploited the finer oracle properties of the adaptive Lasso.

In practice the the tuning parameters are often chosen by cross validation, which may correspond to a choice giving the smallest prediction error. It is not within the scope of this paper to prove that with cross validation, thresholding and the adaptive Lasso again have comparable theoretical performance, although we do believe this to be the case. As for the computational aspect, we observe the following. For the solution path for all λadap, the adaptive Lasso needs O(n|Ŝinit| min (n,|Ŝinit|)) essential operation counts. The same order of operation counts is needed when computing the thresholded Lasso for the whole solution path over all λthres. Therefore, the two methods are also computationally comparable.

The noiseless case

Consider a fixed target [formula]. Let [formula] and let [formula] be the projection of [formula] on the |S|-dimensional linear space spanned by the variables {ψj}j∈S. We denote the coefficients of fS by bS, i.e.,

[formula]

The oracle set S0 is defined by trading off dimension against fit, namely

[formula]

where the constants are now from Theorem [\ref=weight] (or its Corollary [\ref=init.corollary]). We call fS0 the oracle, and we let b0: = bS0, i.e., fS0  =  fb0.

For simplicity, we assume throughout that

[formula]

which roughly says that the approximation error does not overrule the penalty term.

The initial Lasso is

[formula]

We assume that the tuning parameter λinit is set at some fixed value. Of course, in the noiseless case, the optimal - in terms of prediction error - value for λinit is λinit = 0. However, in the noisy case, a strictly positive lower bound for λinit is dictated by the noise level. Write

[formula]

Let for δ  >  0,

[formula]

Then fSδinit = fbSδinit is the refitted Lasso after thresholding at δ. Note that we express explicitly the dependence of the thresholded estimator on the threshold level, which we now call δ (instead of λthres as we did in the introduction). The reason for this is that the analysis of the adaptive Lasso will go via the thresholded Lasso with a choice of the threshold δ that trades off prediction error against estimation error (see ([\ref=choicedelta]) in the proof of Theorem [\ref=adap.theorem]).

The adaptive Lasso is

[formula]

The second stage tuning parameter λadap is again assumed to be strictly positive. We denote the resulting adaptive variants of ([\ref=initdef]) by

[formula]

As the initial and adaptive Lasso are special cases of the weighted Lasso, many of the results in Subsections [\ref=init.section], [\ref=thres.section] and [\ref=adap.section] are consequences of those for the weighted Lasso as studied in Subsection [\ref=weight.section]. The weighted Lasso is

[formula]

where the {wj}pj = 1 are non-negative weights.

We set fweight: = fβweight, [formula]. Moreover, we define

[formula]

By the reparametrization β  ↦  γ: = Wβ, where [formula], one sees that the weighted Lasso is a standard Lasso with Gram matrix

[formula]

We emphasize however that Σweight is generally not normalized, i.e., generally [formula].

The weighted Lasso

We first present a bound for the prediction and estimation error and then consider variable selection.

Let S be an index set with cardinality s: = |S|, satisfying for some constants M  ≥  0 and L > 0,

[formula]

Then for all β, we have

[formula]

Moreover, for all β, we have

[formula]

Finally, it holds for all β, that

[formula]

We will apply the above theorem with S the set of the smaller weights.

Fix some arbitrary δ > 0, and let

[formula]

The indices j with wj  =  1 / δ can be put in either Sδweight or in its complement. Suppose that for some α  ≥  0,

[formula]

Taking S = Sδweight, L = 1 and M  =  1 / δ in Theorem [\ref=weight], we get that for all β,

[formula]

Moreover,

[formula]

and

[formula]

In the case α = 0, one may replace in the last bound, φ2 min(2,S0,(2 + α)s0)  =  φmin(2,S0,2s0) by φ(2,S0,2s0).

Our next theme is variable selection. The Karush-Kuhn-Tucker (KKT) conditions (see [\cite=bertsimas1997introduction]) can be invoked to derive Lemma [\ref=weightselect] below, where we use the notation

[formula]

It holds that

[formula]

If [formula], we have

[formula]

The initial Lasso

Recall that

[formula]

For q  ≥  1, we define

[formula]

The prediction error of the initial Lasso has

[formula]

and its estimation error has

[formula]

The initial estimator has number of false positives

[formula]

Considering the variable selection result, it is clear that [formula]. Without further conditions, this cannot be refined, and the eigenvalue Λ2 max can be quite large (yet having the minimal eigenvalue of Σ bounded away from zero). Therefore, the result of Theorem [\ref=init.theorem] needs further conditions for good variable selection properties of the initial Lasso.

Thresholding the initial estimator

Variable selection results by thresholding are not difficult to obtain:

[formula]

Hence, for [formula], we get for q∈{1,2},

[formula]

If the coefficients of the oracle are sufficiently large, thresholding will improve the prediction and estimation error. Here, we do not impose such minimal size conditions. The estimation error of the thresholded Lasso is then still easy to assess. Our bound for the prediction error, however, now depends on maximal sparse eigenvalues.

At this stage, we invoke the noiseless counterparts of Conditions A and AA.

Condition a We have λinit  /  φ2(2,S0)  =  Osuff(δ).

Condition aa We have [formula].

Assume Condition a. Then

[formula]

[formula]

and

[formula]

The expressions for the prediction and estimation error lead to favoring the choice [formula] of Condition aa, which yields

[formula]

[formula]

and

[formula]

The adaptive Lasso

Observe that the adaptive Lasso is somewhat more reluctant than thresholding and refitting: the latter ruthlessly disregards all coefficients with |βj,init|  ≤  δ (i.e., these coefficients get penalty ∞  ), and puts zero penalty on coefficients with |βj,init|  >  δ. The adaptive Lasso gives the coefficients with |βj,init|  ≤  δ a penalty of at least λinit(λadap  /  δ) and those with |βj,init|  >  δ a penalty of at most λinit(λadap  /  δ). (Looking ahead, we will actually need to choose λadap  ≥  δ in the noisy case, see Theorem [\ref=noisyadap.theorem].)

Recall

[formula]

The noiseless versions of Conditions B and BB are:

Condition b We have

[formula]

Condition bb We have

[formula]

Note the slight discrepancy with the noisy versions: the noiseless versions are somewhat better. This is due to the fact that we also will need to choose λadap large enough to handle the noise.

Assume Condition b. Then

[formula]

and

[formula]

and

[formula]

and

[formula]

Considering the bounds for the prediction and estimation error leads to favoring the choice of Condition bb, giving

[formula]

[formula]

[formula]

and

[formula]

The weighted irrepresentable condition

This subsection will show that, even in the noiseless case, exact variable selection needs rather strong conditions. It serves as a motivation for the perhaps more moderate aim of having O(s0) (≤  O(strue)) false positives and detecting only the larger coefficients. Moreover, we illustrate in Example [\ref=separate.example] of this subsection that the lower bound on the non-zero coefficients as given in Corollary [\ref=min.corollary] is tight.

It is known that the initial Lasso essentially needs the irrepresentable condition in order to have no false positives ({[\cite=ZY07]). Similar statements can be made for the weighted Lasso.

For a (p  ×  p) -matrix Σ  =  (σj,k). we define

[formula]

[formula]

We let WS: = diag({wj}j∈S).

Definition We say that the weighted irrepresentable condition holds for S if for all vectors [formula] with [formula], one has

[formula]

The reparametrization β  ↦  γ: = W- 1β leads to the following lemma, which is the weighted variant of the first part of Lemma 6.2 in [\cite=vdG:2009]. Here, we actually take [formula] as target, instead of its [formula]-sparse approximation fS0. Recall

[formula]

[formula] Suppose the weighted irrepresentable condition is met for Strue. Then Sweight  ⊂  Strue.

We now consider conditions for the weighted irrepresentable condition to hold.

Suppose that

[formula]

Then the weighted irrepresentable condition holds for S.

The next example shows that the result of Lemma [\ref=boundirr.lemma] cannot be improved (essentially, up to the strict inequality) without assuming further conditions.

Adding noise

After introducing the notation for the noisy case (Subsection [\ref=noisynotation.section]), we will give the extension of the results for the weighted Lasso to the noisy case (see Theorem [\ref=weightnoise]). Once this is done, results for the initial Lasso, its thresholded version, and for the adaptive Lasso, follow in the same way as in Subsections [\ref=init.section], [\ref=thres.section] and [\ref=adap.section]. The new point is to take care that the tuning parameters are chosen in such a way that the noisy part due to variables in Sc0 are overruled by the penalty term. In our situation, this can be done by taking λinit, as well as λadap  ≥  λinit sufficiently large.

We provide the result for the noisy weighted Lasso in Subsection [\ref=noisyweight.section]. Theorems [\ref=noisyinit.theorem], [\ref=noisythres.theorem] and [\ref=noisyadap.theorem] follow from this and from some further results for the noisy case (their proofs are in Subsection [\ref=preview.proofs]). In Section [\ref=refine.section], we look at more restrictive sparse eigenvalue conditions in the spirit of [\cite=zhang2008sparsity].

Notation for the noisy case

Consider an n-dimensional vector of observations

[formula]

where [formula], with [formula] co-variables in some space X. Let {ψj}pj = 1 be a given dictionary.

The regression [formula], the dictionary {ψj}, and [formula] are now considered as vectors in [formula]. The norm we use is the normalized Euclidean norm

[formula]

induced by the inner product

[formula]

In other words, the probability measure Q is now [formula], the empirical measure of the co-variables [formula]. With some abuse of notation, we also write

[formula]

and

[formula]

The design matrix [formula] is

[formula]

We write the eigenvalues involved as before, e.g., Λ max is the largest eigenvalue of the empirical Gram matrix [formula], and φ2(L,S,N) is the (L,S,N)-restricted eigenvalue of [formula]. The projections in L2(Qn) are also written as before, i.e.

[formula]

The [formula]-sparse projection [formula] is now defined with a larger constant (7 instead of 3) in front of the penalty term, and a larger constant (L = 6 instead of L = 2) in the restrictions of the restricted eigenvalue condition:

[formula]

(compare with formula ([\ref=defineS_0])).

The weighted Lasso is

[formula]

Let

[formula]

The initial and adaptive Lasso are defined as in Section [\ref=introduction.section]. We write init: = finit and adap: = fadap, with active sets [formula] and [formula], respectively. Let

[formula]

be the prediction error of the initial Lasso, and and, for q  ≥  1,

[formula]

be its [formula]-error. Denote the prediction error of the adaptive Lasso by

[formula]

The least squares estimator using only variables in S is also written with a "hat":

[formula]

A threshold level will be denoted by δ, instead of λthres as we do in Section [\ref=introduction.section]. The reason is again that we need to explicitly express dependence on the threshold level. With λthres the notation will be too complicated. We define, for any threshold δ  >  0,

[formula]

The refitted version after thresholding, based on the data [formula], is Ŝδinit.

To handle the (random) noise, we define the set

[formula]

This is the set where the (empirical) correlations between noise and design is "small".

Here λinit is chosen in such a way that

[formula]

where (1 - α) is the confidence we want to achieve.

The noisy weighted Lasso

Suppose we are on T. Let S be a set with cardinality s = |S|, which satisfies for some positive L and M

[formula]

and

[formula]

Then for all β,

[formula]

and

[formula]

[formula]

and

[formula]

[formula]

Moreover, under the condition λweightw minSc  ≥  1,

[formula]

[formula]

When [formula], this implies

[formula]

Another look at the number of false positives

Here, we discuss a refinement, assuming a condition corresponding to the one used in [\cite=zhang2008sparsity].

Condition D It holds for some s*  ≥  s0, that

[formula]

Suppose we are on T. Then under Condition D,

[formula]

Moreover, under Condition B,

[formula]

[formula]

Under Condition BB, this becomes

[formula]

[formula]

Under Condition D, the first term in the right hand side of ([\ref=adap**]) is generally the leading term. We thus see the adaptive Lasso replaces the potentially very large constant

[formula]

in the bound for the number of false positives of the initial Lasso by

[formula]

a constant which is close to 1 if the φ's do not differ too much.

Admittedly, Condition D is difficult to interpret. On the one hand, it wants s* to be large, but on the other hand, a large s* also can render Λsparse(s*) large. We refer to [\cite=zhang2008sparsity] for examples where Condition D is met.

Proofs

We present three subsections, containing respectively the proofs for Section [\ref=noiseless.section], Section [\ref=noise.section], and finally Section [\ref=preview.section].

Proofs for Section [\ref=noiseless.section]: the noiseless case

Proofs for Subsection [\ref=weight.section]: the noiseless weighted Lasso

Proof of Theorem [\ref=weight]. Take

[formula]

We have

[formula]

and hence

[formula]

[formula]

[formula]

Let N  ⊃  S, |N|  =  N. Then

[formula]

and

[formula]

Therefore,

[formula]

[formula]

Case i). If

[formula]

we get

[formula]

[formula]

It follows that

[formula]

But then, by the definition of restricted eigenvalue, and invoking the triangle inequality,

[formula]

[formula]

This gives

[formula]

[formula]

[formula]

[formula]

Hence,

[formula]

Case ii) If

[formula]

we get

[formula]

The first result of the Lemma now follows from taking N  =  S.

For the second result, we add in Case i), [formula] to the left and right hand side of ([\ref=important]):

[formula]

[formula]

[formula]

The same arguments now give

[formula]

[formula]

In Case ii), we have

[formula]

and also

[formula]

So then

[formula]

[formula]

Taking N  =  S gives the second result.

For the third result, we let N be the set S, complemented with the s0 largest - in absolute value - coefficients of (βweight)Sc. Then φ(2L,N)  ≤  φ(2,S,s + s0). Moreover, N  ≥  s0. Thus, from the second result, we get

[formula]

[formula]

Moreover, as is shown in Lemma 2.2 in [\cite=vdG:2009] (with original reference [\cite=candes2005decoding], and [\cite=candes2007dss]),

[formula]

[formula]

So then

[formula]

[formula]

[formula]

We now turn to the proof of Lemma [\ref=weightselect]. An important characterization of the solution βweight can be derived from the Karush-Kuhn-Tucker (KKT) conditions (see [\cite=bertsimas1997introduction]).

Weighted KKT-conditions We have

[formula]

Here, [formula], and moreover

[formula]

Proof of Lemma [\ref=weightselect]. By the weighted KKT conditions, for all j

[formula]

Hence,

[formula]

[formula]

On the other hand

[formula]

Thus, we arrive at inequality ([\ref=stillin]):

[formula]

Clearly,

[formula]

[formula]

Proofs for Subsection [\ref=init.section]: the noiseless initial Lasso

We first present the corollaries of Theorem [\ref=weight] and Lemma [\ref=weightselect] when we apply them to the case where all the weights are equal to one.

For the initial Lasso, wj = 1 for all j, so we can apply Corollary [\ref=ordered.corollary] with δ = 1 and Sδweight  =  S0. Let

[formula]

We have

[formula]

The estimation error can be bounded as follows:

[formula]

and

[formula]

Moreover, application of Lemma [\ref=weightselect] bounds the number of false positives:

[formula]

Proof of Theorem [\ref=init.theorem]. This is now a direct consequence of Corollary [\ref=init.corollary]. [formula]

Proofs for Subsection [\ref=thres.section]: the noiseless thresholded Lasso

We first provide some explicit bounds.

We have

[formula]

and

[formula]

and

[formula]

[formula]

and, for [formula],

[formula]

Proof of Lemma [\ref=betterthres]. To obtain the first result, we use

[formula]

Now,

[formula]

Moreover

[formula]

[formula]

Hence

[formula]

The [formula]-error of the second result follows by the same arguments.

The first inequality of the third result follows from the definition of fSδinit as projection, and the second follows from the triangle inequality, where we invoke that

[formula]

so that

[formula]

and thus

[formula]

The final result follows from

[formula]

[formula]

Proof of Theorem [\ref=thres.theorem].

Inserting the bound [formula] (see Theorem [\ref=init.theorem]), and [formula], we get for λinit  /  φ2(2,S0)  =  O(δ), [formula],

[formula]

[formula]

[formula]

and

[formula]

[formula]

Proofs for Subsection [\ref=adap.section]: the noiseless adaptive Lasso

We use that when [formula], then [formula]. Application of Corollary [\ref=ordered.corollary] then gives

We have, for all [formula], and all β

[formula]

and

[formula]

and

[formula]

and, from Lemma [\ref=betterthres],

[formula]

Furthermore, from Lemma [\ref=weightselect] ,

[formula]

If [formula], we have

[formula]

We note that in the above corollary, the use of the [formula]-error δ2 is rather crucial for the variable selection result: with the weights wj  =  1 / |βj,init|, we have

[formula]

With alternative weights [formula]. the theory can also be developed using only the [formula]-error δ1.

A further observation is that the above corollary is an obstructed oracle inequality, where the oracle is restricted to choose the index set as a thresholded set of the initial Lasso. Concentrating on prediction error, it leads to defining the "oracle" threshold as

[formula]

This oracle has active set Sδ0init, with size |Sδ0init|  =  O(s0). Our following considerations however will not be based on this optimal threshold, but rather on thresholds that allow a comparison with the results for the thresholded initial Lasso. This means that we might loose here some further favorable properties of the adaptive Lasso.

Proof of Theorem [\ref=adap.theorem]. Corollary [\ref=adap.corollary] combined with Lemma [\ref=betterthres] gives that for all [formula],

[formula]

Using moreover that [formula] and the bound of Lemma [\ref=betterthres], we get for [formula],

[formula]

and

[formula]

[formula]

Finally, again for [formula],

[formula]

[formula]

By Corollary [\ref=init.corollary],

[formula]

Taking

[formula]

the requirement that [formula] is fulfilled if take

[formula]

that is, if Condition b holds. We then obtain

[formula]

[formula]

[formula]

and

[formula]

[formula]

Proofs for Subsection [\ref=irrepresentable.section] on the weighted irrepresentable condition

Proof of Lemma [\ref=irrepresentable]. This is the weighted variant of the first part of Lemma 6.2 in [\cite=vdG:2009]. [formula]

Proof of Lemma [\ref=boundirr.lemma]. We define, as in [\cite=vdG:2009], the adaptive restricted regression

[formula]

Here, (f,) denotes the inner product between f and [formula] as elements of L2(Q).

We will show that

[formula]

It is moreover not difficult to see that [formula], so then the proof of Lemma [\ref=boundirr.lemma] is done.

To derive ([\ref=weshow]), we first note that

[formula]

Define

[formula]

Then

[formula]

[formula]

[formula]

[formula]

[formula]

But

[formula]

We conclude that

[formula]

[formula]

[formula]

Proofs for Section [\ref=noise.section]: the noisy case

Theorem [\ref=weightnoise] gives bounds for prediction error, estimation error and the number of false positives of the noisy weighted Lasso.

Proof of Theorem [\ref=weightnoise]. We can derive the prediction and estimation results in the same way as in Theorem [\ref=weight], adding now the noise term:

[formula]

[formula]

[formula]

and hence, using λweightwminSc  ≥  1,

[formula]

[formula]

As [formula] it gives

[formula]

[formula]

Now insert w minSc  ≥  M / L, 1  ≤  λweightM and [formula]:

[formula]

[formula]

The rest of the proof for the prediction and estimation error can therefore carried out in the same way is the proof of Theorem [\ref=weight].

As for variable selection, we use as in Lemma [\ref=weightselect] the weighted KKT conditions: for all j

[formula]

where [formula] and [formula]. Invoking λweightw minSc  ≥  1, we know that for all j∈Sc, λweightwj  ≥  1. Moreover, 2|(ε,ψj)n  ≤  λinit / 2 by the definition of T. Therefore,

[formula]

One can now proceed as in Lemma [\ref=weightselect].

[formula]

Proof of Lemma [\ref=refineselect.lemma] with the more involved conditions

To prove this lemma, we actually need some results in from Section [\ref=preview.section] and an intermediate result in their proof. One may skip the present proof at first reading and first consult the next subsection (Subsection [\ref=preview.proofs]).

The bound for the number of false positives of the initial lasso follows from the inequality

[formula]

This follows from Theorem [\ref=weightnoise], and from inserting the bound of Theorem [\ref=noisyinit.theorem] for init. One can then proceed by applying the inequality

[formula]

The result for the adaptive Lasso can be derived from

[formula]

This follows from ([\ref=adap*]) (which can be found at the end of the proof of Theorem [\ref=noisyadap.theorem]), invoking Condition B, and applying the bound of Theorem [\ref=noisyadap.theorem] for adap, and the bound of Theorem [\ref=noisyinit.theorem] for 2. Insert again ([\ref=s*]) to complete the proof. [formula]

Proofs for Section [\ref=preview.section]

Proof of the probability inequality of Lemma [\ref=noise.lemma]

This follows easily from the probability bound [formula] for a standard normal random variable Z. [formula]

Proof of Theorem [\ref=noisyinit.theorem]: the noisy initial Lasso

Theorem [\ref=noisyinit.theorem] is a simplified formulation of Corollary [\ref=noisyinit.corollary] below. This corollary follows from Theorem [\ref=weightnoise] by taking L = 1 and S = S0.

Let

[formula]

Take λinit  ≥  2λnoise. We have on T,

[formula]

Moreover, on T,

[formula]

and

[formula]

Also, on T,

[formula]

Proof of Theorem [\ref=noisythres.theorem]: the noisy thresholded Lasso

The least squares estimator Ŝδinit using only variables in Ŝδinit (i.e., the projection of [formula] on the linear space spanned by {ψj}j∈Ŝδinit) has similar prediction properties as fŜδinit (the projection of [formula] on the same linear space). This is because, as is shown in the next lemma, their difference is small.

Let [formula]. Then on T,

[formula]

Proof of Lemma [\ref=leastsquares]. This follows from

[formula]

and

[formula]

[formula]

[formula]

Proof of Theorem [\ref=noisythres.theorem] The bound for [formula] can be derived in the same way as in Lemma [\ref=betterthres]. The same is true for the bound

[formula]

[formula]

Assumption A together with Lemma [\ref=leastsquares] complete the proof for the bounds for prediction and estimation error, with the [formula]-bound being a simple consequence of the [formula]-bound. Also, the variable selection result follows from

[formula]

and Assumption A. [formula]

Proof of Theorem [\ref=noisyadap.theorem]: the noisy adaptive Lasso

We first apply Theorem [\ref=weightnoise] to the adaptive Lasso.

Suppose we are on T. Take [formula].

We have, for all [formula], and all β

[formula]

and

[formula]

and

[formula]

Moreover

[formula]

Proof of Theorem [\ref=noisyadap.theorem].

By the same arguments as used in Lemma [\ref=betterthres], for [formula],

[formula]

and [formula]. The prediction and estimation results now follow from Corollary [\ref=noisyadap.corollary] combined with Condition B.

We apply Corollary [\ref=noisyadap.corollary] with

[formula]

Condition B requires that

[formula]

This ensures that [formula] on the set T. Moreover, equation ([\ref=delta]) gives that λadap  ≥  δ as soon as

[formula]

which is also ensured by Condition B.

The variable selection result follows from: for [formula],

[formula]

[formula]

Proof of Lemma [\ref=min.lemma], where coefficients are assumed to be large

On T, for j∈S0, |j,init|  >  ∞, and |j,init| > |b0j| / 2, since |b0j|  >  2∞. Moreover, for j∈Sc0, |j,init|  ≤  ∞. Let

[formula]

So

[formula]

Note that M  ≤  1 / ∞. Since wminSc0  ≥  1 / ∞, the condition λadapM  ≥  1 implies λadapw minSc0  ≥  1.

Apply Theorem [\ref=weightnoise] to the adaptive Lasso with S = S0, and β  =  b0:

[formula]

and

[formula]

and

[formula]

Also, when [formula], it holds that

[formula]

[formula]

[formula]

[formula]