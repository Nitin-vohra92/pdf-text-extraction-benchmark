Ensembles of Classifiers based on Dimensionality Reduction

Lior Rokachb Amir Amitc

Keywords - Ensembles of classifiers; Dimensionality reduction; Out-of-sample extension; Random projections; Diffusion maps, Nystr�m extension

Introduction

Classifiers are predictive models which label data based on a training dataset T whose labels are known a-priory. A classifier is constructed by applying an induction algorithm, or inducer, to T - a process that is commonly known as training. Classifiers differ by the induction algorithms and training sets that are used for their construction. Common induction algorithms include nearest neighbors (NN), decision trees (CART [\citep=CRT93], C4.5 [\citep=C45Quinlan]), Support Vector Machines (SVM) [\citep=SVM99] and Artificial Neural Networks - to name a few. Since every inducer has its advantages and weaknesses, methodologies have been developed to enhance their performance. Ensemble classifiers are one of the most common ways to achieve that.

The need for dimensionality reduction techniques emerged in order to alleviate the so called curse of dimensionality [\citep=JL98]. In many cases, a high-dimensional dataset lies approximately on a low-dimensional manifold in the ambient space. Dimensionality reduction methods embed datasets into a low-dimensional space while preserving as much of the information conveyed by the dataset. The low-dimensional representation is referred to as the embedding of the dataset. Since the information is inherent in the geometrical structure of the dataset (e.g. clusters), a good embedding distorts the structure as little as possible while representing the dataset using a number of features that is substantially smaller than the dimension of the original ambient space. Furthermore, an effective dimensionality reduction algorithm also removes noisy features and inter-feature correlations. Due to its properties, dimensionality reduction is a common step in many machine learning applications in fields such as signal processing [\citep=SchclarDetection2010] [\citep=Schclar-Neta-detection12] [\citep=mine-vehicle-wave07] and image processing [\citep=Luo2012].

Ensembles of Classifiers

Ensembles of classifiers [\citep=EnsembleDiversity04] mimic the human nature to seek advice from several people before making a decision where the underlying assumption is that combining the opinions will produce a decision that is better than each individual opinion. Several classifiers (ensemble members) are constructed and their outputs are combined - usually by voting or an averaged weighting scheme - to yield the final classification [\citep=Polikar] [\citep=Opitz]. In order for this approach to be effective, two criteria must be met: accuracy and diversity [\citep=EnsembleDiversity04]. Accuracy requires each individual classifier to be as accurate as possible i.e. individually minimize the generalization error. Diversity requires to minimize the correlation among the generalization errors of the classifiers. These criteria are contradictory since optimal accuracy achieves a minimum and unique error which contradicts the requirement of diversity. Complete diversity, on the other hand, corresponds to random classification which usually achieves the worst accuracy. Consequently, individual classifiers that produce results which are moderately better than random classification are suitable as ensemble members. In [\citep=KappaError97], kappa-error diagrams are introduced to show the effect of diversity at the expense of reduced individual accuracy.

In this paper we focus on ensemble classifiers that use a single induction algorithm, for example the nearest neighbor inducer. This ensemble construction approach achieves its diversity by manipulating the training set. A well known way to achieve diversity is by bootstrap aggregation (Bagging) [\citep=Bagging96]. Several training sets are constructed by applying bootstrap sampling (each sample may be drawn more than once) to the original training set. Each training set is used to construct a different classifier where the repetitions fortify different training instances. This method is simple yet effective and has been successfully applied to a variety of problems such as spam detection [\citep=BaggEnsembleApp06], analysis of gene expressions [\citep=BaggEnsembleApp03] and image retrieval [\citep=Asymmetric_Bagging_Image_Retrieval].

The award winning Adaptive Boosting (AdaBoost) [\citep=AdaBoost96] algorithm and its subsequent versions e.g. [\citep=AdaBoost_R2] and [\citep=AdaBoost_RT] provide a different approach for the construction of ensemble classifiers based on a single induction algorithm. This approach iteratively assigns weights to each training sample where the weights of the samples that are misclassified are increased according to a global error coefficient. The final classification combines the logarithm of the weights to yield the ensemble's classification.

Rotation Forest [\citep=RotationForest2006] is one of the current state-of-the-art ensemble classifiers. This method constructs different versions of the training set by employing the following steps: First, the feature set is divided into disjoint sets on which the original training set is projected. Next, a random sample of classes is eliminated and a bootstrap sample is selected from every projection result. Principal Component Analysis [\citep=PCA] (see Section [\ref=sub:Dimensionality-reduction]) is then used to rotate each obtained subsample. Finally, the principal components are rearranged to form the dataset that is used to train a single ensemble member. The first two steps provide the required diversity of the constructed ensemble.

Multi-strategy ensemble classifiers [\citep=Rokach2009] aim at combining the advantages of several ensemble algorithms while alleviating their disadvantages. This is achieved by applying an ensemble algorithm to the results produced by another ensemble algorithm. Examples of this approach include multi-training SVM (MTSVM) [\citep=MultitrainingLiATL06], MultiBoosting [\citep=multiboosting:a:Webb00] and its extension using stochastic attribute selection [\citep=multi-strategyensemble:Webb04].

Successful applications of the ensemble methodology can be found in many fields, for example, recommender systems [\citep=SchclarRecSys09], finance [\citep=Leigh02-1], manufacturing [\citep=Rokach08] and medicine [\citep=Mangiameli04].

Dimensionality reduction

The theoretical foundations for dimensionality reduction were established by Johnson and Lindenstrauss [\citep=JL84] who proved its feasibility. Specifically, they showed that N points in an N dimensional space can almost always be projected onto a space of dimension C log N with control over the ratio of distances and the error (distortion). Bourgain [\citep=B85] showed that any metric space with N points can be embedded by a bi-Lipschitz map into an Euclidean space of log N dimension with a bi-Lipschitz constant of log N. Various randomized versions of these theorems were successfully applied to protein mapping [\citep=LLTY00], reconstruction of frequency sparse signals [\citep=CRT06] [\citep=D06], textual and visual information retrieval [\citep=BM2001] and clustering [\citep=fern2003random].

The dimensionality reduction problem can be formally described as follows. Let

[formula]

be the original high-dimensional dataset given as a set of column vectors where [formula], n is the dimension of the ambient space and N is the size of the dataset. All dimensionality reduction methods embed the vectors into a lower dimensional space [formula] where q  ≪  n. Their output is a set of column vectors in the lower dimensional space

[formula]

where q is chosen such that it approximates the intrinsic dimensionality of Γ [\citep=Hein05] [\citep=Wakin07]. We refer to the vectors in the set [formula] as the embedding vectors.

Dimensionality reduction techniques employ two approaches: feature selection and feature extraction. Feature selection methods reduce the dimensionality by choosing q features from the feature vectors according to given criteria. The same features are chosen from all vectors. Current state-of-the-art feature selection methods include, for example, Manhattan non-negative matrix factorization [\citep=ManhattanNonnegativeMatrixFactorization], manifold elastic net [\citep=Manifold_elastic_net] and geometric mean for subspace selection [\citep=GeometricMean_for_SubspaceSelection]. Feature extraction methods, on the other hand, derive features which are functions of the original features.

Dimensionality techniques can also be divided into global and local methods. The former derive embeddings in which all points satisfy a given criterion. Examples for global methods include:

Principal Component Analysis (PCA) [\citep=PCA] which finds a low-dimensional embedding of the data points that best preserves their variance as measured in the ambient (high-dimensional) space;

Kernel PCA (KPCA) [\citep=KPCA98] which is a generalization of PCA that is able to preserve non-linear structures. This ability relies on the kernel trick i.e. any algorithm whose description involves only dot products and does not require explicit usage of the variables can be extended to a non-linear version by using Mercer kernels [\citep=KPCA_book02]. When this principle is applied to dimensionality reduction it means that non-linear structures correspond to linear structures in some high-dimensional space. These structures can be detected by linear methods using kernels.

Multidimensional scaling (MDS) [\citep=MDS64] [\citep=MDS_94] algorithms which find an embedding that best preserves the inter-point distances among the vectors according to a given metric. This is achieved by minimizing a loss/cost stress function that measures the error between the pairwise distances of the embedding and their corresponding distances in the original dataset.

ISOMAP [\citep=ISO00] which applies MDS using the geodesic distance metric. The geodesic distance between a pair of points is defined as the length of the shortest path connecting these points that passes only through points in the dataset.

Random projections [\citep=CRT06] [\citep=D06] in which every high-dimensional vector is projected onto a random matrix in order to obtain the embedding vector. This method is described in details in Section [\ref=sec:Random-Projections].

Contrary to global methods, local methods construct embeddings in which only local neighborhoods are required to meet a given criterion. The global description of the dataset is derived by the aggregation of the local neighborhoods. Common local methods include Local Linear Embedding (LLE) [\citep=LLE00], Laplacian Eigenmaps [\citep=Laplacian03], Hessian Eigenmaps [\citep=Hessian02] and Diffusion Maps [\citep=CL_DM06] [\citep=SchclarDiffusionFrame] which is used in this paper and is described in Section [\ref=sec:Diffusion-Maps]. The patch alignment framework [\citep=PatchAlignment_forDimensionalityReduction] provides a unified framework to local dimensionality reduction techniques that employ two steps: (a) an optimization step where the local criterion is applied; and an alignment step in which the embedding is found. Examples that fit this framework include Local Linear Embedding (LLE) [\citep=LLE00], Laplacian Eigenmaps [\citep=Laplacian03], Hessian Eigenmaps [\citep=Hessian02], Local tangent space alignment [\citep=LocalTanSpAnal02] and Discriminative Locality Alignment (DLA) [\citep=PatchAlignment_forDimensionalityReduction].

A key aspect of dimensionality reduction is how to efficiently embed a new point into a given dimension-reduced space. This is commonly referred to as out-of-sample extension where the sample stands for the original dataset whose dimensionality was reduced and does not include the new point. An accurate embedding of a new point requires the recalculation of the entire embedding. This is impractical in many cases, for example, when the time and space complexity that are required for the dimensionality reduction is quadratic (or higher) in the size of the dataset. An efficient out-of-sample extension algorithm embeds the new point without recalculating the entire embedding - usually at the expense of the embedding accuracy.

The Nystr�m extension [\citep=N28] algorithm, which is used in this paper, embeds a new point in linear time using the quadrature rule when the dimensionality reduction involves eigen-decomposition of a kernel matrix. Algorithms such as Laplacian Eigenmaps, ISOMAP, LLE, and Diffusion Maps are examples that fall into this category and, thus, the embeddings that they produce can be extended using the Nystr�m extension [\citep=kernel_view_of_DR] [\citep=KPCA_view04_1]. A formal description of the Nystr�m extension is given in the Sec. [\ref=sec:OUT-OF-SAMPLE-EXTENSION].

The main contribution of this paper is a novel framework for the construction of ensemble classifiers based on dimensionality reduction and out-of-sample extension. This approach achieves both the diversity and accuracy which are required for the construction of an effective ensemble classifier and it is general in the sense that it can be used with any inducer and any dimensionality reduction algorithm as long as it can be coupled with an out-of-sample extension method that suits it.

The rest of this paper is organized as follows. In Section [\ref=sec:The-proposed-approach] we describe the proposed approach. In Sections [\ref=sec:Diffusion-Maps], [\ref=sec:Random-Projections] and [\ref=sec:Random-Subspaces] we introduce ensemble classifiers that are based on the Diffusion Maps, random projections and random subspaces dimensionality reduction algorithms, respectively. Experimental results are given in Section [\ref=sec:Experimental-results]. We conclude and describe future work in Section [\ref=sec:Conclusion-and-future].

Dimensionality reduction ensemble classifiers

The proposed approach achieves the diversity requirement of ensemble classifiers by applying a given dimensionality reduction algorithm to a given training set using different values for its input parameters. An input parameter that is common to all dimensionality reduction techniques is the dimension of the embedding space. In order to obtain sufficient diversity, the dimensionality reduction algorithm that is used should incorporate additional input parameters or, alternatively, incorporate a randomization step. For example, the Diffusion Maps [\citep=CL_DM06] dimensionality algorithm uses an input parameter that defines the size of the local neighborhood of a point. Variations of this notion appear in other local dimensionality reduction methods such as LLE [\citep=LLE00] and Laplacian Eigenmaps [\citep=Laplacian03]. The Random Projections [\citep=D06] (Section [\ref=sec:Random-Projections]) and Random Subspaces [\citep=RandomSubspaceDecisionForest] [\citep=RandSubSpaceEnsemble04] (Section [\ref=sec:Random-Subspaces]) methods, on the other hand, do not include input parameters other than the dimensionality of the embedding space. However, they incorporate a randomization step which diversifies the data (this approach already demonstrated good results using Random Projections in [\citep=SchclarICEIS09] and we extend them in this paper). In this sense, PCA is not suitable for the proposed framework since it does not include a randomization step and the only input parameter it has is the dimension of the embedding space (this parameter can also be set according to the total amount of variance of the original dataset that the embedding is required to maintain). Thus, PCA offers no way to diversify the data. On the other hand, dimensionality reduction algorithms that are suitable for the proposed method include ISOMAP [\citep=ISO00], LLE , Hessian LLE [\citep=Hessian02], Local tangent space alignment [\citep=LocalTanSpAnal02] and Discriminative Locality Alignment (DLA) [\citep=PatchAlignment_forDimensionalityReduction]. These methods are suitable since they require as input the number of nearest neighbors to determine the size of the local neighborhood of each data point. Laplacian Eigenmaps [\citep=Laplacian03] and KPCA [\citep=KPCA98] are also suitable for the proposed framework as they include a continuous input variable to determine the radius of the local neighborhood of each point.

After the training sets are produced by the dimensionality reduction algorithms, each set is used to train a classifier to produce one of the ensemble members. The training process is illustrated in Fig. [\ref=fig:Ensemble-training].

Employing dimensionality reduction to a training set has the following advantages:

It reduces noise and decorrelates the data.

It reduces the computational complexity of the classifier construction and consequently the complexity of the classification.

It can alleviate over-fitting by constructing combinations of the variables [\citep=Plastria].

These points meet the accuracy and diversity criteria which are required to construct an effective ensemble classifier and thus render dimensionality reduction a technique which is tailored for the construction of ensemble classifiers. Specifically, removing noise from the data contributes to the accuracy of the classifier while diversity is obtained by the various dimension-reduced versions of the data.

In order to classify test samples, they are first embedded into the low-dimensional space of each of the training sets using out-of-sample extension. Next, each ensemble member is applied to its corresponding embedded test sample and the produced results are processed by a voting scheme to derive the result of the ensemble classifier. Specifically, each classification is given as a vector containing the probabilities of each possible label. These vectors are aggregated and the ensemble classification is chosen as the label with the largest probability. Figure [\ref=fig:Ensemble-testing] depicts the classification process of a test sample.

Diffusion Maps

The Diffusion Maps (DM) [\citep=CL_DM06] algorithm embeds data into a low-dimensional space where the geometry of the dataset is defined in terms of the connectivity between every pair of points in the ambient space. Namely, the similarity between two points x and y is determined according to the number of paths connecting x and y via points in the dataset. This measure is robust to noise since it takes into account all the paths connecting x and y. The Euclidean distance between x and y in the dimension-reduced space approximates their connectivity in the ambient space.

Formally, let Γ be a set of points in [formula] as defined in Eq. [\ref=eq:training_set]. A weighted undirected graph [formula] is constructed, where each vertex v∈V corresponds to a point in Γ. The weights of the edges are chosen according to a weight function [formula] which measures the similarities between every pair of points where the parameter ε defines a local neighborhood for each point. The weight function is defined by a kernel function obeying the following properties:

positive semi-definite: for every real-valued bounded function f defined on Γ, [formula]

A common choice that meets these criteria is the Gaussian kernel:

[formula]

A weight matrix wε is used to represent the weights of the edges. Given a graph G, the Graph Laplacian normalization [\citep=C97] is applied to the weight matrix wε and the result is given by M:

[formula]

where [formula] is the degree of x. This transforms wε into a Markov transition matrix corresponding to a random walk through the points in Γ. The probability to move from x to y in one time step is denoted by [formula]. These probabilities measure the connectivity of the points within the graph.

The transition matrix M is conjugate to a symmetric matrix A whose elements are given by [formula] Using matrix notation, A is given by [formula] where D is a diagonal matrix whose values are given by [formula]. The matrix A has n real eigenvalues [formula] where [formula] and a set of orthonormal eigenvectors [formula] in [formula]. Thus, A has the following spectral decomposition:

[formula]

Since M is conjugate to A, the eigenvalues of both matrices are identical. In addition, if [formula] and [formula] are the left and right eigenvectors of M, respectively, then the following equalities hold:

[formula]

From the orthonormality of [formula] and Eq. [\ref=P_eigenvecs] it follows that [formula] and [formula] are bi-orthonormal i.e. 〈φm,ψl〉  =  δml where δml = 1 when m = l and δml = 0, otherwise. Combing Eqs. [\ref=A_spectral] and [\ref=P_eigenvecs] together with the bi-orthogonality of [formula] and [formula] leads to the following eigen-decomposition of the transition matrix M

[formula]

When the spectrum decays rapidly (provided ε is appropriately chosen - see Sec. [\ref=app:choosing_epsilon]), only a few terms are required to achieve a given accuracy in the sum. Namely,

[formula]

where [formula] is the number of terms which are required to achieve a given precision p.

We recall the diffusion distance between two data points x and y as it was defined in [\citep=CL_DM06]:

[formula]

This distance reflects the geometry of the dataset and it depends on the number of paths connecting x and y. Substituting Eq. [\ref=P_spectral] in Eq. [\ref=diffusion_distance] together with the bi-orthogonality property allows to express the diffusion distance using the right eigenvectors of the transition matrix M:

[formula]

Thus, the family of Diffusion Maps [formula] which is defined by

[formula]

embeds the dataset into a Euclidean space. In the new coordinates of Eq. [\ref=eq:DM_family], the Euclidean distance between two points in the embedding space is equal to the diffusion distance between their corresponding two high dimensional points as defined by the random walk. Moreover, this facilitates the embedding of the original points into a low-dimensional Euclidean space [formula] by:

[formula]

which also endows coordinates on the set Γ. Since λ1 = 1 and [formula] is constant, the embedding uses [formula]. Essentially, q  ≪  n due to the fast decay of the eigenvalues of M. Furthermore, q depends only on the dimensionality of the data as captured by the random walk and not on the original dimensionality of the data. Diffusion maps have been successfully applied for acoustic detection of moving vehicles [\citep=SchclarDetection2010] and fusion of data and multicue data matching [\citep=Lafon06datafusion].

Choosing ε

The choice of ε is critical to achieve the optimal performance by the DM algorithm since it defines the size of the local neighborhood of each point. On one hand, a large ε produces a coarse analysis of the data as the neighborhood of each point will contain a large number of points. In this case, the diffusion distance will be close to 1 for most pairs of points. On the other hand, a small ε might produce many neighborhoods that contain only a single point. In this case, the diffusion distance is zero for most pairs of points. The best choice lies between these two extremes. Accordingly, the ensemble classifier which is based on the the Diffusion Maps algorithm will construct different versions of the training set using different values of ε which will be chosen between the shortest and longest pairwise distances.

The Nystr�m out-of-sample extension

The Nystr�m extension [\citep=N28] is an extrapolation method that facilitates the extension of any function [formula] to a set of new points which are added to Γ. Such extensions are required in on-line processes in which new samples arrive and a function f that is defined on Γ needs to be extrapolated to include the new points. These settings exactly fit the settings of the proposed approach since the test samples are given after the dimensionality of the training set was reduced. Specifically, the Nystr�m extension is used to embed a new point into the reduced-dimension space where every coordinate of the low-dimensional embedding constitutes a function that needs to be extended.

We describe the Nystr�m extension scheme for the Gaussian kernel that is used by the Diffusion Maps algorithm. Let Γ be a set of points in [formula] and Ψ be its embedding (Eq. [\ref=eq:DM_family]). Let [formula] be a set in [formula] such that Γ  ⊂  . The Nystr�m extension scheme extends Ψ onto the dataset [formula]. Recall that the eigenvectors and eigenvalues form the dimension-reduced coordinates of Γ (Eq. [\ref=eq:DM_embedding]). The eigenvectors and eigenvalues of a Gaussian kernel with width ε which is used to measure the pairwise similarities in the training set Γ are computed according to

[formula]

If [formula] for every l, the eigenvectors in Eq. [\ref=extension] can be extended to any [formula] by

[formula]

Let f be a function on the training set Γ and let x∉Γ be a new point. In the Diffusion Maps setting, we are interested in approximating

[formula]

The eigenfunctions [formula] are the outcome of the spectral decomposition of a symmetric positive matrix. Thus, they form an orthonormal basis in [formula] where N is the number of points in Γ. Consequently, any function f can be written as a linear combination of this basis:

[formula]

Using the Nystr�m extension, as given in Eq. [\ref=nystrom], f can be defined for any point in [formula] by

[formula]

The above extension facilitates the decomposition of every diffusion coordinate ψi as [formula]. In addition, the embedding of a new point [formula] can be evaluated in the embedding coordinate system by [formula].

Note that the scheme is ill conditioned since [formula] as l  →    ∞  . This can be solved by cutting-off the sum in Eq. [\ref=f_extend] and keeping only the eigenvalues (and their corresponding eigenfunctions) that satisfy λl  ≥  δλ0 (where [formula] and the eigenvalues are given in descending order of magnitude):

[formula]

The result is an extension scheme with a condition number δ. In this new scheme, f and [formula] do not coincide on Γ but they are relatively close. The value of ε controls this error. Thus, choosing ε carefully may improve the accuracy of the extension.

Ensemble via Diffusion maps

Let Γ be a training set as described in Eq. [\ref=eq:training_set]. Every dimension-reduced version of Γ is constructed by applying the Diffusion Maps algorithm to Γ where the parameter ε is randomly chosen from the set of all pairwise Euclidean distances between the points in Γ i.e. from [formula]. The dimension of the reduced space is fixed for all the ensemble members at a given percentage of the ambient space dimension. We denote by [formula] the training set that is obtained from the application of the diffusion maps algorithm to Γ using the randomly chosen value εi where [formula] and K is the number of ensemble members. The ensemble members are constructed by applying a given induction algorithm to each training set [formula]. In order to classify a new sample, it is first embedded into the dimension-reduced space [formula] of each classifier using the Nystr�m extension (Section [\ref=sec:OUT-OF-SAMPLE-EXTENSION]). Then, every ensemble member classifies the new sample and the voting scheme which is described in Section [\ref=sec:The-proposed-approach] is used to produce the ensemble classification. Note that in order for the Nystr�m extension to work, each ensemble member must store the eigenvectors and eigenvalues which were produced by the Diffusion Maps algorithm.

Random Projections

The Random projections algorithm implements the Johnson and Lindenstrauss lemma [\citep=JL84] (see Section [\ref=sub:Dimensionality-reduction]). In order to reduce the dimensionality of a given training set Γ, a set of random vectors [formula] is generated where [formula] are column vectors and [formula]. Two common ways to choose the entries of the vectors [formula] are:

From a uniform (or normal) distribution over the q dimensional unit sphere.

From a Bernoulli +1/-1 distribution. In this case, the vectors are normalized so that [formula] for [formula].

Next, the vectors in Υ are used to form the columns of a q  ×  n matrix

[formula]

The embedding [formula] of xi is obtained by

[formula]

Random projections are well suited for the construction of ensembles of classifiers since the randomization meets the diversity criterion (Section [\ref=sub:Ensemble-Classifiers]) while the bounded distortion rate provides the accuracy.

Random projections have been successfully employed for dimensionality reduction in [\citep=fern2003random] as part of an ensemble algorithm for clustering. An Expectation Maximization (of Gaussian mixtures) clustering algorithm was applied to the dimension-reduced data. The ensemble algorithm achieved results that were superior to those obtained by: (a) a single run of random projection/clustering; and (b) a similar scheme which used PCA to reduce the dimensionality of the data.

Out-of-sample extension

In order to embed a new sample y into the dimension-reduced space [formula] of the i-th ensemble member, the sample is simply projected onto the random matrix R that was used to reduce the dimensionality of the member's training set. The embedding of y is given by  = R  ·  y. Accordingly, each random matrix needs to be stored as part of its corresponding ensemble member in order to allow out-of-sample extension.

Ensemble via Random Projections

In order to construct the dimension-reduced versions of the training set, K random matrices [formula] are constructed (recall that K is the number of ensemble members). The training set is projected onto each random matrix Ri and the dataset which is produced by each projection is denoted by [formula]. The ensemble members are constructed by applying a given inducer to each of the dimension-reduced datasets in [formula].

A new sample is classified by first embedding it into the dimension-reduced space [formula] of every classifier using the scheme in Section [\ref=sub:Out-of-sample-extension]. Then, each ensemble member classifies the new sample and the voting scheme from Section [\ref=sec:The-proposed-approach] is used to determine the classification by the ensemble.

Random Subspaces

The Random subspaces algorithm reduces the dimensionality of a given training set Γ by projecting the vectors onto a random subset of attributes. Formally, let [formula] be a randomly chosen subset of attributes. The embedding [formula] of [formula] is obtained by [formula]. Accordingly, each random set of attributes needs to be stored as part of its corresponding ensemble member.

This method is a special case of the random projections dimensionality reduction algorithm described in Sec. [\ref=sec:Random-Projections] where the rows (and column) of the matrix R in eq. [\ref=eq:Random_proj_matrix] are unique indicator vectors.

Random subspaces have been used to construct decision forests [\citep=RandomSubspaceDecisionForest] - an ensemble of tree classifiers - and also to construct ensemble regressors [\citep=RandSubSpaceEnsemble04]. Ensemble regressors employ a multivariate function instead of a voting scheme to combine the individual results of the ensemble members. The training sets that are constructed by the Random subspaces method are dimension-reduced versions of the original dataset and therefore this method is investigated in our experiments. This method combined with support vector machines has been successfully applied to relevance feedback in image retrieval [\citep=Asymmetric_Bagging_Image_Retrieval].

Out-of-sample extension

In order to embed a new sample y into the dimension-reduced space [formula] of the i-th ensemble member, the sample is simply projected onto [formula] - the member's subset of attributes. The embedding of [formula] is given by [formula].

Ensemble via Random Subspaces

In order to construct the dimension-reduced versions of the training set, K subsets of features are randomly chosen. The training set is projected onto each attribute subset and the ensemble members are constructed by applying a given inducer to each of the dimension-reduced datasets.

A new sample is classified by first embedding it into the dimension-reduced space [formula] of every classifier using the scheme in Section [\ref=sub:Out-of-sample-extension-1]. Then, each ensemble member classifies the new sample and the voting scheme from Section [\ref=sec:The-proposed-approach] is used to determine the ensemble's classification.

Experimental results

In order to evaluate the proposed approach, we used the WEKA framework [\citep=WEKA]. We tested our approach on 17 datasets from the UCI repository [\citep=UCI] which contains benchmark datasets that are commonly used to evaluate machine learning algorithms. The list of datasets and their properties are summarized in Table [\ref=tab:datasets-1].

Experiment configuration

In order to reduce the dimensionality of a given training set, one of two schemes was employed depending on the dimensionality reduction algorithm at hand. The first scheme was used for the Random Projection and the Random Subspaces algorithms and it applied the dimensionality reduction algorithm to the dataset without any pre-processing of the dataset. However, due to the space and time complexity of the Diffusion Maps algorithm, which is quadratic in the size of the dataset, a different scheme was used. First, a random value [formula] was selected. Next, a random sample of 600 unique data items was drawn (this size was set according to time and memory limitations). The Diffusion Maps algorithm was then applied to the sample which produced a dimension-reduced training set. This set was then extended using the Nystr�m extension to include the training samples which were not part of the sample. These steps are summarized in Algorithm [\ref=alg:Steps-for-DM].

All ensemble algorithms were tested using the following inducers: (a) nearest-neighbors (WEKA's B1 inducer); (b) decision tree (WEKA's J48 inducer); and (c) Na�ve Bayes. The ensembles were composed of ten classifiers(the information theoretic problem of choosing the optimal size of an ensemble is out of the scope of this paper. This problem is discussed, for example, in [\citep=Kuncheva:2004]) The dimension-reduced space was set to half of the original dimension of the data. Ten-fold cross validation was used to evaluate each ensemble's performance on each of the datasets.

The constructed ensemble classifiers were compared with: a non-ensemble classifier which applied the induction algorithm to the dataset without dimensionality reduction (we refer to this classifier as the plain classifier). The constructed ensemble classifiers were also compared with the Bagging [\citep=Bagging96], AdaBoost [\citep=AdaBoost96] and Rotation Forest [\citep=RotationForest2006] ensemble algorithms. In order to see whether the Diffusion Maps ensemble classifier can be further improved as part of a multi-strategy ensemble (Section [\ref=sub:Ensemble-Classifiers]), we constructed an ensemble classifier whose members applied the AdaBoost algorithm to their Diffusion Maps dimension-reduced training sets.

We used the default values of the parameters of the WEKA built-in ensemble classifiers in all the experiments. For the sake of simplicity, in the following we refer to the ensemble classifiers which use the Diffusion Maps and Random Projections dimensionality algorithms as the DME and RPE classifiers, respectively. The ensemble classifier which is based on the random subspaces dimensionality reduction algorithm is referred to as the RSE classifier.

Results

Tables [\ref=tab:TestIB1Full], [\ref=tab:TestJ48Full], and [\ref=tab:TestBayesFull] describe the results obtained by the decision tree, nearest-neighbor and Na�ve Bayes inducers, respectively. In each of the tables, the first column specifies the name of the tested dataset and the second column contains the results of the plain classifier. The second to last row contains the average improvement percentage of each algorithm compared to the plain classifier. We calculate the average rank of each inducer across all datasets in the following manner: for each of the datasets, the algorithms are ranked according to the accuracy that they achieved. The average rank of a given inducer is obtained by averaging its obtained ranks over all the datasets. The average rank is given in the last row of each table.

The results of the experimental study indicate that dimensionality reduction is a promising approach for the construction of ensembles of classifiers. In 113 out of 204 cases the dimensionality reduction ensembles outperformed the plain algorithm with the following distribution: RPE (33 cases out of 113), DM+AdaBoost (30 cases), RSE (27 cases) and DM (23 cases).

Ranking all the algorithms according to the average accuracy improvement percentage produces the following order: Rotation Forest (6.4%), Random projection (4%), DM+AdaBoost (2.1%), Bagging (1.5%), AdaBoost (1%), DM (0.7%) and Random subspaces (-6.7%). Note that the RSE algorithm achieved an average decrease of 6.7% in accuracy. A closer look reveals that this was caused by a particularly bad performance when the Na�ve Bayes inducer was used (26% average decrease in accuracy). In contrast, improvement averages of 1.7% and 4.4% were achieved when the RSE algorithm used the nearest-neighbors and J48 inducers, respectively. This may be due to datasets whose features are not independent - a situation which does not conform with the basic assumption of the Na�ve Bayes inducer. For example, the Isolet dataset is composed of acoustic recordings that are decomposed to overlapping segments where features of each segment constitute an instance in the dataset. In these settings, the features are not independent. Since the other algorithms, including the plain one, achieve much better results when applied to this dataset, we can assume that because the RSE algorithm chooses a random subset of features, the chance of obtaining independent features is lower compared to when all features are selected. Moreover, given the voting scheme in Section [\ref=sec:The-proposed-approach], ensemble members which produce wrong classifications with high probabilities damage accurate classifications obtained by other ensemble members. Figure [\ref=fig:RSE-NB-accuracy] demonstrates how the accuracy decreases as the number of members increases when RSE is paired with the Na�ve Bayes inducer. This phenomenon is contrasted in Fig. [\ref=fig:DM-NB-accuracy] where the behavior that is expected from the ensemble is observed. Namely, an increase in accuracy when the number of ensemble members is increased when an ensemble different from the RSE is used (e.g. the DME).

In order to compare the 8 algorithms across all inducers and datasets we applied the procedure presented in [\citep=demsar2006]. The null hypothesis that all methods have the same accuracy could not be rejected by the adjusted Friedman test with a confidence level of 90% (specifically F(7,350)=0.79 < 1.73 with p-value>0.1). Furthermore, the results show there is a dependence between the inducer, dataset and chosen dimensionality reduction algorithm. In the following we investigate the dependence between the latter two for each of the inducers.

Results for the nearest neighbor inducer (IB1)

In terms of the average improvement, the RPE algorithm is ranked first with an average improvement percentage of 5.8%. We compared the various algorithms according to their average rank following the steps described in [\citep=demsar2006]. The RSE and RPE achieved the first and second average rank, respectively. They were followed by Bagging (3rd) and Rotation Forest (4th).

Using the adjusted Friedman test we rejected the null hypothesis that all methods achieve the same classification accuracy with a confidence level of 95% and (7, 112) degrees of freedom (specifically F(7, 112)=2.47 > 2.09 and p-value<0.022). Following the rejection of the null hypothesis, we employed the Nemenyi post-hoc test where in the experiment settings two classifiers are significantly different with a confidence level of 95% if their average ranks differ by at least CD = 2.55. The null hypothesis that any of the non-plain algorithms has the same accuracy as the plain algorithm could not be rejected at confidence level 95%.

Results for the decision tree inducer (J48)

Inspecting the average improvement, the RPE and RSE algorithms are ranked second and third, respectively, after the Rotation Forest algorithm. Following the procedure presented by Demsar [\citep=demsar2006], we compared the various algorithms according to their average rank. The RSE and DM+AdaBoost achieved the second and third best average rank, respectively, after the Rotation Forest algorithm.

The null hypothesis that all methods obtain the same classification accuracy was rejected by the adjusted Friedman test with a confidence level of 95% and (7, 112) degrees of freedom (specifically F(7, 112)=5.17 > 2.09 and p-value<0.0001). As the null hypothesis was rejected, we employed the Nemenyi post-hoc test (CD = 2.55). Only the Rotation Forest algorithm significantly outperformed the plain and the DM algorithms. The null hypothesis that the RPE, RSE, DM and DM+AdaBoost algorithms have the same accuracy as the plain algorithm could not be rejected at confidence level 90%.

Results for the Na�ve Bayes inducer

The DM+AdaBoost algorithm achieved the best average improvement and it is followed by the Rotation Forest algorithm. The DM, RPE and RSE are ranked 5th, 7th and 8th in terms of the average improvement (possible reasons for the RSE algorithm's low ranking were described in the beginning of this section).

Employing the procedure presented in [\citep=demsar2006], we compared the algorithms according to their average ranks. The DM+AdaBoost and DM ensembles achieved the second and fourth best average ranks, respectively while the Rotation Forest and Bagging algorithms achieved the first and third places, respectively. The null hypothesis that all methods have the same classification accuracy was rejected by the adjusted Friedman test with a confidence level of 95% and (7, 112) degrees of freedom (specifically F(7, 112)=7.37 > 2.09 and p-value<1e-6). Since the null hypothesis was rejected, we employed the Nemenyi post-hoc test. As expected, the RSE was significantly inferior to all other algorithms. Furthermore, the Rotation Forest algorithm was significantly better than the RPE algorithms. However, we could not reject at confidence level 95% the null hypothesis that the RPE, DM, DM+AdaBoost and the plain algorithm have the same accuracy.

When we compare the average accuracy improvement across all the inducers, the RPE and DM+AdaBoost were ranked second and third - improving the plain algorithm by 4% and 2.1%, respectively. The Rotation Forest algorithm is ranked first with 6.4% improvement. Comparing only the proposed ensembles according to their average rank as described in [\citep=demsar2006] yielded the following ranking: DM+AdaBoost, RPE, RSE, DM. The null hypothesis that the RPE, RSE, DM and DM+AdaBoost algorithms have the same accuracy as the plain algorithm could not be rejected at confidence level 90%. Thus, according to the average accuracy improvement across all the inducers, RPE performs best. However, according to the average rank, DM+AdaBoost performs best.

Discussion

The results indicate that when a dimensionality reduction algorithm is coupled with an appropriate inducer, an effective ensemble can be constructed. For example, the RPE algorithm achieves the best average improvements when it is paired with the nearest-neighbor and the decision tree inducers. However, when it is used with the Na�ve Bayes inducer, it fails to improve the plain algorithm. On the other hand, the DM+AdaBoost ensemble obtains the best average improvement when it is used with the Na�ve Bayes inducer (better than the current state-of-the-art Rotation Forest ensemble algorithm) and it is less accurate when coupled with the decision tree and nearest-neighbor inducers.

Furthermore, using dimensionality reduction as part of a multi-strategy ensemble classifier improved in most cases the results of the ensemble classifiers which employed only one of the strategies. Specifically, the DM+AdaBoost algorithm achieved higher average ranks compared to the DM and AdaBoost algorithms when the J48 and Na�ve Bayes inducers were used. When the nearest-neighbor inducer was used, the DM+AdaBoost algorithm was ranked after the DM algorithm and before the AdaBoost ensemble which was last.

Conclusion and future work

In this paper we presented dimensionality reduction as a general framework for the construction of ensemble classifiers which use a single induction algorithm. The dimensionality reduction algorithm was applied to the training set where each combination of parameter values produced a different version of the training set. The ensemble members were constructed based on the produced training sets. In order to classify a new sample, it was first embedded into the dimension-reduced space of each training set using out-of-sample extension such as the Nystr�m extension. Then, each classifier was applied to the embedded sample and a voting scheme was used to derive the classification of the ensemble. This approach was demonstrated using three dimensionality reduction algorithms - Random Projections, Diffusion Maps and Random subspaces. A fourth ensemble algorithm employed a multi-strategy approach combining the Diffusion Maps dimensionality reduction algorithm with the AdaBoost ensemble algorithm. The performance of the obtained ensembles was compared with the Bagging, AdaBoost and Rotation Forest ensemble algorithms.

The results in this paper show that the proposed approach is effective in many cases. Each dimensionality reduction algorithm achieved results that were superior in many of the datasets compared to the plain algorithm and in many cases outperformed the reference algorithms. However, when the Na�ve Bayes inducer was combined with the Random Subspaces dimensionality reduction algorithm, the obtained ensemble did not perform well in some of the datasets. Consequently, a question that needs further investigation is how to couple a given dimensionality reduction algorithm with an appropriate inducer to obtain the best performance. Ideally, rigorous criteria should be formulated. However, until such criteria are found, pairing dimensionality reduction algorithms with inducers in order to find the best performing pair can be done empirically using benchmark datasets. Furthermore, other dimensionality reduction techniques should be explored. For this purpose, the Nystr�m out-of-sample extension may be used with any dimensionality reduction method that can be formulated as a kernel method [\citep=kernel_view_of_DR]. Additionally, other out-of-sample extension schemes should also be explored e.g. the Geometric Harmonics [\citep=CL_GH06]. Lastly, a heterogeneous model which combines several dimensionality reduction techniques is currently being investigated by the authors.

Acknowledgments

The authors would like to thank Myron Warach for his insightful remarks.