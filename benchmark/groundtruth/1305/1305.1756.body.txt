0 cm Theorem Lemma Corollary Remark Proposition Observation Problem Example Definition

minimal state space realization, static output feedback and matrix completion of families of systems

Introduction

We first recall in the concept of state space realization. Let F(s) be a p  ×  m-valued rational function (m inputs and p outputs in engineering terminology), analytic at infinity, i.e. [formula] exists. Then, F(s) admits a state space realization

[formula]

with [formula], [formula], [formula] and [formula], namely, [formula].

For convenience, throughout this work the formulation is for complex matrices. However, up to complex conjugation of spectrum, all results hold for real matrices.

A realization is called  minimal whenever ~ n, the dimension of A in [\eqref=StateSpace], is the smallest possible, see e.g. [\cite=So]. It is then equal to the McMillan degree of F(s), see e.g. [\cite=Ka], [\cite=So].

The issue of minimality of realization is fundamental, see e.g. [\cite=DP], [\cite=Ka], [\cite=LR]. Typically, one is first interested in the question of whether or not a given realization is minimal and if not, to find ways to extract out of the given realization, a minimal one. For a survey of works addressing the second question see [\cite=DS]. We here focus on the first problem.

For a (possibly rectangular) system F(s) admitting state space realization [\eqref=StateSpace], we shall also find it convenient to consider the  squared realization matrix Lsq without altering A and thus preserving n. The naive (=inflating) version is obtained as follows: If m > p by simply adding rows of zeros to the bottom L until it is (n + m)  ×  (n + m), and if p > m by adding columns of zeros to the right side of L until it is (n + p)  ×  (n + p). This approach preserves the associated controllable and observable subspaces. In particular, if exists, minimality of realization is maintained. As mentioned, here the dimension of Lsq is equal to the larger dimension of the original L.

In Lemma [\ref=L:TruncSq] below, we show that if a realization L in [\eqref=StateSpace] is minimal, one can multiply the corresponding rational function F(s) by constant matrices: Tc from the left and Tb from the right, so that the resulting TcF(s)Tb is of the smallest possible dimensions (which turn to be square) while in the resulting realization, Lsq, both the A matrix and minimality, are preserved.

Recall that for a given matrix [formula], the  geometric multiplicity  of λ∈spect(A) is the number of linearly independent eigenvectors associated with this λ, see e.g. [\cite=HJ1]. The largest geometric multiplicity among the eigenvalues of A will be denoted by α(A).

The three main results of this work are Proposition [\ref=L:TruncSq], Theorem [\ref=Th:Main] and Proposition [\ref=RationaL] are stated in the Introduction.

Denote by α  =  α(A) the largest geometric multiplicity among the eigenvalues of a given a matrix [formula].

The following is true.

If for a given a matrix [formula] the pair A,B is controllable, then

[formula]

Moreover, there exists a full rank matrix  [formula] (and [formula] is of a full rank) so that the pair A,B̂ is controllable.

If for a given a matrix [formula] the pair A,C is observable, then

[formula]

Moreover, there exists a full rank matrix  [formula] (and [formula] is of a full rank) so that the pair A,Ĉ is observable.

Given a ~ p  ×  m-valued rational function F(s) and the corresponding (n + p)  ×  (n + m) realization L as in [\eqref=StateSpace]. If the realization L is minimal, there exist full rank matrices  [formula] and  [formula] so that the (n + α)  ×  (n + α) matrix

[formula]

is a minimal realization of the  α  ×  α-valued rational function

[formula]

To proceed, we need further notation. For a system F(s) admitting state space realization [\eqref=StateSpace], we shall find it convenient to consider the  associated system, (s), with zero at infinity ("strictly proper" in engineering jargon) i.e.

[formula]

Thus, the corresponding (n + p)  ×  (n + m) realization matrix is of the form

[formula]

Note that in both realizations, L in [\eqref=StateSpace] and L̂ in [\eqref=eq:HatL] the associated controllable (observable) subspace is identical. In particular, minimality of these realizations is equivalent.

Recall that applying a static output feedback, see e.g. [\cite=Ka], to an input-output (associated) system y(s) = (s)u(s) means taking u = Ky + u' with u' an auxiliary input and a (constant) m  ×  p matrix K. The resulting closed loop system is y(s) = Fc.l.(s)u'(s) with Fc.l. = (Ip  -  K)- 1. The corresponding closed loop realization matrix Lcl is

[formula]

We can now state our second main result which is a novel characterization of a minimal realization.

Let [formula] be a realization of a p  ×  m-valued rational function F(s), see [\eqref=StateSpace].

Let [formula] be a realization of the p  ×  m-valued associated system (s) see [\eqref=AssociatedSystem]. A static output feedback u = Ky + u' (with K m  ×  p constant) is applied to (s) so that the closed loop system matrix is as in [\eqref=Lcl].

Let also Lsq be a realization of the corresponding squared system Fsq(s).

For A in [\eqref=StateSpace] or in [\eqref=eq:HatL], for some ~ q∈[1,n] denote

[formula]

with λj distinct.

The following are equivalent.

The realization of F(s) is minimal.

There exists a (constant) [formula] so that in [\eqref=Lcl],

[formula]

Consider the realization L of F(s) in [\eqref=StateSpace] with m = p and assume that B and C are of a full rank. (If this was not the case take the reduced dimension squared counterpart Lsq and Fsq(s) in [\eqref=eq:sq]).  Then,

[formula]

for λj∈spect(A) and some sufficiently small ε > 0.

Consider the realization L of F(s) in [\eqref=StateSpace] with m = p (if [formula] take a squared counterpart Lsq and Fsq(s)).   Then,

[formula]

The above result says that one can study minimality of realization by examining the spectrum of the matrix L and of its submatrix A. Now, in linear algebra it is natural to discuss families of matrices sharing the same properties. It is less common to address families of systems. However, the description of a system by the realization matrix L in [\eqref=StateSpace], suggests us to explore   families  of minimal/non-minimal realizations.

Before introducing the result, recall in the following. Let φ(s) be an arbitrary (possibly complex) scalar rational function (possibly with pole at infinity or even a polynomial). Let L be an arbitrary k  ×  k matrix whose eigenvalues differ from the poles of φ(s). Then the k  ×  k matrix φ(L) is well defined. In fact, from the Cayley-Hamilton Theorem it follows that there always exists a polynomial ψ(s), of degree of at most k - 1, so that φ(L) = ψ(L), see e.g. [\cite=HJ1]. Thus, we address only polynomials.

Although typically one is interested in F(s) (realized by L), but not in the rational function realized by ψ(L), the result below asserts their interdependence.

Let [formula] be a realization of a m  ×  m-valued rational function F(s), see [\eqref=StateSpace].

For an arbitrary (scalar) polynomial ψ(s) consider the (n + m)  ×  (n + m) matrix ψ(L) as a realization matrix, i.e. [formula].

Consider the following statements

(i) [formula].

(ii) The realization ψ(L) is minimal.

(iii) The realization L is minimal.

Then (i)  implies  (ii) which in turn implies  (iii).

If the realization L is not minimal, the realization ψ(L) is not minimal.

The outline of the paper is as follows. In Section [\ref=sec:Motivation] we motivate these results and in Sections [\ref=sec:Background], [\ref=sec:Proof] and [\ref=sec:Families] we prove Proposition [\ref=L:TruncSq], Theorem [\ref=Th:Main] and Proposition [\ref=RationaL] respectively.

Motivation

In this section we put Theorem [\ref=Th:Main] in a broader perspective.

1.  Output feedback  Although typically stated differently, the following is well known, see e.g. [\cite=DP], [\cite=Ka], [\cite=So].

Given an associated system (s) and its realization L̂ see [\eqref=AssociatedSystem] and [\eqref=eq:HatL].

The loop is closed by applying a state feedback gain to a (Luenberger) observed state.

The realization L̂ is minimal if and only if the closed loop poles may be located anywhere in the complex plane.

In engineering jargon, minimal realization enables one to place the poles of a closed loop system anywhere in the complex plane through a   dynamic  output feedback.

The simplicity of   static output feedback has made it very attractive. However, exploring its properties turned out to be challenging, see e.g. [\cite=ElGOAiR], [\cite=HL], [\cite=Ka] and [\cite=SADG].

Condition [\eqref=eq:AclA] in Theorem [\ref=Th:Main] may be viewed as establishing a precise connection between minimal realization and   static output feedback.

2.  Static output feedback and realization of the inverse rational function  Consider a rational function F(s) and its realization as in [\eqref=StateSpace] with D = I, namely, [formula].

On the one hand, it is straightforward to verify that a realization of F- 1(s), the inverse rational function, is given by [formula]. See e.g. [\cite=DP], [\cite=Ka]. (Obviously, this has nothing to do with L- 1, the inverse of the realization matrix, addressed in Proposition [\ref=RationaL] and Section [\ref=sec:Families]).

On the one hand, consider now the associated system [\eqref=AssociatedSystem] and its realization [\eqref=eq:HatL]. Applying to it a static output feedback with K =  - I yields in [\eqref=Lcl] [formula].

In each of the three systems L, Linv and Lcl, the associated controllable (or observable) subspace, is identical. In particular, minimality of the three realization is equivalent. Thus, Theorem [\ref=Th:Main] addresses also the realization of the inverse system (whenever exists). For example in [\cite=MG] the authors in fact studied L, Linv in the context of Linear Fraction Transformation.

3.  Matrix completion  Matrix completion (a.k.a extension) problems have been of interest in the past 60 years. Many of them can be casted in the following framework: A part of a matrix is prescribed, can one complete the missing part so that the full matrix will poses certain properties, typically spectral. For a nice survey, see [\cite=Cr]. The case where the upper triangular part is prescribed was addressed in [\cite=BGRS] (and not cited in [\cite=Cr]).

As a special case, assume that [formula], where A,B,C are prescribed and *   stands for a square unprescribed part. Parameterizing all possible characteristic polynomials of L is known to be difficult, see comment following [\cite=Cr]. Condition [\eqref=NotSpectASpectL] in Theorem [\ref=Th:Main] can be seen as answering a more modest question: Under what conditions can one complete [formula] with D so that the spectra of the resulting L and A will not (or will always) intersect.

4.  Pole placement and matrix completion  The fact that problems matrix completion and pole placement through feedback, are linked is well known. See e.g. [\cite=KRW] or the Introduction of [\cite=RW]. The equivalence of [\eqref=Lcl] [\eqref=NotSpectASpectL] in Theorem [\ref=Th:Main], falls into this category.

5.  A spectral PBH test for minimality  Consider a rational function F(s) and its realization as in [\eqref=StateSpace] or [\eqref=AssociatedSystem]. As already mentioned, the issue of minimality of realization is fundamental.

Adopting Kailath's terminology, for given [formula], [formula] and [formula]   The Popov-Belevitch-Hautus (PBH) Rank Tests, [\cite=DP], [\cite=Ka] say that: A pair A,B is controllable, if and only if

[formula]

A pair A,C is observable, if and only if

[formula]

It is clear that in [\eqref=eq:PBHcont] and [\eqref=eq:PBHobs], without loss of generality, one can confine the search to

[formula]

We here examine two adaptations of these tests:

(i) To minimality of realization (without independently testing for controllability

.

(ii) To consider the spectrum of Lsq a square realization matrix.

To this end recall that a realization is minimal if and only if it is both controllable and observable, see e.g. [\cite=DP], [\cite=Ka] [\cite=So].

From a combination of the PBH Rank Tests in [\eqref=eq:PBHcont] and [\eqref=eq:PBHobs] it follows says that minimal realization implies that

[formula]

The following example illustrates the fact that formulating the converse to [\eqref=eq:Rank] is more delicate. Consider the 2  ×  1 rational function of McMillan degree two

[formula]

Its realization is

[formula]

Namely, n = 2, m = 1 and p = 2. Although this realization is minimal, [\eqref=eq:Rank] does not hold:

[formula]

(indeed, the non-zero vector [formula] is in the nullspace of the rightmost matrix).

On the other hand

[formula]

One may view [\eqref=NotSpectASpectL] in Theorem [\ref=Th:Main] as a correct extension of the PBH Rank tests to the realization matrix L.

6. Rational functions: Scalar (SISO) versus matrix-valued (MIMO)

Let [formula] be a realization of a p  ×  m-valued rational function F(s), see [\eqref=StateSpace] and let Lsq be a realization of the corresponding squared system Fsq(s).

For A in [\eqref=StateSpace] or in [\eqref=eq:HatL], for some q∈[1,n] denote

[formula]

with λj distinct.

Consider the following statements

For any [formula]

[formula]

There exists a [formula] so that

[formula]

Consider the realization L of F(s) in [\eqref=StateSpace] with m = p (if [formula] take the squared counterpart Lsq and Fsq(s)).

For each [formula] there exists [formula] so that

[formula]

The realization is minimal.

Then,

[formula]

If F(s) is a scalar rational function, namely ~ m = p = 1, then [formula].

Indeed, the fact that [formula] is straightforward. The equivalence of (iii) and (iv) is established in Theorem [\ref=Th:Main].

The fact that for scalar systems [formula] was first proved (in an elaborate way) in [\cite=MC]. We next illustrate a straightforward way of showing that.

Without loss of generality one can take A to be in its Jordan canonical form. For example take n = 8, q = 4,

[formula]

where zeros were omitted.

Using the PBH tests, controllability implies, see e.g. [\eqref=eq:PBHcont], that λ1, λ2, λ3, λ4 are distinct and b3, b5, b7, b8 are non-zero. Observability implies, see e.g. [\eqref=eq:PBHobs], that λ1, λ2, λ3, λ4 are distinct and c1, c4, c6, c8 are non-zero. Namely, minimality of realization means that

[formula]

where [formula] stands for a non-zero scalar and *   for "don't care". However, looking at L not as a partitioned array, but as a 9  ×  9 matrix, this is exactly the condition for having for j = 1,2,3,4 the matrices (L - λjI9) of a full rank. (Else, there are rows or columns of zeros). To sum-up, in the context of scalar systems minimality of realization and condition [\eqref=EasySpectASpectL] are equivalent.

The following example illustrates the gap between these conditions for   matrix-valued  rational functions. Specifically, we construct realization matrices A, ~ B, ~ C ~  so that in spite of minimality, for "many" D's not only the relation [\eqref=eq:ExistsD] does not holds, but in fact [formula].

Consider the scalar rational functions [formula] and [formula] The corresponding minimal realizations are

[formula]

Clearly, [formula] and [formula].

From the above f1(s) and f2(s) we now construct the following matrix valued rational function,

[formula]

where d2,d3 are parameters. Its minimal realization is given by

[formula]

Thus, not only [\eqref=EasySpectASpectL] is no longer true, but in fact for all d2,d3.

[formula]

This example will be further discussed in part II of Example [\ref=ExInv].

We conclude this section by pointing out that we do not know whether or not for m = p  ≥  2 minimal realization implies that in [\eqref=eq:HatL]

[formula]

Truncated square realization

In this section we prove Proposition [\ref=L:TruncSq]. To this end we resort to a matrix theory result, whose interest goes beyond the scope of this work: In Lemma [\ref=L:echelon] below we show that if all we know about a given matrix is a list of subsets of its rows which are linearly independent, we can still impose certain restrictions on its structure. This is illustrated through a specific, yet rich, example.

Let B be a 17  ×  m ~  matrix, with ~ m ~  parameter, be partitioned to

[formula]

with [formula], [formula] and [formula]. All that is known is that the following subsets of rows are linearly independent

[formula]

Namely, in B the rows (2,4,6),(9,10),(14,15,16,17) are linearly independent. This implies that m  ≥  4 and that rank(B) can be arbitrary within the range

[formula]

.

(iv) In the framework of the previous item, there exists a full rank matrix [formula] so that in the n  ×  ρ product matrix BT, the rows with the indices Ja, Jb, Jc, [formula] are linearly independent.

Proof  (i)  Assume first that B is n  ×  n non-singular. We shall find it convenient to write down B by its columns as

[formula]

where [formula] are linearly independent. Consider the following procedure starting from the left,

[formula]

By construction,

[formula]

with [formula] standing for a non-zero entry and ~  *  for "don't care".

Without loss of generality, one can now, normalize

[formula]

so that

[formula]

is unitary and still satisfies

[formula]

(ii)  Assume now that [formula] and denote ~ r: = rank(B) (clearly min (m,n)  ≥  r).

Let [formula] an orthonormal basis to the null-space of B*, i.e.

[formula]

Let [formula] an orthonormal completion of this basis to the whole space. Thus,

[formula]

is a unitary n  ×  n matrix so that

[formula]

Moreover the linearly independent columns of [formula] are numbered from the left. Namely [formula] can be written as

[formula]

where the columns [formula] are linearly independent and whenever exist, the β columns depend linearly on the b columns to their left (e.g. βa = 0, both βb and βc depend on b1 and b2, βd depends on b1,b2 b3 etc.). Using [formula] as in item (i) to construct the matrix

[formula]

completes the construction. See part (a) of Example [\ref=ExB] for illustration.

(iii)  This follows from the block diagonal structure of U. In each diagonal block, the procedure in item (ii) is applied. See part (b) of Example [\ref=ExB] for illustration.

(iv)  This is illustrated in part (c) of Example [\ref=ExB] below.

We next illustrate an application of the above lemma.

(a)   Let [formula], with m parameter, be so that rows

[formula]

are linearly independent. This implies that m  ≥  3 and rank(B) can be arbitrary within the range

[formula]

Thus far for matrix theory results. We now address Proposition [\ref=L:TruncSq] and show that if in [\eqref=StateSpace] the realization is minimal, one can find full rank matrices Tb and Tc so that in [\eqref=eq:sq] the realization is of the smallest possible dimensions (which turn to be square) while preserving the A matrix and minimality of the realization. On the way, for a given square matrix A, we introduce a parameterization of all matrices B for which the pairs A,B are controllable.

Recall that the   geometric multiplicity  of ~  λ∈spect(A) ~  is the number of linearly independent eigenvectors associated with this λ, see e.g. [\cite=HJ1]. For a given matrix [formula], we shall denote by α(A) the largest geometric multiplicity among its eigenvalues.

Recall also that a matrix A is called   non-derogatory  whenever α(A) = 1. (This is equivalent to having the characteristic and the minimal polynomials equal, see e.g. [\cite=HJ1], [\cite=HJ2]). In turn, this is closely related to the companion form, see e.g. [\cite=HJ1] (controller form in control engineering terminology, see e.g. [\cite=Ka]). For a nice treatment see [\cite=DP], [\cite=LR].

Proof of Proposition [\ref=L:TruncSq]  If a matrix [formula] is rank deficient, one can always find a non-singular m  ×  m matrix T̃ so that

[formula]

so that [formula] is of a full rank ( = ). Moreover, for arbitrary [formula], the pairs A,B and A, span the same controllable subspace. Thus, with a slight abuse of notation we shall substitute [formula] by B and assume hereafter that

[formula]

From the definition of α  =  α(A), there exists λ∈spect(A) so that

[formula]

Hence,

[formula]

and if α > m, the controllability condition [\eqref=eq:PBHcont] can not hold. Thus, m  ≥  α.

To show that one can always choose Tb so that equality holds, i.e. m = α, recall that without loss of generality one can assume that A is in its Jordan canonical form where [formula] and for [formula] each Aj (with possibly several Jordan blocks) contains a single eigenvalue λj (and [formula] whenever [formula]).

To avoid cumbersome notation we proceed by addressing a specific example. Yet, it is rich enough to cover all cases: Take q = 3 and ~ n = 17 so that,

[formula]

where for Jk(λ) is a k-dimensional Jordan block corresponding to an eigenvalue λ,

[formula]

Then the PBH tests say that controllability of the pair A,B is equivalent to having [formula] (m parameter) whose subsets of rows indicated in [\eqref=eq:LinInd], are linearly independent. (As before, this implies that m  ≥  4 and rank(B) can be arbitrary within the range

[formula]

.

We conclude this section by simple illustration of item (iii) in Proposition [\ref=L:TruncSq]

Given a 1  ×  2-valued rational function (two inputs one output in control terminology)

[formula]

where b,d are parameters. It is realized by

[formula]

Take now [formula] and thus,

[formula]

The corresponding realization matrix is,

[formula]

Minimality of both realizations Lo and Lsq is equivalent to [formula].

This example will be further discussed in part I of Example [\ref=ExInv].

proof of Theorem [\ref=Th:Main]

(i)[formula](ii) Recall that we denote by α the largest geometric multiplicity among the eigenvalues of A. From Lemma [\ref=L:TruncSq] it follows that there exist full rank matrices [formula] and [formula] so that [formula] and [formula] are of full rank and if the realization triple A,B,C was minimal so is A,B̂,Ĉ.

Take now in [\eqref=eq:AclA] K = ηTcTb with the above Tb, Tc and η > 0 is a scalar parameter. By construction [formula] is of rank α and

[formula]

We now show that if [formula] by taking η "sufficiently large", [formula] for [formula].

Let now partition an arbitrary [formula] (the subscript stands for "right") to vr  =  r  +  r where ~ Cr = 0 and r is in the orthogonal complement of the null-space of C.

Note now for a non-zero r, observability implies that

[formula]

On the other hand, for arbitrary non-zero r, [formula] and by construction [formula]. Hence for [formula],

[formula]

Namely, [formula].

Similarly, partition an arbitrary [formula] (the subscript stands for "left") to vl  =  l  +  l where  *lB = 0 and l is in the orthogonal complement of the null-space of B*.

For a non-zero l, controllability implies that

[formula]

On the other hand, for arbitrary non-zero l, [formula] and by construction [formula]. Hence for [formula],

[formula]

Namely, [formula] so this part of the claim is established.

(ii)[formula](iii) First one can always write

[formula]

where

[formula]

Thus, the non-singularity of the (different dimensions) matrices Lsq  -  λIn + p and Acl  -  λIn is equivalent. Namely, λ∈spect(Acl) if and only if λ∈spect(Lsq).

For [formula] take now Dj = (λj  -  ε)Ip with λj∈spect(A) and ε > 0 sufficiently small. From [\eqref=Acl] it follows that that Acl is of the form [\eqref=eq:AclNom] with η  =  ε- 1 thus [formula] and by the above construction [formula], so this part of the claim is established.

(iii) [formula] (iv)  Trivial.

(iv)[formula](i) We find it more convenient to show that if the realization is not minimal, then

[formula]

If the realization L is not controllable, from condition [\eqref=eq:PBHcont] it follows that there exists [formula] so that

[formula]

This implies that for that same λ

[formula]

where *   stands for "don't care". Namely, this λ is in [formula]. Note now that [\eqref=uncont] implies that, this λ is in spect(A) (else rank(λIn - A) = n). Thus, [\eqref=LambdaInSpectL] holds.

Similarly, if the realization L is not observable, from condition [\eqref=eq:PBHobs] it follows that there exists [formula] so that

[formula]

As before, [\eqref=SingL] holds and thus this λ is in [formula]. Note now that [\eqref=unobs] implies that, this λ is in spect(A) (else rank(λIn - A) = n). Thus [\eqref=LambdaInSpectL] holds and the proof is complete.

Families of systems

In linear algebra it is natural to discuss families of matrices sharing common properties, it is less common address families of systems. However, the description through the realization matrix L in [\eqref=StateSpace] actually suggests that. Before going into details, we recall that in [\cite=AL], families of realization matrices L were studied, in a different framework, by the same authors.

Let a (m + n)  ×  (m + n) realization matrix L be given. From the PBH tests it follows that whenever a realization is not observable, L has an eigenvector of the form [formula] for some [formula], i.e. vr belongs to the orthogonal complement of the observable subspace associated with the pair A,C. Similarly, whenever a realization is not controllable L* has an eigenvector of the form [formula] for some [formula], i.e. vl belongs to the orthogonal complement of the controllable subspace associated with the pair A,B.

Hence, it is well known that the controllable (observable) subspace associated with a given

[formula]

is identical for all

[formula]

where *   stands for "don't care". This property was already used in this work.

Note now that using the same reasoning, whenever L is non-singular also

[formula]

shares the same controllable (observable) subspace. From systems point of view this is not so intuitive since there is no apparent connection between F(s) realized by L and Finv(s), the rational function realized by L- 1.

Proposition [\ref=RationaL] goes along the same lines. It is an immediate consequence of the following classical matrix theory observation, whose proof is omitted.

For given: A square matrix L and a (scalar) polynomial ψ(s), consider the square matrix ψ(L).

The right (left) invariant subspaces of L are contained in the right (left) invariant subspaces of ψ(L).

We now illustrate the significance of Proposition [\ref=RationaL].

In Example [\ref=ExNonSq] we showed that the rational functions

[formula]

(with b,d parameters) were related. The realization of Fsq(s) was given by,

[formula]

Consider now the inverse matrix,

[formula]

This is a realization of

[formula]

Thus, Fo(s), Fsq(s) and Finv(s) are all related. The three respective realizations are minimal, if and only if [formula].

II. Recall that in Example [\ref=Ex:forallD] we studied the realization

[formula]

(with d2,d3 parameters) of the rational function,

[formula]

We showed that in spite of the minimality of L, not only [formula], but in fact spect(A)  ⊂  spect(L), for all d2,d3.

Consider now the polynomial ψ(s) = s2 - 2s and using [\eqref=eq:ExL], the corresponding matrix

[formula]

with

[formula]

As, spect(Ã) = {2, ~ 2}  ~   and ~  spect(ψ(L)) = {0, ~ 0, ~ d2d3 + 3, ~ d2d3 + 3}, it follows that whenever [formula],

[formula]

Namely condition [\eqref=eq:ExistsD] holds.  In fact, ψ(L) is a minimal realization of

[formula]

The special case d2d3 =  - 1 illustrates the that in part (a) of Proposition [\ref=RationaL], (iii) strictly  implies (ii). (The fact that the implication from (i) to (ii) is strict was addressed in Example [\ref=Ex:forallD]).

We believe and hope that this work (along with [\cite=AL]) are just a stage in the study of families systems viewed through the corresponding realization matrices L.

Acknowledgement

The authors wish to thank Prof. S. Ter Horst from the Mathematics group in the North West University, South Africa, for pointing out to them reference [\cite=MG].