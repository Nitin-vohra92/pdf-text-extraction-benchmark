Invariant expectation values in the sampling of discrete frequency distributions

Introduction

In many interesting physical, biological and social phenomena, whenever no intrinsic scale for the relevant variables is present, the emergence of "scaling laws" is phenomenologically observed [\cite=Newman]. However, strictly speaking, a power law is not a proper way of fitting empirical data, since no choice of the exponent can keep the higher moments of a power law distribution from diverging, while every phenomenological distribution leads to finite values for all moments. This is not just a technicality: it is rather a reflection of the fact that the long tail of a power law distribution is in practice cutoffed by the existence of some "hidden" scale, irrelevant in the scaling region, but eventually forcing some upper limit on the variables describing the system. It would therefore be convenient to parametrize the data by means of more regular distribution functions, sufficiently damped for very large values of the variables, but admitting power law distributions as regular limits when the control parameter implementing the cutoff is sent to its limiting value.

A related issue concerns the effects of sampling, which may be non trivial even when we restrict our attention to the expectation values of the sampled variables. On average sampling does not affect the distributions of individual objects belonging to different kinds, but when we consider frequency distributions (that is the number of kinds that are represented k times in a given population) we cannot in general expect that the frequency distribution in the samples be the same as in the original population, even after averaging on many different samples, basically because the cutoff induced by sampling acts differently (and in general nontrivially) at different scales. It is therefore quite important to be able to extract from the frequency distribution of the samples some information reflecting directly some intrinsic property of the underlying distribution.

Our purpose is therefore threefold. First we want to discuss the general relationship existing between some arbitrary frequency distribution and the expectation value of the frequency distributions of its samples, and construct observables whose expectation values turn out to be independent of the sample size, and therefore coinciding with the value taken by the same observables in the full distribution.

Moreover we want to study classes of distributions whose samples preserve the functional dependence on the parameters present in the original distribution, establishing the connection between the (a priori unknown) values of the parameters of the distribution and the (empirically measured) parameters of the sample distributions.

Finally we want to study the scaling limit of these distributions (when it exists), in order to explore the possibility of their use for the phenomenological description of systems that are theoretically expected to show scaling in the limit when all empirical cutoffs (including those induced by sampling) are going to disappear.

In Section [\ref=framework] we establish the notation and the general framework of our analysis.

In Section [\ref=moments] we construct a wide set of combinations of expectation values that do not depend on the sample size.

In Section [\ref=ewens] we apply our approach to the popular Ewens sampling formula, showing that its features are consistent with the general pattern and computing its invariant expectation values.

In Section [\ref=small] we consider the limiting case of a small sampling applied to a large population.

In Section [\ref=large] we focus on the case when the original population and its samples are sufficiently large in comparison with typical frequency values, finding a useful mathematical relationship between the generating function of the expectation values of the sample distributions and the generating function of the original distribution.

In Section [\ref=distributions] we analyze a class of distributions (the so-called negative binomial distributions) admitting a scaling limit and enjoying the property that the distribution of expectation values of the samples has the same mathematical form as the original distribution. We also compute in a closed form the values ot the basic invariant expectation values for these distributions.

Finally in Section [\ref=scaling] we analyze the scaling limit itself and discuss the conditions under which one may expect this limit to be a sensible description of the original system.

Appendices are devoted to the proofs of some mathematical results and to discussing the issue of correlation between random samples.

The general framework

We are considering a set of N objects ("individuals") belonging to S different kinds ("species"), and we assume that the set contains N̂a objects of the a-th kind, subject to the constraint [formula].

A sample is a set of n objects, containing n̂a objects of the a-th kind, subject to the constraint [formula].

The probability P{n̂a} of extracting a specific sample {n̂a} from a given set {N̂a} is obtained from the multivariate hypergeometric distribution

[formula]

We can easily compute the relevant expectation values, obtaining in particular

[formula]

where [formula] is the probability of extracting an object of the a-th kind in a single extraction.

It may be useful to consider also the limit of small samples n̂a  ≪  N̂a. In this limit the probability of a specific sample is well approximated by the multinomial distribution

[formula]

A frequency distribution is a set of values {Nk}, where Nk is the number of kinds such that for each of them there are k objects in the original set. According to the definition, the following conditions must be satisfied:

[formula]

The frequency distribution of a sample is a set of values {nl}, satisfying the conditions

[formula]

Notice that the frequency distribution of a sample formally includes the (unobservable) value n0, corresponding to the number of kinds, present in the original set, which are not represented in the sample.

It is in principle possible to compute the probability of any sample distribution {nl} as a function of a given set {Nk}. To this purpose it is convenient to define the intermediate variables Nkl, representing the (random) number of kinds characterized by k objects in the original set and by l (l  ≤  k) objects in the sample. The variables Nkl are strongly constrained, since they must satisfy all the conditions:

[formula]

The probability P{Nkl} of a specific configuration {Nkl} follows from the general probability formula [\cite=Zelterman]:

[formula]

subject to the constraint [formula].

The probability P{nl} of finding a frequency distribution {nl} in a sample is obtained by summing the probabilities P{Nkl} over all configurations satisfiying the constraint [formula]. The corresponding multivariate generating function can be defined as

[formula]

It is also possible (and it will be quite convenient) to define a cumulative generating function for the probability of finding the frequency distributions P{nl} for samples of all possible sizes :

[formula]

where we used the explicit expression of P{Nkl} and all the relevant constraints.

The expectation values 〈nl〉 can be computed starting from the above expressions and from the relationship

[formula]

Straightforward manipulations lead to the results [\cite=Zelterman]

[formula]

It is easy to check that the following relationships are satisfied:

[formula]

In order to fully appreciate the relevance of considerations based on the expectation values we must evaluate the weight of the fluctuations. Taking second derivatives of the generating function E(x;tl) one obtains:

[formula]

Notice that in the large N limit the term quadratic in Nk is depressed by a power of 1 / N. This observation suggests that very important limits of the above results may be obtained when considering large populations.

Invariant expectation values

It is very important to be able to define a set of expectation values that are independent of the size of the sample, and therefore may reflect very directly the properties of the original frequency distribution.

Let's consider the following combinations of expectation values:

[formula]

where pi are I arbitrary positive integer numbers, subject only to the constraint that [formula].

The definition of the quantities appearing in the r.h.s. implies that the derivatives with respect to tqi are the joint factorial moments of the distribution; therefore we are dealing with weighted combinations of joint factorial moments. When some of the indices pi are equal to one, the expectation values may be expressed in terms of a combination of lower rank moments (I'〈I).

It is possible to recognize that the quantities 〈m(P){pi}〉 are related (up to a trivial combinatorial factor taking into account the existence of np coincident values of the indices pi) to the probability of finding the configurations {pi} in the sample containing P elements, and are therefore strictly connected with the probabilities P{np} .

Exploiting the properties of the generating function E(x;{tl}) we prove in Appendix [\ref=Expectation] that

[formula]

for all sets {pi} such that P  ≤  n. Hence the expectation values of the nontrivial invariant moments m{pi} evaluated for samples of arbitrary size n  ≥  P, coincide with the moments M{pi} of the original frequency distribution. If the original set was generated by a random process, also the M{pi} will be expectation values. Recalling that [formula] we may now generate a representation of all P{np} in terms of Nk, without making use of the coefficients Nkl.

The properties of the binomial coefficients make it possible to invert the relationship between invariant moments and joint factorial moments, thus finding that

[formula]

The basic invariant moments are

[formula]

According to the inversion formula

[formula]

One may define generating functions for the expectation values of nl and of the basic invariant moments:

[formula]

Notice that a special case of the above formula is

[formula]

It is immediate to recognize that [formula], and [formula].

Application to Ewens sampling formula

The multivariate Ewens distribution [\cite=Ewens] [\cite=Karlin], called in genetics the Ewens sampling formula, describes a specific probability for the partition of n into parts, and found its main applications in the context of the neutral theory of evolution and in the unified neutral theory of biodiversity [\cite=Hubbell] [\cite=Rosindell]. Since Ewens formula and its possibile generalizations have been the subject of a wide and still growing literature [\cite=Griffiths] [\cite=Etienne] [\cite=Lessard] [\cite=Lambert], it may be interesting to apply the results presented in Section [\ref=moments] to this specific instance. In our notation Ewens probability distribution takes the form

[formula]

where 0 < θ  <    ∞   and [formula].

Joint factorial moments of the Ewens distribution are easily computed [\cite=Johnson] and one can show that

[formula]

where [formula].

We are then left with the task of computing the summations appearing in the equation

[formula]

We prove in Appendix [\ref=Combinatorics] that

[formula]

hence

[formula]

showing explicitly that the expectation values of the invariant moments of the Ewens distribution are independent of the sample size and related to the probability of the configuration {pi} in the sampling of P elements.

We stress that invariant moments, because of their independence from the size of the sample, may become a highly valuable tool in testing the applicability of Ewens distribution (and of the conceptual assumptions underlying its derivation) to the interpretation of actual empirical data.

Large population and small samples

A significant simplification occurs when N  →    ∞   while all other variables are kept finite. Setting [formula] and t0  =  1 in the cumulative generating function and taking the large N limit we obtain

[formula]

Let's now define (for n,l different from zero) the following set of coefficients:

[formula]

where [formula] and [formula], and notice that the definition of Ẽ implies that

[formula]

It is also possible to recognize that, under the same assumptions,

[formula]

where we introduced the large N limit of the basic invariant moments: [formula].

Comparing the two results we conclude that, for each value of n〉0,

[formula]

These equations allow in principle for the recursive determination of all (n)({tl} in terms of {(N)p} (with p  ≤  n), starting from the initial condition (1)  =  t1. Higher rank invariant moments (I〉1) in the large N limit become polynomials in the basic moments. However one must keep in mind that, when the set {Nk} not fixed, but generated by a probability distribution (as in the case of the Ewens formula), the expectation values of the products of basic moments appearing in the l.h.s. do not coincide with the products of the expectation values.

Large populations and large samples

When k,l  ≪  N,n one may systematically exploit the property that, for small a and b,

[formula]

Expressing N and n in terms of Nkl one may then obtain

[formula]

As shown in Appendix [\ref=Fluctuations], when computing expectation values with the above probability distribution, the constraint [formula] becomes irrelevant in the large N limit, and expectation values of products of Nkl with different values of the index k factorize into products of expectation values computed for each separate value of k. We can therefore compute directly the generating function for a fixed sample size, generalizing the multivariate multinomial distribution:

[formula]

A consistency check is easily obtained by observing that ε(n)(1)  =  1, because of the property that [formula].

The expectation value of nl turns out to be:

[formula]

and one may check that the conditions on [formula] and on [formula] are still satisfied.

We can also estimate the behavior of fluctuations when k,l  ≪  N,n:

[formula]

The above expression is always smaller than 〈nl〉 and as a consequence fluctuations become unimportant for sufficiently large values of 〈nl〉.

In the same limit we may derive a notable relationship between the generating function of the original frequency distribution and the generating function of the expectation values of its samples. In fact we may recognize that for sufficiently large N and n

[formula]

Since in general [formula] and [formula], it is then easy to check that in the limit under consideration

[formula]

As a direct consequence of these results, whenever the (size-independent) function [formula] can be cast into a form exhibiting no explicit parametric dependence on N, the expectation values 〈nl〉 can be obtained from Nk by the replacement N  →  n.

Notice that in the limit k,l  ≪  N,n the definition of the basic invariant moments m(n)p simplifies to

[formula]

It is worth analyzing in this limit the explicit expressions of the second basic invariant moment:

[formula]

As shown in Appendix [\ref=Correlation] the above results may be used also in order to parametrize the expected correlation between samples under the assumption of independent random sampling.

A class of distributions and its properties

As mentioned in the Introduction, distributions found in samples may often correspond to systems whose asymptotic (N  →    ∞  ) distribution is expected to obey a scaling law. However the exponent of the scaling law will in general be nontrivial, in contrast with the prediction offered by the simplest neutral models. An example of empirical and theoretical evidence for nontriviality is offered by surname frequency distributions (see Ref. [\cite=Rossi] for a recent review), recalling that surnames are expected to mimick the behavior of selectively neutral alleles. it is therefore especially interesting to consider parametrizations that may reflect notriviality of exponents, ad in particular the class of negative binomial distributions [\cite=Hilbe], which can be obtained starting from the generating function

[formula]

where 0〈x〈1 and the parameter c is assumed to vary in the range 0  ≤  c〈1.

The asymptotic behaviour of the distribution for large k is easily obtained by observing that in this limit

[formula]

We can now compute the generating function for the expectation values of the samples according to the general rule previously discussed, and obtain

[formula]

where we have defined [formula].

The distribution of the samples has the same form as the original distribution, once the replacements N  →  n and x  →  y have been performed, and therefore we obtain the asymptotic behaviour

[formula]

It is possible to define a combination of parameters independent of the dimension of the sample:

[formula]

It is useful to represent x and y in a form showing explicitly their dependence on the dimension of the sample and on the invariant parameter β:

[formula]

It is now possible to evaluate the expectation value of the invariant moments from the expression

[formula]

showing no explicit parametric dependence on N; we therefore obtain (for p  ≠  0)

[formula]

The limit of the above results when c  →  0 is smooth, and it corresponds to Fisher distribution [\cite=Fisher], such that

[formula]

and

[formula]

The generating function of the invariant moments is obtained from γ0(z)  =    -  β ln (1 + z / β), and as a consequence the expected values of the invariant moments (p  ≠  0) are exactly Mp = (p - 1)!  β1 - p.

Notice in particular the relationship β  =  α, peculiar to Fisher distribution. By comparing these results with the large θ limit of the invariant moments of Ewens distribution we may easily check Watterson's [\cite=Watterson] and Hubbell's [\cite=Hubbell] observation that in this limit θ is strictly connected to Fisher's α.

The scaling limit

Let's now consider very large systems, and assume that we can gather information only through the sampling of n objects belonging to the system, with n large but not necessarily comparable to N.

The analysis of the invariant moments may then allow us to check the applicability of a phenomenological description of the samples based on some distribution falling into the classes discussed in the previous Sections. In the case of a positive response to the check it is then possible to find numerical estimates of the parameter β and of the exponent c. Such estimates are clearly meaningful only if β does not turn out to be significantly greater than n.

Under these assumptions, we can infer a description of the original system, and in the case N〉〉n such a description will correspond to computing the limit x  →  1 of the previous results. As a consequence, at least for observable (i.e. not too large) values of k, the original distribution is expected to be well described by the scaling form

[formula]

Expectation values of the invariant moments

Let's consider the cumulative generating function for the expectation value of a given invariant moment (setting [formula]) for samples of all possible sizes:

[formula]

Let's now observe that, due to the property that

[formula]

the derivatives appearing in the above defined cumulative generating function, computed at tl  =  1, can be expressed as summations (over {ki} indices) of products of Nki times a universal x-dependent factor

[formula]

where [formula] and [formula]. However the following identity holds:

[formula]

As a consequence the cumulative generating function is proportional to the factor

[formula]

The summations over the indices {ki} may now be formally performed, and, by matching the coefficient of xN in the two sides of the equation, the result can be easily recognized to coincide with the expected value of the invariant moment computed for the original distribution times the combinatorial factor [formula].

In conclusion we find

[formula]

Expanding the r.h.s. in powers of x and noticing that [formula] we finally obtain

[formula]

implying immediately that, as long as n  ≥  P,

[formula]

Proof of a combinatorial formula

The identity

[formula]

where [formula] and [formula], can be proven by recalling that for real numbers α and positive integers q  ≥  p

[formula]

Hence expanding in powers of x the two sides of the identity

[formula]

and exchanging the order of summations in the l.h.s. we get

[formula]

The desired result is then obtained by setting m = n - P.

Fluctuations of the sample size in the unconstrained large N limit

The multivariate generating function ε({tl}) was computed in Section [\ref=large] after relaxing the constraint [formula]. In order to show that the constraint is automatically satisfied in the large N limit we construct a generating function for the expectation value of the powers of [formula] in the large N distribution:

[formula]

and applying the multinomial formula we obtain

[formula]

Expanding the result in powers of w we immediately obtain P(ν)  =  PNν.

Since ν is distributed according to the binomial distribution, the relevant expectation values are

[formula]

Hence fluctuations of ν / N around ρ vanish like 1 / N in the large N limit.

Correlation between samples

An important test of randomness in sampling is offered by the measure of the correlation between two different samples. Let's consider two random samples, characterized by the sets of values {n̂a} and {a} and by their sizes n and m. The index a labels different kinds, as in Section [\ref=framework]. The correlation between the two samples is

[formula]

Replacing n̂a and n̂2a with their expectation values, computed in Section [\ref=framework], we obtain (in the large N limit)

[formula]

[formula]

By making use of the results presented in Section [\ref=large] we can now express the expected value of the correlation between samples in the form

[formula]

For samples of equal size n the expected value of the correlation takes the form [formula].