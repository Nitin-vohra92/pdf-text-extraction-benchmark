Corollary Lemma Remark Definition Observation Proposition

PARTIAL NORMS AND THE CONVERGENCE OF GENERAL PRODUCTS OF MATRICES

Department of Mathematics University of Connecticut Storrs, CT 06268, USA Hans Schneider Department of Mathematics University of Wisconsin Madison, WI 53706, USA

Introduction

Recently there has been much interest in conditions for the convergence of infinite products of real or complex matrices. Several investigations have concentrated on products taken in one direction - left or right, see for example the recent papers by Beyn and Elsner [\cite=BeEl97] and Hartfiel and Rothblum [\cite=HaRo97]. However, in this paper, we are concerned with general products formed from a given infinite sequence of matrices. These are defined further on in the paper and they have previously been considered for nonnegative and for stochastic matrices by Seneta in [\cite=Sene81].

Our principal result is a sufficient condition for the convergence to 0 of infinite general products of matrices. We pay particular attention to the case where there is a common invariant subspace for all the matrices in the product. As a special case, we obtain a result on the weak ergodicity of an inhomogeneous Markov chain.

The investigations described above are preceded by a study of the interrelations of several types of contractions which may be defined for a single matrix.

The motivation for our study comes from the theory of inhomogeneous Markov chains, see [\cite=Sene73] and [\cite=Sene81] for much further background material. This theory naturally leads to the study of the convergence, ergodicity and weak ergodicity of products of stochastic matrices.

We now describe our paper in more detail. First, in Section 2, we sharpen several observations concerning paracontraction. This we shall achieve by introducing a partial norm on the matrix restricted to an invariant subspace H and the consequent notion of H-contraction. We relate this concept to the notion of paracontraction as introduced by Nelson and Neumann in [\cite=NeNe87] and to the notion of l-contraction recently introduced by Beyn and Elsner in [\cite=BeEl97].

Second, in Section 3, which is essentially independent of Section 2, we turn to our main results. We formulate our sufficient conditions depending on norms for the convergence to 0 of infinite general products of matrices which are formed from a given infinite sequence of matrices. We then apply the results to examine the convergence to 0 of infinite general products on a common invariant subspace of the matrices.

In Section 4 we specialize our results and we investigate products of stochastic matrices to obtain a suffcient condition for weak ergodicity. The [formula] coefficient of ergodicity due to Bauer, Deutsch, Stoer [\cite=BaDeSt69] plays a special role here.

In Section 5 we give bounds on the algebraic and geometric multiplicities of the eigenvalues of the restriction of a matrix to an invariant subspace in terms of these quantities for the entire matrix. These are related to familiar results on stochastic matrices.

Partial norms and Paracontractions

In this paper [formula] will stand for the real field [formula] or the complex field [formula]. We begin by recalling the following definition:

In particular we see that one immediate consequence of the definition is that ν0(A)  ≤  1, where ν0(A) denotes the operator norm corresponding to the norm ν. For a given matrix norm let Nν denote the set of all paracontracting matrices with respect to ν in [formula]. Nelson and Neumann show in [\cite=NeNe87] that if A∈Nν, then lim i  →    ∞Ai exists and that AB∈Nν if A,B∈Nν.

We now introduce the concept of a partial norm with respect to a subspace and the concept of H-contraction:

We shall denote the range of [formula] by R(A) and the nullspace of A by N(A).

If [formula] is nonexpansive with respect to ν then the spectral radius ρ(A) of A satisfies ρ(A)  ≤  ν0(A)  ≤  1. If 1 is an eigenvalue of A, then we must have that ν0(A) = ρ(A) and it follows that all Jordan blocks of A corresponding to 1 are 1  ×  1, (viz. index0(A)  =  1), e.g., Mott and Schneider [\cite=MoSc59]. Hence K = N(I - A) and H = R(I - A) are complementary invariant subspaces.

We next show the connection between paracontraction and H-contraction.

Let [formula] be paracontracting with respect to ν and let K be the subspace of all its fixed points. Then H = R(I - A) is invariant under A, complementary to K, and such that ν0H(A) < 1 so that A is an H-contractor.

Proof:  It follows by Remark [\ref=complement] that H is invariant under A and complementary to K. Since A is paracontracting with respect to ν, we have that ν0(A)  ≤  1. In view of ([\ref=para.cond]), it now follows that ν(Ax) < ν(x), for all x∈H, and so since the unit ball U of ν0H in H is compact, Hence ν0H(A) < 1. [formula]

Recently Beyn and Elsner [\cite=BeEl97] have introduced the notion of l-paracontraction:

In their paper Beyn and Elsner establish several conditions which are equivalent to l-paracontraction. Clearly l-paracontraction implies paracontraction. We next show that H-contraction implies l-paracontraction for a suitable chosen norm and a suitably chosen subspace H.

Let [formula] and suppose that A is nonexpansive with respect to the norm ν on [formula]. Let H = R(I - A) and let K  =  N(I - A). If the norm ν satisfies

[formula]

then the following are equivalent:

(i) A is l-paracontracting with respect to ν.

(ii) A is paracontracting with respect to ν.

(iii) A is an H-contraction with respect to ν.

Proof:  (i) [formula] (ii): Obvious.

(ii) [formula] (iii): By Lemma [\ref=lemma1].

(iii) [formula] (i): Since A is nonexpansive, we have by Remark [\ref=nonexpan] that [formula]. For [formula], write Let k  =  ν0H(A) and note that k  <  1 by (iii). Since x  -  Ax = y  -  Ay∈H it follows that

[formula]

Moreover, using ([\ref=additive]), we obtain

[formula]

We combine ([\ref=inequ1]) and ([\ref=inequ2]) and we deduce that and this is equivalent to ([\ref=beyn.elsner]) with γ  =  (1 - k) / (1 + k). [formula]

Convergence of Infinite Products

In this section we develop our main results concerning the convergence of products of complex matrices taken in an arbitrary order from an infinite sequence of matrices. Such products were considered (in a slightly less general form) in Seneta [\cite=Sene81] in the case of stochastic matrices, see also [\cite=Leiz92] and [\cite=Rhod97].

Let [formula] be a sequence of complex matrices. We shall consider products of matrices obtained from the sequence in the following manner: First choose some permutation of the given infinite sequence to obtain a sequence [formula]. Then form the products Cp,r of the matrices [formula] in some order. We shall call Cp,r a general product from the sequence [formula] and we shall consider the existence of lim r  →    ∞Cp,r. If this limit is 0, for all permutations of [formula] and all p, p  ≥  0, then we shall say that all general products from the the sequence [formula] converge to 0.

As an example of a sequence of general product suppose the chosen order is A9,A7,A5, [formula]. Then the sequence of [formula] may begin thus: Note that, for a given sequence [formula] of general products each factor of Cp,r occurs in Cp,r + 1, but the order in which the factors occur in Cp,r is arbitrary.

Let μ be a matrix norm (viz. a submultiplicative norm on [formula]) and denote

Now let [formula] be a sequence of matrices in [formula] and let μ be a matrix norm. We now define two conditions:

We are now ready to prove the following result:

Let [formula] be a sequence of matrices in [formula]. Let μ be a matrix norm on [formula]. Suppose that the sequence [formula] satisfies Condition (C)  for the norm μ. Then all general products from [formula] are bounded.

Proof:  Let [formula] be a permutation of [formula] and let Cp,r be a product of [formula] in some order. By Condition  (C) and [\cite=Hysl45], [formula] converges and hence [formula] also converges. Thus, by [\cite=Hysl45], the product [formula] converges and so there exists a positive constant M such that [formula], for each [formula]. It follows that

[formula]

[formula]

The above proposition allows us to prove a stronger result under an additional conditions. Note that in the theory of infinite products of nonnegative numbers it is customary to speak of divergence to 0, see e.g. [\cite=Hysl45].

Let [formula] be a sequence of matrices in [formula]. Let μ be a matrix norm on [formula]. Suppose that the sequence [formula] satisfies Conditions (C)  and (D)  for the norm μ. Then all general products from [formula] converge to 0.

Proof:  Let [formula] be a permutation of [formula] and let Cp,r be a product of [formula] in some order. As in the proof of Proposition([\ref=prop.bdd]), we have that

[formula]

By Condition (D), the sum [formula] diverges and so, by [\cite=Hysl45], [formula] diverges. Thus [formula] also diverges.

We again apply [\cite=Hysl45], and we obtain that [formula] diverges. But since μ-(Bi)  ≤  1, the last product must diverge to 0 and the proof is done. [formula]

If H is a subspace of [formula] which is invariant under [formula], we denote the restriction of A to H by A|H. As an immediate application of Theorem [\ref=theorem.conv] we obtain the following result:

Let [formula] be a sequence of matrices and let [formula] be sequence of general products from the [formula]. Let H be a subspace of [formula] which is invariant under each [formula]. Let ν be a norm on [formula]. Suppose that the sequence ((Ai)|H) satisfies conditions (C)  and (D)  for the norm ν0H. If x∈H, then

Proof:  Immediate by Theorem [\ref=theorem.conv]. [formula]

Since ν0H(A)  ≤  ν0(A) when H is an invariant subspace of A, it follows easily that under the conditions of Corollary([\ref=ergod.prod]), Condition (C) (resp. Condition (D)) on the sequence of (Ai) implies Condition (C) (resp. Condition (D)) on the sequence of ((Ai)|H).

Applications to Stochastic Matrices

In this section we apply the foregoing results to stochastic matrices. In order to be consistent with our previous section we consider column stochastic matrices.Thus "stochastic matrix" will mean "column stochastic matrix".

Let [formula]. Hence in this section we shall assume that

[formula]

If A is a stochastic matrix in [formula], then H is invariant under A, but note that H need not be a complement to the space of fixed points of A. If ν is a norm on [formula] and H is given by ([\ref=partic.H]), then the corresponding partial norm ν0H on [formula] will be called a coefficient of ergodicity as is usual in the literature on Markov chains.

The [formula] norm on [formula] plays a special role in this theory and we shall denote it henceforth by ω. The corresponding coefficient of ergodicity was apparently first computed in Bauer, Deutsch, and Stoer [\cite=BaDeSt69], see also [\cite=Zeng72], and equals It is known that ω0H(A)  ≤  1 for all stochastic matrices A and ω0H is the only coefficient of ergodicity that satisfies this inequality, [\cite=KuRh90] and [\cite=Lesa90], but see also [\cite=Rhod97].

Since every x∈H can be written x  =  c(u  -  v), where [formula] are nonnegative and eTu  =  eTv  =  1 and [formula], it is easily seen that, for each product considered, our definition is equivalent to that in [\cite=Hajn58], [\cite=MoSc57], or [\cite=Sene81].

By Theorem [\ref=theorem.conv] we now immediately obtain:

Let ν be a norm on [formula] and let ν0H be the corresponding coefficient of ergodicity. Let [formula] be a sequence of n  ×  n stochastic matrices. Then all general products formed from this sequence are weakly ergodic if

[formula]

and

[formula]

Note that ([\ref=condCW]) is automatically satisfied if ν  =  ω, the [formula]-norm. This special case of Theorem [\ref=wkerg] is observed in [\cite=Sene81]. Results related to this case (and which therefore involve only ([\ref=condDW]) explicitly) are to be found in [\cite=Sene73] and in [\cite=MoSc57]. The theorem in the latter paper is there illustrated by an example of a sequence of stochastic matrices that satisfies ([\ref=condDW]) for the norm ω, see [\cite=MoSc57].

The following corollary is due to Rhodius [\cite=Rhod97] in the case of ν  =  ω, see Leizarowitz [\cite=Leiz92] for a related result.

Let [formula] be a sequence of stochastic matrices. If  ν0H(Pi)   ≤  1 for all [formula], and there exists a point of accumulation c of the sequence [formula] such that c  <  1, then all general products of the sequence are weakly ergodic.

Proof:  Clearly condition ([\ref=condCW]) holds and there is an infinite subsequence [formula] such that ω0H(Pij)  <  (1 + c) / 2  <  1, [formula]. Then condition ([\ref=condDW]) holds for this subsequence. The result follows from Theorem ([\ref=wkerg]). [formula]

Another corollary of Theorem [\ref=wkerg] is [\cite=Leiz92]:

Let [formula] be a sequence of stochastic matrices and let ν be a norm on [formula]. If all points of accumulation c of the sequence ν0H(P1),  [formula] satisfy c  <  1, then all general products of the sequence are weakly ergodic.

Proof:  Since the set of accumulation points of a bounded sequence is compact, there exists d  <  1 such that only a finite number of terms of the sequence [formula] exceed d. Hence ([\ref=condCW]) and ([\ref=condDW]) hold for the sequence of ergodicity coefficients and the corollary follows from Theorem [\ref=wkerg]. [formula]

Bounds for eigenvalues

Let H be an invariant subspace for [formula]. Let αλ(A) and γλ(A) be the algebraic and geometric multiplicities of [formula] as an eigenvalue of A, respectively. Then and

Further, if μ is a matrix norm and |λ|  >  μ0H(A), then αλ(A)  ≤  n  -   dim (H) and γλ(A)  ≤  n  -   dim (H).

Proof:  By a proper choice of basis for [formula] we may put A in the form

[formula]

and the first inequality follows immediately.

The second inequality is an immediate consequence of [\cite=MeRo77], see also [\cite=HeRoSc89].

Finally, since all eigenvalues λ of A22 satisfy |λ|  ≤  μ0H(A), the last part of the theorem follows. [formula]

Theorem ([\ref=thm.bound]) implies several known results. We now give an example below which could also be derived from [\cite=HaRo97]. Let A be a stochastic matrix, let H be defined by ([\ref=partic.H]), and suppose that a coefficient of ergodicity satisfies ν0H(A)  <  1. Since, in this case, the matrix A1,1 in ([\ref=eqn3]) is 1  ×  1, it follows that 1 is an algebraically simple eigenvalue of A and all other eigenvalues λ of A satisfy ν0H(A)  ≥  |λ|.

Acknowledgment

We would like to thank Wenchao Huang for some helpful remarks. We thank Olga Holtz for her careful reading of the manuscript.