Some results concerning maximum Rényi entropy distributions

Lemma Proposition Corollary Conjecture Definition Example Remark Assumption

Proof

Proof .

English abstract: We consider the Student-t and Student-r distributions, which maximise Rényi entropy under a covariance condition. We show that they have information-theoretic properties which mirror those of the Gaussian distributions, which maximise Shannon entropy under the same condition. We introduce a convolution which preserves the Rényi maximising family, and show that the Rényi maximisers are the case of equality in a version of the Entropy Power Inequality. Further, we show that the Rényi maximisers satisfy a version of the heat equation, motivating the definition of a generalized Fisher information.

French title: Quelques résultats au sujet des distributions à entropie de Rényi maximale.

French abstract: Nous considérons les distributions de types Student-t et Student-r qui maximisent l'entropie de Rényi sous contrainte de covariance. Nous montrons qu'elles possèdent des propriétés informationnelles similaires à celles des distributions Gaussiennes, lesquelles maximisent l'entropie de Shannon sous la même contrainte. Nous montrons que ces distributions sont stables pour un certain type de convolution et qu'elles saturent une inégalité de la puissance entropique. De plus nous montrons que les lois à entropie de Rényi maximale vérifient une équation de la chaleur, ce qui permet de définir une information de Fisher généralisée.

Keywords: Entropy Power Inequality, Fisher information, heat equation, maximum entropy, Rényi entropy

Mathematics Subject Classification: Primary 94A17; Secondary 60E99

Introduction

It is natural to ask whether the Shannon entropy of a n-dimensional random vector with density p, defined as

[formula]

represents the only possible measure of uncertainty. For example, Rényi [\cite=renyi2] introduces axioms on how we would expect such a measure to behave, and shows that these axioms are satisfied by a more general definition, as follows:

Given a probability density p valued on [formula], for q  ≠  1 define the q-Rényi entropy to be:

[formula]

Note that by L'Hôpital's rule, since [formula],

[formula]

As Gnedenko and Korolev [\cite=gnedenko2] remark, under a variety of natural conditions the distributions which maximise Shannon entropy are well-known ones, with interesting properties. This paper gives parallels to some of these properties for the Rényi maximisers.

Under a covariance constraint Shannon entropy is maximised by the Gaussian distribution. In Proposition [\ref=prop:renmax] we review the fact that under a covariance constraint Rényi entropy is maximised by Student distributions.

The Gaussians have the appealing property of stability (that is, given Z1 and Z2 Gaussians, Z1 + Z2 is also Gaussian). In Definition [\ref=def:starconv], we introduce the [formula]-convolution, which generalizes the addition operation. In Lemma [\ref=lem:stab], we extend the stability property by showing that if R1 and R2 are Rényi maximisers then so is [formula].

The Entropy Power Inequality (see Equation ([\ref=eq:epi]) below) shows that the Gaussian represents the extreme case for how much entropy can change on addition. Theorem [\ref=thm:epi] gives the equivalent of an Entropy Power Inequality, with the Rényi maximisers playing an extremal role.

The Gaussian density satisfies the heat equation, which leads to a representation of Shannon entropy as an integral of Fisher Informations (known as the de Bruijn identity). In Theorem [\ref=thm:qheat] we show that the Rényi densities satisfy a generalization of the heat equation, and deduce what quantity must replace the Fisher information in general.

First, as in Costa, Hero and Vignat [\cite=costa2], we identify the Rényi maximising densities, which are Student-t and Student-r distributions, and review some of their properties which we will use later in the paper.

For n / (n + 2)  <  q and q  ≠  1, define the n-dimensional probability density [formula] as

[formula]

with

[formula]

and normalization constants

[formula]

Here x+  =   max (x,0) denotes the positive part. We write [formula] for a random variable with density [formula], which has mean [formula] and covariance [formula].

Notice that if we write [formula] for the support of [formula], then for q  >  1, [formula], and for q  <  1, [formula].

Note further that since lim q  →  1Γ(1 / (1 - q))(1 - q)n / 2  /  Γ(1 / (1 - q)  -  n / 2) = 1 and [formula], the limit [formula], the Gaussian density. Throughout this paper, we write [formula] for a [formula] random variable.

We now state the maximum entropy property, as follows.

Given any q  >  n / (n + 2), and positive definite symmetric matrix [formula], among all probability densities f with mean [formula] and [formula], the Rényi entropy is uniquely maximised by [formula], that is

[formula]

with equality if and only if [formula] almost everywhere.

See Section [\ref=sec:maxent]

Throughout this paper, we write χm for a random variable with density

[formula]

(Strictly speaking, this is only a χ random variable when the parameter m is an integer, but it is simpler to adopt the convention of allowing non-integer m than to refer to the square root of a Γ(m) random variable with scale factor 2).

We briefly review stochastic representations of the Rényi maximisers, which we will use throughout the paper. For the sake of completeness, we present proofs of these results in Section [\ref=sec:stochrep]. Part 1. of Proposition [\ref=prop:stochrep] follows for example from P.393 of Eaton [\cite=eaton], Part 2. of Proposition [\ref=prop:stochrep] is stated in Dunnett [\cite=dunnett], and Part 3. of this proposition is a multivariate version of a result stated as long ago as 1915 by Fisher [\cite=fisher].

Writing [formula] for a n-dimensional q-Rényi maximiser with mean [formula] and covariance [formula], and writing [formula] for a [formula]:

Student-r. For any q  >  1, writing m  =  n  +  2q / (q - 1)

[formula]

where U  ~  χm (independent of [formula]).

Student-t. For any n / (n + 2)  <  q < 1, writing m = 2 / (1 - q)  -  n  >  2,

[formula]

where U  ~  χm (independent of [formula]).

Duality. Given matrix [formula], define the map

[formula]

For q < 1, writing m  =  2 / (1 - q)  -  n, if [formula] is a Rényi maximiser, then [formula], where 1 / (p - 1)  =  1 / (1 - q)  -  n / 2 - 1 (so q  <  1 implies that p  >  1) and [formula].

See Section [\ref=sec:stochrep].

Stochastic representations ([\ref=eq:stochastic_r]) and ([\ref=eq:stochastic_t]) can be used to compute the covariance and entropy of [formula]. For example, for q  <  1, since U  ~  χm, the [formula] so that [formula] as claimed.

Similarly for q  <  1, the Shannon entropy [formula] is given by (writing m  =  2 / (1 - q)  -  n)

[formula]

where [formula], and since [formula] where Ψ(  ·  ) is the digamma function, we obtain

[formula]

Indeed, the theory of such stochastic representations can be generalized from the setting of [\cite=lutwak] and [\cite=lutwak2] to multivariate maximizers with different powers. That is, given a positive sequence [formula], the solution to the problem

[formula]

is a random vector [formula] with density given by

[formula]

where it can be shown that the ai all have the same sign as 1 - q. Moreover, if [formula] is such a maximizer with q > 1, then for [formula] random variables Zk = U1 / pkkXk are independently power-exponential distributed with marginal densities

[formula]

when Uk is χ-distributed with [formula] degrees of freedom and independent of [formula].

[formula]-convolution and relative entropy

In this section, we introduce a new operation, which we refer to as the [formula]-convolution. In Lemma [\ref=lem:stab] we show that this [formula]-convolution preserves the class of Rényi entropy maximisers, and in Theorem [\ref=thm:epi] show that it satisfies a version of the entropy power inequality.

We will say that a distribution is q-Rényi if it maximises the q-Rényi entropy. For the sake of simplicity, we write [formula] for the relative entropy between the two densities fX and fY of random variables X and Y. We define a new distance measure:

Given a n-dimensional random vector [formula] with mean [formula] and covariance [formula], we define its distance from a n-dimensional q-Rényi maximiser [formula] (for q > 1) to be

[formula]

where U is a χm random variable (with m  =  n  +  2q / (q - 1) degrees of freedom) independent of [formula], and [formula].

Note that d inherits positive definiteness from D - that is [formula], with equality if and only if [formula]. Note further that Equation ([\ref=eq:mult]) below implies that

[formula]

Motivated by Proposition [\ref=prop:stochrep], we make the following definition:

For fixed q  >  1, given two n-dimensional random vectors [formula], with covariance matrices [formula] and [formula], define the [formula]-convolution (or just [formula]-convolution) of [formula] and [formula] to be the n-dimensional random vector

[formula]

where [formula], and U(S),U(T),V are independent χ random variables, where U(S) and U(T) have m  =  n  +  2q / (q - 1) degrees of freedom, and V has 2q / (q - 1) degrees of freedom.

Again, notice that as q  →  1, U(  ·  ) / (2q / (q - 1))  →  1 and V / (2q / (q - 1))  →  1 by the Law of Large Numbers, so [formula].

For q  >  1, if [formula] and [formula] are q-Rényi entropy maximisers with covariances [formula] and [formula] then [formula] is also a q-Rényi entropy maximiser, with covariance [formula].

By Proposition [\ref=prop:stochrep].1, writing m = n  +  2q / (q - 1), we know that [formula] and [formula] are [formula] and [formula] respectively. We define [formula] by [formula], and write [formula]. Then random variable [formula] is [formula], where [formula].

Then (by Proposition [\ref=prop:stochrep].2) since V has [formula] degrees of freedom, [formula] is [formula]-Rényi, with covariance [formula]. Finally (by Proposition [\ref=prop:stochrep].3), [formula] is q*-Rényi, where 1 / (q* - 1)  =  1 / (1 - q1)  -  n / 2  -  1  =  1 / (q - 1), so in fact it is q-Rényi with covariance [formula]. Hence, [formula] is q-Rényi with covariance [formula], and the result follows.

We now give a new ([formula]-convolution) version of the classical Entropy Power Inequality, which was first stated by Shannon as Theorem 15 of [\cite=shannon], with a 'proof' sketched in Appendix 6. More rigorous proofs appeared in Blachman [\cite=blachman] and later in Dembo, Cover and Thomas [\cite=dembo]. The result gives that for independent n-dimensional random vectors [formula] and [formula],

[formula]

with equality if and only if [formula] and [formula] are Gaussian with proportional covariance matrices.

Writing [formula] for the covariance matrix of [formula], we know that [formula], so that the Entropy Power Inequality ([\ref=eq:epi]) is equivalent to

[formula]

We give an equivalent of Equation ([\ref=eq:relepi]), with the [formula]-convolution replacing the operation of addition.

Given q  >  1, for independent n-dimensional random vectors [formula] with mean [formula] and covariances [formula], [formula],

[formula]

with equality if and only if [formula] and [formula] are q-Rényi with proportional covariance matrices.

By Proposition [\ref=prop:mapgood] below we know that for U(S),U(T),V,W all independent and χ-distributed, where U(S),U(T),W have m  =  n  +  2q / (q - 1) degrees of freedom, and V has 2q / (q - 1) degrees of freedom:

[formula]

We can combine Equations ([\ref=eq:relepi]) and ([\ref=eq:dec1]) to obtain that

[formula]

and the result follows. Equality holds in Equation ([\ref=eq:dec1]) if [formula] is Gaussian. This, along with proportionality of covariance matrices, is also the condition for equality in Equation ([\ref=eq:relepi]).

There is a parallel theory for the case q  <  1, where we define a [formula]-convolution:

For fixed q satisfying n / (n + 2)  <  q  <  1, given two random vectors [formula] and [formula] with covariance matrices [formula] and [formula] respectively, define the [formula]-convolution by

[formula]

with m  =  2 / (1 - q) - n, where the [formula]-convolution is taken with respect to index [formula] satisfying [formula] and

[formula]

This definition satisfies an analogue of Lemma [\ref=lem:stab]:

For q  <  1, if [formula] and [formula] are q-Rényi entropy maximisers with covariances [formula] and [formula] then [formula] is also a q-Rényi entropy maximiser, with covariance [formula].

By Proposition [\ref=prop:stochrep].3, [formula] maximises [formula]-Rényi entropy with  > 1 such that 1 / ( - 1) = 1 / (1 - q) - n / 2 - 1.

Moreover, the covariance matrix of [formula] is [formula]. The same result holds for [formula] and [formula] As a consequence of Lemma [\ref=lem:stab], [formula] is a [formula]-Rényi distribution with covariance [formula]

Since by Proposition [\ref=prop:stochrep].3, [formula], where [formula], taking inverse maps, [formula]. Here [formula], as required.

q-heat equation and q-Fisher information

In this section, we show that the Rényi maximising distributions satisfy a version of the de Bruijn identity. That is, we can define a Fisher information quantity, and show in Equation ([\ref=eq:debruijn]) that it is the derivative of entropy. First, we compute the exact constants in a result of Compte and Jou [\cite=Compte].

For a fixed μ, write fτ for the density of a [formula] random variable. If μ  =  2 / (2  +  n(q - 1) / 2) then fτ satisfies a heat equation of the form

[formula]

with

[formula]

By Equation ([\ref=eq:renmax]), we know that for a general choice of μ:

[formula]

First note that

[formula]

Further, for any k, writing [formula]:

[formula]

Hence, for any k, l:

[formula]

Overall, we deduce that

[formula]

so that equating this with Equation ([\ref=eq:tdiff]) we obtain:

[formula]

Now, we want this to not be a function of τ, so take μ  =  2 / (2  +  n(q - 1)), and substitute for β to obtain

[formula]

as claimed.

Note that the value of the exponent μ coincides with the one given by Compte and Jou [\cite=Compte]. Further, as lim q  →  1Aq - 1q  =  1, so that  lim q  →  1Kq  =  2, as we would expect from the de Bruijn identity given in Lemma 2.2 of Johnson and Suhov [\cite=johnson3].

We now evaluate the derivative of the Rényi entropy, extending the de Bruijn identity:

[formula]

where we make the following definitions:

Given probability density p, define the q-score function

[formula]

and the q-Fisher information matrix to be

[formula]

Note that the numerator is the case p = 2, λ = q of the (p,λ) Fisher information introduced in Equation (7) of [\cite=lutwak2]. We establish a multi-dimensional Cramér-Rao inequality:

For the Fisher information [formula] defined above, given a random variable with density p and covariance [formula] then

[formula]

is positive definite, with equality if and only if [formula] everywhere.

The key is a Stein-like identity, as usual found using integration by parts, since

[formula]

This means that for any real c, the positive definite matrix

[formula]

So we choose [formula], and the result follows. Note that equality holds if and only if [formula] everywhere, since the Rényi maximiser has score function [formula], and [formula].

Now, we can give the extensivity property for Fisher information defined in this way:

For a compound system of independent random vectors [formula] and [formula], for q  >  1 / 2 the q-Fisher information satisfies:

[formula]

where constant [formula] and [formula] similarly.

We write [formula], so that (omitting the arguments for clarity), we can express

[formula]

Then

[formula]

since for q  >  1 / 2, the off-diagonal term

[formula]

since this is a perfect derivative, and since [formula] as [formula]. The result follows since

[formula]

Proofs

Maximum entropy property

In this section we give a proof of Proposition [\ref=prop:renmax], which shows that [formula] are the Rényi entropy maximisers. The proof uses Lemma 1 of Lutwak, Yang and Zhang [\cite=lutwak2], which extends the classical Gibbs inequality, and is equivalent to Lemma [\ref=lem:gibbs] below.

For q  ≠  1, given n-dimensional probability densities f and g, define the relative q-Rényi entropy distance from f to g to be

[formula]

For q = 1, we write [formula] for the standard relative entropy. We justify this as an extension by continuity; as q  →  1, as in ([\ref=eq:entcont]), [formula].

For any q  >  0, and for any probability densities f and g, the relative entropy [formula], with equality if and only if f  =  g almost everywhere.

The case q = 1 is well-known. For q  ≠  1, as in Lutwak, Yang and Zhang [\cite=lutwak2], the result is a direct application of Hölder's inequality to [formula]. Although [\cite=lutwak2] only strictly speaking considers the 1-dimensional case, the general case is precisely the same.

As with the Shannon maximisers, we use this Gibbs inequality Lemma [\ref=lem:gibbs] to show that the densities of Definition [\ref=def:renmax] really do maximise the Rényi entropy.

Since f and [formula] have the same covariance matrix,

[formula]

This means that for q  ≠  1

[formula]

For q = 1, the equivalent of the orthogonality property Equation ([\ref=eq:ortho]) is the well-known fact that

[formula]

Using Equation ([\ref=eq:ortho]) we simply evaluate

[formula]

and so the result follows by Lemma [\ref=lem:gibbs].

Note that this is an alternative proof to that given by Costa, Hero and Vignat [\cite=costa2], who introduced a non-symmetric directed divergence measure

[formula]

The approach of [\cite=costa2] is similar to that used by Cover and Thomas [\cite=cover] in the Gaussian case. The general theory of directed divergence measures is discussed by Csiszar [\cite=csiszar7] and by Ali and Silvey [\cite=ali].

The paper [\cite=lutwak] gives more general results concerning the maximum entropy property, in a more geometric context.

Stochastic Representation

1. By Equation ([\ref=eq:chidensity]), since we take β(q - 1)  =  1 / m in Equation ([\ref=eq:renmax]), the density of [formula] can be expressed as

[formula]

Here since m - n - 2  =  2 / (q - 1), taking [formula], so udu  =  xdx:

[formula]

and the result follows, since the constant

[formula]

since 1 - m / 2  +  1 / (q - 1)  =   - n / 2 and β(q - 1)  =  1 / m.

2. In the same way, the density of [formula] can be expressed as

[formula]

writing [formula], and using the facts that (m + n) / 2  =  1 / (1 - q) and 1 / (m - 2)  =  β(1 - q), the result follows.

3. For this choice of parameters, [formula] has density [formula].

If [formula], we can calculate the Jacobian [formula]. Then, the standard change-of-variables relation gives that, since [formula], we know that [formula] has density

[formula]

Thus, in particular, taking [formula] and [formula], we know that

[formula]

Since p  >  1, we know that [formula] has covariance [formula].

Further [formula] [formula] as required.

Note that an alternate, stochastic proof of Equation ([\ref=eq:stochastic_r]) can be deduced from the polar factorization property of Student-r vectors (see [\cite=barthe3] for a detailed study): if [formula] is orthogonally invariant and [formula] where [formula] is uniformly distributed on the sphere, then [formula] and [formula] are independent. Since [formula] is the marginal of a vector [formula] uniformly distributed on the sphere, we deduce that

[formula]

where [formula] is a Gaussian vector, and where random variable [formula] is chi distributed with m degrees of freedom and independent of [formula]. Thus, multiplying [formula] by an independent chi-distributed random variable with m degrees of freedom yields a Gaussian vector with covariance matrix [formula], which is exactly Equation ([\ref=eq:stochastic_r]).

Projection results

To prove the Entropy Power Inequality, Theorem [\ref=thm:epi], we prove a technical result, Proposition [\ref=prop:mapgood]. This relies on two well-known results, Lemma [\ref=lem:chainrule] and Lemma [\ref=lem:proj]. Firstly as a consequence of the chain rule for relative entropy (see for example Theorem 2.5.3 of Cover and Thomas [\cite=cover]):

For pairs of random variables (X,Y) and (U,V),

[formula]

Equality holds if and only if for each x, the random variables Y|X  =  x and V|U = x have the same distribution. In particular if (X,Y) and (U,V) are independent pairs, equality holds if and only if Y and V have the same distribution.

Secondly, we recall a projection identity, first stated as Corollary 4.1 of [\cite=kullback2]:

For random vectors [formula] and [formula], and for any invertible function Φ:

[formula]

For a n-dimensional random vector [formula], take N  ~  χ2q / (q - 1) and U  ~  χ2q / (q - 1) + n, where [formula] are independent:

[formula]

where equality holds if [formula] is [formula].

By combining Lemmas [\ref=lem:chainrule] and [\ref=lem:proj], if random variables Q and S have the same distribution and [formula] and [formula] each form independent pairs then

[formula]

Now, we define Y  ~  χ2q / (q - 1) and V  ~  χ2q / (q - 1) + n, both independent of [formula], so that U and V have the same distribution, as do N and Y. The LHS of the proposition becomes:

[formula]

and the result follows. Here Equation ([\ref=eq:step2]) follows by Equation ([\ref=eq:mult]), Equation ([\ref=eq:step3]) follows by Lemma [\ref=lem:proj] and Equation ([\ref=eq:step5]) again follows by Equation ([\ref=eq:mult]).

Acknowledgment

This work was done during a visit by CV to OTJ at the Statistical Laboratory, University of Cambridge, in April 2005. CV would like to thank OTJ and the Statistical Laboratory for their hospitality.

Addresses

Oliver Johnson, Statistical Laboratory, DPMMS, Centre for Mathematical Sciences, University of Cambridge, Wilberforce Road, Cambridge CB3 0WB, UK. Fax: +44 1223 337956. Email: .

Christophe Vignat L.I.S., 961 rue de la Houille Blanche, 38402 St. Martin d'Hères cedex, France. Email: .