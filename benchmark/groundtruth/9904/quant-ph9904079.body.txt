Theorem Proposition Lemma Corollary

Proof. [formula]

Average-Case Quantum Query Complexity

A preliminary version of this paper appeared in the Proceedings of 17th Annual Symposium on Theoretical Aspects of Computer Science (STACS'2000), Springer, LNCS 1770, 2000.

Computer Science Department University of California Berkeley CA 94720 USA ambainis@cs.berkeley.edu Ronald de Wolf

CWI P.O. Box 94079 1090 GB Amsterdam The Netherlands rdewolf@cwi.nl

Introduction

The field of quantum computation studies the power of computers based on quantum mechanical principles. So far, most quantum algorithms--and all physically implemented ones--have operated in the so-called black-box setting. In the black-box model, the input of the function f that we want to compute can only be accessed by means of queries to a "black-box". This returns the ith bit of the input when queried on i. The complexity of computing f is measured by the required number of queries. In this setting we want quantum algorithms that use significantly fewer queries than the best classical algorithms. Examples of quantum black-box algorithms that are provably better than any classical algorithm can be found in [\cite=deutsch&jozsa] [\cite=simon:power] [\cite=grover:search] [\cite=bht:collision] [\cite=bhmt:countingj] [\cite=betal:distinctness]. Even Shor's quantum algorithm for period-finding, which is the core of his efficient factoring algorithm [\cite=shor:factoring], can be viewed as a black-box algorithm [\cite=cleve:orderfinding].

We restrict our attention to computing total Boolean functions f on N variables. The query complexity of f depends on the kind of errors one allows. For example, we can distinguish between exact computation, zero-error computation (a.k.a. Las Vegas), and bounded-error computation (Monte Carlo). In each of these models, worst-case complexity is usually considered: the complexity is the number of queries required for the "hardest" input. Let D(f), R(f) and Q(f) denote the worst-case query complexity of computing f for classical deterministic algorithms, classical randomized bounded-error algorithms, and quantum bounded-error algorithms, respectively. More precise definitions will be given in the next section. Since quantum bounded-error algorithms are at least as powerful as classical bounded-error algorithms, and classical bounded-error algorithms are at least as powerful as deterministic algorithms, we have Q(f)  ≤  R(f)  ≤  D(f). The main quantum success here is Grover's algorithm [\cite=grover:search]. It can compute the OR-function with bounded-error using [formula] queries (which is optimal [\cite=bbbv:str&weak] [\cite=bbht:bounds] [\cite=zalka:grover]). Thus [formula], whereas D() = N and R()∈Θ(N). This is the biggest gap known between quantum and classical worst-case complexities for total functions. (In contrast, for partial Boolean functions the gap can be much bigger [\cite=deutsch&jozsa] [\cite=simon:power] [\cite=cleve:orderfinding].) In fact, it is known that the gap between D(f) and Q(f) is at most polynomial for every total f: D(f)∈O(Q(f)6) [\cite=bbcmw:polynomials]. This is similar to the best known relation between classical deterministic and randomized algorithms: D(f)∈O(R(f)3) [\cite=nisan:pram&dt].

Given some probability distribution μ on the set of inputs {0,1}N one may also consider average-case complexity instead of worst-case complexity. Average-case complexity concerns the expected number of queries needed when the input is distributed according to μ. If the hard inputs receive little μ-probability, then average-case complexity can be significantly smaller than worst-case complexity. Let Dμ(f), Rμ(f), and Qμ(f) denote the average-case analogues of D(f), R(f), and Q(f), respectively, to be defined more precisely in the next section. Again Qμ(f)  ≤  Rμ(f)  ≤  Dμ(f). The objective of this paper is to compare these measures and to investigate the possible gaps between them. Our main results are:

Under uniform μ, Qμ(f) and Rμ(f) can be super-exponentially smaller than Dμ(f).

Under uniform μ, Qμ(f) can be exponentially smaller than Rμ(f). Thus the polynomial relation that holds between quantum and classical query complexities in the case of worst-case complexity [\cite=bbcmw:polynomials] does not carry over to the average-case setting.

Under non-uniform μ the gap can be even larger: we give distributions μ where Qμ() is constant, whereas Rμ() is almost [formula].

For every f and μ, Rμ(f) is lower bounded by the expected block sensitivity Eμ[bs(f)] and Qμ(f) is lower bounded by Eμ[null].

For the MAJORITY-function under uniform μ, we have that [formula] and [formula]. In contrast, Rμ(f)∈Ω(N).

For the PARITY-function, the gap between Qμ and Rμ can be quadratic, but not more. Under uniform μ, PARITY has Qμ(f)∈Ω(N).

Definitions

Let f:{0,1}N  →  {0,1} be a Boolean function. This function is symmetric if f(X) only depends on |X|, the Hamming weight (the number of 1s) of X. We will in particular consider the following symmetric functions: (X) = 1 iff |X|  ≥  1; (X) = 1 iff |X| > N / 2; (X) = 1 iff |X| is odd. If X∈{0,1}N is an input and S a set of (indices of) variables, we use XS to denote the input obtained by flipping the values of the S-variables in X. The block sensitivity bsX(f) of f on an input X is the maximal number b for which there are b disjoint sets of variables [formula] such that f(X)  ≠  f(XSi) for all 1  ≤  i  ≤  b. The block sensitivity bs(f) of f is max XbsX(f).

We are interested in the question how many bits of the input have to be queried in order to compute f, either for the worst-case or average-case input. We assume familiarity with classical computation and briefly sketch the definition of quantum query algorithms. For a general introduction to quantum computing, see the book of Nielsen and Chuang [\cite=nielsen&chuang:qc]. For more details about (quantum) query complexity we refer to [\cite=buhrman&wolf:dectreesurvey].

An m-qubit state is a 2m-dimensional unit vector of complex numbers, written [formula]. The complex number αx is called the amplitude of the basis state |x〉. A T-query quantum algorithm corresponds to a unitary transformation

[formula]

Here the Uj are unitary transformations on m qubits. These Uj are independent of the input. Each O corresponds to a query to the input X∈{0,1}N, formalized as the unitary transformation

[formula]

Here [formula], b∈{0,1}, [formula] is addition modulo 2, and z∈{0,1}m -  log N - 1 is the workspace, which remains unaffected by the query. Intuitively, O just gives us the bit xi when queried on i. We will sometimes use the word "oracle" to refer to X as well as to the corresponding O. The initial state of the algorithm is the all-zero state |0m〉. The final state is A|0m〉, which depends on the input X via the T queries that are made. A measurement of a dedicated output bit of the final state will yield the output. It can be shown that this linear-algebraic quantum model is at least as strong as classical randomized computation: any classical T-query randomized algorithm can be simulated by a T-query quantum algorithm having the same error probabilities.

As described above, the quantum algorithm will make exactly T queries on every input X. Since we are interested in average-case number of queries and the required number of queries will depend on the input X, we need to allow the algorithm to give an output after fewer than T queries. We will do that by measuring, after each Uj, a dedicated flag-qubit of the intermediate state at that point (this measurement may alter the state). This bit indicates whether the algorithm is already prepared to stop and output a value. If this bit is 1, then we measure the output bit, output its value A(X)∈{0,1} and stop; if the flag-bit is 0 we let the algorithm continue with the next query O and Uj + 1. Note that the number of queries that the algorithm makes on input X is now a random variable, since it depends on the probabilistic outcome of measuring the flag-qubit after each step. We use TA(X) to denote the expected number of queries that A makes on input X. The Boolean output A(X) of the algorithm is a random variable as well.

We mainly focus on three kinds of algorithms for computing f: classical deterministic, classical randomized bounded-error, and quantum bounded-error algorithms. Let D(f) denote the set of classical deterministic algorithms that compute f. Let [formula] be the set of classical randomized algorithms that compute f with bounded error probability. The error probability 1 / 3 is not essential; it can be reduced to any small ε by running the algorithm O( log (1 / ε)) times and outputting the majority answer of those runs. Similarly we let [formula] be the set of bounded-error quantum algorithms for f. We define the following worst-case complexities:

[formula]

D(f) is also known as the decision tree complexity of f and R(f) as the bounded-error decision tree complexity of f. Since quantum computation generalizes randomized computation and randomized computation generalizes deterministic computation, we have Q(f)  ≤  R(f)  ≤  D(f)  ≤  N for all f. The three worst-case complexities are polynomially related: D(f)∈O(R(f)3) [\cite=nisan:pram&dt] and D(f)∈O(Q(f)6) [\cite=bbcmw:polynomials] for all total f.

Let μ:{0,1}N  →  [0,1] be a probability distribution. We define the average-case complexity of an algorithm A with respect to a distribution μ as:

[formula]

The average-case deterministic, randomized, and quantum complexities of f with respect to μ are

[formula]

Note that the algorithms still have to satisfy the appropriate output requirements (such as outputting f(X) with probability ≥  2 / 3 in case of Rμ or Qμ) on all inputs X, even on X that have μ(X) = 0. Clearly Qμ(f)  ≤  Rμ(f)  ≤  Dμ(f)  ≤  N for all μ and f. Our goal is to examine how large the gaps between these measures can be, in particular for the uniform distribution (X) = 2- N.

The above treatment of average-case complexity is the standard one used in average-case analysis of algorithms [\cite=vitter&flajolet:av]. One counter-intuitive consequence of these definitions, however, is that the average-case performance of polynomially related algorithms can be superpolynomially apart (we will see this happen in Section [\ref=secnonunifgap]). This seemingly paradoxical effect makes these definitions unsuitable for dealing with polynomial-time reducibilities and average-case complexity classes, which is what led Levin to his alternative definition of "polynomial time on average" [\cite=levin:av]. Nevertheless, we feel our definitions are the appropriate ones for our query complexity setting: they are just the average numbers of queries that one needs when the input is drawn according to distribution μ.

Super-Exponential Gap between [formula] and [formula]

Before comparing the power of classical and quantum computing, we first compare the power of deterministic and bounded-error algorithms. It is not hard to show that [formula] can be much larger then [formula] and [formula]:

Define f on N variables such that f(X) = 1 iff |X|  ≥  N / 10. Then [formula] and [formula] are O(1) and [formula].

Suppose we randomly sample k bits of the input. Let a = |X| / N denote the fraction of 1s in the input and ã the fraction of 1s in the sample. The Chernoff bound (see e.g. [\cite=alon&spencer:probmethod]) implies that there is a constant c > 0 such that

[formula]

Now consider the following randomized algorithm for f:

Let i = 100.

Sample ki = i / c bits. If the fraction ãi of 1s is ≥  2 / 10, then output 1 and stop.

If i <  log N, then increase i by 1 and repeat step 2.

If i  ≥   log N, then count |X| exactly using N queries and output the correct answer.

It is easy to see that this is a bounded-error algorithm for f. Let us bound its average-case complexity under the uniform distribution.

If a  ≥  3 / 10, the expected number of queries for step 2 is

[formula]

[formula]

The probability that step 4 is needed (given a  ≥  3 / 10) is at most 2- c log N / c = 1 / N. This adds [formula] to the expected number of queries.

Under the uniform distribution, the probability of the event a < 3 / 10 is at most 2- c'N for some constant c'. This case contributes at most 2- c'N(N + ( log N)2)∈o(1) to the expected number of queries. Thus in total the algorithm uses O(1) queries on average, hence [formula]. Since [formula], we also have [formula].

Since a deterministic classical algorithm for f must be correct on every input X, it is easy to see that it must make at least N / 10 queries on every input, hence [formula].

Accordingly, we can have huge gaps between [formula] and [formula]. However, this example tells us nothing about the gaps between quantum and classical bounded-error algorithms. In the next section we exhibit an f where [formula] is exponentially smaller than the classical bounded-error complexity [formula].

Exponential Gap between Runif(f) and Qunif(f)

The Function

We use the following modification of Simon's problem [\cite=simon:power]:

Input: [formula], where each xi∈{0,1}n.

Output: f(X) = 1 iff there is a non-zero k∈{0,1}n such that for all i∈{0,1}n we have [formula].

Here we treat i∈{0,1}n both as an n-bit string and as a number between 1 and 2n, and [formula] denotes bitwise XOR. Note that this function is total (unlike Simon's). Formally, f is not a Boolean function because the variables are {0,1}n-valued. However, we can replace every variable xi by n Boolean variables and then f becomes a Boolean function of N = n2n variables. The number of queries needed to compute the Boolean function is at least the number of queries needed to compute the function with {0,1}n-valued variables (because we can simulate a query to the Boolean oracle by means of a query to the {0,1}n-valued input-variables, just ignoring the n - 1 bits that we are not interested in) and at most n times the number of queries to the {0,1}n-valued oracle (because one {0,1}n-valued query can be simulated using n Boolean queries). As the numbers of queries are so closely related, it does not make a big difference whether we use the {0,1}n-valued oracle or the Boolean oracle. For simplicity we count queries to the {0,1}n-valued oracle.

We are interested in the average-case complexity of this function. The main result is the following exponential gap, to be proven in the next sections:

For f as above, Qunif(f)  ≤  22n + 1 and Runif(f)∈Ω(2n / 2).

Quantum Upper Bound

The quantum algorithm is similar to Simon's. Start with the 2-register superposition [formula] (for convenience we ignore normalizing factors). Apply the oracle once to obtain

[formula]

Measuring the second register gives some j and collapses the first register to

[formula]

A Hadamard transform H maps bits [formula]. Applying this to each qubit of the first register gives

[formula]

Here (a,b) denotes inner product mod 2; if (a,b) = 0 we say a and b are orthogonal.

If f(X) = 1, then there is a non-zero k such that [formula] for all i. In particular, xi = j iff [formula]. Then the final state ([\ref=eqnsupsimon]) can be rewritten as

[formula]

Notice that |i'〉 has non-zero amplitude only if (k,i') = 0. Hence if f(X) = 1, then measuring the final state gives some i' orthogonal to the unknown k.

To decide if f(X) = 1, we repeat the above process m = 22n times. Let [formula] be the results of the m measurements. If f(X) = 1, there must be a non-zero k that is orthogonal to all ir. Compute the subspace S  ⊆  {0,1}n that is generated by [formula] (i.e. S is the set of binary vectors obtained by taking linear combinations of [formula] over GF(2)). If S = {0,1}n, then the only k that is orthogonal to all ir is k = 0n, so then we know that f(X) = 0. If S  ≠  {0,1}n, we just query all 2n values [formula] and then compute f(X). Of course, this latter step is very expensive, but it is needed only rarely:

Assume that [formula] is chosen uniformly at random from {0,1}N. Then, with probability at least 1 - 2- n, f(X) = 0 and the measured [formula] generate {0,1}n.

It can be shown by a small modification of [\cite=alon&spencer:probmethod] that with probability at least 1 - 2- c2n (c > 0), there are at least 2n / 8 values j such that xi = j for exactly one i∈{0,1}n (and hence f(X) = 0). We assume that this is the case in the following.

If [formula] generate a proper subspace of {0,1}n, then there is a non-zero k∈{0,1}n that is orthogonal to this subspace. We estimate the probability that this happens. Consider some fixed non-zero vector k∈{0,1}n. The probability that i1 and k are orthogonal is at most [formula], as follows. With probability at least 1/8, the measurement of the second register gives j such that f(i) = j for a unique i. In this case, the measurement of the final superposition ([\ref=eqnsupsimon]) gives a uniformly random i'. The probability that a uniformly random i' has (k,i')  ≠  0 is 1/2. Therefore, the probability that (k,i1) = 0 is at most [formula].

The vectors [formula] are chosen independently. Therefore, the probability that k is orthogonal to each of them is at most [formula]. There are 2n - 1 possible non-zero k, so the probability that there is a k which is orthogonal to each of [formula], is ≤  (2n - 1)2- 2n < 2- n.

Note that this algorithm is actually a zero-error algorithm: it always outputs the correct answer. Its expected number of queries on a uniformly random input is at most m = 22n for generating [formula] and at most [formula] for querying all the xi if the first step does not give [formula] that generate {0,1}n. This completes the proof of the first part of Theorem [\ref=thuniformgap]. In contrast, in the appendix we show that the worst-case zero-error quantum complexity of f is Ω(N), which is near-maximal.

Classical Lower Bound

Let D1 be the uniform distribution over all inputs X∈{0,1}N and D2 be the uniform distribution over all X for which there is a unique k  ≠  0 such that [formula] (and hence f(X) = 1). We say an algorithm A distinguishes between D1 and D2 if the average probability that A outputs 0 is ≥  2 / 3 under D1 and the average probability that A outputs 1 is ≥  2 / 3 under D2.

If there is a bounded-error algorithm A that computes f with m = TunifA queries on average, then there is an algorithm that distinguishes between D1 and D2 and uses O(m) queries on all inputs.

Without loss of generality we assume A has error probability ≤  1 / 10. To distinguish D1 and D2, we run A until it stops or makes 10m queries. If it stops, we output the result of A. If it makes 10m queries and has not stopped yet, we output 1.

Under D1, the probability that A outputs 1 is at most 1 / 10 + o(1) (1 / 10 is the maximum probability of error on an input with f(X) = 0 and o(1) is the probability of getting an input with f(X) = 1), so the probability that A outputs 0 is at least 9 / 10 - o(1). The average probability (under D1) that A does not stop before 10m queries is at most 1 / 10, for otherwise the average number of queries would be more than [formula]. Therefore the probability under D1 that A outputs 0 after at most 10m queries, is at least (9 / 10 - o(1)) - 1 / 10 = 4 / 5 - o(1). In contrast, the D2-probability that A outputs 0 is ≤  1 / 10 because f(X) = 1 for any input X from D2. This shows that we can distinguish D1 from D2.

A classical randomized algorithm A that makes m∈o(2n / 2) queries cannot distinguish between D1 and D2.

For a random input from D1, the probability that all answers to m queries are different is

[formula]

For a random input from D2, the probability that there is an i such that A queries both xi and [formula] (k is the hidden vector) is [formula], since:

for every pair of distinct i,j, the probability that [formula] is 1 / (2n - 1)

since A queries only m of the xi, it queries only [formula] distinct pairs i,j

If no pair xi, [formula] is queried, the probability that all answers are different is

[formula]

It is easy to see that all sequences of m different answers are equally likely. Therefore, for both distributions D1 and D2, we get a uniformly random sequence of m different values with probability 1 - o(1) and something else with probability o(1). Thus A cannot "see" the difference between D1 and D2 with sufficient probability to distinguish between them.

The second part of Theorem [\ref=thuniformgap] now follows: a classical algorithm that computes f with an average number of m queries can be used to distinguish between D1 and D2 with O(m) queries (Lemma [\ref=clb1]), but then O(m)∈Ω(2n / 2) (Lemma [\ref=clb2]).

Super-Exponential Gap for Non-Uniform μ

The last section gave an exponential gap between Qμ and Rμ under uniform μ. Here we show that the gap can be even larger for non-uniform μ. Consider the average-case complexity of the OR-function. It is easy to see that Dunif(), Runif(), and Qunif() are all O(1), since the average input will have many 1s under the uniform distribution. Now we give some examples of non-uniform distributions μ where Qμ() is super-exponentially smaller than Rμ():

If α∈(0,1 / 2) and [formula] (c  ≈  1 - α is a normalizing constant), then Rμ()∈Θ(Nα) and Qμ()∈Θ(1).

Any classical algorithm for OR requires Θ(N / (|X| + 1)) queries on an input X. The upper bound follows from random sampling, the lower bound from a block-sensitivity argument [\cite=nisan:pram&dt]. Hence (omitting the intermediate Θs):

[formula]

where the last step can be shown by approximating the sum over t with an integral. Similarly, for a quantum algorithm [formula] queries are necessary and sufficient on an input X [\cite=grover:search] [\cite=bbht:bounds], so

[formula]

In particular, for α = 1 / 2 - ε we have the very large gap of O(1) quantum versus Ω(N1 / 2 - ε) classical. Note that we obtain this super-exponential gap by weighing the complexity of two algorithms (classical and quantum OR-algorithms) which are only quadratically apart on each input X. This is the phenomenon we referred to at the end of Section [\ref=secdefs].

General Bounds for Average-Case Complexity

In this section we prove some general bounds. First we make precise the intuitively obvious fact that if an algorithm A is faster on every input than another algorithm B, then it is also faster on average under any distribution:

If φ:  →   is a concave function and TA(X)  ≤  φ(TB(X)) for all X, then [formula] for every μ.

By Jensen's inequality, if φ is concave then Eμ[φ(T)]  ≤  φ(Eμ[T]), hence

[formula]

In words: taking the average cannot make the complexity-gap between two algorithms smaller. For instance, if [formula] (say, A is Grover's algorithm and B is a classical algorithm for OR), then [formula]. On the other hand, taking the average can make the gap much larger, as we saw in Theorem [\ref=thoravgap]: the quantum algorithm for OR runs only quadratically faster than any classical algorithm on each input, but the average-case gap between quantum and classical can be much bigger than quadratic.

We now prove a general lower bound on Rμ and Qμ. The classical case of the following lemma was shown in [\cite=nisan:pram&dt], the quantum case in [\cite=bbcmw:polynomials]:

Let A be a bounded-error algorithm for some function f. If A is classical then TA(X)∈Ω(bsX(f)), and if A is quantum then [formula].

A lower bound in terms of the μ-expected block sensitivity follows:

For all f, μ: Rμ(f)∈Ω(Eμ[bsX(f)]) and Qμ(f)∈Ω(Eμ[null]).

Average-Case Complexity of MAJORITY

Here we examine the average-case complexity of the MAJORITY-function. The hard inputs for majority occur when t = |X|  ≈  N / 2. Any quantum algorithm needs Ω(N) queries for such inputs [\cite=bbcmw:polynomials]. Since the uniform distribution puts most probability on the set of X with |X| close to N / 2, we might expect an Ω(N) average-case complexity as well. However, we will prove that the complexity is nearly [formula]. For this we need the following result about approximate quantum counting, which is Theorem 13 of [\cite=bhmt:countingj] (this is the upcoming journal version of [\cite=bht:counting] and [\cite=mosca:eigen]; see also [\cite=nayak&wu:median]):

There exists a quantum algorithm QCount with the following property. For every N-bit input X (with t = |X|) and number of queries T, and any integer k  ≥  1, QCount uses T queries and outputs a number t̃ such that

[formula]

with probability at least 8 / π2 if k = 1 and probability ≥  1 - 1 / 2(k - 1) if k > 1.

Using repeated applications of this quantum counting routine we can obtain a quantum algorithm for majority that is fast on average:

[formula].

For all [formula], define [formula]. The probability under the uniform distribution of getting an input X∈Ai is [formula], since the number of inputs X with k 1s is [formula] for all k. The idea of our algorithm is to have log N runs of the quantum counting algorithm, with increasing numbers of queries, such that the majority value of inputs from Ai is probably detected around the ith counting stage. We will use Ti = 100  ·  2i log N queries in the ith counting stage. Our MAJORITY-algorithm is the following:

For i = 1 to log N do: quantum count |X| using Ti queries (call the estimate t̃i) if [formula], then output whether [formula] and stop. Classically count |X| using N queries and output its majority.

Let us analyze the behavior of the algorithm on an input X∈Ai. For t = |X|, we have |t - N / 2|∈(N / 2i + 1,N / 2i]. By Theorem [\ref=thqcounting], with probability > 1 - 1 / 10 log N we have [formula], so with probability (1 - 1 / 10 log N)log N  ≈  e- 1 / 10 > 0.9 we have [formula] for all 1  ≤  i  ≤  N. This ensures that the algorithm outputs the correct value with high probability.

We now bound the expected number of queries the algorithm needs on input X. Consider the (i + 2)nd counting stage. With probability 1 - 1 / 10 log N we will have |t̃i + 2 - t|  ≤  N / 2i + 2. In this case the algorithm will terminate, because

[formula]

Thus with high probability the algorithm needs no more than i + 2 counting stages on input X. Later counting stages take exponentially more queries (Ti + 2 + j = 2jTi + 2), but are needed only with exponentially decreasing probability O(1 / 2j log N): the probability that |t̃i + 2 + j - t| > N / 2i + 2 goes down exponentially with j precisely because the number of queries goes up exponentially. Similarly, the last step of the algorithm (classical counting) is needed only with negligible probability.

Now the expected number of queries on input X can be upper bounded by

[formula]

Therefore under the uniform distribution the average expected number of queries can be upper bounded by [formula]

The nearly matching lower bound is:

[formula].

Let A be a bounded-error quantum algorithm for MAJORITY. It follows from the worst-case results of [\cite=bbcmw:polynomials] that A uses Ω(N) queries on the hardest inputs, which are the X with |X| = N / 2  ±  1. Since the uniform distribution puts [formula] probability on the set of such X, the average-case complexity of A is at least [formula].

What about the classical average-case complexity of MAJORITY? Alonso, Reingold, and Schott [\cite=ars:avmaj] prove the bound [formula] for deterministic classical computers. We can also prove a linear lower bound for the bounded-error classical complexity, using the following lemma:

Let [formula]. Any classical bounded-error algorithm that computes MAJORITY on inputs X with |X|∈{N / 2,N / 2 + Δ} must make Ω(N) queries on all such inputs.

We will prove the lemma for [formula], which is the hardest case. We assume without loss of generality that the algorithm queries its input X at T(X) random positions, and outputs 1 if the fraction of 1s in its sample is at least [formula]. We do not care what the algorithm outputs otherwise. Consider an input X with |X| = N / 2. The algorithm uses T = T(X) queries and should output 0 with probability at least 2 / 3. Thus the probability of output 1 on X must be at most 1 / 3, in particular

[formula]

Since the T queries of the algorithm can be viewed as sampling without replacement from a set containing N / 2 1s and N / 2 0s, this error probability is given by the hypergeometric distribution

[formula]

We can approximate the hypergeometric distribution using the normal distribution, see e.g. [\cite=nicholson:hyper]. Let [formula] and [formula], then the above probability approaches

[formula]

Note that [formula] and that [formula] if T∈o(N). Thus we can only avoid having an error probability close to 1/2 by using T∈Ω(N) queries on X with |X| = N / 2. A similar argument shows that we must also use Ω(N) queries if |X| = N / 2 + Δ.

It now follows that:

[formula].

The previous lemma shows that any algorithm for MAJORITY needs Ω(N) queries on inputs X with |X|∈[null]. Since the uniform distribution puts Ω(1) probability on the set of such X, the theorem follows.

Accordingly, on average a quantum computer can compute MAJORITY almost quadratically faster than a classical computer, whereas for the worst-case input quantum and classical computers are about equally fast (or slow).

Average-Case Complexity of PARITY

Finally we prove some results for the average-case complexity of PARITY. This is in many ways the hardest Boolean function. Firstly, bsX(f) = N for all X, hence by Theorem [\ref=thavgbsbound]:

For every μ, Rμ()∈Ω(N) and [formula].

With high probability we can obtain an exact count of |X|, using [formula] quantum queries [\cite=bhmt:countingj]. Combining this with a μ that puts [formula] probability on the set of all X with |X| > 1 and distributes the remaining probability arbitrarily over the X with |X|  ≤  1, we obtain a distribution μ such that [formula].

We can prove Qμ()  ≤  N / 6 for any μ by the following algorithm: with probability 1 / 3 output 1, with probability 1 / 3 output 0, and with probability 1 / 3 run the exact quantum algorithm for PARITY, which has worst-case complexity N / 2 [\cite=bbcmw:polynomials] [\cite=fggs:parity]. This algorithm has success probability 2 / 3 on every input and has expected number of queries equal to N / 6.

More than a linear speed-up on average is not possible if μ is uniform:

[formula].

Let A be a bounded-error quantum algorithm for PARITY. Let B be an algorithm that flips each bit of its input X with probability 1 / 2, records the number b of actual bitflips, runs A on the changed input Y, and outputs A(Y) + b2. It is easy to see that B is a bounded-error algorithm for PARITY and that it uses an expected number of TμA queries on every input. Using standard techniques, we can turn this into an algorithm for PARITY with worst-case O(TμA) queries. Since the worst-case lower bound for PARITY is N / 2 [\cite=bbcmw:polynomials] [\cite=fggs:parity], the theorem follows.

Acknowledgments

We thank Harry Buhrman for suggesting this topic, and him, Lance Fortnow, Lane Hemaspaandra, Hein Röhrig, Alain Tapp, and Umesh Vazirani for helpful discussions. Also thanks to Alain for sending a draft of [\cite=bhmt:countingj].

Worst-case Complexity of f

In this appendix we will show a lower bound of Ω(N) queries for the zero-error worst-case complexity Q0(f) of the function f on N = n2n binary variables defined in Section [\ref=secgapunif]. (We count binary queries this time.) Consider a quantum algorithm that makes at most T queries and that, for every X, outputs either the correct output f(X) or, with probability ≤  1 / 2, outputs "inconclusive". We use the following lemma from [\cite=bbcmw:polynomials]:

The probability that a T-query quantum algorithm outputs 1 can be written as a multilinear N-variate polynomial P(X) of degree at most 2T.

Consider the polynomial P induced by our T-query algorithm for f. It has the following properties:

P has degree d  ≤  2T

if f(X) = 0 then P(X) = 0

if f(X) = 1 then P(X)∈[1  /  2,1]

We first show that only very few inputs X∈{0,1}N make f(X) = 1. The number of such 1-inputs for f is the number of ways to choose k∈{0,1}n  -  {0n}, times the number of ways to choose 2n / 2 independent xi∈{0,1}n, which is (2n - 1)  ·  (2n)2n / 2 < 2n(2n / 2 + 1). Accordingly, the fraction of 1-inputs among all 2N inputs X is < 2n(2n / 2 + 1) / 2n2n = 2- n(2n / 2 - 1). These X are exactly the X that make P(X)  ≠  0. However, the following result is known [\cite=schwartz:probabilistic] [\cite=nisan&szegedy:degree]:

If P is a non-constant N-variate multilinear polynomial of degree d, then

[formula]

This implies d  ≥  n(2n / 2 - 1) and hence T  ≥  d / 2  ≥  n(2n / 4 - 2)  ≈  N / 4. Thus we have proved that the worst-case zero-error quantum complexity of f is near-maximal:

Q0(f)∈Ω(N).