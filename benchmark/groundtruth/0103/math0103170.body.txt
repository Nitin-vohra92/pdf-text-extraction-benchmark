Minimizing Polynomial Functions

Lemma Corollary Proposition Conjecture Algorithm Definition Remark Example Note

Introduction

This is an expository and experimental paper concerned with the following basic problem. Given a multivariate polynomial function [formula] which is bounded below on [formula], find the global minimum f* and a point p* attaining it:

[formula]

Exact algebraic algorithms for this task find all the critical points and then identifying the smallest value of f at any critical point. Such methods will be discussed in Section 2. The techniques include Gröbner bases, resultants, eigenvalues of companion matrices [\cite=clo], and numerical homotopy methods [\cite=li], [\cite=ver].

An entirely different approach was introduced by N.Z. Shor ([\cite=shor], [\cite=ss]) and further developed in the dissertation of the first author [\cite=pablo]. The idea is to compute the largest real number λ such that f(x)  -  λ is a sum of squares in [formula]. Clearly, λ is a lower bound for the optimal value f*. We show in Section 3 that, when the degree of f is fixed, the lower bound λ can be computed in polynomial time using semidefinite programming [\cite=VaB:96]. If λ  =  f* holds then this is certified by semidefinite programming duality, and the certificate yields the optimal point p*. In our computational experiments, to be presented in Section 5, we found that λ  =  f* almost always holds, and we solved problems up to n = 15.

The objective of this article is to provide a bridge between mathematical programming and algebraic geometry, demonstrating that algorithms from the former have the potential to play a major role in future algorithms in the latter. This will be underlined in Section 6, where we present open problems, and in Section 7 where we show that semidefinite programming in conjunction with the Positivstellensatz is applicable to a wide range of computational problems in real algebraic geometry.

Computational Algebra

In this section we discuss the following approach to our problem ([\ref=DefProb]). Form the partial derivatives of the given polynomial f and consider the ideal they generate:

[formula]

The zeros of the ideal I in complex n-space [formula] are the critical points of f. Their number (counting multiplicity) is the dimension over [formula] of the residue ring:

[formula]

We shall assume that μ is finite. (If μ  =    +    ∞   then one can apply perturbation techniques to reduce to the case μ  <    +    ∞  ). For instance, if f is a dense polynomial of even degree 2d then it follows from Bézout's Theorem that   μ    =    (2d - 1)n.

Consider the subset of real critical points:

[formula]

This set is usually much smaller than the set of all complex critical points, i.e., typically we have ν  ≪  μ. If we know the set [formula], then our problem is solved.

The optimal value is attained at a critical point:

[formula]

The three techniques to be described in this section all compute the set [formula] of real critical points. We will illustrate then for the following example:

[formula]

The optimal value for this problem is   f*    =     - 2.112913882, and, disregarding symmetry, there are three optimal points (x*,y*,z*) attaining this value:

[formula]

Gröbner bases and eigenvalues

We review the method of solving polynomial equations by means of Gröbner bases and eigenvalues [\cite=clo]. We are free to choose an arbitrary term order [formula] on the polynomial ring [formula]. Let [formula] be a Gröbner basis for the critical ideal I with respect to [formula]. While computing Gröbner bases is a time-consuming task in general, this is not an issue in this paper, since in all our examples the n given generators   ∂f  /  ∂xi   already form a Gröbner basis in the total degree order. In our example the Gröbner basis is

[formula]

A monomial [formula] is standard if it is not divisible by the leading term of any element in the Gröbner basis [formula]. The set [formula] of standard monomials is an [formula]-basis for the residue ring [formula]. The standard monomials for ([\ref=GrobnerBasis]) are:

[formula]

For any polynomial [formula] consider the [formula]-linear endomorphism:

[formula]

This endomorphism is represented in the basis [formula] by a real μ  ×  μ-matrix Tg. The entry of Tg with row index [formula] and column index [formula] is the coefficient of [formula] in the normal form of [formula] with respect to [formula].

The optimal value f* is the smallest real eigenvalue of the matrix Tf. Any eigenvector of Tf with eigenvalue f* defines an optimal point [formula] by the eigenvector identities   Txi  ·  v  =  pi  ·  v   for [formula].

This follows from Lemma [\ref=critical] and Theorem (4.5) in the book of Cox-Little-O'Shea [\cite=clo]; see also [\cite=clo].

The resulting algorithm is to compute symbolically the matrices Tf and Txi for [formula], then compute numerically its eigenvalues (and matching eigenvectors) of Tf, and finally determine f* and p* as in the proposition.

In our example the matrix Tf has format 27  ×  27 with rows and columns indexed by ([\ref=exampleB]). Of its 729 entries only 178 are nonzero. For instance, the column indexed by xyz has four nonzero entries, namely, the coefficients of

[formula]

The matrix Tf has maximal rank 27. Of its eigenvalues only three are real:

[formula]

The three real eigenvalues have multiplicity 3,1, and 3 respectively.

Resultants and discriminants

One algebraic method for solving polynomial equations is to use resultants. Closely related to resultants are discriminants. They express the condition on a hypersurface to have a singularity, by means of a polynomial in the coefficients its defining equation. Let t be a new indeterminate and form the discriminant of the polynomial   f(x) - t   with respect to [formula]:

[formula]

Here Δx refers to the A-discriminant, defined in [\cite=gkzbook], where A is the support of f together with the origin. From [\cite=gkzbook] we conclude that the discriminant δ(t) equals the characteristic polynomial of the matrix Tf.

The optimal value f* is the smallest real root of δ(t).

In our example, δ(t) is 256t3 - 512t2 - 96t + 473 times the third power of

[formula]

The optimal value f*  =   - 2.11... is a root of this sextic. This sextic has Galois group S6, so f* cannot be expressed in radicals over the rationals.

The suggested algorithm is to compute δ(t), and minimal polynomials for the coordinates x*i of the optimal point, by elimination of variables using matrix formulas for resultants and discriminants [\cite=gkzbook]. The subsequent numerical computation is to find the roots of a univariate polynomial.

Homotopy methods

The critical equations form a square system: n equations in n variables having finitely many roots. Such a system is well-suited for numerical homotopy continuation methods. For an introduction to this subject see the papers of Li [\cite=li] and Verschelde [\cite=ver]. The basic idea is to introduce a deformation parameter τ into the given system. For instance, we might replace ([\ref=GrobnerBasis]) by the following system which depends on a complex parameter τ:

[formula]

The solutions [formula] are algebraic functions of τ. Our goal is to find them for τ  =  1. It is easy to find the solutions for τ  =  0:

[formula]

Homotopy methods trace the full set of solutions from τ  =  0 to τ  =  1 along a suitable path in the complex τ-plane. We determine f* by evaluating the objective function f(x,y,z) at all the real solutions for τ  =  1.

Homotopy methods are frequently set up so that the system at τ  =  0 breaks up into several systems, each of which consists of binomials. If the input polynomials are sparse, then these are the polyhedral homotopies which take the Newton polytopes of the given equations into consideration. In the sparse case, the number μ will be the mixed volume of the Newton polytopes. For an introduction to these polyhedral methods see [\cite=clo] and the references given there.

How large is the Bézout number ?

A common feature of all three algebraic algorithms in this section is that their running time is controlled by the number μ of complex critical points. In the eigenvalue method we must perform linear algebra on matrices of size μ  ×  μ, in the discriminant method we must find and solve a univariate polynomial of degree μ, and in the homotopy method, we are forced to trace μ paths from τ  =  0 to τ  =  1. Each of these three methods becomes infeasible if the number μ is too big; for instance,   μ  ≥  10,000   might be too big.

Suppose that the given polynomial f in [formula] has even degree 2d and is dense. This will be the case in the family of examples studied in Section 5. Then μ coincides with the Bézout number (2d - 1)n. Some small values for the Bézout number are listed in Table [\ref=tab:matsiz]. Most entries in this table are bigger than 10,000. We are led to believe that the algebraic methods will be infeasible for quartics if n  ≥  8.

Each entry in the first row of Table [\ref=tab:matsiz] is a one. This means we can minimize quadratic polynomial functions by solving a system of linear equations (in polynomial time). The punchline of this paper is to reduce our problem to a semidefinite programming problem which can also be solved in polynomial time for fixed d.

Sums of Squares and Semidefinite Programming

We present the method introduced by N.Z. Shor ([\cite=shor], [\cite=ss]), and further extended by the first author [\cite=pablo], for minimizing polynomial functions. This method is a relaxation: it always produces a lower bound for the value of f*. However, as we shall see in Section 5, this bound very frequently agrees with f*.

We may assume that the given polynomial [formula] has even degree 2d. Let X denote the column vector whose entries are all the monomials in [formula] of degree at most d. The length of the vector X equals the binomial coefficient

[formula]

Let f denote the set of all real symmetric N  ×  N-matrices A such that [formula]. This is an affine subspace in the space of real symmetric N  ×  N-matrices. Assume that the constant monomial 1 is the first entry of X. Let E11 denote the matrix unit whose only nonzero entry is a one in the upper left corner.

For any real number λ, the following two are equivalent:

The polynomial [formula] is a sum of squares in [formula].

There is a matrix A∈f such that   A  -  λ  ·  E11   is positive semidefinite, that is, all eigenvalues of this symmetric matrix are non-negative reals.

The matrix   A  -  λ  ·  E11   is positive semidefinite if and only if there exists a real Cholesky factorization   A  -  λ  ·  E11    =    BT  ·  B. If this holds then

[formula]

is a sum of squares, and every sum of squares representation arises in this way.

We write fsos for the largest real number λ for which the two equivalent conditions are satisfied. We always have   f*  ≥  fsos. This inequality may be strict. It is even possible that   fsos  =    -    ∞  . An example of this form is Motzkin's polynomial

[formula]

It satisfies m(x,y)  ≥   - 1, but m(x,y)  -  λ is not a sum of squares for any [formula]. We refer to [\cite=Reznick] for an excellent survey of the problem of representing a polynomial as a sum of squares, and the important role played by Motzkin's example.

Sums of squares are crucial for us because of the following complexity result.

Fix  deg(f)  =  2d and let the number of variables n vary. Then there exists a polynomial-time algorithm, based on semidefinite programming, for computing fsos from f. The same statement holds if n is fixed and d varies.

Semidefinite programming (SDP) is the study of optimization problems over the cone of all positive semidefinite matrices. This branch of optimization has received a lot of attention in recent years, both for its theoretical elegance and its practical applications. Semidefinite programs can be solved in polynomial time, using interior point methods; see [\cite=NN], [\cite=HandSDP], [\cite=VaB:96]. This complexity result (together with Lemma [\ref=lemma1]) implies Theorem [\ref=polytime] because the quantity [formula] grows polynomially if either n or d is fixed. This result appears in [\cite=pablo].

Available implementations of interior-point methods for semidefinite programming perform extremely well in practice, say, for problems involving matrices up to 500 rows and columns (provided there are not too many variables). This allows for the efficient computation of fsos, and as we shall see in Section 4, SDP duality furnishes a polynomial-time test to check whether f*  =  fsos and for computing the optimal point p* in the affirmative case. A comparison of Tables [\ref=tab:matsiz] and [\ref=binocoeff] suggests that SDP has the potential to compute much larger instances than algebraic methods. Section 5 will show that this is indeed the case.

Our example ([\ref=RunningEx]) has parameters d = 2,n = 3. The affine space f consists of all 10  ×  10-matrices [formula] with λ  =  0 and [formula] arbitrary in the family

[formula]

The rows and columns of this matrix are indexed by the entries of the vector

[formula]

We invite the reader to check the identity

[formula]

The lower bound fsos is the largest real number λ such that, for some choice of [formula], the matrix [formula] has all eigenvalues nonnegative. We find that

[formula]

and the optimal matrix (to five digits) is given by:

[formula]

This matrix is positive semidefinite. By computing a factorization   BT  ·  B   as in the proof of Lemma [\ref=lemma1], we can express f - fsos as a sum of squares. In the next section we show how to recover the points at which the optimal value is achieved.

Note that the number 20 of free parameters is the case "n = 3,d = 2" of:

The dimension of Lf equals the number of linearly independent quadratic relations among the monomials of degree ≤  d in n variables. It equals

[formula]

The codimension (with respect to the space of symmetric matrices) is equal to

[formula]

Semidefinite Programming Duality

In Section 3 we demonstrated that computing fsos is equivalent to minimizing a linear functional over the intersection of the affine space Lf with the cone of positive semidefinite N  ×  N-matrices. In our discussion we have represented the space Lf by a spanning set of matrices. For numerical efficiency reasons it is usually preferable to represent Lf by its defining equations (unless n and d are very small).

Duality is a crucial feature of semidefinite programming. It plays an important role in designing the most efficient interior-point algorithms. In what follows we review the textbook formulation of SDP duality, in terms of matrices. Thereafter we present a reformulation in algebraic geometry language, and we then explain how to test the condition fsos  =  f* and how to recover the optimal point p*.

Matrix Formulation

Let SN denote the real vector space of symmetric N  ×  N-matrices, with the inner product [formula], and the Löwner partial order given by [formula] if B - A is positive semidefinite. Recall that A∈SN is positive semidefinite if xTAx  ≥  0, for all [formula]. This condition is equivalent to nonnegativity of all eigenvalues of A, and to nonnegativity of all principal minors.

The general SDP problem ([\cite=VaB:96], [\cite=HandSDP]) can be expressed in the form:

[formula]

where X,F∈SN, [formula], and [formula] is a linear operator. This is usually called the primal form, in analogy with the linear programming (LP) case.

Notice that ([\ref=eq:primalsdp]) is a convex optimization problem, since the objective function is linear and the feasible set is convex. There is an associated dual problem:

[formula]

where [formula] and [formula] is the operator adjoint to G. Any feasible solution of the dual problem is a lower bound of the optimal value of the primal:

[formula]

The last inequality holds since the inner product of two positive semidefinite matrices is nonnegative. The converse statement (primal feasible solutions give upper bounds on the optimal dual value) is obviously also true. The inequality above is called weak duality. Under certain conditions (notably, the existence of strictly feasible solutions), strong duality also holds: the optimal values of the primal and the dual problems coincide. If strong duality holds, then at optimality the matrix   X  ·  (F  -  G*y)   is zero, since [formula] implies AB = 0. This can be interpreted as a generalization of the usual complementary slackness LP conditions.

Practical implementations of SDP (we will use SeDuMi [\cite=sedumi]) simultaneously compute both the optimal matrix X for ([\ref=eq:primalsdp]) and the optimal vector y for ([\ref=eq:dualsdp]).

Polynomial Formulation

We set m  =  N - 1 and we identify SN with the real vector space [formula] of quadratic forms in m + 1 variables. The vector space dual to SN is now denoted [formula]. The dual pairing is given by differentiation and is denoted [formula]. For any [formula] and any real vector [formula], the following familiar identity holds:

[formula]

We consider the general quadratic programming problem:

[formula]

where [formula] are given and we are looking for an optimal point [formula]. This problem can be relaxed to the following primal SDP:

[formula]

The inequality [formula] means that q is non-negative on [formula], i.e., q is in the positive semidefinite cone in [formula]. In view of ([\ref=pointEval]), the optimal value of ([\ref=QP]) is greater than or equal to the optimal value of the primal SDP, and equality holds if and only if there is an optimal solution of the form [formula].

Every semidefinite programming problem comes with a dual problem, as in the previous subsection; see also [\cite=VaB:96]. In our case the dual SDP takes the form:

[formula]

Assuming the existence of a strictly feasible primal solution, the maximum value in the dual SDP is always equal to the minimum value in the primal SDP. Under this regularity assumption, which is easy to satisfy in our application, we conclude:

If the primal SDP has an optimal solution of the form [formula] then the vector [formula] is an optimal solution for ([\ref=QP]).

Minimizing Quadratic Functions over Toric Varieties

A toric variety is an algebraic variety, in affine space or projective space, which has a parametric representation by monomials. Equivalently, a toric variety is an irreducible variety which is cut out by binomial equations, that is, differences of monomials. Here we will be interested in those projective toric varieties which are defined by quadratic binomials. This class includes many examples from classical algebraic geometry, such as Veronese and Segre varieties. See [\cite=Stbook] for an introduction.

Let X be a toric variety in projective m-space whose defining prime ideal is generated by quadratic binomials [formula] in [formula]. Each generator has the form   ∂i∂j  -  ∂k∂l   for some [formula]. We set   g0(∂)  =  ∂20. Then the equation   g0(∂)  =  1   on X defines an affine toric variety [formula], such that X is the projective closure of [formula]. Every quadratic polynomial function on the affine variety [formula] is represented by a quadratic form [formula] as above. This representation is unique modulo the [formula]-linear span of [formula]. Our problem ([\ref=QP]) is hence equivalent to minimizing a quadratic function over an affine toric variety defined by quadrics:

[formula]

The optimal value of the dual SDP relaxation in Subsection 4.2 is the largest real number λ such that f  -  λ is a sum of squares in the coordinate ring of [formula].

Let us now return to our original problem ([\ref=DefProb]) where the given polynomial is dense of degree 2d in n variables. Here X is the Veronese variety in projective N-dimensional space which is parameterized by all monomials of degree at most d. (If the polynomial in ([\ref=DefProb]) is sparse then another toric variety can be used.) Writing our given polynomial as a quadratic form in homogeneous coordinates on X, our minimization problem ([\ref=DefProb]) is precisely the quadratic toric problem ([\ref=toricQP]).

We solve ([\ref=toricQP]) by simultaneously solving the primal and dual SDP relaxation in Subsection 4.2. If the optimal value λ of the dual SDP agrees with the true minimum of f over [formula] then the primal SDP has an optimal solution [formula] which exhibits an optimal point [formula] at which f is minimized.

In our running example, we have m  =  9 and r = 20, and X is the quadratic Veronese three-fold in projective 9-space which is given parametrically as

[formula]

It is cut out by twenty quadratic binomials such as   x0x5  -  x1x2. These binomials correspond to the parameters ci in the 10  ×  10-matrix [formula] in Section 2.

Experimental Results

We now present our computational experience with Shor's relaxation for global minimization of polynomial functions. As mentioned earlier, the computational advantages of our method are based on the following three independent facts:

The dimension N of the matrix required in the sum of squares formulation is much smaller than the Bézout number μ, since it only scales polynomially with the number of variables. See Tables [\ref=tab:matsiz] and [\ref=binocoeff] above.

Semidefinite programming provides an efficient algorithm for deciding whether a polynomial is a sum of squares, and to find such representations for polynomials whose coefficients may depend linearly on parameters.

The lower bound fsos very often coincides with the exact solution f* of our problem ([\ref=DefProb]), at least for the class of problems analyzed here.

The experimental results in this section strongly support the validity of these facts.

The test problems

For our computations, we fix a positive integer K, and we sample from the following family of polynomials of degree 2d in n variables:

[formula]

where [formula] is a random polynomial of total degree ≤  2d - 1 whose [formula] coefficients are independently and uniformly distributed among integers between - K and K. Thus our family depends on three parameters: n, d and K.

This family has been selected to ensure three important properties:

An important question is if the structure of the polynomials ([\ref=eq:polyfam]) is somehow "biased" towards the application of sum of squares methods. This is a relevant issue, since the performance of algorithms on "random instances" sometimes provides more information on the problem family, rather than on the algorithm itself. Concerning this question, we limit ourselves to notice that, for K sufficiently large, the family ([\ref=eq:polyfam]) does include polynomials f with fsos  <  f*. A simple example is f(x,y)  =  x8  +  y8  +  2700  m(x,y), where m(x,y) is the Motzkin polynomial ([\ref=eq:motzkin]).

The polynomials in our family have global minima that generally have large negative values, of the order of - K2d. This leads to ill-conditioning of the symmetric matrices described in Lemma [\ref=lemma1], and hence to numerical problems for the interior-point algorithm. Our remedy is a simple homogeneous scaling of the form

[formula]

Obviously, this does not affect the properties of being a sum of squares, or whether f* = fsos. However, as is generally the rule in numerical optimization, this scaling step greatly affects both the speed and the accuracy of the SDP solution.

Algorithms and software

Most of the test examples were run on a Pentium III 733Mhz with 256 MB, running Linux version 2.2.16-3, and using MATLAB version 5.3. Because of physical memory limitations, our largest examples (quartics in fifteen variables), were run on a Pentium III 650Mhz with 320 MB, under Windows 2000. The semidefinite programs were solved using the SDP solver SeDuMi [\cite=sedumi], written by Jos Sturm. It is currently one of the most efficient codes available, at least for the restricted class of problems relevant here. SeDuMi can be run from within MATLAB, and implements a self-dual embedding technique. The default parameters are used, and the solutions computed are typically exact to machine precision (SeDuMi provides an estimate of the quality of the solution).

The MATLAB Optimization toolbox was used for the implementation of a local search approach, to be described in Section [\ref=sec:local]. For the numerical homotopy method, we used the software PHCpack [\cite=PHC], written by Jan Verschelde. The computation of the sparse matrix Tf was done using Macaulay 2 [\cite=M2], and its eigenvalues were numerically computed using MATLAB.

We do not make strong claims about the efficiency of our implementations: while reasonable fast, for large scale problems considerable speedups are possible at the expense of customized algorithms. Nevertheless, we believe that the issues raised regarding the applicability of algebra-based techniques to problems with large Bézout number remain valid, independently of the particular software employed.

Standard local optimization

An alternative approach to the problem is given by traditional (nonconvex) numerical optimization. There exist many variations, but arguably the most successful methods for relatively small problems such as the present ones are based on local gradient and Hessian information. Typical algorithms in this class employ an iterative scheme, combining the Newton search direction in combination with a line search [\cite=Nocedal]. These methods are reasonably fast in converging to a local minimum. For the larger problems in our family, they usually converge to a stationary point within 10 seconds. However, they often end up in the wrong solution, unless a very accurate starting point is given.

The drawbacks of local optimization methods are well-known: lacking convexity, there are no guarantees of global (or even local) optimality. Worse, even if in the course of the optimization we actually reach the global minimum, there is usually no computationally feasible way of verifying optimality.

Nevertheless, local optimization is an important tool for polynomial problems, as is the use of homotopy methods to trace the optimal value under small changes in the input data. It would interesting to investigate how these local numerical techniques can be best combined with the computations to be described next.

Experimental results using computational algebra

In Table [\ref=tab:phctimes] we present typical running times for the homotopy based approach, described in Section [\ref=sec:hom]. These were obtained running PHCpack in "black-box" mode (phc -b), that requires no user-specified parameters. The software traces all solutions (not necessarily real), its number being equal to the Bézout number. Comparing with Table [\ref=tab:matsiz], we can notice the adverse effect of large Bézout numbers in the practical performance of the algorithm, in spite of Verschelde's impressive implementation.

For the eigenvalue approach outlined in Section [\ref=sec:eigs], we compute the matrix Tf using a straightforward implementation in Macaulay 2: the endomorphism Timesf is constructed, and applied to the elements of the monomial basis B. The resulting matrix, in a sparse floating point representation, is sent to a file for further processing. We found that the construction of the matrix Tf takes a surprisingly long time. for instance, it took Macaulay 2 over 10 minutes to produce the 125  ×  125-matrix for 2d  =  6, n  =  3. The eigenvalue problem itself is solved using MATLAB; it exploits the sparsity of the matrix, and runs reasonably fast. However, it appears that even a more efficient implementation of this method will not be able to compete with the timings in Table [\ref=tab:phctimes], let alone the timings in Table [\ref=tab:sdptimes].

After several discouraging attempts for small examples, we did not pursue a full implementation for the resultant-based methods sketched in Section 2.2.

Experimental results using semidefinite programming

We ran several instances of polynomials in the family described above, for values of K equal to 100, 1000, and 10000. In Table [\ref=tab:sdptimes] the typical running times for the semidefinite programming based approach on a single instance are presented. These are fairly constant across instances, and no special structure is exploited (besides what SeDuMi does internally).

The number of random instances for each combination of the parameters is shown in Table [\ref=tab:numberofinstances]. These values were chosen in order to keep the total computation time for a given category in the order of a few hours.

Regarding the accuracy of the relaxation, in all cases tested the condition fsos = f* was satisfied. As explained in the previous section, this can be numerically verified by checking if the solution of the corresponding SDP has rank one, from which a candidate global minimizer is obtained. Evaluating the polynomial at this point provides an upper bound on the optimal value, that can be compared with the lower bound fsos. In all our instances, the difference between these two quantities was extremely small, and within the range of numerical error.

As an additional check, when we used different methods for solving the same instance, we have verified the solutions against each other. As expected, the solutions were numerically close, in many cases up to machine precision.

In particular, it is noted that the approach can handle in a reasonable time (less than 35 min.) the case of a quartic polynomial in thirteen variables. Our largest examples have the same degree (2d = 4) and fifteen variables, correspond to an SDP with a matrix of dimensions 136  ×  136 with 3876 auxiliary variables, and can be solved in a few hours. A quick glance at the corresponding Bézout number in Table [\ref=tab:matsiz] makes clear the advantages of the presented approach.

What Next ?

We have demonstrated that the sums of squares relaxation is a powerful and practical technique in polynomial optimization. There are many open questions, both algorithmic and mathematical, which are raised by our experimental results. One obvious question is how often does it occur that fsos  =  f* ? This can be studied for our simple model ([\ref=eq:polyfam]), or, perhaps better, for various natural probability measures on the space of polynomials of bounded degree. This question is closely related to understanding the inclusion of the convex cone of forms that are sums of squares inside the cone of positive semidefinite forms. For the three-dimensional family of symmetric sextics, this problem was studied in detail by Choi, Lam and Reznick [\cite=lam]. Their work is an inspiration, but it also provides a warning as to how difficult the general case will be, even for ternary sextics without symmetry.

We hope to pursue some of the following directions of inquiry in the near future.

Sparseness and Symmetry

Most polynomial systems one encounters are sparse in the sense that there are only few monomials with nonzero coefficients. Methods involving Newton polytopes, such as sparse resultants [\cite=gkzbook] and polyhedral homotopies [\cite=ver], are designed to deal with such problems. Symmetry with respect to finite matrix groups is another feature of many polynomial problems arising in practise. The book of Gatermann [\cite=gat] is an excellent first reference.

We wish to adapt our semidefinite programming approach to input polynomials f which are sparse or symmetric or both. For instance, our polynomial example ([\ref=RunningEx]) is both sparse and invariant under permutation of the variables x,y,z. Both Newton polytope techniques and representation theory can be used to reduce the size of the matrices and the number of free parameters in the semi-definite programs.

Higher degree relaxations

If we are unlucky, then the output produced by SeDuMi will not satisfy the hypothesis of Proposition [\ref=completesquare], and we conclude that the bound fsos is probably strictly smaller than the optimal solution f*. In that event we redo our computation in higher degree, now with a larger SDP. The key idea is that even though f(x) - λ may not be a sum of squares, if there exists a positive polynomial g(x) such that g(x)  ·  (f(x) - λ) is a sum of squares, then λ  ≤  f*. The choice of g can be either made a priori (for instance, [formula]), or as a result of an optimization step (see [\cite=pablo] for details). The Positivstellensatz (see Section 7) ensures that f* will be found if the degree of g is large enough.

Solving polynomial equations

A natural application of Shor's relaxation, hinted at in [\cite=shor], is solving polynomial systems [formula]. The polynomial [formula] satisfies   f*  ≥  fsos  ≥  0, and f*  =  0 holds if and only if the system has a real root. Clearly, fsos  >  0 is a sufficient condition for the nonexistence of real roots. An important open problem, essentially raised in [\cite=shor], is to characterize inconsistent systems [formula] with fsos  =  0. On the other hand, if   f*  =  fsos  =  0   holds then it is possible, at least in principle, to obtain a numerical approximation of real roots using SDP. However, for a robust implementation, perturbation arguments are required and some important numerical issues arise, so the perspectives for practical applications are still unclear.

Minimizing polynomials over polytopes

Consider a compact set

[formula]

where [formula] is a linear form plus a constant, say, P is a polytope with s facets. Handelman's Theorem [\cite=han] states that every polynomial which is strictly positive on P can be expressed as a positive linear combination of products [formula]. Suppose we wish to minimize a given polynomial function f(x) over P. For [formula] we define the D-th Handelman bound   f(D)   to be the largest [formula] such that

[formula]

Handelman's Theorem states that the increasing sequence [formula] converges to the minimum of f over P. Each bound f(D) can be computed using linear programming only. It would be interesting to study the quality of these bounds, and the running time of these linear programs, and to see how things improve as we augment the approach with semidefinite programming techniques.

Which semialgebraic sets are semidefinite ?

The feasible set of an SDP can be expressed by a linear matrix inequality, as in the dual formulation in Section 4.2. It would be interesting to study these feasible sets using techniques from real algebraic geometry, and to identify characteristic features of these sets. Here is a very concrete problem whose solution, to the best of our knowledge, is not known. Fix three real symmetric matrices A,B and C of size N  ×  N. Then

[formula]

is a closed, convex, semialgebraic subset of the plane [formula]. The problem is to find a good characterization of those subsets S. Given a semialgebraic subset [formula] which is closed and convex, how to decide whether a "semidefinite representation" exists, and, in the affirmative case, how to find matrices A,B,C of minimum size.

Numerical Real Algebraic Geometry and The Positivstellensatz

The first part of the above title refers to a paper by Sommese and Wampler [\cite=sommese]. This paper and other more recent ones suggest that numerical algorithms will play an increasingly important role in computational (complex) algebraic geometry, and that polynomial systems will become much more visible in the context of Scientific Computation. Along the same lines, the fastest software for computing Gröbner bases, due to Faugére [\cite=faugere], no longer uses the Buchberger algorithm but replaces it by sophisticated numerical linear algebra. Faugére's scheme

[formula]

has the potential of entering the standard repertoire of Scientific Computation.

Following [\cite=pablo] we propose an analogous scheme for the field of real numbers:

[formula]

In what follows, we shall explain this relationship and why we see the Positivstellensatz as the main catalyst for a future role of real algebra in scientific computation.

The Positivstellensatz [\cite=BCR] is a common generalization of Linear Programming Duality (for linear inequalities) and Hilbert's Nullstellensatz (for an algebraically closed field). It states that, for a system of polynomial equations and inequalities, either there exists a solution in [formula], or there exists a certain polynomial identity which bears witness to the fact that no solution exists. For instance, a single polynomial inequality f(x)  <  0 either has a solution [formula], or there exists an identity   g(x)f(x)  =  h(x)   where g and h are sums of squares. See [\cite=boko] for an exposition of the Positivstellensatz from the perspective of computational geometry. Finding a witness by linear programming is proposed in [\cite=boko].

Here is our punchline, first stated in the dissertation of the first author [\cite=pablo]: A Positivstellensatz witness of bounded degree can be computed by semidefinite programming. Here we can also optimize linear parameters in the coefficients. This suggests the following algorithm for deciding a system of polynomial equations and inequalities: decide whether there exists a witness for infeasibility of degree ≤  D, for some D  ≫  0. If our system is feasible, then we might like to minimize a polynomial f(x) over the solution set. The D-th SDP relaxation would be to ask for the largest real number λ such that the given system together with the inequality   f(x)  -  λ  <  0   has an infeasibility witness of degree D. This generalizes what was proposed in Sections 6.2, 6.3 and 6.4.

It is possible, at least in principle, to use an a priori bound for the degree D in the Positivestellensatz, however, the currently known bounds are still very large. Lombardi and Roy recently announced a bound which is triply-exponential in the number n of variables. We hope that such bounds can be further improved, at least for some natural families of polynomial problems arising in optimization.

Here is a very simple example in the plane to illustrate our method:

[formula]

By the Positivstellensatz, the system   {f  ≥  0,  g  =  0}   has no solution [formula] if and only if there exist polynomials [formula] that satisfy the following:

[formula]

The D-th SDP relaxation of the polynomial problem   {f  ≥  0,  g  =  0}   asks whether there exists a solution (s1,s2,s3) to ([\ref=eq:pstz]) where the polynomial s1 has degree ≤  D and the polynomials s2,s3 have degree ≤  D - 2. For each fixed integer D  >  0 this can be tested by semidefinite programming. For D = 2 we find the solution

[formula]

The resulting identity ([\ref=eq:pstz]) proves the inconsistency of the system   {f  ≥  0,  g  =  0}.