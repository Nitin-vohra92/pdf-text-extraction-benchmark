Dos and don'ts of reduced chi-squared

Introduction

When fitting a model f with parameters [formula] to N data values yn, measured with (uncorrelated) Gaussian errors σn at positions [formula], one needs to minimise

[formula]

This is equivalent to maximising the so-called "likelihood function". If the data's measurement errors are not Gaussian, χ2 should not be used because it is not the maximum-likelihood estimator. For the rest of this manuscript, we shall therefore assume that the data's errors are Gaussian. If K denotes the number of degrees of freedom, reduced χ2 is then defined by

[formula]

[formula] is a quantity widely used in astronomy. It is essentially used for the following purposes:

Single-model assessment: If a model is fitted to data and the resulting [formula] is larger than one, it is considered a "bad" fit, whereas if [formula], it is considered an overfit.

Model comparison: Given data and a set of different models, we ask the question which model fits the data best. Typically, each model is fit to the data and their values of [formula] are compared. The winning model is that one whose value of [formula] is closest to one.

Convergence diagnostic: A fit is typically an iterative process which has to be stopped when converged. Convergence is sometimes diagnosed by monitoring how the value of [formula] evolves during the iteration and the fit is stopped as soon as [formula] reaches a value sufficiently close to one. Sometimes it is claimed then, that "the fit has reached noise level".

Error estimation: One fits a certain model to given data by minimising χ2 and then rescales the data's errors such that the value of [formula] is exactly equal to one. From this one then computes the errors of the model parameters. (It has already been discussed by [\citet=Andrae2010d] that this method is incorrect, so we will not consider it any further here.)

In all these cases, [formula] excels in simplicity, since all one needs to do is divide the value of χ2 by the number of degrees of freedom and compare the resulting value of [formula] to one.

In this manuscript, we want to investigate the conditions under which the aforementioned applications are meaningful - at least the first three. In particular, we discuss the pitfalls that may severly limit the credibility of these applications. We explain the two major problems that typically arise in using [formula] in practice: First, we dicuss the issue of estimating the number of degrees of freedom in Sect. [\ref=sect:dofs]. Second, we explain how the uncertainty in the value of χ2 may affect the above applications in Sect. [\ref=sect:noise_on_chi2]. Section [\ref=sect:alternatives] is then dedicated to explain more reliable methods rather than [formula]. We conclude in Sect. [\ref=sect:conclusions].

Degrees of freedom

Given the definition of [formula], it is evidently necessary to know the number of degrees of freedom of the model. For N data points and P fit parameters, a naïve guess is that the number of degrees of freedom is N - P. However, in this section, we explain why this is not true in general. We begin with a definition of "degrees of freedom" and then split this discussion into three parts: First, we discuss only linear models. Second, we discuss linear models with priors. Third, we discuss nonlinear models. Finally, we discuss whether linearisation may help in the case of nonlinear models.

Definition

For a given parameter estimate, e.g., a model fitted to data, the degrees of freedom are the number of independent pieces of information that were used. The concept of "degrees of freedom" can be defined in different ways. Here, we give a general and simple definition. In the next section, we give a more technical definition that only applies to linear models.

Let us suppose that we are given N measurements yn and a model with P free parameters [formula]. The best-fitting parameter values are found by minimising χ2. This means we impose P constraints of the type

[formula]

onto our N-dimensional system. Hence, the number of degrees of freedom is K = N - P. At first glance, this appears to be a concise and infallible definition. However, we shall see that this is not the case.

Linear models without priors

A linear model is a model, where all fit parameters are linear. This means it is a linear superposition of a set of basis functions,

[formula]

where the coefficients θp are the fit parameters and the [formula] are some (potentially nonlinear) functions of the position [formula]. A typical example is a polynomial fit, where Bp(x) = xp. Inserting such a linear model into χ2 causes χ2 to be a quadratic function of the fit parameters, i.e., the first derivatives - our constraints from Eq. ([\ref=eq:dof_constraints]) - form a set of linear equations that can be solved analytically under certain assumptions.

A natural approach to solve such sets of linear equations is to employ linear algebra. This will lead us to a quantitative definition of the number of degrees of freedom for linear models. Let us introduce the following quantities:

[formula] is the N-dimensional vector of measurements yn.

[formula] is the P-dimensional vector of linear model parameters θp.

[formula] is the N  ×  N covariance matrix of the measurements, which is diagonal in the case of Eq. ([\ref=eq:def:chi2]), i.e., [formula].

[formula] is the so-called design matrix which has format N  ×  P. Its elements are given by [formula], i.e., the p-th basis function evaluated at the n-th measurement point.

Given these definitions, we can now rewrite Eq. ([\ref=eq:def:chi2]) in matrix notation,

[formula]

Minimising χ2 by solving the constraints of Eq. ([\ref=eq:dof_constraints]) then yields the analytic solution

[formula]

where the hat in [formula] accounts for the fact that this is only an estimator for [formula], but not the true [formula] itself. We then obtain the prediction [formula] of the measurements [formula] by,

[formula]

where we have introduced the N  ×  N matrix [formula], which is sometimes called "hat matrix", because it translates the data [formula] into a model prediction [formula]. The number of effective model parameters is then given by the trace of [formula] [\citep=Ye1998] [\citep=Hastie2009],

[formula]

which also equals the rank of the design matrix [formula]. Obviously, [formula], where the equality holds if and only if the design matrix [formula] has full rank. Consequently, for linear models the number of degrees of freedom is

[formula]

The standard claim is that a linear model with P parameters removes P degrees of freedom when fitted to N data points, such that the remaining number of degrees of freedom is K = N - P. Is this correct? No, not necessarily so. The problem is in the required linear independence of the P basis functions. We can also say that the P constraints given by Eq. ([\ref=eq:dof_constraints]) are not automatically independent of each other. Let us consider a trivial example, where the basis functions are clearly not linearly independent:

The linear model [formula] is composed of two constants, θ1 and θ2, i.e., [formula]. Obviously, this two-parameter linear model cannot fit two arbitrary data points and its number of degrees of freedom is not given by N - 2 but N - 1, because the design matrix [formula] only has rank 1, not rank 2. In simple words, the two constraints [formula] and [formula] are not independent of each other.

From this discussion we have to draw the conclusion that for a linear model the number of degrees of freedom is given by N - P if and only if the basis functions are linearly independent for the given data positions [formula], which means that the design matrix [formula] has full rank. In practice, this condition is usually satisfied, but not always. In the more general case, the true number of degrees of freedom for a linear model may be anywhere between N - P and N - 1.

Linear models with priors

Priors are commonly used to restrict the possible values of fit parameters. In practice, priors are usually motivated by physical arguments, e.g., a fit parameter corresponding to the mass of an object must not be negative. Let us consider a very simple example of a prior:

A linear model f(x,a0,a1) = a0 + a1x is given. The value of parameter a0 is not restricted, but a1 must not be negative. Figure [\ref=fig:linear_model_with_prior] demonstrates that this two-parameter model is incapable of sensibly fitting two arbitrary data points because of this prior.

Obviously, priors reduce the flexibility of a model, which is actually what they are designed to do. Consequently, they also affect the number of degrees of freedom. In this case, the prior was a step function (zero for a1 < 0 and one otherwise), i.e., it was highly nonlinear. Consequently, although f(x,a0,a1) = a0 + a1x is itself a linear function of all fit parameters, the overall model including the prior is not linear anymore. This leads us directly to the issue of nonlinear models.

Nonlinear models

We have seen that estimating the number of degrees of freedom is possible in the case of linear models with the help of Eq. ([\ref=eq:P_effective]). However, for a nonlinear model, we cannot rewrite χ2 like in Eq. ([\ref=eq:only4linear]), because a nonlinear model cannot be written as [formula]. Therefore, [formula] does not exist and we cannot use Eq. ([\ref=eq:P_effective]) for estimating the number of degrees of freedom. [\citet=Ye1998] introduces the concept of "generalised degrees of freedom", but concludes that it is infeasible in practice. We now consider two examples in order to get an impression why the concept of degrees of freedom is difficult to grasp for nonlinear models:

Let us consider the following model f(x), having three free parameters A, B, C,

[formula]

If we are given a set of N measurement (xn,yn,σn) such that no two data points have identical xn, then the model f(x) is capable of fitting any such data set perfectly. The way this works is by increasing the "frequency" B such that f(x) can change on arbitrarily short scales. As f(x) provides a perfect fit in this case, χ2 is equal to zero for all possible noise realisations of the data. Evidently, this three-parameter model has infinite flexibility (if there are no priors) and K = N - P is a poor estimate of the number of degrees of freedom, which actually is K = 0.

Let us modify the model of Example [\ref=ex_3] slightly by adding another component with additional free parameters D, E, and F,

[formula]

If the fit parameter D becomes small such that |D|  ≪  |A|, the second component cannot influence the fit anymore and the two model parameters E and F are "lost". In simple words: This model may change its flexibility during the fitting procedure.

Hence, for nonlinear models, K may not even be constant. Of course, these two examples do not verify the claim that always K  ≠  N - P for nonlinear models. However, acting as counter-examples, they clearly falsify the claim that K = N - P is always true for nonlinear models.

Local linearisation

As we have seen above, there is no well-defined method for estimating the number of degrees of freedom for a truly nonlinear model. We may now object that any well-behaved nonlinear model can be linearised around the parameter values which minimise χ2. Let us denote the parameter values that minimise χ2 by [formula]. We can then Taylor-expand χ2 at [formula] to second order,

[formula]

where the first derivative is zero at the minimum. Apparently, χ2 is now a quadratic function of the model parameters, i.e., the model is linearised. Does this mean that we can simply linearise the model in order to get rid of the problems with defining the number of degrees of freedom for a nonlinear model?

The answer is definitely no. The crucial problem is that linearisation is just an approximation. The Taylor expansion of Eq. ([\ref=eq:CLT_expansion]) has been truncated after the second-order term. There are two issues here: First, in general, we do not know how good this approximation really is for a given data sample. Second, we have no way of knowing how good the approximation needs to be in order to sufficiently linearise the model from the number-of-degrees-of-freedom point of view.

Even if these issues did not concern us, would linearising the model really help? Again, the answer is no. As we have seen in Sect. [\ref=sect:dofs_linear], the number of degrees of freedom is also not necessarily given by N - P for a linear model. Moreover, the truly worrisome result of Sect. [\ref=sect:dofs_nonlinear] - that the number of degrees of freedom is not constant - is not overcome by the linearisation. The reason is that the expansion of Eq. ([\ref=eq:CLT_expansion]), and thereby the linearisation, depends nonlinearly upon where the maximum is. Consequently, the uncertainties in the maximum position inherited from the data's noise propagate nonlinearly through the expansion of Eq. ([\ref=eq:CLT_expansion]). Therefore, we have to draw the conclusion that there is no way of reliably estimating the number of degrees of freedom for a nonlinear model.

Summary

Summarising the arguments brought up so far, we have seen that estimating the number of degrees of freedom is absolutely nontrivial. In the case of linear models, the number of degrees of freedom is given by N - P if and only if the basis functions are indeed linearly independent in the regime sampled by the given data. Usually, this is true in practice. Otherwise, the number of degrees of freedom is somewhere between N - P and N - 1. However, in the case of nonlinear models, the number of degrees of freedom can be anywhere between 0 and N - 1 and it is even not necessarily constant during the fit. Linearising the model at the optimum does not really help to infer the number of degrees of freedom, because the linearised model still depends on the optimum parameters in a nonlinear way. Hence, it is questionable whether it is actually possible to compute [formula] for nonlinear models in practice.

Uncertainty in χ2

We now discuss another problem, which is completely independent of our previous considerations. Even if we were able to estimate the number of degrees of freedom reliably, this problem would still interfere with any inference based on [formula]. This problem stems from the fact that the value of χ2 is subject to noise, which is inherited from the random noise of the data. Consequently, there is an "uncertainty" on the value of χ2 and hence on [formula], which is typically ignored in practice. However, we show that this uncertainty is usually large and must not be neglected, because it may have a severe impact on the intended application.

Given some data with Gaussian noise, the true model having the true parameter values will generate a χ2 = N and has N degrees of freedom because there is no fit involved. Hence, it results in a [formula] of 1. We therefore compare the [formula] of our trial model to 1 in order to assess convergence or to compare different models. Is this correct?

In theory, yes. In practice, no. Even in the case of the true model having the true parameter values, where there is no fit at all, the value of χ2 is subject to noise. In this case, we are fortunate enough to be able to quantify this uncertainty. For the true model having the true parameter values and a-priori known measurement errors σn, the normalised residuals,

[formula]

are distributed according to a Gaussian with mean μ = 0 and variance σ2 = 1. In this case only, χ2 is the sum of K = N Gaussian variates and its probability distribution is given by the so-called χ2-distribution,

[formula]

Figure [\ref=fig:chi2_distributions] shows some χ2-distributions with different values of K. The expectation value of the χ2-distribution is,

[formula]

In fact, this expectation value is sometimes used as an alternative definition of "degrees of freedom". As the χ2-distribution is of non-zero width, there is however an uncertainty on this expectation value. More precisely, the variance of the χ2-distribution is given by 2K. This means the expectation value of [formula] for the true model having the true parameter values is indeed one, but it has a variance of 2 / K = 2 / N. If N is large, the χ2-distribution becomes approximately Gaussian and we can take the root of the variance, [formula], as an estimate of the width of the (approximately Gaussian) peak. Let us consider a simple example in order to get a feeling how severe this problem actually is:

We are given a data set comprised of N = 1,000 samples. Let the task be to use [formula] in order to compare different models to select that one which fits the data best, or to fit a single model to this data and assess convergence. The true model having the true parameter values - whether it is given or not - will have a value of [formula] with an (approximated) Gaussian standard deviation of [formula]. Consequently, within the 3σ interval [formula] we can neither reliably differentiate between different models nor assess convergence.

This simple example clearly shows that this problem is very drastic in practice. Moreover, astronomical data sets are often much smaller than N = 1,000, which increases the uncertainty of [formula].

Of course, there is not only an uncertainty on the comparison value of [formula] for the true model having the true parameter values. There is also an uncertainty on the value of [formula] for any other model. Unfortunately, we cannot quantify this uncertainty via [formula] in practice anymore, because the χ2-distribution applies only to the true model having the true parameter values. For any other model the normalised residuals (cf. Eq. ([\ref=eq:normalised_residuals])) are not Gaussian with mean μ = 0 and variance σ2 = 1. Hence, χ2 is not the sum of K Gaussian variates and the derivation of the χ2-distribution is invalid.

Alternative methods

If [formula] does not provide a reliable method for assessing and comparing model fits, convergence tests or error estimation, what other methods can then be used with more confidence? An in-depth survey of alternative methods would be beyond the scope of this manuscript. Therefore, we restrict our discussion on some outstanding methods. Concerning methods for error estimation, we refer the interested reader to [\citet=Andrae2010d].

Residuals

The first and foremost thing to do in order to assess the goodness of fit of some model to some data is to inspect the residuals. This is indeed trivial, because the residuals have already been computed in order to evaluate χ2 (cf. Eq. ([\ref=eq:def:chi2])). For the true model having the true parameter values and a-priori known measurement errors, the distribution of normalised residuals (cf. Eq. ([\ref=eq:normalised_residuals])) is by definition Gaussian with mean μ = 0 and variance σ2 = 1. For any other model, this is not true. Consequently, all one needs to do is to plot the distribution of normalised residuals in a histogram and compare it to a Gaussian of μ = 0 and σ2 = 1. If the histogram exhibits a statistically significant deviation from the Gaussian, we can rule out that the model is the truth. If there is no significant difference between histogram and Gaussian, this can mean (a) we found the truth, or (b) we do not have enough data points to discover the deviation. The comparison of the residuals to this Gaussian should be objectively quantified, e.g., by using a Kolmogorov-Smirnov test [\citep=Kolmogorov1933] [\citep=Smirnov1948].

In theory, this method may be used as a convergence diagnostic. In an iterative fit procedure, compare the distribution of normalised residuals to the Gaussian with μ = 0 and σ2 = 1 in each iteration step, e.g., via a Kolmogorov-Smirnov test. At first, the likelihood of the residuals to be Gaussian will increase as the model fit becomes better. If the fit finds a suitable local minimum, the model may eventually start to overfit the data and the likelihood of the residuals to be Gaussian will decrease again, as the residuals will peak too sharply at zero. When this happens, the fitting procedure should be stopped. In practice, there is no guarantee that this works, as the fit may end up in a local minimum with residuals too widely spread to resemble the Gaussian with μ = 0 and σ2 = 1.

Similarly, this method may also be used for model comparison. Given some data and a set of models, the model favoured by the data is that whose normalised residuals match the Gaussian with μ = 0 and σ2 = 1 best. The winning model does not need to be the truth. Let us consider the following example:

[\citet=Vogt2010] analysed radial-velocity data of the nearby star GJ 581 and came to the conclusion that the data suggests the presence of six exoplanets instead of four as claimed by other authors using different data [\citep=Mayor2009]. [\citet=Vogt2010] assumed circular orbits, which result in nonlinear models of the form of Eq. ([\ref=eq:circular_orbit]) in Example [\ref=ex_3]. Their claim that two additional planets are required is primarily justified from the associated [formula] (cf. Table [\ref=table:KS_results]). We take the identical data as [\citet=Vogt2010] and their asserted orbital parameters of the six planets (their Table 2). For every number of planets, we apply the KS-test to the normalised residuals. Table [\ref=table:KS_results] gives the resulting p-values for the six models. In terms of the KS-test, the data used by [\citet=Vogt2010] strongly favour the model using four planets over the model using six planets. Furthermore, Fig. [\ref=fig:dist_normed_residuals] displays the distributions of normalised residuals for the models using four and six planets. Evidently, both distributions deviate significantly from the Gaussian with μ = 0 and σ2 = 1, which implies that neither model is compatible with the truth. The most likely explanation for this discrepancy is that planetary orbits may be elliptical, whereas [\citet=Vogt2010] assumed circular orbits.

There is a pitfall here, as well. The likelihood of the normalised residuals to come from a Gaussian with μ = 0 and σ2 = 1 is also subject to noise, as in the case of the value of χ2. However, these uncertainties surely cannot explain the large differences in Table [\ref=table:KS_results].

Cross-validation

Cross-validation is one of the most powerful and most reliable methods for model comparison. Unfortunately, it is usually also computationally expensive. However, if [formula] is not applicable, e.g., because the model is nonlinear, computational cost cannot be used as an argument in disfavour of cross-validation.

The most straightforward (and also most expensive) flavour of cross-validation is "leave-one-out cross-validation". We are given N data points and a set of models, and we want to know which model fits the data best. For each model, the goodness of fit is estimated in the following way:

Remove the n-th data point from the data sample.

Fit the model to the remaining N - 1 data points.

Take the model fitted to the N - 1 data points and compute its likelihood for the n-th data point that has been left out.

Repeat steps 1 to 3 from n = 1 to n = N and compute the goodness of the prediction of the whole data set by multiplying the likelihoods obtained in step 3.

Steps 3 and 4 require the data's error distribution to be known in order to evaluate the goodness of the prediction for the left-out data point through its likelihood. For instance, if the data's errors are Gaussian, the goodness of the prediction is simply given by Eq. ([\ref=eq:normalised_residuals]) as usual. Evidently, repeating steps 1 to 3 N times is what makes leave-one-out cross-validation computationally expensive. It is also possible to leave out more than one data point in each step. However, if the given data set is very small, cross-validation becomes unstable. A nice application of cross-validation can be found, e.g., in [\citet=Hogg2008].

Bootstrapping

Bootstrapping is somewhat more general than cross-validation, meaning it requires less knowledge about the origin of the data. Cross-validation requires the data's error distribution to be known in order to evaluate the likelihoods, whereas bootstrapping does not. Of course, this is an advantage if we do not have this knowledge. However, if we do know the data's errors, we should definitely exploit this knowledge by using cross-validation. Bootstrapping is discussed in [\citet=Andrae2010d] in the context of error estimation. Therefore, we restrict its discussion here on the context of model comparison.

Let us suppose we are given 4 measurements y1,y2,y3,y4. We then draw subsamples of size 4 from this data set with replacement. These subsamples are called bootstrap samples. Examples are:

y1,y2,y3,y4 itself,

y1,y2,y2,y4,

y2,y4,y4,y4,

y1,y1,y1,y1,

etc.

We draw a certain number of such bootstrap samples, and to every such sample we then fit all the models that are to be compared.

In the context of model comparison, bootstrapping is typically used as "leave-one-out bootstrap" [\citep=Hastie2009]. The algorithm is given by:

Draw a certain number of bootstrap samples from a given data set.

Fit all the models to every bootstrap sample.

For the n-th data point yn in the given data set, consider only those bootstrap samples that do not contain yn. Predict yn from the models fitted to these bootstrap samples.

Repeat step 3 from n = 1 to n = N and monitor the goodness of the predictions, e.g., by least squares.

Like cross-validation, bootstrapping aims at the prediction error of the model. Therefore, it is sensitive to over- and underfittings.

Conclusions

We have argued that there are two fundamental problems in using [formula], which are completely independent of each other:

In Sect. [\ref=sect:dofs], we have seen that estimating the number of degrees of freedom, which is necessary for evaluating [formula], is absolutely nontrivial in practice:

Concerning linear models, for N given data points and P fit parameters the number of degrees of freedom is somewhere between N - P and N - 1, where it is N - P if and only if the basis functions of the linear model are linearly independent for the given data. Equation ([\ref=eq:P_effective]) provides a quantification for the effective number of fit parameters of a linear model. Priors can cause a linear model to become nonlinear.

Concerning nonlinear models, the number of degrees of freedom is somewhere between zero and N - 1 and it may not even be constant during a fit, i.e., N - P is a completely unjustified guess. The authors are not aware of any method that reliably estimates the number of degrees of freedom for nonlinear models. Consequently, it appears to be impossible to compute [formula] in this case.

In Sect. [\ref=sect:noise_on_chi2], we have seen that the actual value of [formula] is uncertain. If the number N of given data points is large, the uncertainty of [formula] is approximately given by the Gaussian error [formula]. For N = 1,000 data points, this means that within the 3σ-interval [formula] we cannot compare models or assess convergence.

Given these considerations, it appears highly questionable whether the popularity of [formula] - which is certainly due to its apparent simplicity - is indeed justified. As a matter of fact, [formula] cannot be evaluated for a nonlinear model, because the number of degrees of freedom is unknown in this case. This is a severe restriction, because many relevant models are nonlinear. Moreover, even for linear models, [formula] has to be used with due caution, considering the uncertainty in its value.

Concerning alternative methods for model comparison, we have explained cross-validation and bootstrapping in Sect. [\ref=sect:alternatives]. We also explained how the normalised residuals of a model can be used to infer how close this model is to the true model underlying the given data. Concerning alternative methods for error estimation, we refer the interested reader to [\citet=Andrae2010d].

Finally, we want to emphasise that the above considerations concerning [formula] have no impact on the correctness of minimising a χ2 in order to fit a model to data. Fitting models to data is a completely different task that should not be confused with model comparison or convergence testing. Minimising χ2 is the correct thing to do whenever the data's measurement errors are Gaussian and a maximum-likelihood estimate is desired.

Acknowledgements

RA thanks David Hogg for detailed discussions on this subject. David Hogg also came up with a couple of the examples mentioned here. Furthermore, RA thanks Coryn Bailer-Jones for helpful comments on the contents of this manuscript. RA is funded by a Klaus-Tschira scholarship. TS is funded by a grant from the Max Planck Society. PM is supported by the DFG Priority Programme 1177.