Theory and modeling of the magnetic field measurement in LISA PathFinder

Introduction

LISA Pathfinder ( LPF) is a science and technology demonstrator programmed by the European Space Agency ( ESA) within its LISA mission activities [\cite=bib:LISA]. LISA (Laser Interferometer Space Antenna) is a joint ESA-NASA mission which will be the first low frequency (milli-Hz) gravitational wave detector, and also the first space-borne gravitational wave observatory. LPF's payload is the LISA Technology Package (LTP), and will be the highest sensitivity geodesic explorer flown to date. The LTP is designed to measure relative accelerations between two test masses (TM) in nominal free fall (geodesic motion) with a noise budget of

[formula]

in the frequency band between 1 mHz and 30 mHz [\cite=bib:trento].

Noise in the LTP arises as a consequence of various disturbances, mainly generated within the spacecraft itself, which limit the performance of the instrument. A number of these disturbances are monitored and dealt with by means of suitable devices, which form the so-called Diagnostics Subsystem [\cite=bib:ere2006]. In LPF, this includes thermal and magnetic diagnostics, plus the radiation monitor, which provides counting and spectral information on ionizing particles hitting the spacecraft. The magnetic diagnostics system will be the subject of our attention here.

One of the most important functions of the LTP magnetic diagnostics is the determination of the magnetic field and its gradient at the positions of the TMs. For this, it includes a set of four tri-axial fluxgate magnetometers, intended to measure with high precision the magnetic field at the positions they occupy in the spacecraft -- see figure [\ref=fig.1]. Their readouts do not however provide a direct measurement of the magnetic field at the positions where the TMs are, and an interpolation method must therefore be implemented to calculate it. In the circumstances we face, this is a difficult problem, mostly because the magnetometers layout is such that they are too distant from the locations of the TMs compared with the typical scales of the distribution of magnetic sources in the satellite. Its solution is however imperative since magnetic noise can be as high as 40 % of the total budget [\cite=bib:trento] given by Eq. ([\ref=eq.0]), and hence it must be properly quantified.

In order to design a suitable interpolation scheme, information on the actual distribution of magnetic sources is necessary. Data from the spacecraft manufacturer (EADS Astrium Stevenage, UK) have kindly been handed to us [\cite=bib:wealthy] for this purpose. According to these data, magnetic sources can be characterized as magnetic dipoles, whose positions are known and whose magnetic moments are only known in modulus -- not in orientation. Most of these dipoles are associated to electronic boxes, with a few genuinely magnetic elements. An exception to this rule is the solar panels, which cover the entire spacecraft and can hardly be considered as a dipole as seen by the magnetometers. They are however designed so that their cells are arranged to minimize magnetic effects by having their rim wires wound contiguous and in opposite senses.

Astrium data are based on system design, so validation with the real spacecraft must be done by means of experiment, which is of course included in the planned activities before launch. Actually, though, the structure of the magnetic source distribution and their properties will not be directly visible either to the magnetometers or to the interpolation algorithms, which will just work with magnetic field values no matter how they are generated. Nevertheless, we think that the information available so far, though not final, qualifies very well as a guide to the elaboration of a magnetic model which will be needed to define and verify the performance of the analysis algorithms which will eventually be applied to the data delivered by the satellite in flight.

In this paper we will make use of the dipole model of the sources to assess the performance of two different types of interpolation methods: multipole interpolation and neural network algorithms. The first is the more immediate one to try, but as we will show below it is not as efficient as one might expect a priori. To overcome this problem we propose a novel method, based on neural networks. Based on the results obtained with the same dipole source model, our solution looks promising since the errors of the interpolated fields and gradients are significantly smaller than those obtained with the multipole approach. The paper is structured as follows. In Sect. [\ref=chap.2] we provide a general description of the problem. It follows Sect. [\ref=chap.3], where we discuss the multipole interpolation, whereas in Sect. [\ref=chap.4] we explain our neural network approach. The results of applying this algorithm are presented in Sect. [\ref=chap.5], while in Sect. [\ref=chap.6] we summarize our major findings and we draw our conclusions.

General description of the problem

Magnetic noise in the LTP is allowed to be a significant fraction of the total mission acceleration noise: [formula] m s- 2 Hz- 1 / 2 can be apportioned to magnetism, i.e., 40 % of the total noise, 3×  10- 14 m s- 2 Hz- 1 / 2, see Eq. ([\ref=eq.0]). This noise occurs because the residual magnetization and susceptibility of the TMs couple to the surrounding magnetic field, giving rise to a force

[formula]

in each of the TMs. In this expression B is the magnetic field in the TM, χ and M are its magnetic susceptibility and density of magnetic moment (magnetization), respectively, and [formula] is the volume of the TM; μ0 is the vacuum magnetic constant, 4π  ×  10- 7 m kg s- 2 A- 2), and [formula] indicates TM volume average of the enclosed quantity. Moreover, the magnetic field and its gradient randomly fluctuate in the regions occupied by the test masses, thus resulting in a randomly fluctuating force:

[formula]

where [formula] represents the fluctuation of the magnetic field, and δ stands for the fluctuation of the gradient [\cite=bib:ntcs].

Quantitative assessment of magnetic noise in the LTP clearly requires real-time monitoring of the magnetic field, which in LPF is done by means of a set of four tri-axial fluxgate magnetometers [\cite=bib:DDS_LTP]. These devices have a high-permeability magnetic core, which drives a design constraint to keep them somewhat far from the TMs. The price to be paid for this is that the measured field is not directly useful (we need to know it at the positions of the TMs). Hence, a procedure to estimate it at these positions, based on the data delivered by the magnetometers, must be set up.

As previously mentioned, the sources of magnetic field are essentially electronics boxes plus a few genuinely magnetic components inside the spacecraft. The interplanetary magnetic field is orders of magnitude weaker, hence of little relevance to the effects considered here, and solar panel effects will not be considered -- see section [\ref=chap.1]. There are no sources of magnetic field inside the LTP Core Assembly ( LCA), all being placed outside its walls. The number of Astrium identified sources is around 40, and can be modeled as point magnetic dipoles [\cite=bib:wealthy]. Figure [\ref=fig.2] gives an overview of the geometry, see caption for details.

Multipole interpolation theory

Perhaps the most immediate (and obvious) procedure to interpolate the magnetic field is to resort to its multipole structure. This is known to be the best option in some mathematical sense [\cite=bib:jackson]. Consequently, we first describe the details of its implementation, and then we assess its practical merit.

We will treat the LCA region as a vacuum. This is a reasonable hypothesis, as the materials inside it are essentially non-magnetic. Accordingly, the magnetic field has zero divergence and rotational :

[formula]

Since [formula], we thus have

[formula]

where [formula] is a scalar function. Additionally, since [formula] = 0, too, it immediately follows that [formula] is a harmonic function, or

[formula]

The solution to this equation can be expressed as an orthogonal series of the form

[formula]

where

[formula]

are the spherical coordinates of the field point x, whose origin is by (arbitrary) convention assumed in the geometric center of the LCA. Equation ([\ref=eq.7]) could also contain terms proportional to r- l - 1, but these have been dropped because the field cannot diverge at the center of the LCA. Actually, the expansion of Eq. ([\ref=eq.7]) is only valid in a region interior to the closest field source. Finally, the coefficients Mlm(t), which will be called multipole coefficients in the sequel, depend on the sources of magnetic field.

To obtain the field components we take the derivative of Eq. ([\ref=eq.7]) following Eq. ([\ref=eq.5]):

[formula]

According to standard mathematics, the coefficients Mlm(t) can be fully determined if the magnetic field is known at the boundary of the volume where the field equations are considered, in this case the LCA. This data is of course not available to us, since we only know B in four points of the boundary, where the magnetometers are. Therefore the question we need to address is: how many terms of the series can we possibly determine on the basis of the limited information available? Or, equivalently, how many multipole coefficients can we estimate, given the magnetometers readout data? Then, also, to which accuracy can we estimate the actual magnetic field after the maximum number of multipole coefficients have been calculated?

The answer to the first question above is actually not difficult: let us assume that the series in Eq. ([\ref=eq.9]) is truncated after a maximum multipole index value [formula] = L. The estimated field, [formula], is then given by:

[formula]

The number of terms in this sum is

[formula]

which obviously equals the number of multipole coefficients needed to evaluate the sum. For example, we have [formula] = 8 and [formula] = 15. On the other hand, the number of magnetometer data channels is 12 -- three channels per magnetometer, as the devices are tri-axial. This means we cannot push the series beyond the quadrupole ([formula] = 2) terms. This means that since we only have 12 data channels we have some redundancy to determine the first eight Mlm(t) coefficients up to [formula] = 2, though we also lack information to evaluate the next seven octupole terms .

In order to make a best estimate of the Mlm(t), a least-square method is set up as follows. Firstly, we define a quadratic error:

[formula]

where [formula] is the real magnetic field and the sum extends over the number of magnetometers, situated at positions [formula] ([formula] = 1,,4). We then find those values of Mlm which minimize the error:

[formula]

Once this system of equations is solved, the estimated coefficients Mlm(t) are replaced back into Eq. ([\ref=eq.10]) and then the spatial arguments x substituted by the positions of each test mass to finally obtain the interpolated field values. This process needs to be repeated for each instant [formula] of time at which measurements are taken, thereby generating the magnetic field time series. The gradient is estimated by taking the derivatives of Eq. ([\ref=eq.10]):

[formula]

It is to be noted that Eq. ([\ref=eq.10]) is a polynomial of degree [formula]1 in the space coordinates (x,y,z), hence its degree equals 1 when L = 2. Since this is the most we can get of the magnetometer readout channels, the multipole expansion is actually equivalent to a linear interpolation of the field between its values at the boundary of the LCA and its interior. We may therefore not expect this method to produce excellent results, simply because the magnetic field inside the LCA is weaker than at its boundaries, the reason being that the magnetic field sources are outside the LCA. This valley structure of the magnetic field needs at least octupole (quadratic) terms to be approximated, but this would require at least one more vector magnetometer, which is not available. By the same argument, the field gradient can only be approximated by a constant value throughout the LCA -- see Eq. ([\ref=eq.14]).

Numerical simulations

In order to have a quantitative idea of the actual performance of the above interpolation scheme, we make use of the source dipole model. It has the following ingredients and assumptions:

The sources of magnetic are point dipoles outside the LCA.

The sources are those identified by Astrium Stevenage, as already mentioned, whose positions in the S/C are known. The set itself, as well as the source magnetic parameters need to be updated, but the data used (which date back to November 2006) qualifies to verify the performance of the interpolation methods.

The magnetic field created by the dipole distribution at a generic point x and time [formula] is therefore given by

[formula]

where na =  [formula]/[formula] are unit vectors connecting the the a-th dipole ma with the field point x, and [formula] is the number of dipoles.

Fluctuations of the dipoles, both in modulus and direction, are unknown, but this is not essential to assess the numerical performance of the algorithm.

We aim to compare interpolated magnetic field results with exact ones within the context and scope of the above model. To artificially simulate several possible scenarios, we will take advantage of the uncertainties in the source dipole orientations to randomly generate different magnetic field patterns, which we intend to reconstruct based on the multipole expansion. More specifically, the procedure is the following one:

Each dipole has a known fixed position in the spacecraft, and a fixed modulus, also known. The number of magnetic dipoles is also fixed to 37, which is the number in Astrium's list.

The orientations of the dipoles are instead unknown. An example scenario is characterized by a specific selection of the 37 dipole orientations.

In order to explore the behavior of the algorithm, a batch of examples are examined, each corresponding to a randomly generated set of dipole orientations.

In each case, Eq. ([\ref=eq.13]) is solved for Mlm, and the field estimate at each TM is then calculated with Eq. ([\ref=eq.10]). In the last step, the result is compared with the theoretical one given in Eq. ([\ref=eq.15]), and the differences annotated.

Finally, a statistical analysis of the differences (errors) is done.

The random character of the procedure may seem unrealistic, since the actual satellite configuration is not random. In this context, however, randomness is an efficient way of mimicking lack of knowledge. As we will see in the next section, numerical analysis based on this methodology sheds much light on the merits of the interpolation procedure -- as it will also be the case when we come to neural networks performance in section [\ref=chap.5].

Simulation results

In this section we summarise the most relevant results of the analysis of the multipole interpolation method. We use a batch of 1 000 example scenarios such as described above. Magnetic moment orientations were chosen by randomly picking values of the two defining spherical angles (θ,φ) from two independent uniform distributions.

Figure [\ref=fig.2.1] graphically represents a magnetic field map in the LCA region corresponding to an arbitrarily chosen example out of the 1 000 considered. The valley structure is very clear in the [formula] plot, while the [formula] component shows a saddle shape -- see figure caption. [formula] and [formula] show qualitatively similar forms, and thus we do not show them. The elliptical forms in the estimate of [formula] are due to the quadratic combination of the field components. The estimate of [formula] shows instead a linear structure, with constant gradient in all directions. Naked eye inspection immediately reveals a poor resemblance between estimated and exact quantities, but let us elaborate some numerical data.

Figure [\ref=fig.3.1] displays the binned distribution of estimation errors, defined by

[formula]

where we have used a denominator [formula] in ε(Bx) to avoid meaningless infinities when [formula] is close to zero. [formula] and [formula] show similar trends and are not displayed. As can be seen, errors average to zero, but have rms deviations well above 100 %. Even worse, outliers are significant, as can be seen in Table [\ref=tab:table1], where averaged absolute values over the 1 000 simulated cases are displayed. Except, obviously, for the modulus error, we are around 500 %, but detailed examination of individual data further shows that errors as high as 2 000 % eventually happen.

The most salient features of the numerical analysis can be briefly summarized. Firstly we find that magnetic field estimation errors are very variable, ranging from very few percent to over 1 000 % and, secondly, these huge uncertainties happen in an utterly random and fully unpredictable way. The a posteriori conclusion is quite simple: the intrinsic linear character of the interpolation scheme is not capable of reproducing the field structure inside the LCA -- hence at the positions of the TMs -- and, therefore, can produce very good or very bad results just by accident. In addition to not being predictable, the average error is any case too large. The ultimate reason for such poor performance is the the small number of magnetometers as well as their positioning: four magnetometers only allow for a field multipole expansion up to quadrupole terms, which means that the field values at the TMs are just linearly interpolated between magnetometer readouts at the boundary of the LCA. On the other hand, the magnetometers are closer to the magnetic field sources than they are to the TMs, which prevents resolution of the spatial field structure details inside the LCA with only linear terms in the space coordinates.

A novel approach: neural networks

Search for an alternative approach to the above interpolation schemes is imperative, otherwise the information provided by the magnetometers will hardly be useful for the main goal of the LTP magnetic diagnostics system, i.e., to quantify the contribution of the magnetic noise to the total system noise. Here some promising results are presented on the implementation of a completely different methodology: neural networks [\cite=bib:Kecman].

Artificial neural networks are made up of interconnecting artificial neurons (programming constructs that mimic the properties of biological neurons) that have the capacity to learn from processing data. Neural networks are often used in solving nonlinear classification and regression tasks by learning from data, hence are worth trying with the present problem [\cite=bib:neuralPaper].

There are four sets of tasks which need to be implemented when solving a problem with artificial neural networks:

Neuron model selection

Model and architecture selection

Learning paradigm and learning algorithm selection

Performance assessment

We next go through them, one by one.

Neuron model

The neuron is the basic unit of any neural network. It performs the following two operations:

It collects the inputs from all other neurons connected to it and computes a weighted sum of the signals the latter inject into it, generally adding a bias as well. If we represent the inputs by a vector x [formula] [formula], and the weights by a w [formula] [formula] then this operation consists in calculating the sum

[formula]

where the superindex [formula] stands for transpose matrix; in this case, wT is a row vector while x is a column vector, so that [formula] is the scalar product of w and x. A term w0 is added to form the most general linear function of the vector argument x; it is called the bias.

The above sum is used as the argument to the so-called activation function, φ(Σ). The neuron's output, also known as its activation, is thus

[formula]

In general, φ(Σ) can be selected in many different ways. Here, differentiable activation functions will be used, which suit well the gradient descent back-propagation learning algorithm -- see sections below.

Neural network architecture

Artificial neural networks are software or hardware models inspired by the structure and behavior of biological systems, and they are created by a set of neurons distributed in layers. There are many different types of neural networks in use today, but the architecture of a so-called feed-forward network, where each layer of neurons is linked with the next by means of a set of weights, is the most commonly used, and will also be used here. The specific architecture adopted in this study is shown in figure [\ref=fig.4]. The data streams coming from the magnetometers will be considered the system inputs, while magnetic field results and their gradients at the positions of the test masses will be the system outputs.

Learning paradigms and learning/training algorithms

The investigation of learning algorithms is currently an active field of research. The design and implementation of an adequate training scheme is the essential ingredient for obtaining a good-quality estimate of the magnetic field and its gradient at the LTP TMs.

Learning paradigms

There are two major learning paradigms, each corresponding to a particular abstract learning task. These are supervised learning and unsupervised learning.

Supervised learning. The idea of this paradigm is quite clearly suggested by its very name. A set of examples is filed, each set consisting in a number of vector of inputs (the magnetometers' readouts in this case) and the corresponding values of the magnetic field and its gradient at the TMs for a given distribution of dipoles in the spacecraft. Let x represent a generic input vector, and y the associated vector output. These two vectors constitute an example. The set of filed examples for supervised learning is thus a set of pairs (x,y), where x[formula] and y[formula], [formula] and [formula] being some suitable sample spaces. The network is then fed the inputs x of one example and let it work out an output, o, say. This output is then compared with the correct one, y, and an error is calculated if o ≠   y. Iterations are then triggered to adjust the weighting factors such that the error is minimized. These will however vary as different examples are run, so a cost function is defined which enables the network to optimise the set of weights which works best for the set of examples analyzed, based on some suitable criterion.

Unsupervised learning. In unsupervised learning a cost function is to be minimized as well, but this function can be any relationship between x and the network output, o, but never taking into account the real expected target. The cost function is determined by the task formulation. Unsupervised learning is thus a form of self-adaptive system, whose guide is not an a priori knowledge of the final result but knowledge gained from experience.

In either case, the learning process is based on the architecture of the network, i.e., number of neurons and layers and their interconnections, as well as on the activation functions. These are parameters which, at least in the simplest cases, are tuned ab initio by the user based on observed performance of the network. In this study, supervised learning has been the implemented learning paradigm.

Learning algorithms

There are many algorithms for training neural networks. When training feed-forward neural networks with supervised learning, a back-propagation algorithm is usually implemented. The error of the mapping at the output is propagated backwards in order to readjust the weights and improve the output error for the next iteration. The propagation can be implemented with different methods, the Ideal Gradient Descent being a classic which will also be used here, with slight modifications that make the algorithm convergence faster.

Iterations on the weights of the different neurons at the different layers proceed according to the following algorithm:

[formula]

where [formula] labels the current iteration step, and [formula] is the learning rate, adjustable by the user. [formula] is the sum over the set of training examples of the square errors of the outputs:

[formula]

where [formula] stands for the number of examples, o is the (vector) output from the network, while y is the target, or correct output in the corresponding example. The quantity [formula] can only be defined in supervised learning, of course, and the idea of the above procedure is to find that point in weight space where [formula] is an absolute minimum. [formula] can therefore be considered the cost function to be minimized in this particular supervised training scheme, also known as batch mode as the analysis is done across the entire set of training patterns in a single block.

There are a number of technical issues in pursuing the iterations in Eq. ([\ref=eq.19]), such as the choice of the initial set of weights, the identification of local minima of [formula], the boundary effects, which need to be addressed in each specific case. We skip a detailed discussion of these matters here and we focus on the results obtained using our method. For further details, the reader is referred to Refs. [\cite=bib:Kecman] and [\cite=bib:neuralPrunning].

Performance assessment

In this last step, the trained network must be tested with examples which differ from those used in the learning process. This is needed to assess whether or not the trained neural network is able to generate the expected results when fed with previously unseen inputs, hence determine its usability for the specific purpose it is intended.

Results

Training and testing have been done based on different field realizations, using the same model of sources and magnetic field described in section [\ref=chap.3.1], i.e., each example will consist in the magnetic field at the magnetometers' positions, plus the magnetic field and gradient at the TM positions, all of them corresponding to a given configuration of the 37 Astrium dipoles.

Two different batches of examples, each including 1 000 realizations of a possible magnetic environment, have been generated following the directives explained in section [\ref=chap.3.1]. The first batch has been used as the training set for a neural network with 12 inputs (3 inputs for each of the 4 vector magnetometers) and 16 outputs representing the field information at the position of the two test masses (3 field plus 5 gradient components per test mass ). The second batch has been used for validation to assess the performance of the network in front of unseen magnetometers readings.

Field estimation

Figure [\ref=fig.5] shows the distribution of relative errors (in percentage) of the estimated components of the magnetic field at the positions of each TM. The plot is based on the results of the 1 000 validation runs described in the previous section. As can be observed, the order of magnitude of the errors of the estimated fields are now within much more acceptable margins (below ~  15%). This represents a reduction of estimation errors of more than one order of magnitude in comparison with the multipole expansion method.

During the training process, the neural network eventually learns that the magnetic field at the TMs is generally smaller than the magnetometers read -- with occasional exceptions due to the rich and complex structure of the field inside the LCA, see e.g. figure [\ref=fig.2.1]. The neural network is able to derive an inference procedure which is actually quite efficient, and it does so by proper adjustment of its weight matrix coefficients w as explained in section [\ref=chap.4.3.i]. In order to better understand the reaction of the trained neural network to the magnetometers' data, we found instructive and expedient to look into relationships between the data read by the magnetometers and the magnetic field estimates generated at the output of the neural network. We chose to calculate correlation coefficients between input and output data, and the results are displayed in Figure [\ref=fig.5.1]. The following major features are identified:

Each component of the field is basically estimated from the magnetometers reading of the same component. For example, the interpolation of the [formula] component in test mass 1 is mostly dependent on the [formula] readings of the magnetometers.

The measurements of the magnetometers closer to the interpolation points have larger weights. For instance, when the field is estimated at the position of TM1, to which M4 is the closest magnetometer, the value it measures is the largest contributor to the interpolated field in TM1. At the same time, M1 and M3 being nearly equidistant from both test masses, their weights are almost identical.

Gradient interpolation

The magnetic field gradient can also be estimated. The 9 components ∂Bi  /  ∂Bj of the gradient are not independent, since they must verify Eqs. ([\ref=eq.4]), which reduce their number to 5. The remaining 4 components can be easily calculated thereafter. Another option is to estimate the 9 gradient components regardless of the previously mentioned constraint, in which case they are actually found not to satisfy them. Discrepancies are however within the estimation error range, so we do not adopt this option here as it is slightly more cumbersome due to the correspondingly increased complexity of the network.

Results on gradient estimation are shown in Figure [\ref=fig.6] for Bx at the positions of both TMs. As can be observed, they are also within much more acceptable margins than the earlier interpolation approach could possibly produce. It is to be noted that no apparent or easily deductible physical relationship is found between the estimated gradient at the test mass positions and the magnetometer inputs, in contrast with what we have found for the field estimation.

Statistical analysis

In Table [\ref=tab:table2] we present a statistical comparison of the properties of the distribution of interpolated magnetic fields. For the sake of conciseness we only list the statistical properties of the interpolated modulus and x-component of the magnetic field. In particular, we show the standard deviation (σ) of the interpolating errors for both the multipole interpolation and the neural network estimate, the skewness of the distribution (γ1) and the corresponding kurtosis (γ2). Clearly, and as already mentioned, the interpolating errors are very large for the case in which a multipole interpolating scheme is used, as clearly shown by the very large standard deviation obtained when using this method. Also interesting to note is that for the case of the x-component of the magnetic field both methods yield distributions which are almost symmetrical. However this is not the case for the modulus of the magnetic field when the multipole interpolating method is used. Finally, the kurtosis of the multipole interpolation is very large, revealing a large number of outliers. All in all, a look at Table [\ref=tab:table2] reveals that the neural network method presents much better statistical properties than the multipole interpolation.

Conclusions

The magnetic diagnostics sensor set in the LTP is such that to infer the magnetic field and gradient at the positions of the TMs based on the readouts of the magnetometers is far from simple. The more standard interpolation scheme, based on a multipole expansion of the magnetic field inside the LCA volume, cannot go beyond quadrupole order which, in practice, means that just a linear approximation can be done, due to the reduced number of magnetometers available. This grossly fails to produce reliable results, with errors exceedingly large. This has motivated our search for better alternatives. Artificial neural networks have been presented as a more elaborate, non-linear procedure to estimate the required field values at the TM positions. In this paper we have presented results which very significantly improve the performance of the multipole expansion technique by almost two orders of magnitude. This a very encouraging outcome which points to the use of the neural networks as the baseline tool to analyse LTP magnetic data.

One of the main problems of using the neural network to assess the magnetic field at the positions of the test masses is to find a training process adequate to the set of data that the magnetometers will deliver in flight. This underlines the need to characterize on ground to our best ability the magnetic field distribution across the LCA for as many as possible foreseeable working conditions, both regarding DC and fluctuating values. Reliable information on this is essential for a meaningful assessment of magnetic noise in the LTP. However, the neural network analyses presented in this paper only apply to static fields. What they actually show is that neural networks work very well (~   10 % accuracies) no matter which the source dipole configuration is. A different issue, which is beyond the scope of this paper and it is currently under investigation, is how to deal with time series of magnetometer readouts, which is of course the kind of data the satellite will transmit to ground. Features such as trends, field fluctuations, will likely happen during mission operations, and the neural network algorithm must be trained to properly deal with them. Preliminary results indicate that the network is able to deal with moderate trends and levels of fluctuations, but further effort is needed to explore alternatives, e.g. self-adaptability, which will make more robust the performance of the system.