Lemma Corollary Proposition Remark Example

Symmetries in Linear and Integer Programs

Introduction

Order, beauty and perfection - these are the words we typically associate with symmetry. Generally, we expect the structure of objects with many symmetries to be uniform and regular, thus not too complicated. Therefore, symmetries usually are very welcomed in many scientific areas, especially in mathematics. However, in integer programming, the reverse seems to be true. In practice, highly symmetric integer programs often turn out to be particularly hard to solve. The problem is that branch-and-bound or branch-and-cut algorithms, which are commonly used to solve integer programs, work efficiently only if the bulk of the branches of the search tree can be pruned. Since symmetry in integer programs usually entails many equivalent solutions, the branches belonging to these solutions cannot be pruned, which leads to a very poor performance of the algorithm.

Only in the last few years first efforts were made to tackle this irritating problem. In 2002, Margot presented an algorithm that cuts feasible integer points without changing the optimal value of the problem, compare [\cite=margot1]. Improvements and generalizations of this basic idea can be found in [\cite=margot2] [\cite=margot3]. In [\cite=linderoth1] [\cite=linderoth2], Linderoth et al. concentrate on improving branching methods for packing and covering integer problems by using information about the symmetries of the integer programs. Another interesting approach to these kind of problems has been developed by Kaibel and Pfetsch. In [\cite=kaibel1], the authors introduce special polyhedra, called orbitopes, which they use in [\cite=kaibel2] to remove redundant branches of the search tree. Friedman's fundamental domains in [\cite=friedman] are also aimed at avoiding the evaluation of redundant solutions. For selected integer programs like generalized bin-packing problems there exists a completely different idea how to deal with symmetries, see e.g. [\cite=fekete]. Instead of eliminating the effects of symmetry during the branch-and-bound process, the authors exclude symmetry already in the formulation of the problem by choosing an appropriate representation for feasible packings.

All ideas in the aforementioned papers finally rely on the branch-and-bound algorithm, or they are only applicable to selected problems. In contrast to this optimizational or specialized point of view, we want to approach the topic from a more general and algebraic angle and detach ourselves from the classical optimization methods like branch-and-bound. In this paper we will examine symmetries of linear programs in their natural environment, the field of group theory. Our main objective aims at a better understanding of the role of symmetry in the context of linear and integer programming. In a subsequent paper we will discuss symmetries of integer programs.

Preliminaries

Optimization problems whose solutions must satisfy several constraints are called restricted optimization problems. If all constraints as well as the objective function are linear, we call them linear programs, LP for short. The linearity of such problems suggests the following canonical formulation for arbitrary LP problems.

[formula]

where [formula] and [formula]. We are especially interested in points that are candidates for solutions of an LP. Hence, the set of feasible points X of ([\ref=LP1_ohne_rnpl]) is given by

[formula]

We can interpret the feasible region of an LP in a geometric sense. The following definition is adopted from [\cite=schrijver], p. 87. Note that every row of the system Ax  ≤  b defines an affine half-space. Obviously, the set X is a polyhedron. Since every affine half-space is convex, the intersection of affine half-spaces - hence, any polyhedron - is convex as well. Therefore, we can now state the convexity of X.

The feasible region of an LP is convex.

Whenever we consider linear programs, we are particularly interested in points with maximal utility values that satisfy all the constraints. If we additionally insist on integrality of the solution, we get a so-called integer program, IP for short. According to the LP formulation in ([\ref=LP1_ohne_rnpl]), the appropriate formulation for the related IP is given by

[formula]

where [formula] and [formula]. Analogously, the set of feasible points XI of ([\ref=IP1_ohne_rnpl]) is given by

[formula]

We now want to clarify the meaning of the term symmetry in the context of linear and integer programming.

Symmetries

In general, symmetries are automorphisms, that is, operations that map an object to itself in a bijective way compatible with its structure. Concerning linear and integer programs, we therefore have to consider operations that preserve both the utility vector and the inequality system, thus, in particular, the polyhedron which is described by the inequality system. By the usage of matrix notation, this polyhedron is already embedded in Euclidean space [formula]. This is the point where we have to decide whether we want to regard [formula] as an affine or as a linear space. In respect of the algorithms we are going to develop, we follow the general tendency in the literature and choose the linear perspective for the sake of a simpler group structure. Hence, the operations we consider are automorphisms of the linear space [formula], that is, elements of the general linear group [formula]. Furthermore, it is reasonable to restrict the set of possible symmetries even to isometries taking into account that the angles and the lengths of the edges of the polyhedron need to be preserved. Since the set of all automorphisms of an object always is a group, we therefore suggest that the symmetries of a linear or an integer program form a subgroup of the orthogonal group [formula]. In general, a linear program and the associated integer program need not have the same symmetries. The following two examples illustrate this fact.

Since we are forced to rely on the linear description of an integer program to gain information about its symmetries, we want to make sure that any symmetry of a linear program is a symmetry of the associated integer program as well. As integer programs are naturally confined to the standard lattice [formula], we only consider orthogonal operations that leave the lattice invariant. In particular, such an operation represented by a matrix [formula] maps any standard basis vector ej to an integer vector

[formula]

which is the j-th column of the matrix M. Hence, all columns of M have to be integral, that is,

[formula]

where [formula] is the group of all integrally invertible matrices. Note that orthogonal matrices with integral entries always are integrally invertible. We want to learn more about [formula]. Since any map [formula] preserves the distance, the column [formula] is an integer vector of length 1, thus

[formula]

for 1  ≤  j  ≤  n. Therefore, the set [formula] only consists of signed permutation matrices. In fact, since every signed permutation matrix is orthogonal and integral, the set of signed permutation matrices is equal to [formula]. Apparently, the group [formula] acts on the set

[formula]

The kernel of this action is the group of sign-changes denoted by [formula]. Hence, the group [formula] is a normal subgroup of [formula]. Furthermore, any element of [formula], that is, any signed permutation matrix M has a unique representation M = DP, where D is a sign-changing matrix, thus a diagonal matrix with entries ±  1 on its diagonal, and P is a permutation matrix. Therefore, we have

[formula]

where [formula] denotes the subgroup of all (n  ×  n)-permutation matrices. Since [formula] and Pn intersect trivially, we finally conclude that [formula] splits over [formula].

The group [formula] is the semidirect product

[formula]

In the literature, the group [formula] appears in the context of finite reflection groups. More precisely, the group [formula] is the Coxeter group Bn of rank n, compare [\cite=humphreys], p. 5. Due to the invariance of the standard lattice [formula] under [formula], the elements of [formula] have the potential to satisfy the strict requirements we made on symmetries. That is, elements of [formula] that preserve an LP, i.e., its inequality system and its utility vector, also leave the associated IP invariant.

The invariance of an LP under an element of [formula] implies the invariance of the related IP under the same element.

However, the reverse does not hold in general, since we can always add asymmetric cuts to an LP without affecting the set of feasible points of the corresponding IP, compare Figure 1b. We could now define symmetries of linear and integer programs as elements of [formula] that leave invariant the inequality system and the utility vector of the problem. But if we take into account the usual linear and integer programming constraint [formula], which forces non-negativity of the solutions, the set of possible symmetries shrinks from [formula] to the group of permutation matrices [formula].

Now, how should we imagine the action of a symmetry group G  ≤  Pn on a linear or an integer program? Since G is an automorphism group of [formula], its elements permute the standard basis vectors [formula]. Considering the bijective G-equivariant mapping

[formula]

the permutation of the subscripts of the basic vectors is an action of a group [formula] on the set of indices [formula] which is isomorphic to the action of G on the standard basis. Hence, we can always think of symmetry groups of linear or integer programs as subgroups of [formula].

A group [formula] acts on the linear space [formula] via the G-equivariant mapping

[formula]

where B is the set of the standard basis vectors [formula] of [formula].

Due to Remark [\ref=LP_IP_symmetries_equal], we are able to formulate the definition of symmetries of linear programs and the corresponding integer programs simultaneously. Consider an LP of the form

[formula]

where [formula] and [formula]. Note that the LP ([\ref=LP1]) and the IP ([\ref=IP1]) have the additional constraint [formula]. Apparently, applying a permutation to the matrix A according to Remark [\ref=action_of_Sn_on_Rn] translates into permuting the columns of A. Since the ordering of the inequalities does not affect the object they describe, we need to allow for arbitrary row permutations of the matrix A. The following definition takes these thoughts into account. This is a definition of symmetry as it can be found in literature as well, see e.g. [\cite=margot2].

Unfortunately, we cannot predict the effect on the symmetry group in general if we add constraints to the inequality system. This is impossible even in the special case where the corresponding polyhedron stays unaltered, as we will see in Example [\ref=ex_poly_ineq_not_same_sym]. However, in some cases we can at least guarantee that the symmetry group of the inequality system does not get smaller.

Let g∈G be a symmetry of Ax  ≤  b via the row permutation [formula], and a symmetry of A'x  ≤  b' via [formula], that is,

[formula]

Then we get

[formula]

and

[formula]

Hence, the permutation g is a symmetry of the inequality system

[formula]

via the row permutation [formula].

Orbits

The following basic terms, notations and first insights into actions of symmetry groups on linear programs will turn out to be useful. If G is the symmetry group of an LP with the feasible region X, the group G leaves X invariant. Hence, a point x is feasible if and only if all elements of xG are feasible as well.

Given a symmetry group [formula] of an LP Λ, a point x is feasible for Λ if and only if every element of the orbit xG is feasible for Λ.

The following theorem states that applying symmetries does not change the value of the utility function.

By definition, every symmetry g∈G fixes the utility vector c. Therefore, we have

[formula]

for every element xg of xG.

The orbits of two elements [formula] are equal if and only if x and [formula] are equivalent, i.e., there exists an element g∈G with [formula]. Acting on the standard basis [formula] of [formula], the group G splits B into k disjoint orbits. Formulating an LP problem, the variables can be named in an arbitrary way. Therefore, we can always assume that the decomposition into orbits is aligned to the order of the basis B, in the following sense:

Without loss of generality, the orbits of G on B are given by

[formula]

for [formula], where k is the number of orbits, ni the number of elements in orbit Oi, and si is defined by [formula].

Applying Theorem [\ref=constant_objective_value] to a unit vector ei, we get some important information about the structure of the utility vector c.

Let ei,ej∈B be two elements of the same orbit O under a group G. Then the entries ci and cj of the utility vector c are equal.

Referring to Remark [\ref=ordered_orbits], the utility vector c has the following structure:

[formula]

We do not want to suppress the fact that there are other ways to define symmetries of linear programs. Apart from the question whether to consider affine or linear transformations, another important subject needs to be put up for discussion. As already mentioned in the introduction, the original motivation for the study of such symmetries was the unnecessarily large size of the branch-and-cut trees caused by symmetric solutions sharing the same utility value. Hence, we should focus on operations that leave invariant the utility vector and the feasible region, which is the polyhedron described by the inequality system of the linear program. Now obviously, many different inequality systems give rise to the same polyhedron. Therefore, the invariance of an inequality system implies the invariance of the polyhedron, but the reverse is not true, as the following example illustrates:

Consider the LP given by the utility vector c = (1,1)t and the inequality system Obviously, the permutation [formula] is a symmetry of the inequality system, thus a symmetry of the feasible region. If we add the redundant constraint

[formula]

the new inequality system describes the same polyhedron. Hence g still is a symmetry of the feasible region, but the inequality system itself does not show any symmetry anymore. Adding another redundant constraint

[formula]

we retrieve the original symmetry group of the inequality system, again without changing the symmetry group of the feasible region.

So why did we choose this restrictive definition of symmetries for linear and integer programs? The main problem is the lack of apposite descriptions of the feasible region. The inequality system is the only source of information in this context, and the conversion into a description that provides direct access to the symmetries of the feasible region might already be equivalent to solving the problem itself.

The Set of Fixed Points

Symmetries in linear programs do not attract much attention in the literature, maybe because they do not influence the performance of standard solving procedures like the simplex algorithm in a negative way. But even though linear programs are solvable in polynomial time, it is always worth looking for generic methods to save calculation time. In this section, we will focus on the question how symmetries can contribute to this goal.

As it will turn out later in this section, the points in [formula] that are fixed by a permutation group [formula] play the key role in our approach. Hence, we will use the first part of this section to tame these points by means of linear algebra. Recall that a group [formula] acts on [formula] as described in Remark [\ref=action_of_Sn_on_Rn], thus we interpret G as a linear group. In terms of linear algebra, the set of fixed points [formula] is the eigenspace [formula] corresponding to the eigenvalue 1. Since [formula] is the intersection of all of those eigenspaces, the structure of [formula] is not arbitrary.

The set of fixed points [formula] with respect to a group [formula] is a subspace of [formula].

Now let [formula] be a symmetry group of a linear program Λ, compare ([\ref=LP1]). Then the utility vector c of the linear program is fixed by every g in G. Since G acts as a linear group, the line

[formula]

is pointwise fixed by every g in G, that is, the line l is in [formula] for every g in G, thus in the intersection of these sets.

The line l through the origin spanned by c is a subspace of the set of fixed points [formula].

We are particularly interested in the exact dimension of [formula]. By Remark [\ref=l_in_FixGR], we already know that [formula] is at least one-dimensional. To determine its dimension precisely, we first need to consider the dimension of a certain subspace of [formula].

Let O be a subset of the standard basis B of [formula] and [formula] a group acting transitively on O. Then the intersection [formula] of the span V: = 〈O〉 and [formula] is determined by

[formula]

In particular, the subspace [formula] is one-dimensional.

Without loss of generality, let [formula]. Since O is invariant under G, the vector

[formula]

is fixed by G, thus

[formula]

and further [formula]. In order to prove the converse inclusion, it suffices to show that the dimension of [formula] is not greater than 1. To this end, we define the (m - 1)-dimensional subspaces Wj  ≤  V by

[formula]

Assume that the dimension of [formula] is greater than 1. By the dimension formula, we then have

[formula]

for every [formula]. In particular, there exists a vector

[formula]

with at least one coefficient al  ≠  0. Since G acts transitively on O, we find an element g∈G that maps el to em. Being an element of [formula], the vector w is fixed by g. Thus, we can write

[formula]

contradicting the fact that [formula]. Consequently, we have [formula], and therefore

[formula]

By Lemma [\ref=orbitspan_intersects_fix_one-dim], we are now able to establish a direct relation between the dimension of [formula] and the number of orbits generated by G. For orbits and the corresponding spans we use the notation we introduced in Section [\ref=section_orbits].

In both parts of the proof we will use the fact that [formula] is a subspace of [formula], which we already know by Remark [\ref=FixGR_is_subspace]. We start with the proof for the special representation of [formula].

Since the set of orbits [formula] is a partition of the basis B of [formula], we have

[formula]

for i  ≠  j, and further

[formula]

Thus, we can write

[formula]

Hence, any point [formula] has a unique representation [formula], where vi∈Vi. For this representation, we get for any g∈G

[formula]

The uniqueness of the representation implies that g maps each vi to a certain vj∈Vj of the representation. But since every subspace Vi is invariant under G, we get vgi∈Vi, thus vgi = vi, due to ([\ref=dim_FixGR_equals_number_of_orbits_Vi_trivial_intersection]). Hence, we have proved the inclusion

[formula]

The converse inclusion is immediate, thus we finally get

[formula]

In order to prove the statement on the dimension of [formula], we recall that G acts transitively on every orbit Oi. Therefore, Lemma [\ref=orbitspan_intersects_fix_one-dim] yields

[formula]

for all [formula]. Using i), the dimension of [formula] can therefore be computed as

[formula]

The statement in Theorem [\ref=dim_FixGR_equals_number_of_orbits] is particularly interesting if the group G generates only one single orbit.

If G acts transitively on the standard basis B, the set of fixed points [formula] is one-dimensional.

We complete our studies on the set of fixed points with a simple example.

Consider the LP given by

[formula]

subject to where

[formula]

Then the LP has the full symmetry group [formula]. In this special case, the set of fixed points [formula] coincides with the line l through the origin spanned by the utility vector c, compare Remark [\ref=l_in_FixGR]. The following figure shows the graphical representation of the LP.

Now we focus on the solutions of the linear program given in Example [\ref=ex_set_of_fixed_points]. Obviously, the point [formula] is the solution of the LP provided by the simplex algorithm. In fact, all points on the bold line parallel to the hyperplane ctx = z are solutions of the LP. In particular, this is also true for the intersection point [formula]. Hence, by generalizing Example [\ref=ex_set_of_fixed_points], we get to the assumption that for any n-dimensional linear program with full symmetry group [formula], we can always find a solution in the associated set of fixed points [formula]. We will check this assumption in the following section.

Solutions in the Set of Fixed Points

Before we turn to the main issue, we need to introduce a special representation of the barycenter of an orbit, which plays an essential role in our approach.

Given [formula], the barycenter of the orbit xG can be written as follows:

[formula]

Since the stabilizer Gx is a subgroup of G, we have

[formula]

Let [formula] be a set of representatives of the family of cosets Gxg. Then

[formula]

Furthermore, the orbit-stabilizer theorem yields the relation

[formula]

so we get

[formula]

Since xg = x for all g∈Gx, we have

[formula]

and therefore

[formula]

Considering the disjoint representation

[formula]

of G, we finally obtain

[formula]

The representation of the barycenter provided by Lemma [\ref=representation_barycenter] facilitates the proof of the following statement about feasible points in the set of fixed points.

We define

[formula]

Since [formula] is the barycenter of xG, it belongs to the convex hull of xG. The feasibility of the elements of xG, compare Remark [\ref=feasibility_of_orbit], and the convexity of X now imply that [formula] is feasible, too. Applying Lemma [\ref=representation_barycenter] and the linearity of G, we have

[formula]

for all g'∈G. This proves that [formula] is a fixed point of G, thus [formula]. By Theorem [\ref=constant_objective_value], we already know that

[formula]

for all g∈G, hence

[formula]

This shows that [formula] has the same utility value as x.

The application of Theorem [\ref=feasible_point_in_FixGR] to a solution [formula] of Λ leads to a remarkable result. The following corollary, which records this result, is of vital importance, since it prepares the ground for the algorithm we are going to present subsequent to this theoretical part.

If Λ has a solution, there also exists a solution [formula].

In particular, this result shows that the existence of a solution of Λ implies the existence of a solution of Λ restricted to [formula]. Furthermore, the point [formula] - and therefore every solution of the restricted LP - has the same objective value as a solution of Λ. Consequently, we only need to solve the restricted problem to get a solution for the original LP. As we will see in the next section, this kind of relationship between two LP problems can be very useful. Therefore, we introduce an appropriate partial order [formula] on the family of LP problems of dimension n reflecting this relationship. Obviously, the relation [formula] is reflexive, asymmetric and transitive, and thus a partial order.

Before we turn to practical aspects, we want to direct attention to a special property of the result in Corollary [\ref=solution_in_FixGR]. The statement connotes that the symmetry of a linear program is always reflected in one of its solutions. This is what W. C. Waterhouse calls the Purkiss Principle in his studies on the question:

Do symmetric problems have symmetric solutions?

In one of his papers, see [\cite=waterhouse], he gives a list of concrete examples for this principle, but he also shows that this property can not be taken for granted.

Substitutions and Retractions

In this section our goal is to benefit from the results of the previous section by exploiting the ordering of an LP and its restriction to the set of fixed points with respect to [formula]. The following theorem yields a detailed insight into this relation.

Let O be the set of orbits O of G on B. Referring to Remark [\ref=ordered_orbits], we consider the orbits O to be of the form

[formula]

Let [formula] be the subspace of [formula] defined by

[formula]

In order to project every Vi onto [formula], we define the linear maps fPi by

[formula]

Hence, the first element of Oi is mapped to the sum of the elements of Oi, while the other elements of Oi are mapped to 0. The n  ×  n-matrix Pi corresponding to fPi is defined by

[formula]

where [formula]. According to Theorem [\ref=dim_FixGR_equals_number_of_orbits], we have

[formula]

Therefore, we are now able to define the map [formula] by

[formula]

where

[formula]

By Corollary [\ref=solution_in_FixGR], we know that the restricted LP

[formula]

The transitivity of [formula] now implies that the LP ([\ref=LP_in_PLP_preceq_LP]) is less or equal than Λ.

Variables of a linear program that are tied together in one orbit are closely related. Therefore, we introduce a notation for sets of such variables In order to translate the result of Theorem [\ref=PLP_preceq_LP] into an applicable algorithm, we perform the so-called substitution procedure computing t = ctP and Â = AP in the LP ([\ref=LP_in_PLP_preceq_LP]). According to the definition of P, see ([\ref=P_def]), the resulting LP is given by

[formula]

where

[formula]

and

[formula]

Straightforward computation yields

[formula]

Furthermore, we have

[formula]

This representation reveals that - except for the constraint [formula] - the inequality system of the new LP does not depend on the variables

[formula]

for all [formula]. Furthermore, the coefficient of the representative xsi - 1 + 1 of each set [formula] accumulates the original coefficients of all variables in [formula]. Therefore, we can interpret this procedure as a simultaneous substitution of the elements of each [formula] by the representatives xsi - 1 + 1. By Theorem [\ref=PLP_preceq_LP], we already know that [formula] is less or equal than Λ. Hence, we only need to solve [formula] to obtain a solution of Λ. This fact can be expressed in the following way.

Every solution of [formula] is a solution of Λ as well.

A first application of the substitution procedure to a basic example will shed light on the effectiveness and the potential of the algorithm.

Consider the following LP Λ0 given by the inequality system Ax  ≤  b and the utility vector c, where

[formula]

and

[formula]

We can expand this LP to and

[formula]

Obviously, we can exchange x1 and x2 without affecting c or the inequality system if we exchange x3 and x4 at the same time. Therefore, this LP has

[formula]

as a symmetry group, and G divides B into the two orbits O1  =  {e1,e2} and O2  =  {e3,e4}. Applying the substitution procedure to the set of orbits O  =  {O1,O2}, we obtain the new LP [formula] defined by Âx  ≤  b and [formula], where

[formula]

and

[formula]

The expanded version of the new LP is given by and

[formula]

where [formula]. According to Corollary [\ref=solution_of_SubLP_solves_LP], we only need to solve the LP [formula] which is almost independent of the variables x2 and x4.

Note that the LP [formula] can actually be derived from the original LP by substituting x1 for x2 and x3 for x4. Furthermore, we observe that we do not use any detailed knowledge about the structure of the group G except for the specific decomposition of B into orbits. Therefore, we can apply the substitution procedure to any LP problem with known orbit decomposition even if we do not have any additional information about the group structure of the symmetry group G of the linear program.

Regarding Theorem [\ref=PLP_preceq_LP] and the substitution procedure, the assumption of having a certain group G can be relaxed to the assumption of having a certain orbit decomposition.

Except for the constraint [formula], the LP [formula] is completely independent of certain variables. Therefore, we now focus on a reduction of the dimension of the LP. This reduction can be realized by a certain operator, which we are now going to introduce. Note that in contrast to the n-dimensional LP [formula], the dimension of [formula] is equal to the number of orbits, which coincides with the dimension of the set of fixed points [formula], see Theorem [\ref=dim_FixGR_equals_number_of_orbits].

Given a linear program Λ with the set of orbits [formula], the retract [formula] of [formula] is a linear program of dimension k.

To justify the term retraction, we introduce an appropriate inclusion ι satisfying

[formula]

The retraction r applied to the LP [formula] eliminates the zeros in the representations ([\ref=c_hat_with_zeros]) and ([\ref=A_hat_with_zeros]) of [formula] and Â. Conversely, the inclusion ι reintroduces these zeros in the following sense: Obviously, Mι can be written as

[formula]

where ei is the i-th unit vector in [formula]. Referring to the block representation [formula] given in ([\ref=P_def]), we have

[formula]

for every [formula], and therefore

[formula]

Using the property PP = P of the projection matrix P, we finally get

[formula]

and

[formula]

This shows that [formula], and thus

[formula]

With respect to this category theoretical property, we now want to show that we only need to solve the retract [formula] of [formula]. For this, we analyze the linear maps

[formula]

by considering the corresponding matrices Mr and Mι. On the one hand, the retraction r maps any element of [formula] to an element of [formula]. On the other hand, the map ι applied to a vector [formula] picks exactly the representative xsi - 1 + 1 of each set [formula]. Concerning the LP problems [formula] and [formula], this behavior has the following effect.

Let X be the feasible region of [formula]. Then the following statements hold:

If y is feasible for [formula], then x: = Mry is feasible for [formula].

The feasible region of [formula] is given by Y: = MιX.

The LP problems [formula] and [formula] have the same maximal utility value.

We will use the statement in [\ref=regain_h_feasible_Mry]) to prove [\ref=regain_h_feasible_regionY]), and the representation in [\ref=regain_h_feasible_regionY]) to show [\ref=regain_h_same_maxVal]).

Let y be a feasible point of [formula]. Since y is in [formula] and r maps [formula] to [formula], the point x = Mry is in [formula]. Moreover, we have

[formula]

that is, the point x is feasible for [formula].

Let x be in X. Then x is feasible for [formula], and thus

[formula]

Therefore, we have Px = x and [formula]. By the equality MrMι = P, see ([\ref=M_rM_iota_equals_P]), we obtain

[formula]

that is, Mιx is feasible for [formula]. Conversely, let y be a feasible point of [formula]. According to [\ref=regain_h_feasible_Mry]), the point x = Mry is feasible for [formula]. Straightforward computation yields

[formula]

Hence, y can be written as

[formula]

Therefore, any feasible point of [formula] is in MιX. Conclusively, the set Y = MιX defines the feasible region of [formula].

Since X is a subset of [formula], the definition of Y given in [\ref=regain_h_feasible_regionY]) yields

[formula]

Therefore, we can write

[formula]

hence the optimal values of [formula] and [formula] are equal.

The relations MιMr = Ik and MrMιx = x for all [formula] which we used in our proof reveal in particular that ι and r are bijective and mutually inverse if we restrict ι to [formula]. The following corollary records this interesting relationship.

The linear maps

[formula]

are bijective and mutually inverse.

The following theorem proves that we only need to solve [formula] instead of [formula]. Furthermore, it provides a method how to regain a solution of [formula] from a solution of [formula].

The proof essentially relies on Lemma [\ref=regain_h].

Let [formula] be a solution of [formula]. We show that [formula] defined by

[formula]

is a solution of [formula]. By part [\ref=regain_h_feasible_regionY]) of Lemma [\ref=regain_h], the feasible region of [formula] is given by Y: = MιX. Since [formula] is in X, the point [formula] is feasible for [formula]. According to [\ref=regain_h] [\ref=regain_h_same_maxVal]), we have

[formula]

that is, the point [formula] is a solution of [formula].

Let [formula] be a solution of [formula]. Using [\ref=regain_h] [\ref=regain_h_feasible_Mry]), the point [formula] is feasible for [formula]. By [\ref=regain_h] [\ref=regain_h_same_maxVal]), we now get

[formula]

This shows that [formula] is a solution of [formula].

Combining Corollary [\ref=solution_of_SubLP_solves_LP] and Theorem [\ref=regain_h], we conclude that it suffices to solve the k-dimensional retract [formula], compare Remark [\ref=dim_RetLP], in order to obtain a solution of [formula], which then is a solution of the original n-dimensional linear program Λ.

Finally, we resume Example [\ref=ex_subsAlgo] to study the effects of the final two steps of the algorithm.

Example [\ref=ex_subsAlgo] (continued). Consider the LP [formula] defined by

[formula]

Then the LP [formula] is given by

[formula]

where

[formula]

and

[formula]

Expanding [formula], we get and

[formula]

Obviously, this LP can be solved at a glance. The solution is given by

[formula]

In order to get a solution of [formula], we multiply [formula] by Mr. By Theorem [\ref=regain], the point

[formula]

is a solution of [formula]. Finally, Corollary [\ref=solution_of_SubLP_solves_LP] guarantees that [formula] is a solution of Λ0 as well.

In the procedure we developed, we take advantage of symmetries by deriving a linear program of smaller dimension, which still contains enough information to extract a solution of the original LP. The elaboration of our method revealed that the complexity of the derived linear program solely depends on the number of orbits, not on the concrete structure of the symmetry group. Therefore, transitivity of the symmetry group suffices to obtain the best possible result. But even the knowledge about one single symmetry of a linear program already effects a reduction of the dimension, since every symmetry generates a symmetry group of the linear program and reduces the number of orbits. Sometimes, the derived linear program [formula] shows further symmetries, even if we already considered the full symmetry group of the original problem. In that case, we can apply the substitution algorithm iteratively. Of course, it is not clear how to determine symmetries of arbitrary linear programs. But in practice, some of the symmetries already attract attention during the construction of the linear programs. For instance, think of the graph-coloring problem, where it is obvious that the variables representing the colors can be exchanged. Therefore, the substitution procedure or algorithm should be understood not so much as a part of the solving process, but as a pre-processing step in order to produce a lower-dimensional linear program. In this respect, it would be reasonable to formulate linear programs as symmetric as possible.