=1

=←

Building Rules on Top of Ontologiesfor the Semantic Webwith Inductive Logic Programming

Introduction

During the last decade increasing attention has been paid on ontologies and their role in Knowledge Engineering [\cite=StaabS04]. In the philosophical sense, we may refer to an ontology as a particular system of categories accounting for a certain vision of the world. As such, this system does not depend on a particular language: Aristotle's ontology is always the same, independently of the language used to describe it. On the other hand, in its most prevalent use in Artificial Intelligence, an ontology refers to an engineering artifact (more precisely, produced according to the principles of Ontological Engineering [\cite=GomezPerez04]), constituted by a specific vocabulary used to describe a certain reality, plus a set of explicit assumptions regarding the intended meaning of the vocabulary words. This set of assumptions has usually the form of a first-order logical theory, where vocabulary words appear as unary or binary predicate names, respectively called concepts and relations. In the simplest case, an ontology describes a hierarchy of concepts related by subsumption relationships; in more sophisticated cases, suitable axioms are added in order to express other relationships between concepts and to constrain their intended interpretation. The two readings of ontology described above are indeed related each other, but in order to solve the terminological impasse the word conceptualization is used to refer to the philosophical reading as appear in the following definition, based on [\cite=Gruber93]: An ontology is a formal explicit specification of a shared conceptualization for a domain of interest. Among the other things, this definition emphasizes the fact that an ontology has to be specified in a language that comes with a formal semantics. Only by using such a formal approach ontologies provide the machine interpretable meaning of concepts and relations that is expected when using an ontology-based approach. Among the formalisms proposed by Ontological Engineering, the most currently used are Description Logics (DLs) [\cite=BaaderCMcGNPS03]. Note that DLs are decidable fragments of First Order Logic (FOL) that are incomparable with Horn Clausal Logic (HCL) as regards the expressive power [\cite=Borgida96] and the semantics [\cite=Rosati05].

Ontology Engineering, notably its DL-based approach, is playing a relevant role in the definition of the Semantic Web. The Semantic Web is the vision of the World Wide Web enriched by machine-processable information which supports the user in his tasks [\cite=Berners-Lee01]. The architecture of the Semantic Web is shown in Figure [\ref=fig:sw-architecture]. It consists of several layers, each of which is equipped with an ad-hoc mark-up language. In particular, the design of the mark-up language for the ontological layer, OWL, has been based on the very expressive DL () [\cite=HorrocksST00] [\cite=HorrocksP-SvH03]. Whereas OWL is already undergoing the standardization process at W3C, the debate around a unified language for rules is still ongoing. Proposals like SWRL extend OWL with constructs inspired to Horn clauses in order to meet the primary requirement of the logical layer: 'to build rules on top of ontologies'. SWRL is intended to bridge the notorious gaps between DLs and HCL in a way that is similar in the spirit to hybridization in Knowledge Representation and Reasoning (KR&R) systems such as [formula]-log [\cite=Donini98]. Generally speaking, hybrid systems are KR&R systems which are constituted by two or more subsystems dealing with distinct portions of a single knowledge base by performing specific reasoning procedures [\cite=FrischC91]. The motivation for investigating and developing such systems is to improve on two basic features of KR&R formalisms, namely representational adequacy and deductive power, by preserving the other crucial feature, i.e. decidability. In particular, combining DLs with HCL can easily yield to undecidability if the interface between them is not reduced [\cite=LevyR98]. The hybrid system [formula]-log integrates [formula] [\cite=Schmidt-Schauss91] and Datalog [\cite=Ceri90] by using [formula] concept assertions essentially as type constraints on variables. It has been very recently mentioned as the blueprint for well-founded Semantic Web rule mark-up languages because its underlying form of integration (called safe) assures semantic and computational advantages that SWRL - though more expressive than [formula]-log - currently can not assure [\cite=Rosati05].

Defining rules (including the ones for the Semantic Web) has been usually considered as a demanding task from the viewpoint of Knowledge Engineering. It is often supported by Machine Learning algorithms that can vary in the approaches. The approach known under the name of Inductive Logic Programming (ILP) seems to be promising for the case at hand due to the common roots with Logic Programming [\cite=FlachL02]. ILP has been historically concerned with rule induction from examples and background knowledge within the representation framework of HCL and with the aim of prediction [\cite=Nienhuys97]. More recently ILP has moved towards either different FOL fragments (e.g., DLs) or new learning goals (e.g., description). In this paper we resort to the methodological apparatus of ILP to define a general framework for learning rules on top of ontologies for the Semantic Web within the KR&R framework of [formula]-log. The framework proposed is general in the sense that it is valid whatever the scope of induction (description vs. prediction) is. For the sake of illustration we concentrate on an instantiation of the framework for the case of description.

The paper is organized as follows. Section [\ref=sect:al-log] introduces the basic notions of [formula]-log. Section [\ref=sect:learning-fwk] defines the framework for learning rules in [formula]-log. Section [\ref=sect:inst-fwk] illustrates an instantiation of the framework. Section [\ref=sect:concl] concludes the paper with final remarks. [\ref=sect:owl] clarifies the links between OWL and DLs.

Basics of [formula]-log

The system [formula]-log [\cite=Donini98] integrates two KR&R systems: Structural and relational.

The structural subsystem

The structural part Σ is based on [formula] [\cite=Schmidt-Schauss91] and allows for the specification of knowledge in terms of classes (concepts), binary relations between classes (roles), and instances (individuals). Complex concepts can be defined from atomic concepts and roles by means of constructors (see Table [\ref=tab:ALC]). Also Σ can state both is-a relations between concepts (axioms) and instance-of relations between individuals (resp. couples of individuals) and concepts (resp. roles) (assertions). An interpretation I = (ΔI,  ·  I) for Σ consists of a domain ΔI and a mapping function ·  I. In particular, individuals are mapped to elements of ΔI such that aI  ≠  bI if a  ≠  b ( Unique Names Assumption (UNA) [\cite=Reiter80b]). If O  ⊆  ΔI and [formula], I is called O-interpretation. Also Σ represents many different interpretations, i.e. all its models (Open World Assumption (OWA) [\cite=BaaderCMcGNPS03]).

The main reasoning task for Σ is the consistency check. This test is performed with a tableau calculus that starts with the tableau branch S  =  Σ and adds assertions to S by means of propagation rules such as

[formula] if

[formula] is in S,

D = C1 and D = C2,

neither s:C1 nor s:C2 is in S

[formula] if

[formula] is in S,

sRt is in S,

t:C is not in S

[formula] if

[formula] is in S,

s appears in S,

[formula] is the NNF concept equivalent to [formula]

[formula] is not in S

[formula] if

s:A and [formula] are in S, or

[formula] is in S,

[formula] is not in S

until either a contradiction is generated or an interpretation satisfying S can be easily obtained from it.

The relational subsystem

The relational part of [formula]-log allows one to define Datalog programs enriched with constraints of the form s:C where s is either a constant or a variable, and C is an [formula]-concept. Note that the usage of concepts as typing constraints applies only to variables and constants that already appear in the clause. The symbol & separates constraints from Datalog atoms in a clause.

A constrained Datalog clause is an implication of the form

[formula]

where m  ≥  0, n  ≥  0, αi are Datalog atoms and γj are constraints. A constrained Datalog program Π is a set of constrained Datalog clauses.

An [formula]-log knowledge base B is the pair 〈Σ,Π〉 where Σ is an [formula] knowledge base and Π is a constrained Datalog program. For a knowledge base to be acceptable, it must satisfy the following conditions:

The set of Datalog predicate symbols appearing in Π is disjoint from the set of concept and role symbols appearing in Σ.

The alphabet of constants in Π coincides with the alphabet O of the individuals in Σ. Furthermore, every constant in Π appears also in Σ.

For each clause in Π, each variable occurring in the constraint part occurs also in the Datalog part.

These properties state a safe interaction between the structural and the relational part of an [formula]-log knowledge base, thus solving the semantic mismatch between the OWA of [formula] and the CWA of Datalog [\cite=Rosati05]. This interaction is also at the basis of a model-theoretic semantics for [formula]-log. We call ΠD the set of Datalog clauses obtained from the clauses of Π by deleting their constraints. We define an interpretation J for B as the union of an O-interpretation IO for Σ (i.e. an interpretation compliant with the unique names assumption) and an Herbrand interpretation IH for ΠD. An interpretation J is a model of B if IO is a model of Σ, and for each ground instance [formula] of each clause [formula] in Π, either there exists one [formula], [formula], that is not satisfied by J, or [formula] is satisfied by J. The notion of logical consequence paves the way to the definition of answer set for queries. Queries to [formula]-log knowledge bases are special cases of Definition [\ref=def:constr-datalog-clause]. An answer to the query Q is a ground substitution σ for the variables in Q. The answer σ is correct w.r.t. a [formula]-log knowledge base B if Qσ is a logical consequence of B ([formula]). The answer set of Q in B contains all the correct answers to Q w.r.t. B.

Reasoning for [formula]-log knowledge bases is based on constrained SLD-resolution [\cite=Donini98], i.e. an extension of SLD-resolution to deal with constraints. In particular, the constraints of the resolvent of a query Q and a constrained Datalog clause E are recursively simplified by replacing couples of constraints t:C, t:D with the equivalent constraint [formula]. The one-to-one mapping between constrained SLD-derivations and the SLD-derivations obtained by ignoring the constraints is exploited to extend known results for Datalog to [formula]-log. Note that in [formula]-log a derivation of the empty clause with associated constraints does not represent a refutation. It actually infers that the query is true in those models of B that satisfy its constraints. Therefore in order to answer a query it is necessary to collect enough derivations ending with a constrained empty clause such that every model of B satisfies the constraints associated with the final query of at least one derivation.

Let Q(0) be a query [formula] to a [formula]-log knowledge base B . A constrained SLD-refutation for Q(0) in B is a finite set [formula] of constrained SLD-derivations for Q(0) in B such that:

for each derivation di, 1  ≤  i  ≤  s, the last query Q(ni) of di is a constrained empty clause;

for every model J of B, there exists at least one derivation di, 1  ≤  i  ≤  s, such that [formula]

Constrained SLD-refutation is a complete and sound method for answering ground queries [\cite=Donini98].

Let Q be a ground query to an [formula]-log knowledge base B. It holds that [formula] if and only if [formula].

An answer σ to a query Q is a computed answer if there exists a constrained SLD-refutation for Qσ in B ([formula]). The set of computed answers is called the success set of Q in B. Furthermore, given any query Q, the success set of Q in B coincides with the answer set of Q in B. This provides an operational means for computing correct answers to queries. Indeed, it is straightforward to see that the usual reasoning methods for Datalog allow us to collect in a finite number of steps enough constrained SLD-derivations for Q in B to construct a refutation - if any. Derivations must satisfy both conditions of Definition [\ref=def:constrained-refutation]. In particular, the latter requires some reasoning on the structural component of B. This is done by applying the tableau calculus as shown in the following example.

Constrained SLD-resolution is decidable [\cite=Donini98]. Furthermore, because of the safe interaction between [formula] and Datalog, it supports a form of closed world reasoning, i.e. it allows one to pose queries under the assumption that part of the knowledge base is complete [\cite=Rosati05].

The general framework for learning rules in [formula]-log

In our framework for learning in [formula]-log we represent inductive hypotheses as constrained Datalog clauses and data as an [formula]-log knowledge base B. In particular B is composed of a background knowledge K and a set O of observations. We assume [formula].

To define the framework we resort to the methodological apparatus of ILP which requires the following ingredients to be chosen:

the language L of hypotheses

a generality order [formula] for L to structure the space of hypotheses

a relation to test the coverage of hypotheses in L against observations in O w.r.t. K

The framework is general, meaning that it is valid whatever the scope of induction (description/prediction) is. Therefore the Datalog literal [formula] in the head of hypotheses represents a concept to be either discriminated from others (discriminant induction) or characterized (characteristic induction).

This section collects and upgrades theoretical results published in [\cite=LisiM03-aiia] [\cite=LisiM03-ilp] [\cite=LisiE04-ilp].

The language of hypotheses

To be suitable as language of hypotheses, constrained Datalog clauses must satisfy the following restrictions.

First, we impose constrained Datalog clauses to be linked and connected (or range-restricted) as usual in ILP [\cite=Nienhuys97].

Let H be a constrained Datalog clause. A term t in some literal li∈H is linked with linking-chain of length 0, if t occurs in head(H), and is linked with linking-chain of length d + 1, if some other term in li is linked with linking-chain of length d. The link-depth of a term t in some li∈H is the length of the shortest linking-chain of t. A literal li∈H is linked if at least one of its terms is linked. The clause H itself is linked if each li∈H is linked. The clause H is connected if each variable occurring in head(H) also occur in body(H).

Second, we impose constrained Datalog clauses to be compliant with the bias of Object Identity (OI) [\cite=Semeraro98]. This bias can be considered as an extension of the UNA from the semantic level to the syntactic one of [formula]-log. We would like to remind the reader that this assumption holds in [formula]. Also it holds naturally for ground constrained Datalog clauses because the semantics of [formula]-log adopts Herbrand models for the Datalog part and O-models for the constraint part. Conversely it is not guaranteed in the case of non-ground constrained Datalog clauses, e.g. different variables can be unified. The OI bias can be the starting point for the definition of either an equational theory or a quasi-order for constrained Datalog clauses. The latter option relies on a restricted form of substitution whose bindings avoid the identification of terms.

A substitution σ is an OI-substitution w.r.t. a set of terms T iff [formula] yields that t1σ  ≠  t2σ.

From now on, we assume that substitutions are OI-compliant.

The generality relation

In ILP the key mechanism is generalization intended as a search process through a partially ordered space of hypotheses [\cite=Mitchell82]. The definition of a generality relation for constrained Datalog clauses can disregard neither the peculiarities of [formula]-log nor the methodological apparatus of ILP. Therefore we rely on the reasoning mechanisms made available by [formula]-log knowledge bases and propose to adapt Buntine's generalized subsumption [\cite=Buntine88] to our framework as follows.

Let H be a constrained Datalog clause, α a ground Datalog atom, and J an interpretation. We say that H covers α under J if there is a ground substitution θ for H (Hθ is ground) such that body(H)θ is true under J and head(H)θ  =  α.

Let H1, H2 be two constrained Datalog clauses and B an [formula]-log knowledge base. We say that H1 B-subsumes H2 if for every model J of B and every ground atom α such that H2 covers α under J, we have that H1 covers α under J.

We can define a generality relation [formula] for constrained Datalog clauses on the basis of B-subsumption. It can be easily proven that [formula] is a quasi-order (i.e. it is a reflexive and transitive relation) for constrained Datalog clauses.

Let H1, H2 be two constrained Datalog clauses and B an [formula]-log knowledge base. We say that H1 is at least as general as H2 under B-subsumption, [formula], iff H1 B-subsumes H2. Furthermore, H1 is more general than H2 under B-subsumption, [formula], iff [formula] and [formula]. Finally, H1 is equivalent to H2 under B-subsumption, H1  ~  BH2, iff [formula] and [formula].

The next lemma shows the definition of B-subsumption to be equivalent to another formulation, which will be more convenient in later proofs than the definition based on covering.

Let B be an [formula]-log knowledge base and H be a constrained Datalog clause. Let [formula] be all the variables appearing in H, and [formula] be distinct constants (individuals) not appearing in B or H. Then the substitution [formula] is called a Skolem substitution for H w.r.t. B.

Let H1, H2 be two constrained Datalog clauses, B an [formula]-log knowledge base, and σ a Skolem substitution for H2 with respect to [formula]. We say that [formula] iff there exists a ground substitution θ for H1 such that (i) head(H1)θ = head(H2)σ and (ii) [formula].

The relation between B-subsumption and constrained SLD-resolution is given below. It provides an operational means for checking B-subsumption.

Let H1, H2 be two constrained Datalog clauses, B an [formula]-log knowledge base, and σ a Skolem substitution for H2 with respect to [formula]. We say that [formula] iff there exists a substitution θ for H1 such that (i) head(H1)θ = head(H2) and (ii) [formula] where body(H1)θσ is ground.

By Lemma [\ref=lemma:B-subsumption1], we have [formula] iff there exists a ground substitution [formula] for H1, such that [formula] and [formula]. Since σ is a Skolem substitution, we can define a substitution θ such that [formula] and none of the Skolem constants of σ occurs in θ. Then head(H1)θ = head(H2) and [formula]. Since body(H1)θσ is ground, by Lemma [\ref=lemma:ground-query-answering] we have [formula], so the thesis follows.

The decidability of B-subsumption follows from the decidability of both generalized subsumption in Datalog [\cite=Buntine88] and query answering in [formula]-log [\cite=Donini98].

Coverage relations

When defining coverage relations we make assumptions as regards the representation of observations because it impacts the definition of coverage. In ILP there are two choices: we can represent an observation as either a ground definite clause or a set of ground unit clauses. The former is peculiar to the normal ILP setting (also called learning from implications) [\cite=FrazierP93], whereas the latter is usual in the logical setting of learning from interpretations [\cite=DeRaedtD94]. The representation choice for observations and the scope of induction are orthogonal dimensions as clearly explained in [\cite=DeRaedt97]. Therefore we prefer the term 'observation' to the term 'example' for the sake of generality.

In the logical setting of learning from entailment extended to [formula]-log, an observation oi∈O is represented as a ground constrained Datalog clause having a ground atom [formula] in the head.

Let H∈L be a hypothesis, K a background knowledge and oi∈O an observation. We say that H covers oi under entailment w.r.t K iff [formula].

In order to provide an operational means for testing this coverage relation we resort to the Deduction Theorem for first-order logic formulas [\cite=Nienhuys97].

Let Σ be a set of formulas, and φ and ψ be formulas. We say that [formula] iff [formula].

Let H∈L be a hypothesis, K a background knowledge, and oi∈O an observation. We say that H covers oi under entailment w.r.t. K iff [formula].

The following chain of equivalences holds:

H covers oi under entailment w.r.t. K ↔   (by Definition [\ref=def:coverage_ent])

[formula] ↔   (by Theorem [\ref=thm:ded-thm])

[formula] ↔   (by Lemma [\ref=lemma:ground-query-answering])

[formula]

In the logical setting of learning from interpretations extended to [formula]-log, an observation oi∈O is represented as a couple [formula] where Ai is a set containing ground Datalog facts concerning the individual i.

Let H∈L be a hypothesis, K a background knowledge and oi∈O an observation. We say that H covers oi under interpretations w.r.t. K iff [formula].

Let H∈L be a hypothesis, K a background knowledge, and oi∈O an observation. We say that H covers oi under interpretations w.r.t. K iff [formula].

Since [formula] is a ground query to the [formula]-log knowledge base [formula], the thesis follows from Definition [\ref=def:coverage_int] and Lemma [\ref=lemma:ground-query-answering].

Note that both coverage tests can be reduced to query answering.

Instantiating the framework for Ontology Refinement

Ontology Refinement is a phase in the Ontology Learning process that aims at the adaptation of an existing ontology to a specific domain or the needs of a particular user [\cite=MaedcheS04]. In this section we consider the problem of Concept Refinement which is about refining a known concept, called reference concept, belonging to an existing taxonomic ontology in the light of new knowledge coming from a relational data source. A taxonomic ontology is an ontology organized around the is-a relationship between concepts [\cite=GomezPerez04]. We assume that a concept C consists of two parts: an intension int(C) and an extension ext(C). The former is an expression belonging to a logical language L whereas the latter is a set of objects that satisfy the former. More formally, given

a taxonomic ontology Σ,

a reference concept Cref∈Σ,

a relational data source Π,

a logical language L

the Concept Refinement problem is to find a taxonomy G of concepts Ci such that (i) int(Ci)∈L and (ii) ext(Ci)  ⊂  ext(Cref). Therefore G is structured according to the subset relation between concept extensions. Note that Cref is among both the concepts defined in Σ and the symbols of L. Furthermore ext(Ci) relies on notion of satisfiability of int(Ci) w.r.t. [formula]. We would like to emphasize that B includes Σ because in Ontology Refinement, as opposite to other forms of Ontology Learning such as Ontology Extraction (or Building), it is mandatory to consider the existing ontology and its existing connections. Thus, a formalism like [formula]-log suits very well the hybrid nature of B (see Section [\ref=sect:kr]).

In our ILP approach the Ontology Refinement problem at hand is reformulated as a Concept Formation problem [\cite=LisiE07-ilp]. Concept Formation indicates a ML task that refers to the acquisition of conceptual hierarchies in which each concept has a flexible, non-logical definition and in which learning occurs incrementally and without supervision [\cite=Langley87]. More precisely, it is to take a large number of unlabeled training instances: to find clusterings that group those instances in categories: to find an intensional definition for each category that summarized its instances; and to find a hierarchical organization for those categories [\cite=GennariLF89]. Concept Formation stems from Conceptual Clustering [\cite=MichalskiS83]. The two differ substantially in the methods: The latter usually applies bottom-up batch algorithms whereas the former prefers top-down incremental ones. Yet the methods are similar in the scope of induction, i.e. prediction, as opposite to (Statistical) Clustering [\cite=Hartigan01] and Frequent Pattern Discovery [\cite=Mannila97] whose goal is to describe a data set. According to the commonly accepted formulation of the task [\cite=Langley87] [\cite=GennariLF89], Concept Formation can be decomposed in two sub-tasks:

clustering

characterization

The former consists of using internalised heuristics to organize the observations into categories whereas the latter consists in determining a concept (that is, an intensional description) for each extensionally defined subset discovered by clustering. We propose a pattern-based approach for the former (see Section [\ref=sect:phase1]) and a bias-based approach for the latter (see Section [\ref=sect:phase2]). In particular, the clustering approach is pattern-based because it relies on the aforementioned commonalities between Clustering and Frequent Pattern Discovery. Descriptive tasks fit the ILP setting of characteristic induction [\cite=DeRaedtD97]. A distinguishing feature of this form of induction is the density of solution space. The setting of learning from interpretations has been shown to be a promising way of dealing with such spaces [\cite=Blockeel99].

Let L be a hypothesis language, K a background knowledge, O a set of observations, and M(B) a model constructed from [formula]. The goal of characteristic induction from interpretations is to find a set H  ⊆  L of hypotheses such that (i) H is true in M(B), and (ii) for each H∈L, if H is true in M(B) then [formula].

In the following subsection we will clarify the nature of K and O.

Representation Choice

The KR&R framework for conceptual knowledge in the Concept Refinement problem at hand is the one offered by [formula]-log.

The taxonomic ontology Σ is a [formula] knowledge base. From now on we will call input concepts all the concepts occurring in Σ.

Throughout this Section, we will refer to the [formula] ontology [formula] (see Figure [\ref=fig:cia-owl]) concerning countries, ethnic groups, languages, and religions of the world, and built according to Wikipedia taxonomies. For instance, the expression MMMM MMMMMM MM [formula]. is an equivance axiom that defines the concept [formula] as an Asian country which hosts at least one Middle Eastern ethnic group. In particular, Armenia ('ARM') and Iran ('IR') are two of the 15 countries that are classified as Middle Eastern.

The relational data source Π is a Datalog program. The extensional part of Π is partitioned into portions Ai each of which refers to an individual ai of Cref. The link between Ai and ai is represented with the Datalog literal q(ai). The pair (q(ai),Ai) is called observation. The intensional part (IDB) of Π together with the whole Σ is considered as background knowledge for the problem at hand.

An [formula]-log knowledge base [formula] has been obtained by integrating [formula] and a Datalog program [formula] based on the on-line 1996 CIA World Fact Book. The extensional part of [formula] consists of Datalog facts grouped according to the individuals of MiddleEastCountry. In particular, the observation [formula] contains Datalog facts such as MMMMM MMMMM MM language('IR','Persian',58). religion('IR','ShiaMuslim',89). religion('IR','SunniMuslim',10). concerning the individual 'IR'. The intensional part of [formula] defines two views on language and religion: that can deduce new Datalog facts when triggered on [formula]. It forms the background knowledge [formula] together with the whole [formula].

The language L contains expressions, called O-queries, relating individuals of Cref to individuals of other input concepts (task-relevant concepts). An O-query is a constrained Datalog clause of the form

[formula],

which is compliant with the properties mentioned in Section [\ref=sect:hyp-lang]. The O-query

Qt  =  q(X)←&X:Cref

is called trivial for L because it only contains the constraint for the distinguished variable X. Furthermore, the language L is multi-grained, i.e. it contains expressions at multiple levels of description granularity. Indeed it is implicitly defined by a declarative bias specification which consists of a finite alphabet A of Datalog predicate names appearing in Π and finite alphabets Γl (one for each level l of description granularity) of [formula] concept names occurring in Σ. Note that αi's are taken from A and γj's are taken from Γl. We impose L to be finite by specifying some bounds, mainly maxD for the maximum depth of search and maxG for the maximum level of granularity.

We want to refine the concept [formula] belonging to [formula] in the light of the new knowledge coming from [formula]. More precisely, we want to describe Middle East countries (individuals of the reference concept) with respect to the religions believed and the languages spoken (individuals of the task-relevant concepts) at three levels of granularity (maxG = 3). To this aim we define [formula] as the set of O-queries with Cref  =   that can be generated from the alphabet MMMM MM MM A= {believes/2, speaks/2} of Datalog binary predicate names, and the alphabets MMMM MM MM Γ1= {Language, Religion} Γ2= [formula] Γ3= [formula] of [formula] concept names for 1  ≤  l  ≤  3, up to maxD = 5. Note that the names in A are taken from [formula] whereas the names in Γl's are taken from [formula]. Examples of O-queries in [formula] are: MMMMMM MM MM Qt= q(X) ← & X:MiddleEastCountry Q1= q(X) ← speaks(X,Y) & X:MiddleEastCountry, Y:Language Q2= q(X) ← speaks(X,Y) & X:MiddleEastCountry, Y:IndoEuropeanLanguage Q3= q(X) ← believes(X,Y)& X:MiddleEastCountry, Y:MuslimReligion where Qt is the trivial O-query for [formula], [formula], [formula], and [formula].

Output concepts are the concepts automatically formed out of the input ones by taking into account the relational data source. Thus, an output concept C has an O-query Q∈L as intension and the set [formula] of correct answers to Q w.r.t. B as extension. Note that this set contains the substitutions θi's for the distinguished variable of Q such that there exists a correct answer to body(Q)θi w.r.t. B. In other words, the extension is the set of individuals of Cref satisfying the intension. Also with reference to Section [\ref=sect:cover-test] note that proving that an O-query Q covers an observation (q(ai),Ai) w.r.t. K equals to proving that θi  =  {X  /  ai} is a correct answer to Q w.r.t. [formula].

The output concept having Q1 as intension has extension [formula] = {'ARM', 'IR', 'SA', 'UAE'}. In particular, Q1 covers the observation [formula] w.r.t. [formula]. This coverage test is equivalent to answering the query ← q('IR') w.r.t. [formula].

Output concepts are organized into a taxonomy G rooted in Cref and structured as a Directed Acyclic Graph (DAG) according to the subset relation between concept extensions. Note that one such ordering is in line with the set-theoretic semantics of the subsumption relation in ontology languages (see, e.g., the semantics of [formula] in [formula]).

Pattern-based clustering

Frequent Pattern Discovery is about the discovery of regularities in a data set [\cite=Mannila97]. A frequent pattern is an intensional description, expressed in a language L, of a subset of a given data set r whose cardinality exceeds a user-defined threshold (minimum support). Note that patterns can refer to multiple levels of description granularity (multi-grained patterns) [\cite=HanF99]. Here r typically encompasses a taxonomy T. More precisely, the problem of frequent pattern discovery at l levels of description granularity, 1  ≤  l  ≤  maxG, is to find the set F of all the frequent patterns expressible in a multi-grained language L  =  {Ll}1  ≤  l  ≤  maxG and evaluated against r w.r.t. a set {minsupl}1  ≤  l  ≤  maxG of minimum support thresholds by means of the evaluation function supp. In this case, P∈Ll with support s is frequent in r if (i) s  ≥  minsupl and (ii) all ancestors of P w.r.t. T are frequent in r. The blueprint of most algorithms for frequent pattern discovery is the levelwise search method [\cite=Mannila97] which searches the space [formula] of patterns organized according to a generality order [formula] in a breadth-first manner, starting from the most general pattern in L and alternating candidate generation and candidate evaluation phases. The underlying assumption is that [formula] is a quasi-order monotonic w.r.t. supp. Note that the method proposed in [\cite=Mannila97] is also at the basis of algorithms for the variant of the task defined in [\cite=HanF99].

A frequent pattern highlights a regularity in r, therefore it can be considered as the clue of a data cluster. Note that clusters are concepts partially specified (called emerging concepts): only the extension is known. We propose to detect emerging concepts by applying the method of [\cite=LisiM04] for frequent pattern discovery at l, 1  ≤  l  ≤  maxG, levels of description granularity and k, 1  ≤  k  ≤  maxD, levels of search depth. It adapts [\cite=Mannila97] [\cite=HanF99] to the KR&R framework of [formula]-log as follows. For L being a multi-grained language of O-queries, we need to define first supp, then [formula]. The support of an O-query Q∈L w.r.t. an [formula]-log knowledge base B is defined as

[formula]

and supplies the percentage of individuals of Cref that satisfy Q.

Since [formula], then [formula].

Being a special case of constrained Datalog clauses, O-queries can be ordered according to the B-subsumption relation introduced in Section [\ref=sect:gen-rel]. It has been proved that [formula] is a quasi-order that fulfills the condition of monotonicity w.r.t. supp [\cite=LisiM04]. Also note that the underlying reasoning mechanism of [formula]-log makes B-subsumption more powerful than generalized subsumption as illustrated in the following example.

It can be checked that [formula] by choosing σ={X/a, Y/b} as a Skolem substitution for Q2 w.r.t. [formula] and [formula] as a substitution for Q1. Similarly it can be proved that [formula]. Furthermore, it can be easily verified that Q3 B-subsumes the following O-query in [formula] MMMMMM MM MM Q4= q(A) ← believes(A,B), believes(A,C)& A:MiddleEastCountry, B:MuslimReligion by choosing σ={A/a, B/b, C/c} as a Skolem substitution for Q4 w.r.t. [formula] and θ={X/A, Y/B} as a substitution for Q3. Note that [formula] under the OI bias. Indeed this bias does not admit the substitution {A/X, B/Y, C/Y} for Q4 which would make it possible to verify conditions (i) and (ii) of the [formula] test.

We would like to emphasize that Σ, besides contributing to the definition of L (see Section [\ref=sect:kr]), plays a key role in the [formula] test.

Bias-based characterization

Since several frequent patterns can have the same set of supporting individuals, turning clusters into concepts is crucial in our approach. Biases can be of help. A bias concerns anything which constrains the search for theories [\cite=UtgoffM82]. In ILP language bias has to do with constraints on the clauses in the search space whereas search bias has to do with the way a system searches its space of permitted clauses [\cite=NedellecRABT96]. The choice criterion for concept intensions has been obtained by combining two orthogonal biases: a language bias and a search bias [\cite=LisiE06-ecai]. The former allows the user to define conditions on the form of O-queries to be accepted as concept intensions. E.g., it is possible to state which is the minimum level of description granularity (parameter minG) and whether (all) the variables must be ontologically constrained or not. The latter allows the user to define a preference criterion based on B-subsumption. More precisely, it is possible to state whether the most general description (m.g.d.) or the most specific description (m.s.d.) w.r.t. [formula] has to be preferrred. Since [formula] is not a total order, it can happen that two patterns P and Q, belonging to the same language L, can not be compared w.r.t. [formula]. In this case, the m.g.d. (resp. m.s.d) of P and Q is the union (resp. conjunction) of P and Q.

The patterns MMMM MMM MMM MMM MMM q(A) ← speaks(A,B), believes(A,C) & A:MiddleEastCountry, B:ArabicLanguage and MMMM MMM MMM MMM MMM q(A) ← believes(A,B), speaks(A,C) & A:MiddleEastCountry, B:MuslimReligion have the same answer set {'ARM', 'IR'} but are incomparable w.r.t. [formula]. Their m.g.d. is the union of the two: MMMM MMM MMM MMM MMM q(A) ← speaks(A,B), believes(A,C) & A:MiddleEastCountry, B:ArabicLanguage q(A) ← believes(A,B), speaks(A,C) & A:MiddleEastCountry, B:MuslimReligion Their m.s.d. is the conjunction of the two: MMMM MMM MMM MMM MMM q(A) ← believes(A,B), speaks(A,C), speaks(A,D), believes(A,E) & A:MiddleEastCountry, B:MuslimReligion, C:ArabicLanguage The extension of the subsequent concept will be {'ARM', 'IR'}.

The two biases are combined as follows. For each frequent pattern P∈L that fulfills the language bias specification, the procedure for building the taxonomy G from the set F  =  {Flk|1  ≤  l  ≤  maxG,1  ≤  k  ≤  maxD} checks whether a concept C with ext(C) = answerset(P) already exists in G. If one such concept is not retrieved, a new node C with int(C) = P and ext(C) = answerset(P) is added to G. Note that the insertion of a node can imply the reorganization of G to keep it compliant with the subset relation on extents. If the node already occurs in G, its intension is updated according to the search bias specification.

Experimental Results

In order to test the approach we have extended the ILP system [formula]-QuIn [\cite=Lisi06-ppswr] with a module for post-processing frequent patterns into concepts. The goal of the experiments is to provide an empirical evidence of the orthogonality of the two biases and of the potential of their combination as choice criterion. The results reported in the following are obtained for the problem introduced in Example [\ref=ex:sw-pat] by setting the parameters for the frequent pattern discovery phase as follows: maxD = 5, maxG = 3, minsup1 = 20%, minsup2 = 13%, and minsup3 = 10%. Thus each experiment starts from the same set F of 53 frequent patterns out of 99 candidate patterns. Also all the experiments require the descriptions to have all the variables ontologically constrained but vary as to the user preferences for the minimum level of description granularity (minG) and the search bias (m.g.d./m.s.d.).

The first two experiments both require the descriptions to have all the variables ontologically constrained by concepts from the second granularity level on (minG = 2). When the m.g.d. criterion is adopted, the procedure of taxonomy building returns the following twelve concepts: MMMM MMM MMM MMM MMM ∈F11 q(A) ← A:MiddleEastCountry {ARM, BRN, IR, IRQ, IL, JOR, KWT, RL, OM, Q, SA, SYR, TR, UAE, YE} MMMM MMM MMM MMM MMM ∈F23 q(A) ← believes(A,B) & A:MiddleEastCountry, B:MonotheisticReligion {ARM, BRN, IR, IRQ, IL, JOR, KWT, RL, OM, Q, SA, SYR, TR, UAE} MMMM MMM MMM MMM MMM ∈F23 q(A) ← speaks(A,B) & A:MiddleEastCountry, B:AfroAsiaticLanguage {IR, SA, YE} MMMM MMM MMM MMM MMM ∈F23 q(A) ← speaks(A,B) & A:MiddleEastCountry, B:IndoEuropeanLanguage {ARM, IR} MMMM MMM MMM MMM MMM ∈F25 q(A) ← speaks(A,B), believes(A,C) & A:MiddleEastCountry, B:AfroAsiaticLanguage, C:MonotheisticReligion {IR, SA} MMMM MMM MMM MMM MMM ∈F25 q(A) ← believes(A,B), believes(A,C) & A:MiddleEastCountry, B:MonotheisticReligion, C:MonotheisticReligion {BRN, IR, IRQ, IL, JOR, RL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,'Druze') & A:MiddleEastCountry {IL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,B) & A:MiddleEastCountry, B:JewishReligion {IR, IL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,B) & A:MiddleEastCountry, B:ChristianReligion {ARM, IR, IRQ, IL, JOR, RL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,B) & A:MiddleEastCountry, B:MuslimReligion {BRN, IR, IRQ, IL, JOR, KWT, RL, OM, Q, SA, SYR, TR, UAE} MMMM MMM MMM MMM MMM ∈F35 q(A) ← believes(A,B), believes(A,C) & A:MiddleEastCountry, B:ChristianReligion, C:MuslimReligion {IR, IRQ, IL, JOR, RL, SYR} MMMM MMM MMM MMM MMM ∈F35 q(A) ← believes(A,B), believes(A,C) & A:MiddleEastCountry, B:MuslimReligion, C:MuslimReligion {BRN, IR, SYR} organized in the DAG [formula] (see Figure [\ref=fig:taxonomy1a]). They are numbered according to the chronological order of insertion in [formula] and annotated with information of the generation step. From a qualitative point of view, concepts [formula] and [formula] well characterize Middle East countries. Armenia (ARM), as opposite to Iran (IR), does not fall in these concepts. It rather belongs to the weaker characterizations [formula] and [formula]. This suggests that our procedure performs a 'sensible' clustering. Indeed Armenia is a well-known borderline case for the geo-political concept of Middle East, though the Armenian is usually listed among Middle Eastern ethnic groups. Modern experts tend nowadays to consider it as part of Europe, therefore out of Middle East. But in 1996 the on-line CIA World Fact Book still considered Armenia as part of Asia.

When the m.s.d. criterion is adopted (see Figure [\ref=fig:taxonomy1b]), the intensions for the concepts [formula], [formula], [formula], [formula] and [formula] change as follows: MMMM MMM MMM MMM MMM ∈F23 q(A) ← speaks(A,B) & A:MiddleEastCountry, B:ArabicLanguage {IR, SA, YE} MMMM MMM MMM MMM MMM ∈F23 q(A) ← speaks(A,B) & A:MiddleEastCountry, B:IndoIranianLanguage {ARM, IR} MMMM MMM MMM MMM MMM ∈F25 q(A) ← speaks(A,B), believes(A,C) & A:MiddleEastCountry, B:ArabicLanguage, C:MuslimReligion {IR, SA} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,'Druze'), believes(A,B), believes(A,C), believes(A,D) & A:MiddleEastCountry, B:JewishReligion, C:ChristianReligion, D:MuslimReligion {IL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,B), believes(A,C), believes(A,D) & A:MiddleEastCountry, B:JewishReligion, C:ChristianReligion, D:MuslimReligion {IR, IL, SYR} In particular [formula] and [formula] look quite overfitted to data. Yet overfitting allows us to realize that what distinguishes Israel (IL) and Syria (SYR) from Iran is just the presence of Druze people. Note that the clusters do not change because the search bias only affects the characterization step.

The other two experiments further restrict the conditions of the language bias specification. Here only descriptions with variables constrained by concepts of granularity from the third level on (minG = 3) are considered. When the m.g.d. option is selected, the procedure for taxonomy building returns the following nine concepts: MMMM MMM MMM MMM MMM ∈F11 q(A) ← A:MiddleEastCountry {ARM, BRN, IR, IRQ, IL, JOR, KWT, RL, OM, Q, SA, SYR, TR, UAE, YE} MMMM MMM MMM MMM MMM ∈F33 q(A) ← speaks(A,B) & A:MiddleEastCountry, B:ArabicLanguage {IR, SA, YE} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,'Druze') & A:MiddleEastCountry {IL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,B) & A:MiddleEastCountry, B:JewishReligion {IR, IL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,B) & A:MiddleEastCountry, B:ChristianReligion {ARM, IR, IRQ, IL, JOR, RL, SYR} MMMM MMM MMM MMM MMM ∈F33 q(A) ← believes(A,B) & A:MiddleEastCountry, B:MuslimReligion {BRN, IR, IRQ, IL, JOR, KWT, RL, OM, Q, SA, SYR, TR, UAE} MMMM MMM MMM MMM MMM ∈F35 q(A) ← speaks(A,B), believes(A,C) & A:MiddleEastCountry, B:ArabicLanguage, C:MuslimReligion {IR, SA} MMMM MMM MMM MMM MMM ∈F35 q(A) ← believes(A,B), believes(A,C) & A:MiddleEastCountry, B:ChristianReligion, C:MuslimReligion {IR, IRQ, IL, JOR, RL, SYR} MMMM MMM MMM MMM MMM ∈F35 q(A) ← believes(A,B), believes(A,C) & A:MiddleEastCountry, B:MuslimReligion, C:MuslimReligion {BRN, IR, SYR} organized in a DAG [formula] (see Figure [\ref=fig:taxonomy2a]) which partially reproduces [formula]. Note that the stricter conditions set in the language bias cause three concepts occurring in [formula] not to appear in [formula]: the scarsely significant [formula] and [formula], and the quite interesting [formula]. Therefore the language bias can prune the space of clusters. Note that the other concepts of [formula] emerged at l = 2 do remain in [formula] as clusters but with a different characterization: [formula] and [formula] instead of [formula] and [formula], respectively.

When the m.s.d. condition is chosen (see Figure [\ref=fig:taxonomy2b]), the intensions for the concepts [formula] and [formula] change analogously to [formula]. Note that both [formula] and [formula] are hierarchical taxonomies. It can be empirically observed that the possibility of producing a hierarchy increases as the conditions of the language bias become stricter.

Conclusions

Building rules on top of ontologies for the Semantic Web is a task that can be automated by applying Machine Learning algorithms to data expressed with hybrid formalisms combining DLs and Horn clauses. Learning in DL-based hybrid languages has very recently attracted attention in the ILP community. In [\cite=RouveirolV2000] the chosen language is Carin-[formula], therefore example coverage and subsumption between two hypotheses are based on the existential entailment algorithm of Carin [\cite=LevyR98]. Following [\cite=RouveirolV2000], Kietz studies the learnability of Carin-[formula], thus providing a pre-processing method which enables ILP systems to learn Carin-[formula] rules [\cite=Kietz03]. Closely related to DL-based hybrid systems are the proposals arising from the study of many-sorted logics, where a first-order language is combined with a sort language which can be regarded as an elementary DL [\cite=Frisch91]. In this respect the study of a sorted downward refinement [\cite=Frisch99] can be also considered a contribution to learning in hybrid languages. In this paper we have proposed a general framework for learning in [formula]-log. We would like to emphasize that the DL-safeness and the decidability of [formula]-log are two desirable properties which are particularly appreciated both in ILP and in the Semantic Web application area.

As an instantiation of the framework we have considered the case of characteristic induction from interpretations, more precisely the task of Frequent Pattern Discovery, and an application to Ontology Refinement. The specific problem at hand takes an ontology as input and returns subconcepts of one of the concepts in the ontology. A distinguishing feature of our setting for this problem is that the intensions of these subconcepts are in the form of rules that are automatically built by discovering strong associations between concepts in the input ontology. The idea of resorting to Frequent Pattern Discovery in Ontology Learning has been already investigated in [\cite=MaedcheS00]. Yet there are several differences between [\cite=MaedcheS00] and the present work: [\cite=MaedcheS00] is conceived for Ontology Extraction instead of Ontology Refinement, uses generalized association patterns (bottom-up search) instead of multi-level association patterns (top-down search), adopts propositional logic instead of FOL. Within the same application area, [\cite=MaedcheZ02] proposes a distance-based method for clustering in RDF which is not conceptual. Also the relation between Frequent Pattern Discovery and Concept Formation as such has never been investigated. Rather our pattern-based approach to clustering is inspired by [\cite=XiongSRK05]. Some contact points can be also found with [\cite=ZimmermannDR04] that defines the problem of cluster-grouping and a solution to it that integrates Subgroup Discovery, Correlated Pattern Mining and Conceptual Clustering. Note that neither [\cite=XiongSRK05] nor [\cite=ZimmermannDR04] deal with (fragments of) FOL. Conversely, [\cite=Stumme04] combines the notions of frequent Datalog query and iceberg concept lattices to upgrade Formal Concept Analysis (a well-established and widely used approach for Conceptual Clustering) to FOL. Generally speaking, very few works on Conceptual Clustering and Concept Formation in FOL can be found in the literature. They vary as for the approaches (distance-based, probabilistic, etc.) and/or the representations (description logics, conceptual graphs, E/R models, etc.) adopted. The closest work to ours is Vrain's proposal [\cite=Vrain96] of a top-down incremental but distance-based method for Conceptual Clustering in a mixed object-logical representation.

For the future we plan to extensively evaluate this approach on significantly big and expressive ontologies. Without doubt, there is a lack of evaluation standards in Ontology Learning. Comparative work in this field would help an ontology engineer to choose the appropriate method. One step in this direction is the framework presented in [\cite=BissonNC00] but it is conceived for Ontology Extraction. The evaluation of our approach can follow the criteria outlined in [\cite=DellschaftS06] or criteria from the ML tradition like measuring the cluster validity [\cite=HalkidiBV01], or the category utility [\cite=Fisher87]. Anyway, due to the peculiarities of our approach, the evaluation itself requires a preliminary work from the methodological point of view. Regardless of performance, each approach has its own benefits. Our approach has the advantages of dealing with expressive ontologies and being conceptual. One such approach, and in particular its ability of forming concepts with an intensional description in the form of rule, can support many of the use cases defined by the W3C Rule Interchange Format Working Group. Another direction of future work can be the extension of the present work towards hybrid formalisms, e.g. [\cite=MotikSS04], that are more expressive than [formula]-log and more inspiring for prototipical SWRL reasoners. Also we would like to investigate other instantiations of the framework, e.g. the ones in the case of discriminant induction to learn predictive rules.

The semantic mark-up language OWL

The Web Ontology Language OWL is a semantic mark-up language for publishing and sharing ontologies on the World Wide Web [\cite=HorrocksP-SvH03]. An OWL ontology is an RDF graph, which is in turn a set of RDF triples. As with any RDF graph, an OWL ontology graph can be written in many different syntactic forms. However, the meaning of an OWL ontology is solely determined by the RDF graph. Thus, it is allowable to use other syntactic RDF/XML forms, as long as these result in the same underlying set of RDF triples.

OWL provides three increasingly expressive sublanguages designed for use by specific communities of implementers and users.

OWL Lite supports those users primarily needing a classification hierarchy and simple constraints. E.g., while it supports cardinality constraints, it only permits cardinality values of 0 or 1. It should be simpler to provide tool support for OWL Lite than its more expressive relatives, and OWL Lite provides a quick migration path for thesauri and other taxonomies. OWL Lite also has a lower formal complexity than OWL DL.

OWL DL supports those users who want the maximum expressiveness while retaining computational completeness and decidability. OWL DL includes all OWL language constructs, but they can be used only under certain restrictions (e.g., while a class may be a subclass of many classes, a class cannot be an instance of another class). OWL DL is so named due to its correspondence with the very expressive DL () [\cite=HorrocksST00] which thus provides a logical foundation to OWL. The mapping from [formula] to OWL is reported in Table [\ref=tab:ALCvsOWL].

OWL Full is meant for users who want maximum expressiveness and the syntactic freedom of RDF with no computational guarantees. For example, in OWL Full a class can be treated simultaneously as a collection of individuals and as an individual in its own right. OWL Full allows an ontology to augment the meaning of the pre-defined (RDF or OWL) vocabulary. It is unlikely that any reasoning software will be able to support complete reasoning for every feature of OWL Full.

Each of these sublanguages is an extension of its simpler predecessor, both in what can be legally expressed and in what can be validly concluded.