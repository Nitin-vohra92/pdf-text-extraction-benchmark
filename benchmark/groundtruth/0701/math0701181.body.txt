Introduction

Consider two discrete-time, stationary, zero-mean, (real-valued for notational convenience) random processes [formula] and [formula] ([formula]) having power spectral densities [formula] and [formula] (θ∈[ -   π,π]), and autocorrelation functions [formula] and [formula] ([formula]), respectively, i.e.,

[formula]

and similarly for the "hatted" quantities. When the power spectrum contains a singular part, then f(θ)dθ needs to be replaced by a non-negative finite spectral measure dμ(θ).

We are interested in quantifying the distance between respective spectra and statistics for two such random process [formula] and [formula]. When two vectors

[formula]

of autocorrelation samples are available and need to be compared, one may use any metric in [formula] for that purpose, as for instance [formula]. However, we are not aware of any significance that can be attached to such a distance other than the fact that it is a metric in [formula]. Our goal in this paper, is to seek a metric which can be physically motivated.

Similarly, if we are to compare [formula] and [formula], it appears difficult to motivate the use of an L2-distance [formula]. For one thing, the L2-distance cannot be generalized to deal with spectral measures when singular parts are present. There are certainly other alternatives. In the speech processing literature in particular there is a plethora of distances that, however, are not metrics [\cite=Gray] but have been motivated by specific needs. Function theoretic alternatives that one can use (e.g., Lp-norms, etc.) including Wasserstein-like transportation measures typically lack a physical interpretation. In a recent study [\cite=Geo2007a] [\cite=Geo2007b] a pseudometric was constructed as a geodesic between spectral densities/measures with respect to a rather natural Riemannian metric --this metric quantifies the degradation of predictive-error variance when the predictor is designed based on the wrong choice between two alternatives and the geometry is, in essence, Euclidean but only after we transform spectral densities using the logarithmic map.

In the current paper we focus on the L1 distance

[formula]

which has also a rather natural interpretation. After a brief discussion of the relevance of the L1-distance, following a similar rationale, we will develop an analogous metric between finite partial covariance data of the corresponding random processes.

Interpretation of the L1 distance

Given [formula] and [formula] we postulate that ther exist two random processes ψk and ψ̂k so that

[formula]

Alternatively, we postulate that there exists a random process [formula] and that the two original random processes relate to [formula] via

[formula]

It is natural to seek such perturbations of minimal total combined variance

[formula]

that is sufficient to "reconcile" the two processes. The combined variance E{ψ2k} + E{ψ̂2k} represents the minimal amount of "energy" of perturbations in the two time-series that is needed to render the two indistinguishable. Intuitively, the minimal combined variance which is consistent with the available data quantifies the distance between the two.

Given [formula], the optimal choice consists of random processes ψk and ψ̂k such that ψk and ψ̂k are independent, [formula] and ψ̂k are also independent, and Then, the power spectrum of the "sum"

[formula]

is simply

[formula]

and

[formula]

Obviously, this construction extends in the obvious way to the case of not-necessarily absolutely continuous power spectra as well, and the metric includes the measure of any discrepancies between the singular parts of the two spectral measures. Clearly, [formula] is a metric as seen from ([\ref=L1]). Building on a similar rationale, in the next section, we develop a metric for covariance matrices.

A distance between covariance matrices

It is often the case that only a finite segment of the autocorrelation function of time-series [formula] and [formula] is available (and even then, possibly uncertain). Thus, it is of interest to consider distances between the partial autocorrelation statistics R and [formula]. To this end, we follow the dictum of the previous section and define as a distance measure the minimal combined variance of random processes ψk and ψ̂k for which ([\ref=equalityholds]) holds. Naturally, since only partial covariance samples are available, the random processes ψk,ψ̂k and ([\ref=equalityholds]) need to be consistent with these data.

First denote by

[formula]

the n  ×  n covariance matrix corresponding to [formula] and the covariance samples in Rn and, similarly, n for the Toeplitz matrix based on n. If [formula] denote the corresponding finite Toeplitz covariances of the random processes ψk and ψ̂k, respectively, for which ([\ref=equalityholds]) holds, then

[formula]

and the minimal sum Q0  +  0 of the respective variances can serve as a metric quantifying the distance between [formula] and [formula].

The computation of [formula] minimizing the sum Q0  +  0, or equivalently minimizing

[formula]

is a convex problem -since the positivity constraints are convex. The Toeplitz structure is peripheral, and the idea of defining such metrics extends equally well to non-negative definite Hermitian matrices and to more general positive operators. For notational convenience we develop the framework in the context of real symmetric matrices.

So, we let

[formula]

be the cone of non-negative symmetric n  ×  n-matrices and

[formula]

be the cone of non-negative Toeplitz matrices in Mn. We address the case of matrices in Mn and define a suitable metric, which is then specialized to Tn.

Given M1,M2∈Mn,

[formula]

is a (convex) cone of non-negative definite matrices. It follows that there is an element M12∈C(M1,M2) having minimal trace.

Clearly δ(M1,M2) is symmetric in its arguments and takes positive values unless M1 = M2, in which case δ(M1,M2) = 0. Thus, we only need to prove the triangle inequality. Given Mi∈Mn for i∈{1,2,3}, we denote by Mik corresponding minimal elements as above for i,k∈{1,2,3}, and we let

[formula]

These matrices are non-negative by construction, the identities

[formula]

hold, and

[formula]

for i,k∈{1,2,3}. But then,

[formula]

and hence,

[formula]

From the minimal property of Δ13 and of Δ31 with regard to having the least value for the combined trace so that M1  +  Δ31 = M3  +  Δ13, it follows that

[formula]

Therefore,

[formula]

which completes the proof. [formula]

We now observe that the steps of the proof of Proposition [\ref=proposition2] permit incorporating linear constraints on the structure of elements of Mn, such as the constraint of all matrices being Toeplitz. Hence, whereas δ(  ·  ,  ·  ) may be used directly as a distance measure between elements of Tn, the corresponding minimal-trace perturbations Δik may not belong to Tn in general. But, since the Toeplitz property is a linear constraint, we may define a completely analogous distance measure enforcing such perturbations (if so desired) to be Toeplitz.

The proof follows the steps of the proof of Proposition [\ref=proposition2] verbatim, except for the fact that we now constraint all matrices to belong to Tn. [formula]

Clearly

[formula]

since a choice of ψk and ψ̂k with power spectra as in ([\ref=psi]-[\ref=psihat]) gives rise to partial covariance matrices [formula], n, for all n, for which ([\ref=RQ]) holds. The respective 0th elements Q0 and 0 remain the same for all n and the left hand side is

[formula]

since the power spectra in ([\ref=psi]-[\ref=psihat]) have no overlap in their support.

To show the converse inequality, consider the sequence of minimizing [formula], n. These are Toeplitz matrices with bounded entries (since their corresponding 0th element is bounded by [formula]). Each can be extended to an infinite Toeplitz matrix, and thereby, gives rise to power spectral densities qn and n such that the first n Fourier coefficients of [formula] and [formula] coincide. The spectral densities qn and n can be obtained from [formula], n by any particular positive extension, for instance a "maximum entropy" one. We can take those as pairs, and since they are bounded there exists a subsequence weakly convergent to possibly non-negative measures, dμ and d, such that

[formula]

since their Fourier coefficients must coincide. If dμ, d do have singular parts then these should be identical and the absolutely continuous parts must balance as well, so there exist power spectral densities q and [formula] such that

[formula]

But then,

[formula]

the last inequality from ([\ref=balance]). [formula]

An example

The metric [formula] of the previous section admits no simple expression in terms of the respective eigenvalues. This should be contrasted with its limiting value [formula] which is the L1 distance between the corresponding power spectral densities. We highlight this with an example.

Let

[formula]

and

[formula]

Then, clearly,

[formula]

and

[formula]

where

[formula]

and x is minimal subject to [formula] as well as 3  ≥  0. The last two inequalities imply that

[formula]

It follows that the optimal choice (minimal x) is

[formula]

Then

[formula]

while the respective eigenvalues are

[formula]

It appears that there is no simple expression for [formula] based solely on knowledge of [formula] and spec(3).

The covariance [formula] has a unique extension and corresponds to a measure with unit weight at θ = 0, i.e., a spectral line (Dirac delta) at θ = 0. Assuming that 3 originates from a spectral measure which has a similar weight of amplitude 1 / 2 at θ = 0 and a uniform absolutely continuous part of amplitude 1 / 2, then

[formula]

adding the L1-norm of the difference of the absolutely continuous parts with the absolute integral of the discrepancy between the two measures. We leave it as an exercise to the reader to verify that if n is as we just assumed, namely R̂k = 1 / 2 for k  ≥  1, and similarly, Rk = 1 for all k, then [formula] as n  →    ∞  .

Approximating sample covariances

It is often the case that the autocovariance matrix [formula] of a random process [formula] is estimated in a way that does not guarantee this to be Toeplitz. For instance, it is quite common for [formula] to be estimated by averaging observation samples

[formula]

The estimate n is non-negative definite by construction, but may not be Toeplitz. Yet, for purposes of analysis it is often beneficial to approximate [formula] by a Toeplitz one, or possibly, by one with additional structure (e.g., corresponding to a moving average process or, more generally, to the state of a known dynamical system). The problem of seeking such an approximant which is closest to [formula] in δ(  ·  ,  ·  ), is readily solvable via convex optimization.

Comparison with the von Neumann entropy

In [\cite=kimurafest], the question was raised as to what are appropriate ways to approximate a given sample covariance with one that abides by a known linear structure. It was proposed that the Kullback-Leibler-von Neuman distance

[formula]

provides a convenient convex functional for which the optimal approximant is uniquely defined. An academic example was presented in [\cite=kimurafest] which is recapitulated here as it helps underscore differences with approximation in the sense of minimizing [formula].

Consider the positive-definite matrix below as the estimated value for a covariance matrix

[formula]

The minimizer of

[formula]

is unique (see [\cite=kimurafest]) and given by

[formula]

It is interesting to point out the the closest Toeplitz matrix to [formula] in the least-squares sense fails to be positive-definite ([\cite=kimurafest], cf. [\cite=IT]). On the other hand, the optimal approximant in δ(  ·  ,  ·  )-sense can be obtained by observation and is equal to

[formula]

In the above, a second subscript indicates the sense in which the matrix approximates 3. Obviously the traces of [formula] and 3 are not the same, in general. However, equality of the traces can be easily imposed as an added linear constraint.

Structured covariances

For purposes of illustration, consider a moving average process

[formula]

where [formula] is a zero-mean, unit-variance, Gaussian white noise process. The autocorrelation sequence of [formula] is

[formula]

Simulating [formula] over a window [formula], and based on a particular such realization, the corresponding n  ×  n sample covariance matrix, for n = 5, was computed to be

[formula]

Obviously, this matrix is not Toeplitz due to the finiteness of the observation record. The closest Toeplitz approximant to 5, in the sense of the metric δ(  ·  ,  ·  ), turns out to be

[formula]

for which

[formula]

Interestingly, [formula] does not correspond to a moving average process of order 2 (or even, of order 3,4) as it can be readily verified by the fact that the trigonometric polynomials, e.g.,

[formula]

takes negative values.

The set of covariance matrices which are generated by moving average processes of a given order, is convex and admits a characterization via a set of linear matrix inequalities ([\cite=StoicaMA] [\cite=GeoDecomposition]). Thus, the closest approximant to [formula] which corresponds to a moving average process of any given order can be readily computed. In particular, if we specify the order to be 2, then the optimal approximant to 5 becomes

[formula]

for which

[formula]