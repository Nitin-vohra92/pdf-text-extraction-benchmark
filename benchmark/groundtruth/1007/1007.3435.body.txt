Two-step Nonnegative Matrix Factorization Algorithm for the Approximate Realization of Hidden Markov Models

Introduction

Hidden Markov models (HMM) are a simple, yet very rich, class of stochastic processes which has become ubiquitous in several areas of signals, systems, and control. Here we restrict attention to HMMs [formula] taking values into a finite set Y. The HMMs of this type can always be represented as deterministic functions of some Markov chain [formula], taking values in a finite set S, in the sense that [formula], for all [formula], where the symbol [formula] means that the processes (Yt) and (f(St)) have the same distributions. The cardinality of S is called the size of the representation [formula].

Since functions of Markov chains generally loose the Markov property, HMMs can be used to model also dynamical behaviors exhibiting complex dependencies from the past, which cannot be described within the class of Markov chains. At the same time HMMs admit a simple parametric description, through the transition probabilities matrix of (St) and the deterministic function f. Being flexible and easy to describe it comes as no surprise that the class of HMMs is extensively employed in many applications to real data. The applications span the fields of engineering (modeling stochastic automata, for automatic speech recognition and for communication networks), genetics (sequence analysis), biology (to study neuro-transmission through ion-channels), mathematical finance (to model rating transitions, or to solve asset allocation problems), and many others.

Although by now many approaches to solve inferential problems about HMMs have been proposed in the engineering and statistical literature, an algorithm to solve the exact stochastic realization problem from distributional data is still missing. Even the weakest form of stochastic realization, given the finite dimensional distributions pY(Yn1) of a HMM find the parameters of any of its representations (f(St)), has not yet been solved satisfactorily. The early attack dates back to [\cite=furst], and [\cite=heller1965], the stochastic realization point of view was introduced later in [\cite=picci1978] and [\cite=piccivans], the most recent results can be found in [\cite=bdo1999] and [\cite=vid2004].

In the present paper we focus on the simpler approximate realization problem which can be roughly formulated as follows. Given the distributions qY(Yn1) of a stationary process, find a realization of a HMM of assigned size, which best approximates qY(Yn1) in divergence rate, a most natural criterion of closeness between distributions. Unfortunately there exists no general, closed form, analytic expression of the divergence rate between HMMs [\cite=karan], [\cite=hanmarcus2006], let alone of the divergence rate between a general stationary process and a HMM. To obviate the difficulty we formulate an alternative criterion, in terms of the informational divergence between nonnegative, pseudo Hankel matrices, representing the finite dimensional distributions of the processes. The approximate realization problem becomes then amenable to the use of Nonnegative Matrix Factorization (NMF) techniques. This approach was already investigated in [\cite=finessospreij2002], and [\cite=FGS-arxiv], where we proposed a three step, NMF based, optimization procedure to construct the parameters of the best approximate realization. The same approach, in slightly different contexts, has been later followed in [\cite=DLW] and [\cite=cybenko], the latter proposing a two step algorithm based on estimated data instead of exact distributional data, with the second step carried out via linear programming.

The remainder of the paper is organized as follows. Section [\ref=sec:preliminaries] contains preliminaries on HMMs. In Section [\ref=sec:hankel] we introduce the pseudo Hankel matrix of the finite dimensional distributions of a stationary process and study its factorization properties for the class of HMMs. Section [\ref=sec:algorithm] introduces the two-step approximation algorithm. The last Section discusses some of the numerical issues and provides examples of order reduction for HMMs.

Preliminaries

A stochastic process [formula] with values in [formula] is a stationary hidden Markov model (HMM) of size N if for some process [formula], with values in [formula], the pair (Xt,Yt) is jointly stationary and, for all y∈Y, j∈X,

[formula]

From ([\ref=splitting_property]) it follows that both, the pair (St): = (Xt,Yt), and (Xt) alone, are Markov chains, a property that shows why a HMM can always be described as a deterministic function of a Markov chain, i.e. the projection onto the second component of (St). The distribution functions, [formula], of (Yt) are specified by the m nonnegative N  ×  N matrices and by a row vector π   =   πA, where [formula] is the transition probabilities matrix of (Xt). For any (finite) string [formula] one gets

[formula]

where [formula]. Define [formula]. It follows that, for any pair of (finite length) strings u and v, denoting by uv their concatenation, one has

[formula]

For any finite length strings u and v and any y∈Y the vectors π(u) : =  πM(u) (row) and γ(v) : =  M(v)e (column) satisfy the following relations In common usage the recurrence equations (the ones on the left) are called respectively forward equation (for π), and backward equation (for γ).

Hankel matrices and their factorizations

We introduce two lexicographic orders on the set Yn. The first lexical order (flo), increasing from right to left, and the last lexical order (llo), increasing from left to right. By way of example, if Y  =  {0,1} and n = 2, the strings in increasing flo are (00,10,01,11), while, in increasing llo, are (00,01,10,11).

(Hankel matrices of stationary processes) If qY(  ·  ) is the finite dimensional distribution of a stationary measure Q,

[formula]

where ur and vs run through all of Yn in flo and in llo respectively.

The matrix [formula] is generally called the Hankel matrix associated to Q, see e.g. [\cite=bdo1999], but it only has a structural resemblance to a genuine Hankel matrix. The Hankel matrices of HMM measures admit several nonnegative factorizations stemming from ([\ref=p(uv)]). Specifically, if pY is the finite dimensional distribution of a stationary HMM measure P,

[formula]

where [formula], and [formula] are matrices of sizes mn  ×  N and N  ×  mn respectively [formula]

The matrices [formula], and [formula], in turn, can be factored as follows

[formula]

[formula]

where ";" and "," denote, respectively, vertical and horizontal stacking of the blocks. Introducing the matrix

[formula]

equations ([\ref=Q1_L+1]) can be compactly written

[formula]

where

[formula]

Note that both [formula] and [formula] are readily obtained from [formula] and [formula] by way of the marginal relations (the ones on the right) given in ([\ref=recs]).

Approximation problem and two-step algorithm

We now pose the problem of best approximation of a given stationary process with a HMM of assigned size, convert it into an approximate nonnegative matrix factorization (NMF), and propose an algorithm for its numerical solution. The following definition will be used throughout the paper.

(Divergence between positive matrices) Let [formula],

[formula]

We are now ready to pose the approximation problem as a nonnegative matrix factorization (NMF) problem.

Given a stationary probability measure Q on Y∞ and [formula], find the parameters {M*(y),  y∈Y} of a HMM of size N whose Hankel matrix [formula] (n  >  2N) is closest to [formula] in divergence. More compactly: solve the minimization problem

[formula]

under constraints [formula], and [formula].

The motivation for this setup comes from the fact that the distributions of a HMM of size N are fully determined by any of its Hankel matrices [formula] with n > 2N see e.g. [\cite=carlyle]. The constraints are imposed by the definitions of the factors [formula] and [formula] given above.

A minimizing nonnegative factorization [formula] always exists, see [\cite=finessospreij2005], Proposition 2.1, but Problem [\ref=approx-prob] also calls for the construction of the corresponding parameters M*(y). The analysis of the ideal case will serve as a guide. If Q were a HMM law, the following exact nonnegative factorizations would hold by equations ([\ref=fatt_blocco_H_{KL}]) and ([\ref=factrecur])

[formula]

This can be considered as an ideal algorithm. Feeding into the system ([\ref=ideal1]), ([\ref=ideal2]) the input [formula], which is known since Q is given, produces the output [formula], whose blocks contain the parameters M(y) sought for. In real situations these exact factorizations are generally not valid since Q might not be a HMM, or might be a HMM of order larger than N. This suggests constructing a two step algorithm where ([\ref=ideal1]) and ([\ref=ideal2]) are substituted with NMFs. The scheme below illustrates the two steps.

Step 1: Law Approximation Step

[formula]

Note that [formula] and [formula] are of respective sizes (mn  ×  N) and (N  ×  mn). Using equation ([\ref=newgamma]) one can build the matrix [formula] needed below.

Step 2: Parametrization Step (version with Γ)

[formula]

Note that the constraint [formula], imposed at Step 2, corresponds to the requirement that the transition matrix of the underlying Markov chain be stochastic. The resulting [formula] can be used to compute the parameter π*  =  π*A*.

Step 1 of the algorithm behaves like a typical EM method, with convergence of the NMF algorithm to local minima of the divergence and dependence on the initial conditions [formula]. Step 2 behaves much better, as one of the factors is fixed. The following Lemma summarizes the convergence properties of NMF algorithms in the general setup of Step 2 of the algorithm. Although an easy consequence of the results in [\cite=ct1984], to the best of our knowledge the Lemma was not introduced before, at least not in the literature on NMF method

Let S and Γ be given stochastic matrices of sizes m  ×  m and N  ×  m respectively. Let M0 be a given stochastic matrix of size m  ×  N with strictly positive elements. The NMF iterative algorithm [\cite=lee], [\cite=finessospreij2005] applied to the problem

[formula]

with initial condition M0, produces a sequence of matrices Mk  →  M* (elementwise) where M* is the minimizer of [formula].

The proof follows directly from Theorem 5 of [\cite=ct1984], once it is recognized that the problem is decoupled in the rows of M.

The analysis of the equations ([\ref=fatt_blocco_H_{KL}]) and ([\ref=factrecur]), valid in the ideal HMM case, shows that as an alternative one could write the system

[formula]

This suggests that it would be possible to substitute Step 2 of the proposed algorithm with the alternative Step2.alt below. Note that from the output [formula] of Step 1, one can construct, as noted in Remark [\ref=remark1], the matrix [formula] and also repackage it, using equation ([\ref=factrecur]), to form the matrix [formula].

Step 2.alt: Parametrization Step (version with Π)

[formula]

Numerical Simulations

In this section we show the results of numerical simulations designed to assess the performance of the algorithm on HMM order reduction problems. We have also tried to evaluate possible differences in performance related to the version of Step 2 being used. We selected two examples. In both cases the given process is a 4 state HMM with binary output.

Example 1: The given HMM is specified by its transition probabilities matrix A and its read-out matrix B. Recall that M(y) = ABy where By is the diagonal matrix with the y-th column of B on the diagonal. For this example y∈{0,1}.

[formula]

[formula]

Example 2: Same read-out matrix B as Example 1. The transition probabilties matrix is

[formula]

Discussion of Step 2 of the Algorithm

As seen at the end of the previous section, Step 2 of the algorithm can be implemented in two versions. We will call the first the Γ version and the alternative the Π version. By Lemma 1, and a variant of it, both NMFs are guaranteed to converge to the global optimum, irrespective of the initial condition M0. We are interested in evaluating numerically if there are significant differences in the speed of convergence of the Γ and Π versions of Step 2. For the purpose of this comparison Step 1 was fixed and taken as an order reduction step, from size 4 to size 2, for both Example 1 and Example 2. We tested, on each example, the Γ and the Π versions of Step 2, running for each 30 NMFs choosing M0 randomly. The results were compared in terms of speed of convergence of the divergence and variability of the resulting *'s. As an index of variability of the * parameters we computed the sum of the sample variances of the elements of * resulting for each of the 30 runs, i.e. where N = 2, is the size of the reduced HMM, T = 30 is the number of different initial conditions M0, and [formula]. Fig. [\ref=fig:ex3.variabilityM.N4NP2] and Fig. [\ref=fig:ex3.I-divergence.N4NP2], relative to Example 1, display the variability index R and the averaged (over the 30 runs with randomly chosen M0) divergence decay against the number of NMF iterations. Fig. [\ref=fig:ex4.variabilityM.N4NP2] and Fig. [\ref=fig:ex4.I-divergence.N4NP2] show the same for Example 2.

As expected, both versions of Step 2 are not sensitive to the initial conditions and the NMF iterations converge. What is particularly pleasing is that the resulting * is the same for both versions of Step 2. The divergence appears to converge much faster than the parameter *, a clear sign of the flatness of the criterion near optimality. Indicating with *Γ and *Π the parameters of the best approximation obtained with the two versions of Step 2, we see that, for Example 1

[formula]

[formula]

and for Example 2

[formula]

[formula]

Results along the same lines were obtained running many examples, corresponding to different sizes of order reduction, and/or different given HMM processes. The speeds of convergence of the divergence for the two versions of Step 2 remain always comparable. The variability of * is sometimes slightly different for the two versions. The conclusion we drew is that there is not a version that systematically outperforms the other. For top performance in terms of variability it seems more appropriate to evaluate on a case by case basis which version is most suitable.

HMMs order reduction

We collect here the results of four experiments of HMM order reduction. For each of the HMMs of size 4 given as Example 1 and Example 2, we ran the algorithm to produce HMMs of order reduced to 3 and 2. In all cases Step 2 was performed in the Γ version. The set-up of the algorithm was fixed as follows: 3,000 NMF iterations for Step 1 in all cases, 3,000 and 20,000 NMF iterations for Step 2 for the order reductions 4  →  2 and 4  →  3 respectively. The number of NMF iterations was decided fixing a threshold on the M* variability index, R. We randomly chose 30 initial conditions [formula] for Step 1. As the NMF problem posed in Step 1 is sensitive to initial conditions (it behaves like a standard EM) we obtained 30 different pairs (Π*n,Γ*n) at its output. We therefore had 30 different NMF problems as input to Step 2 leading to different *. In the following tables we report the divergence values, before and after each of the two steps of the algorithm (DIV1b, DIV1, DIV2b, DIV2) and the final divergence (DIV) between the original process and the best approximation obtained with the resulting *. Specifically Table [\ref=tab:example3_N4_NP2] and Table [\ref=tab:example4_N4_NP2] refer to the HMM order reduction 4  →  2 for Example 1 and Example 2 respectively. Likewise Table [\ref=tab:example3_N4_NP3] and Table [\ref=tab:example4_N4_NP3] report the results of the order reduction 4  →  3 for the two examples. The high variability of the divergence after Step 2 (DIV2), especially in the case 4  →  3, is not surprising. It depends on the fact that each run corresponds to a different NMF problem for Step 2.

It is interesting to notice that the final divergence (DIV) between the original process and the best approximation have the same order of magnitude, irrespective of the random initializations for the NMFs of the two steps. In Fig. [\ref=fig:ex3.divergence.N4NP2], relative to Example 1, and Fig. [\ref=fig:ex4.divergence.N4NP2], relative to Example 2, are plotted the divergences between the original process and the output process of the algorithm for the order reduction case 4  →  2. Fig. [\ref=fig:ex3.divergence.N4NP3] and Fig. [\ref=fig:ex4.divergence.N4NP3] show the case 4  →  3. In the plots the best numerical approximations are highlighted in red.