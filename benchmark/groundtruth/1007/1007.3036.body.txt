Greedy algorithm for stochastic matching is a 2-approximation

Definition Remark Corollary Lemma

Introduction

Problem statement

We are given an undirected graph G = (V,E) in which every edge uv∈E is assigned a real number 0  <  puv  ≤  1. Every vertex v∈V has a positive integer number tv, called a patience number, associated with it. In every step we can probe any edge uv∈E, but only if tu  >  0 and tv  >  0. Probing edge uv will end with success with probability puv, and after that vertices u,v will be removed from the graph with all edges going out of them. In case of a failure, which has probability 1 - puv, edge uv is removed from graph, and patience numbers tu,tv are decreased by 1. Results of all probes are independent. If after a certain step the patience number tv of vertex v becomes 0, we remove vertex v with all edges incident to it. Our goal is to maximize the average number of successful probes.

An instance of our problem will be pair (G,t) - undirected weighted graph G with patience numbers tv for every vertex v∈G. Set of edges which were probed successfully forms a matching. We will call such edges as taken into matching.

Motivation

We will give a motivations for this problem. More detailed justification of the model can be found in [\cite=ammh].

Kidney Exchange

A patient waiting for a kidney may have a donor, a friend or a family member, who would like to donate him a kidney. But it may happen that the patient has incompatible donors among his friends and family. If we consider many such patient-donor pairs, it may be possible to find two such pairs in which donor from the first pair may donate a kidney to a second patient, and the second donor may donate kidney to a first patient. Performing such operations on four people is called kidney exchange. To know whether two pairs can perform kidney exchange, we need to make a set of three tests. The first two tests are easy to make, e.g. they include blood-type tests. The third test is relatively hard, and checks possibility of exchange only between two particular pairs. Also it is required to perform exchange as soon as two pairs were matched. Thus we can not check every two pairs to see whether they can exchange or not. However, we can use the results of first two tests to estimate the probability of a successful result in the third test. Moreover, every patient can only be put to a limited number of tests, depending on his health. The goal is to perform maximal number of transplants.

Dating

Let us consider a dating portal, where users are offered to make acquaintances with other users. On the basis of personal details of each user we can estimate a probability of matching for each pair of users. We can suggest a meeting to a pair of users, but only after the meeting we can say if they like each other. In case of a successful matching, we assume they will leave the portal. While patience of each user is an individual case, every user will give up after several unsuccessful meetings and will stop using the portal. The objective is to suggest this acquaintances to the users in the way which maximize the number of satisfied couples.

Related work

Problem we are considering belongs to the field of stochastic optimization. It includes many of well known problems to which some elements of uncertainty were added. Since matchings are very popular combinatorial object used in models from many areas, lots of applications motivate stochastic matching problems, e.g. the classic problem of online bipartite matching, solved by Karp et. al. [\cite=obm], has evolved in many ways to face needs of online advertising [\cite=agom] [\cite=osmboe]. In those problems we are given one side of bipartite graph (advertisers), and each vertex from the other side (ad impression) is revealed to us one by one together with edges coming out of it. Right after arrival of a vertex we need to match it with some vertex from the first side. Although this problem is not precisely related to ours, Bansal et. al. [\cite=asm] gave an online variant of our stochastic matchings problem which can be seen as a generalization of online bipartite matching.

First work concerning our problem was the paper of Chen et. al. [\cite=ammh]. This work introduced a stochastic matching problem together with its modification - multiple-rounds matching. In this modified version we are given a fixed number of rounds, and in each we are allowed to probe any set of edges which forms a matching. They showed that greedy algorithm for the main version of the problem is a 4-approximation for all graphs. For the multiple-rounds version they gave O( log n) approximation. They also proved that finding an optimal multiple-rounds strategy is NP-hard. Authors also asked about weighted version of this problem in which every edge has assigned real and positive weight. Here the goal is to maximize the expected weight of taken matching. Unfortunately, modification of the greedy algorithm which probes edges sorted by its product of weight and probability does not work, and can not give any constant-factor approximation.

Recently Bansal et. al. [\cite=asm] and Li with Mestre [\cite=ibfsm] showed how linear programming can be used to obtain constant approximation ratio for stochastic matchings and multiple-rounds matching. A very huge advantage of this approach is that considering weighted edges, almost does not make any difference.

Bansal et. al. obtained 5.75 ratio for weighted case. They also showed LP-based analysis to prove that greedy algorithm is a 5-approximation. For multiple-rounds matching they gave a constant-factor approximation breaking O( log n) ratio. They also introduced two new modifications. First is an online version which can be seen as a generalization of online bipartite matching mentioned earlier. Second modification considers case when we are probing hyperedges. For both variants constant-ratio approximations were given.

Li and Mestre also gave a constant-factor approximations for weighted and multiple-rounds versions. They also improved bounds for the basic version of the problem. In special case, when all probabilities are equal and graph is bipartite, they got a factor [formula]. When probabilities can be various their algorithm gives ratio worse than 4, but it can be combined with greedy algorithm to get a 3.51 factor for bipartite graphs and 3.88 for all graphs.

Our results

In our paper we are focused on the main version of stochastic matching problem, and we do not consider multiple-rounds model nor weighted graphs. The main result of this paper is improved analysis of the greedy algorithm showing that this is a 2-approximation which confirms the hypothesis stated by Chen et. al.

Preliminaries

If in a certain step any algorithm probes edge coming out of vertex α∈G, then we will say that algorithm probes vertex α.

We consider any algorithm deterministic, if probe in each step is unambiguous and depends only on previous steps.

If ALG denotes an algorithm, then [formula] is expected number of edges taken into matching by this algorithm. Moreover, we will use (GALG,tALG) to denote the instance on which ALG is executed.

Decision tree

Each deterministic algorithm ALG can be represented by its decision tree TALG (which has an exponential size). Each node of that tree corresponds to probing an edge. Node v∈TALG is assigned a value pv equal to pαβ where edge αβ is probed in v. The left subtree of node v∈TALG represents proceeding of algorithm after a successful probe in node v, the right subtree - after failure. More precisely:

the left subtree corresponds to an algorithm on the instance [formula]

the right subtree corresponds to an algorithm on the instance [formula] where t'α  =  tα - 1,t'β  =  tβ - 1 and t'γ  =  tγ for other vertices γ

Probability of reaching a node v∈TALG will be denoted qv. The performance of an algorithm ALG can be expressed using the decision tree: We will denote above sum as [formula]. For a node v∈TALG, we denote as T(v) subtree of TALG rooted in v, and as L(v),R(v) its left and right subtree respectively.

Optimal algorithm

The optimal algorithm on instance (G,t) will be denoted OPT(G) - we will not use OPT(G,t), because it will always be clear which patience numbers we are using. We can assume without loss of generality that OPT(G) is deterministic. This lets us represent optimal algorithm by a decision tree. We will also assume that every subtree of the tree TOPT representing optimal algorithm is optimal on its instance, even when probability of reaching such subtree is zero.

Greedy algorithm

Let us consider the greedy algorithm for a given graph G. It will be denoted as GRD(G).

Empty graph

It is reasonable to consider also an empty graph, because it may appear during our inductive proof. Of course in this case performance of any algorithm is zero.

Analysis of the greedy algorithm

We will start with an important lemma from [\cite=ammh] (Lemma 3.1.).

For any node v∈TOPT, [formula].

Algorithm which follows R(v) is a proper algorithm for instance on which T(v) works, so [formula] - because every subtree of TOPT is optimal. Hence This gives [formula], and finally [formula].

The following theorem is the main result of this paper.

Greedy algorithm for any instance (G,t) of stochastic matching problem is a 2-approximation, i.e. [formula].

The sketch of the proof is as follows. Proof is inductive with respect to subinstances (G',t') of problem (G,t) where G' is a subgraph of G and t'v  ≤  tv for every vertex v∈G'. Using optimal algorithm on instance (G,t), we derive two algorithms - first on instance (GLGRD,tLGRD) and second on (GRGRD,tRGRD). We do not know explicitly how those algorithms work. We upperbound performance of OPT(G) by those algorithms and some residues. Then we use inductive assumption to bound performance of those derived algorithms by greedy algorithms, and show that the residues are "small".

The case when graph has no edges is trivial. So suppose it has at least one edge. Let αβ be the first edge probed by the greedy algorithm. Denote as LGRD,RGRD algorithms which follow left and right subtree of TGRD respectively. When it does not make a problem, we use OPT instead of OPT(G).

Algorithm for instance (GLGRD,tLGRD)

In this paragraph we will derive algorithm proper for instance (GLGRD,tLGRD). We will not obtain it directly from OPT, but from algorithm OPT' which also works on the whole instance (G,t).

Let X be the set of nodes of TOPT which correspond to probing edge αβ. Define algorithm OPT' which follows algorithm OPT(G), until it reaches a node x∈X. After reaching that node OPT' probes edge αβ, but then goes straight to the subtree L(x) regardless of the probe's result, i.e. after probe it runs as if the result was successful. This means also that OPT' after probing αβ will not probe α nor β again. In a node x∈X the expected number of edges taken in the subtree T(x) of TOPT is equal to [formula], but in the subtree T(x) of TOPT' it is [formula] - because OPT' goes to L(x) regardless of the result. We can bound performance of OPT using OPT':

[formula]

From decision tree of OPT we get

[formula]

so

[formula]

All we need now is to notice that [formula].

Now we will use algorithm OPT' to construct algorithm for instance (GLGRD,tLGRD). Let algorithm ALGL follow all moves of OPT', but only those which does not probe vertices α and β - when algorithm OPT' probes vertex α or β, then algorithm ALGL does not probe any edge, and waits for result of OPT'. We can also define ALGL using decision trees. ALGL follows decision tree of OPT', but upon reaching a node v, which probes vertex α or β, it flips a coin, and with probability pv it goes to the left subtree L(v), and with probability 1 - pv it goes to the right subtree R(v). This (randomized) algorithm is a proper algorithm for instance (GLGRD,tLGRD), because graph GLGRD is made from G by removing vertices α and β. Moreover, for every vertex v∈GLGRD we have tLGRDv  =  tv. Performance of algorithm ALGL is equal to the performance of OPT' minus penalty for skipped probes - let RL denote this penalty. Hence

[formula]

Let us look at RL under two conditions - when OPT' probes αβ, and when it does not probe that edge. If OPT' probes edge αβ, then with probability pαβ it will take this edge, and after that all probes of OPT' will be valid probes for ALGL, because they will not probe α nor β. Thus the penalty in that case is equal to [formula]. If OPT' does not probe αβ, then the penalty is equal to the expected number of edges incident to αβ taken by OPT' under this condition:

[formula]

Thus the whole [formula] is equal to

[formula]

From the definition, OPT' works just like OPT, unless it reaches αβ, so in place of above expression we can write

[formula]

We will introduce shorter notation. Denote the event of probing αβ by OPT as "αβ", and "[formula]" as opposite event. The event of taking α (or β) by OPT under the condition of not probing αβ will be denoted as "[formula]" ( or "[formula]"). Thus we can write that

[formula]

Joining all gives:

[formula]

Finally we get that

[formula]

Algorithm for instance (GRGRD,tRGRD)

Instance (GRGRD,tRGRD) is made of (G,t) by removing edge αβ and decreasing patience numbers of α and β, i.e. tRGRDα  =  tα  -  1 and tRGRDβ  =  tβ  -  1. Define algorithm ALGR on instance (GRGRD,tRGRD) which follows OPT(G), unless OPT(G) probes edge which ALGR can not. If OPT(G) makes such probe, then ALGR does not do anything, waits for OPT(G) and follows it further. Definition with decision tree: ALGR follows decision tree of OPT, but upon reaching node v for which it cannot make a probe it flips a coin and with probability pv it goes to the left subtree L(v), and with probability 1 - pv it goes to the right subtree R(v).

Let us now describe such invalid probes.

Consider one particular execution of OPT(G). Suppose that in this execution OPT(G) probes αβ. Algorithm ALGR can not make this probe because αβ∉GRGRD. But before that every probe of OPT(G) was a valid probe for ALGR. After OPT(G) probes αβ, the patience numbers of α and β will become equal for algorithms OPT(G) and ALGR, so afterwards every probe of OPT(G) will be a valid probe for ALGR. So in this case any possible loss of algorithm ALGR can be the edge αβ, but only if OPT(G) probed it successfully. Suppose now that OPT(G) did not probe αβ in that execution. The only probes of OPT(G) which are invalid for ALGR are probe number tα  =  tRGRDα + 1 of vertex α, and probe number tβ  =  tRGRDβ + 1 of vertex β. Thus in this case ALGR can lose only edges probed then.

As before, we write performance of OPT as performance of ALGR plus penalty RR:

[formula]

From the above explanation we have

[formula]

Let us write "αtα" instead of "OPTαtα", and analogically with β. Now we can write above equality shorter: Putting this into [\eqref=optr] gives

[formula]

Combining

Multiplying inequality [\eqref=algl] by pαβ, and equality [\eqref=algr] by 1 - pαβ, and adding them give

[formula]

After grouping terms in above expression we obtain

[formula]

We state now the key lemma.

Probability of taking α into matching is the sum of probabilities of taking each edge incident to α, so Edge αγ can be taken into matching, if we probed this edge, and the probe was successful, i.e.

[formula]

Probe number tα is the last probe of vertex α regardless of its result. Thus its result and the fact that OPT does not probe αβ are independent. This gives

[formula]

Function [formula] is decreasing, and pαβ is the greatest weight in the whole graph, so we get

[formula]

The justification of the last equality is the same as in [\eqref=indsucc].

Just like at the beginning

[formula]

so the lemma is proved.

It follows from the lemma [\ref=keylem], and the obvious inequality

Of course, lemma [\ref=keylem] and corollary [\ref=keycor] are true with β in place of α.

The event that OPT does not take α is opposite to "α", so we will denote it as "[formula]", and analogically for β. This means also that

[formula]

Finally we get

[formula]

Inductive assumption gives

[formula]

Hence

[formula]

This completes the proof.