Strong Converse to the Quantum Channel Coding Theorem

Introduction

Recently, the quantum channel coding theorem was established by Holevo [\cite=Holevo-QCTh] and by Schumacher and Westmoreland [\cite=Schumacher-Westmoreland], after the breakthrough of Hausladen et al. [\cite=Hausladen-et-al.]. Furthermore, a upper bound on the probability of decoding error, in case rate below capacity, was derived by Burnashev and Holevo [\cite=Bur-Holevo]. It is limited in pure signal state. They conjectured on a upper bound in general signal state, which corresponds to Gallager's bound [\cite=Gallager] in classical information theory. We will show a lower bound on the probability of decoding error, in case rate above capacity, which corresponds to Arimoto's bound [\cite=Arimoto]. The strong converse to the quantum channel coding theorem is shown immediately from the lower bound.

Let H be Hilbert space which represents a physical system of information carrier. We suppose dim H  <    ∞   for simplicity. Quantum channel [\cite=Holevo-channel] is defined as mapping [formula], where [formula] is the set of input alphabet and [formula] is a density operator in H, i.e., non-negative operator with trace one. For a more general treatment, see Fujiwara and Nagaoka [\cite=Fujiwara-Nagaoka].

To describe asymptotic property, we use n-th extension of the channel. The messages [formula] is encoded to a codebook [formula], where each [formula] [formula] is a codeword, and is mapped to [formula], which is a density operator in [formula]. Decoding process [formula] is a quantum measurement [\cite=Holevo-channel], that is a resolution of identity in [formula], i.e., [formula] and [formula]. We think of X0 as evasion of decoder. A pair of encoding and decoding process (C(n),X(n)) is called a code with cardinality Mn. Rn  =   log Mn  /  n is called transmission rate for a code (C(n),X(n)). In the sequel, we will omit the subscript n when no confusion is likely to arise.

The conditional probability of output k, when message l was sent, is given by P(k|l) =   ρulXk. If all messages arise with uniform probability, the average error probability of code (C,X) is

[formula]

Let us denote the minimum of the average error probability as

[formula]

The (operational) capacity [\cite=Holevo-capacity] is defined as the number C such that (enR,n) tends to zero as n  →    ∞   for any 0  ≤  R < C and does not tend to zero if R > C.

Let π  =  {πi}ai = 1 be a probability distribution on X, and define (formal) quantum mutual information [\cite=Holevo-channel] as

[formula]

where [formula] and H(ρ) =  -   ρ log ρ, which is Von Neumann entropy. The quantum coding theorem states that max πI(π) is equal to the operational capacity C. The aim of this correspondence is to show the strong converse to the quantum channel coding theorem, i.e., (enR,n) tends to one exponentially as n  →    ∞   if R > C.

lower bound on the average error probability

To begin with, we will show the following Lemma.

For an arbitrary measurement X = {Xk}Mk = 0

[formula]

holds.

[formula] is obvious. Since xβ  (0  <  β  ≤  1) is a operator monotone function (see ex. [\cite=Hansen-Pedersen]),

[formula]

holds. Hence,

[formula]

where we used [formula] in the last inequality.

Following Arimoto [\cite=Arimoto], let us apply random coding technic to Lemma [\ref=lemma1] with a probability distribution

[formula]

For this purpose, we shall need next two conditions.

[formula]

[formula] is invariant under a permutation of [formula].

Actually, such a probability distribution on the set of all codebook exists. Suppose that [formula] attains the minimum of condition 1, then from symmetry of average error probability, a permutation of [formula] also attains the minimum. Therefore

[formula]

is the probability distribution which satisfies above two conditions. Furthermore, the marginal probability distributions of [formula] does not depend on [formula] by condition 2, i.e.,

[formula]

By taking average of ([\ref=sitaosae]) with P̂, we obtain

[formula]

Using Jensen type inequality, which is derived from operator concavity of xβ  (0  <  β  ≤  1) (see [\cite=Hansen-Pedersen]), ([\ref=beforeJensen]) is bounded as

[formula]

where we used the notation PXn, which represents the set of all the probability distributions on Xn.

The following Lemma is the same as classical information theory (see [\cite=Gallager][\cite=Arimoto]) except for one point that derivative of a function is not easy due to non-commutativity. We will give the proof for convenience.

Let PX be the set of all the probability distributions on X. Then

[formula]

Let us define a function of π∈PX as

[formula]

We note that f is a concave function. First, we show that necessary and sufficient condition on the probability distribution π*∈PX, which attains the maximum of f(π), is

[formula]

To show this, introduce Lagrange multiplier [formula] and λ, define another function of π as

[formula]

differentiate g(π) by πi and make it to 0. From general theory of Lagrange multiplier method (see ex. [\cite=Luenberger]), we assert that necessary and sufficient condition on π*∈PX, which attains maximum of f(π) (i.e. minimum of g(π)), is that there exist [formula] and λ which satisfies next conditions.

[formula]

where we used derivative of f(π) (see Appendix Lemma [\ref=lemma:derivative])

[formula]

By multiplying π*i to the both sides of ([\ref=lagbibun]) and summing over, we obtain

[formula]

Meanwhile, si = 0 if π*i > 0 by ([\ref=spi]). Hence ([\ref=lagbibun])([\ref=spi]) is equivalent to

[formula]

Moreover, this is equivalent to ([\ref=KTcond1])([\ref=KTcond2]). Now, Suppose π* satisfies ([\ref=KTcond1])([\ref=KTcond2]), and put

[formula]

which is i.i.d. extension of π*. Then it is clear P* satisfies

[formula]

Hence

[formula]

Now, from ([\ref=max_Xn]) and Lemma [\ref=maxlemma] we obtain

[formula]

Let us put s = β - 1, recall R =  log M / n and define

[formula]

then we have proved the following theorem.

For all code (C,X)

[formula]

([\ref=E(s] [\ref=pi)]) has appeared in [\cite=Bur-Holevo] as a conjecture on the upper bound on the average error probability, which forms dual with ([\ref=rand_low]). They proved it in case that all [formula] are pure.

Strong converse to the quantum channel coding theorem

To understand the graph of E0(s,π), we show the following lemma.

[formula]

([\ref=E1]) is obvious. From the following Lemma, E0(s,π) is shown to be non-decreasing in ( - 1,0  ]. That is why ([\ref=E2])([\ref=E3]) holds. Using Appendix Lemma [\ref=lemma:derivative], we can calculate directly the derivative of E0(s,π) to obtain ([\ref=E4]), or using

[formula]

where we put [formula] , we obtain

[formula]

Burnashev and Holevo [\cite=Bur-Holevo] showed, in case that all [formula] are pure and 0  ≤  s  ≤  1, ([\ref=E1])-([\ref=E4]) and

[formula]

Let [formula] be non-negative bounded operators in H. If 0  <  α  ≤  β  ≤  1 then

[formula]

First, we will show

[formula]

for 0 < γ  ≤  1 and [formula]. Let us put unitary operator U in [formula] as

[formula]

and projection P as

[formula]

Generally, for 0  <  γ  ≤  1 and operator A, C (||C||  ≤  1), C*AγC  ≤  (C*AC)γ holds. (see [\cite=Hansen-Pedersen], or [\cite=Oya-Petz], p.18) Using this property, we obtain

[formula]

which shows ([\ref=gammaineq]). Now, change γ into [formula] and Ai into [formula] in ([\ref=gammaineq]), then

[formula]

Since xβ is a operator monotone function,

[formula]

Now, from Lemma [\ref=lemma:graph] if R > C there exist - 1 < t < 0 such that for all s∈(t,0),   - sR  +   min πE0(s,π) > 0. Thus, we state the following strong converse theorem.

If R > C, then for all code (C,X), (C,X) goes to 1 exponentially as n  →    ∞  .

Acknowledgment

The authors wish to thank Dr. Keiji Matsumoto for useful discussion about Lemma [\ref=lemma:derivative].

Appendix

Let f:(a,b)  →   be an analytic function and X(t) a Hermitian with real parameter t, the spectrum of which is in (a,b). Then,

[formula]

where f' is the derivative of f.

Leu us expand f around x0∈(a,b) as [formula] and put X̂(t) = X(t) - x0I. We can calculate as follows.

[formula]