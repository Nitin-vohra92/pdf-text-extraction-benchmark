Lemma Proposition Corollary

Definition Remark

Factoring bivariate lacunary polynomials without heights

Part of this work was done while the authors were visiting the University of Toronto.

Bruno Grenet

Pascal Koiran Natacha Portier Yann Strozecki

Introduction

The lacunary, or supersparse, representation of a polynomial

[formula]

is the list of the tuples [formula] for 1  ≤  j  ≤  k. This representation allows very high degree polynomials to be represented in a concise manner. The factorization of lacunary polynomials has been investigated in a series of papers. Cucker, Koiran and Smale first proved that integer roots of univariate integer lacunary polynomials can be found in polynomial time [\cite=CKS99]. This result was generalized by Lenstra who proved that low-degree factors of univariate lacunary polynomials over algebraic number fields can also be found in polynomial time [\cite=Len99]. More recently, Kaltofen and Koiran generalized Lenstra's results to bivariate and then multivariate lacunary polynomials [\cite=KK05] [\cite=KK06]. A common point to these algorithms is that they all rely on a so-called Gap Theorem: If F is a factor of [formula], then there exists k0 such that F is a factor of both [formula] and [formula]. Moreover, the different Gap Theorems in these papers are all based on the notion of height of an algebraic number, and some of them use quite sophisticated results of number theory.

In this paper, we are interested in more elementary proofs for some of these results. We focus on Kaltofen and Koiran's first paper [\cite=KK05] dealing with linear factors of bivariate lacunary polynomials. We show how a Gap Theorem that does not depend on the height of an algebraic number can be proved. In particular, our Gap Theorem is valid for any field of characteristic zero. As a result, we get a new, more elementary algorithm for finding linear factors of bivariate lacunary polynomials over an algebraic number field. In particular, this new algorithm is easier to implement since there is no need to explicitly compute some constants from number theory, and the use of the Gap Theorem does not require to evaluate the heights of the coefficients of the polynomial. Moreover we use the same methods to prove a Gap Theorem for polynomials over some fields of positive characteristic, yielding an algorithm to find linear factors of bivariate lacunary polynomials of the form (uX + vY + w) with [formula]. Finding linear factors with u = 0 is [formula]-hard, and the same is true for linear factors with v = 0 or w = 0. This follows from the fact that finding univariate linear factors over finite fields is [formula]-hard [\cite=KiSha99] [\cite=BCR12] [\cite=KL13]. In algebraic number fields we can find all linear factors in polynomial time, even those with uvw = 0. For this we rely as Kaltofen and Koiran on Lenstra's univariate algorithm [\cite=Len99].

Our Gap Theorem is based on the valuation of a univariate polynomial, defined as the maximum integer v such that Xv divides the polynomial. We give an upper bound on the valuation of a nonzero polynomial

[formula]

This bound can be viewed as an extension of a result due to Hajós [\cite=Hajos] [\cite=MS77]. We also note that Kayal and Saha recently used the valuation of square roots of polynomials to make some progress on the "Sum of Square Roots" problem [\cite=KS11].

Lacunary polynomials have also been studied with respect to other computational tasks. For instance, Plaisted showed the [formula]-hardness of computing the greatest common divisor (GCD) of two univariate integer lacunary polynomials [\cite=Pla77], and his results were extended to finite fields [\cite=vzGKS96] [\cite=KaShp99] [\cite=KK05]. On the other hand, some important special cases were identified for which the GCD of two lacunary polynomials can be computed in polynomial time [\cite=FGS08]. Other efficient algorithms for lacunary polynomials have been recently given, for instance for the detection of perfect powers [\cite=GR08] [\cite=GR11] or interpolation [\cite=KN11].

Acknowledgments.

We wish to thank Sébastien Tavenas for his help on Proposition [\ref=prop:LowerBound], and Erich L. Kaltofen for pointing us out a mistake in Theorem [\ref=thm:FactorPosChar] in a previous version of this paper.

Bound on the valuation

In this section, we consider a field [formula] of characteristic zero and polynomials over [formula].

Let [formula] with [formula]. If P is not identically zero then its valuation is at most [formula].

A lower bound for the valuation of P is clearly α1 (and it is attained when α2  >  α1 for instance). If the family (Xαj(1 + X)βj)1  ≤  j  ≤  k is linearly independent over [formula], the upper bound we get is actually [formula]: At most the first [formula] lowest-degree monomials can be cancelled. If αj  =  α1 for all j, Hajós' Lemma [\cite=Hajos] [\cite=MS77] gives the better bound α1 + (k - 1). (This bound can be shown to be tight by expanding Xk  -  1  =  (  -  1  +  (X  +  1))k  -  1 with the binomial formula.) This is not true anymore when the αj's are not all equal. One can show that the valuation can be as large as α1 + (2k - 3) (see Proposition [\ref=prop:LowerBound]). The exact bound remains unknown, and whether this bound is still linear as in Hajós' Lemma or quadratic is open.

Our proof of Theorem [\ref=thm:val] is based on the so-called Wronskian of a family of polynomials. This is a classical tool for the study of differential equations but it has recently been used to bound the valuation of a sum of square roots of polynomials [\cite=KS11] and also to bound the number of real roots of some sparse-like polynomials [\cite=KPT12].

Let [formula]. Their Wronskian is the determinant of the Wronskian matrix

[formula]

The main property of the Wronskian is its relation to linear independence. The following result is classical (see [\cite=BoDu10] for a simple proof of this fact).

The Wronskian of [formula] is nonzero if and only if the fj's are linearly independent over [formula].

The next two lemmas are our main ingredients to give a bound on the valuation for P, using a bound on the valuation of some Wronskian.

Let [formula]. Then

[formula]

Each term of the determinant is a product of k terms, one from each column and one from each row. The valuation of such a term is at least [formula] since for all i,j, [formula]. The result follows.

We can slightly refine the bound in this lemma. The term of valuation [formula] in the Wronskian is indeed the determinant of the matrix made of the smallest degree monomials of each f(i)j. This determinant can vanish. In fact, one can easily see that this is the case if two fj's have the same valuation since this yields two proportional columns in the matrix. To use this idea more generally, consider that the fj's are ordered by increasing valuation. We define a plateau to be a set [formula] such that for 0 < t  ≤  s, [formula]. The fj's are naturally partitioned into plateaux. Suppose that there are (m + 1) plateaux, of length p0, , pm respectively, and let fj0, , fjm their respective first elements. Generalizing the previous remark to plateaux, it can be shown that

[formula]

This bound is at least as large as in the lemma. If all the fj's have a different valuation, then the bound is equal to the bound stated in the lemma since there are in this case k plateaux, each of length 1. On the other side, if they all have the same valuation α, there is one plateau of length k and the bound is [formula]. We investigate the implications of this refinement after the proof of Theorem [\ref=thm:val].

Let fj = Xαj(1 + X)βj, 1  ≤  j  ≤  k, such that αj,βj  ≥  k for all j. If the fj's are linearly independent, then

[formula]

By Leibniz rule, for all i,j

[formula]

where [formula] is the falling factorial. Since αj - t  ≥  αj - i and βj - i + t  ≥  βj - i for all t, Furthermore, since αj  ≥  k  ≥  i, we can write Xαj - i = Xαj - kXk - i and since βj  ≥  k  ≥  i, (1 + X)βj - i = (1 + X)βj - k(1 + X)k - i. Thus, Xαj - k(1 + X)βj - k is a common factor of the entries of the j-th column of the Wronskian matrix, and Xk - i(1 + X)k - i is a common factor of the entries of the i-th row. Together, we get

[formula]

where the matrix M is defined by

[formula]

The polynomial det (M) is nonzero since the fj's are supposed linearly independent and its degree is at most [formula]. Therefore its valuation cannot be larger than its degree and is bounded by [formula].

Altogether, the valuation of the Wronskian is bounded by [formula].

Let [formula], and let fj = Xαj(1 + X)βj. We assume first that αj,βj  ≥  k for all j, and that the fj's are linearly independent. Note that [formula] for all j.

Let [formula] denote the Wronskian of the fj's. We can replace f1 by P in the first column of the Wronskian matrix using column operations which multiply the determinant by a1 (its valuation does not change). The matrix we obtain is the Wronskian matrix of [formula]. Now using Lemma [\ref=lemma:valinf], we get

[formula]

This inequality combined with Lemma [\ref=lemma:valsup] shows that

[formula]

We now aim to remove our two previous assumptions. If the fj's are not linearly independent, we can extract from this family a basis [formula]. Then P can be expressed in this basis as [formula]. We can apply Equation [\eqref=eq:valLinIndep] to fj1,, fjd and obtain [formula]. Since jd  ≤  k, we have j1 + d - 1  ≤  k and [formula]. The value of j1 being unknown, we conclude that

[formula]

The second assumption is that αj,βj  ≥  k. Given P, consider [formula]. Then P̃ satisfies ,  ≥  k, whence by Equation [\eqref=eq:valMax], [formula]. Since [formula] and   =  αj + k, the result follows.

In Theorem [\ref=thm:val], we can replace (1 + X) by (uX + v) for any [formula]. Indeed, we can write [formula] and then use the change of variables [formula]. This gives us a polynomial of the same form as in the theorem, with the same valuation as the original one.

Theorem [\ref=thm:val] does not hold in positive characteristic as shown by the equality (1 + X)2n + (1 + X)2n + 1 = X2n(1 + X) mod 2. Section [\ref=section:posChar] investigates the case of positive characteristic in more details.

We argued after Lemma [\ref=lemma:valinf] that it can be refined. In the previous proof, it is used with P, f2, , fk. If all the fj's have the same valuation α, Equation [\eqref=eq:refinement] gives the bound [formula], whence [formula]. In this case, replacing Lemma [\ref=lemma:valinf] by Equation [\eqref=eq:refinement] gives us a new proof of Hajós' Lemma, with the correct bound.

On the other hand, if the fj's have pairwise distinct valuations, Equation [\ref=eq:refinement] gives the same bound as Lemma [\ref=lemma:valinf]. Yet in this case Lemma [\ref=lemma:valsup] can be refined to obtain the bound [formula]. Again, we find the optimal bound for the valuation, that is [formula] here.

The refinement of Lemma [\ref=lemma:valinf] alone is not sufficient to improve Theorem [\ref=thm:val] in the general case. To this end, one needs to improve Lemma [\ref=lemma:valsup] as well. As already mentioned, it is an open problem to determine the best achievable bound for Theorem [\ref=thm:val]. The next proposition shows that it cannot be as low as in Hajós' Lemma.

For k  ≥  3, there exists a linearly independent family of polynomials (Xαj(1 + X)βj)1  ≤  j  ≤  k, [formula] and a family of rational coefficients (aj)1  ≤  j  ≤  k such that the polynomial

[formula]

is nonzero and has valuation α1 + (2k - 3).

A polynomial that achieves this bound is

[formula]

where

[formula]

We aim to prove that Pk(X) = X2k + 3. Since it has (k + 3) terms and α1 = 0, this proves the proposition. To prove the result for an arbitrary value of α1, it is sufficient to multiply Pk by some power of X.

It is clear that Pk has degree (2k + 3) and is monic. Let Pk be the coefficient of the monomial Xm in Pk. Then for m > 0

[formula]

We aim to prove that Pk = 0 as soon as m < 2k + 3. Using the definition of the aj's, this is equivalent to proving

[formula]

To prove this equality, we rely on Wilf and Zeilberger's algorithm [\cite=PetkovsekWilfZeilberger], and its implementation in the Maple package EKHAD of Doron Zeilberger (see [\cite=PetkovsekWilfZeilberger] for more on this package). The program asserts the correctness of the equality and provides a recurrence relation satisfied by the summand that we can verify by hand.

Let F(m,j) be the summand in equation [\eqref=eq:coeffPk] divided by [formula]. We thus want to prove that [formula]. The EKHAD package provides

[formula]

and claims that

[formula]

In the rest of the proof, we show why this claim implies Equation [\eqref=eq:coeffPk], and then that the claim holds.

Suppose first that Equation [\eqref=eq:WZ] holds and let us prove Equation [\eqref=eq:coeffPk]. If we sum Equation [\eqref=eq:WZ] for j = 0 to k, we obtain

[formula]

Since R(m,0) = 0 and F(m,k + 1) = 0, [formula] is constant with respect to m. One can easily check that the sum is 1 when m = 2k + 2. (Actually the only nonzero term in this case is for j = k.) Therefore, we deduce that for all m < 2k + 3, [formula], that is Equation [\eqref=eq:coeffPk] is true.

To prove Equation [\eqref=eq:WZ], note that

[formula]

and

[formula]

Therefore, to prove the equality, it is sufficient to check that

[formula]

This is done by a mere computation.

From Theorem [\ref=thm:val], we can deduce the following Gap Theorem.

Let [formula] with u,v  ≠  0 and αj + 1  ≥  αj, 0  ≤  j < k. Assume that there exists [formula] such that

[formula]

Then P is identically zero if and only if the polynomials [formula] and [formula] are both identically zero.

In particular, the smallest [formula] satisfying [\eqref=eq:gap] is the smallest [formula] satisfying

[formula]

Let [formula] and R = P - Q. Suppose that Q is not identically zero. By Theorem [\ref=thm:val], its valuation is at most [formula]. Since [formula] for [formula], the valuation of R is at least [formula]. Therefore, if Q is not identically zero, its monomial of lowest degree cannot be canceled by a monomial of R. In other words, P = Q + R is not identically zero.

For the second part of the theorem, consider the smallest [formula] satisfying Equation [\eqref=eq:gap]. It is clear that [formula]. Moreover for all [formula], [formula]. We now prove by induction on j that [formula] for all [formula]. This is obviously true for j = 1. Let [formula] and suppose that for all i  ≤  j, [formula]. Then

[formula]

To conclude, we remark that [formula] for all i < j.

It is straightforward to extend this theorem to more gaps. The theorem can be recursively applied to Q and R (as defined in the proof). Then, if [formula] where there is a gap between Pt and Pt + 1 for 1  ≤  t < s, then P is identically zero if and only if each Pt is zero.

Algorithms

In this section, we prove that there exists a deterministic polynomial-time algorithm to test if a polynomial of the form

[formula]

is identically zero and give a deterministic polynomial-time algorithm to compute the linear factors of a lacunary bivariate polynomial. The size of P is defined by

[formula]

The algorithms use Lenstra's algorithm [\cite=Len99] or a variant of it for treating some special cases. This use of Lenstra's algorithm implies some restrictions on the field [formula] in which the coefficients of the polynomials lie. In this section, [formula] is an algebraic number field, and it is represented as [formula] where [formula] is a monic irreducible polynomial. Elements of [formula] are given as vectors in the basis [formula]. That is for [formula], [formula] with et = nt / dt for each t where [formula]. Then

[formula]

The size of a polynomial defined as above is then approximately the number of bits needed to write down its binary representation.

Theorems [\ref=thm:pit] and [\ref=thm:factorization] were already proven in [\cite=KK05]. We give here new proofs based on our Gap Theorem. The structures of the algorithms we propose are the same as in [\cite=KK05]. The only differences are the ones induced by the use of a different Gap Theorem. This implies some differences in terms of the complexity that are discussed at the end of this section.

There exists a deterministic polynomial-time algorithm to decide if a polynomial of the form [\eqref=eq:linear] is identically zero.

We assume without loss of generality that αj + 1  ≥  αj for all j and α1 = 0. If α1 is nonzero, Xα1 is a factor of P and we consider P / Xα1.

Suppose first that u = 0. Then P is given as a sum of monomials, and we only have to test each coefficient for zero. Note that the αj's are not distinct. Thus the coefficients are of the form [formula]. Lenstra [\cite=Len99] gives an algorithm to find low-degree factors of univariate lacunary polynomials. It is easy to deduce from his algorithm an algorithm to test such sums for zero. A strategy could be to simply apply Lenstra's algorithm to [formula] and then check whether (X - v) is a factor, but one can actually improve the complexity by extracting from his algorithm the relevant part. The case v = 0 is similar.

We assume now that u,v  ≠  0. We split P into small parts [formula], such that according to the Gap Theorem, P is identically zero if and only if each part Pt is identically zero. Formally, let I1, , Is be the (unique) partition of [formula] into intervals defined recursively as follows. Let 1∈I1. For 1  ≤  j < k, suppose that [formula] has been partitioned into [formula], and let it be the smallest element of It. Then (j + 1)∈It if [formula], and (j + 1)∈It + 1 otherwise. The polynomials [formula] satisfy the conditions of Theorem [\ref=thm:gap]. Therefore, we are left with testing if the Pt's are identically zero. Moreover, Xαit divides Pt for each t and it is thus equivalent to be able to test if each Pt / Xαit is identically zero.

To this end, let Q be a polynomial of the form [\eqref=eq:linear] satisfying α1 = 0 and [formula] for all j. In particular, [formula]. Consider the change of variables Y = uX + v. Then

[formula]

is identically zero if and only if Q(X) is. We can express Q(Y) as a sum of powers of Y:

[formula]

There are at most [formula] monomials. Then, testing if Q(Y) is identically zero consists in testing each coefficient for zero. Moreover, each coefficient has the form [formula] where the sum ranges over at most k indices. Since [formula] for all j, the terms in these sums have polynomial bit-lengths. Therefore, the coefficients can be tested for zero in polynomial time.

Altogether, this gives a polynomial-time algorithm to test if P is identically zero.

Let

[formula]

There exists a deterministic polynomial-time algorithm that finds all the linear factors of P, together with their multiplicities.

A linear factor of P is either of the form (Y - uX - v) or (X - a). To search factors of the form (X - a), we see P as a univariate polynomial in Y whose coefficients are univariate polynomials in X. Then, (X - a) is a factor of P if and only if it is a factor of all the coefficients of P viewed as a polynomial in Y. Lenstra gives an algorithm to compute linear factors of univariate lacunary polynomials [\cite=Len99]. Thus, we can find all the factors of the form (X - a) and their multiplicities using his algorithm.

Now (Y - uX - v) is a factor of P if and only if P(X,uX + v) vanishes identically. We can assume that u  ≠  0. If v = 0, [formula]. Therefore, it vanishes if and only if each coefficient vanishes. But a coefficient of this polynomial is of the form [formula]. Testing such a coefficient for zero is done in polynomial time using Lenstra's algorithm as in the proof of Theorem [\ref=thm:pit], and there are at most k of them to test.

Suppose now that u,v  ≠  0. Since P(X,uX + v) is of the form [\eqref=eq:linear], we can use our Gap Theorem (Theorem [\ref=thm:gap]) as in the proof of Theorem [\ref=thm:pit]: Let [formula] where each Pi is of the form [\eqref=eq:linear] and satisfies α1 = 0 and [formula]. Then by Theorem [\ref=thm:gap], P(X,uX + v) vanishes if and only if Pi(X,uX + v) vanishes for every i. Now apply the same transformation to each Pi, inverting the roles of X and Y. Then each Pi can be written as the sum [formula] where each [formula] is of the form [\eqref=eq:linear] and satisfies α1  =  β1 = 0 and [formula]. Furthermore, P(X,uX + v) vanishes if and only if all the [formula] vanish.

Since the [formula]'s are low-degree polynomials, and there are at most k of them, one can find all their linear factors. This relies on one of the numerous deterministic polynomial-time algorithms to factor dense multivariate polynomials that appear in the literature, from [\cite=Kal85] [\cite=Len87] to [\cite=Gao03] [\cite=Lec07]. By the above discussion, the linear factors of P are exactly the linear factors that all the [formula]'s have in common. Several strategies can be used to find these linear factors: Either we search the linear factors of all the [formula]'s and keep only the ones they have in common, or we search the linear factors of one particular [formula] (for instance the one of smallest degree) and test if they are factors of the other [formula]'s using our PIT algorithm, or we compute the gcd of all the [formula]'s and then search its linear factors. In particular, this last solution directly gives the multiplicities of the factors of P, since it is the same as their multiplicities in the gcd.

As Kaltofen and Koiran's algorithm [\cite=KK05], our algorithm uses Lenstra's algorithm for univariate lacunary polynomials [\cite=Len99] to find univariate factors of the input polynomial. To compare both algorithms, let us thus focus on the task on finding truly bivariate linear factors, that is of the form (Y - uX - v) with uv  ≠  0.

A first remark concerns the simplicity of the algorithm. The computation of the gap function is much simpler in our case since we do not have to compute the height of the coefficients. This means that the task of finding the gaps in the input polynomial is reduced to completely combinatorial considerations.

Both our and Kaltofen and Koiran's algorithms use a dense factorization algorithm as a subroutine. This is in both cases the main computational task since the rest of the algorithm is devoted to the computation of the gaps in the input polynomial. Thus, a relevant measure to estimate the complexity of these algorithms is the maximum degree of the polynomials given as input to the dense factorization algorithm. This maximum degree is given by the values of the gaps in the two Gap Theorems. In our algorithm, the maximum degree is [formula]. In Kaltofen and Koiran's, it is O(k log k + k log hP) where hP is the height of the polynomial P and the value log (hP) is a bound on the size of the coefficients of P. For instance, if the coefficients of P are integers, then hP is the maximum of their absolute values. Therefore, our algorithm has a better asymptotic complexity as soon as the size of the coefficients exceeds the number k of terms. Furthermore, the hidden constant in the bound for Kaltofen and Koiran's algorithm is only known to be bounded by approximately 15 while the corresponding constant in our case is 1 / 2.

Note that an improvement of Theorem [\ref=thm:val] to a linear bound instead of a quadratic one would give us a better complexity than Kaltofen and Koiran's algorithm for all polynomials. Finally, it is naturally possible to combine both Gap Theorems in order to obtain the best complexity in all cases.

Generalizations

In this section, we aim to prove some generalizations of the results obtained in Sections [\ref=sec:val] and [\ref=sec:algo]. The field [formula] is still supposed to be an algebraic number field as in Section [\ref=sec:algo], unless otherwise stated.

Our first generalization shows that the identity test algorithm of Theorem [\ref=thm:pit] can be extended to a slightly more general family of polynomials. Namely, the linear polynomial (uX + v) can be replaced by any 2-sparse polynomial.

Let [formula]. There exists a deterministic polynomial-time algorithm to decide if the polynomial P is identically zero.

In the theorem, (uXd + v) could be replaced by the seemingly more general expression (uXd + vXd') with d > d' > 0. Yet, in this case we can factor out Xd'. A term Xαj(uXd + vXd')βj can thus be written Xαj + d'βj(uXd - d' + v)βj. This has the same form as in the theorem, replacing αj by (αj + d'βj) and d by (d - d').

The size of the polynomial in the statement of the theorem is defined as in Equation [\eqref=eq:sizeP] of Section [\ref=sec:algo] with the additional term log d in the sum. This means that the complexity of the algorithm is still polylogarithmic in the degree.

For all j we consider the Euclidean division of αj by d: αj = qjd + rj with rj < d. We rewrite P as

[formula]

Let us group in the sum all the terms with a common rj. That is, let

[formula]

for 0  ≤  i < d. We remark that regardless of the value of d, the number of nonzero Pi's is bounded by k. We have [formula]. Each monomial Xα of XiPi(Xd) satisfies [formula]. Therefore, P is identically zero if and only if all the Pi's are identically zero.

Since each Pi is of the form [\eqref=eq:linear], and there are at most k of them, we can apply the algorithm of Theorem [\ref=thm:pit] to each of them.

We now state a generalization of Theorem [\ref=thm:val]. A special case of this generalization is used in the following to extend our factorization algorithm of Theorem [\ref=thm:factorization]. It is not known whether the most general version of the theorem can be used to further extend our algorithms to be able to find small-degree factors of lacunary polynomials.

Note that this result holds whatever field [formula] of characteristic zero is considered.

Let [formula] and

[formula]

where the degree of [formula] is di for all i. Let [formula] and denote by μi the multiplicity of ξ as a root of fi. Then the multiplicity μP(ξ) of ξ as a root of P satisfies

[formula]

A proof of this theorem is given in Appendix [\ref=app:generalization]. Note that it can be stated in the more general settings of rational exponents αij. It can then be seen as a generalization of a result of Kayal and Saha [\cite=KS11].

The following corollary, used to find multilinear factors of bivariate lacunary polynomials, is a direct consequence of the theorem.

Let [formula], [formula]. If P is nonzero, its valuation is at most [formula].

We now describe how to use this corollary to get a new factorization algorithm. Compared to Theorem [\ref=thm:factorization], we are now able to find the multilinear factors instead of the linear ones.

Let [formula]. There exists a deterministic polynomial time algorithm to compute all the multilinear factors of P, with multiplicity.

The proof goes along the same lines as the proof of Theorem [\ref=thm:factorization]. Suppose that XY - (aX - bY + c) is a factor of P. Then the rational function [formula] vanishes identically. Let us assume for simplicity that a,b,c  ≠  0. (The other cases can be handled separately, as in the proof of Theorem [\ref=thm:factorization].) Let

[formula]

where γj  =   max i(βi)  -  βj. Then Q is a polynomial and it vanishes if and only if the rational function [formula] does. By Corollary [\ref=lemma:prod3], if Q is nonzero its valuation is at most [formula]. We can deduce a Gap Theorem: For 1  ≤  k0  ≤  k, let

[formula]

and Q1 = Q - Q0. Suppose that [formula]. Then Q vanishes identically if and only if Q0 and Q1 both vanish identically. Hence, XY - (aX - bY + c) is a factor of P if and only if it is a factor of both P0 and P1, defined by analogy with Q0 and Q1: P0 is the sum of the k0 first terms of P and P1 the sum of the (k - k0) last terms.

This proves that P can be written as a sum of [formula]'s as in the proof of Theorem [\ref=thm:factorization] such that the multilinear factors of P are the common multilinear factors of the [formula]'s, and such that each [formula] is of the same form as P and satisfies [formula]. It thus remains to find the common multilinear factors of some low-degree polynomials. Since there are at most k of them, this can be done in polynomial time.

Positive characteristic

As mentioned earlier, Theorem [\ref=thm:val] does not hold in positive characteristic. We considered the polynomial (1 + X)2n + (1 + X)2n + 1 = X2n(X + 1) in characteristic 2. It only has two terms, but its valuation equals 2n. Therefore, its valuation cannot be bounded by a function of the number of terms. Note that this can be generalized to any positive characteristic. In characteristic p, one can consider the polynomial [formula].

Nevertheless, the exponents used in all the examples depend on the characteristic. In particular, the characteristic is always smaller than the largest exponent that appears. We shall show that in large characteristic, Theorem [\ref=thm:val] still holds. This contrasts with the previous result [\cite=KK05] that uses the notion of height of an algebraic number, and which is thus not valid in any positive characteristic.

In fact, Theorem [\ref=thm:val] holds as soon as [formula] does not vanish. The difficulty in positive characteristic is that Proposition [\ref=prop:wronskian] does not hold anymore. Yet, the Wronskian is still related to linear independence by the following result (see [\cite=Kaplansky]):

Let [formula] be a field of characteristic p and [formula]. Then f1, , fk are linearly independent over [formula] if and only if their Wronskian does not vanish.

This allows us to give an equivalent of Theorem [\ref=thm:val] in large positive characteristic.

Let [formula] with [formula]. If the characteristic p of [formula] satisfies p >  max j(αj  +  βj), then the valuation of P is at most [formula], provided P does not vanish identically.

Let fj = Xαj(1 + X)βj for 1  ≤  j  ≤  k. The proof of Theorem [\ref=thm:val] has two steps: We prove that we can assume that the Wronskian of the fj's does not vanish, and then under this assumption we get a bound of the valuation of the polynomial. The second part only uses the non-vanishing of the Wronskian and can be used here too. We are left with proving that the Wronskian of the fj's can be assumed to not vanish when the characteristic is large enough.

Assume that the Wronskian of the fj's is zero: By Proposition [\ref=prop:wronskian], there is a vanishing linear combination of the fj's with coefficients bj in [formula]. Let us write [formula]. Then [formula]. Since deg fj  =  αj  +  βj < p, [formula] for all i. We have thus proved that there is a linear combination of the fj's equal to zero with coefficients in [formula]. Therefore, we can assume we have a basis of the fj's whose Wronskian is nonzero and use the same argument as for the characteristic zero.

Based on this result, the algorithms we develop in characteristic zero for PIT and factorization can be used for large enough characteristics. Computing with lacunary polynomials in positive characteristic has been shown to be hard in many cases [\cite=vzGKS96] [\cite=KaShp99] [\cite=KiSha99] [\cite=KK05] [\cite=BCR12] [\cite=KL13]. In particular, it is shown in a very recent paper that it is [formula]-hard to find roots in [formula] for polynomials over [formula] [\cite=BCR12].

Let [formula] be the field with ps elements for p a prime number and s > 0. In the algorithms, it is given as [formula] where φ is a monic irreducible polynomial of degree s with coefficients in [formula].

Let [formula], where p >  max j(αj  +  βj). There exists a polynomial-time deterministic algorithm to test if P vanishes identically.

The proof of this theorem is very similar to the proof of Theorem [\ref=thm:pit], using Theorem [\ref=thm:valPosChar] instead of Theorem [\ref=thm:val]. The main difference occurs when u = 0 or v = 0. In these cases, we rely in characteristic zero on an external algorithm to test sums of the form [formula] for zero. This external algorithm does not work in positive characteristic, but these tests are actually much simpler. These sums can be evaluated using repeated squaring in time polynomial in log βj, that is polynomial in the input length.

Note that the condition p >  max j(αj  +  βj) means that p has to be greater than the degree of P. This condition is a fairly natural condition for many algorithms dealing with polynomials over finite fields, especially prime fields, for instance for root finding algorithms [\cite=BCR12].

The basic operations in the algorithm are operations in the ground field [formula]. Therefore, the result also holds if bit operations are considered. The only place where computations in [formula] have to be performed in the algorithm is the tests for zero of coefficients of the form [formula] where the αj's and [formula]'s are integers and [formula], and the sum has at most k terms. The binomial coefficient is to be computed modulo p using for instance Lucas' Theorem [\cite=Lucas1878].

We now turn to the problem of finding linear factors of lacunary bivariate polynomials.

Let [formula], where p >  max j(αj  +  βj). There exists a probabilistic polynomial-time algorithm to find all the linear factors of P of the form (uX + vY + w) with uvw  ≠  0.

Furthermore, deciding the existence of factors of the form (X - w), (Y - w) or (X - wY) with w  ≠  0 is [formula]-hard under randomized reductions.

The second part of the theorem is the consequence of the [formula]-hardness (under randomized reductions) of finding roots in [formula] of lacunary univariate polynomials with coefficients in [formula] [\cite=KiSha99] [\cite=BCR12] [\cite=KL13]: Let Q be a lacunary univariate polynomial over [formula], and define P(X,Y) = Q(X). Then P has the same form as in the theorem with βj = 0 for all j, and factors of the form (X - w) of P are in one-to-one correspondence with roots w of Q. Thus, detecting such factors is [formula]-hard under randomized reductions. The same applies to factors of the form (Y - w). Finally, let us now define P as the homogeneization of Q, that is P(X,Y) = Ydeg (Q)Q(X / Y). Then, P(wY,Y) = Ydeg (Q)P(w,1) = Ydeg (Q)Q(w). In other words, factors of P of the form (X - wY) correspond to roots w of Q. Thus detecting such factors is also [formula]-hard under randomized reduction.

For the first part, the algorithm we propose is actually the same as in characteristic zero (Theorem [\ref=thm:factorization]). This means that it relies on known results for factorization of dense polynomials. Yet, the only polynomial-time algorithms known for factorization in positive characteristic are probabilistic [\cite=vzGG03]. Therefore our algorithm is probabilistic and not deterministic as in characteristic zero.

Proof of Theorem [\ref=thm:generalization]

Let [formula] for 1  ≤  j  ≤  k. As in the proof of Theorem [\ref=thm:val], we first assume that the Pj's are linearly independent, and the αij's not less than (k - 1).

We can use a generalized Leibniz rule to compute the derivatives of the Pj's. Namely

[formula]

where [formula] is the multinomial coefficient. Consider now a derivative of the form (fα)(t). This is a sum of terms, each of which contains a factor fα - t. (The worst case happens when t different copies of f have been each derived once.) In Equation [\eqref=eq:leibnizP], each ti is bounded by l. This means that [formula] for some polynomial Ql,j. Since the degree of P(l)j equals [formula], Ql,j has degree [formula].

Consider now the Wronskian [formula] of the Pj's. We can factor out in each column [formula] and in each row [formula]. At row l and column j, we therefore factor out [formula]. Thus,

[formula]

where Ml,j = Ql,j. Thus, det M is a polynomial of degree at most [formula].

Therefore, the multiplicity [formula] of ξ as a root of [formula] is bounded by its multiplicity as a root of [formula] plus the degree of det M. We get

[formula]

To conclude the proof, it remains to remember Lemma [\ref=lemma:valinf] and use the same proof technique as in Theorem [\ref=thm:val]. It was expressed in terms of the valuation of the polynomials, but remains valid with the multiplicity of a root. In this case, it can be written as [formula] where [formula] is the Wronskian of the Pj's. Using column operations, we can replace the first column of the Wronskian matrix of the Pj's by the polynomial P and its derivatives. We get [formula], where [formula].

Together with [\eqref=eq:multUpBd], we get

[formula]

It remains to remove our two assumptions. If the Pj's are not linearly independent, we can extract a basis [formula]. We obtain [formula]. Since d  ≤  k + 1 - j1, we have

[formula]

The second assumption is that αij  ≥  k - 1 for all i and j. Let

[formula]

Since   =  αij + k - 1  ≥  k - 1,

[formula]

Since [formula], the result follows.

The ordering of the Pj's in the theorem is arbitrary. Yet the value of the bound depends on this ordering. Therefore, it is possible to optimize this bound by using the ordering on the Pj's that minimizes the bound. Let us define

[formula]

The theorem states that μP(ξ)  ≤   max jsj. Let j1 < j2 such that [formula]. Then sj1 > sj2. These two terms appear in the maximum when Pj1 is before Pj2 in the ordering. If Pj1 and Pj2 are exchanged, the two terms are replaced by [formula] and [formula]. Neither term is greater than sj1. This means that an exchange of Pj1 and Pj2 in the ordering cannot increase the bound in the theorem.

This proves that to minimize the bound the Pj's must be ordered with respect to the value of [formula]. This is consistent with the order on the αj's chosen in Theorem [\ref=thm:val]. We also note that the bound in Theorem [\ref=thm:val] is exactly recovered as a special case.