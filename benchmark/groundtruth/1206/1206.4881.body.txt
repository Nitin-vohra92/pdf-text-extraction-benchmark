=1

The Two-Way Likelihood Ratio (G) Test and Comparison to Two-Way χ2 Test

One-Way Likelihood Ratio or χ2 test

Suppose we have a set of data [formula] and two hypotheses HR and HS. We wish to know which hypothesis explains the data better. To do this, we compute the likelihood ratio

[formula]

Assuming the data are i.i.d given each hypothesis, we have [formula], where J∈R,S, and thus the likelihood ratio is

[formula]

The Bayesian formulation of the problem could be approached by parameterising HR and HS with some unknown parameters, θR and θS, respectively. The posterior distribution over these parameters is then given by integrating the likelihoods over all possible values

[formula]

These integrations can sometimes be performed analytically, or using some numerical integration techniques. However, we will focus instead on a simple heuristic method which is related to the χ2 statistics discussed above. Note that David MacKay [\cite=MacKayBayesNote] explicitly assumes the parameters have an 'intrinsic' arity to them (multinomials with an intrinsic number of bins). This assumption may not be always correct, and in fact, may lead to incorrect assumptions.

Now suppose that the hypotheses are multinomial probability distributions [formula], with the constraint that [formula], and each ri corresponds to some range (bin) of the data [formula] (and similarly we have si for HS), then the likelihood ratio can be written as a sum over the N bins by grouping terms in Equation [\ref=lrat1] into the bins:

[formula]

where Fi is the number of data that fall into bin i.

The equivalent chi-squared test is to compute the χ2 statistic for each hypothesis

[formula]

and compare them, choosing the one with the smaller χ2.

David MacKay argues effectively for the use of the likelihood ratio [\cite=MacKayBayesNote]. We will see in more detail the conditions in which the chi-squared test is not applicable in Section [\ref=sec:gtest].

Two-Way Likelihood Ratio Test

If we wish to compare two sets of data, [formula] and [formula], and ask whether they are drawn from the same distribution or from two different distributions, then our first hypothesis is that there are two models HR and HS to explain the data, and the second hypothesis is that there is a single model HR + S that explains the data. Thus, the question can be formulated as the likelihood ratio

[formula]

where we have made the assumption that [formula] is independent of Hs (and vice-versa) if the two distributions are different, and that [formula] is independent of [formula] given HR + S if the two distributions are the same, both of which are true given the i.i.d assumption of data given hypotheses.

The Bayesian formulation of the problem is to parameterise HR,HS and HR + S with some unknown parameters, θR,θS and θR + S, respectively. The likelihoods in ([\ref=likerat1]) are then given by integrating over all possible parameter values

[formula]

These integrations can sometimes be performed analytically, or using some numerical integration techniques. However, in this note, we will use the most likely estimate for the parameters, given the data. This simple method is related to the χ2 statistics discussed above, but will see some limitations of it in Section [\ref=sec:gtest].

We can estimate the parameters of HR directly from the data, as the most likely estimate using a multinomial with values ri = Ri / R, with Ri being the number of data points in [formula] that fall into bin i, and [formula]. Similarly for Hs is a multinomial si = Si / S, and [formula]. Finally, we can estimate HR + S in the same way given both datasets, to give a multinomial with values (Ri + Si) / (R + S). Using the same transformation (from data to bins) as above, the likelihood ratio becomes

[formula]

which is simply the weighted sum of the Kullback-Leibler divergences of the two datasets from the average distribution

[formula]

where [formula] is the probability of a data point falling in bin i estimated from both sets of data. It is also a symmetrised relative entropy measure comparing the data to its own distribution (e.g. Ri to Ri / R) and to the average distribution of both sets of data ((Ri + Si) / (R + S). We can see this better by expanding out the logs of fractions as differences of logs and cancelling terms to obtain.

[formula]

or

[formula]

The first term is the (negative) entropy of the distribution ri (scaled by the number of datapoints), the second is the negative entropy of si, and the third is the entropy of the joint distributions. Denoting γr,γs,γp as the entropy of ri, si and pi, respectively, we have

[formula]

where the entropy γ(x) =  - x log (x). Equation [\ref=likerat-ent] can be understood by noting that if the two distributions HR and HS are the same, then averaging them will make no difference to the entropy of the distributions. If, on the other hand, HR and HS are different, then the average of the two will have higher entropy. Thus, γp will be larger if the distributions are different, making L also larger (due to the negative sign), which is what we expect from the original definition of the likelihood ratio for the two-way problem as given in ([\ref=likerat1]).

More precisely, it is the case that the sum of the entropy of any two probability distributions will be less than the entropy of their average. To show this, note that the entropy γ(x) =  - x log (x) is a concave function, meaning every point on every chord lies on or below the function [\cite=BishopBook06], so that

[formula]

where α  +  β = 1, and equality is achieved when r = s. By induction, this is true even for a weighted sum:

[formula]

If we use [formula] and [formula], then pi  =  αri  +  βsi, and Equation ([\ref=ineq]) says that the square bracket in Equation ([\ref=likerat-ent2]) is always negative, so that L  ≥  0. The extreme cases are

ri and si are identical, then L = 0.

ri = 0 for all i where si > 0, and si = 0 for all i where ri > 0. In this case, either ri or si is zero, and

[formula]

Since α  +  β = 1, this function has a maximum of (R + S) / 2 at α = 0.5, and a minimum of 0 at α = 1 or 0.

Thus, we can see that [formula], with the minimum achieved for identical distributions, and the maximum achieved for maximally different distributions.

Two-Way χ2 test

If instead, we use the two-way χ2 test, we compute the expected counts, which is the average distribution of the two datasets. Since [formula] is the average distribution given both sets of data, we have the expected counts in bin i for the two datasets as

[formula]

In many treatments of this problem, particularly in the biological sciences, the [formula] are referred to as the rows and the datasets {R,S} are referred to as the columns in a contingency table. Typically, the rows are a set of features of the data, and the columns are two different datasets, usually obtained in two different conditions.

To answer the question of whether the two datasets are drawn from the same hypothesis or not, we formulate the null hypothesis, which states that they are, and then figure out the expected counts as above. The chi-squared statistic for the two sets of data is

[formula]

putting in the definitions of the expected counts from ([\ref=expcounts]) above, and doing some algebra, we get

[formula]

exactly equation (14.3.3) in [\cite=NumRec].

This value of χ2, if large, tells us that the null hypothesis can be rejected, and thus that the distributions are likely to be different. To know what "large" means, we can use a chi-squared probability test, that gives us the probability that the sum of the squares of ν random normal variables of unit variance and zero mean will be greater than χ2 [\cite=NumRec]. Another way to say this is the probability that a particular value of χ2 would have occurred by chance if the null hypothesis was correct. The chi-squared probability test is therefore simply the integral of the probability density of the χ2 distribution:

[formula]

The number of degrees of freedom in the hypotheses is ν. If the two datasets are drawn without regard for each other (no constraints on the number of datapoints drawn), then the number of degrees of freedom, ν, is the number of bins in which one of the datasets has at least one count. Typically, if P(χ2|ν)  <  0.05 (the "p-value"), the chi-squared test is deemed significant, and the null hypothesis can be safely rejected. A simple test that can be used is to reject the null hypothesis if χ2  >  ν [\cite=NumRec](p661).

One- and Two-Way G-test

Interestingly, the likelihood ratio can be more formally related to the χ2 test, by considering the G-test, defined as [\cite=Sokal94]

[formula]

where Oi is the observed counts and Ei is the expected counts. Note that this is simply the Kullback-Leibler divergence between observed and expected counts, multiplied by a factor of two. When summed over all data points in our two-column example, this is

[formula]

putting in the expressions for the expected counts from above ([\ref=expcounts]), we obtain exactly G = 2L, given by Equation ([\ref=gtest]) above. In general, with smaller amounts of data, the chi-squared test will sometimes give incorrect answers, whereas the G-test will not, and so is the recommended test [\cite=MacKayBayesNote] [\cite=Sokal94]. To see in more detail why this is so, we can write Oi = Ei  +  δi, with [formula] so that the total number of counts stays the same. The G-test is then

[formula]

If we Taylor expand this around [formula] (the point at which Oi and Ei agree), and using [formula], we get

[formula]

and so, we see that G  ≈  χ2 when Oi is close to Ei. However, the more Oi and Ei are different, the less well this approximation will work, and χ2 will tend to compute erroneous answers. The effects of a single outlier in a small sample set will be more pronounced, which explains why the χ2 often fails in situations with little data. This is the same reason why a linear regression can fail with little data, due to the strong effects of outliers.

Since the χ2 value is just an approximation to the G-value, the G-value can also be used in the chi-squared probability test. This method is recommended by most texts on statistics for the biological sciences. However, it is unclear why one would want to do this, and what the validity is since the chi-squared test is based on the pdf of χ2. The G-test directly gives (twice) the log likelihood of the ratio of one hypothesis vs. the other, and so a significance can be attributed directly. However, recall that these tests are both based on models or hypotheses whose parameters are derived from the data itself. Instead of computing Equation ([\ref=likeratbayes]) directly, as we should do, we are taking the most likely estimate of the parameters θR,θS and θR + S (those derived directly from the data), and collapsing the integrals to these point estimates. One implication of this is that the G-values will depend on the complexity of our models (e.g. the number of bins in our multinomials/histograms). This is simply the model overfitting the data: the models derived from each data set R and S will, with enough complexity, perfectly fit the data. Therefore, to interpret the G-value from Equation ([\ref=eqn:gtest]), we must take the complexity of the model into account. To evaluate significance, the value of the likelihood ratio (G/2) should be compared to the number of degrees of freedom, ν. If G  >  2ν, then the null hypothesis can be safely rejected. This corresponds roughly to a p < 0.05.

Likelihood ratio tests for dynamic models

In the previous sections, we assumed the data were i.i.d distributed, and that the models (hypotheses) were simple multinomials. It is also possible that the data are sequentially dependent, such as when they come from a dynamic model. For example, if the data arise from a hidden Markov model, then the same considerations apply as above. For any type of model HJ,J∈{R,S,R + S} trained on the data in J, we can compute each of [formula], [formula], [formula] and [formula], and then use Equation ([\ref=likerat1]) to compute the likelihood ratio, and use a chi-squared probability test as usual. If the H are hidden Markov models, then the likelihoods will be computed using the standard forward equations [\cite=Dempster77].

Acknowledgements

Thanks to Chris Williams for explaining the factor of 2 in G and its relationship to χ2, to Stephen McKenna for pointing to the Bayesian solution for the problem of integrating over all parameters, which resolves the issue of why a significance test is necessary, and to Olivia Stevenson for pointing out the possibility for emotional creativity.