21cm 20pt 0pt

A new approach for physiological time series

Introduction

An understanding of physiological time series such as the heart-beat intervals is important to many areas, like heart-attack prediction, cardiovascular health, sport and exercise, etc. The study of time series can reveal underlying mechanisms of the physiological system, which usually contains both deterministic and stochastic components. Therefore the analysis of time series is very complicated because of the nonlinear and non-stationary characteristics of physiological time series data. Over the past years, time series analysis methods are applied to quantify physiological data for identification and classification [\citep=Kantz] [\citep=Schreiber]. The applications of physiological time series analysis commonly focus on measuring different aspects of time series data such as complexity, regularity, predictability, dimensionality, randomness, self similarity, etc. The tools used in these techniques include but not restrict to the mean, standard deviation, Fourier transform, wavelet, entropy, fractal dimension, pattern detection [\citep=Kantz2] [\citep=Tong].

Recently a new mathematical tool, empirical mode decomposition (EMD), was proposed by Norden Huang and his collaborators [\citep=Huang] [\citep=Huang2]. It decomposes a time series into a finite sum of intrinsic mode functions (IMF) that generally admit well-behaved Hilbert transforms. This decomposition is based on the local characteristic time scale of the data, which makes EMD applicable to analyze nonlinear and non-stationary signals. EMD and Hilbert transform together, called the Hilbert-Huang transform (HHT), usually allow to construct meaningful time-frequency representations of signals using instantaneous frequency of the data. EMD and HHT have been applied with great success in many application areas such as biological and medical sciences, geology, astronomy, engineering, and others [\citep=Huang] [\citep=Chen] [\citep=Echeverria] [\citep=Huang2] [\citep=Pines] [\citep=Liu]. Another interesting set of examples is the work of L.Yang and his collaborators, who have successfully applied EMD based techniques for texture analysis and Chinese handwriting recognition [\citep=Yang] [\citep=Yang2] [\citep=Zheng].

The main purpose of this paper is to develop a new approach for the analysis of physiological times series. Our approach is motivated by two intuitions and coupled with modern machine learning techniques. The first intuition comes from a belief that a physiological system should contain a deterministic part that reflects the basic mechanism for the system to survive and a stochastic part that represents the variability of resilience. Mathematically they can be represented by the low frequency and high frequency components of a physiological signal. This motivates the application of methods of decomposing signals into various components according to frequencies in the quantitative analysis of physiological time series. Examples include the Fourier transform, wavelets, and EMD. In our method we will use an iterative convolution filter which is an alternative of EMD. The second intuition comes from a statistical perspective of irregularity. A lot of study has proved that normal physiological systems show irregularity due to the existence of stochastic components while the decrease of irregularity usually implies the abnormality. From statistical perspective, irregularity of a data set is represented by the "outliers". This motivates us to study the statistics of outliers in physiological time series. However, we must be careful in doing so. Practical physiological times series usually contain noise which may also appear as outliers. We have to guarantee the "outliers" we examined are not pure noise. This is possible because true outliers do not have informative structures and could be detected. The second intuition is the motivation for our feature construction in Section [\ref=sec:feature].

These two intuitions enable us to decompose the physiological times series and construct features for our quantitative analysis. Combining with the well established feature selection techniques in machine learning we can remove the redundancy of the features and find relevant statistics for classification of physiological time series. Support vector machine recursive feature elimination(SVM-RFE) is suggested in this paper for linear classification problems. The details of our approach will be described in Section [\ref=sec:method].

We will use our approach to analyze the heart beat interval time series and study the congestive heart failure problem. The study of heart diseases such as congestive heart failure by using heart beat interval times series has a long history. Decrease of heart rate variability or cardiac chaos has been found in congestive heart failure [\citep=poon1997decrease] [\citep=casolo1989decreased]. In the literature, many methods and metrics have been proposed to analyze the difference between the heart rate times series of healthy people and congestive heart failure patients, to name a few, the detrended fluctuation analysis [\citep=peng1995quantification], multifractality [\citep=ivanov1999multifractality], multiscaling entropy [\citep=med1], hierarchical entropy [\citep=jiang2011hierarchical]. Our approach is different from the methods in the literature. It incorporates advanced machine learning techniques and allows the data "to speak by itself." By applying our approach, the purposes are two-fold: The first is to build good classifiers to enable good diagnosis. The second is to find what kind of irregularity is associated to the heart health. The results and discussions are summarized in Section [\ref=sec:experiment].

The novelty of our method is mainly the following two points. Firstly, although we decompose the time series into components of different frequencies, we do not compare them from the frequency domain. Secondly, we proved that the outliers in a physiological time series are usually not pure noise but are informative instead. Interestingly, although this idea is motivated by physiological times series analysis, it is also found successful in the stylometry analysis of artworks [\citep=hughes2012empirical].

Method

Signal decomposition

Let L be a low pass filter. Denote by T the weak limit of the the operator (I - L)n as n  →    ∞  , i.e., for a discrete signal X and time t

[formula]

Using this operator iteratively, a signal X can be decomposed as follows: Let F1  =  T(X) and for k  ≥  2,

[formula]

After m steps we get [formula] which we call mode functions and the residual

[formula]

Then we have

[formula]

In this decomposition, roughly speaking the former mode functions are noise or high frequency components and the latter mode functions are low frequency components and R is the trend.

This procedure follows the spirit of the traditional EMD introduced in [\cite=Huang]. In the traditional EMD, the low pass filter L is chosen as the average of the upper envelope (the cubic spline connecting the local maxima) and the lower envelope (the cubic spline connecting the local minima). This method, although has been successfully used in many applications, is lack of theoretical foundation and has its limitations.

In [\cite=Lin] a new approach is proposed. In this new approach the low pass filter is a moving average generated by a mask [formula] that gives the L(X) as the convolution of a and X, i.e.,

[formula]

With this choice of L we call the operator T an iterative convolution filter. A rigorous mathematical foundation and convergence analysis is given in [\cite=Lin] [\cite=Wang2]. Note the mask [formula] is finitely supported on

[formula]

Feature extraction

After decomposing the signal into the mode functions and the trend, we need to extract statistics that can characterize the essential features of these components. This step requires a priori knowledge of the problem under consideration. It could be rather weak. But without any priori knowledge, it is difficult to get proper statistics. Also, this step is strongly problem dependent. In the following let us use the heart-beat intervals as an example to illustrate how to construct the features.

In this application, each time series is a record of heart beat intervals in 24 hours [\citep=med1]. It is first decomposed into several mode functions. To extract the features, for each mode function Fi, we first get its mean mi and standard deviation σi. By the previous studies [\citep=poon1997decrease] [\citep=casolo1989decreased] [\citep=med1] the healthy heart beats more irregularly than the unhealthy heart. This motivates us to design the statistics to measure the irregularity. To this end, we consider the terms that are larger than mi  +  σi and find their mean mi,1 and standard deviation σi,1. We also find the mean mi,2 and standard deviation σi,2 of the terms that are larger than mi + 2σi. Symmetrically we also get the mean mi, - 1 and standard deviation σi, - 1 of those terms that are smaller than mi  -  σi, and the mean mi, - 2 and standard deviation σi, - 2 of those terms that are smaller than mi - 2σi. This procedure gives us 10 statistics. Note all those terms that are outside the one or two standard deviations are in some sense "outliers" and it is natural to use their statistics (mi,j and σi,j for j = 1,2, - 1, - 2) to characterize the irregularity. Next we consider the times series Ui composed of local maxima of Fi and the time series Li composed of the local minima of Fi. These two series measure the local amplitude. For each series we compute the 10 statistics by the same procedure above as for Fi. Therefore for each mode function Fi we get 30 statistics.

Unlike in [\citep=med1], we use the whole 24-hour heart beat time series and assume we do not know the periods for different activities such as sleeping and walking. We think the statistics for different periods should be different and not all of them represent the difference between the healthy and unhealthy people. This motivates the idea of splitting the whole time series into subseries. Suppose we split the time series into K subseries for each subject. Correspondingly we also split each mode function Fi into K subcomponents, which are denoted by [formula] For each subcomponent Fki, we compute the 30 statistics as above: 10 for Fki itself, 10 for the local maxima Uki, and 10 for the local minima Lki. For each i and each statistics, we have K values from the K subcomponents. We compute the mean, the first quartile (the 25th quantile), the third quartile (the 75th quantile) of these K values to obtain 3 features. This gives 90 features. So for each model function Fi we get 120 features in total.

For physiological signals, we believe the trend and low frequency components are determined by the fundamental mechanism while the individual differences should be reflected by the high frequency components. In case that we do not have much knowledge about the disease to be diagnosed we may assume the features may also come from the trend. So the same 120 statistics are also computed for the trend component.

To represent these features, we introduce the notations for the statistics and three subscripts to indicated how the statistics is calculated. The detailed descriptions are listed in Table [\ref=table:notation].

Feature subset selection

After the above two steps we get a large number of features for the data. Usually only a small part of them are related to the diagnosis and the physiological mechanism of the disease. The task of the third step is to find the relevant ones. This will be realized by eliminating the irrelevant ones step by step.

Firstly, if a statistic is almost constant, then it is useless in the diagnosis and should be eliminated. For example, the means of the mode functions mi are all approximately zero and should be eliminated.

Next we use the SVM-RFE method [\citep=SVMRFE] to rank the features. In this method, given a set of training samples, we first train linear SVM to get a linear classifier and then rank the features according to the weights. Because of large feature size and small training samples, the classifier might not be as good. Also, the high correlation between the features may result the relevant features to have small weights. These reasons could lead the rank to be inaccurate. In order to refine the rank we eliminate the least important feature and repeat the process to re-rank the remained features. Running this process iteratively we finally get the refined rank of the features.

With this rank of features we can conclude which statistics are useful for the diagnosis and characterize the essence of the underlying physiological mechanism. Good classifiers can then be built to make accurate diagnosis.

Experiments and Results

In this section we apply our new method described above to the heart beat interval times series and report our results and conclusions.

The data set

The data set includes the heart beat interval time series of 72 healthy people and 43 CHF patients. For each subject the heart beat interval is measured for 24 hours under various activities. In our experiment we will assume the activity period is not known. The CHF of 43 patients are classified into 4 degrees where the degree I is a slight CHF and the degree IV is a severe CHF. Most CHF patients are of the degree III.

A primary study

Before using our new method, we study the classification ability of two simple statistics: mean and variance. In Figure [\ref=fig:meanvar] we plot the mean and variance of the heart beat intervals for the healthy people and CHF patients. We see that the healthy people and the CHF patients can be roughly separated. The average heart beat interval of healthy people is larger and so is the variance. It shows the heart of healthy people beats slower and more irregularly. This observation is consistent with the previous studies.

At the same time, we notice that several CHF patients falling into the cluster of healthy people show to be severe CHF patients. So we conjecture that the mean and variance might not reflect the essence of the underlying mechanism, although they have good separability.

Experiment: feature extraction

For each time series, we use the iterative convolution filter to realize the signal decomposition. In this step we need to specify the window size of the mask. It turns out it should be chosen between 50 and 100 to be stable. In our experiment it is chosen to be 50.

We then calculate the statistics proposed in Section [\ref=sec:feature]. Here we need to specify the parameter K, the number of subseries. If a statistic really captures the essence of the data set, it should be stable and independent of the choice of K once it is chosen within a reasonable interval. Our experiments show that K = 50 is a good choice. Most heart beat signals were recorded for a little bit more than 24 hours. Thus when K = 50, each subseries is around 30 minutes of record.

Previous studies have shown that healthy heart beats irregularly. In statistics, irregularity could be measured by statistics of "outliers" that are not due to noise. This motivates us to consider the statistics of upward and downward fluctuations. At the same time, from the study in Section [\ref=sec:primary] we find that a healthy heart beats slower than an unhealthy heart in average. These two intuitions enlighten us to conjecture that those larger heart beat intervals (i.e. slower heart beats) in the time series characterize the difference between the healthy people and CHF patients. To confirm this, we do a correlation analysis.

For each of the first two mode functions and each j = 1,2, - 1, - 2, we calculate and sort the means mkij and standard deviations σkij for the K = 50 subcomponents. For each order statistics we compute its correlation to the CHF disease. The result is plotted in Figure [\ref=fig:updown]. From the comparison we see that, in average, correlations of the statistics associated to upward fluctuations are larger than those associated to downward fluctuations. This observation tells that we may disregard the statistics for the downward fluctuations.

Feature ranking and subset selection

To rank the features, we randomly split the data set into two subsets as the training set and the test set, respectively. In the training set we have 50 healthy subjects and 30 CHF subjects and in the test set there are 22 healthy and 13 CHF subjects. We use the training set to build the SVM classifier and use the test set to control the accuracy. Using the SVM-RFE methods described in Subsection 2.3 we rank the features. To guarantee the stability of the rank we repeat this procedure 1000 times and choose the statistics that appear most frequently in the model.

In all 1000 repeats, the classification error on the test data set is summarized in the following table:

The top 10 features selected by the procedure are listed in Table [\ref=table:topfeatures]. We see 9 of them are related to the first two IMFs. Although the trend is in general not considered relevant, the last feature, associated to the trend, appears. We suspect a probable reason is that using only two mode functions in the signal decomposition leaves some relevant information in the trend. It is interesting to notice that these 10 statistics that appear most frequently in the model all measure the irregularity of the local amplitude. Take Statistics 1 and Statistics 7 as the example. They are obtained as the following. To get Statistics 1, for the first mode function F1, find the local maxima U1 and compute its mean m1,0,U and the standard deviation σ1,0,U. Then we choose terms greater than m1,0,U + 2σ1,0,U and find their standard deviation. To get Statistics 7, for the subcomponents of the second mode function, [formula], compute the mean mk2 and the standard deviation σk2. Then we choose terms greater than mk2 + 2σk2 of Fk2 and find their standard deviations σk2,2. Then we compute the mean of K such standard deviations. In Figure [\ref=fig01] we show the distribution of the healthy people and CHF patients using these two statistics. It is easy to see that healthy people and CHF patients are well separated.

Observing these two statistics, we find that both of them measure the ability of the heart beat to become extremely slower than usual. It leads to the conjecture that the strong adaptability of extremely slower heart beat might be the irregularity that characterizes the healthy hearts.

Reliability of the top features

We have found that the most relevant features are statistics for the "outliers" in the mode functions, i.e., those items larger than mean plus two times standard deviations, or items less than mean minus two times standard deviations. A natural question arises: "Is this accidental?" This is equivalent to ask whether the outliers taken into account are noise or informative.

In order to answer this question we further analyze these outliers. Firstly we notice that the up and down fluctuations are not balanced for both healthy people and CHF patients. The percentage of items larger than mean plus two times standard deviation for healthy people is 2.84% and those items smaller than the mean minus two stand deviation is only 2.35%. For CHF patients the percentages are 2.49% and 2.17%, respectively. This observation is the first evidence that outliers are not due to noise because otherwise they should be balanced distributed. Moreover, recall for Gaussian white noise the percentage of one-side outliers outside the two times standard deviation is 2.28%. We see the outliers for CHF is closer to it while those for healthy subjects are much larger. We think that the outliers for CHF patients involve more noise while the outliers for healthy subjects are probably informative.

To further confirm our conclusion, we do the following test: for F1, we calculate the statistics for the terms greater than the mean plus v times standard deviation with the variable v changing from 0 to 2 and investigate their correlation to the CHF disease. Here we consider mean of the 50 standard deviations of such terms in the 50 subcomponents. Note Statistics 3 in Table [\ref=table:topfeatures] corresponds to v = 2. The correlation is plotted in Figure [\ref=fig:stable]. From this analysis, we see the correlation increases with v. Such a trend appears also in other statistics. This clear trend implies that the relevancy between these statistics and the CHF disease is not accidental. Instead, we should consider the outliers informative and their properties characterize the essence difference between healthy people and CHF patients.

Conclusions and discussions

In this paper we developed a new approach for the analysis of the physiological times series. The motivation comes from that the physiological times series usually contains both deterministic and stochastic parts and they can be represented by the low and high frequency components of the times series. Our new method uses an iterative filter to realize the decomposition of the times series into high and low frequency components and study their statistics. SVM-RFE is then used to select highly relevant features.

Our method is applied to analyze the heart beat interval time series for CHF disease. The top features are found to measure the ability of hearts to beat extremely slowly. Healthy hearts show strong ability which we conjecture is due to the strong resilience to the environment and human activities.

Acknowledgement

Y. Wang was partially supported by NSF DMS-1043032 and AFOSR FA9550-12-1-0455.