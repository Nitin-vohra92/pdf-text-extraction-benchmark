Lemma Proposition Corollary Definition Claim Remark

Dirty Paper Arbitrarily Varying Channelwith a State-Aware Adversary

Introduction

In this paper, we study the problem of writing on a dirty paper in the presence of an adversary. In a celebrated paper [\cite=costa], Costa determined the capacity of an AWGN channel with an additive white Gaussian state, where the state is known non-causally only to the encoder. Using the coding scheme of Gelfand and Pinsker [\cite=gelfand-pinsker], he showed that the effect of the state can be completely nullified. The capacity of this "dirty paper channel" was shown to be equal to that of a standard AWGN channel with no state. Our aim in this work is to study communication over this dirty paper channel while under attack by a state-aware jamming adversary. We model the communication channel as an arbitrarily varying channel (AVC).

The AVC model was introduced by Blackwell et al. in a work [\cite=blackwell-ams1959] which studied the problem of communication over a channel where certain parameters of the channel were not known to the user and varied arbitrarily. The aim for the user was to communicate under any realization of these unknown parameters, which for instance, may be controlled by the adversary. It is observed that, in general, the results presented upon analysis of such AVC communication systems depend upon several factors, viz., possibility of randomization (unknown to the adversary) for the users, the probability of error criterion, assumptions on the jammer's knowledge, etc.

Several works have analysed AVC models. Restricting the discussion to the continuous alphabet case (to which this paper primarily belongs), Hughes and Narayan in [\cite=hughes-narayan-it1987] analysed the Gaussian AVC and determined its capacity under the assumption of shared randomness and maximal probability of error criterion. Here, the jammer was assumed to be oblivious of the user's signal transmitted on the channel. The case where the jammer knew the transmitted codeword, was analysed in [\cite=agarwal]. In [\cite=sarwate-spcom2012], Sarwate considered a 'myopic' adversary, i.e., an adversary which listens to the channel and observes a noisy version of the transmitted codeword, and determined the capacity of such a Gaussian AVC. The scenario when the encoder possessed only private randomness but not randomness shared with the decoder, was analysed in [\cite=haddadpour-isit2013]. In a work related to ours, the Gaussian AVC with a state was studied in [\cite=sarwate-isit2008]. Unlike our setup, however, the adversary was assumed to be unaware of the random state. In addition, the user was allowed only deterministic codes while the error criterion was the average probability of error. For an in depth discussion on AVCs, one may refer [\cite=lapidoth-narayan-it1998], [\cite=sarwate-thesis] and the references therein.

In this paper, we assume that the encoder and decoder share randomness which is unknown to the jammer and consider the maximal probability of error as the error criterion. Similar to the encoder, the adversary in this model is a state-aware entity, i.e., it possesses a non-causal knowledge of the state. The main result of this work is the determination of the capacity of this Gaussian AVC. We show that the capacity achieving scheme is a dirty paper coding scheme. Interestingly, the state-aware adversary completely disregards the state knowledge and essentially performs independent and identically distributed (i.i.d.) Gaussian jamming, independent of the state.

This problem is related to our earlier work [\cite=amitalok-costa-isit2014]. There, for the very same AWGN channel with an additive white Gaussian state and a state-aware adversary, we took a game-theoretic approach to model the user-jammer interaction as a mutual information (zero sum) game [\cite=medard-1997] and analysed its Nash equilibrium. We defined the capacity of the resulting channel as the unique Nash equilibrium utility [\cite=owen] of this zero sum game, which we determined. We also identified an interesting elementary equilibrium pair of user and jammer strategies. We showed that at equilibrium, similar to the result in this work, the user chose a dirty paper coding scheme while the jammer performed i.i.d. Gaussian jamming, independent of state. The following is the organization of the paper. In Section [\ref=sec:system:model], we describe the communication setup and provide the problem details. We state the main result of this work in Section [\ref=sec:main:result]. Next, in Section [\ref=sec:analysis] we perform the analysis and prove the main result. In Section [\ref=sec:discussion], we briefly discuss the discrete memoryless channel version of this problem and make some overall concluding remarks in Section [\ref=sec:conclusion].

System Model and Problem Description

The dirty paper AVC setup is depicted in Fig. [\ref=fig:main:setup]. The transmitter aims to send a message M to the receiver through n channel uses in the presence of an adversary. The communication channel is an AWGN channel with an additive white Gaussian state and an additive jamming interference. The encoder and the decoder share an unbounded amount of common randomness, Ω, unknown to the jammer. Let us denote by [formula], the signal received at the decoder. Then where [formula], [formula], [formula] and [formula] are the encoder's input to the channel, the additive white Gaussian state, jammer's channel input and the channel noise respectively. The components of [formula] are i.i.d. with Si  ~  N(0,σ2S) for [formula]. The components of [formula] are i.i.d. with Zi  ~  N(0,σ2) for [formula]. The state vector [formula] is known non-causally to both the encoder and the jammer, but it is not known to the decoder. The encoder picks a codeword [formula] and transmits it on the channel. The encoder has a power constraint P, i.e. [formula], where [formula] denotes the norm of a vector. Similarly, the adversary's power constraint is Λ and hence, the signal [formula] is such that [formula]. Let [formula].

An (n,R,P) deterministic code of block length n, rate R and average power P is a pair (f,g) of encoder map [formula], such that [formula] [formula], and decoder map [formula].

An (n,R,P) randomized code is a random variable (F,G) (=  Ω in our notation) which takes values in the set of (n,R,P) deterministic codes.

For an (n,R,P) randomized code with encoder-decoder pair (F,G), the maximal probability of error (Pne) is given as The rate R is achievable if for every ε > 0, there exists an (n,R,P) randomized code for some n such that Pne  <  ε . We define the capacity of the dirty paper AVC as the supremum of all achievable rates.

The Main Result

Our main contribution is the determination of the capacity of the Gaussian AVC with an additive white Gaussian state in the presence of a state-aware adversary under a shared randomness and a maximal error probability criterion model.

The capacity of a Gaussian AVC with a Gaussian additive state in the presence of an adversary, where the encoder and the adversary have non-causal access to the state, is

[formula]

Note that this result implies that even under non-causal knowledge of the state [formula], the adversary completely disregards this knowledge and essentially inputs i.i.d. Gaussian jamming noise.

Proof of Theorem [\ref=thm:main:result]

In this section, we discuss an achievable scheme for the main result stated in Section [\ref=sec:main:result]. Before we proceed, let us introduce some useful notation. For any [formula], [formula], we denote the unit vector in the direction of [formula] as [formula]. Thus, [formula]. Next, given two vectors [formula], [formula] denotes their dot (inner) product.

Codebook Construction

Our code uses Costa's dirty paper coding scheme [\cite=costa], which involves an auxiliary random variable denoted as U and a fixed parameter α. Encoding:

The encoder generates a code book comprising 2nRU = 2n(R + ) i.i.d. vectors [formula], where [formula] and [formula]. Here, there are 2nR bins with each bin containing 2n codewords. Every codeword [formula] is chosen uniformly at random over the surface of the n-sphere of radius [formula], where PU = P + α2σ2S and α = P / (P + Λ  +  σ2).

Given a message m to be sent and having observed a priori the state [formula], the encoder looks within the bin m for some [formula], [formula] such that

[formula]

for some appropriately small δ0 > 0. If no such [formula] is found, then the encoder sends the zero vector. If more than one [formula] satisfying [\eqref=eq:encoder:condition] exists, the encoder chooses one uniformly at random from amongst them. Let [formula] denote this codeword. The encoder then transmits [formula] over the channel.

Decoding: We employ the minimum angle decoder. When [formula] is received at the decoder, its estimate of the message, [formula], is the solution of the following optimization problem.

Some Important Lemmas

We now state some important results which are required toward the probability of error analysis of this code. The proof details for Lemmas [\ref=lem:binning:rate], [\ref=lem:J:U], and [\ref=lem:Y:U:unit:expr] can be found in the appendix. Lemma [\ref=lem:csiszar:narayan] is recapitulated from [\cite=csiszar-narayan-it1991]. Finally, the proof of Lemma [\ref=lem:double:exp] is elementary, and hence, excluded.

The following lemma gives a lower bound on [formula] (denoted by [formula]) under which encoding succeeds with high probability.

If [formula], then the encoder finds at least one [formula] satisfying [\eqref=eq:encoder:condition] with probability approaching 1 as n  →    ∞  .

The next result captures the correlation an adversary can induce with the codeword through the choice of its jamming signal.

For any δ > 0 and any jamming strategy [formula], as n  →    ∞  .

An important result which directly follows from  [\cite=csiszar-narayan-it1991] is stated next.

Consider any [formula] on the unit n-sphere and suppose an independent random vector [formula] is uniformly distributed on this sphere. Then for any [formula], we have

The following lemma shows that the inner product [formula] is at least (θ  -  δ) with high probability irrespective of the jammer's strategy [formula].

Let the codeword chosen be [formula] and let [formula] be transmitted on the channel which results in a channel output [formula]. Then for any δ > 0 and any jamming strategy [formula], we have as n  →    ∞  , where

[formula]

Finally, we close with the following result.

If f(n) = 2- na1, where a1 > 0, then

Probability of Error Analysis

We start with a brief outline of the analysis. From Lemma [\ref=lem:Y:U:unit:expr] we know that regardless of the strategy the adversary employs, a decoding error occurs only if any other codeword [formula], for some m'  ≠  m, [formula], is such that [formula]. Our aim will be to show that this event has a vanishing probability.

Achievability: Fix some ε > 0, and let [formula] and [formula]. Hence, RU = R +  = C +   -  ε / 2 = CU  -  ε / 2, where

Let Em denote the error event when m is the message sent. Hence, we get Let θ be as in [\eqref=eq:theta]. For any δ > 0, conditioned on M = m

[formula]

Lemma [\ref=lem:Y:U:unit:expr] implies that the first term can be made arbitrarily small. So, let us now consider the second term. Due to the independence of vectors [formula] and [formula], m'  ≠  m, [formula], we get (a). To establish (b), note firstly that [formula] is independent of [formula]. In addition, recall Lemma [\ref=lem:csiszar:narayan], and replace [formula] by [formula] and γ by (θ  -  δ).

Now choosing a small enough δ > 0  such that

[formula]

and applying Lemma [\ref=lem:double:exp], it can be seen that as n  →    ∞  . Thus, the second term of the RHS of [\eqref=eq:P:Em:RHS:2] can be made arbitrarily small. Thus, P(Em) can be made arbitrarily close to zero.

Converse: Let the jammer choose a jamming signal [formula] and [formula] uniformly distributed over the sphere of radius [formula]. We already know the capacity of such a channel and it is given by [\eqref=eq:capacity].

Discussion

We now briefly discuss the discrete alphabet version of the AVC with a state-aware adversary. Let X, S, J and Y be finite alphabet sets. Let x∈X and y∈Y denote resp., the user's input and the channel's output. We define W  =  {W.|.,S,J:S∈S,J∈J} as an AVC with a random state S parametrized by an adversarial state J.

We use the notation P(A|B) to denote the set of all conditional distributions PA|B for a random variable A with alphabet A conditioned on a random variable B with alphabet B. Next, let [formula]. Finally, given a state distribution PS, for a fixed distribution PU,X|S and a fixed channel [formula], let I(U;Y) and I(U;S) denote resp., the mutual information quantities evaluated with respect to the marginals PUY and PUS.

We now state without proof the following result.

The capacity of the discrete memoryless AVC with a random state, when both the encoder and the decoder have non-causal access to the state sequence is

Conclusion

We determined the capacity of a Gaussian AVC with an additive Gaussian state in the presence of an adversary, where the state is known to the encoder as well as the adversary. The surprising fact that the worst-case adversary disregards state knowledge and inputs white Gaussian noise into the channel was proved. Overall, it was shown that the effect of the state was completely eliminated and the capacity of a Gaussian AVC with state and a state-aware adversary is equal to that of a standard Gaussian AVC with no state and an independent adversary.

Acknowledgment

The first author thanks Prof. Anand D. Sarwate for helpful early discussions and suggestions on the problem.

Proof of Lemma [\ref=lem:binning:rate]

From [\cite=costa], we know that if [formula] then there exists at least one [formula] satisfying [\eqref=eq:encoder:condition]. Recall that [formula], where [formula] and [formula] are as described earlier. The result now follows by simply evaluating this mutual information quantity [\cite=cover-thomas].

Proof of Lemma [\ref=lem:J:U]

Let us resolve the components of [formula] and [formula] along directions parallel and orthogonal to [formula]. We denote the latter components as [formula] and [formula] respectively. Note that [formula], and thus To establish the result in Lemma [\ref=lem:J:U] we need to show that for any δ > 0, as n  →    ∞  , [formula], i.e., [formula] and [formula] are nearly orthogonal. To proceed, we introduce some notation. Let [formula] and for any [formula], let [formula] denote the (n - 1) subspace orthogonal to [formula]. Claim 1: Conditioned on M = m, [formula] and [formula], the random vector [formula] is uniformly distributed over

[formula]

and,

[formula]

Proof of Claim: Given the symmetry of the codebook generation and the encoding, we know that the chosen codeword vector [formula] is uniformly distributed over the set [formula]. Now conditioned on message M = m, state [formula] and [formula], it follows that the codeword vector [formula] is uniformly distributed over the following set.

[formula]

To proceed further, we show that [formula]. The claim then follows from observing that [formula] is uniformly distributed over the set [formula].

To show [formula]. Let [formula]. Expressing [formula] through its two components, one in the direction parallel to [formula] and the other orthogonal to it, we get Note here that [formula] and [formula]. Comparison with [\eqref=eq:B] completes the proof for the forward part.

To show [formula]. Consider some vector [formula]. Using [\eqref=eq:B:tilde], we can write [formula], [formula], and where [formula] is as given in [\eqref=eq:rho:z:s]. It can be easily verified that [formula]. Also, [formula], and hence, it can be immediately seen that [formula]. Thus, [formula].

Claim 2: For any δ > 0, Proof of Claim: We first prove the conditional version of this claim. Again, let us condition on M = m, state [formula] and [formula]. From Claim 1, we know that [formula], [formula] with [formula] as given in [\eqref=eq:rho:z:s]. Now for γ > 0, we have Here, (a) follows from noting that [formula] and [formula].

Since the shared randomness Ω is unavailable to the jammer, conditioned on m, [formula] and z, we have [formula]. Also, both [formula] and [formula] lie in the (n - 1) hyperplane orthogonal to [formula]. Now using the result in Lemma [\ref=lem:csiszar:narayan], we have where [formula] and [formula]. Since the upper bound in [\eqref=eq:J:V:gamma] tends to zero as n  →    ∞  , the conditional version of the result follows. However, note here that the bound in [\eqref=eq:J:V:gamma] does not depend on m, [formula] or z. Hence, the unconditioned result is also true, and the claim follows.

Proof of Lemma [\ref=lem:Y:U:unit:expr]

We know that and Let M = m and let us define the following events. From Lemma [\ref=lem:binning:rate], [formula] as n  →    ∞   for δ0 > 0. Since [formula] is independent of [formula], [formula] and [formula], [formula], [formula] and [formula] as n  →    ∞   for any δ1 > 0, δ2 > 0 and δ3 > 0 respectively. [formula] and [formula] are i.i.d. Gaussian vectors with variance σ2 and σ2S resp., so for δ4 > 0, δ5 > 0, [formula], [formula] as n  →    ∞  . Finally, using Lemma [\ref=lem:J:U], [formula] as n  →    ∞   for any δ6 > 0.

Let us define [formula] and let

[formula]

Since [formula], we have V2  ≤  1. It follows from [formula], that 0  ≤  W  ≤  Λ. As argued above, [formula], where ε can be made arbitrarily small by choosing n large enough for any δi, [formula].

Recall that the codewords are chosen over the surface of an n-sphere of radius [formula], and hence, from [\eqref=eq:Y:U:dot], [\eqref=eq:Y:Y], conditioned on the event Ec and and thus, where, δa, δb > 0 and δa, δb  →  0 as δi  →  0, [formula]. Here, using PU = P + α2σ2S and α = P / (P + Λ  +  σ2) and simplifying results in (a). Furthermore, we have where,  > 0 and [formula] as δa, δb  →  0. Hence, it follows that [formula] if The following claim completes the proof. Claim: If then, for all - 1  ≤  V  ≤  1 and 0  ≤  W  ≤  Λ, where, [formula]. Proof of Claim: We show that for - 1  ≤  V  ≤  1 and 0  ≤  W  ≤  Λ,

[formula]

Let us now establish the simple fact that f(V,W)  ≥  0. Consider the numerator term in [\eqref=eq:f:def]. Here, (a) follows by substituting for α. Next, (b) is true since V >   ≥   - 1 while (c) follows from noting that the parenthetic term in the step prior to (c) is non-negative. Hence, we conclude that the numerator of [\eqref=eq:f:def] is non-negative, and f(V,W)  ≥  0. Now, since f(V,Λ)  ≥  0 for - 1  ≤  V  ≤  1 and 0  ≤  W  ≤  Λ, to show [\eqref=eq:f:fmin] it is sufficient to prove

[formula]

for [formula] and 0  ≤  W  ≤  Λ. Hence, by [\eqref=eq:theta] and [\eqref=eq:f:def], we want to show that Since W  ≤  Λ, the RHS above is negative. However, - 1  ≤  V  ≤  1, and hence, V2  ≥  0. Thus, [\eqref=eq:f:squared] immediately follows and we conclude that f(V,W)  ≥  f(0,Λ), for - 1  ≤  V  ≤  1 and W  ≤  Λ. This concludes the proof of the claim.

Hence, using the result of the previous claim and from [\eqref=eq:f:1], it can be seen that [formula] can be made arbitrarily small. This establishes the result in Lemma [\ref=lem:Y:U:unit:expr].