Can Partial Strong Labels Boost Multi-label Object Recognition?

Introduction

Recently, the availability of large amount of labeled data has greatly boosted the development of feature learning methods for classification. In particular, convolutional neural networks (CNN) achieve great success in visual recognition/classification tasks. Features extracted from CNN can provide powerful global representations for the single object recognition problem [\cite=Alex2012] [\cite=Sermanet2013] [\cite=Razavian2014]. However, conventional CNN features may not generalize well for images containing multiple objects as these objects can be in different locations, scales and categories. Since such multi-label classification task is more general and practical in real world applications, many methods [\cite=OquabCVPR2014] [\cite=Oquab2014] [\cite=Wei2014] have been proposed to address the problem.

For multi-label object recognition tasks, there exist different "levels" of labels, e.g., "weak" labels that only indicate the presence of the objects and "strong" labels that include ground truth bounding boxes for specific objects.

A well known fact is that weak labels can be utilized to fine-tune a pre-trained CNN model and produce good global representations. However, due to the diversity and complexity of multi-label images, classifiers trained from such global representation might suffer from generalization problems. For example, a binary classifier 'car', which is forced to account for hundreds of different variations of car images that may also contain multiple other objects, could have under-fitting problems such that it falls short to detect new cars [\cite=Yu2014]. Furthermore, if the training instances are unevenly distributed in the feature space, the model complexity required in different regions of the space might be different. As a consequence, for a classifier trained with single global representations, it can be successful near regions of the decision boundary that are densely populated with training instances, but may fail in poorly sampled areas of the feature space.

Inspired by local learning [\cite=Bottou1992], we address this problem by utilizing a subset of the data that are most relevant to a particular test instance. However, such relevance is not easy to obtain only through weak supervision, i.e., image level labels. Fortunately, for many multi-label applications, we can exploit the strong supervision information, i.e., ground truth bounding boxes, to solve the problem. On the other hand, we understand the difficulty and ambiguity of obtaining accurate bounding boxes. Yet if we can utilize the existing bounding box annotations in training to boost the performance of recognition even for unseen categories, it would be of great benefits.

In this paper, we utilize both strong and weak labels as two views, and propose a multi-view multi-instance framework to tackle the multi-label object recognition task. In our framework, strong labels and weak labels are used separately to generate a label view and a feature view, respectively. Specifically, we utilize ground truth bounding boxes to encode local spatial label configurations of each object proposal, which forms the label view. In this way, the spatial class label configuration in the vicinity of every proposal is accounted for. Weak labels are used to fine-tune a CNN network to extract a global representation for each proposal as the feature view. When combining local label configurations encoded by the label view with the global representations in the feature view to train classifiers, we can achieve a balance between global semantic abstraction and local similarity, and thus greatly enhance the discriminative power of the feature. More importantly, as the strong labels are indirectly utilized as label view to encode local label distribution around a proposal, the proposed framework can generalize well to the whole feature space, even with only partial bounding boxes, making it more practical. The overview of our framework is shown in Fig. [\ref=system].

Related Works

Our paper mainly relates to the areas of computer vision and machine learning: CNN based multi-label object recognition, multi-view and multi-instance learning and local and metric learning. We review the related works in these areas below.

CNN based Multi-label Object Recognition. Recently, CNN models have been adopted to solve the multi-label object recognition problem. Many works [\cite=girshick2014] [\cite=Sermanet2013] [\cite=OquabCVPR2014] [\cite=Oquab2014] [\cite=Wei2014] have demonstrated that CNN models pre-trained on a large dataset such as ILSVRC can be used to extract features for other applications without enough training data. A typical way ([\cite=Sermanet2013], [\cite=Razavian2014] and [\cite=Chatfield2014]) is to directly apply a pre-trained CNN model to extract an off-the-shelf global feature for each image from a multi-label dataset, and use these features for classification. However, different from single-object images from the ImageNet, multi-label images usually have multiple objects in different locations and scales and could also be occluded, and thus global representations are not optimal for solving the problem [\cite=Wei2014]. More recently, [\cite=OquabCVPR2014] and [\cite=girshick2014] propose two proposal-based methods for multi-label recognition and detection tasks with the help of ground truth bounding boxes. These methods achieve significant improvement over single global representations. On the other hand, [\cite=Oquab2014] and [\cite=Wei2014] handle the problem in a weakly supervised manner by max-pooling image scores from the proposal scores. Moreover, [\cite=Simonyan2014] employs a very deep CNN, aggregates multiple features from different scales of the image and achieves state-of-the-art results.

Multi-View and Multi-Instance Learning. Multi-view learning deals with data from multiple sources or feature sets. The goal of multi-view learning is to exploit the relationship between views to improve the performance or reduce model complexity. Multi-view learning is well studied in conjunction with semi-supervised learning or active learning. To combine information from multi-views for supervised learning, fusion techniques at feature level, or classifier level can be employed [\cite=Zhang2014]. Multi-instance learning aims at separating bags containing multiple instances. Over the years, many multi-instance learning algorithms have been proposed, including miBoosting [\cite=Xu2004], miSVM [\cite=Andrews2003], MILES [\cite=Chen2006] and miGraph [\cite=Zhou2009]. Several works also studied the combination of multi-view and multi-instance learning and its application to computer vision tasks.

Local and Metric Learning. Existing local learning methods mainly vary in the way they utilize the labeled instance nearest to a test instance. One way is to only use a fixed number of nearest neighbors to the test point to train a model using neural network, SVM or just voting. The other way is to learn a transformation of the feature space (e.g., Linear Discriminant Analysis). With the projection, the learned model will be better tailored for the test instance's neighborhood property [\cite=Hastie1996]. Metric learning is closely related to local learning as a good distance metric is crucial for the success of local learning. Generally, metric learning methods optimize a distance metric to best satisfy known similarity constraints between training data [\cite=Bellet2013]. Some metric learning methods learn a single global metric [\cite=Weinberger2009]. Others learn local metrics that vary in different regions of the feature space [\cite=Yang2006].

Multi-Label as Multi-Instance

In this paper, we formulate the multi-label object recognition problem as a multi-instance learning (MIL) problem. To be specific, given a set of n training images [formula], we extract ni object proposals [formula] from each image [formula] using general object detection techniques. By decomposing images into object proposals, each image [formula] becomes a bag containing several positive instances, i.e., proposals with the target objects, and negative instances, i.e., proposals with background or other objects. The problem of classifying [formula] thus is transformed from a multi-label classification problem to a multi-class MIL problem. The merit of such a transformation is that we do not need to consider the complexities of recognizing multiple objects in multiple scales, locations and categories in a single image. Instead, we only need to identify whether there exist target objects in the proposals, which is proven to be the forte of CNN features.

As extensively compared and evaluated in [\cite=Hosang2014], state-of-the-art general object detection methods like BING [\cite=Cheng2014], selective search [\cite=Uijlings2013], MCG [\cite=Arbelaez2014] and EdgeBoxes [\cite=Zitnick2014] can reach reasonably good recall rates with several hundreds of proposals. Therefore, if we sample enough proposals from each image, we can safely assume that these proposals can cover all objects (or at least all object categories) in an image, thus fulfilling the assumption of multi-instance learning (that is, every positive bag contains at least one positive instance).

In particular, we employ the unsupervised selective search method [\cite=Uijlings2013] for object proposal generation. Selective search have proven to achieve a balance between effectiveness and efficiency in [\cite=Hosang2014]. More importantly, as it is unsupervised, no extra training data or ground truth bounding boxes are needed in this stage. Example of proposals extracted by selective search can be found in Fig. [\ref=selective-search].

Traditionally, MIL is formulated as a max-margin classification problem with latent parameters optimized using alternating optimization. Typical examples include miSVM [\cite=Andrews2003] and Latent SVM [\cite=Felzenszwalb2010]. However, although these methods can achieve satisfactory accuracies, their scalability ability limits the applicability to current large scale image classification tasks. For large scale MIL problem, [\cite=Wu2014] shows that Fisher vector [\cite=Perronnin2010] [\cite=Sanchez2013] (FV) can be used as an efficient and effective holistic representation for a bag. Moreover, as Fisher vector is originally derived for image classification tasks, it should be more suitable for our task. Thus, we choose to represent each bag [formula] as a FV.

Assume we have a K-component Gaussian Mixture Model (GMM) with parameters [formula], where ωk, [formula] and Σk are the mixture weight, mean vector and covariance matrix of the k-th Gaussian, respectively. The covariance matrices Σk are assumed to be diagonal, whose diagonal entries form a vector σk. We have:

[formula]

where γj(k) is the soft assignment weight, which is also the probability for xij to be generated by the k-th Gaussian:

[formula]

We map all the proposals [formula] in an image [formula] to a FV by concatenated [formula], [formula] and [formula] for all [formula], and denoted as [formula]. [formula] will be used as the final feature to train the classifiers. Note that for simplicity, we abuse the notation of [formula] for both proposal j in image i and its corresponding feature representation. In the next section, we will describe how to generate the feature representation [formula] for the proposal.

From Global Representation to Local Similarity

Once we obtain object proposals for each image, we can naturally use CNN features to represent these proposals. We call this global representation as the feature view [formula] for proposal [formula] from image [formula]. Subsequently, each image can be encoded with Fisher vector. By utilizing just the feature view, we are able to get reasonably good results.

However, since these proposals contain different objects as well as random background, we can expect large variances and imbalanced distributions. As a consequence , the global representation might not be accurate enough. Local learning solves the data density and intra-class variation problems by focusing on a subset of the data that are more relevant to a particular instance [\cite=Bottou1992]. Inspired by the idea of local learning, we propose to add local spatial configuration information as the label view (cf. Fig. [\ref=system]) to enhance the discriminative power of the feature.

To effectively encode the local spatial configuration of a proposal, we need to solve two key problems: How to form a good candidate pool for local learning and how to determine which candidates are relevant to a particular new proposal. For the former problem, since we have some ground truth objects from the strong labels, we could use them as the candidate pool assuming all of the ground truth objects are useful. For the latter one, we follow most existing methods' assumption and assume that the most relevant proposals are the nearest neighbors. Now the problem becomes how to define "nearest". Many studies [\cite=Bellet2013] [\cite=Weinberger2009] have shown that the distance metric is critical to the performances of local learning.

CNN as Metric Learning

Metric learning studies the problem of learning a discriminative distance metric. Conventional Mahalanobis metric learning methods optimize the parameters of a distance function in order to best satisfy known similarity or dissimilarity constraints between training instances [\cite=Bellet2013]. To be specific, given a set of n labeled training instances [formula], the goal of metric learning is to learn a square matrix [formula] such that the distance mapped between training data, represented as

[formula]

satisfies certain constraints. Since [formula] is symmetric and positive semi-definite, it can be decomposed as [formula], and [formula] can be rewritten as:

[formula]

We can see that learning a distance metric is equivalent to learning linear projection W that maps the data from input space to a transformed space. In this sense, the extracted CNN features (treated as W) from the original raw pixel space can also be viewed as a form of metric learning, only the process to generate W is highly nonlinear. However, the goal of CNN is usually to minimize the classification error using loss functions such as the logistic loss, which may not be suitable for local encoding.

Our desired metric should be discriminative such that all categories are well separated, as well as compact so that we can find more accurate nearest neighbors. Specifically, we want the pairwise distance between instances from the same class to be smaller than that between instances from different classes. In order to achieve such a goal, [\cite=Weinberger2009] proposed the large-margin distance to minimize the following objective function:

[formula]

Here η encodes target nearest neighbor information, where ηij  =  1 if [formula] is one of the [formula] positive nearest neighbors of [formula]; otherwise ηij  =  0. y is the label information where yil  =  1 if [formula] and [formula] are in the same class; otherwise yil  =  0. +  =   max (  ·  ,0) is the hinge loss function. C is the trade-off parameter. The first term in Eq. [\ref=objective] penalizes large distances between instances and target neighbors, and the second term penalizes small distances between each instance and all other instances that do not share the same label. By employing such an objective function, we can ensure that the [formula]-nearest neighbors of an instance belong to the same class, while instances from different classes are separated by a large margin.

In order to learn a discriminative metric, we propose to learn a large-margin nearest neighbor (LMNN) CNN. Specifically, we replace the logistic loss with the large margin nearest neighbor loss and train a network with low-dimension output utilizing the strong labels. Details of training and fine-tuning the LMNN CNN can be found in Section [\ref=details]. The output of the proposed LMNN network is a low-dimensional feature that shares the good semantic abstraction power of conventional CNN feature and the good neighborhood property of large-margin metric learning. We then build the candidate pool with the LMNN CNN features extracted from ground truth objects.

Encoding Local Spatial Label Distribution as the Label View

With the low-dimensional features learned from LMNN CNN and the ground truth objects, we are able to build a good neighborhood for each proposal. To encode such neighborhood information, a straight forward way is to define a neighborhood vector [formula] for any instance [formula] similar to [\cite=Wu2011], where [formula] is a n  ×  1 vector with [formula] if [formula] is in the k nearest neighbor of [formula] otherwise [formula]. This neighborhood vector emphasizes the structural similarity between instances and achieves success in imbalanced classification [\cite=Wu2011]. However, although the vector is sparse, it grows linearly with the size of the candidate pool and can have scalability issues when applying PCA in the Fisher vector extraction stage.

Inspired by the idea of [\cite=Loog2005] that incorporates class labels of its neighborhood in the linear discriminant analysis, we encode the neighborhood as the label view. Specifically, we extract features from each proposal [formula] using the LMNN CNN, then find k nearest neighbors of [formula] in the candidate feature pool as [formula] and record their labels [formula]. The label information is encoded as a binary vector. If we have C categories and a label vector [formula], then lj  =  1 [formula] if the object is annotated with class j; otherwise lj  =  0. Therefore, [formula] is a 1  ×  kC vector and it will be used as the feature for the label view.

The merit of indirectly utilizing ground truth bounding boxes as the label view is the good generalization ability. As the label view is a form of local structure representation, for unseen categories, the encoding process naturally exploits existing semantically or visually closed categories to build local support. For example, suppose we do not have the bounding box annotations for "cat" and "train", a proposal containing "cat" might have nearest neighbors of "dog", "tiger" or other related animals, and a proposal containing "train" might have nearest neighbors of "car", "truck" or other vehicles. Although lacking the exact annotations of certain category, the label view is still able to encode the local structure with semantically or visually similar objects. Therefore, our framework can make use of existing strong supervision information to boost the overall performance. The experimental results shown in Section [\ref=results] validate this argument.

We directly fuse the feature view and label view to form the final representation of each proposal [formula] as [formula]. λ is the trade-off parameter between the feature view and the label view.

Network Configurations and Implementation Details

Our framework consists of two networks, a large-margin nearest neighbor (LMNN) CNN and a standard CNN. Both networks' architectures are similar to [\cite=Chatfield2014] with 5 convolutional layers and 3 fully-connected layers. Details of the network architecture can be found in Table [\ref=cnn]. The main difference of these two networks is the dimensionality of fully-connected layer 7 and the fine-tuning process. For LMNN CNN, we set d in full7 to be 128, and for CNN we set it to be 2048.

Data pre-processing and pre-training. We use the ILSVRC 2012 dataset to pre-train both networks. Given an image, we resize the short side to 224 with bilinear interpolation and perform a center crop to generate the standard 224  ×  224 input. Each of these inputs is then pre-processed by subtracting the mean of ILSVRC images.

Fine-tuning. To better adopt the pre-trained network for specific applications, we also fine-tune these networks using task relevant data. Unlike [\cite=Chatfield2014], currently our implementation does not involve any data augmentation in the fine-tuning stage.

For the standard CNN used for feature view, we only fine-tune the network with weak labels on the whole image. As our task is multi-label recognition, following [\cite=Wei2014], we use square loss instead of the logistic loss. To be specific, suppose we have a label vector [formula] for the i-th image. yij  =  1 [formula] if the image is annotated with class j; otherwise yij  =  0. The ground truth probability vector of the i-th image is defined as [formula] and the predictive probability vector is [formula]. And then the cost function to be minimized is defined as

[formula]

During the fine-tuning, the parameters of first seven layers of the network are initialized with the pre-trained parameters. The parameters of the last fully connected layer is initialized with a Gaussian distribution. We tune the network for 10 epochs in total.

For the large-margin NN (LMNN) CNN used for label view, we execute a three-step fine-tuning. The first step is image level fine-tuning similar to the process we have described above. The second step is ground truth objects fine-tuning, where we fine-tune the network using ground truth objects with the logistic loss. The third and final step is large-margin nearest neighbor fine-tuning. We fine-tune the network with the loss of [\eqref=objective] described in Section [\ref=largeNN]. To accelerate the process, in this step, we fixed all parameters of the first seven layers and only fine-tune the parameters of the last fully connected layer.

Experimental Results

In this section, we present the experimental results of the proposed multi-view multi-instance framework on multi-label object recognition tasks.

Datasets and Baselines

We evaluate our method on the PASCAL Visual Object Classes Challenge (VOC) datasets [\cite=VOC], which are widely used as benchmark datasets for the multi-label object recognition task. In particular, we use the VOC 2007 and VOC 2012 datasets. The details of these datasets can be found in Table [\ref=datasets]. These two datasets have a pre-defined split of train, val and test sets. We use train and val for training and test for testing. The evaluation method is average precision (AP) and mean average precision (mAP).

We compare the proposed framework with state-of-the-art approaches. Specifically, we compare with methods based on CNN features as well as methods based on hand-crafted features.

It is difficult to make a complete fair comparison between different methods as CNN configurations, data augmentation and pre-training could substantially influence the results. To fairly compare the proposed framework with state-of-the-art methods, we first compare results pre-trained on ILSVRC 2102 dataset with 1000 categories on 8-layer CNN. All CNN based method can benefit from extra training data and more powerful networks as shown in [\cite=OquabCVPR2014] [\cite=Wei2014] [\cite=Oquab2014] [\cite=Chatfield2014] [\cite=Simonyan2014]. We will compare these results separately.

We choose TopPush [\cite=Li2014] to learn linear classifiers for classification. All the experiments are run on a computer with Intel i7-3930K CPU, 32G main memory and a nVIDIA Tesla K40 card.

Proposal Extraction and Parameters

In this paper, we employ selection search [\cite=Uijlings2013] to generate object proposals. Specifically, selective search combines the strength of exhaustive search and segmentation to extract object proposals at all locations and scales. As it is unsupervised, selective search does not need extra information for training and can still achieve good recall rate compared to other supervised methods as shown in [\cite=Uijlings2013] [\cite=Hosang2014].

With the parameters suggested by [\cite=Uijlings2013] in their demo code, on average around 1500 proposals are extracted from every image in the PASCAL VOC 2007 dataset. Considering the computational time and the limitation of hardware, we employ a tighter threshold k  =  400 to reduce the number of proposals. As in our application, we care more about recall rate and less about the mean average best overlap performance, the tighter threshold should not affect much. With this parameter, we extract on average 800 proposals from each image. During training and testing, we filter out proposals with less than 40 pixels in width and use a maximum number of 500 proposals per image to further improve efficiency.

For parameters of Fisher vector, we follow [\cite=Perronnin2010] to first employ PCA to reduce the dimension of the original features to preserve around 90% energy. For VOC 2007 and 2012 datasets, after PCA, the standard CNN features is reduced to around 450-d. Due to the limitations of the hardware, we use at most 200 proposals from each image for clustering and generate 128 GMM codewords. After FV encoding, we apply power normalization and L2 normalization subsequently.

In fine-tuning the LMNN CNN, we set the trade-off parameter C = 1 and the nearest neighbor number  = 10 (for training) in the LMNN loss. For the label view encoding, we have mainly two parameters. The trade-off parameter λ is selected from {1,0.5,0.25} by cross-validation. We illustrate the impact of different nearest neighbor number k (for testing) on VOC 2007 test set in Fig. [\ref=NN]. Generally, bigger k leads to better accuracy, but there seems to be no performance gain for k  >  50. Therefore, we set k = 50. For faster NN search, we employ FLANN [\cite=flann] with "autotuned" parameters.

Image Classification Results

Image Classification on VOC 2007: Table [\ref=voc2007-s] reports our experimental results compared with state-of-the-art methods on VOC 2007. In the upper part of the table we compare with hand-crafted feature based methods and CNN based methods pre-trained on ILSVRC 2012 using 8-layer network. [\cite=Wei2014] also report results with additional 1000 categories from ImageNet that are semantically close to VOC 2007 categories (HCP-2000C) and [\cite=Simonyan2014] is trained on very-deep CNN. We list them in the lower part of the table.

From the experimental results, we can see that using just feature view (shown as FV in the table), we already outperform other proposal-based methods PRE-1000C and HCP-1000C by 7.6% and 3.8%, respectively. At this stage, no ground truth bounding box annotations are utilized. From this comparison, we can conclude that Fisher vector as a holistic representation for bags is superior than max-pooling. With all bounding boxes, the multi-view framework (FV+LV-20) demonstrates a 2.5% performance gain over just the feature view. This significant performance gain validates the effectiveness of the label view. In particular, our framework even scores higher than HCP pre-trained on additional 1000 categories of images that are semantically closed to VOC data (HCP-2000C). Our framework shows good performance especially for difficult categories such as bottle, cow, table, motor and plant.

More importantly, if we just use the bounding boxes from first 10 categories (plane to cow), our framework (FV+LV-10) still outperforms single feature view by a margin of 1.3%. We can naturally infer that using the bounding boxes from plane to cow can boost the performance of these categories as shown in the table. Since the proposed label view encoding is a form of local similarity representation, it can generalize quite well to unseen categories. We can see that the label view also improves the accuracies of unseen categories such as horse, person and tv.

To demonstrate the potential of our framework, we also show the results with very-deep CNN [\cite=Simonyan2014] (FV+LV-20-VD). Although we do not employ any data augmentation or multi-scale dense sampling in the feature extraction stage, we still outperform [\cite=Simonyan2014], which densely extracts multiple CNN features from 5 scales and combines two very-deep CNN models (16-layer an 19-layer), by nearly 1%. We also observe that there is complementarity between multi-scale CNN extracted from the whole image and our proposal-based framework. To verify this observation, a late fusion between the predicted scores (Fusion) is executed, and we can achieve state-of-the-art 92.0% mAP. Note that we directly apply 0.5 as the trade-off parameter λ in the fusion stage, we believe tuning this parameter can further improve the performance of the framework.

Image Classification on VOC 2012: Table [\ref=voc2012-s] reports our experimental results compared with state-of-the-art methods on VOC 2012. Similar to Table [\ref=voc2007-s], we compare with hand-crafted feature based methods and CNN based methods pre-trained on ILSVRC 2012 using 8-layer SVM-layer network in the upper part and methods trained with additional data and very-deep network in the lower part.

The results are consistent with that from VOC 2007. Our framework still outperforms state-of-the-art hand-crafted feature method (NUS-PSL) and proposal-based CNN methods (PRE-1000C and HCP-1000C) by a significant margin. Even with just feature view, we outperform NUS-PSL by 1.8% and HCP-1000C by 2.3%. With the aid of the label view, we can achieve better mAP than two proposal-based methods pre-trained on additional 512 or 1000 categories of image data (PRE-1512C and HCP-2000C) and comparable results with [\cite=Oquab2014], which is pre-trained on additional 512 categories of data. Our framework again demonstrates good performance for hard categories.

By employing just 10 categories of bounding boxes, the mAP performance does not degenerate much. As expected, for the first 10 categories with the aid of bounding boxes, the performance are quite good. FV+LV-10 achieves 5 best AP among the first 10 categories. Even without the ground truth bounding box for the last 10 categories, the proposed framework still boost the accuracies. These results proves the generalization ability of our framework.

When employed with very-deep network, our framework (FV+LV-20-VD) achieves similar performance as the combination of two very-deep networks (VeryDeep). When we fused the whole image representation [\cite=Simonyan2014] and our proposal-based representation, our method (Fusion) achieves state-of-the-art results, outperforming [\cite=Wei2014], which is pre-trained on an extended 2000-class ILSVRC dataset and benefits from the fusion with an object detection-assisted classification pipeline.

Conclusion

In this paper, we have proposed a multi-view multi-instance framework for solving the multi-label classification problem. Compared with existing works, our framework naturally makes use of the two levels of labels in the multi-label problem and have the generalization ability to boost performance of categories without bounding box annotations. The proposed framework is extensively compared with state-of-the-art hand-crafted feature based and CNN based methods in two multi-label benchmark datasets. The experimental results validates the discriminative power and generalization ability of the proposed framework.

For future directions, there are several possibilities to explore. First of all, we can improve the scalability and possibly also improve the performance of the framework by establishing a proposal selection criteria to filter out noisy proposals. Secondly, we may build a suitable candidate pool directly from the extracted proposals. If we are able to select good proposals to establish such candidate pool without the aid of ground truth bounding boxes, the generalization ability of our framework will be further improved.