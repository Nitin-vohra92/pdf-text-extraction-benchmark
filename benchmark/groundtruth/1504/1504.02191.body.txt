Lemma Definition Problem Proposition Corollary Remark Example Question

'local' vs. 'global' parameters - breaking the gaussian complexity barrier

Introduction

The focus of this article is on the question of prediction. Given a class of functions F defined on a probability space (Ω,μ) and an unknown target random variable Y, one would like to identify an element of F whose 'predictive capabilities' are (almost) the best possible in the class. The notion of 'best' is measured via the point-wise cost of predicting f(x) instead of y, and the best function in the class is the one that minimizes the average cost. Here, we will consider the squared loss: the cost of predicting f(x) rather than y is (f(x) - y)2, and if X is distributed according to μ, the goal is to identify

[formula]

where the expectation is taken with respect to the joint distribution of X and Y on the product space [formula].

The information at one's disposal is rather limited: a random sample (Xi,Yi)Ni = 1, selected according to the N-product of the joint distribution of X and Y. And, using this data, one must select some (random) f∈F.

Given a sample size N and a class F defined on (Ω,μ), a learning procedure is a map [formula]. For a set Y of admissible targets, Ψ performs with confidence 1 - δ and accuracy Ep if for every Y∈Y, and setting   =  Ψ((Xi,Yi)Ni = 1),

[formula]

with probability at least 1 - δ relative to the N-product of the joint distribution of X and Y.

The accuracy (or error) Ep is a function of F, N and δ, and may depend on some features of the target Y as well, for example, its norm in some Lq space.

A fundamental problem in Learning Theory is to identify the features of the underlying class F and of the set of admissible targets Y that govern Ep; in particular, the way Ep scales with the sample size N (the so-called error rate). This question has been studied extensively, and we refer the reader to the manuscripts [\cite=MR1240719] [\cite=MR2319879] [\cite=MR2829871] [\cite=MR2724359] [\cite=MR2807761] [\cite=MR2807761] [\cite=Men-LWC] [\cite=Men-LWCG] for more information on its history and on some more recent progress.

Here, the aim is to obtain matching upper and lower bounds on Ep that hold for any reasonable class F, at least under some assumptions which we will now outline.

It is well understood that the ability to predict is quantified by various complexity parameters of the underlying class. Frequently, one encounters parameters that are based on various gaussian and empirical/multiplier processes indexed by 'localizations' of F (see, e.g., [\cite=LM13]), and any hope of obtaining matching bounds on Ep must be based on sharp estimates on these processes. Unfortunately, the analysis of empirical/multiplier processes is, in general, highly nontrivial. Moreover, and unlike gaussian processes, there is no clear path that leads to sharp bounds on empirical processes, and even when upper estimates are available, they are often loose and lead to suboptimal bounds on Ep.

The one generic example in which a more satisfactory theory of empirical/multiplier processes is known, is when the indexing class is L-subgaussian.

A class F  ⊂  L2(μ) is L-subgaussian with respect to the measure μ if for every p  ≥  2 and every [formula],

[formula]

and if the canonical gaussian process {Gf:f∈F} is bounded (see the book [\cite=MR1720712] for a detailed survey on gaussian processes).

More facts on subgaussian classes may be found in [\cite=LT:91] [\cite=vanderVaartWellner] [\cite=MR1720712] [\cite=MR2373017] [\cite=LM13]. For our purposes, the main feature of subgaussian classes is that the empirical and multiplier processes that govern Ep may be bounded from above using properties of the canonical gaussian process indexed by the class, giving one some hope of obtaining sharp estimates. Because of that feature, we will focus in what follows on subgaussian classes.

Despite their importance, complexity parameters are not the entire story when it comes to Ep. For example, it is possible to construct a class consisting of just two functions, {f1,f2}, but if the target Y is a [formula]-perturbation of the midpoint (f1 + f2) / 2, no learning procedure can perform with an error that is better than [formula] having been given a sample of cardinality N (see, e.g., [\cite=MR1741038]). Thus, rather than being solely determined by the complexity of the underlying class, there is an additional geometric requirement on F and Y which is there to ensure that all the admissible targets in Y are located in a favourable position relative of F (see [\cite=MR2426759] for more details). One may show that if F  ⊂  L2(μ) is compact and convex, any target Y∈L2 is in a favourable position relative to F. Therefore, to remove possible geometric obstructions, we will assume that F  ⊂  L2(μ) is compact and convex.

Finally, for a reason that will become clear later, we will not study a general class of admissible targets Y, but rather consider targets of the form Y = f(X) + W for some f∈F and W that is orthogonal to span(F) (e.g., W∈L2 that is a mean-zero random variable and is independent of X is a 'legal' choice).

With all these assumptions in place, let us formulate the question we would like to study:

Let F  ⊂  L2(μ) be a compact, convex class that is L-subgaussian with respect to μ. Given targets of the form Y = f(X) + W as above, find matching upper and lower bounds (up to constants) on Ep.

Let us recall the following standard definitions.

Let F  ⊂  L2(μ). Set

[formula]

Denote by

[formula]

the star-shaped hull of F with 0; F is star-shaped around 0 if star(F) = F.

Let {Gf:f∈F} be the canonical gaussian process indexed by F and set

[formula]

Finally, let D be the unit ball in L2(μ).

The best known bounds on Ep in the subgaussian context have been established in [\cite=LM13] and are based on two fixed points:

For κ1,κ2 > 0, set

[formula]

and

[formula]

Put

[formula]

In the context of the problem we are interested in, one has the following:

[\cite=LM13] For every L  ≥  1 there exist constants c1,c2,c3 and c4 that depend only on L for which the following holds. Let F  ⊂  L2(μ) be a compact, convex, L-subgaussian class of functions, set Y = f0(X) + W and assume that for every p  ≥  2, [formula]. There is a learning procedure (empirical risk minimization performed in F) for which, if

[formula]

then with probability at least

[formula]

the error of the procedure is at most Ep  ≤  r2.

The lower bound that complements Theorem [\ref=thm:LM-main-upper] uses 'local' analogs of rM and rQ that are based on the notion of packing numbers.

Let E be a normed space and set B to be its unit ball. Let M(A,rB) be the cardinality of a maximal r-separated subset of A with respect to the given norm, that is, the cardinality of the largest subset (ai)mi = 1  ⊂  A for which [formula] for every [formula].

For η1,η2 > 0 set

[formula]

and

[formula]

Put

[formula]

[\cite=LM13] There exist absolute constants c1 and c2 for which the following holds. Let F be a class of functions, set W be a centred normal random variable and for every f∈F put Yf = f(X) + W. If Ψ is a learning procedure that performs for every target Yf with confidence at least 3 / 4, then there is some Yf for which [formula].

One should note that a lower bound that is based on γQ was not known.

The connection between the two types of parameters is Sudakov's inequality (see, e.g. [\cite=LT:91]): there is an absolute constant c for which, for every H  ⊂  L2(μ),

[formula]

To see the connection, assume that for every f∈F, [formula], which means that rM(κ1)  ≤  4r. Applying Sudakov's inequality to [formula] and for the choice of ε = r / 2,

[formula]

hence, γM(c1κ1)  ≤  r. A similar observation is true for rQ and γQ, which shows that γM and γQ are intrinsically smaller than rQ and rM respectively, for the right choice of constants.

The starting point of this article is fact that the gap between these upper and lower estimates on Ep is more than a mere technicality.

The core issue is that the parameters rM and rQ are 'global' in nature, whereas γM and γQ are 'local'. Indeed, although [formula] is a localized set, [formula] is not determined solely by the effects of a 'level' that is proportional r. For example, it is straightforward to construct examples in which [formula] because of a very large, ρ-separated subset of [formula], for ρ that is much smaller than r. Thus, even if rM or rQ are of order r, this need not be 'exhibited' by [formula] at a scale that is proportional to r. In contrast, γM and γQ are 'local': the degree of separation is proportional to the diameter of the separated set, and the fixed point indicates that [formula] is truly 'rich' at a scale that is proportional to r.

As noted in [\cite=LM13], the upper and lower estimates coincide when the 'local' and 'global' parameters are equivalent, but that is not a typical situation - in the generic case, there is a gap between the two. An example of that fact will be presented in Section [\ref=sec:remarks].

Given that there is a gap between the two sets of parameters, one must face the obvious question: which of the two captures Ep? Is it the 'global' pair, rQ and rM, or the 'local' one of γQ and γM?

Our main result is that the 'local' parameters are the right answer - at least in the setup outlined above. To that end, we shall improve the upper bound in Theorem [\ref=thm:LM-main-upper] and add the missing component in Theorem [\ref=thm:LM-main-lower].

For every L > 1 and q > 2 there are constants c0,...,c5 that depend only of q and L for which the following holds. Let F  ⊂  L2(μ) be a compact, convex, L-subgaussian class of functions with respect to μ. There is a learning procedure [formula], for which, if Y = f(X) + W for f∈F and W∈Lq that is orthogonal to span(F), then with probability at least

[formula]

[formula]

The term r2Q(c4) exp ( - c5 exp (N)) is almost certainly an artifact of the proof, but in any case, it is significantly smaller than the dominating term in any reasonable example.

To complement Theorem [\ref=thm:main-upper] we obtain the following lower bound.

There exist absolute constants c0 and c1 for which the following holds. Let F  ⊂  L2(μ) be a convex, centrally-symmetric class of functions and let Ψ be any learning procedure that performs with confidence 7 / 8 for any target of the form Y = f(X) + W for some f∈F and W∈L2 that is orthogonal to span(F).

An outcome of Theorem [\ref=thm:main-upper] and Theorem [\ref=thm:main-lower] is that if W is a centred gaussian random variable that is independent of X, then for any convex, centrally-symmetric, L-subgaussian class F, the upper and lower estimates match (up to the parasitic and negligible term r2Q(c4) exp ( - c5 exp (N)) in the upper bound): when considering targets of the form Y = f(X) + W for f∈F,

[formula]

The second part of Theorem [\ref=thm:main-lower] follows from Theorem [\ref=thm:LM-main-lower]. We have chosen to present a new proof of that fact - a proof we believe is both instructive and less restrictive than existing proofs. The first part of Theorem [\ref=thm:main-lower] is, to the best of our knowledge, new.

Let us mention that if F happens to be convex and centrally symmetric (i.e. if f∈F then - f∈F), what is essentially the 'richest' shift of F is the 0-shift. Indeed, since F - F = 2F, it is evident that for every f∈F

[formula]

This makes one's life much simpler when studying lower bounds, as it gives an obvious choice of where to look. Indeed, the 'richest' part of F is the hardest part for a learning procedure to deal with - and that part is a neighbourhood of 0.

The idea of the proof of the upper bound

The proof of the upper bound is based on the following decomposition of the squared excess loss: let Y be the unknown target and set [formula]. For every f∈F, let [formula] and set

[formula]

Let [formula] and set

[formula]

to be the empirical minimizer in F. The learning procedure that assigns to every sample (Xi,Yi)Ni = 1 the empirical minimizer in F is called Empirical Risk Minimization (ERM).

Clearly, LFf* = 0, and thus, for every sample (Xi,Yi)Ni = 1, implying that members of the random set {f∈F:PNLFf  >  0} cannot be empirical minimizers. One way of identifying that set is via the decomposition [\eqref=eq:quad-decomp]: assume that (Xi,Yi)Ni = 1 is a sample for which, if [formula], one has

[formula]

and

[formula]

Since F is compact and convex, by properties of the metric projection onto a closed convex set in an inner product space,

[formula]

for every f∈F. Therefore, setting ξ = f*(X) - Y and ξi = f*(Xi) - Yi,

[formula]

for every f∈F that satisfies [formula]. Thus, if [\eqref=eq:quad-intro] and [\eqref=eq:multi-intro] hold for the sample (Xi,Yi)Ni = 1, then

[formula]

implying that [formula].

This argument has been used in [\cite=Men-LWC] and was then extended in [\cite=Men-LWCG], showing that

[formula]

- which is the type of result one is looking for. This method of proof leads to the complexity parameters rQ and rM: the former controls the quadratic component [\eqref=eq:quad-intro] and the latter the multiplier component [\eqref=eq:multi-intro]. The 'global' nature of rQ and rM, i.e., the fact that the two depend on the gaussian oscillation [formula] cannot be helped: the oscillations of the quadratic and multiplier processes are highly affected by the 'richness' of F around f* at every 'level'. A rather obvious idea for improving the upper estimate is 'erasing' all the fine structure of F, for example, by replacing F with an appropriate separated subset. The difficultly in such an approach is that the geometry of a separated set is problematic, and [\eqref=eq:conv-type-cond] will no longer be true for an arbitrary target Y. This is why we only consider targets of the form f(X) + W for f∈F and W that is orthogonal to span(F). For such targets, a version of [\eqref=eq:conv-type-cond] happens to be true even if F is replaced by a separated set.

The path we will take in proving the upper bound is as follows:

Therefore, the empirical excess loss relative to V satisfies

Preliminaries

Let us begin with some natation. Throughout, absolute constants are denoted by c,c1,... etc. Their value may change from line to line. c(α) is a constant that depends only on the parameter α. We use κ1,κ2,η1,η2 etc. to denote fixed constants whose value remains unchanged throughout the article.

In what follows, we will, at times, abuse notation and not specify the probability space on which each random variable is defined. For example, [formula] and integration is with respect to the joint distribution of X and Y, while [formula], in which case integration is with respect to μ.

Next, let us turn to the notions of cover and covering numbers.

Let B be a unit ball of a norm. Set N(A,B) to be the minimal number of centres a1,...,an∈A for which [formula]. (ai)ni = 1 is called a cover of A with respect to B. An r-cover is a cover with respect to the set rB.

It is standard to verify that if a1,...,am is a maximal separated subset with respect to B then it is also a cover with respect to B. Indeed, the maximality of the separated set implies that every point a∈A has some ai for which [formula], i.e, a∈ai + B. Therefore, N(A,B)  ≤  M(A,B). In the reverse direction, if a1,...,an is a cover with respect to B, then each one of the balls ai + B contains at most one point in any 2-separated set. Thus, M(A,2B)  ≤  N(A,B).

The following lemma is straightforward but it plays a crucial part in what follows.

Let T  ⊂  W  ⊂  L2(μ). For s > r > 0, set

[formula]

Then

for a suitable absolute constant c0.

Proof.   Fix w∈W and let [formula] be centres of a minimal s / 2-cover of that set. For every 1  ≤  i  ≤  N,

[formula]

and [formula], because ti∈T  ⊂  W. Therefore,

[formula]

Turning to the second part of the claim, assume that T and W are star-shaped around 0. Let w∈W, set t1,...,tm to be a maximal s / 2-separated subset of [formula] with respect to the L2(μ) norm and put yi  =  (r / s)ti. Since T is star-shaped around 0, yi∈T and (yi)mi = 1 is an r / 2-separated subset of (r / s)w + rD. For the same reason, (r / s)w∈W, and

[formula]

Using the standard connection between packing numbers and covering numbers and taking the supremum over w,

[formula]

Iterating the first part of the lemma,

[formula]

as claimed.

Before we turn to the proof of the upper bound, let us revisit the complexity parameters in question. Since F is a convex class, F - f is star-shaped around 0; hence, if s > r

[formula]

In particular, if γM(η1,f) < r then

[formula]

implying that γM(η1,f)  <  s as well.

This simple argument shows that if r  <  γM(η1,f) then

[formula]

while if r > γM(η1,f), the reverse inequality holds.

A similar assertion holds for γQ, rM and rQ; the rather standard proof of these facts, which is almost identical to the argument used above, is omitted.

The upper bound

Let F  ⊂  L2(μ) be a compact, convex class of functions. Fix r > 0 that will be named later and let V to be a maximal r-separated subset of F. Note that for every v0∈V, Fv0 = F - v0 is star-shaped around 0, and star(V - v0)  ⊂  F - v0. Using the notation of Lemma [\ref=lemma:covering], let T = W = Fv0, and for s > 2r > 0,

[formula]

Also, observe that [formula], implying that

[formula]

Moreover, the same estimate holds for [formula], and since V - v0 is r-separated,

[formula]

With that in mind, fix constants η1,η2,κ2 and κ3 that will be specified later, and for that choice of constants, let r > 0 for which

[formula]

and

[formula]

that is,

[formula]

Let V be a maximal r-separated subset of F with respect to the L2(μ) norm. Following the path outlined earlier, the idea is to study ERM in V, given the data (Xi,Yi)Ni = 1 for Y = f0(X) + W. To that end, one must control the multiplier and quadratic components in the decomposition of the squared loss relative to V: if [formula],

[formula]

Let us begin with the multiplier component:

Fix 0 < θ < 1, L > 1 and q > 2. There exist constants c0, c1 and c2 that depend only on L and q and for which the following holds. Let F be a convex, L-subgaussian class, set ξ∈Lq for some q > 2 and put [formula]. Then, for every v0∈V, with probability at least

[formula]

[formula]

The proof of Lemma [\ref=lemma:local-multiplier] is based on the following fact from [\cite=Men-Bern].

For L > 1 and q > 2 there exist constants c0, c1 and c2 that depend only on L and q for which the following holds. Let ξ∈Lq, set H to be an L-subgaussian class and denote by [formula]. For w,u  ≥  8, with probability at least

[formula]

[formula]

Proof of Lemma [\ref=lemma:local-multiplier]. The proof consists of two parts: first, controlling the process indexed by [formula] where s = (3 / 2)rM(η1,v0), and then treating the process indexed by [formula]. Clearly, without loss of generality one may assume that r  ≤  rM(η1,v0).

By the regularity of rM and since s > rM(η1,v0),

[formula]

Moreover, [formula], and since s / 4  ≤  rM(η1,v0), the regularity of rM implies that

[formula]

Therefore, applying Theorem [\ref=thm:multiplier] to the set [formula], there are constants c1, c2 and c3 that depend only on q and L for which, with probability at least

[formula]

if f∈F and [formula],

[formula]

Clearly, ( * )  ≤  θs2 if [formula], and for such a choice, if [formula] then

[formula]

since F - v0 is star-shaped around 0, [\eqref=eq:multiplier-in-proof-1] holds on the same event for every f∈F for which [formula].

Next, one has to control the process indexed by [formula]. Set j0  =  ⌈s / r⌉, fix sj = 2jr for 0  ≤  j  ≤  j0 and let [formula]. By Theorem [\ref=thm:multiplier], on an event Aj, for every h∈Vj,

[formula]

The aim it to ensure that ( *  * )j  ≤  θs2j / 4 and that Aj is of high enough probability. Indeed, on Aj, if v∈V and [formula],

[formula]

To that end, let [formula], recall that [formula] and thus dVj = sj = r2j. Put

[formula]

and consider two cases: first, if uj  >  8 then clearly, ( * )  ≤  θs2j / 4 and

[formula]

Alternatively, if uj = 8, then

[formula]

Also, by [\eqref=eq:usefulcover-nostar], Vj has at most [formula] extreme points. Since

[formula]

by standard properties of gaussian processes

[formula]

Hence, there are constants c11 and c12 that depend only on q and L for which if [formula].

Therefore, in both cases, there are constants c13 and c14 that depend only on q and L, and with probability at least

[formula]

[formula]

The claim follows by applying the union bound to this estimate for 0  ≤  j  ≤  j0.

Next, let us turn to the infimum of the quadratic process

[formula]

where r was selected in [\eqref=eq:condition-on-r] for a well-chosen η2 and where c is a suitable constant.

For every L > 1 there exist constants c0,c1 and c2 that depend only on L for which the following holds. For every v0∈V, with probability at least 1 - 2 exp ( - c0N), if v∈V and [formula] then

[formula]

The proof of Lemma [\ref=thm:quadratic-local] is similar to the one used in the analysis of the multiplier process: controlling relatively 'large distances' in F, i.e., when f∈F for which [formula]; and then 'small distances' in V, that is, v∈V for which [formula] (again, one may assume that r < rQ(η2)). For the constant η2 (yet to be specified), one has

and

The required lower bound on the infimum of the quadratic process [\eqref=eq:inf-quad-process] is based on estimates from [\cite=Men-LWC] and [\cite=Men-LWCG], which will be formulated under the subgaussian assumption, rather than using the original (and much weaker) small-ball condition.

For every L > 1 there are constants κ4, κ5 and κ6 that depend only on L for which the following holds. Let H be an L-subgaussian class that is star-shaped around zero. Set [formula] and fix ρ for which

[formula]

Then, with probability at least 1 - 2 exp ( - κ5N),

[formula]

We will apply Theorem [\ref=thm:quadratic-small-ball] to the class [formula] (large distances) and then to [formula] for sj = 2jr (small distances).

There exist absolute constants c0 and c1 for which the following holds. For every s > ρ  ≥  c0r,

[formula]

In particular, setting ρ = sj / 2 for η2 = c2κ4, one has

[formula]

Proof.   Fix ρ  <  sj and note that by Dudley's entropy integral bound (see, e.g., [\cite=LT:91] [\cite=vanderVaartWellner]),

[formula]

Applying [\eqref=eq:useful-covering-star] and since

[formula]

it follows that for r < t < ρ,

[formula]

Moreover, by [\eqref=eq:usefulcover-nostar],

[formula]

Hence, Vj is the union of at most exp ( * ) 'intervals' of the from

[formula]

log(V ρ D, tD) ≤ c(η N log(2s/r) + log(2ρ/t)).

[formula]

(v-v)(X) ≥ κ v-v.

[formula]

1-c - 2exp(-cηrN),

[formula]

| (v(X)-Y)(v-v)(X)-(v(X)-Y)(v-v)(X)| ≤ v-v.

[formula]

-v ≤ cr.

[formula]

The lower bound

The lower estimates presented below are based on a volumetric argument. The idea is that if a learning procedure is 'too successful', a well-separated subset of F endows a well-separated subset in [formula] (a set that depends on X1,...,XN). However, because of some volumetric constraint, there is not 'enough room' for such a separated set to exist, leading to a contradiction.

The notions of volume are different in the two estimates: one is based on the Lebesgue measure while the other is determined by the choice of the 'noise' W, which is, in our case, gaussian.

Let F be a class of functions and assume that [formula]. For every f∈F, set

[formula]

The set [formula] is called the version space of F associated with f and [formula].

In other words, [formula] consists of all the functions in F that agree with f on [formula]. Naturally, in the context of learning, [formula] is a random sample (Xi)Ni = 1, selected according to the underlying measure μ.

The diameter of the version space is a reasonable choice for a lower bound on the performance of any learning procedure: if Yi = f(Xi) + Wi, a learning procedure cannot distinguish between f and any other function in the version space associated with f and (Xi)Ni = 1. Hence, the largest typical diameter of a version space should be a lower estimate on the performance of any learning procedure, as the following well-known fact shows (see, e.g., [\cite=LM13]).

Given a random variable W, for every f∈F set Yf = f(X) + W. If Ψ is a learning procedure, then where the probability is relative to the product measure endowed on [formula] by the N-product of the joint distribution of X and W.

Clearly, if W is orthogonal to span(F), then for every h∈F and every target Yf, [formula]. Thus, the largest typical diameter of a version space [formula] is a lower bound on Ep for the set of admissible targets Y  =  {f(X) + W:f∈F}.

This leads to the following question:

Given a class F defined on a probability space (Ω,μ), f∈F and [formula], find a lower estimate on

[formula]

One situation in which Question [\ref=qu:lower-est-version-space] is of independent interest is when [formula] is a convex body (i.e., a convex, centrally-symmetric set with a nonempty interior) and [formula] is the class of linear functionals associated with T. For every [formula] set [formula], and let [formula] be the matrix whose rows are x1,...,xN. Thus,

[formula]

If μ is an isotropic, L-subgaussian measure on [formula], one may show that with probability at least 1 - 2 exp ( - c0N),

[formula]

(see [\cite=MR2373017]). This extends the celebrated result of Pajor and Tomczak-Jaegermann [\cite=MR941809] [\cite=MR845980], that [\eqref=eq:PT] holds for the Haar measure on Sn - 1 (and thus, also for the gaussian measure on [formula]). It turns out that [\eqref=eq:PT] is not far from optimal:

There exists an absolute constant c for which the following holds. Let F  ⊂  L2(μ) be a convex and centrally-symmetric set. If

[formula]

then for every [formula],

[formula]

Since F is convex and centrally-symmetric, F - F = 2F and 0∈F. Therefore,

[formula]

Hence, Theorem [\ref=thm:kernel] shows that if γQ(c,0)  >  r then for every [formula], [formula]. In particular, for every W∈L2 that is orthogonal to span(F), the best possible error rate in F that holds for every target Yf = f(X) + W, is at least γ2Q(c,0)  ≥  c1γ2Q(c).

Proof.   Let f1,...,fm be r / 4-separated in [formula]. Set

[formula]

and observe that [formula]. Also, for every h∈Ai, [formula]; therefore, if hi∈Ai and [formula], then [formula].

Fix [formula] and for A  ⊂  F set

[formula]

the coordinate projection of A associated with [formula]. Clearly, for every 1  ≤  i  ≤  m,

[formula]

Consider two possibilities. First, if there are [formula] for which [formula], there are hi∈Ai and [formula] that satisfy [formula], thus showing that [formula].

Otherwise, the sets [formula] are disjoint subsets of [formula]. And, setting [formula], [\eqref=eq:separated-empirical] implies that M(T,T / 32)  ≥  m. Since T is a convex, centrally symmetric subset of [formula], a standard volumetric argument shows that M(T,T / 32)  ≤   exp (cN) for a suitable absolute constant c. Thus, if m  >   exp (cN), [formula], as claimed.

The final result of this section is the 'noise-dependent' lower bound.

There exist absolute constants c1 and c2 for which the following holds. Let F  ⊂  L2(μ) be a convex, centrally-symmetric class of functions, set W to be a centred normal random variable that is independent of X, and for every f∈F, put Yf = f(X) + W. If Ψ is a learning procedure that performs with confidence of at least 7 / 8 for every Yf, there is some Yf for which

[formula]

Stronger versions of Theorem [\ref=thm:lower-bound] (without the assumption that F is convex and centrally-symmetric) may be proved in several different ways: using information theoretic tools (see, Theorem 2.5 in [\cite=MR2724359]), or, alternatively, by applying the gaussian isoperimetric inequality as in [\cite=LM13]. Both these arguments are rather restrictive, because they relay on rather special properties of the noise.

Although the proof we present below is also for a gaussian noise, the argument is less restrictive and may be extended to other choices of noise (e.g. when W is log-concave rather than gaussian). The argument is essentially the same as Talagrand's proof of the dual-Sudakov inequality [\cite=LT:91], and as such is volumetric in nature: obtaining a lower bound on the measure of a shift of a centrally-symmetric set in terms of the Euclidean norm of the shift.

Let [formula] be centrally symmetric and set [formula]. If ν is the centred gaussian measure on [formula] with covariance σ2IN and | | denotes the Euclidean norm on [formula], then

[formula]

Proof.   A change of variables shows that

[formula]

Let [formula] be the expectation with respect to the gaussian measure ν, conditioned on A. Thus,

[formula]

Since A is symmetric, [formula], and by Jensen's inequality

Proof of Theorem [\ref=thm:lower-bound]. Let Ψ be a learning procedure that performs with accuracy Ep for every target Yf  =  f(X) + W for f∈F and W  ~  N(0,σ2) that is independent of X. Note that for the target Yf, the true minimizer in F is f* = f and for every h∈F,

[formula]

Thus, if [formula] is a sample on which Ψ performs with accuracy Ep relative to the target Yf, then [formula].

Let (fj)mj = 1 be a subset of [formula] that is r / 2 separated in L2(μ) for (r / 2)2 = 9Ep and fix [formula].

For every 1  ≤  j  ≤  m, put

[formula]

i.e., [formula] consists of all the vectors [formula], for which, upon receiving the data (xi,fj(xi) + wi)Ni = 1, Ψ selects a point whose L2 distance to fj is at most [formula].

Let ν be the centred gaussian measure on [formula] with covariance σ2IN. Since W is a centred gaussian random variable with variance σ2, (wi)Ni = 1 is distributed according to ν, and since it is independent of X, if Ψ performs with accuracy Ep and with probability at least 7 / 8, it is evident that

[formula]

A standard Fubini argument shows that there is an event Cj  ⊂  ΩN of μN probability at least 1 / 2, and for every [formula], [formula]. Observe that if [formula] then by the symmetry of ν, [formula], and the centrally-symmetric set [formula] satisfies that

[formula]

Let zj = (fj(xi))Ni = 1. If [formula], the sets [formula] and [formula] are disjoint, because Ψ maps [formula] to an r / 6-neighbourhood of fj and [formula] to an r / 6-neighbourhood of [formula] - but [formula]. Therefore

[formula]

integrating with respect to μN,

[formula]

and all that remains is to control [formula] from below.

Applying Lemma [\ref=lemma:lemma-from-dual-sudakov],

[formula]

By Chebychev's inequality and recalling that [formula],

[formula]

for an appropriate choice of an absolute constant c0 and for every 1  ≤  j  ≤  m. Thus, on an event of μN measure at least 1 / 4, [formula], [formula] and [formula]; therefore, Hence, log m  ≤  c22Nr2  /  σ2, i.e., [formula], implying that Ep  ≥  c3γ2M(c2  /  σ).

Some Remarks

We begin this section with an example of 'natural' sets, for which there is a true gap between the two sets of parameters: rQ / rM and γQ  /  γM.

Let [formula] be a convex body in [formula] (i.e., a convex, centrally-symmetric set with a nonempty interior), put [formula], the class of linear functionals associated with T and set μ to be the gaussian measure on [formula].

It is straightforward to verify that for every r > 0, [formula] is isometric to [formula], where Bn2 is the Euclidean unit ball in [formula]. Let 1  ≤  p < 2, and set T = Bnp, the unit ball in [formula]. One may show (see [\cite=LM13]) that when p = 1, rM and γM are equivalent, as are rQ and γQ. However, such an equivalence is no longer true for 1 < p < 2 (of course, as long as p > 1 + 1 /  log n - otherwise, [formula] is equivalent to [formula]).

To see how that gap between the 'global' and 'local' parameters is exhibited in Bnp for 1 < p < 2, let x = (xi)ni = 1∈Bnp and set (x*i)ni = 1 to be the non-increasing rearrangement of (|xi|)ni = 1; thus, x*i  ≤  i- 1 / p. Recall the well known fact (see, e.g., [\cite=MR2371614]), that [formula] is equivalent to Thus, if N  ≤  n2 / p,

[formula]

Let us consider the case in which 1  >  r  ≫  c2(p)n- (1 / p - 1 / 2). Set [formula] and observe that

[formula]

Clearly, for a well-chosen constant c3 one has [formula], and

[formula]

where BIq,  ∞ is the unit ball in [formula] endowed with the weak [formula] norm. In particular, if [formula], BIcp,  ∞  ⊂  (r / 10)BIc2 and the impact of those 'small' coordinates on Euclidean distances is negligible:

[formula]

Hence, separation at scale r occurs only because of the largest [formula] coordinates of the vectors involved.

On the other hand, the contribution of those 'large' coordinates to [formula] is equally negligible. Indeed, if [formula] for some m  ≤  n / 2 and α  ≥  1, it is standard to verify that

[formula]

If [formula] and since r  ≫  c2(p)n- (1 / p - 1 / 2), it follows that

[formula]

Thus, as long as r is significantly larger than c2(p)n- (1 / p - 1 / 2), the gaussian average of the intersection body [formula] originates from the 'small coordinates' in the monotone rearrangement, and in particular, from vectors whose Euclidean norm is significantly smaller than r. Such vectors are 'invisible' to γM, which is why γM is much smaller than rM.

The role of fixed points

Fixed points are encountered frequently in Empirical Processes and Statistics literature, and almost always with the same goal: obtaining 'relative' upper bounds on various empirical processes. To obtain such bounds, one has to compare the oscillation (i.e., the behaviour of the process indexed by [formula]) with some function of r.

One usually obtains upper bounds on the oscillation via a symmetrization argument, leading to a sample-dependent Bernoulli process. Thus, the standard outcome is a fixed point equation, linking an entropy integral relative to the random L2 metric and generated by the sample X1,...,XN, with the desired function of r (see [\cite=vanderVaartWellner] for numerous examples).

Still within the realm of entropy integrals, it is possible to impose additional structure on the problem, which allows one to replace the empirical L2 (random) metrics with the global L2(μ) metric. For example, a fixed point equation with the same normalization as rM may be found in [\cite=MR1240719], where the setup allows the transition between the random metric and the deterministic one - but the 'philosophy' of the proof is the same: it is based on an entropy integral.

Since the entropy integral is only upper estimate on the supremum of the empirical process in question - regardless of the underlying assumptions, it is often loose. Therefore, one would like to find a general argument bypassing the whole mechanism of entropy integrals.

As a first step, and because it is natural to expect that the empirical processes in question converges to a gaussian limit, one may try a 'gaussian'-based fixed point, which relies on [formula], rather than on an entropy integral bound. And, indeed, under a subgaussian assumption, the results of [\cite=LM13] lead to the gaussian-based rM and rQ.

Our results show that rM and rQ are not the end of the story and can be improved - at least for the special learning problems we consider. The 'right' fixed points should involve the smaller local entropy estimates rather than the oscillation of the gaussian process. One fixed point that seems closer in nature to γM than to rM may be found in the celebrated work of Yang and Barron [\cite=MR1742500], though a closer inspection shows that this impression is inaccurate.

Comparing [\cite=MR1742500] to our results is somewhat unnatural because the setup in [\cite=MR1742500] is completely different: a function class consisting of uniformly bounded functions and an independent gaussian noise, both of which are crucial to the proof (see Section 3.2 in [\cite=MR1742500]). Also, the upper estimate is an existence result of a 'good' procedure - rather than a specific choice of a procedure; the estimate holds in expectation and not with high probability; and it does not tend to zero with the 'noise level' of the problem.

All these differences are significant, but are still not a conclusive indication that the nature of the complexity parameter in [\cite=MR1742500] is different from ours. That indication is the key to the results in [\cite=MR1742500]: the assumption that the underlying class 'large' - in the sense that

[formula]

One should note that this assumption immediately excludes all the modern high-dimensional problems, involving classes indexed by subsets of [formula]. Indeed, for any convex subset of [formula], the liminf above is 1 rather than strictly greater than 1. Equation [\eqref=eq:Yang-Barron] has two significant implications:

(if L  >  4 then the gaussian process {Gf:f∈F} is not bounded and the class F is not subgaussian, while if L = 4 an entropy estimate is not enough to determine whether the gaussian process is bounded and thus requires a more subtle analysis). When L < 4 and because the entropy of the 'local' set [formula] is equivalent to the entropy of F, it follows that there are 0 < q1  ≤  q2 < 2 for which, for every ε  ≤  R small enough,

Using Dudley's entropy integral for the upper bound and Sudakov's minoration for the lower one, it is straightforward to verify that

Therefore, the 'global' parameters rM and rQ are equivalent to the local ones γM and γQ; in fact, the 'local' and 'global' parameters are even equivalent to the ones defined via the entropy integral. Thus, the typical situation in [\cite=MR1742500] is very different from the problems studied here - mainly because of [\eqref=eq:Yang-Barron].