STRUCTURE AND DECAYS OF PARTICLES CONTAINING HEAVY QUARKS (THEORY) Kacper Zalewski Institute of Physics, Jagellonian University, ul. Reymonta 4 Poland 30 059 Kraków

Some recent results and problems in the theory of particles containing heavy quarks are reviewed.

Introduction

The subject of this review is the status of the theory of particles containing heavy quarks, except for production processes, which will be discussed by P.Nason1) in the next talk. Let us note first that there is much activity in this field of research. In the high energy physics data base hep-ph, in 1994 I have found more than four hundred papers on this subject. Out of that about 15% were concerned with production processes, but what is left is more than one paper per day including sundays. A linear extrapolation of the number of papers submitted to this data base in the first five months of this year leads to an expectation of over five hundred papers in 1995; thus the activity keeps increasing. This implies that in my presentation I shall have to be very selective.

I shall discuss five problems.

Determination of the fundamental parameters of the standard model. This is perhaps the most often invoked motivation for the study of heavy quark physics. Ten of these parameters have names of heavy quarks in their symbols ([formula]). Moreover, some others can be determined using heavy quark systems. An example is the determination of αs from the decays of quarkonia, which has been discussed at this conference by M. Schmelling2).

The heavy quark expansions. Such expansions, there is more than one, have been a break through in heavy quark physics. They were the subject of a separate talk at the previous Physics in Collision Conference3).

CP nonconservation and mixing. Here the predictions of the standard model are particularly stringent and "new physics" has perhaps a chance of becoming visible.

Rare decays. Here the rapidly improving accuracy of the experimental data and of the theoretical analyses has already provided a striking confirmation of the standard model, but further progress is possible and will lead to important tests of the model.

Nonleptonic decays. This is a difficult problem4), where progress since 1987 has been slow and new ideas seem to be particularly needed.

Quark masses

The values of the quark masses are among the fundamental parameters of the standard model and have been used for many years and in many calculations. Nevertheless; their values are poorly known and controversial. In the Particle Data Group Tables they were listed for the first time in 19925) and even then with the comment that they are given "to provoke discussion". The values proposed for the heavy quark masses at that time were mc  =  (1.31.7) GeV, mb  =  (4.75.3) GeV and mt  >  91 GeV. Since then the discovery of the t quark has been a break through on the experimental side6), but also on the theoretical side much clarifying work has been done.

It has been realized, how important is the question "what do you mean by the quark mass?". In order to apply the standard definition [formula] one would have to perform the measurements on a free quark. This, however, according to quantum chromodynamics (QCD) is impossible. The next choice is the use of the term [formula], which occurs in the QCD Lagrangian and contains explicitly the quark mass. This, however is the bare mass, which is not measurable. Related to measurable quantities is the self-energy, which is the sum of the bare mass and of corrections corresponding to Feynman diagrams containing loops. The first correction corresponds to the diagram, where the quark emits and reabsorbs a virtual gluon. It reads, see e.g.7),

[formula]

where [formula] is the Euler constant and the limit ε  →  0 is understood. This formula clearly exhibits the two main difficulties. The first term in the square bracket is infinite and the scale factor μ is arbitrary. The remedy is to rewrite the self-energy in the form

[formula]

and to choose δm so that Σ(1)  -  δm is finite. Since the bare quark mass m is unmeasurable, one can assume that the new quark mass

[formula]

is finite. This is, of course, the standard mass renormalization procedure. Including more Feynman diagrams, one has to redefine δm and mQ, but let us consider first the one gluon loop result ([\ref=sig1lo]). Many choices of δm are possible. For instance, choosing δm to cancel the [formula] term only one obtains the MS (minimal subtraction) mass, choosing δm to cancel also the -  γ  +   log (4π) term one obtains the so called [formula] mass and choosing δm to cancel all the Σ(1) expression one obtains the pole mass. Using elementary algebra one can correlate the various mass definitions. For instance, the relation (in the one gluon loop approximation!) between the [formula] mass calculated for [formula], further called the running mass, and the pole mass is

[formula]

The differences between the pole masses and the running masses are very significant. From the simple formula given above, substituting typical values αs(mc)  ≈  0.35, αs(mb)  ≈  0.20 and αs(mt)  ≈  0.10, one finds the differences [formula] approximately equal to 0.17 GeV for the c-quark, 0.34 GeV for the b-quark and 7 GeV for the t-quark. A more careful calculation by Titard and Yndurain8) gives 0.26 GeV for the c-quark and 0.51 GeV for the b-quark. For the t-quark typical values are (89)GeV.

The recent Particle Data Group Tables9) contain a discussion of several definitions of quark masses and choose for the c-quark and for the b-quark the masses [formula] to put into the tables. They propose mc  =  (11.6) GeV and mb  =  (4.14.5) GeV. The uncertainties remain very large, but the definition is now clear. An obvious question is, how the newly measured mass of the t-quark6) mt  ≈  174 GeV should be interpreted? Even with the present uncertainties, the almost 10 GeV difference between the pole mass and the running mass for the t-quark is not negligible. In principle, the experimental procedure defines a new mass mt. In practice, however, everybody expects that this definition is close (to better than one GeV say) to the pole mass or to the running mass. Analogies with resonance decays and the fact that the t and [formula] at the Tevatron are usually produced in the colour octet state, i.e. do not interact very much with each other, suggest the pole mass, but arguments in favour of the running mass are also quoted. The problem is not yet quite solved. Here we would only like to stress its importance.

Taking into account higher orders of perturbation theory one finds an interesting difficulty in connection with the definition of the pole mass. The series used to define this mass is found to diverge10),11),12). This is usually described in terms of renormalons, which have been discussed at this conference by M. Karliner13). The renormalons can be eliminated in favour of terms divergent like powers of the cutoff, or terms divergent like inverse powers of the the lattice spacing, but the final result is always the same -- the pole mass cannot be defined better than with an uncertainty of about 50 MeV. A recent review of this problem has been given by Sachrajda14). There are two ways out of this difficulty. One is to abandon the pole masses and to express everything in terms of running masses. This is being advocated e.g. by Bigi and collaborators11). Another way is to introduce nonperturbative subtractions, which tame the divergence and enable a precise definition of a "subtracted pole mass", as proposed by Martinelli and Sachrajda, see14) and references given there.

Heavy quark expansions

The heavy quark expansions use the fact that the masses of the heavy quarks are much larger than the low energy mass scales, ΛQCD,  Λchiral and [formula], where MH denotes the mass of a hadron containing one heavy quark Q. An extensive review has been given in Physics Reports by Neubert15). Very recent reviews include the review by Bigi16) for inclusive processes and by Mannel17) for exclusive processes. At this conference applications have been presented by P. Jarry18) and R. van Kooten19).

Already the zero order approach, where m- 1Q  →  0, leads to many beautiful results, like the prediction that

[formula]

or the expression of the six form factors necessary to describe the semileptonic decays B  →  D( * )lν in terms of a single Isgur-Wise function ξ(ω), where ω is the Lorentz factor of the D( * )-meson in the rest frame of the B-meson. Since this has been reviewed many times, however, it may be more interesting to discuss the difficulties of the heavy quark approach. Difficulties is here not an euphemism for disagreement with experiment; we shall discuss problems, which required, or still require hard work and/or new ideas. We shall discuss three problems

Corrections from quantum field theory (QFT),

How to compare theory with experiment in spite of the occurrence of arbitrary functions in the theoretical expressions,

Corrections from higher orders in the expansion in powers of m- 1Q.

QFT corrections

A nice introduction to the methods of calculating the quantum field theory (QFT) corrections can be found in Neubert's review15). The present status of such calculations for weak decays has been very recently summarized by Buras20). The calculations are often hard work -- years of work for teams of experts. They are nevertheless essential for a quantitative comparison of the theory with experiment. In particular, they reduce or eliminate the dependence of the results on the scale parameter μ. Much remains to be done. For example, the difference between the pole mass and the running mass of the t-quark is known with an uncertainty much larger than that caused by renormalons, because the necessary QFT corrections have not yet been calculated.

Arbitrary functions

In order to illustrate how the arbitrary functions enter the predictions, let us consider the semileptonic decay B  →  D( * )lν. In the heavy quark picture this process can be decomposed into two steps. First the b quark emits the lepton pair and goes over into the c-quark. This happens at a high energy scale, where the light components of the mesons are ineffective. The matrix element for this part of the process is calculated as if the heavy quarks were free. Then the light component of the B-meson with velocity [formula] must reorganize itself into the light component of the D( * ) meson recoiling with velocity [formula]. The probability amplitude for that to happen is (almost) unknown and is the Isgur-Wise function ξ(ω), where ω  =  vμvμ'. Thus, the decay amplitude for the decay is proportional to

[formula]

A typical problem is how to extract from the experimental data the Cabibbo-Kobayashi-Masakawa (CKM) matrix element Vcb in spite of the unknown factor ξ(ω). Two approaches to this problem have been popular. In the exclusive approach, reviewed recently by Neubert21), one uses the fact that in the case of no recoil [formula], or equivalently ω  =  1, the light components of the B-meson and of the D( * )-meson are almost the same. Thus ξ(1)  ≈  1. The corrections are small and calculable. In the inclusive approach reviewed recently by Bigi16), one uses the fact that the sum of probabilities for all the possible rearrangements of the light component must be equal one. Thus, in each case one identifies an experimental quantity -- the semileptonic decay probability to D( * ) at zero recoil in the exclusive method, the total semileptonic decay width in the inclusive method -- for which the theoretical prediction does not depend on the arbitrary function. So the CKM matrix element Vcb can be extracted in a model independent way.

An obvious question is, which method is better? Since both methods are applied to the same experiments, a good measure of their quality is the error on Vcb. Neubert21) quotes for the exclusive method the error ±  0.003  ±  0.002 and for the inclusive method ±  0.001  ±  0.005, where in each case the first error is experimental and the second is theoretical. Bigi16) finds for the corresponding errors ±  0.003  ±  0.002 and ±  0.002  ±  0.002. A comparison of these numbers shows that experimentally the inclusive method is easier. There is a controversy, however, concerning the theoretical uncertainties in the inclusive method. In practice, of course, one should use both methods and average the results.

Let us note finally that interesting results can also be obtained by studying the inclusive processes in the small recoil velocity limit22),23), which combines the inclusiveness with the selection of a simplifying kinematical configuration.

Higher orders in m- 1Q

The limit m- 1Q  →  0 gives many beautiful results, but sometimes it is too crude. For instance, we may not be satisfied with the prediction MB*  ≈  MB. Assuming that the corrections are proportional to m- 1Q one gets immediately

[formula]

which is very reasonable.

The problem is to find a systematic expansion of the quantities of interest into inverse powers of the quark mass. For simplicity we shall use the language of quantum mechanics, but most of the results presented here are available also in full quantum field theory. One replaces the bispinors ψ describing the heavy quarks by new bispinors ψ' defined by the relation

[formula]

This corresponds to the replacement of the QCD Lagrangian L by a new Lagrangian L' so that

[formula]

Any current can be rewritten in terms of the transformed bispinors e.g.

[formula]

The problem would be solved, if one could choose the matrix U so that the bispinors ψ' do not depend on the quark masses, while the operator [formula] is a power series in inverse powers of the heavy quark mass. In practice the matrix U is built in a series of steps.

First, a transformation is chosen so that the bispinors ψ' satisfy the relation

[formula]

where v is the velocity of the hadron containing the heavy quark. In the rest frame of the hadron this simply means that only the first two components of the bispinor can be different from zero. This step is easy and there are various ways of performing it. Thus, Georgi24) uses the equations of motion to eliminate the "small" components of the bispinor, Körner and Thompson25) use an analogue of the Foldy-Wouthuysen transformation and Mannel and collaborators26) use the functional formalism.

The second step is to ensure a normalization of the bispinor ψ', which does not depend on mass, e.g. a normalization to unity. In the Foldy-Wouthuysen approach this is automatic. In the others one has to introduce a suitable normalizing factor, but this also is not difficult.

Finally, it is necessary to make the two independent components of ψ' independent of the quark mass and here there is no simple method. In practice one uses perturbation theory to express the original bispinors ψ', which are solutions of the equations of motion corresponding to the Lagrangian L' and inherit its dependence on mass, by solutions of the equations of motion with an approximate Lagrangian L0', which does not introduce a mass dependence into the solutions. Perturbative expressions, however, become very complicated, when one goes to higher orders. Therefore, in practice it is difficult to go beyond one or two nonvanishing corrections to the leading term.

Let us summarize the situation. The leading terms are well understood and rather simple. For low order corrections and suitably chosen quantities it is often possible to express the quantities of interest in terms of a few constants with clear physical interpretations. A much discussed constant, which occurs at order m0Q in mass calculations, is

[formula]

Since the hadron mass MH is measurable, a discussion of this constant reduces to the discussion of the quark mass, slightly complicate by the fact that [formula] should not depend on the quark mass. In the next order of the mass calculations two more constants appear

[formula]

We have quoted here both of the two most popular notations. These constants have simple physical interpretations. The first corresponds to the interaction of the chromomagnetic moment of the heavy quark with the chromomagnetic field produced by the light component. This interaction is responsible for the mass splitting between the B and the B*, therefore it can be reliably determined from experiment. Since for a spin one half fermion its (chromo)magnetic moment is inversly proportional to its mass, the constant appears at order m- 1Q. The second constant is related to the kinetic energy of the heavy quark in the hadron. In the rest frame of the hadron, the momentum of the heavy quark is equal to the momentum of the light component and does not depend (to zero order!) on the flavour of the heavy quark. The kinetic energy has, however, an additional factor m- 1Q, which introduces a flavour dependence at this order. This constant is not directly measurable. A lower limit for it

[formula]

has been found in refs27),28),23). The upper limit given in formula (14) is just a popular guess. The higher order calculations contain so much arbitrariness that new ideas may be needed to make them useful.

Mixing and CP nonconservation

According to the standard model CP nonconservation and its effects in particle-antiparticle mixing result only from the fact that the CKM matrix is not real. In terms of the parameter [formula], the orders of the real and imaginary parts of the elements of the CKM matrix are as follows

[formula]

Here, as usual, the columns correspond to the quarks d,  s,  b and the rows to the quarks u,  c,  t. Thus, e.g. the entry in the upper right corner indicates that the matrix element Vub has both the real and the imaginary part of order λ3. This element and Vtd are the only two, which have comparable real and imaginary parts. The imaginary parts of the other elements are either zero, or very small. It is also useful to keep in mind the experimental uncertainties of the elements. These are summarized below

[formula]

All these data are from the latest Particle Data Group Tables9). They have been obtained using the unitarity of the CKM matrix

[formula]

It is only because of unitarity that Vtb is the best known number in the table. The unitarity of the CKM matrix V implies also that the scalar product of any two rows, as well as the scalar product of any two columns, vanishes; for j  ≠  k:

[formula]

The fact that the sum of three complex numbers vanishes means geometrically that the corresponding vectors form a triangle in the complex plane. There are, therefore, six unitarity triangles corresponding to the three pairs of rows and the three pairs of columns. Four out of these triangles are almost degenerate, with one side much shorter than the other two. The other two contain each essentially the same information, thus it is enough to use one of them, e.g. the triangle corresponding to the product of the first and the third column. The complex number V*cdVcb is almost purely real and reasonably well known. Therefore, it is usual to divide the lengths of all sides by the absolute value of this number and thus to obtain a reduced triangle with vertices at the points (0,0) and (1,0). The challenge is to obtain the two coordinates of its third vertex.

Knowing the position of this vertex one can in principle, i.e. assuming some values for a number of less fundamental parameters like decay constants, bag constants, phase shifts defining final state interactions etc., predict all the asymmetries and mixings in heavy quark systems. So many predictions can be made that this looks like a very stringent test of the standard model and a good place to look for new physics.

For the moment our knowledge about the position of the third vertex is rather vague. Let us denote by α,  β  γ the angles of the triangle at the vertices (0,0), (1,0) and at the third vertex respectively. The knowledge of these angles is, of course, equivalent to the knowledge of the position of the third vertex. A recent analysis of the available experimental data performed by Pich and Prades29) has given the following limits for these angles

[formula]

These bounds are not very sure -- in the same paper one can find another set, though less favoured by the authors -- but they show that nontrivial statements about the angles i.e. about the position of the third vertex are just beginning to be possible. The field is of great interest and potential importance. Incidentally, it should be kept in mind that the limits on the single angles are somewhat misleading, because the angles are correlated. Besides the obvious correlation α  +  β  +  γ  =  π, there are also important correlations between pairs of angles.

It is remarkable that the lengths of the sides of the triangle, which is another way of saying the position of the third vertex, can be obtained from processes unrelated to CP nonconservation cf. e.g.30). The absolute value |Vub| can be obtained from studies of the decays B  →  Xulν.The absolute value |Vtd| can be obtained either from the mass difference [formula], or from the rare decays B  →  Xdγ. Unfortunately, none of the corresponding experiments is easy. There seems to be a kind of complementarity at work here. When a measurement is comparatively easy, like looking for charmless semileptonic B decays, its theoretical interpretation is unclear. When theory provides a nice prediction like31)

[formula]

the necessary measurements are difficult. In this particular case [formula] is sufficiently well known19), but [formula] is proportional to the parameter xs, for which only a lower limit xs  >  9 is known from experiment31), while the expectation is xs  ≈  20, which may be beyond the range even of HERA B.

Rare decays

The name rare decays is applied to decays involving neutral flavour-changing neutral currents. The most popular process is b  →  sγ, for which experimental data is available32),33). Such processes are forbidden at the tree level and they are ascribed to pingwin diagrams, where the b-quark dissociates into a virtual t-quark and a virtual W- boson and then the t-quark reabsorbs the W- boson and goes over into an s-quark. The photon can be emitted from any of the lines of the diagram. The contribution from these diagrams involves two elements of the CKM matrix, the very well-known element Vtb and the reasonably well known element Vts. Thus, reliable predictions are possible. For a recent review see20).

The rare decays are of interest for a variety of reasons. They demonstrate the importance of pingwin diagrams. They test our understanding of QCD corrections, which enhance the decay probability by more than a factor of two. It is also amusing to compare the simple pingwin diagram, which is used for rapid presentations, with the complexity of the real calculation necessary to obtain reliable numbers. In the real calculations one starts with an effective hamiltonian, which is a linear combination of about ten operators

[formula]

Then at some high scale one calculates the coefficients ci as if a simple perturbative calculations were valid. This is known as matching. Then one uses the renormalization group equations to find the values of these coefficients at a scale μ of the order of mb. In order to do that, one needs the anomalous dimensions, which form a [formula] matrix. A typical final result is34)

[formula]

to be compared with the corresponding experimental result33) -- 2.3  ±  0.7. The main source of uncertainty in the theoretical prediction is the uncertainty about the final scale, which should be used. Following Ali and Greub35) one uses

[formula]

and includes the corresponding spread of the results into the uncertainty of the prediction. The scale can be obtained from a higher order calculation, but this calculation is so difficult that it has yet not been completed, though it is in progress. The corrections of order 0(m- 2b) are believed to be below 10 % and, therefore, not very important at present. Thus, in a few years the theoretical uncertainty is expected to decrease significantly. A precise comparison of the predictions of the standard model with the data for rare decays is of great interest for people working on extensions of the standard model. Already at the present level of precision many possibilities of going beyond the standard model have been excluded.

A few other rare processes should be mentioned. The branching ratio for the exclusive process B  →  K*γ has been measured32). Theoretical calculations can reproduce the result, but they involve assumptions about the structure of the K* and, consequently, are more model dependent than the calculations for the inclusive process. The process B  →  Xse+e- is of interest, because in it the γ produced in the process b  →  sγ has non-zero mass and consequently our understanding of additional formfactors can be tested. The branching ratio for this process has not yet been measured. Finally, measurements of the rare processes involving the decay b  →  dγ could be used to get information about the CKM matrix element Vtd.

Nonleptonic decays

For nonleptonic decays of particles containig heavy quarks, the standard reference is still the paper of Bauer Stech and Wirbel4). According to the model presented in this paper (BSW model), for two body decays two basic processes contribute. In class I decays, the heavy quark decays and emits a virtual W-boson, which goes over into a meson. Then the quark produced from the initial heavy quark recombines with the spectator(s) quark(s) and produces another hadron. The probability amplitude for class I processes is multiplied by a phenomenological constant a1  ≈  1. In class II decays, after the decay of the heavy quark the virtual W-boson decays into a quark-antiquark pair. The antiquark from this pair forms a hadron by recombination with the quark produced from the initial heavy quark and the quark forms another hadron by joining the spectator(s). The probability amplitude for class II processes is multiplied by a phenomenological constant a2, which in absolute magnitude is much smaller than a1. Finally, class III decays can proceed via both mechanisms. Their study makes it possible to determine the relative sign of the constants a1 and a2. For a recent determination and discussion of these constants see36).

The BSW model becomes particularly simple, when the final state interaction between the final hadrons is neglected. In this approximation, which is known as the factorization approximation, the amplitudes for the nonleptonic two-body decays can be expressed in terms of amplitudes for simpler processes. The decay of the heavy quark is described by the amplitude occurring in the corresponding semileptonic decay and the hadronization of the W-boson into meson M is described by the amplitude of the decay of meson M into leptons. Thus, except for the phenomenological constants a1,  a2, all the dynamical information necessary to describe the decay Hi  →  HfM is contained in the decay constant of meson M and in the matrix elements < Hf|Jμ|Hi  >   for the decay of hadron Hi into a lepton pair and hadron Hf. This is immediately seen from the graph for class I decays, but it can also be made plausible for class II decays, which implies it for class III decays. Much work is done on the problem, to what extent factorization is a good assumption. When meson M is fast, as is the case when it is a light meson e.g. a pion, it leaves the interaction range before it has time to interact. This effect is well known from discussions of the formation zone in scattering on nuclei. A more formal discussion has been given by Dugan and Grinstein37). Data have been analysed e.g. by Rieckert38) and this prediction has been confirmed. On the other hand, a counterexample against the factorization assumption is provided by the result9)

[formula]

The quarks are, in the initial state [formula] and in the final state [formula]. According to the factorization approximation to the BSW model such a process is impossible, because at least one of the initial quarks (the spectator) must be present also in the final state, which here is not the case. On the other hand, taking into account the final state interaction, one finds the two step process

[formula]

The first step can be described by the BSW model with the factorization aproximation, while the second is an allowed strong interaction process. Thus, the problem is not so much to prove or disprove factorization, but to find its validity range and practical methods to go beyond it.

Nonleptonic inclusive decays have been recently reviewed by Bigi16). Let us mention two problems, which may indicate difficulties for the theory, though in both cases certainly much work is needed before a firm conclusion can be drawn.

There were worries that the semileptonic branching ratios for B mesons come out from theoretical calculations significantly larger than measured in experiment39). This could be traced to an underestimate of the nonleptonic decay widths in theoretical calculations. Careful evaluations of the decay probabilities [formula] taking into account the finite mass of the c quarks and estimating the uncertainties of the calculation40),41),42) have shown that one can easily reproduce the experimental result at the expense, however, of predicting a larger decay width for the decays [formula] than reported by experimentalists. The discrepancy is below 3 standard deviations, but the problem certainly deserves further study.

Another problem concerns the life time of the baryon Λb. Using a suitable heavy quark expansion one finds16) that the spectator model should be a very good approximation for the inclusive decays of hadrons containing b-quarks. According to the spectator model, the life times of all these hadrons should be equal to each other and equal to the life time of the b-quark. An estimate of the corrections to this approximation has been made16) and one finds in particular

[formula]

while the experimental value for this ratio19) is 0.72  ±  0.06. Thus, once more the dicrepancy is of about three standard deviations.

As time goes on, one sees many such difficulties of the standard model appearing and disappearing, but since most physicists hope for new physics to make itself finally visible, it is interesting to know, which points now seem to deserve particular attention.