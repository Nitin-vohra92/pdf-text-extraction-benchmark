Lattice Field Theory

Introduction

Overview

Progress in lattice QCD tends to be incremental. Through improved formulations and increased computer power, we have been steadily gaining control over all the systematic approximations inherent in numerical simulations. Rarely, progress is revolutionary. Happily, we are in the midst of such a major leap forward, through recent demonstrations that lattice formulations, which preserve exact chiral symmetry, work in practice. Combined with increasingly cost-effective computing technology, a period of accelerated progress, impacting directly on phenomenologically important QCD calculations, can be foreseen over the next five years.

In this review, I describe recent progress demonstrating that chirally symmetric formulations are feasible and results for phenomenologically relevant quantities. I omit results for QCD thermodynamics and, with one exception, for non-QCD theories. Although there has been a lot of work on the challenging problem of the confinement mechanism, the understanding achieved so far is partial and the picture is still too confusing to do it justice in a review such as this. For results in these areas, other exploratory phenomenological applications and most of the technical details, I refer you to the latest in the annual series of lattice conference proceedings[\cite=lat99].

Objectives of Lattice Field Theory

The primary objective of lattice field theory is to determine the parameters of the Standard Model and, thereby, to seek signals of new physics. Due to confinement, the quark sector is not directly accessible by experiment and numerical simulations of QCD are needed to provide the missing link. In principle, lattice QCD offers model-independent computations of hadronic masses and matrix elements. Ultimately, it should test QCD as the theory of strong interactions and provide an understanding of confinement. The name of the game is the control of systematics, particularly to quantify dynamical quark effects and reliably simulate at, or extrapolate to the physical values of the light and heavy quark masses.

The second objective is to determine the phase structure of hadronic matter. Both the location and the order of the line of phase transitions, separating the confined, hadronic phase from the deconfined, quark-gluon plasma phase, in the temperature (T), chemical potential (μ) plane, are sensitive to the flavour content. Since Tc is close to the strange quark mass, it is particularly important to simulate the strange quark accurately, and this awaits simulations with more realistic dynamical light flavours. Also, the μ  ≠  0 plane is not accessible to simulations with current algorithms, because the action is complex and Monte Carlo importance sampling fails. Significant progress has been made in QCD thermodynamics at μ = 0, and new approaches using anisotropic lattices should yield spectroscopic results to help better understand leptonic decays of vector mesons, strangeness production and J / ψ suppression.

Finally, looking beyond QCD, we aim to develop simulations into a general purpose non-perturbative tool. Recent progress in formulating lattice chiral symmetry has reawakened hopes of being able to simulate chiral and SUSY theories.

Theoretical Progress

Lattice Chiral Symmetry

Major progress has been achieved over the past eight years following the rediscovery of the Ginsparg-Wilson (GW) relation[\cite=gw_relation]. This states that if the lattice Dirac operator, D, is chosen to satisfy

[formula]

where a is the lattice spacing, then the theory possesses an exact chiral symmetry. Such a formulation has the great virtue that it separates the chiral and continuum limits. It also forbids O(a) terms, so that the resulting fermion actions are improved. The GW relation languished for many years, because there was no practical implementation. The breakthrough was the discovery of three constructions: the overlap[\cite=overlap], domain wall[\cite=domain_wall] and perfect action[\cite=perfect_action]. As a consequence, lattice simulations with exact chiral symmetry are now a reality (see Kikukawa's talk[\cite=kikukawa] and other reviews[\cite=gw_reviews]).

For vector theories like QCD, this means that we can maintain a global chiral symmetry at non-zero lattice spacing, so that simulations should be able to approach the physical u and d quark masses in a controlled fashion. More importantly, the mixing of operators of different chiralities, which has plagued kaon mixing and decay calculations, is avoided. Formally, abelian and non-abelian chiral gauge theories have been constructed on the lattice[\cite=chiral_gauge_theories], so the Standard Model can now be defined non-perturbatively. Somewhat more speculatively, for SUSY theories without scalar fields, lattice chiral symmetry forbids relevant SUSY-violating terms in the action, and so offers the prospect of lattice simulations without fine tuning[\cite=lat0002030].

Overlap Quarks

In the overlap formulation, the lattice Dirac operator has the form

[formula]

where μ is the bare quark mass, and HW is the (hermitian) Wilson-Dirac matrix with a large negative mass. Dov(0) obeys the GW relation (Eq. ([\ref=eq:gw_relation])). Numerically, the challenge is to compute accurately the sign of the large sparse matrix, HW, eg using a rational approximation[\cite=lat0001013]:

[formula]

This approximation breaks down for small eigenvalues, so it is necessary to project out the lowest eigenvectors and treat their signs exactly.

The resulting implementation has been tested for quenched QCD[\cite=lat0006004]. The results in Figure [\ref=fig:overlap_quark_mass] show that, as expected, there is no additive quark mass renormalisation, ie

[formula]

The quenched light hadron spectrum has also been computed[\cite=lat0006004], albeit in a small volume, and evidence is found that the mass ratios roughly scale at fixed quark mass, showing that it is now possible to do phenomenologically interesting calculations in these new chirally symmetric formulations.

Unfortunately, even for quenched QCD, the computational cost of simulating overlap quarks is comparable to that of dynamical simulations[\cite=lat0001008], due to the extra work involved in computing the sign function (Eq. ([\ref=eq:approx_sgn])). Little is known yet about the cost of simulating dynamical quarks this way.

The approximation of the sign function in Eq. ([\ref=eq:approx_sgn]) may be replaced by local interations amongst a set of 2N auxiliary fields:

[formula]

This shows that the overlap formulation may be thought of as five-dimensional, in which the fifth dimension is like flavour[\cite=lat0005004].

Domain Wall Quarks

Here the fermions live in five dimensions and are coupled to a mass defect located on a four-dimensional hyperplane. On a finite lattice, with Ls sites in the fifth dimension, the Dirac operator may be written

[formula]

Here T is the transfer matrix in the fifth dimension and μ is again the bare mass. The strong similarity with the overlap, Eq. ([\ref=eq:overlap]), is evident. In fact, the two formulations are identical in the limits of Ls  →    ∞   and zero lattice spacing in the fifth direction.

For finite Ls, the chiral modes of opposite chirality are trapped on the four-dimensional domain walls at each end of the fifth dimension. Chiral symmetry is broken, but the breaking is exponentially suppressed by the size of the fifth dimension. Several groups have tested this in quenched QCD[\cite=lat9909117] [\cite=lat0007014]. It is found that the pion mass does not always vanish with quark mass, in the limit Ls  →    ∞  , due to near unit eigenvalues of T, which allow unsuppressed interactions between the LH and RH fermions. This is a strong-coupling effect, which goes away for weak enough coupling, or using a renormalisation-group improved action[\cite=lat0007014]. The problem may be controlled numerically by projecting out the low eigenvectors and taking their contribution with infinite Ls[\cite=lat0005002].

Thus, there is now a good understanding of how to achieve a close approximation to exact chiral symmetry in lattice QCD simulations. Although numerically relatively expensive, it is early days and more efficient algorithms may yet be found. Already, the extra degree of control given by exact symmetry probably outweighs the cost for matrix element calculations, and we can anticipate rapid progress over the next few years.

String Breaking

For the remainder of this review, I will describe results obtained using traditional formulations of lattice QCD, focusing on the effects of dynamical quarks in order to try to quantify quenching errors for as many quantities as possible.

Perhaps, the first effect of dynamical quarks we might hope to see is string breaking. This flattening of the potential between two static charges, as they are separated, has been observed as level crossing in the confinement phase of the SU(2) Higgs model[\cite=lat9909164]. At a particular separation, the string state becomes degenerate with the state of two static-light "mesons", as shown in Figure [\ref=fig:string_breaking].

Demonstrating string breaking in QCD at zero temperature is proving to be much more challenging than expected. As yet there is no completely convincing signal. This is because there is poor overlap between the string states used to compute the static quark potential and the broken-string state, comprising two static-light mesons. Including the latter is computationally very costly, because it requires quark propagators at all sites.

However, the mixing matrix element has been computed for two dynamical flavours and found to be non-zero[\cite=lat0001015]. Using only string states, there are hints of a flattening potential, just as the signal becomes swamped by noise[\cite=lat9909118], but a recent high-statistics calculation provides pretty conclusive evidence that such attempts are doomed[\cite=lat0005018] and the two-meson state must be included (as was done for the Higgs model[\cite=lat9909164]). The situation is frustrating, but it is only a matter of time before sufficient computing resources are brought to bear.

Hadron Spectrum

The important result that the quenched light hadron spectrum disagrees with experiment was finally established by the CP-PACS Collaboration[\cite=lat9904012] in 1998 and announced at ICHEP98. This had proved difficult, requiring high statistics, because the deviation is less than 10%. This small deviation is good news for phenomenological applications of lattice QCD, which still rely heavily on the quenched approximation. The main symptom of quenching is that it is not possible consistently to define the strange quark mass - the two spectra obtained from using the K and the φ meson to determine the strange quark mass disagree.

Since 1998, the focus has been on simulations with two degenerate dynamical flavours, which are identified with the u and d quarks. The strange and higher-mass quarks are still treated in the quenched approximation. At this conference, CP-PACS reported that the resulting strange meson spectrum is much closer to experiment[\cite=kanaya], as shown in Figure [\ref=fig:strange_spectrum].

The glueball spectrum has only been computed in quenched QCD and the results[\cite=quenched_glueballs] reported at ICHEP98 remain state of the art. This calculation was hard because of strong scaling violations. Better scaling has been reported recently using the fixed-point action[\cite=lat0007007]. Mixing with quark states should be very important. A first attempt to compute the mixing of the lightest scalar glueball with the lightest scalar quarkonium states has concluded that the f0(1710) is 74% glueball, whereas the f0(1500) is 98% quarkonium[\cite=lat9910008].

Now that two-flavour simulations are possible, it is interesting to try to compute the flavour-singlet meson masses. A first attempt has produced a result that the η, η' mixing angle is around [formula] in the [formula] basis, but, despite sophisticated variance reduction techniques, at least ten-times better statistics is needed[\cite=lat0006020].

Quark Masses

Quark masses are encoded within hadron masses. They are scale and renormalisation-scheme dependent. To be useful for phenomenology, it is necessary to compute the scale evolution of the quark mass, from the lattice scale at which it is determined, to a suitably high scale where it can be matched to a perturbative scheme.

The usual approach is to define an intermediate scheme, such as the Schrödinger Functional (SF), in which the scale dependence can be computed non-perturbatively (see Heitger's talk[\cite=heitger]). Once this scale dependence is known to a high enough energy, it can be continued to infinite energy, using the perturbative renormalisation group, to define the ratio of the lattice mass to the renormalisation-group (RG) invariant mass, M[\cite=alpha_quark_mass_scaling]. Thus the lattice quark mass fixes M and, from it, perturbation theory may be used to determine the quark mass in any chosen scheme.

Figure [\ref=fig:quenched_strange_mass] shows the sum of the RG-invariant average u and d, and s quark masses, computed in this way for quenched QCD (with the K mass as input)[\cite=lat9906013]. The resulting estimate for the strange quark mass is

[formula]

At this conference, CP-PACS announced results for light quark masses in two-flavour QCD[\cite=kanaya], updating earlier results[\cite=lat0004010]. They use an improved action, with a fixed lattice size of (2.5 ~ fm)3, and extrapolate downwards in the sea-quark mass, from masses corresponding to pseudoscalar-to-vector meson mass ratios above 0.6, to the average u and d quark mass. Currently, non-perturbative matching is not available for two-flavour QCD, so CP-PACS uses mean-field improved 1-loop matching. The results are shown in Figure [\ref=fig:ud_quark_mass], for three different definitions of quark mass at non-zero lattice spacing, and compared with quenched QCD. The different definitions permit consistent continuum extrapolations and the final results are from combined fits with a single limit. They find a big effect from the inclusion of dynamical quarks. The average u and d quark mass is reduced by roughly 25%:

[formula]

Treating the strange quark in the quenched approximation, CP-PACS finds that the 20% inconsistency in the strange quark mass in quenched QCD disappears with dynamical u and d quarks (within 10% errors), as can be seen in Figure [\ref=fig:s_quark_mass]. The continuum estimates are

[formula]

Again, the mass is reduced substantially compared to quenched QCD. Such a low strange quark mass suggests a large value of ε' / ε, and raises the interesting question how much lower the strange quark mass would be if it were treated dynamically. The result ms / mud = 26(2)[\cite=kanaya] agrees with the chiral perturbation theory estimate of 24.4(1.5).

The b quark mass, in this world in which only the u and d quarks are dynamical, obtained from the Bs binding energy at leading order in 1 / mb and using NNLO perturbative matching, is[\cite=lat0002007]

[formula]

Heavy-Quark Decays

The calculation of hadronic matrix elements associated with the weak decays of b quarks is the most successful phenomenological application of lattice QCD (see Kronfeld's talk[\cite=kronfeld]).

Leptonic Decays and Mixing

The top-quark CKM matrix elements and the neutral Bq mass difference are related through the hadronic matrix element [formula]:

[formula]

Traditionally, fB and B̂B have been computed separately in lattice QCD. Quenched estimates for fB have stabilised in recent years, but suffer a relatively large irreducible scale uncertainty due to the quenched approximation. Including two dynamical flavours increases fB by around 20%, although statistical errors currently overwhelm systematic effects. Table [\ref=tab:f_B] shows the best estimates of the B decay constants, presented at Lattice 99. The only direct comparison with experiment is for fDs, and here the lattice estimates,

[formula]

are consistent with experiment (eg, ALEPH's result of 285(45) MeV[\cite=golutvin]), although the errors are too large to expose systematic effects.

The combination [formula] may be computed directly in lattice QCD and, to the extent that systematic errors in fB and B̂B are correlated, this may be more reliable than separate determinations of fB and B̂B. A recent non-perturbatively-renormalised result in quenched QCD is[\cite=lat0002025]

[formula]

Systematics should also cancel in ratios, so that quenched results such as[\cite=lat0002025]

[formula]

are probably the most reliable.

Lifetimes

Lifetime calculations are at an exploratory stage. Two groups have computed the Bs lifetime difference, ΔΓBs  /  ΓBs, obtaining 0.047(15)(16)[\cite=ph0006135] and 0.107(26)(14)(17)[\cite=lat0004022], to be compared with the experimental upper bound of 0.31. Although they use quite different lattice techniques, the matrix element calculations are consistent, and the discrepancy in the final results is due to one using the quenched, and the other using the unquenched value of fBs.

The Λb lifetime is a puzzle, because the experimental measurement for the ratio τ(Λb) / τ(B0) = 0.79(5) is significantly different from one, whereas, to leading order in the heavy-quark mass, all b hadrons have the same lifetime. A preliminary lattice calculation, however, does indicate that spectator effects are significant at the 6-10% level[\cite=lat9906031].

Exclusive Semileptonic Decays

The main purpose of computing exclusive semileptonic B decay form factors is to extract model-independent estimates of |Vub| and |Vcb| from experiment. All the results I present were computed in quenched QCD, although dynamical-quark effects are now beginning to be explored[\cite=lat9909076]. They are expected to be around 10%.

During the past two years, the most progress has been in calculating the form factors for [formula], defined by

[formula]

Here, lattice QCD fixes the normalisation of the form factors, unlike heavy-quark effective theory (HQET). However, today's lattice spacings are too large to represent a high-momentum pion accurately. So the kinematic range is restricted to near zero recoil, and model-independent results are only possible for the differential decay rate[\cite=lat9910010] [\cite=lat9911011], as shown in Figure [\ref=fig:B_to_pi_diff_rate]. Differential rates should be measured experimentally soon, at which point, direct comparison with the lattice results will provide a model-independent estimate for |Vub|.

Model-dependent extrapolation is needed to obtain the full kinematic range. A dipole fit to the UKQCD results for f+(q2) is shown in Figure [\ref=fig:B_to_pi_f_+] (a simultaneous pole fit to f0(q2) imposes the constraint f0(0) = f+(0)). This fit gives a total decay rate

[formula]

There is good agreement with the results from other groups and with light-cone sum rules at low q2. The form factor f0 provides an important consistency check on the lattice results through the soft-pion theorem:

[formula]

in the limit of zero pion mass. Whether this is satisfied by current simulations is somewhat controversial[\cite=lat9909100] [\cite=lat9909076], but this will have to be resolved if we are to have full confidence in the lattice results.

There have been no new results for [formula] and [formula] and the present status is described in Lellouch's review[\cite=ph9912353].

The form factors for the heavy-to-heavy decays, [formula], are better suited to lattice calculations, because the recoil is smaller and present-day lattices can cover the full kinematic range. However, HQET is able to determine the normalisation at zero recoil in the heavy-quark symmetry limit. So, lattice QCD is left with the tougher task of quantifying the deviations from the symmetry limit at physical quark masses, needed to extract |Vcb| from experiment, which requires few percent accuracy. The FNAL group has devised a technique for determining these power corrections at zero recoil, from ratios of matrix elements, in which many statistical and systematic errors cancel[\cite=kronfeld] [\cite=ph9906376], obtaining, eg,

[formula]

From the recoil dependence of the form factors, it is possible to extract the Isgur-Wise function, ξ(ω). UKQCD has done this in quenched QCD[\cite=lat9909126], using the [formula] form factor. The resulting estimate of ξ(ω), shown in Figure [\ref=fig:isgur_wise], is independent of the heavy-quark masses, for masses around that of the charm quark, and is insensitive to the lattice spacings used. Thus, it is demonstrably the Isgur-Wise function for quenched QCD.

Kaon Physics

Mixing

The B-parameter for neutral kaon mixing,

[formula]

is probably the best determined weak matrix element in quenched QCD. The most reliable result comes from staggered quarks, because the mixing due to chiral symmetry breaking is non-trivial for Wilson quarks. The error in the continuum result[\cite=lat9910032],

[formula]

is mostly from perturbative matching, and should be reduced when non-perturbative renormalisation becomes available. Dynamical quark effects raise BK by around 5% at fixed lattice spacing, but it is not known yet how this affects the continuum result.

Non-Leptonic Decays

Although there are no new results, there is renewed optimism for lattice calculations of K  →  ππ decays (see Testa's talk[\cite=testa]), because we now have more sophisticated techniques, which may afford control over the severe cancellations between the matrix elements concerned. The new chirally symmetric lattice formulations should avoid the mixing between operators of different chiralities, and the large measured value for ε' / ε reassures us that a signal should exist.

The fundamental problem for lattice QCD is that, according to the Maiani-Testa no-go theorem, there is no general method for dealing with multi-hadron final states in Euclidean space. The traditional approach around this is to use chiral perturbation theory to relate those matrix elements which can be computed, such as K  →   vacuum, π, or ππ at unphysical momenta, to the desired physical matrix elements[\cite=lat0006029]. A new proposal is to tune the lattice volume so that one of the (discrete) energy levels of the two pions equals the K mass, and then relate the transition matrix element to the decay rate in infinite volume[\cite=lat0003023].

The impending flood of experimental data for non-leptonic B decays presents a formidable challenge to lattice QCD. Chiral perturbation theory no longer helps. Perhaps, the B  →  ππ factorisation[\cite=ph0006124], proved for mb  ≫  ΛQCD, can be exploited in some way?

Structure Functions

Lattice QCD can provide the normalisation for parton densities. Ultimately, this should test QCD and the validity of perturbation theory. It enables us to disentangle power corrections and, where experimental information is scarce, such as for the gluon distribution for x > 0.4, lattice QCD can help phenomenology. Dynamical quark effects are presumably crucial. Although results so far are for quenched QCD, this will soon change (see Jansen's talk[\cite=jansen]).

The traditional approach uses the operator product expansion to relate moments of structure functions to hadronic matrix elements of local operators:

[formula]

The Wilson coefficients, C(2)n, are determined in perturbation theory and the hadronic matrix elements, A(2)n are determined on the lattice. Renormalisation is the major source of systematic error, since the product C(μ)A(μ) must be independent of the scale μ. This is achieved using a non-perturbative intermediate scheme, in the same way as for quark masses:

[formula]

The INT=SF scheme[\cite=lat9901016] uses a step scaling function to relate the matrix element, renormalised at the lattice scale, to a high scale where perturbation theory can be used to determine the RG-invariant matrix element in the limit μ  →    ∞  . This, in turn, may be related via perturbation theory to [formula]. At this conference, Jansen[\cite=jansen] reported that the average momentum of partons in the pion in quenched QCD, computed in this way, is

[formula]

to be compared with the experimental result of 0.23(2), and confirming early lattice results that the quenched estimate is larger than experiment.

A quite different method involves computing the current-current matrix element, 〈h|JμJν|h〉, which appears in the cross-section, directly on the lattice[\cite=ph9906320]. The Wilson coefficients are determined non-perturbatively from matrix elements between quark states, by inverting

[formula]

thereby avoiding mixing and renormalon ambiguities. Using 62 operators and 70 momenta to extract the C's, and reconstructing 〈N|JμJν|N〉 from them and nucleon matrix elements, QCDSF obtained the lowest non-trivial moment of the unpolarised structure function[\cite=ph9906320], shown in Figure [\ref=fig:m_2]. This indicates large power corrections and strong mixing between twist-2 and twist-4 operators.

Machines and Prospects

Progress in lattice QCD, and particularly its application to phenomenology, continues to be critically dependent on increasing computer power. Three machines dominate lattice QCD today. Historically, the first was CP-PACS's 300 Gflops (sustained) Hitachi SR2201. This has been operating since 1996 and cost approximately $70/Mflops. The second was the QCDSP, custom built using 32-bit digital signal processors, which has been sustaining 120 Gflops and 180 Gflops at Columbia and Brookhaven, respectively, since 1998. Its cost was around $10/Mflops. This year, APE's latest fully-customised 32-bit machine, called APEmille, began operation at Pisa, Rome and Zeuthen, sustaining around 70 Gflops in the largest configuration so far (this will double by the end of 2000). Its cost is $5/Mflops. These machines show an encouraging trend towards greater cost-effectiveness.

In December 1999, an ECFA Working Panel concluded[\cite=ecfa] that "the future research programme using lattice simulations is a very rich one, investigating problems of central importance to the development of our understanding of particle physics". It also concluded that "to remain competitive, the community will require access to a number of 10 Tflops machines by 2003" and "it is unlikely to be able to procure a 10 Tflops machine commercially at a reasonable price by 2003".

Two new projects are targeting 10 Tflops 64-bit machines, with a price/performance of $1/Mflops, by 2003. The QCDOC project, involving Columbia and UKQCD will employ PowerPC nodes in a 4-dimensional mesh interconnect. The apeNEXT project, involving INFN and DESY, will continue the APE architecture of custom nodes in a 3-dimensional mesh. Two US projects, Cornell-Fermilab-MILC and JLAB-MIT, are exploring Alpha and Pentium clusters using commodity (Myrinet) interconnect, in the hope that these commodity components can be made to scale to many thousands of processors and that the intense market competition will drive the price very low. These developments, together with the highly parallel algorithms employed for QCD, suggest there will be no obstacle to multi-teraflops machines for QCD except money!

However, we still do not understand the scaling of our algorithms well enough to predict how much computer power will be needed. Our best estimates are that to achieve comparable precision to quenched QCD, in simulations with two dynamical flavours with masses around 15 MeV, will require between 15 and 150 Tflops years[\cite=lat9911016]. However, we know nothing about simulations with light enough quarks for ρ  →  ππ! We still have a great deal to learn.

In conclusion, the range of phenomenological applications of lattice QCD continues to expand. Key developments have been improved actions (reported at ICHEP98) and non-perturbative renormalisation, both of which have considerably increased our confidence in matrix element calculations. Lattice QCD continues to drive the development of cost-effective high-performance computing technology and there is no technological limit in sight. The primary objective will be to extend the range of quark masses which may be simulated reliably, and it is hard to see how we will get away with less than 100 Tflops machines. Finally, in the discovery of lattice chiral symmetry, we are witnessing the "Second Lattice Field Theory Revolution", and this will vastly increase the reach of ab initio computer simulations.