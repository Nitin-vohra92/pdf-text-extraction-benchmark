Efficient Distributed Estimation of Inverse Covariance Matrices

Introduction

The collection of copious and meticulous amounts of information has led to the modern phenomena of datasets being both high-dimensional and very large in sample size. These massive datasets are distributed over multiple machines due to size limitations or because data is collected and stored independently. Even in the case when a single machine is large enough, there are efficiency, security, and privacy concerns in aggregating all the data onto one machine. Bandwidth restrictions can make impossible or inefficient to send large amounts of data and the more communication in the system, the more vulnerable it is to attacks. In addition, the raw dataset may contain sensitive information in the individual samples such as in medical or financial records. Thus, it is advantageous, if not necessary, for each machine to be able to calculate compact estimates that can be efficiently communicated and preserve the confidentiality of the samples.

Estimation of the inverse covariance matrix is used in the analysis of different types of data, such as gene expression or brain imaging. In particular, when samples are Gaussian, the non-zero entries of the inverse covariance correspond to the edges of a Gaussian Markov random field. Thus, accurately estimating the non-zero pattern of the inverse covariance matrix is crucial.

In this paper, we develop a method for estimating a sparse inverse covariance matrix when the samples of the data are distributed among machines. Our method is efficient in communication, in the sense that only a single round of communication between each machine and a central hub is sufficient, and the bandwidth required is small compared to the size of the matrix estimated.

Related Work

High-dimensional estimation of inverse covariance matrices is usually addressed by [formula] penalized convex optimization [\cite=banerjee2008model] [\cite=friedman2008sparse]. In a distributed setting, general frameworks for convex optimization involve multiple rounds of communication between all machines [\cite=Boyd:2011:DOS:2185815.2185816] [\cite=duchi2012dual], which can be expensive. For some [formula] penalization problems, the communication cost can be reduced to a single round of communication [\cite=Zhang:2013:CAS:2567709.2567769] [\cite=lee2015] by reducing the bias with bootstrapped or debiased estimators [\cite=2013Javanmard]. Our work follows a similar approach as [\cite=lee2015] for penalized linear regression; however, we introduce efficiency not only in the amount of communication, but also in the size of the communication channel. Related work on inverse covariance estimation has focused on the case where variables are distributed and the structure of the associated graphical model is known [\cite=meng2014marginal]. Here, our samples are distributed, and we must estimate the structure of the matrix and the value of its entries.

Outline

The rest of this paper is organized as follows: Section 2 presents a communication efficient method for inverse covariance estimation in a distributed setting. Section 3 studies the error rates of the estimator and shows that model selection consistency is possible. Section 4 studies the practical performance and compares our method with other distributed and non-distributed approaches. Proofs of the theoretical results are included in the appendix.

Distributed Inverse covariance estimation by debiasing and thresholding

We work in a setting where samples are distributed among M different machines. Each observation is a vector of size p coming from a distribution with covariance Σ and inverse covariance Θ. Let s be the number of non-zero entries in Θ and d the maximum number of non-zeros per row. Let S denote the set of non-zero entries of Θ and Sc the set of zeros. We assume that the data is split equally over all machines, where each machine has n observations, so we denote [formula] as the data matrix on machine m. We are interested in estimating Θ.

In a distributed setting, it is desirable to have a single round of communication between each machine and the central hub. Moreover, bandwidth or storage capacity often limits the amount of data that can be shared with the central hub, forcing the data to be distributed. Thus, our approach is based on constructing sparse estimators on each machine that when aggregated in the central hub, provide a good estimation for Θ.

In the high dimensional setting, where n  ≪  p, a common approach to obtain a sparse estimator of Θ is by minimizing the [formula]-penalized log-determinant Bregman divergence. Thus on each machine, this estimator, known as graphical lasso, is defined as

[formula]

where [formula] denotes the [formula] norm of the off-diagonal entries of a matrix, and [formula] is the cone of positive definite matrices of size p. For a single machine, this estimator has been studied and shown to be asymptotically consistent in mean squared error with rate [formula] [\cite=rothman2008]. Moreover, the set of non-zero entries of [formula] coincides with S when [formula] and under certain conditions [\cite=ravikumar2011].

A naive approach for distributed estimation would be to average the estimated m from each machine. However, this estimator is biased due to the [formula] penalty, and averaging only improves the variance, not the bias. We adopt a similar approach as [\cite=lee2015] did for lasso regression by trading-off the bias for variance.

The debiased graphical lasso estimator was proposed in order to construct confidence intervals for the entries of Θ [\cite=jankova2015]. The idea of this estimator is to invert the Karush-Kuhn-Tucker (KKT) conditions of the optimization problem in equation [\eqref=l1-opt] in order to get a debiased estimator defined as

[formula]

The debiased graphical lasso has the appealing property that each entry of the matrix is asymptotically normal distributed. It is shown that dm can also be written as

[formula]

where [formula] accounts for the bias, and the second term in the equation is asymptotically normal [\cite=jankova2015]. The bias of these estimators is of smaller order than the bias of the graphical lasso, and the variance is reduced when we average these estimators in the central hub to get an overall estimator [formula]. When the data is not distributed into too many machines, the averaged estimator can get similar error rates in [formula] norm as the graphical lasso performed on all the data (see Lemma [\ref=lemma:avgglasso]).

The debiased graphical lasso estimator is not sparse. This fact is problematic in a distributed setting, since it will require the transfer of p2 entries, which might be larger than the data on each machine. Under the sparsity assumption on Θ, we are actually only interested in the value of s + p entries. Hence, on each machine we select the most significant coefficients of m and send them to the central hub. The sparse estimator is defined as where σ̂ij is an estimator for [formula] and [formula] is the indicator of the event E. For gaussian random vectors, a good estimator for this quantity is σ̂2ij  =  iijj  +  2ij (Lemma 2 of [\cite=jankova2015]). In order to achieve correct estimation (in the central hub) of the support of Θ, it is optimal to send as many entries as possible. So we let the threshold parameter ρ be a function of B, the bandwidth of the communication channel from each machine to the central hub, and we set ρ as the smallest threshold that still fits in the channel. In Algorithm [\ref=alg_machine], we summarize the estimation procedure on each machine.

The average debiased graphical lasso estimator is also not sparse, which might be unpractical, in particular because estimating the set of non-zeros can be more important than the values of the entries themselves. However, if the averaged estimator is thresholded at a certain level [formula], correct model selection on the non-zero entries of the matrix is possible (see Theorem [\ref=theorem:rates]). Each entry again requires an estimator of σ2ij. For normally distributed data, we use iijj  +  2ij, where [formula]. Algorithm [\ref=alg_hub] shows the complete procedure in the central hub.

Theoretical results

In this section, we derive bounds for the estimation error of our method. We show that the error of our distributed estimator has the same rate as the non-distributed graphical lasso when the number of machines increases slower than [formula]. Moreover, we show that a bandwidth of size O(p2 - c), with c an absolute constant, is enough to correctly identify the set of non-zero entries of Θ.

The following assumptions are necessary in order for the graphical lasso and debiased graphical lasso to have a good estimation performance [\cite=ravikumar2011] [\cite=jankova2015], and we require them for our theoretical results.

There exists some α∈(0,1] such that [formula] where [formula] is the Hessian of ([\ref=l1-opt]).

There exists a constant L  <    ∞   such that 1 / L  ≤  Λmin(Σ)  ≤  Λmax(Σ)  ≤  L where Λmin(Σ) and Λmax(Σ) are the minimum and maximum eigenvalues of Σ.

The samples of the data are subgaussian random variables [formula], with [formula], [formula] and subgaussian norm [formula].

The quantities [formula] and [formula] are bounded, where [formula] is the opertor norm of a matrix.

In the literature, it is common to allow the error rates to depend on the bounding constants from the previous assumptions. Here, in order to keep the results simple, we state the error rates as a function of the dimensionality (n,p,M), sparsity (s, d), and smallest entries of Θ, while keeping the other quantities bounded by absolute constants.

The work from [\cite=jankova2015] studies the error rate of debiased graphical lasso. This result can be extended to the average of multiple debiased estimators as follows.

Suppose that assumptions [\ref=a1] through [\ref=a4] hold, M < p and define [formula] in equation [\eqref=l1-opt]. Then, the averaged debiased graphical lasso estimator satisfies

The previous lemma splits the error rate of the distributed estimator into two parts, which correspond to the variance and the bias. The variance vanishes as the number of machines increases, but the bias remains. However, when [formula], the variance becomes dominant and the error is [formula]. This is the same error rate as the graphical lasso when the data is not distributed [\cite=rothman2008]. Previous work shows similar results for distributed [formula] penalized regression [\cite=lee2015]. Our method expands the framework to the graphical lasso estimator. Additionally, in the next theorem we show that a bandwith of size [formula] is enough to select the correct set of entries and a similar rate for the mean squared error as the graphical lasso on the full data.

Suppose that [\ref=a1] through [\ref=a4] hold and M < p. Define the tuning parameters of the algorithms as [formula] and [formula]. If [formula], then there exists a constant c∈(0,1] such that the following results hold for a bandwidth [formula].

Algorithm 2 recovers the correct set of non-zero entries of Θ with high probability, that is,

[formula]

The mean squared error of the estimator given by Algorithm 2 satisfies

[formula]

Simulation results

To evaluate the performance of our distributed estimator, we conduct a simulation study. We study the effect of varying the total sample size by changing the number of machines in the distributed system. We fix the number of variables to p = 1000 and the sample size on each machine to n = 100. Samples were generated from a normal distribution N(0p,Σ) such that Θ = (Σ)- 1 has the form Θi,i = 1 and Θi,i + 1  =  Θi,i - 1 = 0.4 for [formula]. Thus, the associated Gaussian graph is a chain. All of our simulation results are calculated over 100 different trials.

To solve the problem [\eqref=l1-opt], we use GLasso [\cite=friedman2008sparse] with the R package huge [\cite=zhao2012huge]. We set the tuning parameters λ and τ according to the rates in Theorem [\ref=theorem:rates], so for each machine [formula] and [formula]. The bandwidth is set to B = 10p, so only 1% of the entries of dm are sent to the central hub.

We compare the performance of our distributed estimator (Distributed) with the following estimators.

(Naive) An estimator based on averaging the graphical lasso estimators from each machine [formula].

(Full) An estimator based on the full non-distributed data given by the graphical lasso [formula].

(Full Debiased) Since debiasing decreases the bias of the estimation, we also compare with a thresholded debiased graphical lasso estimator on the full data [formula].

In Figure [\ref=fig:errorplots], the errors of the different estimators are shown. In general, a better estimator can be obtained using the full data, which is expected. However, when the number of machines is small, the performance of our distributed estimator is comparable to the estimators on the full data, both in mean squared error and [formula] norm. In mean squared error, when M is small, the debiased estimators perform better because they trade off bias for variance. Thus, Debiased Full performs the best and Distributed has similar performance. Naive performs poorly under this norm for all M. In [formula] norm, the performance of Full and Full Debiased is exactly the same because the largest error is from the entries in the diagonal, which are equal since the diagonal is not penalized. The error of Distributed is approximately the same for a wide range of values of M. However, in M = 2, the performance of Distributed is affected by the bandwidth since missing edges in any machine have a larger impact on the error. But as the number of machines increases, the estimation of Distributed becomes insensitive to the bandwidth.

To evaluate the robustness of the method against the selection of the tuning parameters λ and τ, we measure false positive rate (defined as percentage of zeros of Θ identified as edges by the estimator) and false negative rate (defined as percentage of edges missed by the estimator). We use the same settings as the previous scenario, but the number of machines is fixed to M = 10. The values of the tuning parameters for our method vary proportionally to [formula] and [formula] with β between 0.2 and 2. The bandwidth is still fixed at 1% of the entries. We observe that for a wide range of parameters we recover the correct set of non-zeros.

Discussion

We have proposed a method for estimating sparse inverse covariance matrices when the samples of a dataset are distributed over different machines. Our method agrees with other results for efficient distributed estimation in high-dimensional settings, and we also introduced efficiency in the bandwidth size. Asymptotically, the performance of our estimator is analogous to estimators with samples that are not distributed. Our simulation results are consistent with the theoretical rates and also show that we perform significantly better than a naive approach to estimation in a distributed setting.

Appendix

Using the decomposition given in equation [\eqref=loss], we have where [formula] is the covariance matrix using the full data. Thus, the first term can be bounded as [formula] (equation (11) of [\cite=jankova2015]) and the second term can be bounded as when M  <  p and [formula] [\cite=ravikumar2011].

Suppose assumptions [\ref=a1] through [\ref=a4] hold. Let γ be a constant in

[formula]

By the asymptotic normality of the debiased graphical lasso entries [\cite=jankova2015], Dij(0) is asymptotically distributed as N(Θij,σ2ij / (nM)). Therefore,

[formula]

where Z is a standard normal random variable. Using a similar argument, the second event can be bounded as

[formula]

Using tails of a normal distribution, the results follow.

Part 1. In order to recover the correct set of non-zero entries, the event [formula] must hold. Note that if in Algorithm 1, the threshold given by the bandwidth contains all non-zero entries, then Dij(ρ) = Dij(0), for (i,j)∈S. Moreover, since [formula] for (i,j)∈SC and all h > 0, then, by conditioning on the event [formula], it holds that

Using the conditions of Theorem [\ref=theorem:rates], in particular the size of [formula], and by Lemma [\ref=lemma:MSprob], it holds that the first term of the product is [formula]. Similarly, using Lemma [\ref=lemma:MSprob] again, we calculate the probability of correct recovery in all machines

[formula]

which is [formula] when M < p, and c1 > 0 is an absolute constant. Thus, correct model selection holds with high probability.

Part 2. Using the equivalence between [formula] and Frobenius norm together with the correct recovery result of part 1, we obtain equation [\ref=mse] as a consequence of multiplying the result of Lemma [\ref=lemma:avgglasso] by p + s.

Acknowledgments

The research in this paper was partially supported by the Consortium for Verification Technology under Department of Energy National Nuclear Security Administration award number DE-NA0002534.