.tex

Robust designs for experiments with blocks

Rena K. Mann, Roderick Edwards and Julie Zhou

Department of Mathematics and Statistics

University of Victoria, Victoria, BC, Canada V8W 2Y2

Key words and phrases: Autocorrelation, block design, covariance neighbourhood, D-optimal design, generalized least squares estimator, linear regression, minimax design, spatial correlation.

MSC 2010: 62K10, 62K05.

ABSTRACT

For experiments running in field plots or over time, the observations are often correlated due to spatial or serial correlation, which leads to correlated errors in a linear model analyzing the treatment means. Without knowing the exact correlation matrix of the errors, it is not possible to compute the generalized least squares estimator for the treatment means and use it to construct optimal designs for the experiments. In this paper we propose to use neighbourhoods to model the covariance matrix of the errors, and apply a modified generalized least squares estimator to construct robust designs for experiments with blocks. A minimax design criterion is investigated, and a simulated annealing algorithm is developed to find robust designs. We have derived several theoretical results, and representative examples are presented.

Introduction

Consider a linear regression model,

[formula]

where the response variable yi is observed at design point [formula] from design space S∈Rq, [formula] is a known function of [formula], parameter vector [formula] belongs to Rp, and the errors εi are uncorrelated and have mean zero and variance σ2. Let [formula], then [formula], where [formula] is the N  ×  N identity matrix. The least squares estimator (LSE) of [formula] is given by [formula], where [formula] is the model matrix and [formula] is the vector of responses. The LSE is the best linear unbiased estimator (BLUE) and its covariance matrix is [formula].

Optimal regression designs, minimizing some scalar functions of Cov() over the choices of [formula], have been investigated extensively in the literature, for example, see Fedorov (2010) and Pukelsheim (1993). Commonly used design criteria include D-optimal and A-optimal criteria. D-optimal and A-optimal designs minimize, respectively, the determinant and the trace of Cov(). However, these optimal designs are very sensitive to the model assumptions. If there are violations to the model assumptions, the optimal designs may produce large variance and/or large bias of [formula]. Therefore robust regression designs have been investigated against various departures from the model.

Robust designs against the misspecification of the response surface function were studied by, among many others, Box and Draper (1959), Huber (1975), and Wiens (1992). Robust designs against the autocorrelation among the errors were explored in, for example, Bickel and Herzberg (1979) and Bickel, Herzberg and Schilling (1981). Developments of robust designs against both departures in the response surface function and in the autocorrelation include Wiens and Zhou (1997, 1999), Shi, Ye and Zhou (2007), and Zhou (2001).

In the design of experiments, due to randomization of experimental runs, the errors in the linear models investigating the effects of factors are usually considered to be uncorrelated. See, for example, Montgomery (2012). However, for some experiments including field experiments, the errors are often correlated, which has been recognized by many researchers, such as Williams (1952), Herzberg (1982), Martin (1982, 1986). In particular, serial correlation over time (or the order of experimental runs) and spatial correlation over field plots are quite common. Since regression models can be built to analyze the factor effects, optimal or robust designs of experiments can be studied similarly as in optimal or robust regression designs. This leads to the research in Wiens and Zhou (2008) and Ou and Zhou (2009) using the minimax approach to find robust designs for field experiments. The designs in Ou and Zhou (2009) are robust against departures from the covariance structure of the errors, while the designs in Wiens and Zhou (2008) are robust against departures from both the covariance structure of the errors and the response function.

In this paper, we extend the work in Wiens and Zhou (2008) and Ou and Zhou (2009) to find robust designs for experiments that need to be performed in several blocks. Because of the blocks, the covariance structure of the errors is more complicated than those in Wiens and Zhou (2008) and Ou and Zhou (2009). The designs we construct here are robust against departures from the covariance structure of the errors. The applications include experiments with serial correlation or spatial correlation.

The rest of the paper is organized as follows. In Section 2, we first discuss the linear model to analyze experiments with blocks and its related regression model, and present the least squares estimator and the generalized least squares estimator. Then we use one experiment with blocks to illustrate the influence of the error correlation on the covariance of the LSE. In Section 3, two neighborhoods of the covariance matrix of the errors are defined. Based on the covariance neighborhoods, a robust design criterion is proposed. In Section 4, robust designs are studied and constructed. A simulated annealing algorithm is developed to compute robust designs, and applications are presented. In addition, several theoretical results are derived. Concluding remarks are in Section 5. All proofs are given in the Appendix.

Linear models and estimators

Linear models

Consider experiments with blocks to compare t treatment means, where blocking is used to eliminate nuisance sources of variability in the experiments. Complete block designs allow one replicate of t treatment runs within each block, while incomplete block designs have less than t runs in each block. In this paper we consider complete block designs and assume the block effects are fixed. Suppose there are b blocks with b  ≥  2, and treatments are numbered as [formula]. The linear effects model can be written as

[formula]

where yij is the ith response in the jth block from the rth treatment ([formula]), μ is the overall mean, τr is the rth treatment effect, βj is the jth block effect, and εij is the random error term. In order to identify the parameters uniquely in the model, the treatment effects and block effects satisfy constraints [formula] and [formula].

A regression model can also be used to analyze the treatment effects. Define the following vectors and matrices to present the regression model:

[formula]

and

[formula]

where matrix [formula] (t  ×  t) is the design matrix for the t treatments in block j, [formula], and matrix [formula] is the model matrix for the block effects. The elements of [formula] are either 0 or 1, and each row has only one 1. If yij received treatment r, then the element at the ith row and rth column of [formula] is 1. Since [formula], parameter [formula]. Therefore we only need b - 1 parameters in vector [formula] for the regression model, and matrices [formula] (t  ×  (b - 1)) for the b blocks are given by

[formula]

Now the regression model for the effects model ([\ref=eqn2.1]) is given by

[formula]

Notice that there is no grand mean (or intercept) in this model, since vector [formula] includes the grand mean component μ in each μr, [formula].

Estimators

In order to estimate [formula] and [formula] efficiently, it is important to know the covariance matrix of the error vector [formula]. Two cases are discussed below.

Define

[formula]

Then model ([\ref=reg1]) becomes, [formula]. The LSE and the generalized least squares estimator (GLSE) are, respectively,

[formula]

From Section 2.1, it is easy to verify that

[formula]

which implies that the regressors for [formula] and [formula] are orthogonal. For block designs, we are mainly interested in estimating and comparing the treatment effects, so we will look at the variances of L and G to construct optimal/robust designs in Sections 3 and 4.

For Case (i), the LSE is the BLUE, and

[formula]

For Case (ii), the GLSE is the BLUE, and

[formula]

where matrix [formula] is the submatrix of [formula], consisting of the first t rows and the first t columns.

An example

We use one example of randomized complete block design to illustrate the influence of the error correlation on the covariance of the LSE in ([\ref=CovLSE2]).

There is one example of a randomized complete block design in Montgomery (2012, page 178) to study the effect of three different lubricating oils (treatments) on fuel consumption in diesel truck engines. Five different truck engines are available for the experiment. Since there may be differences among truck engines, a randomized complete block design is used, where the five truck engines are the five blocks. The observed data on fuel consumption are given in Table [\ref=table1].

We use model ([\ref=reg1]) to analyze the treatment means, where [formula], and [formula]. Since we do not know the run order in each block, we just use the standard order in Table {[\ref=table1]. So the design matrices are

[formula]

Using the LSE, we get [formula] and σ̂ = 0.023. If the errors are uncorrelated, then from ([\ref=CovLSE1]) the covariance matrix of L is

[formula]

The run order does not affect this covariance when the errors are uncorrelated. Thus the estimated standard error for i is [formula], for all i = 1,2,3. Inferences can be made for any linear functions of [formula].

If the errors are correlated, then from ([\ref=CovLSE2]) the covariance matrix of L is

[formula]

In this case, the run order affects [formula]. Suppose [formula]. Since the runs in this experiment are conducted over time, it may be reasonable to model [formula] with a nearest neighbour correlation matrix with

[formula]

Define [formula], where d indicates a design. Notice that design matrices [formula] depend on the run order in each block, and each can be obtained by permuting the rows of the matrix in ([\ref=Xj]). Then the covariance matrix in ([\ref=CovLSE3]) is affected by the run order of the three treatments in each block. Consider the three designs in Table [\ref=table2] and ρ = 0.2. It can be easily shown that,

[formula]

and [formula], [formula]. It is clear that the run order affects the [formula] in ([\ref=CovLSE2]). Similarly we can show that the run order also affects the [formula] in ([\ref=CovGLSE2]).

In practice, if we do not have any information on the correlation matrix [formula], the randomized run order should be used in each block. However, if we have some information on the correlation matrix [formula], we can use an "optimal" run order in each block to minimize the [formula] or [formula]. In the next Section, we will propose a robust design criterion to find the "optimal" run order.

Minimax design criterion

For practical applications, we never know the exact covariance of the errors in model ([\ref=reg1]), but we may have some information about the correlation structure. A flexible model for the Cov() is to use a neighborhood of covariance matrices, which is defined in Section 3.1. Since we do not know the [formula] in the GLSE in ([\ref=GLSE1]), we will modify it in Section 3.2 using the information in the neighborhood of Cov(). Based on the neighborhood of Cov() and the modified GLSE, a robust design criterion is given in Section 3.3 to construct the optimal run order in each block.

Neighbourhoods of covariance matrices

Let [formula]. In Mann (2011), two neighbourhoods of [formula] were proposed, which are extensions of the neighbourhoods of covariance matrices in Wiens and Zhou (2008). We briefly describe them below.

Suppose [formula], where [formula] are known correlation matrices. Often [formula] are viewed as our prior knowledge of the error process in model ([\ref=reg1]). Commonly used error processes for field plots include the nearest neighbour (NN), moving average (MA), doubly geometric (DG) and discrete exponential (DE) processes, which are reviewed in detail in Mann (2011). Two options for neighbourhoods of [formula] are defined around [formula] using the following procedure.

where α  ≥  0, and [formula] is either [formula] or [formula]. The matrix ordering is by positive semi-definiteness, i.e., [formula] means that [formula] is positive semi-definite. For the applications in Section 4.2, we take [formula], so [formula] does not depend on j. Thus, for simplicity, we omit the subscript j in [formula] in the following.

where [formula] is either [formula] or [formula]. So the two neighbourhoods are [formula] and [formula].

We can also use matrix norms ||  ·  ||1 or ||  ·  ||2 (Horn and Johnson, 1985, page 291) to define a neighbourhood of [formula]. Let

[formula]

l = 1, ~ 2. However, it is shown in Wiens and Zhou (2008) that [formula] with [formula]. Thus we will only use the neighbourhoods [formula] to define and construct robust designs in this paper.

It is obvious that parameter α controls the size of the neighbourhoods of [formula]. The larger the α is, the bigger the neighbourhood is. It is also clear that [formula] for all α  ≥  0, and [formula] can be viewed as a center of the neighbourhoods.

Modified GLSE

We cannot compute the GLSE in ([\ref=GLSE1]) without knowing matrix [formula]. A modified GLSE (MGLSE) is proposed when [formula] belongs to [formula]. The original idea is from Martin (1986), but it is applied for [formula] in Mann (2011). Define the MGLSE as

[formula]

Then the covariance matrix of M is

[formula]

where [formula] is a t  ×  (t + b - 1) matrix, and [formula] is the true (but unknown) covariance matrix of the errors.

Design criterion

Suppose [formula] is an estimator of [formula], such as the LSE or the MGLSE. Let function [formula] be a measure of the covariance matrix. The commonly used measures L include the determinant and trace. Function gL depends on the estimator [formula], model matrix [formula] and the error covariance matrix [formula]; see ([\ref=CovLSE2]) and ([\ref=COVM]). Since matrix [formula] is fixed in [formula], we write gL depending on [formula] only through [formula].

Since the covariance matrix of [formula] depends on the unknown [formula], we cannot minimize [formula] directly to construct optimal designs. A minimax approach has been used to construct robust designs for various models. See, for example, Huber (1975), Wiens (1992), and Ou and Zhou (2009). The minimax approach will also be applied here to define robust designs.

Define the maximum loss function as

[formula]

Use φA or φD to denote the φL when measure L is the trace or determinant respectively. A minimax (robust) design ξL is defined to be the design that minimizes [formula] over design matrix [formula].

From the definition, the minimax design may depend on the estimator [formula]. For the LSE, from ([\ref=CovLSE2]) and [formula],

[formula]

For the MGLSE, from ([\ref=COVM]),

[formula]

The following theorem gives the maximum loss function φL for the LSE and MGLSE.

For the neighbourhoods [formula] defined in ([\ref=NBR]) and measure L being monotonic according to the ordering of positive definiteness, we have

[formula]

where [formula].

The proof of Theorem 1 is given in the Appendix. The results in Theorem 1 are very useful, and we only need to minimize ([\ref=maxLoss2]) or ([\ref=maxLoss3]) to construct robust designs. In the next section, we will discuss two algorithms to find robust designs, present representative examples, and derive several theoretical results.

Construction of robust designs

Numerical algorithms

Minimizing ([\ref=maxLoss2]) or ([\ref=maxLoss3]) over [formula] is a combinatorial optimization problem. When the number of treatments and the number of blocks are small, a complete search method to find robust designs is feasible. However, when the number of treatments and/or the number of blocks are big, it is too expensive to use a complete search method. In this situation, there are various algorithms available that can be applied to construct robust designs. One of them is a simulated annealing algorithm, which is known in the literature to be effective in searching for optimal and robust designs. For example, see Elliott, Eccleston and Martin (1999), Fang and Wiens (2000), and Wilmut and Zhou (2011).

An annealing algorithm minimizing [formula] includes the following main steps.

The cooling temperature parameter, T, plays an important role in the algorithm, and it has influence on the speed of convergence of the designs. The detailed discussions about setting the initial cooling temperature and how to update it can be found in Fang and Wiens (2000) and the references therein. The acceptance rule is as follows. If l1  ≤  l0, then [formula] is accepted. If l1  >  l0, then [formula] is accepted with a probability exp ( - (l1 - l0) / T).

At each iteration, a new design needs to be generated, and it is usually obtained by modifying the current design with a small change. A good scheme for generating new designs should allow us to access all possible designs for [formula]. Since we can randomly assign the numbers to the t treatments, without loss of generality we fix the allocation of treatments in block 1, and only search for optimal allocations in blocks 2 to b. A new design [formula] is obtained from [formula] by randomly choosing a block number from 2 to b and switching two treatment numbers in the selected block.

There are other modifications that can improve the searching. Two small steps are added in our computation. One is to record the best design, say [formula], with the smallest φL during the iterations. Notice that [formula] is updated at each iteration. At the end, if [formula] has smaller φL value than [formula], then [formula] is considered as an approximate robust design. Another step is to start with the approximate robust design from the annealing algorithm and do an additional steepest descent procedure as in Elliott, Eccleston and Martin (1999). This can be done by running the above annealing algorithm again and only accepting the new design when l1  ≤  l0.

Applications

We consider a general setting for each block in the following examples. Suppose each block contains m  ×  n small plots arranged in a rectangular area as in Table [\ref=table3]. This is common for field experiments, and each small plot receives a treatment. Assume each block has one replicate of t treatments, so t = mn. Let (k, s) indicate the position of a small plot, [formula]. In model ([\ref=reg1]), we define the error vector for the jth block as

[formula]

If it is not a field experiment but the runs in each block are conducted over time, then it can be viewed as a special case with n = 1. Four representative examples are presented next to show the robust designs. In all the examples we set σ2 = 1 to present the loss function values.

Example 1 Construct the robust design for b = 2, ~ t = 7, ~ m = 7, and n = 1. The neighbourhood is [formula] with α = 0.25 and [formula], where [formula] is from the first order NN process with correlation ρ = 0.15, i.e.,

[formula]

Using the MGLSE, we minimize [formula] to get the D-optimal robust design. Notice that the result does not depend on the value of σ2 or α. A complete search method is applied, and the results show that the D-optimal robust design is not unique.

Figure [\ref=7treat_onecol_arrangement] presents one robust design with [formula]. We notice that the two blocks have different treatment allocations. In fact, if two treatments are neighbours in block 1, then they are not neighbours in block 2.

Example 2 Construct the robust design for b = 5, ~ t = 3, ~ m = 3, and n = 1. This experiment is discussed in Section 2.3. The neighbourhood is [formula] with α = 0.2 and [formula], where [formula] (3  ×  3) is from the first order NN process with correlation ρ = 0.20. One D-optimal robust design minimizing [formula] is given in Figure [\ref=Example21], obtained from a complete search method. This design has [formula].

One D-optimal robust design minimizing [formula] is given in Figure [\ref=Example22], which gives [formula]. The D-optimal robust designs are not unique. We can randomly permute the treatment numbers and block numbers. We can also change the orientation of blocks if matrix [formula] is from a weakly stationary error process. This implies that the design in Figure [\ref=Example22] is the same as design d1 in Table [\ref=table2], and the design in Figure [\ref=Example21] is the same as design d2 in Table [\ref=table2]. The D-optimal design based on the LSE puts the same treatment in the middle plot of all the 5 blocks, while the D-optimal design based on the MGLSE distributes the three treatments in the middle plot almost uniformly. In addition, if [formula], we get the same D-optimal robust designs in Figures [\ref=Example21] and [\ref=Example22] for the MGLSE and LSE, respectively.

Example 3 Construct the robust design for b = 2, ~ t = 12, ~ m = 6, and n = 2. The neighbourhood is [formula] with α = 0.3 and [formula]. Take σ2 = 1. The correlation matrix [formula] (12  ×  12) is from the DG with parameter λ, i.e., the correlation between two small plots at locations (k1,s1) and (k2,s2) is given by λ|k1 - k2| + |s1 - s2|. Robust designs are found using the annealing algorithm and are presented for two values of λ in Figure [\ref=Dopt_DG12]. We have [formula] and 0.58963 for λ = 0.01 and 0.3, respectively.

Example 4 Robust designs are constructed for b = 2, n = 2 and various values of t. The first order NN correlation structure is used and ρ = 0.2, and the neighborhood is [formula] with α = 0.3 and [formula]. The designs are presented in Figure [\ref=Dopt_NN_vart], and they minimize [formula]. The minimum loss function values are [formula] and 0.58662 for t = 10,12,14,16 and 18, respectively. All the designs have the property that the neighbours in block one are not neighbours in block two.

Theoretical properties

Analytical solutions for robust designs are hard to obtain in general, but we are able to derive several theoretical results for block designs here.

For neighbourhood [formula] with [formula] , the design with the same treatment allocation in all the b blocks is a D-optimal robust design, which minimizes [formula].

The proof of Theorem 2 is in the Appendix. The result is true for any [formula], b  ≥  2 and n  ≥  1. As indicated in Example 2, we can also permute the treatment numbers and block numbers in the robust designs. In addition, if [formula] is from a weakly stationary error process, then we can change the orientation of any number of blocks in the D-optimal robust designs.

For the LSE and neighbourhood [formula] with [formula] or [formula], any design is an A-optimal robust design, which minimizes [formula].

The proof of Theorem 3 is in the Appendix. The result is true for any number of blocks and n  ≥  1. The result implies that the trace is not a good measure to differentiate the designs for the LSE and neighbourhood [formula].

Consider block designs with b = 2, n = 1 and t > 3. For the MGLSE and neighbourhood [formula] with [formula] being the DG or DE correlation matrix, the D-optimal robust design, which minimizes [formula], does not have the same treatment allocation in the two blocks.

The proof of Theorem 4 is in the Appendix. The result shows that the D-optimal robust designs based on the LSE and the MGLSE are different. In addition, from the proofs of Theorems 2 and 4, we can see that

[formula]

Thus the MGLSE should be applied when there is information about the error correlation. Theorem 4 is for a specific situation, but we conjecture that the result is true in general. This could be a future research topic.

Guidelines for using robust designs

Robust designs studied in this paper can be applied to any block experiment in which there is a possibility of correlated errors. Here is a detailed procedure for practical applications.

Since we never know the exact covariance matrix of the errors in practice, robust designs perform well in a neighborhood of the covariance matrix [formula]. In addition, our study indicates that robust designs are not very sensitive to the choices of [formula] and α, from many examples we have constructed. For instance, in Example 1 the robust design does not depend on the value of α, and the robust design is highly efficient for a range of ρ values in [formula].

We define an efficiency measure to compare a design [formula] with the robust design [formula],

[formula]

For Example 1, we compute the efficiencies for the following three representative designs [formula], and [formula]. The first block of the three designs is the same as in [formula]. The orderings of the second block for the three designs are as follows, [formula]: 7, 6, 5, 4, 3, 2, 1; [formula]: 1, 2, 3, 4, 7, 6, 5; [formula]: 2, 1, 4, 3, 6, 5, 7. The efficiencies are given in Table [\ref=Table4]. It is clear that the robust design is more efficient than the three designs for a range of ρ values.

Conclusion

We have investigated robust designs for experiments running in b blocks, where a complete replicate of t treatments is run in each block. These designs are robust against possible misspecification of the covariance matrix of the errors within each block. We used a neighbourhood to model the unknown covariance matrix of the errors instead of specifying it exactly. Robust designs are defined using a minimax approach, i.e., minimizing the maximum loss of [formula], where the estimator can be the LSE or the MGLSE. Several interesting theoretical results and examples have been obtained and presented. In particular, the robust designs based on the LSE and MGLSE are quite different. The results in this paper indicate that when there is information about the correlation of the errors, the MGLSE should be used to construct robust designs and to estimate the treatment means.

In this paper we have focused on the block designs with one replicate within each block, and a measure of [formula] is minimized. However, the methodology can be easily extended to situations where

For (a), we only need to make some dimensional changes in matrices [formula], [formula] and [formula]. For (b), notice that [formula]. But interesting results may be derived for various contrast matrices. If there is a control in the t treatments, say treatment 1, then it is natural to compare each treatment with the control and the contrast matrix can be defined as

[formula]

We can also define other contrast matrices to study linear combinations of the t treatment means.

Appendix: Proofs

Proof of Theorem 1: From ([\ref=gloss1]), we have

[formula]

then from the definition of [formula] in ([\ref=NBR]), we get

[formula]

where [formula], with the same dimensions as matrix [formula]. Since measure L is monotonic according to the ordering of positive definiteness, it is clear that

[formula]

Thus, from ([\ref=maxLoss1]),

[formula]

Putting [formula] and [formula] in the above equation gives the results in ([\ref=maxLoss2]). For the MGLSE, the covariance matrix is in ([\ref=COVM]) and the loss function is in ([\ref=gloss2]). By a similar proof to the proof for the LSE above, we can get the result in ([\ref=maxLoss3]).

Proof of Theorem 2: From ([\ref=maxLoss2]), we have

[formula]

Notice that [formula] and [formula], which gives

[formula]

Since the treatment labels are randomly assigned, without loss of generality we can number the treatments in block 1 such that [formula]. In addition we can write matrix [formula], where [formula] is a (t  ×  t) permutation matrix, [formula]. It is obvious that [formula]. Then from ([\ref=TH21]), we get

[formula]

Define [formula], [formula], and [formula]. It is obvious that [formula], since [formula]. Using Minkowski's inequality in Horn and Johnson (1985, page 482), we can show that [formula], where the equality holds if [formula]. Thus, [formula] in ([\ref=Th22]) is minimized when [formula]. This implies that the design with the same treatment allocation in all the b blocks is a D-optimal robust design, which minimizes [formula].

Proof of Theorem 3: We prove the result for [formula] here. The result for [formula] can be proved similarly. Notice that for any design, [formula], for [formula]. From ([\ref=maxLoss2]), we have

[formula]

which does not depend on design [formula]. Therefore any design is an A-optimal robust design.

Proof of Theorem 4: For n = 1, the DG and DE correlation matrix have the same form, which is given by, for λ∈(0,1),

[formula]

and it is easy to verify that its inverse matrix is

[formula]

For b = 2, [formula], [formula], and [formula], where [formula] (t  ×  1) is a vector of ones. As in the proof of Theorem 2, let [formula] and [formula], where [formula] is a permutation matrix. Then straightforward calculation gives

[formula]

Let [formula]. It is clear that c0  >  0. Now we have If [formula], then

[formula]

and from ([\ref=maxLoss3]), we have

[formula]

If [formula] but [formula], where [formula] is also a permutation matrix, then it is easy to verify that [formula]. Thus

[formula]

Using Minkowski's inequality in Horn and Johnson (1985, page 482), we can show that

[formula]

which is the value of [formula] in ([\ref=lossSameB1]). This implies that [formula] is not minimized by [formula]. This completes the proof.

Acknowledgements

This research was partially supported by Discovery Grants from the Natural Sciences and Engineering Research Council of Canada. The authors are grateful to the Editor and referee for their helpful comments and suggestions.

References