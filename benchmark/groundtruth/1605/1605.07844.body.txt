Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation

Introduction

Pseudo-relevance feedback (PRF) has been shown to be an effective approach for updating query language models in information retrieval (IR) [\cite=Lv:2014]. In cross-language environments there are a couple of top-ranked document sets in different languages building such models requires bridging the gap of the languages. To this end, cross-lingual topical relevance models (CLTRLM) aims at finding a way for transferring knowledge of the sets to the query using bilingual topic modeling and bilingual dictionaries [\cite=Lavrenko:2002] [\cite=Ganguly:2012]. In this paper we propose a new method for building translation models on pseudo-relevant collections using neural network language modelling. The proposed cross-lingual word embedding translation model (CLWETM) takes advantage of a query-dependent transformation matrix between low-dimensional vectors of the languages. Indeed, we aim at finding a transformation matrix to bring the vector of each query term to dimensionality of the target language and then computing translation probabilities based on the vector similarities. To this aim, first we learn word representations of the pseudo-relevant collections separately and then focus on finding a transformation matrix minimizing a distance function between all the translation pairs. Finally, a softmax function is used for building a query-dependent translation model based on similarity of transformed vector of each query term with translation candidates in the target language.

Unlike CLTRLM, CLWETM considers the relative positions of the co-occurred words and therefore we believe that the obtained model would capture the context of the query more accurately. Furthermore, the obtained model is incorporated within a language modelling framework, the state-of-the-art retrieval framework in the literature, and therefore the proposed method does not suffer from disadvantages of low-dimensional document representations in IR [\cite=Vulic:2015b].

Experimental results on four CLEF collections in French, Spanish, German, and Persian demonstrate that the proposed method outperforms all competitive baselines in language modelling, particularly when it is combined with a collection-dependent or query-independent translation model.

Related Works

Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [\cite=Lv:2014] [\cite=Lv:2010]. But, in multi-lingual environments there are only a limited number of methods to this end; cross-lingual relevance models (CLRLM) and CLTRLM are state-of-the-art methods in this area [\cite=Ganguly:2012] [\cite=Vulic:2015a] [\cite=Lavrenko:2002]. Unlike CLRLM that depends on parallel corpora and bilingual lexicons, CLTRLM aims at finding a number of bilingual topical variables from a comparable corpus in order to transfer relevance score of a term from one language to another. Ganguly et al. proposed to use these variables for query translation and demonstrated that CLTRLM is an effective method particularly for resource-lean languages [\cite=Ganguly:2012].

In CLTRLM top-ranked documents Fs  =  {ds1,ds2,..,ds|Fs|} retrieved in response to s, the user-specified query, and top-ranked documents Ft  =  {dt1,dt2,..,dt|Ft|} retrieved in response to t, translation of the query, are assumed to be relevant documents. CLTRLM assumes that each word wt in target language is generated either from a target language generation event or a source language generation event as follows:

[formula]

in which t is a topical variable on Ft and s is a translation of wt in the dictionary. For estimating p(t|s) and p(s|s) a topic model of Ft and a topic model of Fs are used, respectively and then topic projection is calculated based on translation probabilities provided in the dictionary [\cite=Ganguly:2012]. It is noteworthy that topic modeling considers co-occurrences of the terms within documents which means that the positions of the terms do not play an important role in the model.

Language modeling based on neural networks is a popular technique for capturing both co-occurrences of the terms within a constant window c and also their positions in the context [\cite=Mikolov:2013]. Learning vectors of terms relies on minimizing a loss function like a binary regression function L(θ) as follows:

[formula]

in which [formula] and [formula] are the vectors of a term and its context respectively. z = 1 for valid pairs (w,c)∈D and z = 0 for invalid ones (w,c)∈D'. But when it comes to cross-lingual environments a constraint is required for integrating dimensions in the languages. There are a couple of methods to this end; on-line methods and off-line methods. On-line methods aim at finding a unified space for the languages during the learning process as follows [\cite=Gouws:2014]:

[formula]

Off-line methods learn the representation vectors separately and then aims at finding a transformation matrix by minimizing a constraint function f as follows [\cite=Mikolov:2013b]:

[formula]

in which [formula] and [formula] are the vectors of translation pairs from a dictionary or a parallel corpus.

Linear Projection between Languages based on Pseudo comparable Documents

Skip-gram negative sampling has been shown to be an effective approach for learning low-dimensional vectors for each word of a language [\cite=Mikolov:2013]. As shown in Equation [\ref=eq:obj_func] our goal is to minimize f with respect to a transformation matrix [formula], a number of word pairs (i.e., ws from the source collection and wt from the bilingual dictionary appeared in the target collection), and their low-dimensional representations [formula] and [formula]; f is defines as follow:

[formula]

To solve this problem we opted the gradient descent algorithm (i.e., [formula]):

[formula]

Billingual Representations and Translation Models

Based on Dimension Projection

Vector space models based on low-dimensional vectors are suffering from a number of challenges in IR including but not limited to document representation, document length normalization, and similarity measures to some extent [\cite=Vulic:2015b] [\cite=Le:2014]. In this section we introduce a method based on bilingual word representations for building a translation model for query translation and then incorporating within a state-of-the-art retrieval framework.

[formula]

Based on Mixed Word Embedding

Vulic et al., introduced a mixed word embedding method based on a shuffling approach over comparable corpora [\cite=Vulic:2015b]. We employed this method as one of the baselines of the proposed method. We build translation models as follows:

[formula]

Combining Translation Models

It is noteworthy that since is a probabilistic translation model we can interpolate it with other models using a constant controlling parameter as follow:

[formula]

Experiments

Experimental Setup

Overview of the used collections are provided in Table [\ref=tab:dataset]. The source collection for learning ws is a pool of Associated Press 1988-89, Los Angeles Times 1994, and Glasgow Herald 1995 collections. These collections are used in previous TREC and CLEF evaluation campaigns for ad-hoc retrieval.

In all experiments, we use the language modeling framework with the KL-divergence retrieval model and Dirichlet smoothing method to estimate the document language models, where we set the dirichlet prior smoothing parameter μ to the typical value of 1000. To improve the retrieval performance, we use the mixture model for pseudo-relevance feedback with the feedback coefficient of 0.5. The number of feedback documents and feedback terms are set to the typical values of 10 and 50, respectively.

All European dictionaries, documents, and queries are normalized and stemmed using the Porter stemmer. Documents and queries in Persian are remained intact [\cite=Hashemi:2014]. Stopword removal is also performed..The Lemur toolkit is employed as the retrieval engine in our experiments.

We use the Google dictionaries in our experiments. In the European languages, we do not transliterate out of vocabulary (OOV) terms of the source languages. The OOVs of the target language are used as their original forms in the source documents, since they are cognate languages; but, in the Persian language, as it has a different alphabet, we transliterate the OOVs. Note that we use the uniform distribution approach as the initial translation model for retrieving top documents. It is worth mentioning that p(wt|s) in Equation [\ref=CLTRLM] is estimated by a bi-gram coherence translation model (BiCTM) introduced in [\cite=Monz:2005]. Weights of the edges of the graph are estimated by p(wj|wi) computed by SRILM toolkit . BiCTM is also used as T2 in Equation [\ref=eq.linear].

For the PARALLEL baselines we use GIZA++ toolkit in order to learn translation model on TEP parallel corpus for Persian and on DGT for the European languages [\cite=AKER:14]. To avoid apple-to-orange comparisons, we ignore domain adaptation methods in the literature or other parallel corpora [\cite=Nie:2012].

As discussed in Section [\ref=Linear_Projection_between_Languages_based_on_Pseudo_comparable_Documents], we used stochastic gradient descent for learning [formula] which is initialized with random values in

[formula]

Performance Comparison and Discussion

In this section we want to compare of a number of competitive methods in CLIR . We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) model obtained from the parallel corpora (PARALLEL), (4) (BiCTM) proposed in [\cite=Monz:2005] , (5) the JCLTRLM method proposed in [\cite=Ganguly:2012], and (6) the mixed word embedding translation model (MIXWETM), introduced in Section [\ref=sec.mixedwe]. We provided the results of CLWETM for α = 0 (i.e., without interpolation. See Equation [\ref=eq.linear].) and α* which is determined by a 2-fold cross-validation over each query set. All the results are summarized in Table [\ref=tab:exp2]. As shown in the table, both MIXWETM and CLWETM outperform other methods in terms of MAP, in all the collections. Except in Spanish, MIXWETM and CLWETM consistently achieved better results compared to others in terms of ; but, CLWETM is clearly more effective than MIXWETM, whose context is by far larger than of CLWETM. The results of JCLTRLM reveals that top-ranked documents in SP and DE are not as comparable as FA or FR. Therefore we hope more improvements in FA and FR than others for CLWETM and MIXWETM. It is noteworthy that in DE the performance of CLWETM is improved by increasing n even for smaller values of α (see Figure [\ref=fig:param1]);

In FA, FR, ES, and DE, CLWETM achieved 75.81%, 78.21%, 76.30%, and 66.67% of of the monolingual runs, respectively. CLWETM outperforms the original BiCTM in terms of MAP by 8.9%, 4.6%, 4.2%, and 10.93% on FA, FR, SP, and DE respectively. CLWETMα = 0 outperforms TOP-1, UNIFORM, and PARALLEL, all the off-line translation models, on all the collections in almost all the metrics. It demonstrates : importance of tailoring top-ranked documents in both languages, incorporating a positional language model on the documents (compare the results of CLWETM and MIXWETM), and employing an effective transformation framework between the languages.

Parameter Sensitivity

We investigate the sensitivity of the proposed method to two parameters α and n in Figure 1. We first fix one parameter to its optimal value and then try to get optimal value of the other one . It demonstrates that both parameters work stably across SP, FA and FR collections. The optimal α value is 0.6 and the optimal n value is 10 in these three collections. In DE collection, the optimal values for the both parameters are somehow different; the optimal α value is 0.2, suggesting that CLWETM works well without even being combined with BiCTM. The optimal n value is 50 in DE, suggesting that top-retrieved documents are not good enough. increasing number of top-retrieved documents improvesb the retrieval performance.

Conclusion and Future Works