Introduction: The New Data-Rich Astronomy

A major paradigm shift is now taking place in astronomy and space science. Astronomy has suddenly become an immensely data-rich field, with numerous digital sky surveys across a range of wavelengths, with many Terabytes of pixels and with billions of detected sources, often with tens of measured parameters for each object. This is a great change from the past, when often a single object or a small sample of objects were used in individual studies. Instead, we can now map the universe systematically, and in a panchromatic manner. This will enable quantitatively and qualitatively new science, from statistical studies of our Galaxy and the large-scale structure in the universe, to the discoveries of rare, unusual, or even completely new types of astronomical objects and phenomena. This new digital sky, data-mining astronomy will also enable and empower scientists and students anywhere, without an access to large telescopes, to do first-rate science. This can only invigorate the field, as it opens the access to unprecedented amounts of data to a fresh pool of talent.

Handling and exploring these vast new data volumes, and actually making real scientific discoveries poses a considerable technical challenge. The traditional astronomical data analysis methods are inadequate to cope with this sudden increase in the data volume (by several orders of magnitude). These problems are common to all data-intensive fields today, and indeed we expect that some of the products and experiences from this work would find uses in other areas of science and technology. As a testbed for these software technologies, astronomy provides a number of benefits: the size and complexity of the data sets are nontrivial but manageable, the data generally are in the public-domain, and the knowledge gained by understanding this data is of broad public appeal.

In this chapter, we provide an overview of the state of massive datasets in astronomy as of mid-2001. In Section [\ref=astrodata], we briefly discuss the nature of astronomical data, with an emphasis on understanding the inherent complexity of data in the field. In Section [\ref=dataset], we present overviews of many of the largest datasets, including a discussion of how the data are utilized and archived. Section [\ref=vo-initaive] provides a thorough discussion of the virtual observatory initiative, which aims to federate all of the distributed datasets described in Section [\ref=dataset] into a coherent archival framework. We conclude this chapter with a summary of the current state of massive datasets in astronomy.

The Nature of Astronomical Data

By its inherent nature, astronomical data are extremely heterogeneous, in both format and content. Astronomers are now exploring all regions of the electromagnetic spectrum, from gamma-rays through radio wavelengths. With the advent of new facilities, previously unexplored domains in the gravitational spectrum will soon be available, and exciting work in the astro-particle domain is beginning to shed light on our Universe. Computational advances have enabled detailed physical simulations which rival the largest observational datasets in terms of complexity. In order to truly understand our cosmos, we need to assimilate all of this data, each presenting its own physical view of the Universe, and requiring its own technology.

Despite all of this heterogeneity, however, astronomical data and its subsequent analysis can be broadly classified into five domains. In order to clarify later discussions, we briefly discuss these domains and define some key astrophysical concepts which will be utilized frequently throughout this chapter.

Imaging data is the fundamental constituent of astronomical observations, capturing a two-dimensional spatial picture of the Universe within a narrow wavelength region at a particular epoch or instant of time. While this may seem obvious to most people--after all, who hasn't seen a photograph--astrophysical pictures (see, e.g.,  Figures [\ref=crab] and [\ref=image]) are generally taken through a specific filter, or with an instrument covering a limited range of the electromagnetic spectrum, which defines the wavelength region of the observation. Astronomical images can be acquired directly, e.g.,  with imaging arrays such as CCDs, or synthesized from interferometric observations as is customarily done in radio astronomy.

Catalogs are generated by processing the imaging data. Each detected source can have a large number of measured parameters, including coordinates, various flux quantities, morphological information, and areal extant. In order to be detected, a source must stand out from the background noise (which can be either cosmic or instrumental in origin). The significance of a detection is generally quoted in terms of σ, which is a relative measure of the strength of the source signal relative to the dispersion in the background noise. We note that the source detection process is generally limited both in terms of the flux (total signal over the background) and surface brightness (intensity contrast relative to the background).

Coordinates are used to specify the location of astronomical sources in the sky. While this might seem obvious, the fact that we are sited in a non-stationary reference frame ( e.g.,  the earth rotates, revolves around the sun, and the sun revolves around the center of our Galaxy) complicates the quantification of a coordinate location. In addition, the Earth's polar axis precesses, introducing a further complication. As a result, coordinate systems, like Equatorial coordinates, must be fixed at a particular instant of time (or epoch), to which the actual observations, which are made at different times, can be transformed. One of the most popular coordinate systems is J2000 Equatorial, which is fixed to the initial instant (zero hours universal time) of January 1, 2000. One final caveat is that nearby objects ( e.g.,  solar system bodies or nearby stars) move on measurable timescales. Thus the date or precise time of a given observations must also be recorded.

Flux quantities determine the amount of energy that is being received from a particular source. Since different physical processes emit radiation at different wavelengths, most astronomical images are obtained through specific filters. The specific filter(s) used varies, depending on the primary purpose of the observations and the type of recording device. Historically, photographic surveys used filters which were well matched to the photographic material, and have names like O, E, J, F, and N. More modern digital detectors have different characteristics (including much higher sensitivity), and work primarily with different filter systems, which have names like U, B, V, R, and I, or g, r, i in the optical, and J, H, K, L, M, and N in the near-infrared.

In the optical and infrared regimes, the flux is measured in units of magnitudes (which is essentially a logarithmic rescaling of the measured flux) with one magnitude equivalent to - 4 decibels. This is the result of the historical fact that the human eye is essentially a logarithmic detector, and astronomical observations have been made and recorded for many centuries by our ancestors. The zeropoint of the magnitude scale is determined by the star Vega, and thus all flux measurements are relative to the absolute flux measurements of this star. Measured flux values in a particular filter are indicated as B  =  23 magnitudes, which means the measured B band flux is 100.4  ×  23 times fainter than the star Vega in this band. At other wavelengths, like x-ray and radio, the flux is generally quantified in standard physical units such as ergs cm- 2 s- 1 Hz- 1. In the radio, observations often include not only the total intensity (indicated by the Stokes I parameter), but also the linear polarization parameters (indicated by the Stokes Q, and U parameters).

Spectroscopy, Polarization, and other follow-up measurements provide detailed physical quantification of the target systems, including distance information ( e.g.,  redshift, denoted by z for extragalactic objects), chemical composition (quantified in terms of abundances of heavier elements relative to hydrogen), and measurements of the physical ( e.g.,  electromagnetic, or gravitational) fields present at the source. An example spectrum is presented in Figure [\ref=spectra], which also shows the three optical filters used in the DPOSS survey (see below) superimposed.

Studying the time domain (see, e.g.,  Figure [\ref=time]) provides important insights into the nature of the Universe, by identifying moving objects ( e.g.,  near-Earth objects, and comets), variable sources ( e.g.,  pulsating stars), or transient objects ( e.g.,  supernovae, and gamma-ray bursts). Studies in the time domain either require multiple epoch observations of fields (which is possible in the overlap regions of surveys), or a dedicated synoptic survey. In either case, the data volume, and thus the difficulty in handling and analyzing the resulting data, increases significantly.

Numerical Simulations are theoretical tools which can be compared with observational data. Examples include simulations of the formation and evolution of large-scale structure in the Universe, star formation in our Galaxy, supernova explosions, etc.  Since we only have one Universe and cannot modify the initial conditions, simulations provide a valuable tool in understanding how the Universe and its constituents formed and have evolved. In addition, many of the physical processes that are involved in these studies are inherently complex. Thus direct analytic solutions are often not feasible, and numerical analysis is required.

Large Astronomical Datasets

As demonstrated below, there is currently a great deal of archived data in Astronomy at a variety of locations in a variety of different database systems systems. In this section we focus on ground-based surveys, ground-based observatories, and space-based observatories. We do not include any discussion of information repositories such as the Astrophysics Data System (ADS), the Set of Identifications, Measurements, and Bibliography for Astronomical Data (SIMBAD), or the NASA Extragalactic Database (NED), extremely valuable as they are. This review focuses on more homogeneous collections of data from digital sky surveys and specific missions rather than archives which are more appropriately described as digital libraries for astronomy.

Furthermore, we do not discuss the large number of new initiatives, including the Large-Aperture Synoptic Survey Telescope (LSST), the California Extremely Large Telescope (CELT), the Visible and Infrared Survey Telescope (VISTA), or the Next Generation Space Telescope (NGST), which will provide vast increases in the quality and quantity of astronomical data.

Ground-Based Sky Surveys

Of all of the different astronomical sources of data, digital sky surveys are the major drivers behind the fundamental changes underway in the field. Primarily this is the result of two factors: first, the sheer quantity of data being generated over multiple wavelengths, and second, as a result of the homogeneity of the data within each survey. The federation of different surveys would further improve the efficacy of future ground- and space-based targeted observations, and also open up entirely new avenues for research.

In this chapter, we describe only some of the currently existing astronomical archives as examples of the types, richness, and quantity of astronomical data which is already available. Due to the space limitations, we cannot cover many other, valuable and useful surveys, experiments and archives, and we apologize for any omissions. This summary is not meant to be complete, but merely illusory.

Photographic plates have long endured as efficient mechanisms for recording surveys (they have useful lifetimes in excess of one hundred years and offer superb information storage capacity, but unfortunately they are not directly computer-accessible and must be digitized before being put to a modern scientific use). Their preeminence in a digital world, however, is being challenged by new technologies. While many photographic surveys have been performed, e.g.,  from the Palomar Schmidt telescope in California, and the UK Schmidt telescope in New South Wales, Australia, these data become most useful when the plates are digitized and cataloged.

While we describe two specific projects, as examples, several other groups have digitized photographic surveys and generated and archived the resulting catalogs, including the Minnesota Automated Plate Scanner [\citep=pennington93], the Automated Plate Measuring Machine [\citep=mcmahon92] at the Institute of Astronomy, Cambridge, UK, the coordinates, sizes, magnitudes, orientations, and shapes [\citep=yentis92] and its successor, SuperCOSMOS, plate scanning machines at the Royal Observatory Edinburgh. Probably the most popular of the digitized sky surveys (DSS) are those produced at the Space Telescope Science Institute (STScI) and its mirror sites in Canada, Europe, and Japan.

There is a large number of experiments and surveys at aimed at detecting time-variable sources or phenomena, such as gravitational microlensing, optical flashes from cosmic gamma-ray bursts, near-Earth asteroids and other solar system objects, etc.  A good list of project web-sites is maintained by Professor Bohdan Paczynski at Princeton. Here we describe several interesting examples of such projects.

a digital atlas of the sky comprising more than 1 million 8'x16' images having about 4 " spatial resolution in each of the three wavelength bands,

a point source catalog containing accurate (better than 0.5 ") positions and fluxes (less than 5% for KS  >  13) for approximately 300,000,000 stars.

an extended source catalog containing positions and total magnitudes for more than 500,000 galaxies and other nebulae.

Ground-Based Observatory Archives

Traditional ground-based observatories have been saving data, mainly as emergency back-ups for the users, for a significant time, accumulating impressive quantities of highly valuable, but heterogeneous data. Unfortunately, with some notable exceptions, the heterogeneity and a lack of adequate funding have limited the efforts to properly archive this wealth of information and make it easily available to the broad astronomical community. We see the development of good archives for major ground-based observatories as one of the most pressing needs in this field, and a necessary step in the development of the National Virtual Observatory.

In this section, we discuss three specific ground-based observatories: the National Optical Astronomical Observatories (NOAO), the National Radio Astronomical Observatory (NRAO), and the European Southern Observatory (ESO), focusing on their archival efforts. In addition to these three, many other observatories have extensive archives, including the Canada-France-Hawaii telescope (CHFT), the James Clerk Maxwell Telescope (JCMT), the Isaac Newton Group of telescopes at La Palma (ING), the Anglo-Australian Observatory (AAT), the United Kingdom Infrared Telescope (UKIRT), and the Australia National Telescope Facility (ATNF).

NASA Space-Based Observatory Archives

With the continual advancement of technology, ground-based observations continue to make important discoveries. Our atmosphere, however, absorbs radiation from the majority of the electromagnetic spectrum, which, while important to the survival of life, is a major hindrance when trying to untangle the mysteries of the cosmos. Thus space-based observations are critical, yet they are extremely expensive. The resulting data is extremely valuable, and all of the generated data is archived. While there have been (and continue to be) a large number of satellite missions, we will focus on three major NASA archival centers: MAST, IRSA, and HEASARC (officially designated as NASA's distributed Space Science Data Services), the Chandra X-ray Observatory archive (CXO), and the National Space Science Data Center (NSSDC).

the Hubble Space Telescope (HST),

the International Ultraviolet Explorer (IUE),

Copernicus UV Satellite (OAO-3),

the Extreme Ultraviolet Explorer (EUVE),

the two space shuttle ASTRO Missions:

the Ultraviolet Imaging Telescope (UIT),

the Wisconsin Ultraviolet Photo Polarimetry Experiment (WUPPE),

the Hopkins Ultraviolet Telescope (HUT),

the Two Micron All-Sky Survey (2MASS),

the Space Infrared Telescope Facility (SIRTF),

the European Infrared Space Observatory (ISO),

the Midcourse Space Experiment (MSX),

the Infrared Astronomical Satellite (IRAS).

the Roentgen Satellite (ROSAT),

the Advanced Satellite for Cosmology (ASCA),

BeppoSAX,

the Compton Gamma Ray Observatory (CGRO), the second of NASA's great observatories,

the Extreme Ultraviolet Explorer (EUVE),

the High Energy Astrophysics Observatory (HEAO 1),

the Einstein Observatory (HEAO 2),

the European Space Agency's X-ray Observatory (EXOSAT),

the Rossi X-ray Timing Explorer (Rossi XTE),

the Advanced Satellite for X-ray Astronomy (Chandra),

the X-ray Multiple Mirror Satellite (XMM-Newton),

the High Energy Transient Explorer (HETE-2),

The Future of Observational Astronomy: Virtual Observatories

Raw data, no matter how expensively obtained, are no good without an effective ability to process them quickly and thoroughly, and to refine the essence of scientific knowledge from them. This problem has suddenly increased by orders of magnitude, and it keeps growing.

A prime example is the efficient scientific exploration of the new multi-Terabyte digital sky surveys and archives. How can one make efficiently discoveries in a database of billions of objects or data vectors? What good are the vast new data sets if we cannot fully exploit them?

In order to cope with this data flood, the astronomical community started a grassroots initiative, the National (and ultimately Global) Virtual Observatory [\citep=NVO2K]. Recognizing the urgent need, the National Academy of Science Astronomy and Astrophysics Survey Committee, in its new decadal survey entitled Astronomy and Astrophysics in the New Millennium, recommends, as a first priority, the establishment of a National Virtual Observatory. The NVO will likely grow into a Global Virtual Observatory, serving as the fundamental information infrastructure for astronomy and astrophysics in the next century. We envision productive international cooperation in this rapidly developing new field.

The NVO would federate numerous large digital sky archives, provide the information infrastructure and standards for ingestion of new data and surveys, and develop the computational and analysis tools with which to explore these vast data volumes. It would provide new opportunities for scientific discovery that were unimaginable just a few years ago. Entirely new and unexpected scientific results of major significance will emerge from the combined use of the resulting datasets, science that would not be possible from such sets used singly. The NVO will serve as an engine of discovery for astronomy [\citep=nvo-white].

Implementation of the NVO involves significant technical challenges on many fronts: How to manage, combine, analyze and explore these vast amounts of information, and to do it quickly and efficiently? We know how to collect many bits of information, but can we effectively refine the essence of knowledge from this mass of bits? Many individual digital sky survey archives, servers, and digital libraries already exist, and represent essential tools of modern astronomy. However, in order to join or federate these valuable resources, and to enable a smooth inclusion of even greater data sets to come, a more powerful infrastructure and a set of tools are needed.

The rest of this review focuses on the two core challenges that must be tackled to enable the new, virtual astronomy:

Effective federation of large, geographically distributed data sets and digital sky archives, their matching, their structuring in new ways so as to optimize the use of data-mining algorithms, and fast data extraction from them.

Data mining and "knowledge discovery in databases" (KDD) algorithms and techniques for the exploration and scientific utilization of large digital sky surveys, including combined, multi-wavelength data sets.

These services would carry significant relevance beyond Astronomy as many aspects of society are struggling with information overload. This development can only be done by a wide collaboration, that involves not only astronomers, but computer scientists, statisticians and even participants from the IT industry.

Architecting the Virtual Observatory

The foundation and structure of the National Virtual Observatory (NVO) are not yet clearly defined, and are currently the subject of many vigorous development efforts. One framework for many of the basic architectural concepts and associated components of a virtual observatory, however, has become popular. First is the requirement that, if at all possible, all data must be maintained and curated by the respective groups who know it best -- the survey originators. This requires a fully distributed system, as each survey must provide the storage, documentation, and services that are required to participate in a virtual observatory.

The interconnection of the different archive sites will need to utilize the planned high-speed networks, of which there are several testbed programs already available or in development. A significant fraction of the technology for the future Internet backbone is already available, the problem is finding real-world applications which can provide a sufficient load. A Virtual Observatory, would, of course, provide heavy network traffic and is, therefore, a prime candidate for early testing of any future high-speed networks.

The distributed approach advocated by this framework (see Figure [\ref=cit-vo] for a demonstration) relies heavily on an the ability of different archives to participate in "collaborative querying". This tight integration requires that everything must be built using appropriately developed standards, detailing everything from how archives are "discovered" and "join" the virtual observatory, to how queries are expressed and data is transferred. Once these standards have been developed, implementation (or retrofitting as the case may be) of tools, interfaces, and protocols that operate within the virtual observatory can begin.

The architecture of a virtual observatory is not only dependent on the participating data centers, but also on the users it must support. For example, it is quite likely that the general astronomy public ( e.g.,  amateur astronomers, K-12 classrooms, etc.) would use a virtual observatory in a casual lookup manner ( i.e.  the web model). On the other hand, a typical researcher would require more complex services, such as large data retrieval ( e.g.,  images) or cross-archive joins. Finally, there will also be the "power users" who would require heavy post-processing of query results using super-computing resources ( e.g.,  clustering analysis).

From these user models we can derive "use cases", which detail how a virtual observatory might be utilized. Initially, one would expect a large number of distinct "exploratory" queries as astronomers explore the multi-dimensional nature of the data. Eventually the queries will become more complex and employ a larger scope or more powerful services. This model requires the support of several methods for data interaction: manual browsing, where a researcher explores the properties of an interesting class of objects; cross-identification queries, where a user wants to find all known information for a given set of sources; sweeping queries, where large amounts of data ( e.g.,  large areal extents, rare object searches) are processed with complex relationships; and the creation of new "personal" subsets or "official" data products. This approach leads, by necessity, to allowing the user to perform customizable, complex analysis ( i.e.  data-mining) on the extracted data stream.

Connecting Distributed Archives

As can be seen from Section [\ref=dataset], a considerable amount of effort has been expended within the astronomical community on archiving and processing astronomical data. On the other hand, very little has been accomplished in enabling collaborative, cross-archive data manipulation (see Figure [\ref=as]). This has been due, in part, to the previous dearth of large, homogeneous, multi-wavelength surveys; in other words, the payoff for federating the disparate datasets has previously been too small to make the effort worthwhile. Here we briefly outline some of the key problem areas [\citep=fgcs] for a more detailed discussion), that must be addressed in order to properly build the foundation for the future virtual observatories.

Communication Fundamentals

The first requirement for connecting highly distributed datasets is that they must be able to communicate with each other. This communication takes multiple roles, including the initiation of communication, discovering the holdings and capabilities of each archive, the actual process of querying, the streaming of data, and an overall control structure. None of these ideas are entirely new, the general Information Technology field has been confronting similar issues and solutions, such as Grid frameworks [\citep=grid], JINI and the Web services model (see, e.g.,  the IBM Web Service web-site) for more information) are equally applicable.

Clearly, the language for communicating will be the extensible markup language (XML), using a community defined standard schema. This will allow for control of the inter-archive communication and processing ( e.g.,  the ability to perform basic checkpoint operations on a query: stop, pause, restart, abort, and provide feedback to the end-user). A promising, and simple mechanism for providing the archive communication endpoints is through web services, which would be built using the Simple Object Access Protocol (SOAP), Web Services Description Language (WSDL), and a common Universal Description, Discovery, and Integration (UDDI) registry. An additional benefit of this approach is that pre-existing or legacy archival services can be retrofitted (by mapping a new service onto existing services) in order to participate in collaborative querying.

This model also allows for certain optimizations to be performed depending on the status of the archival connections ( e.g.,  network weather). Eventually, a learning mechanism can be applied to analyze queries, and using the accumulated knowledge gained from past observations ( i.e.  artificial intelligence), queries can be rearranged in order to provide further performance enhancements.

Archival Metadata

In order for the discovery process to be successful, the archives must communicate using shared semantics. Not only must this semantic format allow for the transfer of data contents and formats between archives, but it also should clearly describe the specific services that an archive can support (such as cross-identification or image registration) and the expected format of the input and output data. Using the web service model, our services would be registered in a well known UDDI registry, and communicate their capabilities using WSDL. Depending on the need of the consumer, different amounts (or levels) of detailed information might be required, leading to the need for a hierarchical representation. Once again, the combination of XML and a standardized XML Schema language provides an extremely powerful solution, as is easily generated, and can be parsed by machines and read by humans with equal ease. By adopting a standardized schema, metadata can be easily archived and accessed by any conforming application.

High Performance Data Streaming

Traditionally, astronomers have communicated data either in ASCII text (either straight or compressed), or by using the community standard Flexible Image Transport Standard (FITS). The true efficacy of the FITS format as a streaming format, however, is not clear, due to the difficulty of randomly extracting desired data or shutting off the stream. The ideal solution would pass different types of data ( i.e.  tabular, spectral, or imaging data) in a streaming fashion (similar to MPI--Message Passing Interface), so that analysis of the data does not need to wait for the entire dataset before proceeding. In the web services model, this would allow different services to cooperate in a head-to-tail fashion ( i.e.  the UNIX pipe scenario). This is still a potential concern, as the ability to handle XML encoded binary data is not known.

Astronomical Data Federation

Separate from the concerns of the physical federation of astronomical data via a virtual observatory paradigm is the issue of actually correlating the catalog information from the diverse array of multiwavelength data (see the skyserver project  for more information). While seemingly simple, the problem is complicated by the several factors.

First is the sheer size of the problem, as the cross-identification of billions of sources in both a static and dynamic state over thousands of square degrees in a multi-wavelength domain (Radio to X-Ray) is clearly a computationally challenging problem, even for a consolidated archive. The problem is further complicated by the fact that observational data is always limited by the available technology, which varies greatly in sensitivity and angular resolution as a function of wavelength ( e.g.,  optical-infrared resolution is generally superior to high energy resolution).

Furthermore, the quality of the data calibration (either spectral, temporal, or spatial) can also vary greatly, making it extremely difficult to to unambiguously match sources between different wavelength surveys. Finally, the sky looks different at different wavelengths (see, e.g.,  Figure [\ref=crab]), which can produce one-to-one, many-to-one, one-to-one, many-to-many, and even one/many-to-none scenarios when federating multiwavelength datasets. As a result, sometimes the source associations must be made using probabilistic methods [\citep=lonsdale98] [\citep=rutledge2k].

Data Mining and Knowledge Discovery

Key to maximizing the knowledge extracted from the ever-growing quantities of astronomical (or any other type of) data is the successful application of data-mining and knowledge discovery techniques. This effort as a step towards the development of the next generation of science analysis tools that redefine the way scientists interact and extract information from large data sets, here specifically the large new digital sky survey archives, which are driving the need for a virtual observatory (see, e.g.,  Figure [\ref=param-space] for an illustration).

Such techniques are rather general, and should find numerous applications outside astronomy and space science. In fact, these techniques can find application in virtually every data-intensive field. Here we briefly outline some of the applications of these technologies on massive datasets, namely, unsupervised clustering, other Bayesian inference and cluster analysis tools, as well as novel multidimensional image and catalog visualization techniques. Examples of particular studies may include:

Various classification techniques, including decision tree ensembles and nearest-neighbor classifiers to categorize objects or clusters of objects of interest. Do the objectively found groupings of data vectors correspond to physically meaningful, distinct types of objects? Are the known types recovered, and are there new ones? Can we refine astronomical classifications of object types ( e.g.,  the Hubble sequence, the stellar spectral types) in an objective manner?

Clustering techniques, such as the expectation maximization (EM) algorithm with mixture models to find groups of interest, to come up with descriptive summaries, and to build density estimates for large data sets. How many distinct types of objects are present in the data, in some statistical and objective sense? This would also be an effective way to group the data for specific studies, e.g.,  some users would want only stars, others only galaxies, or only objects with an IR excess, etc.

Use of genetic algorithms to devise improved detection and supervised classification methods. This would be especially interesting in the context of interaction between the image (pixel) and catalog (attribute) domains.

Clustering techniques to detect rare, anomalous, or somehow unusual objects, e.g.,  as outliers in the parameter space, to be selected for further investigation. This would include both known but rare classes of objects, e.g.,  brown dwarfs, high-redshift quasars, and also possibly new and previously unrecognized types of objects and phenomena.

Use of semi-autonomous AI or software agents to explore the large data parameter spaces and report on the occurrences of unusual instances or classes of objects. How can the data be structured to allow for an optimal exploration of the parameter spaces in this manner?

Effective new data visualization and presentation techniques, which can convey most of the multidimensional information in a way more easily grasped by a human user. We could use three graphical dimensions, plus object shapes and coloring to encode a variety of parameters, and to cross-link the image (or pixel) and catalog domains.

Notice that the above examples are moving beyond merely providing assistance with handling of huge data sets: these software tools may become capable of independent or cooperative discoveries, and their application may greatly enhance the productivity of practicing scientists.

It is quite likely that many of the advanced tools needed for these tasks already exist or be under development in the various fields of computer science and statistics. In creating a virtual observatory, one of the most important requirements is to bridge the gap between the disciplines, and introduce modern data management and analysis software technologies into astronomy and astrophysics.

Applied Unsupervised Classification

Some preliminary and illusory experiments using Bayesian clustering algorithms were designed to classify objects present in the DPOSS catalogs [\citep=decarvalho95] using the AutoClass software [\citep=cheeseman88]. The program was able separate the data into four recognizable and astronomically meaningful classes: stars, galaxies with bright central cores, galaxies without bright cores, and stars with a visible "fuzz" around them. Thus, the object classes found by AutoClass are astronomically meaningful--even though the program itself does not know about stars, galaxies and such! Moreover, the two morphologically distinct classes of galaxies populate different regions of the data space, and have systematically different colors and concentration indices, even though AutoClass was not given the color information. Thus, the program has found astrophysically meaningful distinction between these classes of objects, which is then confirmed by independent data.

One critical point in constructing scientifically useful object catalogs from sky surveys is the classification of astronomical sources into either stars or galaxies. Various supervised classification schemes can be used for this task, including decision trees [\citep=weir95] or neural nets [\citep=odewahn92]. A more difficult problem is to provide at least rough morphological types for the galaxies detected, in a systematic and objective way, without visual inspection of the images, which is obviously impractical. This actually provides an interesting opportunity--the application of new clustering analysis and unsupervised classification techniques may divide the parent galaxy population into astronomically meaningful morphological types on the basis of the data themselves, rather than some preconceived, human-imposed scheme.

Another demonstration of the utility of these techniques can be seen in Figure [\ref=emc]. In this experiment, the Expectation Maximization technique was applied on a star-galaxy training data set of approximately 11,300 objects with 15 parameters each. This is an unsupervised classification method which fits a number of multivariate Gaussians to the data, and decides on the optimal number of clusters needed to describe the data. Monte-Carlo cross validation was used to decide on the optimal number of clusters [\citep=smyth00]. The program found that there are indeed two dominant classes of objects, viz., stars and galaxies, containing about 90% of all objects, but that there are also a half-dozen other significant clusters, most of which correspond to legitimate subclasses such as saturated stars, etc.  Again, this illustrates the potential of unsupervised classification techniques for objective partitioning of data, identification of artifacts, and possibly even discovery of new classes of objects.

Analyzing Large, Complex Datasets

Most clustering work in the past has only been applied to small data sets. The main reasons for this are due to memory storage and processing speed. With orders of magnitude improvement in both, we can now begin to contemplate performing clustering on the large scale. However, clustering algorithms have high computational complexity (from high polynomial, order 3 or 4, to exponential search). Hence a rewriting of these algorithms, shifting the focus from performing expensive searches over small data sets, to robust (computationally cheap) estimation over very large data sets is in order.

The reason we need to use large data sets is motivated by the fact that new classes to be discovered in the data are likely to be rare occurrences (else humans would have surely found them). For example, objects like quasars (extremely luminous, very distant sources that are believed to be powered by supermassive black holes) constitute a tiny proportion of the huge number of objects detectable in our survey, yet they are an extremely important class of objects. Unknown types (classes) of objects that may potentially be present in data are likely to be as rare. Hence, if an automated discovery algorithm is to have any hope of finding them, it must be able to process a huge amount of data, millions of objects or more.

Current clustering algorithms simply cannot run on more than a few thousand cases in less than 10-dimensional space, without requiring weeks of CPU time.

Many clustering codes ( e.g.,  AutoClass) are written to demonstrate the method, and are ill-suited for data sets containing millions or billions of data vectors in tens of dimensions. Improving the efficiency of these algorithms as the size and complexity of the datasets is increased is an important issue.

With datasets of this size and complexity, multi-resolution clustering is a must. In this regime, expensive parameters to estimate, such as the number of classes and the initial broad clustering are quickly estimated using traditional techniques like K-means clustering or other simple distance-based method [\citep=duda81]. With such a clustering one would proceed to refine the model locally and globally. This involves iterating over the refinements until some objective (like a Bayesian criterion) is satisfied.

Intelligent sampling methods where one forms "prototypes"of the case vectors and thus reduces the number of cases to process. Prototypes can be determined based on nearest-neighbor type algorithms or K-means to get a rough estimate, then more sophisticated estimation techniques can refine this. A prototype can represent a large population of examples. A weighting scheme based on number of cases represented by each prototype, as well as variance parameters attached to the feature values assigned to the prototype based on values of the population it represents, are used to describe them. A clustering algorithm can operate in prototype space. The clusters found can later refined by locally replacing each prototype by its constituent population and reanalyzing the cluster.

Techniques for dimensionality reduction, including principal component analysis and others can be used as preprocessing techniques to automatically derive the dimensions that contain most of the relevant information. See, e.g.,  the singular-valued decomposition scheme to find the eigenvectors dominant in the data set in a related application involving finding small volcanos in Magellan images of Venus [\citep=fayyad95].

Scientific verification and evaluation, testing, and follow-up on any of the newly discovered classes of objects, physical clusters discovered by these methods, and other astrophysical analysis of the results. This is essential in order to demonstrate the actual usefulness of these techniques for the NVO.

Scientific Verification

Testing of these techniques in a real-life data environment, on a set of representative science use cases, is essential to validate and improve their utility and functionality. Some of the specific scientific verification tests may include:

A novel and powerful form of the quality control for our data products, as multidimensional clustering can reveal subtle mismatch patterns between individual sky survey fields or strips, e.g.,  due to otherwise imperceptible calibration variations. This would apply to virtually any other digital sky survey or other patch-wise collated data sets. Assured and quantified uniformity of digital sky surveys data is essential for many prospective applications, e.g.,  studies of the large-scale structure in the universe, etc.

A new, objective approach to star-galaxy separation could overcome the restrictions of the current accuracies of star-galaxy classifications that effectively limits the scientific applications of any sky survey catalog. Related to this is an objective, automated, multi-wavelength approach to morphological classification of galaxies, e.g.,  quantitative typing along the Hubble sequence, or one of the more modern, multidimensional classification schemes.

An automated search for rare, but known classes of objects, through their clustering in the parameter space. Examples include high-redshift quasars, brown dwarfs, or ultraluminous dusty galaxies.

An automated search for rare and as yet unknown classes of astronomical objects or phenomena, as outliers or sparse clusters in the parameter space, not corresponding to the known types of objects. They would be found in a systematic way, with fully quantifiable selection limits.

Objective discovery of clusters of stars or galaxies in the physical space, by utilizing full information available in the surveys. This should be superior to most of the simple density-enhancement type algorithms now commonly used in individual surveys.

A general, unbiased, multiwavelength search for AGN, and specifically a search for the long-sought population of Type 2 quasars. A discovery of such a population would be a major step in our understanding of the unification models of AGN, with consequences for many astrophysical problems, e.g.,  the origins of the cosmic x-ray background.

This clustering analysis would be performed in the (reduced) measurement space of the catalogs. But suppose the clustering algorithm picks out a persistent pattern, e.g.,  a set of objects, that for reasons not obvious to human from the measurements, are consistently clustered separately from the data. The next step is for the astronomer to examine actual survey images to study this class further to verify discovery or explain scientifically why the statistical algorithms find these objects different.

Some enhanced tools for image processing, in particular, probabilistic methods for segmentation (region growing) that are based both on pixel value, adjacency information, and the prior expectation of the scientist will need to be used to aid in analysis and possibly overcome some loss of information incurred when global image processing was performed.

There are also potential applications of interest for the searches for Earth-crossing asteroids, where a substantial portion of the sky would be covered a few times per night, every night. The addition of the time dimension in surveys with repeated observations such as these, would add a novel and interesting dimension to the problem. While variable objects obviously draw attention to themselves ( e.g.,  supernovae, gamma-ray bursts, classical pulsating variables, etc.), the truth is that we know very little about the variability of the deep sky in general, and a systematic search for variability in large and cross-wavelength digital sky archives is practically guaranteed to bring some new discoveries.

Astronomical Data Visualization

Effective and powerful data visualization would be an essential part of any virtual observatory. The human eye and brain are remarkably powerful in pattern recognition, and selection of interesting features. The technical challenge here is posed by the sheer size of the datasets (both in the image and catalog domains), and the need to move through them quickly and to interact with them "on the fly".

The more traditional aspect of this is the display of various large-format survey images. One of the new challenges is in streaming of the data volume itself, now already in the multi-TB range for any given survey. A user may need to shift quickly through different spatial scales ( i.e.  zoom in or out) on the display, from the entire sky down to the resolution limit of the data, spanning up to a factor of approximately 1011 (!) in solid angle coverage. Combining the image data from different surveys with widely different spatial resolutions poses additional challenges. So does the co-registration of images from different surveys where small, but always present systematic distortions in astrometric solutions must be corrected before the images are overlaid.

Another set of challenges is presented by displaying the information in the parameter spaces defined in the catalog domain, where each object may be represented by a data vector in tens or even hundreds of dimensions, but only a few can be displayed at any given time ( e.g.,  3 spatial dimensions, color, shape, and intensity for displayed objects). Each of the object attributes, or any user defined mathematical combination of object attributes ( e.g.,  colors) should be encodeable on demand as any of the displayed dimensions. This approach will also need to be extended to enable the display of data from more than one survey at a time, and to combine object attributes from matched catalogs.

However, probably the most interesting and novel aspect is the combination and interaction between the image and catalog domains. This is only becoming possible now, due to the ability to store multi-TB data sets on line, and it opens a completely new territory. In the simplest approach this would involve marking or overplotting of sources detected in one survey, or selected in some manner, e.g.,  in clustering analysis, on displayed images.

In the next level of functionality, the user would be able to mark the individual sources or delineate areas on the display, and retrieve the catalog information for the contained sources from the catalog domain (see, e.g.,  Figure [\ref=proto-vis] for a demonstration). Likewise, it may be necessary to remeasure object parameters in the pixel domain and update or create new catalog entries. An example may be measuring of low-level signals or upper limits at locations where no statistically significant source was cataloged originally, but where a source detection is made in some other survey, e.g.,  faint optical counterparts of IR, radio, or x-ray sources. An even more sophisticated approach may involve development of new object classifiers through interaction of catalog and image domains, e.g.,  using genetic algorithms.

Visualization of these large digital sky surveys is also a powerful education and public outreach tool. An example of this is the virtual sky project (see Figure [\ref=vo] for the project homepage).

Summary

We are at the start of a new era of information-rich astronomy. Numerous ongoing sky surveys over a range of wavelengths are already generating data sets measured in the tens of Terabytes. These surveys are creating catalogs of objects (stars, galaxies, quasars, etc.) numbering in billions, with tens or hundreds of measured numbers for each object. Yet, this is just a foretaste of the much larger data sets to come, with multi-Petabyte data sets already on the horizon. Large digital sky surveys and data archives are thus becoming the principal sources of data in astronomy. The very style of observational astronomy is changing: systematic sky surveys are now used both to answer some well-defined questions which require large samples of objects, and to discover and select interesting targets for follow-up studies with space-based or large ground-based telescopes.

This vast amount of new information about the universe will enable and stimulate a new way of doing astronomy. We will be able to tackle some major problems with an unprecedented accuracy, e.g.,  mapping of the large-scale structure of the universe, the structure of our Galaxy, etc.  The unprecedented size of the data sets will enable searches for extremely rare types of astronomical objects ( e.g.,  high-redshift quasars, brown dwarfs, etc.) and may well lead to surprising new discoveries of previously unknown types of objects or new astrophysical phenomena. Combining surveys done at different wavelengths, from radio and infrared, through visible light, ultraviolet, and x-rays, both from the ground-based telescopes and from space observatories, would provide a new, panchromatic picture of our universe, and lead to a better understanding of the objects in it. These are the types of scientific investigations which were not feasible with the more limited data sets of the past.

Many individual digital sky survey archives, servers, and digital libraries already exist, and represent essential tools of modern astronomy. We have reviewed some of them, and there are many others existing and still under development. However, in order to join or federate these valuable resources, and to enable a smooth inclusion of even greater data sets to come, a more powerful infrastructure and a set of tools are needed.

The concept of a virtual observatory thus emerged, including the incipient National Virtual Observatory (NVO), and its future global counterparts. A virtual observatory would be a set of federated, geographically distributed, major digital sky archives, with the software tools and infrastructure to combine them in an efficient and user-friendly manner, and to explore the resulting data sets whose sheer size and complexity are beyond the reach of traditional approaches. It would help solve the technical problems common to most large digital sky surveys, and optimize the use of our resources.

This systematic, panchromatic approach would enable new science, in addition to what can be done with individual surveys. It would enable meaningful, effective experiments within these vast data parameter spaces. It would also facilitate the inclusion of new massive data sets, and optimize the design of future surveys and space missions. Most importantly, the NVO would provide access to powerful new resources to scientists and students everywhere, who could do first-rate observational astronomy regardless of their access to large ground-based telescopes. Finally, the NVO would be a powerful educational and public outreach tool.

Technological challenges inherent in the design and implementation of the NVO are similar to those which are now being encountered in other sciences, and offer great opportunities for multi-disciplinary collaborations. This is a part of the rapidly changing, information-driven scientific landscape of the new century.