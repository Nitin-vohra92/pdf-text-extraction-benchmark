Multimodal Transitions for Generative Stochastic Networks

Introduction

A new approach to unsupervised learning of probabilistic models has recently been proposed [\citep=Bengio-et-al-NIPS2013-small] [\citep=Bengio+Laufer-arxiv-2013], called Generative Stochastic Networks (GSNs), based on learning the operator (transition distribution) of a Markov chain that generates samples from the learned distribution. In Section [\ref=sec:gsn], GSNs are formally defined and mathematical results are provided on the consistency achieved when training GSNs with a denoising criterion. Denoising had previously been proposed as a criterion for unsupervised feature learning of denoising auto-encoders [\citep=VincentPLarochelleH2008-small] [\citep=Vincent-JMLR-2010-small], one of the successful building blocks for deep learning [\citep=Bengio-2009-book]. The motivation for GSNs is that the transition distribution of a Markov chain, given the previous state, is generally simpler than the stationary distribution of the Markov chain, i.e., it has a simpler structure and less major modes (the stationary distribution can be viewed as a huge mixture, over all states, of these transition distributions). This could make it easier to learn GSNs because one factor in the difficulty of training probabilistic models is the complexity of approximating their normalizing constant (partition function). If a distribution has fewer major modes or less complicated structure in them (in the extreme, if it is factorized or unimodal), then its partition function can be computed or approximated more easily. However, previous work with GSNs has focused only on the extreme case where the transition distribution is parametrized by a factorized or unimodal distribution. This paper starts by reminding the reader of good reasons for unsupervised learning of generative models (Section [\ref=sec:unsupervised]) and two of the fundamental challenges involved, namely the difficulty of mixing between modes and presence of a potentially very large number of non-tiny modes. It argues in favor of GSNs with multi-modal transition distributions (Section [\ref=sec:multimodal]) to address the second problem and proposes (Section [\ref=sec:NADE]) to use conditional NADE [\citep=Larochelle+Murray-2011] [\citep=Boulanger+al-ICML2012-small] models (similar to conditional RBMs [\citep=Taylor+2007] [\citep=TaylorHintonICML2009] but with a tractable likelihood) to represent multi-modal output distributions for these transition operators. Experiments are performed with denoising auto-encoder neural networks, which were originally proposed as simple implementations of GSNs (Section [\ref=sec:exp]). The results on both artificial (2D for easy visualization) and real data (MNIST) clearly show that multimodality helps GSNs to better capture the data generating distribution, reducing spurious modes by allowing the transition operator to not be forced to put probability mass in the middle of two or more major modes of the true transition distribution.

Promises and Challenges of Unsupervised Learning of Generative Models

Unsupervised learning remains one of the core challenges for progress in deep learning algorithms, with current applications of deep learning mostly based on supervised deep nets. Unsupervised learning could hold the key to many advantages for which purely supervised deep learning is inadequate:

It allows a learner to take advantage of unlabeled data. Most of the data available to machines (and to humans and animals) is unlabeled, i.e., without a precise and symbolic characterization of its semantics and of the outputs desired from a learner.

It allows a learner to capture enough information about the observed variables so as to be able to answer new questions about them in the future, that were not anticipated at the time of seeing the training examples.

It has been shown to be a good regularizer for supervised learning [\citep=Erhan+al-2010], meaning that it can help the learner generalize better, especially when the number of labeled examples is small. This advantage clearly shows up (e.g., as illustrated in the transfer learning competitions with unsupervised deep learning [\cite=UTLC+DL+tutorial-2011-small] [\cite=UTLC+LISA-2011-small] [\cite=Goodfellow+all-NIPS2011]) in practical applications where the distribution changes or new classes or domains are considered (transfer learning, domain adaptation), when some classes are frequent while many others are rare (fat tail or Zipf distribution), or when new classes are shown with zero, one or very few examples (zero-shot and one-shot learning  [\citep=Larochelle2008] [\citep=Lake-et-al-NIPS2013]).

There is evidence suggesting that unsupervised learning can be successfully achieved mostly from a local training signal (as indicated by the success of the unsupervised layer-wise pre-training procedures [\citep=Bengio-2009-book], semi-supervised embedding [\citep=WestonJ2008-small], and intermediate-level hints [\citep=Gulcehre+Bengio-ICLR2013]), i.e., that it may suffer less from the difficulty of propagating credit across a large network, which has been observed for supervised learning.

A recent trend in machine learning research is to apply machine learning to problems where the output variable is very high-dimensional (structured output models), instead of just a few numbers or classes. For example, the output could be a sentence or an image. In that case, the mathematical and computational issues involved in unsupervised learning also arise.

What is holding us back from fully taking advantage of all these advanges? We hypothesise that this is mainly because unsupervised learning procedures for flexible and expressive models based on probability theory and the maximum likelihood principle or its close approximations [\citep=MurphyBook2012] are almost all facing the challenge of marginalizing (summing, sampling, or maximizing) across many explanations or across many configurations of random variables. This is also true of structured output models, or anytime we care about capturing complicated joint probabilistic relationships between many variables. This is best exemplified with the most classical of the deep learning algorithms, i.e., Boltzmann machines [\citep=Ackley85] with latent variables. In such models, one faces the two common intractable sums involved when trying to compute the data likelihood or its gradient (which is necessary for learning), or simply to use the trained model for new inputs:

The intractable sum or maximization over latent variables, given inputs, which is necessary for inference, i.e., when using the model to answer questions about new examples, or as part of the training procedure.

The intractable sum or maximization over all the variables (the observed ones and the latent ones), which is necessary for learning, in order to compute or estimate the gradient of the log-likelihood, in particular the part due to the normalization constant of the probability distribution. This intractable sum is often approximated with Monte-Carlo Markov Chain (MCMC) methods, variational methods, or optimization methods (the MAP or Maximum A Posteriori approximation).

In principle, MCMC methods should be able to handle both of these challenges. However, MCMC methods can suffer from two fundamental issues, discussed in [\citet=Bengio+Laufer-arxiv-2013] in more detail:

Mixing between well-separated modes. A mode is a local maximum of probability (by extension, we refer to a connected region of high probability). According to the manifold hypothesis [\citep=Cayton-2005] [\citep=Narayanan+Mitter-NIPS2010-short], high-probability modes are separated by vast areas of low probability. Unfortunately, this makes it exponentially difficult for MCMC methods to travel between such modes, making the resulting Monte-Carlo averages unreliable (i.e., of very high variance).

A potentially huge number of large modes. It may be the case that the number of high-probability modes is very large (up to exponentially large), when considering problems with many explanatory factors. When this is so, the traditional approximations based on MCMC, variational methods, or MAP approximations could all fall on their face. In particular, since MCMC methods can only visit one mode at a time, the resulting estimates could be highly unreliable.

These are of course hypotheses, they need to be better tested, and may be more or less severe depending on the type of data generating distribution being estimated. The work on GSNs aims at giving us a tool to test these hypotheses by providing algorithms where the issue of normalization constant can be greatly reduced.

Generative Stochastic Networks

A very recent result [\citep=Bengio-et-al-NIPS2013-small] [\citep=Bengio+Laufer-arxiv-2013] shows both in theory and in experiments that it is possible to capture the data generating distribution with a training principle that is completely different from the maximum likelihood principle. Instead of directly modeling P(x) for observed variables x, one learns the transition operator of a Markov chain whose stationary distribution estimates P(x). The idea is that a conditional distribution of a Markov chain typically makes only "small" or "local" moves, making the conditional distribution simpler in the sense of having fewer dominant modes. The surprising result is that there is at least one simple training criterion for such a transition operator, based on probabilistically undoing the perturbations introduced by some noise source. The transition operator of the Markov chain, P(xt,ht|xt - 1,ht - 1) (with observed variable x and latent state variables h) can be trained by using a denoising objective, in which xt - 1 = x is first mapped into a learned f(xt - 1,ht - 1,zt) (with a noise source zt) that destroys information about x, and from which the clean data point x is predicted and its probability P(xt = x|f(xt - 1 = x,ht - 1,zt)) approximately maximized. Since the corruption only leaves a relatively small number of configurations of x as probable explanations for the computed value of f(xt - 1 = x,ht - 1,zt), the reconstruction distribution P(xt = x|f(xt - 1 = x,ht - 1,zt)) can generally be approximated with a much simpler model (compared to P(x)). In a sense, this is something that we are directly testing in this paper.

The GSN framework therefore only addresses the issue of a potentially huge number of modes (problem 2, above). However, by training GSNs with deep representations, one can hope to take advantage of the recently observed superiority of trained representations (such as in deep auto-encoders or deep belief nets and deep Boltzmann machines) in terms of faster mixing between well separated modes [\citep=Bengio-et-al-ICML2013].

The core mathematical result justifying this procedure is the following theorem, proven in [\citet=Bengio+Laufer-arxiv-2013], and stating sufficient conditions for the denoising criterion to yield a consistent estimator of the data generating distribution, as the stationary distribution of the Markov chain:

Let training data X  ~  P(X) and independent noise Z  ~  P(Z) and introduce a sequence of latent variables H defined iteratively through a function f with Ht  =  fθ1(Xt - 1,Zt - 1,Ht - 1) for a given sequence of Xt's. Consider a model Pθ2(X|fθ1(X,Zt - 1,Ht - 1)) trained (over both θ1 and θ2) so that Pθ2(X|H), for a given θ1, is a consistent estimator of the true P(X|H). Consider the Markov chain defined above and assume that it converges to a stationary distribution πn over the X and H and with marginal πn(X), even in the limit as the number of training examples n  →    ∞  . Then πn(X)  →  P(X) as n  →    ∞  .

A particularly simple family of GSNs is the denoising auto-encoder, studied in [\cite=Bengio-et-al-NIPS2013-small] as a GSN, i.e., as a generative model. In that case, Ht = fθ1(Xt - 1,Zt - 1,Ht - 1) is just the parameter-less corruption of Xt - 1 according to a predetermined corruption process such as adding Gaussian noise or multiplying by masking noise, also known as dropout (and Ht does not depend on a previous Ht - 1).

In  [\citet=Bengio+Laufer-arxiv-2013], this approach is used to successfully train an unsupervised architecture mimicking the structure of a Deep Boltzmann Machine. In [\citet=Goodfellow-et-al-NIPS2013] the same principle is used to provide a successful sampling scheme for the Multi-Prediction Deep Boltzmann Machine (MP-DBM), when its training procedure is viewed as a particular form of GSN. Note that the MP-DBM was shown to be particularly successful at classification tasks, in spite of being trained with both the image and class being treated as visible variables in a single integrated training framework, unlike previous approaches for training DBMs [\citep=Salakhutdinov2009]. Much more work is needed to better understand the GSN approach and expand it to larger models, structured output problems, and applications, and more importantly, to explore the space of architectures and models that can be trained by this novel principle. In this paper, we focus on one particular aspect of research on GSNs, namely the need for multimodality of the transition distribution, the one that computes the probability of the next visible state given the previous state, P(xt|f(xt - 1,ht - 1,zt)).

The Need for Multimodality of the Transition Distribution

Close inspection of Theorem [\ref=thm:gsn] reveals that if the "true" data generating distribution requires P(xt = x|f(xt - 1 = x,ht - 1,zt)) to be multimodal, then that capability is required of our parametrization of the transition probability in order to get consistency. Otherwise we only get consistency in the family of functions that can be represented by such unimodal transition operators. We already know from a mathematical result in [\citet=Alain+Bengio-ICLR2013] that when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density suffices to fully capture the joint distribution. The price to pay for this easier parametrization of the conditional distribution is that the associated Markov chain would mix very slowly, making it much less useful as a generative model. At the other extreme, if the amount of corruption noise is "infinite" (destroying all information about the previous state), then P(xt = x|f(xt - 1 = x,ht - 1,zt)) = P(xt = x) and we are back to a normal probabilistic model, which generally has a lot of modes.

The advantage of multimodal GSNs such as discussed in this paper is that they allow us to explore the realm in between these extremes, where the transition operator is multimodal but yet the number of such modes remains small, making it relatively easy to learn the required conditional distributions. This is illustrated in Figure [\ref=fig:multimodal-example], which shows an example where having a unimodal reconstruction distribution (factorial GSN, i.e., a regular denoising auto-encoder with factorized Gaussian or factorized Bernoulli output units) would yield a poor model of the data, with spurious samples generated in between the arms of the spiral near which the data generating distribution concentrates.

NADE and GSN-NADE Models

The Neural Auto-regressive Density Estimator (NADE) family of probability models [\citep=Larochelle+Murray-2011] can capture complex distributions (more so by increasing capacity, just like other neural nets or mixture models) while allowing the analytic and tractable computation of the likelihood. It is a descendant of previous neural "auto-regressive" neural networks [\citep=Bengio+Bengio-NIPS99], also based on decomposing [formula] into [formula], with a group of output units for each variable i and an architecture that prevents input xj to enter into the computation of [formula] when j  ≥  i. Whereas the models of [\citet=Bengio+Bengio-NIPS99] [\citet=Larochelle+Murray-2011] were developed for discrete variables, NADE was recently extended [\citep=Benigno-et-al-NIPS2013-small] to continuous variables by making [formula] a mixture of k Gaussians (whose means and variances may depend on [formula]).

Like other parametric models, NADE can be used as the output distribution of a conditional probability. This has already been done in the context of modeling musical sequences [\citep=Boulanger+al-ICML2012-small], with the outputs of a recurrent neural networks being the bias parameters of a NADE model capturing the conditional distribution of the next frame (a set of musical notes), given previous frames (as captured by the state of the recurrent neural network). In this paper we propose to use a NADE or RNADE model to capture the output distribution of a transition operator for the Markov chain associated with a GSN. More specifically, the experiments performed here are for the case of a denoising auto-encoder, i.e., a neural network takes the corrupted previous state [formula] as input and attempts to denoise it probabilistically, i.e., it outputs the parameters of a NADE model (here, again the hidden and output layer biases) associated with the distribution of the next state (i.e. the clean input X), given [formula]. The weights of the NADE model are kept unconditional: because these weight matrices can be large, it makes more sense to try to only condition the biases, like has been done before with conditional RBMs [\citep=Taylor+2007] [\citep=TaylorHintonICML2009].

Experiments

In order to demonstrate the advantages of using a multimodal reconstruction distribution over the unimodal one, we firstly report experimental results on a 2D real-valued dataset. Two types of GSNs are trained and then used to generate samples, shown in Figure [\ref=fig:multimodal-example]. Both GSN-NADE and fatorial GSN use Gaussian corruption p(|X) = X  +  N(0,σ). To make the comparison fair, both models use σ = 0.3. When the added noise is significantly large, using a multimodal reconstruction distribution p(X|) is particularly advantageous, as is clearly demonstrated in the Figure. The samples generated by the GSN-NADE model resemble closely to the original training examples.

The second set of experiments are conducted on MNIST, the handwritten digits dataset. The real-valued pixels are binarized with the threshold 0.5. A 784-2000-2000-784 NADE model is chosen as the reconstruction distribution of the GSN-NADE model. The biases of the first 2000 hidden units of the NADE network are outputs of a 784-2000 neural network, which has [formula] as the input. The training of model uses the same procedure as in [\citep=Benigno-et-al-arxiv-2013]. In addition, a dynamic noise is added on input pixels: each training example is corrupted with a salt-and-pepper noise that is uniformly sampled between 0 and 1. We find in practice that using dynamic noise removes more spurious modes in the samples. To evaluate the quality of trained GSNs as generative models, we adopt the Conservative Sampling-based Log-likelihood (CSL) estimator proposed in  [\citep=Bengio+Yao-arxiv-2013]. The CSL estimates are computed on different numbers of consecutive samples, all starting from a random intialization, as opposed to computing the estimates based on samples collected every 100 steps originally reported in  [\citep=Bengio+Yao-arxiv-2013]. The detailed comparison is shown in Table [\ref=tab:CSL]. In the table, GSN-NADE denotes the model we propose in this work. The GSN-1-w is a GSN with factorial reconstruction distribution trained with the walkback procedure proposed in [\citet=Bengio-et-al-NIPS2013-small]. GSN-1 denotes a GSN with a factorial reconstruction distribution with no walkback training. For the details of models being compared, please refer to  [\citep=Bengio+Yao-arxiv-2013]. Figure [\ref=mnist:dae], Figure [\ref=mnist:gsn-walkback] and Figure [\ref=mnist:gsn-nade] show collections of consecutively generated samples from all 3 types of models after training.

Experiments confirm that the GSN model with a NADE output distribution does a much better job than the GSN model with a factorized output distribution on learning and generating samples for the real-valued 2 dimensional spiral manifold (Figure [\ref=fig:multimodal-example]). On MNIST, the differences between generated samples are also visually striking. As demonstrated in the picture, a factorial GSN without the help of the walkback training (Figure [\ref=mnist:dae]) tends to generate sharper digits but suffers from often getting stuck in the non-digit-like spurious modes. On the contrary, a GSN-NADE without the walkback training (Figure [\ref=mnist:gsn-nade]) alleviates this issue. In addition, GSN-NADE mixes much better among digits, It generates samples that are visually much better than the rest two factorial models. To show that GSN-NADE mixes better, we also computed the CSL estimate of the same GSN-NADE reported in Table [\ref=tab:CSL] with 10k samples but all of which are collected every 100 steps. Surprisingly, the CSL estimate remains exactly the same as the one where samples are collected after every step of the Markov chain (shown in Table [\ref=tab:CSL]). Furthermore, as shown in Figure [\ref=mnist:gsn-nade], its samples present more diversity in writing styles, directly resulting a significantly better CSL estimate (CSL estimates are in log scale) than its factorial counterpart. GSN-NADE, however, is not able to outperform the factorial GSN trained with the walkback training (Figure [\ref=mnist:gsn-walkback]) in terms of the CSL estimates. This is because factorial GSNs trained with walkbacks win by suppressing almost all the spurious modes, resulting in higher CSL estimates of the testset log-likelihood (CSL prefers blury digits to spurious modes).

Experimental setup

2D spiral

1D manifold in 10D

MNIST

Training procedures

Experimental results

Conclusion

In this paper we have motivated a form of generative stochastic networks (GSNs) in which the next-state distribution is multimodal and we have proposed to use conditional NADE distributions to capture this multimodality. Experiments confirm that this allows the GSN to more easily capture the data distribution, avoiding the spurious samples that would otherwise be generated by a unimodal (factorial) output distribution (the kind that has been used in the past for GSNs and denoising auto-encoders).

In the MNIST experiments, we found that the benefit of the walkback procedure was actually greater than the benefit of the multimodality brought by the NADE output distribution. How about combining both NADE and walkback training? The basic procedure for sampling from NADE is expensive because it involves sampling each input variable given all the previous ones, each time having to run through the NADE neural net. Applying the walkback training procedure to the GSN-NADE models therefore poses a computational problem because the walkback procedure involves sampling a few steps from the chain in the middle of the training loop (in a way similar to contrastive divergence training): whereas getting the likelihood gradient is fast with NADE (order of number of inputs times number of hidden units), sampling from NADE is much more expensive (multiply that by the number of inputs). Future work should therefore investigate other multimodal distributions (for which sampling is cheap) or ways to approximate the NADE sampling procedure with faster procedures.