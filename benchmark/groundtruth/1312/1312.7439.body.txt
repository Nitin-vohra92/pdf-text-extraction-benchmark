6.5 in 9.1 in 0.0 in 0.0 in 0.0 in = 0.0 in 0.0 in 1mm

Corollary

Distribution-free factor analysis -- Estimation theory and applicability to high-dimensional data.

Key words: EFA; FA; fixed point iterations; likelihood equations; more variables than observations; SVD.

Introduction

In this paper we consider parameter estimation in a distribution-free version of the standard (Gaussian) factor analysis (FA) model, with special emphasis on the case of more variables than observations. The FA model means describing a sample [formula] of p-dimensional vectors as

[formula]

Here μ is the mean value vector, Λ is a p  ×  k coefficients (loadings) matrix, k <  min (n,p), and the fis are mutually independent latent k-vectors (factor scores), standardized to zero mean and unit covariance matrix Ik (for identifiability). The eis are assumed mutually independent p-vectors with uncorrelated components and diagonal covariance matrix Ψ2. Also, fi and ei should be mutually independent. In matrix form we write ([\ref=eq:Modelforx]) as [formula], with the vectors of ([\ref=eq:Modelforx]) as rows.

Usually, normality of f and e in ([\ref=eq:Modelforx]) is assumed, and more observations than variables, that is n > p. Then Gaussian maximum likelihood methods can be used, and are more or less standard. However, in recent years interest has increased both in more robust methods and in methods for the case of more variables than observations, p > n. Among papers having appeared after the comprehensive review by Bartholomew & Knott (1999, ch. 3), we mention Robertson & Symons (2007), who study extension of Gaussian maximum likelihood to the case p > n, and a number of papers by Trendafilov and Unkel, in particular Trendafilov & Unkel (2011) and Unkel & Trendafilov (2010a&b), also dealing with the case p > n but proposing alternative models and estimation methods. Trendafilov & Unkel (2011) appear skeptical to the results of Robertson & Symons (2007), and proclaim that when p > n the model assumption of the latter, that Ψ2 is positive definite, is inconsistent with their own model for data. That is certainly right, and we argue below (Sec. 6) that the model for data used by Trendafilov & Unkel is artificial and unrealistic.

Our main aim, however, is to show that the fitting of models of type ([\ref=eq:Modelforx]) in the case of large p is not problematic, and that in any case there is no need to assume normality. We will first derive some basic distribution-free properties of model ([\ref=eq:Modelforx]). These are expressed in a normalization of the x-components by Ψ, shown to be suitable for our purpose. It will turn out without difficulties that these properties lead to estimating equations that are the same as the well-known likelihood equations for n > p, thus yielding distribution-free support to the normality-based MLE.

Another well-known technique for dimension reduction is principal components analysis (PCA). PCA aims at describing as much as possible of Σxx by a number of principal components (PCs, linear forms in x). There is no model behind PCA, but sometimes the PCs are regarded as representing latent variables in a different, less well-defined way. PCA techniques also have a role in factor analysis. Due to its scale-dependence, the choice of scaling is important.

In the very special case when the error e vanishes, i.e. Ψ2 = 0 in ([\ref=eq:Sigmaxx]), ΛΛT can be determined by a PCA on Σxx, or estimated by a PCA on the sample covariance matrix Sxx (or an SVD on the x-data matrix itself). Similarly, if Ψ2 were not zero but regarded as known, we could subtract it from Σxx or Sxx and in this way open for use of PCA. This was the basis for the early Principal Factor Analysis method of fitting the FA model: Use some initial Ψ2 to subtract from Sxx, find PCs yielding an estimate of ΛΛT, use this to calculate a new Ψ2, etc. Such methods were found inefficient and unstable, however. In particular they were not scale invariant, in contrast to Gaussian ML (see Bartholomew & Knott, 1999, Sec. 3.17). From the time when ML methods became computationally feasible and attractive (Jöreskog, 1967, Lawley, 1967), ML estimation has widely replaced the principal factor analysis method.

In the present paper a new distribution-free method for FA model fitting is proposed, that utilizes principal components of a naturally rescaled instead of reduced sample covariance matrix. To our surprise we have not seen this approach in the literature. The methodology has the following properties:

It yields the same equations as Gaussian ML-FA for p < n, and therefore supports the use of these estimation equations even when the Gaussian distribution is questionable;

It is scale invariant in the sense mentioned above;

without problems, it allows more variables than observations (p > n);

It yields estimated or predicted factor scores of high precision when p is large.

The basic model properties to be derived in the next section will naturally lead to estimating equations for distribution-free parameter estimation. Different iterative methods to solve these equations are discussed in Section 3. Use of singular value decompositions (SVD) will not only make the computations fast, but also yield some further insight (Sec. 4). The SVD tool is used in Sec. 5 to yield expressions for factor scores and residuals. These are compared in Sec. 6 with the model properties of Trendafilov & Unkel (2011). Finally, in Sec. 7, the recommended iteration method is successfully tried on gene expression data with p >  > n.

As mentioned above, we assume we have a sample of multivariate x-data xi, [formula], dim (x) = p. We will later assume that the x-sample is mean-standardized, so we need only consider the sample covariance matrix Sxx = XTX / (n - 1) and the corresponding population covariance matrix Σxx. In the next section, we concentrate on Σxx, so the sample size n and its relation to the dimension p will not yet be a question.

A canonical distribution-free introduction to the FA model

For the FA model ([\ref=eq:Modelforx]), the population covariance matrix Σxx (p  ×  p) is

[formula]

There is a rotational ambiguity in the loading parameters of this representation. For uniqueness we will use the same well-known and natural constraint as in the standard Gaussian ML approach:

[formula]

This demand will be equivalent with an assumption that the p  ×  k matrix Ψ- 1Λ has orthogonal columns. Our motivation to make this particular choice will be clear below.

As mentioned in Sec. 1, classical Principal Factor Analysis requires an initial or current estimate of Ψ2 to be subtracted from Sxx, so that ideally we would get ΛΛT. PCA is now used on the resulting reduced covariance matrix Sxx  -  Ψ2. Below we will instead use a rescaled covariance matrix, that will be demonstrated to have much better properties.

Consider rescaling the vector x to z = Ψ- 1x, neglecting for a moment the fact that Ψ is unknown (later we will update Ψ iteratively). This will make all observation components have the same error variance. The total covariance matrix Σzz for a z-vector is

[formula]

where Ip denotes the p  ×  p identity matrix and Λz  =  Ψ- 1Λ is p  ×  k, cf. ([\ref=eq:Sigmaxx]). Because of assumption ([\ref=eq:diagonal]) we know that Λz has orthogonal columns, and it follows that these columns are eigenvectors of the matrix Σzz. In a condensed representation we can write

[formula]

where Ωz is a diagonal k  ×  k matrix with the corresponding eigenvalues as diagonal elements, that is

[formula]

The sum of these k eigenvalues is

[formula]

If k latent factors are both necessary and sufficient for the model to hold, precisely these k eigenvalues of Σzz will be > 1. For a complete set of eigenvectors of Σzz, we need to supplement Λz by p - k vectors spanning the orthogonal complement of the space spanned by Λz. They will all have the eigenvalue 1.

Equation ([\ref=eq:eigenrelation]) does not specify the length of the eigenvectors in Λz. For that reason we also introduce the corresponding set of normalized eigenvectors Φz,

[formula]

which is a p  ×  k matrix of k < p orthonormal eigenvectors. Thus, ΦTzΦz  =  Ik, the k  ×  k identity matrix. The matrix Φz of course satisfies the same relation ([\ref=eq:eigenrelation]) as Λz:

[formula]

Thus, if we knew Ψ and Σxx, we could form Σzz and calculate its first (=largest) k eigenvectors Φz, with their eigenvalues Ωz, and solve for the loadings matrix Λ  =  ΨΛz:

[formula]

and

[formula]

This tells how we can compute Λ as a function of Ψ and Σxx. In addition, ([\ref=eq:Sigmaxx]) yields a trivially simple formula for the diagonal matrix Ψ2 as a function of Λ, given Σxx:

[formula]

where diag stands for the diagonal part of the matrices, as a vector. An equivalent alternative is

[formula]

Here the left hand side can be obtained by elementwise multiplication of the diagonals of Ψ- 2 and Σxx, or equivalently as [formula].

Parameter estimation

For parameter estimation based on data, the formulae above can be used with Sxx inserted for Σxx: This yields an estimating equation for Λ as

[formula]

Here it is indicated that Λ̂ from ([\ref=eq:Lambda]) is a function of Ψ, and that Φz and Ωz are obtained from Szz  =  Ψ- 1SxxΨ- 1 and not from the theoretical Σzz. The other estimating equation is obtained from formula ([\ref=eq:diagPsi1]) or ([\ref=eq:diagPsi2]) with Sxx for Σxx:

[formula]

We thus want a solution of these two estimating equations relating Ψ and Λ.

When p > n, these estimating equations turn out to be identically the same as the Gaussian model likelihood equations. This can be taken either as a robustness argument for the Gaussian ML estimates, or as well as a strong argument for the distribution-free method, at least for large n. They are also generally quite intuitive. Formula ([\ref=eq:diagPsi]) is an obvious demand, and formula ([\ref=eq:Lambda]) or ([\ref=eq:Lambdahat]) is a truncated PCA on ΛΛT after a suitable, albeit parameter-dependent rescaling.

There is no explicit solution to the set of equations for Λ and Ψ2. Thus we have to use some iterative method, and a partial choice is obvious: Select Ψ2 in some way and use this Ψ2 in a calculation of a corresponding Λ, to be used to update Ψ2, etc. The step yielding Λ will be taken as given in most of the sequel. The question remains how to update Ψ2. Unless some care is used, such equations might yield impossible diagonal elements for Ψ2. We return to this question in the next paragraph.

There are alternative estimation methods to ML proposed in the FA literature. Among unweighted and weighted LS metods, the one denoted Δ2 in Bartholomew & Knott (1999) appears to be of particular interest in the present context, since it weights data by Ψ- 1, thus corresponding to our transformation of data. For given Ψ, the Δ2 method yields identically the same estimating equation ([\ref=eq:Lambdahat]) for Λ as the ML method. To estimate Ψ2 by the Δ2 method is (quoting Bartholomew & Knott) a good deal more complicated. The choice of Ψ should be such that the sum of squared differences from 1 of the p - k smallest eigenvalues of Ψ- 1SxxΨ- 1 is as small as possible, under the constraint that they are all ≥  1. This constraint, however, excludes the case of a singular Sxx and in particular the case p > n, and the method is therefore of little interest here.

Another type of estimation method are the estimation procedures in for example Trendafilov & Unkel (2011), jointly estimating F, Λ and Ψ2. They are based on a different model with additional constraints, which are not adequate in the present setting. They will be further commented in Section [\ref=sec:U&T].

Iterative solution of the estimating equation system ([\ref=eq:Lambdahat]) and ([\ref=eq:diagPsi])

The pair of estimating equations ([\ref=eq:Lambdahat]) and ([\ref=eq:diagPsi]) leads naturally to an iterative procedure, where we start with a provisional Ψ, calculate Λ by ([\ref=eq:Lambdahat]), calculate a new Ψ by ([\ref=eq:diagPsi]), etc. Such calculations are simplified by use of SVD on the sample of z-vectors, see next section. However, some variants are possible when using the equations for Ψ.

The simplest version is to use ([\ref=eq:diagPsi]) to express the new Ψ2, in component form

[formula]

with the current Λ, based on the previous Ψ, on the right hand side. This procedure has a long history, where it turned out to often converge slowly and sometimes to stop before true convergence was achieved. Even worse, the iteration could sometimes yield one or more negative Ψ2 components, known as Generalized Heywood cases. This might be because the best values had not yet been found, but a contributing reason could be the wrong k or an otherwise inadequate model. For these reasons, this iteration procedure for Gaussian ML estimation was abandoned, and replaced by a step of direct likelihood maximization to yield Ψ for given Λ (Jöreskog, 1967; Lawley, 1967). Another alternative is to use the EM algorithm (Rubin & Thayer, 1982).

The equivalent formula ([\ref=eq:diagPsi2]) suggests a different iteration procedure than ([\ref=eq:hatpsi1]). Calculate the new Ψ by ([\ref=eq:diagPsi2]), with the current Λz on the right hand side. This yields the iteration step in component form given by

[formula]

One advantage of this is that it yields a positive Ψ2 whatever is the current Λz. On the other hand, our experiences indicate that it is a slower algorithm, and we do not recommend it.

Theoretical investigation of the rate of convergence of these methods is difficult, due to the updating of eigenvectors involved. On the other hand, we have used the updating formula ([\ref=eq:hatpsi1]) on data with large p (p >  > n) without any problems, see further discussion in Section [\ref=sec:SVD] and Section [\ref=sec:genedata].

Use of the singular value decomposition (SVD)

Let X be the n  ×  p matrix of column mean-centered x-data, and correspondingly Z = XΨ- 1 for a provisional Ψ. A convenient procedure for carrying out the computations above is to calculate and use the singular value decomposition (SVD) of the matrix Z, given Ψ:

[formula]

where U (n  ×  p if p < n) and V (p  ×  p) have orthonormal columns (the left and right singular vectors), and D is a diagonal p  ×  p matrix whose diagonal elements, the singular values, are, in decreasing order, the square roots of the eigenvalues of ZTZ = VD2VT. When p > n, less than n singular values can be positive (typically n - 1), and then we let U and D be n  ×  n, and V be p  ×  n.

The right singular vectors forming V are the orthonormal eigenvectors of ZTZ (or of the covariance matrix ZTZ / (n - 1)). Corresponding to the FA model, we truncate the SVD by using only the first k singular vectors, U1 (n  ×  k) and V1 (p  ×  k), say, corresponding to Φz. That is, we partition Z as

[formula]

where U = (U1,  U2), etc. Note that it does not affect U1D1VT1 whether p < n or p > n, but only the second term, where D2 is either (p - k)  ×  (p - k) or (n - k)  ×  (n - k), respectively.

Since V1 is formed by the normalized eigenvectors of (n - 1)Szz with the k highest eigenvalues, and these are given by the diagonal D21, we can identify V1  =  Φz and D21 = (n - 1)Ωz from equation ([\ref=eq:Lambdahat]). Thus the estimating equation ([\ref=eq:Lambdahat]) for Λ can be expressed in terms of V1 and D1, and for the estimation of Λ (given Ψ) we will need only U1D1VT1. More precisely, Λ  =  ΨΛz in combination with

[formula]

Iteration step ([\ref=eq:hatpsi1]) for Ψ2 takes the following form in terms of V1 and D1:

[formula]

The alternative iteration step ([\ref=eq:hatpsi2]) takes the form

[formula]

The right hand side of ([\ref=eq:DiagPsi2New]) may alternatively be expressed as

[formula]

which shows that it is obtained by replacing the first k singular values or eigenvalues in Szz by the value 1. Consequently, the iteration method cannot possibly yield zero or negative values in Ψ2 in any iteration step (presuming start values are positive). What might possibly go wrong, as indicated by ([\ref=eq:hatLambda_z]), is that D21 / (n - 1)  -  Ik is not positive definite. In the case p > n, however, we give below some more results about D21 and D22, showing that we need not worry.

Note first that when Ψ and Λ satisfy the estimating equations, all the p diagonal elements of Szz  -  Λ̂zΛ̂Tz are 1, so its eigenvalues sum to p. At the same time,

[formula]

Thus, under the same conditions,

[formula]

If k is not higher than motivated by data, we expect the diagonal matrix Ωz  -  Ik in ([\ref=eq:hatLambda_z]) to have all its diagonal elements positive. When p < n, this can fail, and the estimation process too. When p > n( > k), however, the diagonal elements are necessarily positive, at least in a vicinity of the estimation point. To see this, note first that D22 contains less than n - k positive values, but has [formula]. Thus, the average value is at least (p - k) / (n - k) > 1. Since the k diagonal values in Ωz = D21 / (n - 1) are larger than this, by selection, the corresponding elements of Ωz  -  Ik are necessarily positive, which was to be shown.

In passing, we supplement by an expression for the average of the k first eigenvalues of Szz, cf. ([\ref=eq:traceOmega]). This average can be written

[formula]

where θ > 1 is the inverse of the harmonic mean of the p unique factor variance proportions ψ̂2j / (Sxx)jj,

[formula]

This is seen by subtracting p - k from [formula]. Note the proportionality to the dimension p in the second term of [formula], showing the benefit of large p. Note also that when k is increased, θ will also increase.

Factor scores and model residuals

The SVD approach can be used to obtain relatively directly the most common estimates or predictions of the scores fi, or the whole n  ×  k scores matrix F with the f-vectors as rows. As usual in the context of scores estimation/prediction, we provisionally regard the parameters as known (but they are of course estimated). The Bartlett scores, or weighted least squares scores regressing X on Λ, are given by

[formula]

so first we can note that with Z as data, Bartlett scores are standard (i.e. equal weights) least squares scores. Continuing from ([\ref=eq:Bartlett1]),

[formula]

using the fact that Φz  =  V1. This implies that the Bartlett score components are proportional to the SVD vectors U1. More precisely, since D21  =  (n - 1)Ωz, we achieve the following estimation/prediction formula (two equivalent versions related by ([\ref=eq:Omega])):

[formula]

To the right of [formula] is a diagonal matrix that scales the jth column of U1 by the factor [formula], [formula]. Thus, this is Bartlett's formula in a disguised but computationally convenient form. Typically, if p is large and k is not too large, all k ω-values will be large (proportionally to p, cf. ([\ref=eq:traceOmega])), and then with good approximation [formula].

If we instead predict the scores F by the linear regression of F on the observed X-data (or on Z), the best linear predictor [formula] is given by the so called regression or Thomson scores

[formula]

The difference from ([\ref=eq:Bartlett2]) is the diagonal matrix factor Ω- 1z  (Ωz  -  Ik) (cf. Bartholomew & Knott, 1999, sec. 3.24, or Krzanowski & Marriott, 1995, sec. 12.27). Again, if p is large, but not k, [formula].

For high dimension p but small or moderate sample size n we cannot expect high precision in the estimation of Λ or Ψ. Estimation/prediction of the scores fi, however, will be more precise with increasing p. More precisely, it can be shown that under mild conditions the variance of the factor estimator/predictor [formula] or [formula] goes to zero as p increases but k and n are kept constant. To be specific, consider the Bartlett score vector [formula] for an arbitrary observation i,  = (ΛTzΛz)- 1ΛTzz.

First, if the difference between Λ̂z and Λz is still neglected, formula ([\ref=eq:Bartlett1]) yields the well-known result

[formula]

Due to ([\ref=eq:traceOmega]), we may conclude that this diagonal matrix will have small elements when p is large and k is not too large.

The argument above is not justified when n < p, however. In that case, let us still regard Λ̂z as given, but with Ψ̂2 differing from the right Ψ2. Formula ([\ref=eq:varf1]) should then replaced by

[formula]

This will differ from the corresponding element of ([\ref=eq:varf1]) by less than a factor

[formula]

We do not know the true ψ2j-values, but if there are no components with quite little estimated noise ψ̂2j, and provided the elements of (Λ̂TzΛ̂z)- 1 are quite small, we can feel sure the precision in [formula] is high.

When the scores matrix F has been estimated/predicted, we can form the matrix of residuals, for example Êx  =  X - Λ̂T. In order to make them all comparable on the same scale, we must variance-standardize to Êz = Z - Λ̂Tz. Now note that

[formula]

so the standardized residuals matrix is

[formula]

Thus, the sum over [formula] of the mean squared standardized residuals is V2{D22 / (n - 1)}VT2. This may be compared with the result ([\ref=eq:residuals1]), which tells that the trace of D22 / (n - 1) is only p - k, and not p, so the mean squared standardized residuals are "too small", and must be normalized by p - k instead of p to have the right average size over [formula]. This corresponds to the residual degrees of freedom for unbiased variance estimation in a linear model for Z, regarding Λ as given and the k(n - 1) free elements of F as unknowns.

Models with nonrandom common factors, when p > n

In recent years, methods have been advocated for fitting fixed factor models to data, where also F is regarded as a set of unknown parameters, see the review by Unkel & Trendafilov (2010b). Several papers by those two authors treat the case p > n. The methods of Unkel & Trendafilov (2010a) and Trendafilov & Unkel (2011) proceed from a least squares method minimizing a loss function based on the Frobenius norm of data matrices. Quite generally, the fixed model requires more restrictions than the random model, for uniqueness, and when p > n. the authors are led to impose special constraints. Let us write X = FΛT  +  ΨEz, so we can let Ez exist also when Ψ2 contains zero variances. The papers referred to above assume the model satisfies the constraints ETzF  =  0, [formula], and (unless p > n) [formula]. When p > n they find that ETzEz  =  Ip cannot be fulfilled, because the rank of Ez can be at most n, and conclude that they need to allow at least p - n unique factors to have zero variances, corresponding to a singular Ψ2. In that situation they weaken the constraint ETzEz  =  Ip to the eigenvector relation ETzEzΨ  =  Ψ.

On the other hand, a result by Robertson & Symons (2007) states that the Gaussian model likelihood typically (depending on k) has a unique global maximum also when p > n, and with a nonsingular Ψ. Trendafilov & Unkel (2011) correctly remark that this result is not consistent with their own model. That the rank of Ez can be at most n (or n - 1, considering that data are centered) is trivially true for the sample of data, but not for the underlying statistical models assumed by Robertson & Symons (2007) and by us in the present paper. Our conclusion is that their constraints are artificial, and that their method only represents a constrained partitioning of data, and that it does not represent the fitting of a reasonable statistical model.

We shed further light on this situation here by comparing with our distribution-free but ML-related approach as far as it leads to the eigenvector relation for Λz and the Bartlett scores for estimating the scores matrix F, with any given Ψ: The constraint ETzF  =  0 is satisfied also for the fitted random model and its Bartlett scores [formula], according to Section [\ref=sec:scores]. The constraint [formula] is not exactly consistent with Bartlett scores but with the large p approximation [formula]. The constraints [formula] for n > p and ETzEzΨ  =  Ψ for p > n are not consistent with our fitted model. and other features of our fitted model, in particular since it does not allow noise outside the diagonal of ETzEz. Nor is the constraint consistent with Bartlett scores and other features of our model.

As their first illustration, Trendafilov & Unkel (2011) use Thurstone's 26-variable box data, consisting of a set of n = 20 boxes and p = 26 > 20 variables for each box, representing various aspects of size. When they fit a model with three factors (k = 3), they get 13 or 14 zero-valued ψ2j-values (depending on algorithm). When we fit our model we clearly get no more than 6 zeros, and they can be explained by the peculiarities of the data set. In fact, there are only three original variables in the data set: length, width and height. All other variables are constructed as functions of them. In a model with three latent factors, the factors turn out to be precisely length, width and height, and that explains three zeros. Three other variables are linear functions of length, width and height, and that explains the remaining three zeros. So for example adding a little computer-generated random measurement noise to the variables makes the zero variances disappear completely. Thus, all their zero unique factor variances are not really due to n < p, but to a combination of their assumed artificial data structure (model) and associated fitting method, and the peculiarities of the data set. An example of more applied relevance is studied in Section [\ref=sec:genedata].

A gene expression example, with p = 2000

We tried the model and the iteration methods on a microarray data set from Alon et al (1999), with 62 tissue samples (a colon cancer sample from each of 40 individuals and non-cancer samples from 22 of these individuals), and p = 2000 genes (selected by theses authors from a larger set of genes). The data are available on www.bioconductor.org, from where they were fetched. The data have earlier been used for illustrative purposes by McLachlan et al (2003, 2004). The response was taken to be the gene expression on log scale (natural log). Each gene was mean- and variance-standardized, but no other normalization of the data was made. None of the biological structure imposed by the experiment was used in the model, since our aim was not to draw biological conclusions but only to try our methods for model fitting.

We tested the estimation method on the data of all tissue samples (n = 62), but mostly on the data of only non-cancer tissue (n = 22). The iteration method ([\ref=eq:hatpsi2]) was found to be slower and generally inferior to the method ([\ref=eq:hatpsi1]). The experiences from running the iteration method ([\ref=eq:hatpsi1]) were extremely satisfactory. The method converged in about 10 iterations for small k and not more than 20 to 30 iterations for larger k, somewhat also depending on the choice of starting values for Ψ2. The time per iteration step seemed to be slowly increasing with k, but even with an extremely large k, k = 20 say, iterations did not require more time than a second each, on an ordinary laptop. There was no problem of Heywood type during the iterations. Even if the minimum of the unique factor variances in Ψ2 naturally decreased with k, it was in no case estimated to be zero (we tried k-values up to 20 for n = 62, and k = 12 for n = 22). After quite few iterations, [formula] was reasonably close to p - k, cf. ([\ref=eq:p-k]). The statements about [formula] and about the minimum of the unique factor variances are illustrated in Figures 1 and 2 below, showing how these quantities rapidly converge as the iteration number increases. Both for a small factor dimension (k = 2) and a moderate (k = 5) or large such dimension (k = 12) there are no problems at all, but k = 10 is also included for the little bump it shows in Figure 1. Starting values were ψ2j = 1 / 2 for all j.

We have thus found substantial support for the conjecture, that the iteration method works so well not despite the large p-value, but due to the large p.

Conclusions

Summing up, we have come to the following conclusions from the investigations in this paper.

Distribution-free estimating equations for the parameters of the standard FA model (with random factors), ([\ref=eq:Lambdahat]) and ([\ref=eq:diagPsi]), are easily derived in a set-up where variables are variance-normalized by their specific factor standard deviations (Ψ). This theory extends the Gaussian likelihood equations both to distribution-free settings and to the case p > n. The estimating equations are conveniently expressed by use of a singular value decomposition (SVD) under the same normalization.

An iteration scheme that has been much used for MLE computation when p < n, but also criticized as unreliable in such cases, is shown to have much stronger properties when p > n. The theoretical results are supported empirically in an illustration with p >  > n, where the method was seen to converge quite rapidly.

Another result for situations of type p >  > n is that even though the model parameters cannot be precisely estimated when n is small, the factor scores can be precisely estimated/predicted when p is large.

References

Alon, U. et al. (1999). Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissue probed by oligonucleotide arrays. Proc. Nat. Acad. Sci. USA, 6745-6750. Bartholomew, D.J. & Knott, M. (1999). Latent variable models and factor analysis, 2nd edn. Arnold, London Jöreskog, K.G. (1967). Some contributions to maximum likelihood factor analysis. Psychometrika, 443-482. Krzanowski, W.J. & Marriott, F.H.C. (1995). Multivariate analysis, part 2. Arnold, London Lawley, D.N. (1967). Some new results in maximum likelihood factor analysis. Proc. Roy. Soc. Edinburgh A, 256-264. McLachlan, G.J., Peel, D. & Bean, R.W. (2003). Modelling high-dimensional data by mixtures of factor analyzers. Comp. Stat. & Data Analysis, 379-388. McLachlan, G.J., Do, K.-A. & Ambroise, C. (2004). Analyzing microarray gene expression data. Wiley, Hoboken. Rubin, D.B. & Thayer, D.T. (1982). EM algorithms for ML factor analysis. Psychometrika, 443-482. Robertson, D. & Symons, J. (2007). Maximum likelihood factor analysis with rank-deficient sample covariance matrices. Journal of Multivariate Analysis, 813-828. Trendafilov, N.T. & Unkel, S. (2011). Exploratory factor analysis of data matrices with more variables than observations. J. Comp. Graph. Stat., 874-891. Unkel, S & Trendafilov, N.T. (2010a). A majorization algorithm for simultaneous parameter estimation in robust exploratory factor analysis. Comp. Stat. & Data Analysis, 3348-3358. Unkel, S & Trendafilov, N.T. (2010b). Simultaneous parameter estimation in exploratory factor analysis: an expository review. Int. Stat. Rev., 363-382.

Addresses: Rolf Sundberg, Mathem. statistics, Stockholm University, Sweden, rolfs@math.su.se; Uwe Feldmann, Medical biometry, University of Saarland, Germany, uf@med-imbei.uni-saarland.de Corresponding author: Rolf Sundberg