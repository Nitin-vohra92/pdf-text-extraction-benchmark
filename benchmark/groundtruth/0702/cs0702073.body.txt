Tradeoff between decoding complexity and rate for codes on graphs

Introduction

In [\cite=khandekarthesis] and [\cite=mceliece], Khandekar and McEliece suggested studying the problem the problem of per bit decoding complexity for capacity achieving codes. The authors conjectured on some lower bounds for the asymptotic decoding complexity for a sequence of codes achieving a rate which is ε close to the channel capacity. For general memoryless channels, with message passing decoding, the authors conjectured a lower bound of [formula] operations per bit at the decoder. There is one exception: for the BEC, codes were already known [\cite=amincapacity] which achieve capacity with [formula] operations per bit at the decoder (see [\cite=sason]). The BEC is an exception because along each edge in the graph, a message needs to be passed only once.

Soon after, in [\cite=UrbankeSason][\cite=anastapopulis] the authors constructed codes which achieve capacity with bounded decoding complexity for the BEC. In [\cite=PfisterSason], the authors construct systematic Accumulate-Repeat-Accumulate codes which achieve capacity with bounded decoding complexity. Interestingly, all the known codes which achieve capacity with bounded decoding complexity use accumulation.

It is a natural question whether there exist codes which achieve capacity with bounded decoding complexity under message passing decoding for general memoryless channels. It is a hard question, too, for we yet do not know of any codes which achieve capacity under message passing decoding for general memoryless channels.

In this paper, using an insight developed in [\cite=etesami], we show that a lower bound of [formula] holds for a large class of codes, including LDPC codes and LDGM codes, and any sequence of systematic codes.

This paper is organized as follows. In Section [\ref=sec:notation] we introduce the notation and some definitions used in this article. In Section [\ref=sec:ourresults] we put forward our lower bound on the decoding complexity. We also find some bounds on the rate of codes given their decoding performance. In Section [\ref=sec:exist] we observe a necessary condition for codes to achieve capacity with bounded decoding complexity: for infinite block-length, there should be no decrease in probability of error in finite number of operations per bit at the decoder. Section [\ref=sec:conclusions] concludes the paper.

Notation and Definitions

The block length of a code is denoted by n. A vector comprising of first i elements of a sequence of random variables is denoted in bold letters with superscript i. For example, the channel input vector is denoted by [formula]. Similarly, the channel output vector is denoted by [formula]. kth element of any vector [formula] is denoted by Xk. It would also be convenient to define [formula] as the outputs of the variable nodes in the neighborhood of node i till the lth iteration (excluding node i). C is used to denote the capacity of the channel under consideration. We assume that the channel is binary input, symmetric, and memoryless, but is otherwise arbitrary.

Let ε be the gap from capacity, R = C - ε. Denote the 'success in decoding' by a parameter τ. τ may correspond to decrease in probability of error, or a similar parameter characterizing some success in decoding.

We denote the number of decoding operations required per information bit for decoding a sequence of codes by χ(τ)D(ε), where ε is the difference between the code rate R and the channel capacity C.

All the results in this paper hold for general memoryless channel holds with one exception, the BEC.

An upper bound on the rate and a lower bound on the decoding complexity

Suppose we have a sequence of LDPC codes of fixed degree distributions (λ,ρ). In asymptotic (large block-length) analysis, the probability density of the messages passed [\cite=urbankecapacity] converges in distribution to δ∞, a point mass at infinity, as the number of iterations l converges to infinity. A message to ith variable node is a log likelihood ratio [formula]. Therefore, the random variable [formula], and [formula] as l  →    ∞  . The randomness is over the channel realization, the choice of the code in the ensemble, and the choice of the node.

Given some success in decoding by the lth iteration, with high probability the conditional entropy of the ith output bit Yi can then be bounded as follows

[formula]

This was first observed in [\cite=etesami] in the context of LT codes.

For some finite decoding success in constant number of computations, we prove that the rate is bounded below channel capacity, by explicitly finding the bound. We then arrive at a lower bound on the decoding complexity for a code being used at rate R = C - ε. For derivation of these bounds, we first need a bound on [formula].

Bound on [formula]

Consider the decoding up to l iterations. Let εl denote the probability of error after l iterations.

After l iterations, the conditional entropy [formula] can be bounded above by

[formula]

Using this bound, we wish to upper bound the entropy of vector [formula]. Using the chain rule

[formula]

Naturally enough, we would like to use the fact that conditioning reduces entropy to upper bound the individual conditional entropies in the sum. To that end, we want to remove some elements of the vector [formula] in order to arrive at [formula]. But notice that this would not always be possible. In fact, it is not difficult to show that in general, no particular permutation of the vector [formula] can ensure that [formula] for a constant fraction of Yi's.

In [\cite=etesami], this problem is dealt with by showing that the set [formula] can be considered to be erased for the analysis, and they try to find another constant τ' which bounds the entropy [formula]. In the following, we show a neater way to bound [formula] which directly leads to a clean upper bound on the rate. Moreover, this method works for LDPC codes as well as some other classes of codes, which is not the case for the the method in [\cite=etesami].

Consider all the n! permutations of [formula]. Consider the set [formula], constituted by the output values of the bits corresponding to the nodes in the neighborhood after l iterations. For each i, this set has a constant number of elements equalling kl (say). It is important to note that for a fixed sequence of codes, kl is independent of n. Because this set is constant in size, we try to find the fraction of permutations for which this set occurs before the particular Yi we are considering. The problem is equivalent to finding the number of permutations of a set in which a particular element of a subset occurs last amongst all the elements of the subset. This would be true for exactly [formula] fraction of total permutations, since it is the same for each element of the subset.

The size of the neighborhood set [formula], where α (β) is the average variable (check) node degree. Also, [formula], where R is the code rate.

We now derive the bound. In the following the bracketed (j) represents the jth permutation ordering of the vector [formula].

[formula]

Exchanging the two summations

[formula]

As argued in the previous paragraph, the terms in the summation can be bounded above by H(Yi) - τ for [formula] fraction of these permutations. Therefore,

[formula]

Since Yi's are identically distributed, we get the bound

[formula]

Per se, this bound is very loose, since the fraction [formula] converges to 0 polynomially in the average degree.

In comparison with [\cite=etesami], this bound is explicit on its dependance on the degree distribution. In the following section, we show how this bound leads to a bound on the achievable rate for LDPC codes.

Bound on the achievable rate for given performance after a fixed number of iterations

Using Fano's inequality,

[formula]

Using standard information-theoretic equalities

[formula]

For any code, [formula] is the encoded data which is in 1-1 mapping with the information symbols. The information symbols are uniformly distributed over the 2nR values. Thus, [formula] takes any value from the 2nR codewords with uniform probability distribution. Therefore

[formula]

Using the fact that the channel is memoryless

[formula]

Notice that in [\eqref=eq:galeq1], it now suffices to upper bound the entropy [formula], which is what we found in [\eqref=eq:permutebound]. Along with [\eqref=eq:fano] we obtain

[formula]

Therefore, for Pe  →  0, we get the following bound on the achievable rate

[formula]

Also, notice that [\eqref=eq:usingfano] gives us a lower bound on the probability of error for the case when this bound is violated.

Tightening the bound with more knowledge of the code performance

Since the bound is valid for all iterations l, we get the following tighter bound

[formula]

where the minimum is taken over all the values of l for which the decoding performance is known.

Bound for given decoding success in fixed number of computations

Denote the neighborhood size by k(c) (which depends on l and α), and the success in decoding by τ(c). Then it is easy to see that the same derivation still works, and we get the following bound on entropy of [formula]

[formula]

The total number of operations at the decoder is (α  ×  l + β  ×  l)n. Therefore, the number of operations per information bit,

[formula]

Define [formula] as the maximum value of k(c) for fixed c.

[formula]

Then,

[formula]

since R  ≤  1. Define

[formula]

Then k(c)bd  <    ∞   for each finite c because as l  →  1, k(c)bd  →  (c)4, and as l  →    ∞  , k(c)bd  →  0.

Thus a bound on the achievable rate is

[formula]

Since k(c)bd  <    ∞  , R is bounded below capacity.

A lower bound on decoding complexity as a function of gap from capacity

Substituting R = C - ε in [\eqref=eq:bd],

[formula]

Numerically, we observed that k(c)bd  <  ec for c large enough. Therefore, for fixed τ, the per bit decoding complexity

[formula]

Here τ is the parameter which reflects the success in decoding in [\eqref=eq:givenbound].

Generalization to other codes

Notice that we did not use any structure of LDPC codes for deriving the bound above. All we require is some finite success in decoding the output bits in order to decode the information bits. The bounds above, therefore, generalize to LDGM codes.

If we assume plain message passing decoding, then for any systematic code, there has to be a finite success in the decoding of the information bits with a finite neighborhood size. This implies a finite success in the decoding of output bits! Therefore, no systematic code can achieve capacity with bounded decoding complexity under plain message passing decoding for general memoryless channels.

A necessary condition for codes which achieve capacity with bounded decoding complexity

Note that the results in Section [\ref=sec:ourresults] are not valid for the BEC. It is shown in [\cite=UrbankeSason][\cite=anastapopulis] that there exist codes which achieve capacity with bounded decoding complexity for the BEC. Considering the above bound, it is intriguing that such codes exist. After all, the above bound only depends on some knowledge of marginal of output bits. As long as the probability of the output bits being 1 is bounded away from 0.5, the bound holds.

The codes in [\cite=UrbankeSason] are non-systematic IRA codes. IRA codes have an LDGM inner code, and a convolution code as the outer code. The convolution structure means that for any particular bit, for any finite number of iterations l, the marginal probability [formula]. The marginal uncertainty of the output bits does not decrease with increase in l! This also hints why systematic IRA codes, for which some of the output bits are decoded in finite time, fail to achieve capacity with bounded decoding complexity. The accumulation operation results in differential encoding of the inner code. The marginal uncertainty of the output bits does not decrease. But the joint uncertainty decreases substantially, giving us an estimate of the inner code bits, and therefore the information bits.

The arguments above are not tight for the BEC, because each edge in the graph needs to be used only once for the BEC. However, they hold for general memoryless channels. The bounds in Section [\ref=sec:ourresults] therefore imply that an accumulation-like operation is necessary for any sequence of codes to achieve capacity with bounded decoding complexity. Accumulation is really an example of a convolution code. Therefore, an outer convolution code would mean that the bounds in Section [\ref=sec:ourresults] do not apply.

Discussions and Conclusions

We showed that under message passing decoding, the achievable rate of a sparse graph code sequence is bounded below capacity if there is a finite success with bounded complexity decoding. The bound in Section [\ref=sec:ourresults] is actually because a bounded decoding complexity also bounds the neighborhood size. Some success in decoding with a bounded neighborhood size suggests strong local structures in the code, which is detrimental to its performance.

The bounds imply that an accumulation-type outer code would be necessary in order to achieve capacity with bounded decoding complexity for general memoryless channels.