Lemma Theorem Definition Definition Transform Conjecture Note Corollary Open Problem Observation Fact Big Inequality Algorithm Property

The Problem

[formula]

A Static Optimality Transformation with Applications to Planar Point Location

The Polytechnic Institute of New York University Brooklyn, New York USA Wolfgang Mulzer Institut für Informatik Freie Universität Berlin Germany

=10000 = 10000

Introduction

1-D History

Comparison-based predecessor search constitutes one of the oldest problems in computer science. Geometrically viewed, the task is as follows: we are given a partition of the line into n intervals, and for each query point we must find the interval that contains it. The classic solution is to use binary search in order to answer each query in time O( log n).

However, the story does not end here. Early in the history of computer science, researches realized that for a sufficiently biased distribution of the query results, o( log n) expected-time queries were possible. This insight led to the invention of optimal data structures for the case that the results of the queries were independently drawn from a known fixed distribution. Such data structures are known as optimal trees, and a wide literature studying their variants and extensions developed [\cite=optimum1] [\cite=optimum2] [\cite=optimum3] [\cite=optimum4] [\cite=optimum5] [\cite=optimum6] [\cite=optimum7] [\cite=optimum8] [\cite=optimum9] [\cite=optimum10] [\cite=optimum11] [\cite=optimum12] [\cite=optimum13] [\cite=optimum14] [\cite=optimum15]. This optimality is relative to the entropy of the probabilities. If the probability of each outcome is pi, the entropy H is defined to be [formula] and is a lower bound when the searches are drawn independently from a fixed distribution.

All these results required that the distribution, or a suitable approximation thereof, be known in advance. This situation changed when Sleator and Tarjan [\cite=splay] introduced splay trees. These trees have many amazing properties, one of which is known as static optimality. This means that for any sufficiently long sequence of operations, splay trees are asymptotically as fast as optimal search trees. However, splay trees do not need any prior information on the query distribution.

2-D History

Planar point location is a fundamental problem in computational geometry. Given a partition S of the plane into triangles (a triangulation), we need to find a data structure that for any query point p returns the region of S containing p. Such a structure is called a point location structure.

There are several point location structures with O( log n) query time. The use of the planar separator theorem [\cite=DBLP:conf/focs/LiptonT77] [\cite=lipton:177], Kirkpatrick's successive refinement algorithm [\cite=kirk], the use of persistence [\cite=ppl5], the layered-DAG approach [\cite=DBLP:journals/siamcomp/EdelsbrunnerGS86], and the randomized incremental algorithm [\cite=trapseidel] [\cite=DBLP:journals/jsc/Mulmuley90] are all notable not only for supporting O( log n) queries, but for doing so through very different methods.

Once again, it makes sense to consider biased query distributions. For a known fixed distribution of the point location queries, there are several data structures that are expected to answer the queries optimally, assuming that they are independent of each other. These structures, which we call biased, are analogous to the work on optimal binary search trees, and the lower bound of the entropy of the probabilities of the queried regions (H) holds here as well.

A series of papers published by Arya et al.  [\cite=DBLP:journals/talg/AryaMM07] [\cite=DBLP:journals/siamcomp/AryaMMW07] [\cite=DBLP:conf/soda/AryaMM01] [\cite=DBLP:conf/soda/AryaMM01a] [\cite=DBLP:conf/focs/AryaMM00] [\cite=DBLP:conf/swat/AryaCMR00], converge on two algorithms. One supports queries in time [formula] and O(n) space while another, simpler algorithm supports queries in time (5 ln 2)H + O(1) and O(n log n) space. The latter algorithm is a truly simple variant of the randomized incremental construction [\cite=trapseidel] [\cite=DBLP:journals/jsc/Mulmuley90], where the randomization is biased according to the distribution. Both structures are randomized and have superlinear construction costs. Iacono [\cite=DBLP:journals/comgeo/Iacono04] presented a data structure that supports O(H) queries in O(n) space, but, unlike the aforementioned results, it is deterministic, can be constructed in linear time, and has terrible constants.

Creating a point location structure with the static optimality bound

In view of the developments for optimal binary search trees, one question presents itself: Is there a point location structure that is asymptotically as fast as the biased structures, without explicit knowledge of the query distribution? Or, put another way, is there a point location structure with a runtime bound similar to the static optimality bound of splay trees? This open problem, which we resolve here, explicitly appears in several previous works on point location, for example in Arya et al. [\cite=DBLP:journals/siamcomp/AryaMMW07]:

Taking this in a different direction, suppose that the query distribution is not known at all. That is, the probabilities that the query point lies within the various cells of the subdivision are unknown. In the 1-dimensional case it is known that there exist self-adjusting data structures, such as splay trees, that achieve good expected query time in the limit. Do such self-adjusting structures exist for planar point location?

There are several possible approaches towards a statically optimal point location structure. One, suggested above, would be to create some sort of self-adjusting point location structure and to analyze it in a similar way to splay trees. This has not been done; we suspect the main stumbling block is that all O( log n) structures with reasonable space requirements are comparison DAGs, not trees, and it is unclear how to make rotation-like local changes in such DAGs. Another possible avenue is to use splay trees in an existing structure. Goodrich et al. [\cite=goodrich2] followed this approach, which is essentially a hybrid of splay trees and the persistent line-sweep method. Unfortunately, their method does not give a result optimal with respect to the entropy of the original query distribution, but rather to the entropy of the probabilities of querying regions of a trapezoidation of the triangulation. The latter is obtained by drawing vertical lines through every point of the triangulation. This subdivision refinement could result in a O( log n) factor degradation of the entropy, the worst possible.

Another possible method would be to create a structure with the working set property. This property, originally used in the analysis of spay trees, states that the time for a query is the logarithm of the number of different queries since the last query that returned the same item. The working set property implies static optimality [\cite=DBLP:conf/soda/Iacono01a]; it has also been of use in several other contexts [\cite=DBLP:conf/latin/BoseDDH10] [\cite=DBLP:journals/ijfcs/Elmasry06] [\cite=DBLP:conf/soda/BoseDL08] [\cite=DBLP:journals/algorithmica/Iacono05] [\cite=DBLP:conf/swat/Iacono00]. Most importantly, there is a general transformation from a dynamic O( log n) structure into one with the working set property [\cite=DBLP:conf/soda/Iacono01a]. The problem with using this approach for point location is that, despite the invention of data structures such as AVL trees [\cite=avl] and red-black trees [\cite=redblack] for the 1-D problem, discovering an O( log n) insert/delete/query point location structure remains a prominent open problem. (Note that [\cite=Talib:1996:TME:792755.792810] claims to modify Kirkpatrick's method to allow for O( log n) insertions, deletions, and queries. Their claimed result is wrong.)

Our solution to the problem of developing a statically optimal structure is very simple: use a biased structure that needs to be initialized with distributional information, and rebuild it periodically using the observed frequencies for each region. We do not store all the regions in the biased structure, as this would cause the rebuilding cost to be too expensive. Instead, when rebuilding we create a structure storing only the nβ most frequent items observed so far, and resort to a static O( log n) structure to complete all queries that are unsuccessful in the biased structure. This is a simple and general method of converting biased structures into statically optimal structures. Thus, our transformation can waive the requirement of distributional knowledge present in all previous biased point location structures.

Notation

Let U be some universal set and let S be a partition of U into n pieces. A location query q(p) takes some p∈U and returns the element s∈S such that p∈s.

A data structure to efficiently answer location queries is a location query structure. We let [formula] be a sequence of m queries executed on the location query structure, and we use [formula] to denote the results of these queries.

Let fj(s) be the number of occurrences of s in the first j elements of Q, and let the unsubscripted f(s) denote the number of occurrences in the entire sequence, fm(s). We define tj(s) as the index of j occurrence of s in Q; thus ftj(s)(s) = j.

We use log x to refer to max (1, log 2x); this avoids clutter generated by additive terms that would otherwise be needed to handle degenerate cases of our analysis.

We call a query data structure D biased if given S and some function [formula] it executes P in time

[formula]

The term cD(n) is called the construction cost of the structure.

In order to minimize the runtime in a biased structure, the function w should be proportional to the number of queries that return the given region, e.g. let w(s) = fm(s). Thus, if the weights are set properly, the amortized runtime per query is (order of) the entropy of the query distribution; information theory [\cite=weaver] gives entropy as a lower bound for such comparison based structures if the queries are independently drawn from a fixed distribution.

A data structure D that given S executes P in time

[formula]

is called statically optimal with construction cost cD(n).

Note that a statically optimal structure is not given the frequency function f or any weights in advance.

We provide a simple method to transform a biased query structure into a statically optimal structure, subject to a few technical requirements. The main such requirement is that we should be able to construct the biased query structure not just on the set S but on any subset S' of S. For a data structure constructed to work on such a subset, we wish to have it perform as quickly as a biased structure on S for any query to an element in S', and to report in O( log n) time if the query is in some (unknown) element of [formula]. Formally:

We call a query data structure subset-biased on S if given any subset S' of S of size n', and a weight function [formula], the query sequence P is executed in time

[formula]

For each query p∈P, if there is a s'∈S' such that p∈s', then q(p) = s'; otherwise [formula]. The term c'D(n') is called the construction cost of the structure.

Note that [formula] is just the number of queries that result in failure. Given Definition [\ref=def:subset_biased], we may now state our main theorem:

Given an O( log n)-time query data structure D on S with construction cost O(n) and a subset-biased query structure on S with construction cost O(n' log n'), then there is a statically optimal structure with construction cost O(n).

Our transformation

This section provides a structure which proves Theorem [\ref=main]. As such, we assume that for any given partition S, there is a O( log n)-time query data structure D on S with construction cost O(n), and a subset-biased query structure for any subset S' of S with construction cost O(n' log n'), where n' = |S'|.

Description of the structure

The idea is simple, and is dependent on two constants, α and β, chosen such that 0 < β  <  α < 1 - β < 1. Every nα queries, we rebuild a subset-biased data structure for the nβ most commonly accessed regions, in O(nβ log n) time. We keep a static O( log n)-time structure that will be used for failed queries in the subset-biased structure. Formally, the structure has several parts:

A static O( log n) query time structure.

A structure which keeps track of the number of searches for each region, and is capable of reporting the top-k searched regions in O(k) time. Such a structure is easy to make since we only increment the search counts by one. This can be done in linear space and constant time per update. (The additional space overhead can be made sublinear at the expense of determinism thorough the use of a streaming algorithm for the so-called heavy hitters problem. See, for example [\cite=DBLP:journals/tods/BerindeICS10]).

A subset-biased structure that is built after the nα query and rebuilt every nα query thereafter. It contains the nβ most frequently queried items at the time of the rebuilding, where the weight of a region s, w'(s), is the number of queries to s at the time of the rebuilding. I.e., if the rebuilding is at time k, w'(s) = fk(s). Computing the nβ most frequently queried items and the weight function w' takes time O(nβ) using the structure of ([\ref=pointtwo]). Since the size of the subset is nβ, the construction cost of the subset biased structure is O(nβ log n).

A search is executed on the subset-biased structure first. If it fails (at cost O( log n)), it is executed in the static O( log n)-time structure. As such, the worst-case time for a search is O( log n).

Initial analysis of structure

We start with a lemma which illustrates what the rebuilding process gives:

Consider the i query pi, and let qi denote the resulting region q(pi)∈S. Suppose that fi(qi)  ≥  2nα. Then the cost to execute query pi is

[formula]

If we were to rebuild a biased structure after every query with the entire set S (as opposed to a subset), the biased structure would have a runtime of [formula]. However, the rebuilding is periodic, so the frequency (f-value) that the structure was rebuilt with for a region and the current frequency for that region could be off by at most the number of queries between rebuilds, nα. Finally, we note that any search that results in failure in the biased structure takes time O( log n) to execute and the lemma holds in these cases since then the frequency is low enough that [formula].

The total runtime to initialize our structure and execute all queries is

[formula]

The main summation is over every element s of S. For each element s, the first up to 2nα queries are given the trivial bound of O( log n), whereas the remaining queries (if any) are given the bound of Lemma [\ref=singlequery]. The first additive term at the end is the construction cost of O(nβ log n) of the subset-biased structure of size nβ which is incurred every nα operations. The final term is the linear one-time cost to build the static structure.

Technical Lemmas

In order to simplify the runtime given by Lemma [\ref=bigruntime], this subsection presents two technical lemmas.

For each s∈S, we have

[formula]

Since ftj(s)(s)  =  j  ≥  2nα and tj(s)  ≤  m, we have

[formula]

Given a constant γ, α  <  γ < 1, if m  ≥  nγ, then

[formula]

There are two cases depending on which of f(s) and 2nα is smaller; the lemma trivially holds in either case.

Main theorem

The total runtime to initialize our structure and execute all queries is

[formula]

Recall the runtime established in Lemma [\ref=bigruntime], we get

[formula]

Using Lemma [\ref=longmath], and noting that [formula], this becomes

[formula]

If m  ≤  n1 - β the summation is at most n1 - β log n  =  o(n) and the whole expression simplifies to O(n) which gives the theorem. If this is not the case, m  ≥  n1 - β, Lemma [\ref=smallcase] applies, and the min (f(s),2nα) log n collapse into f(s) log (m / f(s)) to give the theorem.

Point location

There is a data structure for planar point location in a connected triangulation of size n that can execute any query sequence of length m in time

[formula]

Applying our general transformation to the problem of planar point location in a triangulation is easy, as all of the required ingredients are well-known. It is assumed that the triangulation is given in a standard representation such as a doubly-connected edge list (see, for example [\cite=marks]).

For the O( log n) query-time structure with O(n) construction time, Kirkpatrick's algorithm can be used. For the required subset-biased structure, the provided subset of size n' of a triangulation may not be a connected triangulation and thus needs to be triangulated; this takes time O(n' log n') using the classic line sweep approach [\cite=DBLP:journals/siamcomp/LeeP77], The resultant triangulation has size O(n'). Newly introduced triangles are given small weights and the resultant triangulation and weighting is given to a biased structure such as ours [\cite=DBLP:journals/comgeo/Iacono04].

Our choice of structures reflects a desire to achieve the strongest asymptotic bounds possible, and thus we have avoided structures that are randomized or that have non-linear construction cost; such structures, however, have far superior constants than those we have chosen. If a data structure for the static O( log n)-time queries was to be used that has a O(n log n) construction cost instead of O(n), this would simply change the linear additive term in Theorem [\ref=ppl] to a n log n.

Point location in polygonal subdivisions with non-constant sized cells

Our work applies to point location in triangulations, and by easy extension, to polygonal subdivisions where each region has constant complexity. Recently, several data structures have been developed for optimal point location where the distribution is known in advance for convex connected [\cite=DBLP:conf/soda/ColletteDILM08], connected [\cite=DBLP:journals/corr/abs-0901-1908], and arbitrary polygonal [\cite=DBLP:journals/corr/abs-1001-2763] subdivisions of the plane, as well as the more general odds-on trees [\cite=DBLP:journals/corr/abs-1002-1092]. The entropy-based lower bounds used in optimal trees, biased point location structures, and here are not meaningful in these contexts; a convex k-gon splits the plane into but two regions and has constant entropy among the query probabilities of the two regions, yet for some distributions can require Ω( log n) time for a point location query.

The entropy-sensitive structures for non-triangulations all basically work by triangulating the given subdivision as a function of the provided probability distribution, and then using one of the biased structures on the resultant triangulation. The main conceptual problem in using our framework with such a structure is that it is unclear how to triangulate during the rebuilding process, since the optimal triangulation is not known in advance. One could imagine that triangulating during each rebuild based on the observed queries so far would work well, but proving this would require a more complex and specialized analysis than what has been presented in this paper.

Acknowledgments

The second author would like to thank Pat Morin for suggesting the problem to him, for stimulating discussions on the subject, and for hosting him during a wonderful stay at the Computational Geometry Lab at Carleton University.