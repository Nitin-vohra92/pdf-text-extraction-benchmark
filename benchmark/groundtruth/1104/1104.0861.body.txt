10.9514pt plus.8pt minus .6pt

GENERALIZED DOUBLE PARETO SHRINKAGE

Artin Armagan, David B. Dunson and Jaeyong Lee

SAS Institute Inc., Duke University and Seoul National University

911.5pt plus.8pt minus .6pt

10.9514pt plus.8pt minus .6pt

1. Introduction

There has been a great deal of work in shrinkage estimation and simultaneous variable selection in the frequentist framework. The [formula] of Tibshirani (1996) has drawn much attention to the area, particularly after the introduction of [formula] (Efron et al. (2004)) due to its superb computational performance. There is a rich literature analyzing the [formula] and related approaches (Fu (1998), Knight and Fu (2000), Fan and Li (2001), Yuan and Lin (2005), Zhao and Yu (2006), Zou (2006), Zou and Li (2008)), with a number of articles considering asymptotic properties.

Bayesian approaches to the same problem became popular with the works of Tipping (2001) and Figueiredo (2003). By expressing Student's t priors for basis coefficients as scale mixtures of normals (West (1987)), and relying on type II maximum likelihood estimation (Berger (1985)), Tipping (2001) developed the relevance vector machine for sparse estimation in kernel regression. In this setting, however, exact sparsity comes with the price of forfeiting propriety of the posterior by driving the scale parameter of the Student's t distribution toward zero. In fact, driving both the scale parameter and the degrees of freedom to zero yields the so-called Normal-Jeffreys' prior, [formula]. The name emerges due to the fact that the hierarchy follows as θ  ~  (0,τ), [formula], where the latter is the Jeffreys' prior on the prior variance of θ. Figueiredo (2003) proposed an expectation-maximization algorithm for maximum a posteriori estimation under Laplace and Normal-Jeffreys' priors, with estimates under the Laplace corresponding to the [formula]. The Normal-Jeffreys' prior leads to substantially improved performance with finite samples due to the property of strongly shrinking small coefficients to zero while minimally shrinking large coefficients due to the heavy tails; however, it has no meaning from an inferential aspect as it leads to an improper posterior.

A Bayesian [formula] was proposed by Park and Casella (2008) and Hans (2009). However, these procedures inherit the problem of over-shrinking large coefficients due to the relatively light tails of the Laplace prior. Strawderman-Berger priors (Strawderman (1971), Berger (1980)) have some desirable properties yet lack a simple analytic form. Recently proposed priors have been designed to have high density near zero and heavy tails without the impropriety problem of Normal-Jeffreys. The horseshoe prior of Carvalho, Polson, and Scott (2009, 2010) is induced through a carefully-specified mixture of normals, leading to such desirable properties as an infinite spike at zero and very heavy tails. They studied sparse shrinkage estimation properties of the horseshoe in a normal means problem. Griffin and Brown (2007, 2010) proposed an alternative class of hierarchical priors for shrinkage with some similarities to the prior we propose, but it lacks a simple analytic form that facilitates the study of some properties.

There is a need for alternative shrinkage priors that lead to sparse point estimates if desired, do not over-shrink coefficients that are not close to zero, facilitate straightforward computation even in large p cases, and result in a joint posterior distribution that does a good job of quantifying uncertainty. We propose the generalized double Pareto prior which independently finds mention in Cevher (2009). It has a simple analytic form, yields a proper posterior, and possesses such appealing properties as a spike at zero, Student's t-like tails, and a simple characterization as a scale mixture of normals that leads to a straightforward Gibbs sampler for posterior inferences. We consider both fully Bayesian and frequentist penalized likelihood approaches based on this prior. We show that the induced penalty in the regularization framework yields a consistent thresholding rule having the continuity property in the orthogonal case, with a simple expectation-maximization algorithm described for sparse estimation in non-orthogonal cases. In another independent work motivated by applications to genome wide associations studies, Lee et al. (2011) consider a generalized t prior (McDonald and Newey (1988)) that includes the generalized double Pareto as a special case. Similarities to previous work are limited and our contributions beyond them are (i) the formal introduction of a generalized Pareto density, thresholded and folded at zero, as a shrinkage prior in Bayesian analysis, (ii) the scale mixture representation of the generalized double Pareto in Proposition 1 which is central to our work, (iii) its connection to the Laplace and Normal-Jeffreys' priors as limiting cases in Proposition 2, (iv) the resulting fully conditional posteriors in a linear regression setting along with a simple Gibbs sampling procedure, (v) a detailed discussion on the hyper-parameters α and η and their treatment, along with the incorporation of a griddy sampling scheme into the Gibbs sampler, (vi) a detailed analysis of the induced penalty by the generalized double Pareto prior and the properties of the resulting thresholding rule, (vii) an explicit analytic form for the maximum a posteriori estimator in orthogonal cases, (viii) an expectation-maximization procedure to obtain the maximum a posteriori estimate in non-orthogonal cases using the normal mixture representation, (ix) the one-step estimator (Zou and Li (2008)) resulting from the Laplace mixture representation, revealing the connection of the resulting procedure to the adaptive [formula] of Zou (2006), and (x) the oracle properties of the resulting estimators.

2. Generalized Double Pareto Prior

The generalized double Pareto density is

[formula]

where ξ > 0 is a scale parameter and α > 0 is a shape parameter. In contrast to ([\ref=eq:gdP]), the generalized Pareto density of Pickands (1975) is parametrized in terms of a location parameter [formula], a scale parameter ξ > 0, and a shape parameter [formula] as

[formula]

with θ  ≥  μ for α  >  0 and μ  ≤  θ  ≤  μ  -  ξα for α  <  0. The mean and variance for the generalized Pareto distribution are [formula] for α∉[0,1] and [formula] for α∉[0,2]. If we let μ = 0, ([\ref=eq:gP]) becomes an exponential density as α  →    ∞   with mean ξ and variance ξ2.

To modify the generalized Pareto density to be a shrinkage prior, we let μ  =  0 and reflect the positive part about the origin, assuming α  >  0, for a density that is symmetric about zero. The mean and variance for the generalized double Pareto distribution are [formula] for α > 1 and [formula] for α > 2. The dispersion is controlled by ξ and α, with α controlling the tail heaviness and α = 1 corresponding to Cauchy-like tails and no finite moments.

Figure [\ref=fig1] compares the density in ([\ref=eq:gdP]) to Cauchy and Laplace densities for the special case ξ  =  α  =  1, so that f(θ) = 1 / {2(1 + |θ|)2}. We refer to this form as the standard double Pareto. Near zero, the standard double Pareto resembles the Laplace density, suggesting similar sparse shrinkage properties of small coefficients in maximum a posteriori estimation. It also has Cauchy-like tails, which is appealing in avoiding over-shrinkage away from the origin. This is illustrated in Figure [\ref=fig1](a). Figure [\ref=fig1](b) illustrates how the density in ([\ref=eq:gdP]) changes for different values of ξ and α.

Prior ([\ref=eq:gdP]) can be represented as a scale mixture of normal distributions leading to computational simplifications. As shorthand notation, let [formula] denote that θ has density ([\ref=eq:gdP]).

Let θ  ~  (0,τ), τ  ~  (λ2 / 2), and λ  ~  (α,η), where α > 0 and η > 0. The resulting marginal density for θ is [formula].

Proposition 1 reveals a relationship between the prior in ([\ref=eq:gdP]) and the prior of Griffin and Brown (2007), with the difference being that Griffin and Brown (2007) place a mixing distribution on λ2 leading to a marginal density on θ with no simple analytic form.

In Proposition 2 we show that the prior in ([\ref=eq:gdP]) forms a bridge between two limiting cases - Laplace and Normal-Jeffreys' priors.

Given the representation in Proposition 1, [formula] implies

[formula] for α = 0 and η = 0,

f(θ|λ') = (λ' / 2) exp (  -  λ'|θ|) for α  →    ∞  , α  /  η  =  λ' and 0 < λ' <   ∞  .

For the first item, setting α  =  η = 0 implies placing a Jeffreys' prior on λ, [formula]. Integration over λ yields [formula], which implies the Normal-Jeffreys' prior on θ. For the second item, notice that π(λ) = δ(λ  -  λ'), where δ(.) denotes the Dirac delta function, since [formula] and [formula]. Thus, [formula].

As noted in Polson and Scott (2010), if π(τ) has exponential or lighter tails, observations are shrunk towards zero by some non-diminishing amount, regardless of size. This phenomenon is well-understood and commonly observed in estimation under the Laplace prior, where an exponential density mixes a normal density. The higher-level mixing (over λ) in Proposition 1 allows π(τ) to have heavier tails, remedying the unwanted bias.

As α grows, the density becomes lighter tailed, more peaked and the variance becomes smaller, while as η grows, the density becomes flatter and the variance increases. Hence if we increase α, we may cause unwanted bias for large signals, though causing stronger shrinkage for noise-like signals; if we increase η we may lose the ability to shrink noise-like signals, as the density is not as pronounced around zero; and finally, if we increase α and η at the same rate, the variance remains constant but the tails become lighter, converging to a Laplace density in the limit. This leads to over-shrinking of coefficients that are away from zero. As a typical default specification for the hyper-parameters, one can take α  =  η  =  1. This choice leads to Cauchy-like tail behavior, which is well-known to have desirable Bayesian robustness properties.

To motivate this default choice, we assess the behavior of the prior shrinkage factor κ  =  1 / (1 + τ)∈(0,1), where [formula] is the parameter of interest (Carvalho et al. (2010)). As κ  →  0, the prior imposes no shrinkage, while as κ  →  1 it has a strong pull towards zero. The generalized double Pareto distribution implies a prior π(κ) on κ upon integration over λ in Proposition 1. For the standard double Pareto, this is

[formula]

where (.) denotes the complementary error function. In Figure 2.2, we compare π(κ) under the standard double Pareto, Strawderman-Berger, horseshoe, and Cauchy priors, which may all be considered default choices. The priors behave similarly for κ  ≈  0, implying similar tail behavior. The behavior of π(κ) for κ  ≈  1 governs the strength of shrinkage of small signals. As κ  →  1, π(κ) tends towards zero for the Cauchy, implying weak shrinkage, while π(κ) is unbounded for the horseshoe, suggesting a strong pull towards zero for small signals. The Strawderman-Berger and standard double Pareto priors are a compromise between these extremes, with π(κ) bounded for κ  →  1 in both cases. The standard double Pareto assigns higher density close to one than the Strawderman-Berger prior, and has the advantage of a simple analytic form over the Strawderman-Berger and horseshoe priors.

Of course it is best to adjust α and η according to any available prior information pertaining to the sparsity structure of the estimated vector. For general α > 0 and η > 0 values, the prior on κ is

[formula]

where [formula] denotes the confluent hypergeometric function. Note that π(κ|α,η) takes a "horseshoe" shape when α  =  η = 0. Carvalho, Polson, and Scott (2010) show that [formula] implies a Normal-Jeffreys' prior on θ, which can also be observed by setting α  =  η = 0 in ([\ref=kappa]) in conjunction with Proposition 1. Hence π(κ|α,η) is unbounded at κ = 1 forcing π(θ|α,η) to be unbounded at 0 only if η = 0. The effects of α and η are now observed with better clarity from Figure [\ref=fig_shrink2]. As η increases, less and less density is assigned to the neighborhood of [formula], repressing shrinkage. On the other hand, increasing α values place more and more density in the neighborhood of [formula] promoting further shrinkage. This notion is later reinforced by Proposition 3, such that the prior induces a thresholding rule under maximum a posteriori estimation if [formula]. Hence, we need to carefully pick these hyper-parameters, in particular α, as there is a trade-off between the magnitude of shrinkage and tail robustness.

3. Bayesian Inference in Linear Models

Consider the linear regression model [formula], where [formula] is an n-dimensional vector of responses, [formula] is the n  ×  p design matrix and [formula]. Letting [formula] independently for [formula],

[formula]

From Proposition 1, this prior is equivalent to [formula], with τj  ~  (λ2j / 2) and λj  ~  (α,η). We place the Jeffreys' prior on the error variance, [formula].

Using the scale mixture of normals representation, we obtain a simple data augmentation Gibbs sampler having the conditional posteriors [formula], [formula], (λj|βj,σ2)  ~  (α + 1,|βj| / σ  +  η), (τ- 1j|βj,λj,σ2)  ~  {μ = (λ2jσ2  /  β2j)1 / 2,ρ  =  λ2}, where [formula] and [formula] denotes the inverse Gaussian distribution with location and scale parameters μ and ρ. In our experience, this Gibbs sampler is efficient with fast rates of convergence and mixing.

In the absence of any prior information on α and η, one may either set them to their default values or, as an alternative, choose hyper-priors to allow the data to inform about the values of α and η. We use π(α) = 1 / (1 + α)2 and π(η) = 1 / (1 + η)2 to correspond to generalized Pareto hyper-priors with location parameter 0, scale parameter 1 and shape parameter 1. The median value of the resulting distribution for α and η is 1, centered at the default choices suggested earlier, while the mean and variance do not exist.

For sampling purposes, let a = 1 / (1 + α) and e = 1 / (1 + η). These transformations suggest a uniform prior on a and e in (0,1) given the generalized Pareto priors on α and η. Consequently, the conditional posteriors for a and e are

[formula]

We propose the embedded griddy Gibbs (Ritter and Tanner (1992)) sampling scheme:

Form a grid of m points [formula] in the interval (0,1).

Calculate [formula].

Normalize the weights, [formula].

Draw a sample from the set [formula] with probabilities [formula], and set α = 1 / a - 1 to be used at the current iteration of the Gibbs sampler.

Repeat the same procedure for e and obtain a random draw for η. We also experiment with fixing η as 1 while treating α as unknown. In this case, the prior variance of [formula] is determined by α.

In what follows we establish the ties between the Bayesian approach we have taken and some frequentist regularization approaches. The simple analytic structure of the generalized double Pareto prior facilitates analyses while its hierarchical formulation leads to straight-forward computation.

4. Sparse Maximum a Posteriori Estimation

The generalized double Pareto distribution can be used not only as a prior in a Bayesian analysis, but also to induce a sparsity-favoring penalty in regularized least squares:

[formula]

where [formula] is initially assumed to have orthonormal columns and p(.) denotes the penalty function implied by the prior on the regression coefficients. Following Fan and Li (2001), let [formula], and denote the minimization problem in ([\ref=pls]) for a component of [formula] as

[formula]

with the penalty function [formula] that simply retains the term in -   log π(βj|α,η) that depends on βj.

From Fan and Li (2001), a good penalty function should result in an estimator that is (i) nearly unbiased when the true unknown parameter is large, (ii) a thresholding rule that automatically sets small estimated coefficients to zero to reduce model complexity, and (iii) continuous in data (j) to avoid instability in model prediction. In the following, we show that the penalty function induced by prior ([\ref=eq:gdP2]) may achieve these properties.

4.1. Near-unbiasedness

The first order derivative of ([\ref=pls_one]) with respect to βj is (βj){|βj| + σ2p'(|βj|)}  -  j  =  (βj){|βj| + σ2(α + 1) / (ση + |βj|)}  -  j, where p'(|βj|) = ∂p(|βj|) / ∂|βj| is the term causing bias in estimation. Although it is appealing to introduce bias in small coefficients to reduce the mean squared error and model complexity, it is also desirable to limit the shrinkage of large coefficients with p'(|βj|)  →  0 as |βj|  →    ∞  . In addition, it is desirable for p'(|βj|) to approach zero rapidly, implying shrinkage, and the associated introduction of bias rapidly decreases as coefficients get further away from zero. In fact, the rate of convergence of p'(|βj|) to zero is of the same order under the generalized double Pareto and Normal-Jeffreys' priors, with lim |βj|  →    ∞{(α + 1) / (ση + |βj|)}  /  {1 / |βj|}  =  α + 1. As α controls the tail heaviness in the generalized double Pareto prior, with lighter tails for larger values of α, convergence of the ratio to (α + 1) is intuitive. In the case of [formula], the bias, p'(|βj|), remains constant regardless of |βj|, which can also be observed in Figure [\ref=fig3](b).

4.2. Sparsity

As noted in Fan and Li (2001), a sufficient condition for the resulting estimator to be a thresholding rule is that the minimum of the function |βj| + σ2p'(|βj|) is positive.

Under the formulation in Proposition 1, prior ([\ref=eq:gdP2]) implies a penalty yielding an estimator that is a thresholding rule if [formula].

This result is obtained by finding the minimum of |βj| + σ2p'(|βj|) and taking it greater than zero. The thresholding is a direct consequence of the fact that when |j| <  min βj{|βj| + σ2(α + 1) / (ση + |βj|)}, which requires that min βj{|βj| + σ2p'(|βj|)} > 0, the derivative of ([\ref=pls_one]) is positive for all positive βj and negative for all negative βj. In this case, the penalized least squares estimator is zero. When |j| >  min βj{|βj| + σ2(α + 1) / (ση + |βj|)}, two roots may exist. The larger one (in absolute value) or zero is the penalized least squares estimator. To elaborate more on this, the root(s) may exist for (βj){|βj| + σ2p'(|βj|)}  -  j = 0 only when |j| >  min βj{|βj| + σ2p'(|βj|)}. A helpful illustration is Figure 3 of Fan and Li (2001).

4.3. Continuity

Continuity in data is important if an estimator is to avoid instabilities in prediction. As in Breiman (1996), "a regularization procedure is unstable if a small change in data can make large changes in the regularized estimator". Discontinuities in the thresholding rule may result in inclusion or dismissal of a signal with minor changes in the data used (see Figure [\ref=fig3](b)). Hard-thresholding, the "usual" variable selection, is an unstable procedure, while ridge and [formula] estimates are considered stable.

A necessary and sufficient condition for continuity is that the minimum of the function |βj| + σ2p'(|βj|) is at zero (Fan and Li (2001)). For our prior, the minimum of this function is obtained at [formula]. Therefore [formula] yields an estimator with this property.

Under the formulation in Proposition 1, a subfamily of prior ([\ref=eq:gdP]) with [formula] yields an estimator with the continuity property.

In this particular case, the penalized likelihood estimator is set to zero if [formula]. When [formula],

[formula]

As can be observed in Figure [\ref=fig3](a), ensuring continuity by letting [formula] creates a trade-off between sparsity and tail-robustness. As the thresholding region becomes wider, the larger values are penalized further, yet not nearly at the level of [formula].

4.4. Maximum a Posteriori Estimation via Expectation-Maximization

We assume a normal likelihood to formulate the procedure for non-orthogonal linear regression. Estimation is carried out via the expectation-maximization ([formula]) algorithm.

4.4.1. Exploiting the Normal Mixture Representation

We take the expectation of the log-posterior with respect to the conditional posterior distributions of (τ- 1j|β(k)j,λj,σ2(k)) and (λj|β(k)j,σ2(k)) at the kth step, then maximize with respect to βj and σ2 to get the values for the (k + 1)th step.

E-step:

[formula]

M-step: Letting [formula], we have

[formula]

We refer to this estimator as [formula].

4.4.2. Exploiting the Laplace Mixture Representation and the One-step Estimator

In the proof of Proposition 1, the integration over [formula] leads to a Laplace mixture representation of the prior. Since the mixing distribution of the Laplace is a known distribution the required expectation is obtained with ease, resulting in the maximization step,

[formula]

where a  =   - (n + p + 2), [formula], and [formula]. The component-specific multiplier on |βj| is obtained from the expectation of λj with respect to its conditional posterior distribution, π(λj|βj,σ2). Similar results to ([\ref=kstep]) are in Candes, Wakin and Boyd (2008), Cevher (2009), and Garrigues (2009).

An intuitive relationship to the adaptive [formula] of Zou (2006) and the one-step sparse estimator of Zou and Li (2008) can be seen via the Laplace mixture representation. As a computationally fast alternative to estimating the exact mode via the above [formula] algorithm, we can obtain a "one-step estimator" and exploit the [formula] algorithm as in Zou and Li (2008). The one-step estimator is

[formula]

with α† = 2σ2(0)(α + 1) and η†  =  σ(0)η. This estimator resembles the adaptive [formula]. The [formula] algorithm can be used to obtain [formula] very quickly. We refer to this estimator as [formula].

Remark 1. For η† = 0, the [formula] solution path for varying α† is identical to the adaptive [formula] solution path with γ = 1 (see (4) in Zou (2006)) using identical [formula].

Remark 2. [formula] forms a bridge between the [formula] and the adaptive [formula]: as η†  →    ∞   and α†  /  η†  →  λ†  <    ∞  , [formula] gives the [formula] solution with penalty parameter λ†.

We derive the [formula] estimator only to reveal a close connection with the adaptive [formula] of Zou (2006) and do not use it in our experiments.

4.4.3. Normal vs. Laplace Representations in Computation

As pointed out by an anonymous referee, it is appropriate to compare the convergence behavior of the [formula] algorithms that exploit different mixture representations. We generated n = {200,400,600,800,1000} observations from [formula], where the xij were independent standard normals for p = {20,40,60,80,100}, εi  ~  (0,σ2), and σ = 3. We set the first p / 4 components of [formula] to be 1 and the rest to 0. For each (n,p) combination we simulated 100 data sets and ran the [formula] algorithms obtained from normal and Laplace scale mixture representations. Figure [\ref=fig4] illustrates the number of iterations taken by the two algorithms until [formula]. As expected, the convergence under the Laplace mixture representation was much faster with the intermediary mixing parameter τj integrated out rather than using the expectation step in the [formula] algorithm.

4.5. Oracle Properties

Following Zou (2006) and Zou and Li (2008), we show that the [formula] and [formula] estimators possess oracle properties. Relaxing the normality assumption on the error term leads to two conditions for Theorem 2 and Theorem 3.

[formula] where [formula] are independent and identically distributed with mean 0 and variance σ2.

[formula], where [formula] is a positive definite matrix.

In what follows, [formula], [formula] retains the entries of [formula] indexed by A, and [formula] retains the rows and columns of [formula] indexed by A.

Let

[formula]

denote the [formula] estimator, where α'n = 2σ2(αn + 1) and η'n  =  σηn. Let [formula]. Suppose that α'n  →    ∞  , [formula] and, [formula]. Then [formula] is

consistent in variable selection in that [formula];

asymptotically normal with [formula].

Remark 3. More generally, the above results hold if [formula] and [formula].

Let [formula] denote the [formula] estimator in ([\ref=one_step]) and [formula]. Suppose that α†n  →    ∞  , [formula], and [formula]. Then [formula] is

consistent in variable selection in that [formula];

asymptotically normal with [formula].

The proofs are deferred to Section 8.

5. Experiments

5.1. Simulation

In this section, we compare the proposed estimators to the posterior means obtained under the normal, Laplace, and horseshoe priors, to the Bayesian model averaged ([formula]) estimator, as well as to the sparse estimates resulting from [formula] (Tibshirani (1996)) and [formula] (Fan and Li (2001)). [formula] and [formula] denote the posterior mean and the [formula] estimates, respectively, under the generalized double Pareto prior. Hyper-parameter values are provided in footnotes of Tables [\ref=tab1] and [\ref=tab2] when fixed in advance and are otherwise treated as random with the priors specified in Section 3. When not fixed, we first obtain the posterior means of the hyper-parameters from an initial Bayesian analysis, then use them in the calculation of the [formula] estimates.

We generated n = {50,400} observations from [formula], where the xij were standard normals with (xj,xj') = 0.5|j - j'|, εi  ~  (0,σ2), and σ = 3. We used the following [formula] configurations:

Model 1: 5 randomly chosen components of [formula] set to 1 and the rest to 0. Model 2: 5 randomly chosen components of [formula] set to 3 and the rest to 0. Model 3: 10 randomly chosen components of [formula] set to 1 and the rest to 0. Model 4: 10 randomly chosen components of [formula] set to 3 and the rest to 0. Model 5: [formula].

In our experiments [formula] and the columns of [formula] were centered and the columns of [formula] scaled to have unit length. For the calculation of competing estimators we used lars (Hastie and Efron (2011)), SIS (Fan et al. (2010)), monomvn (Gramacy (2010)) and BAS (Clyde and Littman (2005), Clyde, Ghosh, and Littman (2010)) packages in R. We mainly followed the default settings provided by the packages. Under the normal prior, the so-called "ridge" parameter was given an inverse gamma prior with shape and scale parameters 10- 3. Under the Laplace prior, as a default choice, a gamma prior was placed on the "[formula] parameter" λ2, as given in (6) of Park and Casella (2008), with shape and rate parameters 2 and 0.1, respectively. Under the horseshoe prior, the monomvn package uses the hierarchy given in Section 1.1 of Carvalho, Polson, and Scott (2010). For [formula], we used the default settings of the BAS package that employs the Zellner-Siow prior given in Section 3.1 of Liang et al. (2008). The tuning for [formula] and [formula] were carried out by the criteria given in Yuan and Lin (2005) and Wang, Li, and Tsai (2007), respectively, avoiding cross-validation.

100 data sets were generated for each case. In Table [\ref=tab1], we report the median model error. Model error was calculated as [formula], where [formula] is the variance-covariance matrix that generated [formula] and [formula] denotes the estimator in use. The values in the subscripts give the bootstrap standard error of the median model error values obtained. The bootstrap standard error was calculated by generating 500 bootstrap samples from 100 model error values, finding the median model error for each case, and then calculating the standard error for it. Under each model, the best three performances are boldfaced in the tables.

[formula] estimates showed a similar performance to that of horseshoe under sparse setups. [formula] (with α and η unknown) also showed great flexibility in adapting to dense models with small signals. [formula] estimates performed similarly to [formula] and much better than [formula], particularly so with increasing sparsity, signal and/or sample size. The [formula] and [formula] calculations are straightforward and computationally inexpensive due to the normal (and Laplace) scale mixture representation used. Being able to use a simple Gibbs sampler (especially when α  =  η = 1) makes the procedure attractive for the average user.

Letting α  =  η = 1 may be somewhat restrictive if the underlying model is very dense or very sparse, but in the cases we considered, it performed comparably to others and we believe that it constitutes a good default prior similar to standard Cauchy with the added advantage of thresholding ability. Although we do not take up p  ≫  n cases in this paper, in such situations much larger values of α would need to be chosen to adjust for multiplicity.

5.2. Inferences on Hyper-parameters

Here we take a closer look at the inferences on the hyper-parameters obtained from an individual data set for Models 2 and 5 from Section 5.1. This gives us some insight into how α and η are inferred with changing sample size and sparsity structure. Note that [formula] is more restrictive than [formula] as η is fixed, treating only α as unknown. Figure [\ref=fig5] gives the marginal posteriors of α and η in cases of [formula] and [formula] as described in Section 5.1, while Table [\ref=tab2] reports the posterior means for α and η, as well as model error (ME) performance (as calculated in Section 5.1) on the particular data set used. We clearly observe the adaptive nature and higher flexibility of [formula] moving from a sparse to a dense model with a big increase, particularly in η, flattening the prior on [formula]. There is not quite as much wiggle room in the case of [formula]. All it can do is to drive α smaller to allow heavier tails to accommodate a dense structure. As observed in Table [\ref=tab1], however, [formula] performs comparably in sparse cases.

6. Data Example

We consider the ozone data analyzed by Breiman and Friedman (1985) and by Casella and Moreno (2006). The original data set contains 13 variables and 366 observations. The modeled response is the daily maximum one-hour averaged ozone reading in Los Angeles over 330 days in 1976. There are p = 12 predictors considered and deleting incomplete observations leaves n = 203 observations. For validation, the data were split into a training set containing 180 observations and a test set containing 23 observations. We considered models including main effects, quadratic, and two-way interaction terms resulting in 290 possible subsets. The complex correlation structure of the data is illustrated in Figure [\ref=fig6].

Figure [\ref=fig7] summarizes the performance of the proposed estimators and their competitors. Median values for [formula] and the [formula] standard error intervals were obtained by running the methods on 100 different random training-test splits. Standard errors were computed via bootstrapping the medians 500 times.

The median number of predictors retained in the model by all three [formula] estimates was only 4 while it was 14 and 9 for [formula] and [formula]. Hence [formula] promoted much sparser models. In terms of prediction, [formula] yielded the second best results after [formula], with [formula], [formula], and the horseshoe estimator all having somewhat worse performance. These shrinkage priors are designed to mimic model averaging behavior, so we expected to obtain results that were competitive with, but not better than, [formula]. The improved performance for [formula] may be attributed to the use of default hyper-parameter values that were fixed in advance at values thought to produce good performance in sparse settings. Treating the hyper-parameters as unknown is appealing from the standpoint of flexibility, but in practice the data may not inform sufficiently about their values to outperform a good default choice. [formula] and [formula] both performed within the standard error range of [formula], while retaining a smaller number of variables in the model. As it is important to account for model uncertainty in prediction, the posterior mean estimator under the [formula] prior is appealing in mimicking [formula]. In addition, obtaining a simple model containing a relatively small number of predictors is often important, since such models are more likely to be used in fields in which predictive black boxes are not acceptable and practitioners desire interpretable predictive models.

7. Discussion

We have proposed a hierarchical prior obtained through a particular scale mixture of normals where the resulting marginal prior has a folded generalized Pareto density thresholded at zero. This prior combines the best of both worlds in that fully Bayes inferences are feasible through its hierarchical representation, providing a measure of uncertainty in estimation, while the resulting marginal prior on the regression coefficients induces a penalty function that allows for the analysis of frequentist properties under maximum a posteriori estimation. The resulting posterior mean estimator can be argued to be mimicking a Bayesian model averaging behavior through mixing over higher level hyper-parameters. Although Bayesian model averaging is appealing, it can be argued that allowing parameters to be arbitrarily close to zero instead of exactly equal to zero may be more natural in some problems. Hence we have a procedure that not only bridges two paradigms - Bayesian shrinkage estimation and regularization - but also yields three useful tools: a sparse estimator with good frequentist properties through maximum a posteriori estimation, a posterior mean estimator that mimics a model averaging behavior, and a useful measure of uncertainty around the observed estimates. In addition, the proposed methods have substantial computational advantages in relying on simple block-updated Gibbs sampling, while [formula] requires sampling from a model space with 2p models. Given the simple and fast computation and the excellent performance in small sample simulation studies, the generalized double Pareto should be useful as a shrinkage prior in a broad variety of Bayesian hierarchical models, while also suggesting close relationships with frequentist penalized likelihood approaches. The proposed prior can be used in generalized linear models, shrinkage of basis coefficients in nonparametric regression, and in such settings as factor analysis and nonparametric Bayes modeling.

8. Technical Details

The proof follows along similar lines as does the proof of Theorem 2 in Zou (2006). We first prove asymptotic normality. Let [formula] and Let [formula], suggesting [formula]. Now and we know that [formula] and [formula]. Consider the limiting behavior of the third term, noting that lim a  →    ∞(1 + b / a)a  =  b. If β*j  ≠  0, then [formula]. If β*j = 0, then [formula] which is 0 if uj = 0, and diverges otherwise. By Slutsky's Theorem [formula] is convex and the unique minimum of the right hand side is [formula]. By epiconvergence (Geyer (1994), Knight and Fu (2000)),

[formula]

Since [formula], this proves asymptotic normality.

Now [formula], [formula]; thus [formula]. Hence for consistency, it is sufficient to show that [formula], [formula]. Consider the event j'∈An. By the KKT optimality conditions, [formula]. Noting that [formula] by ([\ref=asymp_norm]), [formula], while By ([\ref=asymp_norm]) and Slutsky's Theorem, we know that both terms in the brackets converge in distribution to some normal, so This concludes the proof.

We modify the proof of Theorem 2 in Zou (2006). Here [formula] denotes the least squares estimator. We first prove asymptotic normality. Let [formula] and Let [formula], suggesting [formula]. Now

[formula]

and we know that [formula] and [formula]. Consider the limiting behavior of the third term. If β*nj  ≠  0 then, by the Continuous Mapping Theorem, [formula] and [formula]. By Slutsky's Theorem, [formula]. If β*nj = 0, then [formula] and [formula], where [formula]. Again by Slutsky's Theorem, [formula] is convex and the unique minimum of the right hand side is [formula] By epiconvergence (Geyer (1994), Knight and Fu (2000)),

[formula]

Since [formula], this proves the asymptotic normality.

Now [formula], [formula]; thus [formula]. We show that for all j'∉A, [formula]. Consider the event j'∈An. By the KKT optimality conditions, [formula]. We know that [formula], while By ([\ref=asymp_norm2]) and Slutsky's Theorem, we know that both terms in the brackets converge in distribution to some normal, so which proves consistency.

Acknowledgments

This work was supported by Award Number R01ES017436 from the National Institute of Environmental Health Sciences. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institute of Environmental Health Sciences or the National Institutes of Health. Jaeyong Lee was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (20110027353).

References

SAS Institute Inc., Durham, NC 27513, USA E-mail: artin.armagan@sas.com

Department of Statistical Science, Duke University, Durham, NC 27708, USA E-mail: dunson@stat.duke.edu

Department of Statistics, Seoul National University, Seoul, 151-747, Korea E-mail: leejyc@gmail.com