=1

A Round-Robin Tournament of the Iterated Prisoner's Dilemma with Complete Memory-Size-Three Strategies

Introduction and Motivation

The prisoner's dilemma [\cite=Flood1958] [\cite=_Axelrod1985] is probably the most prominent and most discussed example from game theory, which is a result of it standing as the model of the formation of coöperation in the course of biological as well as cultural evolution [\cite=_Axelrod1985] [\cite=Axelrod1981].

A naïve interpretation of Darwin's theory might suggest evolution favoring nothing but direct battle and plain competition. However numerous observations of coöperation in the animal kingdom oppose this idea by plain evidence. While such examples among animals are impressive in itself, clearly the most complex and complicated interplay of coöperation and competition occurs with humans; a fact which becomes most obvious when a large number of humans gathers as a crowd in spatial proximity. There are astonishing and well-known examples for both: altruism among strangers under dangerous external conditions [\cite=Sime1980] [\cite=Keating1982] [\cite=Laur1997] [\cite=Quarantelli2001] [\cite=Clarke2002] [\cite=Mawson2005] [\cite=Fahy2005] [\cite=Drury2009] as well as fierce competition on goods with very limited material value often linked with a lack of information [\cite=McFadden1991] [\cite=Schelajew2000] - and anything in between these two extremes; see for example the overviews in [\cite=Kretz2007a] [\cite=Schreckenberg2008]. In relation to these events - and possible similar events of pedestrian and evacuation dynamics [\cite=Schadschneider2009] to come in the future - the wide-spread naïve interpretation of the theory of evolution in a sense poses a danger, as it might give people in such situations the wrong idea of what their fellows surrounding them are going to do and by this in turn suggest overly competitive and dangerous behavior. Knowing of said historic events together with having an idea of theories that suggest why coöperation against immediate maximal self-benefit can be rational hopefully can immunize against such destructive thoughts and actions.

From the beginning the prisoner's dilemma was investigated in an iterated way [\cite=_Rapoport1965] [\cite=Trivers1971], often including that the ability of strategies to hark back on course of events of the tournament [\cite=_Axelrod1985] [\cite=_Axelrod1997] was unlimited, i.e. they had a memory potentially including every own and opponents' steps. Despite the possibility of using more memory the first strategy emerging as winner - tit-for-tat - did with a memory of only the most recent action of the opponent. Another famous and successful strategy - pavlov - also makes only use of a small memory: it just needs to remember its own and the opponent's action. In this contribution the effect of an extension of the memory up to the three latest actions of the opponent and up to two latest own actions is investigated.

In the course of discussion of the prisoner's dilemma a number of methods have been introduced like probabilistic strategies to model errors ("noise") [\cite=Nowak1993], evolutionary (ecologic) investigation [\cite=_Axelrod1985], spatial relations (players only play against spatially neighbored opponents) [\cite=Baek1989] [\cite=Nowak1992] [\cite=Nowak1993b] [\cite=Grim1997] [\cite=Nakamaru1997] [\cite=Kirchkamp2000] [\cite=Schweitzer2002] [\cite=Masuda2003] [\cite=Fort2005] [\cite=Alonso2006], and creation of strategies by genetic programming [\cite=Axelrod1981] [\cite=Nowak1993] [\cite=Holland1992] [\cite=Michalewicz1996] [\cite=Salhi1996]. Most of these can be combined. For an overview on further variants see review works like [\cite=Doebeli2005] [\cite=Kuhn2007].

Contrary to these elaborate methods, a main guiding line in this work is to avoid arbitrary and probabilistic decisions like choosing a subset of strategies of a class or locating strategies spatially in neighborhoods; spatial variants as well as a genetic approach are excluded. Instead each strategy of the class participates and plays against each other. A consequence from investigating complete classes is that it is impossible to have a continuous element as constructing element of a strategy; this forbids probabilistic strategies. The round-robin mode as well - at least in parts - is a consequence of avoiding arbitrariness: drawing lots to choose pairs of competitors like in tournaments would bring in a probabilistic element. In other words: the source code written for this investigation does not at any point make use of random numbers. It is a deterministic brute force calculation of a large number of strategies and a very large number of single games. The relevance lies not in modeling a specific system of reality, but in the completeness of the investigated class and in general the small degree of freedom (arbitrariness) of the system.

By the strictness and generality of the procedure, a strategy can be seen as a Mealy automaton or the iterative game between two strategies as a Moore machine [\cite=Moore1956] [\cite=Abreu1988] [\cite=Linster1992] [\cite=Miller1996] respectively a spatially zero-dimensional cellular automaton [\cite=_Wolfram1986] [\cite=_Wolfram2002] (see section [\ref=sec:ca]).

Definition of a Strategy

In the sense of this paper a strategy with a memory size n has n + 1 sub-strategies to define the action in the first, second, ... nth and any further iteration. The sub-strategy for the first iteration only decides, how a strategy starts the tournament, the sub-strategy for the second iteration depends on the action(s) of the first iteration, the sub-strategy for the third iteration depends on the actions in the first and second iteration (if memory size is larger one) and the sub-strategy for the (N > n)th iteration depends on the actions in the (N - n) to (N - 1)st iteration (compare Figure [\ref=fig:example]).

A similar approach has been followed in [\cite=Beaufils1998], but there are differences in the definition of the class concerning the behavior in the first n - 1 iterations and most important it has not been used for a round-robin tournament with all strategies of a class, but combined with a genetic approach.

Another investigation dealing with effects of memory size is [\cite=Hauert1997]. The difference there is that the strategies are probabilistic and (therefore) not all strategies participate in the process.

Data Size of a Strategy, Number of Strategies, and Number of Games

Since at the beginning there is no information from the opponent a strategy consists of a decision, how to begin an iterated game (one bit). In the second round, there is only information on one past step from the opponent, so the strategy includes the decision how to react on this (two bits), the third step is still part of the starting phase and therefore also has its own part of the strategy (four bits, if the decision does not depend on a strategy's own preceding action). Therefore there are 128 strategies if there is a no-own-two-opponent memory. Finally with size-three memory, one has eight more bits. As an example in Figure [\ref=fig:example] it is shown, how one calculates the number combination (1/2/12/240) from the tit-for-tat strategy. These 15 bits lead to a total of N = 32768 different strategies. If each strategy plays against each other strategy and against itself one has to calculate N  ·  (N + 1) / 2 = 229 different iterated prisoner's dilemmas.

Table [\ref=tab:memorysizes] sums up these numbers for different memorysizes. To remember the last n actions of a pair of strategies, one needs 2n bits and for the results of a strategy over the course of iterations one needs - depending on the kind of evaluation - a few bytes for each pair of strategies. The number of pairs of strategies - and this is the limiting component - grows at least approximately like 22n + 2 - 3. On today's common PCs RAM demands are therefore trivial up to a memory size of n = 2, in the lower range of 64-bit technology (some GBs of RAM) for n = 3, and totally unavailable for n = 4 and larger (more than an exabyte).

The Cellular Automata Perspective

This section serves to have another perspective at the system in terms of cellular automata. This can help to get a visual idea of the system dynamics. However, the reader may well skip this and proceed to the next section.

Wolfram's elementary cellular automata are defined (or interpreted) to exist in one spatial plus one temporal dimension. However, one can also apply the rules to a point-like cellular automaton with memory. Figure [\ref=fig:ca-demo1] shows an example for this. One can also interpret this system not as a cellular automaton that has a memory and a binary state, but as an automaton that can have one of eight states with transitions between the states being restricted. For the full set of 256 rules each state can be reached in principle from two other states and from a particular state also two states can be reached. Choosing a specific rule is selecting one incoming and one outgoing state. This is exemplified in Figure [\ref=fig:transitions] for rule 110. For the iterated prisoner's dilemma one needs two such cellular automata that interact that determine their next state from the data of the other automaton as shown in Figure [\ref=fig:ca-demo2]. It is of course possible to interpret two interacting cellular automata as one single point-like cellular automaton with a larger set of states. Then Figure [\ref=fig:ca-demo2] would translate to Figure [\ref=fig:ca-demo3]. One now again could draw a transition graph (with 64 nodes that all have one of four possible incoming and outgoing links or a specific combination of rules) for further theoretical analysis. For this work we shall now abandon these basic and theoretical considerations and just adhere to the fact that the implementation of the process can be seen as a cellular automaton, more precisely an enormous number of combination of interacting very simple cellular automata.

Payoff Matrix

The four values T, R, P, and S of the payoff matrix (see Table [\ref=tab:payoffmatrix]) need to fulfill the relation

[formula]

to be faced with a prisoner's dilemma. For the purpose of this contribution one can choose S = 0 without loss of generality, as whenever the payoff matrix is applied all strategies have played the same number of games. In addition to equation [\ref=eq:basic] it is often postulated that

[formula]

holds.

The equation

[formula]

as well marks a special set of payoff matrices, as those values can be seen as model of a trading process, where the exchanged good has a higher value for the buyer i than the seller j:

[formula]

where δ = 1, if a player coöperates and δ = 0 if he defects. Therefore β can directly be interpreted as the "gain from receiving" value and γ the "cost from giving" value. α is a constant to guarantee pij  ≥  0. T, R, P for technical convenience, and S can be calculated from these: T = α  +  β, R = α  +  β  -  γ, P = α, and S = α  -  γ. Aside from the descriptive interpretation as "gain from receive" and "cost to give" this reparametrization has the advantage that the original condition equation ([\ref=eq:basic]) and the additional conditions equation ([\ref=eq:2RT]) and S = 0 reduce to β  >  γ  =  α. Furthermore it is the form of the basic equation in George Price's model for the evolution of coöperation [\cite=Price1970] [\cite=Frank1995].

As we do not only want to investigate payoff matrices, where equations ([\ref=eq:2RT]) and ([\ref=eq:TSPR]) hold, we rewrite

[formula]

In principle one could set P = 1 without loss of generality, but then it was not possible to write all combinations holds/does not hold of equations ([\ref=eq:2RT]) and ([\ref=eq:TSPR]) with integer-valued T and R. Now equation ([\ref=eq:TSPR]) simply can be written as

[formula]

and shall be investigated as one variant next to b > 1 and b < 1. And equation ([\ref=eq:2RT]) writes

[formula]

Here as well a + 1 = b and a + 1 < b will be investigated (always taking care that a > 0 and b > 0 hold). Finally, a( < , = , > )1 and a( < , = , > )b are relevant conditions, if it's possible to distinguish in this way.

Obviously not all combinations of these conditions can hold simultaneously. (a + 1 < b, b < 1) for example has no allowed solution. The allowed combinations and the values for T, R, and P are shown in Table [\ref=tab:TRP]. For each combination of conditions an infinite number of values could have been found. One could have chosen to interpret ">  " as "much greater than" but then selecting specific numbers in a way would have been arbitrary. So the smallest numbers to fulfill a set of conditions have been chosen as representatives.

Iteration, Tournament, and Scoring

In an iteration step all strategies play a prisoner's dilemma against any of the other strategies and themselves. For this a strategy calculates its action from the preceding actions of the specific opponent. If Ntij, Nrij, Npij, Nsij are the counters, how often strategy i received a T, R, P or S payoff playing against a specific strategy j, in each iteration step for each i and each j one of the four Nxij is increased by one.

Now all the payoff matrices from Table [\ref=tab:TRP] are applied one after the other to calculate for each payoff matrix for each strategy i the total payoff G1i:

[formula]

The strategy (or set of strategies) i yielding the highest G1i is one of the main results for a specific iteration round and a specific payoff matrix.

Then the tournament is started. Each tournament round g is started by calculating the average payoff of the preceding tournament round:

[formula]

where δgi = 1, if strategy i was still participating in the tournament in tournament round g and δgi = 0 else. Then δg + 1i is set to 0, if δgi = 0, or if a strategy scored below average:

[formula]

The payoff for the next tournament round g + 1 is calculated then for all strategies still participating in the tournament:

[formula]

The tournament ends, if only one strategy remains or if all remaining strategies score equal in a tournament round (i.e. they have identical Ggi). The strategies, which manage to emerge as winners of such a tournament are the second main result for a specific iteration step and a specific payoff matrix.

Such an elimination tournament can be interpreted as an evolutionary tournament, where the frequency values for the strategies can only take the values f = 0 and f = 1.

To state it explicitly: all strategies participate again in the next iteration step for another first round of the tournament. The elimination process only takes place within an iteration step and not across iteration steps, and there is no prisoner's dilemma game played in or between the rounds of a tournament. As all strategies are deterministic this procedure is equivalent to playing the prisoner's dilemma a fixed number of iterations, evaluate the scores, eliminate all strategies scoring below average and play again the fixed number of iterations with the remaining strategies, and so on.

Results

In this section for all payoff matrices of Table [\ref=tab:TRP] the strategies are given that for large numbers of iteration steps have the highest payoff G1i in the first round of the tournament and those strategies that win the tournament - if the system stabilizes to one winner. Additionally the iteration round is given, when this winning strategy (strategies) appeared for the first time to stay continuously until the last calculated iteration. This implies that for a certain payoff matrix prior to this iteration the number of iterations is important for the question which strategy will emerge as the best (in the sense described in section [\ref=sec:score]).

Results for No-Own-One-Opponent Memory

With only one action to remember, there are just 8 strategies (named (0/0) to (1/3)). (0/0) never coöperates, (1/3) always. TFT is (1/2). 1000 iteration steps were done. It's safe to say that this is sufficiently long, as the results - shown in tables [\ref=tab:1-0] and [\ref=tab:1-1] - stabilize at latest in iteration step 16 (respectively 179).

Results for One-Own-One-Opponent Memory

With this configuration beginning with the second iteration step strategies base their decision on two bits, one (the higher bit) in which is encoded the action of their opponent and one in which is remembered their own action. For an overview in Table [\ref=tab:11] numbers and behaviors are compared.

For this and all further settings 10,000 iterations (and in special cases more) have been simulated. Results are shown in tables [\ref=tab:11-0] and [\ref=tab:11-1].

Results for No-Own-Two-Opponent Memory

Now 10,000 iteration steps were carried out. Again this is far more than the largest number of iterations before the process settles down in some way. Now TFT is (1/2/12) and TF2T is (1/3/14). Results are shown in tables [\ref=tab:2-0] and [\ref=tab:2-1].

Results for One-Own-Two-Opponent Memory

In this case, one could in principle reduce the size of the strategy, as it makes no sense to distinguish between strategies that coöperate or defect in the second iteration, if hypothetically they coöperate in the first iteration, when in fact they defect in the first iteration. For the simulation the number of strategies has not been reduced to the subset of distinguishable ones, as this would have been a source of error for the source code, and at this stage, the effect on required resources for computation is negligible. Thus for each strategy there are three more that yield exactly the same results against each of the strategies. In the table of results (table [\ref=tab:12-0]) just one of the four equivalent strategies is given - the one with the smallest number. This means that in case of initial defection adding 2, 8, or 10 to the middle number gives the equivalent strategies and in case of initial coöperation, it is 1, 4, or 5. Therefore TFT is (1/8/240), (1/9/240), (1/12/240), and/or (1/13/240). Even when the results are reduced by naming only one of four strategies linked in this way, this is the first configuration, where the results are too complicated to be understandable at a glance.

There are even more strategies that yield identical results in any combination with any other player: for all strategies that continue to defect (coöperate) on own defection (coöperation) those elements of the strategy that determine what to do, following an own coöperation (defection) are never applied and the value of these elements has no effect. This phenomenon leads to a large number of strategies winning the tournament. Interestingly for some of the payoff matrices the number of winners is smaller around 20 or 30 iterations than at larger numbers of iterations.

For this memory configuration there is almost no difference in the results, if strategies play against themselves or not: the strategies with the most points in the first round of the tournament, and the number of strategies winning the tournament are the same in both cases. Only if the number of strategies winning the tournament is large, a small number of strategies might be exchanged and the iteration round, when the results are stable, is different. In iteration rounds before stability, there can be larger differences, however. We refrain from giving a result table for the case when strategies do not play against themselves.

Results for Two-Own-One-Opponent Memory

This configuration is interesting as one can interpret a strategy considering a remembered opponent's action as reaction to an as well remembered own action. While TFT is (1/8/240), a strategy additionally coöperating in such a case would be (1/8/244). As Table [\ref=tab:21-0] shows, sometimes only TFT appears among the winners of the tournament, sometimes both these strategies. Only with payoff matrix 6-5-2 the more forgiving strategy wins but not TFT. It is the more tricky strategy (1/8/228) that applies this kind of forgiveness, which is more successful than TFT.

In this setting as well, it has only minor effects if a strategy plays against itself or not. Therefore the results for the case when they do not is omitted.

Results for No-Own-Three-Opponent Memory

Regarding the number of strategies this setting is the largest investigated in this work. The number of iterations until the results settle varies greatly among the various payoff matrices. In fact for some payoff matrices they did not stabilize before the 30,000th iteration. At this point we refrained from performing further calculations and accepted the (non-)result as open issue for future investigations. However, even for payoff matrices with which stable results appear to have been reached it cannot be excluded that after some 10,000 iterations more different winners would result, as in the more volatile cases. Another surprising observation was that the results sometimes appeared to have reached a final state but then started changing again. After all, for remembering one opponent's action, stable results appeared after approximately 10 iterations, for remembering two opponents' moves it was about 1,000 iterations. So, it is not unrealistic to assume that remembering three opponents' actions may need 100,000 or even more iterations until the results do not change anymore.

Further difficulties may arise from precision issues in the calculation. During the tournament it is decided by comparison with the average of points, if the strategies may participate in the next round. The average is calculated by dividing one very large number by another very large number. As a consequence the size comparison between average and individual result may be faulty, if in fact a strategy has exactly achieved the average of points and by this kicked out of the tournament. Another resource problem is the possibility that the sum of points produces an overflow in the corresponding integer variable. That such considerations could be relevant when dealing with such large numbers is based on general experience with complex simulations; in the results there was no explicit hint that such issues really occured, except for that the long instability of results that appeared to be surprising in principle could be attributed to them. Ruling them out would need a second computer system with different hardware architecture or a very thorough understanding of the CPU and the compiler that were used. None of these were sufficiently available. Additionally one has to consider that each simulation run currently takes days to arrive at the number of iterations where these issues could be relevant. In a nutshell: using up-to-date standard computer systems the no-own-three-opponent-memory case today is at the edge of what is accessible. Definately ruling out negative effects that falsify the results and doing this with maintainable effort remains to be done in the future.

As calculating the payoff and evaluating the tournament takes more computation time than calculating the results of the dilemma, beyond 10,000 iterations only for the last one hundred iterations ahead of the full thousands payoff and tournament were calculated. This in turn implies that the iteration number after which the results did not change anymore can only be given approximately.

Having said all this, it becomes obvious that the results of this section need to be considered as preliminary - the more the later the assumed stability was observed.

A different problem is that in some cases the number of winners of the tournament is too large to give all of the winning strategies in this paper. However, the remaining cases should be sufficient to demonstrate the type and especially variants of strategies winning the tournament

A majority of the strategies winning the first round of the tournament coöperate, when the earliest opponent's action they remember was coöperation and any other defection. This is a trend which was already present with the one element smaller memory, but it was not as pronounced. This strategy is interesting in a sense as it uses the last chance to avoid breaking entirely with the opponent. To find a catchy name for this strategy, recall Mephisto's behavior toward God in the Prologue in Heaven of Faust I: "The ancient one I like sometimes to see, And not to break with him am always civil", where even considering all the competition between the two, Mephisto avoids entirely abandoning coöperation. If one extrapolates Mephisto to even larger memory sizes, coöperation vanishes more and more, although there is some basic coöperative tendency kept in the strategy. There are two questions: if this trend would actually continue infinitely, when memory size is increased further, and what it means that for example the case of one-own-two-opponent-memory-size yields strategies as winners of the first round of the tournament that have entirely different characteristics.

The results are shown in table [\ref=tab:3-0].

Summary and Outlook

The calculations of this work reveal a strong dependence of the results of the tournament on the details of the payoff matrix. It is not sufficient to distinguish, if T  +  S  =  R  +  P and 2R  >  T  +  S hold or not. This means that one has to be careful drawing conclusions, if the prisoner's dilemma is used as a toy model for some real system. Of course, as this work restricted strategies to limited memory size, there might be strategies relying on infinite memory that outperform all of these regardless of the details of the payoff matrix. So, the main result of this work is not that everything changes with a different payoff matrix, but that one should not be too faithful that the precise choice of the payoff matrix is irrelevant.

As expected the two basic relations T  +  S  =  R  +  P and 2R  >  T  +  S clearly have an influence on the results, as subsets of strategies appear among the winners in tendency depending if these relations hold or if they do not so. The picture is a bit different for the winner of the first round of the tournament, when all strategies still participate: there are fewer strategies appearing as winners, but if there is more than one for a memory configuration, there is no obvious pattern based on these relations that tells which strategy wins if a specific payoff matrix is applied. In total, one cannot claim that the details of the payoff matrix will dominate each element of the results in any case. However, in general one can say that the results do depend on the specific choice of the payoff matrix. Furthermore it is not only not possible to find one generally best or a set of generally best strategies, but - if one compares the winners of the first round of the tournament and the tournament as a whole - even for a specific payoff matrix it cannot be decided in general, if coöperating is a good or bad idea, as this depends on the kind of result that decides about the winner.

While for these reasons, it is usually not possible to use the prisoner's dilemma as some kind of proof that in some real system coöperating yields best payoff, the results of this work - as of a lot of preceding works - helps to bear in remembrance that coöperating might be the better idea, even if at first sight one might have the opposite impression. The iterated prisoner's dilemma obviously is an abstract and simplified model for any real social system and the four entries of the payoff matrix often are not set quantitatively by the real system. In such cases conclusions drawn from calculations can only be valid if the results do not significantly depend on details of the payoff matrix.

In some cases the results stabilized only after a very large number of iterations, a number far larger than for example the number of iterations in the tournaments performed by Axelrod [\cite=_Axelrod1985]. This does not necessarily mean that it is useless to investigate cases with fewer iterations, as also before the results stabilize, the results oscillate between two sets or between a set and a proper subset. As the number of iterations for stability grows with the number of participating strategies and as the number of participating strategies is already quite large in cases, when stability only occurs beyond 1000 iterations, one can assume that for most investigations of the iterated prisoner's dilemma that have been published so far, the number of iterations was sufficiently high. Still, the results of this work indicate that an investigation of the effect of having ±  20 iterations usually should be worth the effort.

The results show a tendency that for increased memory size somewhat coöperative strategies score better. There have been investigations on the dependency of good memory and scoring in an iterated prisoner's dilemma [\cite=Milinski1998] [\cite=Winkler2008], however, the facing work is rather indifferent on this issue. With memory size also the number of strategies increases and coöperative strategies find more strategies that coöperate as well. A comparison of Tables [\ref=tab:1-0] and [\ref=tab:1-1] supports this idea, as it shows how it benefits coöperative strategies, when there is one more coöperative counterpart (themselves) participating in the tournament. The fact that with increasing memory size in the end it does not play any further role, if strategies play themselves or not, shows that in these cases the strategies are related to some of the others, in a way that in effect playing against them is as playing against themselves. On the other hand, if a good memory would not matter then there should be more strategies among the winners that do not make use of principally available more past information.

In this work the results have mainly been presented and - despite the considerable extent of the paper - only scarcely been analyzed and discussed. There are plenty of possibilities to discuss the success or poor performance of a specific strategy in a specific memory configuration with a specific payoff matrix in analytical terms. For settings that yield large sets of tournament winners, the results can be investigated statistically. Once stronger computational resources are available larger memories can be investigated and the case of remembering three opponent's actions can be investigated more reliable.

In this work the idea was to simulate as many rounds as are necessary to yield stable results. The development of the results over the rounds was not and thus could be investigated in further studies.

For the tournament itself one can think of many variants. One could for example only eliminate those strategies scoring worst in an iteration, or eliminate always (as far as possible) exactly half of the strategies still running. It is also possible to allow initial population weights different than one.

And finally the role of the payoff matrix can be investigated in greater depth. In this work no two payoff matrices always gave the same result (although the results of 7-4-2 and 9-5-3 were always at least similar). Is it possible at all that two payoff matrices that are not related trivially yield the same results? And if this is the case, what is (if it exists) the simplest parametrization and set of relations between the parameters, which generates all payoff matrices that yield all possible results? Can the winning strategies or the number of iterations until stability be derived analytically?

The differences between the results with different payoff matrices might as well reduce, if the tournament were not carried out in a binary way, but if the frequency of a strategy could take a real value and frequencies of a round were dependent on the score (fitness) of the preceding round. It would then be possible for a strategy to score below average for example in the first round, but recover in subsequent rounds.

Acknowledgments

The author is grateful to his company PTV - Planung Transport Verkehr for providing the computing hardware and computation time.