Definition Remark Lemma Corollary

Introduction

A basic problem in pattern recognition [\cite=Duda-2000-PCL] is to find a numerical value that represents the dissimilarity or 'distance' between any two input patterns of the domain. For instance, between two binary sequences that represent document files or between genetic sequences of two living organisms. There are many distances defined in different fields of mathematics, engineering and computer and information sciences [\cite=Deza-09]. A good distance is one which picks out only the 'true' dissimilarities and ignores those that arise from irrelevant attributes or due to noise. In most applications the design of a good distance requires inside information about the domain, for instance, in the field of information retrieval [\cite=YAtes99] the distance between two documents is weighted largely by words that appear less frequently since the words which appear more frequently are less informative. The ubiquitous Levenshtein-distance [\cite=Levenshtein1966] measures the distance between two sequences (strings) as the minimal number of edits (insertion, deletion or substitution of a single character) needed to transform one string into another. Approximate string matching [\cite=Nava] is an area that uses such edit-distances to find matches for short strings inside long texts. Typically, different domains require the design of different distance functions which take such specific prior knowledge into account. It can therefore be an expensive process to acquire expertise in order to formulate a good distance. The paper of [\cite=LZ76] introduced a notion of complexity of finite binary string which does not require any prior knowledge about the domain or context represented by the string (this is sometimes referred to as the universal property). This complexity (called the production complexity of a string) is defined as the minimal number of copy-operations needed to produce the string from a starting short-string called the base. This definition of complexity is related to Levenshtein-distance mentioned above. It is proportional to the number of distinct phrases and the rate of their occurrence along the sequence. There has been some work on using the LZ-complexity to define a sequence-distance measure in bioinformatics [\cite=Sayood2003]. Other applications of the LZ-complexity include: approximate matching of strings [\cite=Nava], analysis of complexity of biomedical signals [\cite=Aboy2006], recognition of structural regularities [\cite=Orlov2004], characterization of DNA sequences [\cite=Gusev1999] and responses of neurons to different stimuli [\cite=Amigo2003], study of brain function [\cite=Wu-1991] and brain information transmission [\cite=Xu-97] and EEG complexity in patients [\cite=Abasolo-2006].

In the current paper we introduce a distance function between two strings which also possesses this universal property. Our approach is to consider a binary string as a set of substrings [\cite=Ratsaby-CRI-2008]. To represent the complexity of such a set we use the notion of combinatorial entropy [\cite=Ratsaby2006a] and introduce a new set distance function. We proceed to describe some fundamental concepts concerning entropy and information of sets.

Entropy and information of a set

Kolmogorov [\cite=Kolmogorov65] investigated a non-stochastic measure of information for an object y. Here y is taken to be any element in a finite space [formula] of objects. He defines the 'entropy' of [formula] as [formula] where [formula] denotes the cardinality of [formula] and all logarithms henceforth are taken with respect to 2.

As he writes, if it is known that [formula] then this provides [formula] bits of 'information' or in his words "this much entropy is eliminated". To represent partial information about [formula] based on another information source [formula] let [formula] be a general finite domain and consider a set

[formula]

that consists of all permissible pairs (x,y)∈R (in the usual probabilistic-based representation of information this is analogous to having a uniform prior probability distribution over a certain region of the domain). The entropy of [formula] is defined as

[formula]

where [formula] denotes the projection of A on [formula]. Consider the restriction of A on [formula] based on x which is defined as

[formula]

then the conditional combinatorial entropy of [formula] given x is defined as

[formula]

Kolmogorov defines the information conveyed by x about [formula] by the quantity

[formula]

In [\cite=Ratsaby_IW] an alternative view of [formula] is defined as the information that a set Yx conveys about another set [formula] satisfying [formula]. Here the domain R is defined based on the previous set A as [formula] which consists of all permissible pairs (y,y') of objects. Knowledge of [formula] means knowing the set Ax  ⊆  R, [formula]. The information between Yx and [formula] is then defined as

[formula]

Clearly, [formula]. Note that [formula] measures the difference in description length of any pair of objects [formula] when no 'labeling' information exists versus that when there exists information which labels one of them as being an element of Yx. Thus the second term in ([\ref=eq:btts]) can be viewed as the conditional combinatorial entropy of [formula] given the set Yx. In [\cite=Ratsaby2006a] [\cite=Ratsaby_IW] [\cite=RATSABY_DBLP:conf/sofsem/Ratsaby07] this is used to extend Kolmogorov's combinatorial information to a more general setting where knowledge of x still leaves some vagueness about the possible value of y.

While the distance that we introduce in this paper is general enough for any objects, our interest is to introduce a combinatorial distance for binary strings. We henceforth drop the finiteness constraint on [formula] and [formula] and refer to [formula] as the set of finite binary strings x. Each string [formula] is a description of a corresponding set Yx contained in the set [formula] of objects y. Our approach to defining a distance between two binary strings x and x' is to relate them to sets of objects and then measure the distance between the two corresponding sets. Denote by PF(X) the set of all finite subsets of a set X. Let [formula] be a function which defines how a description (binary string) x yields a set [formula]. In general, M may be a many-to-one function since there may be several strings (viewed as descriptions of the set) of different lengths for a given set. In the context of the above, we now consider a permissible pair (x,y)∈A to be one which consists of an object y that is contained in a set Yx which is described by x. Clearly, not every possible pair (x,y) is permissible, as for instance, if [formula] then (x,y') is not permissible.

In the next section we introduce a combinatorial information distance. We start with a distance for general sets and then apply it as a distance between binary strings.

The distance

In what follows, Ω is a given non-empty set which serves as the domain of interest. The cardinality of any set A is denoted by [formula] and the set of all finite subsets of Ω is denoted by PF(Ω). Define [formula] as follows:

[formula]

For each pair of finite sets A,B  ⊂  Ω define the following function [formula] which maps a pair of finite sets into the non-negative integers,

[formula]

where [formula] denotes the complement of the set A and log  is with respect to base 2. It is simple to realize that δ(A,B) equals [formula] with the exception when A or B is empty or B  ⊆  A.

Note that δ is non-symmetric, i.e., δ(A,B) is not necessarily equal to δ(B,A). Also, δ(A,B) = 0 when B  ⊆  A (not only when A = B).

From an information theoretical perspective [\cite=Kolmogorov65] the value [formula] represents the additional description length (in bits) of an element in B given a priori knowledge of the set A. Hence we may view A as a partial 'dictionary' while the part of B that is not included in A takes an additional [formula] bits of description given A.

The following set will serve as the underlying space on which we will consider our distance function. It is defined as

[formula]

It is the power set of Ω but without the empty set and singletons. We note that in practice for most domains, as for instance the domain of binary strings considered later, the restriction to sets of size greater than 1 is minor.

The following lemma will be useful in the proof of Theorem [\ref=main-theom].

The function δ satisfies the triangle inequality on any three elements A, B, C∈P+F(Ω) none of which is strictly contained in any of the other two.

Suppose A,B,C are any elements of P+F(Ω) satisfying the given condition. It suffices to show that

[formula]

First we consider the special case where the triplet has an identical pair. If A = C then by Remark [\ref=rem:Note-that-] it follows that δ(A,C) = 0 which is a trivial lower bound so ([\ref=eq:main]) holds. If A = B then δ(A,B) = 0 and both sides of ([\ref=eq:main]) are equal hence the inequality holds (similarly for the case of B = C).

Next we consider the case where each of the following three quantities satisfies

[formula]

By definition of P+F(Ω) we have [formula] hence

[formula]

Next, we claim that [formula]. If [formula] then x∈C and [formula]. Now, either x∈B or [formula] . If x∈B then because [formula] it follows that [formula]. If [formula] then because x∈C it follows that [formula]. This proves the claim. Next, we have

[formula]

It suffices to show that

[formula]

We claim that if three non-empty sets X,Y,Z satisfy [formula] then [formula]. To prove this, it suffices to show that [formula]. That this is true follows from [formula]. By ([\ref=eq:cb]), we may let [formula], [formula] and [formula] and from both of the claims it follows that

[formula]

Taking the log on both sides of ([\ref=eq:logs]) and using the inequality 2  ≤    #  B (which follows from B∈P+F(Ω)) we obtain

[formula]

This proves ([\ref=eq:ca]).

Next, we define the information set-distance.

For any two finite non-empty sets A,B define the information set-distance as

[formula]

In the following result we show that d satisfies the properties of a semi-metric.

The distance function d is a semi-metric on P+F(Ω). It satisfies the triangle inequality for any triplet A,B,C∈P+F(Ω) such that no element in the triplet is strictly contained in any of the other two.

That the function d is symmetric is clear from its definition. From Remark [\ref=rem:Note-that-] it is clear that for A = B, δ(A,B) = δ(B,A) = 0 hence d(A,B) = 0. Consider any pair of sets A,B∈P+F(Ω) such that A  ≠  B. If [formula] or A  ⊂  B or B  ⊂  A then at least one of the two values δ(A,B) or δ(B,A) is greater than zero so d(A,B) > 0. This means that d is a semi-metric on P+F(Ω). Next, we show that it satisfies the triangle inequality for any triplet A,B,C∈P+F(Ω) such that no element is strictly contained in any of the other two. For any non-negative numbers a1, a2, a3, b1, b2, b3, that satisfy

[formula]

we have

[formula]

From Lemma [\ref=rem:Note-that-] it follows that ([\ref=eq:abb]) holds for the following: a1  =  δ(A,C), b1  =  δ(C,A), a2  =  δ(A,B), b2  =  δ(B,A), a3  =  δ(B,C), b3  =  δ(C,B). This yields

[formula]

hence d satisfies the triangle inequality for such a triplet.

Currently, it is an open question as to whether a normalized version of the distance d exists such that the properties stated in Theorem [\ref=main-theom] are still satisfied.

Distance between strings

Let us now define the distance between two binary strings. In this section, we take Ω to be a set [formula] of objects. Denote by [formula] the set of all (finite) binary strings. Our approach to defining a distance between two binary strings x, [formula] is to relate them to subsets [formula] and measure the distance between the two corresponding subsets. Each string [formula] is a description of a corresponding set Yx∈P+F(Ω). Define a function [formula] which dictates how a string x yields a set [formula]. In general, M may be a many-to-one function since there may be several strings (viewed as descriptions of the set) of different lengths for a given set.

Let [formula] be all possible string-object pairs (x,y) and let M be any function [formula]. If [formula] are two binary strings then the information set-distance between them is defined as

[formula]

where the function d is defined in Definition [\ref=def:info-set-dist].

The next result follows directly from Theorem [\ref=main-theom].

Let [formula] be a set of objects y and [formula] a set of all finite binary strings x. Let [formula] be any function that defines the set [formula] of cardinality at least 2 described by x, for all [formula]. The information set-distance dM(x,x') is a semi-metric on [formula] and satisfies the triangle inequality for triplets x, x',x'' whose sets M(x), M(x'), M(x'') are not strictly contained in any of the other two.

As an example, consider a mapping M that takes binary strings to sets Y in [formula] (the k-cube) for some fixed finite k. Denote by k-word a vertex on the cube. Consider the following scheme for associating finite strings x with sets: given a string x, break it into non-overlapping k-words while, if necessary, appending zeros to complete the last k-word. Let the set M(x) = Yx be the collection of these k-words. For instance, if x = 100100110 then with k = 4 we we obtain the set [formula]. If a string has N > 1 repetitions of some k-word then clearly only a single copy will be in Yx. In this respect, M eliminates redundancy in a way that is similar to the method of [\cite=LZ76] which gives the minimal number of copy operations needed to reproduce a string from a set of its substrings.

Another mapping M may be defined by scanning a fixed window of length k across the string x and collecting each substring (captured in the window) as an element of the generated set Yx. For instance, suppose an alphabet has 26 letters and there are 26n possible n-grams (substrings made of n consecutive letters). If x is a document then it can be broken into a set M(x) of n-grams. Each letter is represented by 7 bits. We extract words of length k = 7n bits, starting with the first word in the string then moving 7 bits to the right and extracting the next k-bit word, repetitively, until all words are collected. Thus dM measures the distance between two documents. In comparison, the n-gram model in the area of information retrieval [\cite=YAtes99] represents a document by a binary vector of dimensionality 26n where the ith component is 1 if the document contains the ith particular n-gram and is 0 otherwise. Here a similarity (opposite of distance) between two documents is represented by the inner product of their corresponding binary vectors.

Yet another approach which does not need to choose a value for k is to proceed along the line of work of [\cite=LZ76]. Here we can collect substrings of x (of possibly different lengths) according to a repetitive procedure in order to form the set M(x) (in [\cite=LZ76] the cardinality of the set M(x) is referred to as the complexity of x).

Whichever scheme M is used, to compute the information set-distance dM(x,x') between two finite strings x and x' we first determine the sets M(x) and M(x') and then evaluate their distance according to Definition [\ref=def:comb-dist] to be d(M(x),M(x')).