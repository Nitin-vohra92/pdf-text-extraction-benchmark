Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications

INTRODUCTION

wireless sensor network (WSN) is composed typically of multiple autonomous, tiny, low cost and low power sensor nodes. These nodes gather data about their environment and collaborate to forward sensed data to centralized backend units called base stations or sinks for further processing. The sensor nodes could be equipped with various types of sensors, such as thermal, acoustic, chemical, pressure, weather, and optical sensors. Because of this diversity, WSNs have tremendous potential for building powerful applications, each with its own individual characteristics and requirements. Developing efficient algorithms that are suitable for many different application scenarios is a challenging task. In particular, WSN designers have to address common issues related to data aggregation, data reliability, localization, node clustering, energy aware routing, events scheduling, fault detection and security.

Machine learning (ML) was introduced in the late 1950s as a technique for artificial intelligence (AI) [\cite=ayodeleintroduction2010]. Over time, its focus evolved and shifted more to algorithms which are computationally viable and robust. In the last decade, machine learning techniques have been used extensively for a wide range of tasks including classification, regression and density estimation in a variety of application areas such as bioinformatics, speech recognition, spam detection, computer vision, fraud detection and advertising networks. The algorithms and techniques used come from many diverse fields including statistics, mathematics, neuroscience, and computer science. The following two classical definitions capture the essence of machine learning:

The development of computer models for learning processes that provide solutions to the problem of knowledge acquisition and enhance the performance of developed systems [\cite=590079].

The adoption of computational methods for improving machine performance by detecting and describing consistencies and patterns in training data [\cite=Langley:1995:AML:219717.219768].

Applying these definitions to WSNs, we see that the promise of machine learning lies in exploiting historical data to improve the performance of sensor networks on given tasks without the need for re-programming. More specifically, machine learning is important in WSN applications for the following main reasons:

Sensor networks usually monitor dynamic environments that change rapidly over time. For example, a node's location may change due to soil erosion or sea turbulence. It is desirable to develop sensor networks that can adapt and operate efficiently in such environments.

WSNs may be used for collecting new knowledge about unreachable, dangerous locations [\cite=451250] (e.g., volcano eruption and waste water monitoring) in exploratory applications. Due to the unexpected behavior patterns that may arise in such scenarios, system designers may develop solutions that initially may not operate as expected. System designers would rather have robust machine learning algorithms that are able to calibrate itself to newly acquired knowledge.

WSNs are usually deployed in complicated environments where researchers cannot build accurate mathematical models to describe the system behavior. Meanwhile, some tasks in WSNs can be prescribed using simple mathematical models but may still need complex algorithms to solve them (e.g., the routing problem [\cite=1030829] [\cite=1368893]). Under similar circumstances, machine learning provides low-complexity estimates for the system model.

Sensor network designers often have access to large amounts of data but may be unable to extract important correlations in them. For example, in addition to ensuring communication connectivity and energy sustainability, the WSN application often comes with minimum data coverage requirements that have to be fulfilled by limited sensor hardware resources [\cite=romer2004design]. Machine learning methods can then be used to discover important correlations in the sensor data and propose improved sensor deployment for maximum data coverage.

New uses and integrations of WSNs, such as in cyber-physical systems (CPS), machine-to-machine (M2M) communications, and Internet of things (IoT) technologies, have been introduced with a motivation of supporting more intelligent decision-making and autonomous control [\cite=wan2013machine]. Here, machine learning is important to extract the different levels of abstractions needed to perform the AI tasks with limited human intervention [\cite=bengio2009learning].

However, there are a few drawbacks and limitations that should be considered when using machine learning techniques in wireless sensor networks. Some of these are:

As a resource limited framework, WSN drains a considerable percentage of its energy budget to predict the accurate hypothesis and extract the consensus relationship among data samples. Thus, the designers should consider the trade-off between the algorithm's computational requirements and the learned model's accuracy. Specifically, the higher the required accuracy, the higher the computational requirements, and the higher energy consumptions. Otherwise, the developed systems might be employed with centralized and resource capable computational units to perform the learning task.

Generally speaking, learning by examples requires a large data set of samples to achieve the intended generalization capabilities (i.e., fairly small error bounds), and the algorithm's designer will not have the full control over the knowledge formulation process [\cite=hoffmann1990general].

During the past decade, WSNs have seen increasingly intensive adoption of advanced machine learning techniques. In [\cite=4449882], a short survey of machine learning algorithms applied in WSNs for information processing and for improving network performance was presented. A related survey that discussed the applications of machine learning in wireless ad-hoc networks was published in [\cite=4496871]. The authors of [\cite=forster2010machine] discussed applications of three popular machine learning algorithms (i.e., reinforcement learning, neural networks and decision trees) at all communication layers in the WSNs. In contrast, specialized surveys that touch on machine learning usage in specific WSN challenges have also been written. For instance, [\cite=5451757] [\cite=4589621] addressed the development of efficient outlier detection techniques so that proper actions can be taken, and some of these techniques are based on concepts from machine learning. Meanwhile, [\cite=5473889] discusses computational intelligence methods for tackling challenges in WSNs such as data aggregation and fusion, routing, task scheduling, optimal deployment and localization. Here, computational intelligence is a branch of machine learning that focuses on biologically-inspired approaches such as neural networks, fuzzy systems and evolutionary algorithms [\cite=74851240].

Generally, these early surveys concentrated on reinforcement learning, neural networks and decision trees which were popular due to their efficiency in both theory and practice. In this paper, we decided instead to include a wide variety of important up-to-date machine learning algorithms for a comparison of their strengths and weaknesses. In particular, we provide a comprehensive overview which groups these recent techniques roughly into supervised, unsupervised and reinforcement learning methods. Another distinction between our survey and earlier works is the way that machine learning techniques are presented. Our work discusses machine learning algorithms based on their target WSN challenges, so as to encourage the adoption of existing machine learning solutions in WSN applications. Lastly, we build on existing surveys and go beyond classifying and comparing previous efforts, by providing useful and practical guidelines for WSN researchers and engineers who are interested in exploring new machine learning paradigms for future research.

The rest of the paper is organized as follows:

Section [\ref=sec:overview_ml] introduces the reader to machine learning algorithms and themes that will be referred to in later sections. Simple examples will be given in the context of WSNs.

In Section [\ref=sec:functional_challenges], we review existing machine learning efforts to address functional issues in WSNs such as routing, localization, clustering, data aggregation, query processing and medium access control. Here, an issue is functional if it is essential to the basic operation of the wireless sensor network.

Section [\ref=sec:non_functional_challenges] investigates machine learning solutions in WSNs for fulfilling non-functional requirements, i.e. those which determine the quality or enhance the performance of functional behaviors. Examples of such requirements include security, quality of service (QoS) and data integrity. In this section, we also highlight some unique efforts in specialized WSN applications.

Section [\ref=sec:open_issues] outlines major difficulties and open research problems for machine learning in WSNs.

Finally, we conclude in Section [\ref=sec:CONCLUSIONS-AND-FUTURE] and present a comparative guide with useful paradigms for furthering machine learning research in various WSN applications.

INTRODUCTION TO MACHINE LEARNING IN WIRELESS SENSOR NETWORKS

Usually, sensor network designers characterize machine learning as a collection of tools and algorithms that are used to create prediction models. However, machine learning experts recognize it as a rich field with very large themes and patterns. Understanding such themes will be beneficial to those who wish to apply machine learning to WSNs. Applied to numerous WSNs applications, machine learning algorithms provide tremendous flexibility benefits. This section provides some of the theoretical concepts and strategies of adopting machine learning in the context of WSNs.

Existing machine learning algorithms can be categorized by the intended structure of the model. Most machine learning algorithms fall into the categories of supervised, unsupervised and reinforcement learning [\cite=Abu-Mostafa:2012:LD:2207825]. In the first category, machine learning algorithms are provided with a labeled training data set. This set is used to build the system model representing the learned relation between the input, output and system parameters. In contrast to supervised learning, unsupervised learning algorithms are not provided with labels (i.e., there is no output vector). Basically, the goal of an unsupervised learning algorithm is to classify the sample sets to different groups (i.e., clusters) by investigating the similarity between the input samples. The third category includes reinforcement learning algorithms, in which the agent learns by interacting with its environment (i.e., online learning). Finally, some machine learning algorithms do not naturally fit into this classification since they share characteristics of both supervised and unsupervised learning methods. These hybrid algorithms (often termed as semi-supervised learning) aim to inherit the strengths of these main categories, while minimizing their weaknesses [\cite=chapelle2006semi].

This section is mainly to introduce the reader to the algorithms that will be referred to in later sections. Moreover, examples will be given to demonstrate the process of adopting machine learning in WSNs. In Sections [\ref=sec:functional_challenges] and [\ref=sec:non_functional_challenges], such details will be omitted. For interested reader, please refer to [\cite=Abu-Mostafa:2012:LD:2207825] [\cite=720536] and references therein, for thorough discussions of machine learning theory and its classical concepts.

Supervised Learning

In supervised learning, a labeled training set (i.e., predefined inputs and known outputs) is used to build the system model. This model is used to represent the learned relation between the input, output and system parameters. In this subsection, the major supervised learning algorithms are discussed in the context of WSNs. In fact, supervised learning algorithms are extensively used to solve several challenges in WSNs such as localization and objects targeting (e.g., [\cite=4518167] [\cite=5161349] [\cite=Shareef:2008:LUN:1361492.1361497]), event detection and query processing (e.g., [\cite=1541008] [\cite=5698542] [\cite=1544272] [\cite=5702151]), media access control (e.g., [\cite=5612365] [\cite=Shen2008900] [\cite=Kulkarni:2009:NNB:1704555.1704768]), security and intrusion detection (e.g., [\cite=1665221] [\cite=1648838] [\cite=4496866] [\cite=4289308]), and quality of service (QoS), data integrity and fault detection (e.g., [\cite=1606090] [\cite=4429838] [\cite=Wang:2007:PLQ:1317425.1317434]).

K-nearest neighbor (k-NN)

This supervised learning algorithm classifies a data sample (called a query point) based on the labels (i.e., the output values) of the near data samples. For example, missing readings of a sensor node can be predicted using the average measurements of neighboring sensors within specific diameter limits. There are several functions to determine the nearest set of nodes. A simple method is to use the Euclidean distance between different sensors. K-nearest neighbor does not need high computational power, as the function is computed relative to local points (i.e., k-nearest points, where k is a small positive integer). This factor coupled with the correlated readings of neighboring nodes makes k-nearest neighbor a suitable distributed learning algorithm for WSNs. In [\cite=beyer1999nearest], it has been shown that the k-NN algorithm may provide inaccurate results when analyzing problems with high-dimensional spaces (more than 10-15 dimensions) as the distance to different data samples becomes invariant (i.e., the distances to the nearest and farthest neighbors are slightly similar). In WSNs, the most important application of the k-nearest neighbor algorithm is in the query processing subsystem (e.g., [\cite=1541008] [\cite=5698542]).

Decision tree (DT)

It is a classification method for predicting labels of data by iterating the input data through a learning tree [\cite=ayodele2010types]. During this process, the feature properties are compared relative to decision conditions to reach a specific category. The literature is very rich with solutions that use DT algorithm to resolve different WSNs' design challenges. For example, DT provides a simple, but efficient method to identify link reliability in WSNs by identifying a few critical features such as loss rate, corruption rate, mean time to failure (MTTF) and mean time to restore (MTTR). However, DT works only with linearly separable data and the process of building optimal learning trees is NP-complete [\cite=safavian1991survey].

Neural networks (NNs)

This learning algorithm could be constructed by cascading chains of decision units (e.g., perceptrons or radial basis functions) used to recognize non-linear and complex functions [\cite=bengio2009learning]. In WSNs, using neural networks in distributed manners is still not so pervasive due to the high computational requirements for learning the network weights, as well as the high management overhead. However, in centralized solutions, neural networks can learn multiple outputs and decision boundaries at once [\cite=lippmann1987introduction], which makes them suitable for solving several network challenges using the same model.

We consider a sensor node localization problem (i.e., determining node's geographical position) as an application example of neural network in WSNs. Node localization can be based on propagating angle and distance measurements of the received signals from anchor nodes [\cite=8547966]. Such measurements may include received signal strength indicator (RSSI), time of arrival (TOA), and time difference of arrival (TDOA) as illustrated in Figure [\ref=fig:neural_network_example]. After supervised training, neural network generates an estimated node location as vector-valued coordinates in 3D space. Related algorithms to neural networks include self-organizing map (or Kohonen's maps) and learning vector quantization (LVQ) (see [\cite=Kohonen:2001] and references therein for an introduction to these methods). In addition to function estimation, one of the important applications of neural networks is for big data (high-dimensional and complex data set) tuning and dimensionality reduction [\cite=hinton2006reducing].

Support vector machines (SVMs)

It is a machine learning algorithm that learns to classify data points using labeled training samples [\cite=steinwart2008support]. For example, one approach for detecting malicious behavior of a node is by using SVM to investigate temporal and spatial correlations of data. To illustrate, given WSN's observations as points in the feature space, SVM divides the space into parts. These parts are separated by as wide as possible margins (i.e., separation gaps), and new reading will be classified based on which side of the gaps they fall on as shown in Fig. [\ref=fig:nonlinear_svm]. An SVM algorithm, which includes optimizing a quadratic function with linear constraints (that is, the problem of constructing a set of hyperplanes), provides an alternative method to the multi-layer neural network with non-convex and unconstrained optimization problem [\cite=ayodele2010types]. Potential applications of SVM in WSNs are security (e.g., [\cite=4496866] [\cite=4289308] [\cite=4761978] [\cite=2563258] [\cite=Zhang20131062]) and localization (e.g., [\cite=5506589] [\cite=4384476] [\cite=5487032]). For a detailed discussion of the SVM theory, please refer to [\cite=steinwart2008support].

Bayesian statistics

Unlike most machine learning algorithms, Bayesian inference requires a relatively small number of training samples [\cite=box2011bayesian]. Bayesian methods adapt probability distribution to efficiently learn uncertain concepts (e.g., θ) without over-fitting. The crux of the matter is to use the current knowledge (e.g., collected data abbreviated as D) to update prior beliefs into posterior beliefs [formula], where p(θ|D) is the posterior probability of the parameter θ given the observation D, and p(D|θ) is the likelihood of the observation D given the parameter θ. One application of Bayesian inference in WSNs is assessing event consistency (θ) using incomplete data sets (D) by investigating prior knowledge about the environment. However, such statistical knowledge requirement limits the wide adoption of Bayesian algorithms in WSNs. A related statistical learning algorithm is Gaussian process regression (GPR) model [\cite=Rasmussen06gaussianprocesses].

Unsupervised Learning

Unsupervised learners are not provided with labels (i.e., there is no output vector). Basically, the goal of an unsupervised learning algorithm is to classify the sample set into different groups by investigating the similarity between them. As expected, this theme of learning algorithms is widely used in node clustering and data aggregation problems (e.g., [\cite=5489603] [\cite=5425458] [\cite=5345599] [\cite=5706781] [\cite=5671089] [\cite=4249813] [\cite=985674]). Indeed, this wide adoption is due to data structures (i.e., no labeled data is available) and the desired outcome in such problems.

K-means clustering

The k-means algorithm [\cite=kanungo2002efficient] is used to recognize data into different classes (known as clusters). This unsupervised learning algorithm is widely used in sensor node clustering problem due to its linear complexity and simple implementation. The k-means steps to resolve such node clustering problem are (a) randomly choose k nodes to be the initial centroids for different clusters; (b) label each node with the closest centroid using a distance function; (c) re-compute the centroids using the current node memberships and (d) stop if the convergence condition is valid (e.g., a predefined threshold for the sum of distances between nodes and their perspective centroids), otherwise go back to step (b).

Principal component analysis (PCA)

It is a multivariate method for data compression and dimensionality reduction that aims to extract important information from data and present it as a set of new orthogonal variables called principal components [\cite=jolliffe2002principal]. As shown in Fig. [\ref=fig:pca_example], the principal components are ordered such that the first component corresponds to the highest-variance direction of the data, and so on for the other components. Hence, the least-variance components can be discarded as they contain the least information content. For example, PCA reduces the amount of transmitted data among sensor nodes by finding a small set of uncorrelated linear combinations of original readings. Furthermore, the PCA method simplifies the problem solving by considering only few conditions in very large variable problems (i.e., tuning big data into tiny data representation) [\cite=feldman2013turning]. A thorough discussion of the PCA theory (e.g., the eigenvalue, eigenvector, and covariance matrix analysis) is given in [\cite=jolliffe2002principal].

Reinforcement Learning

Reinforcement learning enables an agent (e.g., a sensor node) to learn by interacting with its environment. The agent will learn to take the best actions that maximize its long-term rewards by using its own experience. The most well-known reinforcement learning technique is Q-learning [\cite=15852369]. As shown in Fig. [\ref=fig:q_learning], an agent regularly updates its achieved rewards based on the taken action at a given state. The future total reward (i.e., the Q-value) of performing an action at at a given state st is computed using Eq. ([\ref=eq:rl_update]).

[formula]

where r(st,at) denotes the immediate reward of performing an action at at a given state st, and γ is the learning rate that determines how fast learning occurs (usually set to value between 0 and 1). This algorithm can be easily implemented in a distributed architecture like WSNs, where each node seeks to choose actions that are expected to maximize its long term rewards. It is important to note that Q-learning has been extensively and efficiently used in WSN routing problem (e.g., [\cite=1181362] [\cite=4411037] [\cite=4496872] [\cite=4496810]).

FUNCTIONAL CHALLENGES

In the design of WSNs, it is important to consider power and memory constraints of sensor nodes, topology changes, communication link failures, and decentralized management. Machine learning paradigms have been successfully adopted to address various functional challenges of wireless sensor networks such as energy aware and real-time routing, query processing and event detection, localization, node clustering and data aggregation.

Routing in WSNs

Designing a routing protocol for WSNs has to consider various design challenges such as energy consumption, fault tolerance, scalability, and data coverage [\cite=1368893]. Sensor nodes are provided with limited processing capabilities, small memory and low bandwidth. Traditionally, it is common to formulate a routing problem in wireless sensor networks as a graph G = (V,E), where V represents the set of all nodes, and E represents the set of bidirectional communication channels connecting the nodes. Using this model, the routing problem can be defined as the process of finding the minimum cost path starting at the source vertex, and reaching all destination vertices, by using the available graph edges. This path is actually a spanning tree T = (V,E) whose vertices include the source (i.e., a root node) and destinations (i.e., leaf nodes that do not have any child nodes). Solving such a tree with optimal data aggregation is found to be NP-hard, even when the full topology is known [\cite=1030829].

Machine learning allows a sensor network to learn from previous experiences, make optimal routing actions and adapt to the dynamic environment. The benefits can be summarized as follows:

Able to learn the optimal routing paths that will result in energy saving and prolonging the lifetime of dynamically changing WSNs.

Reduce the complexity of a typical routing problem by dividing it into simpler sub-routing problems. In each sub-problem, nodes formulate the graph structures by considering only their local neighbors, thus achieving low cost, efficient and real-time routing.

Meet QoS requirements in routing problem using relatively simple computational methods and classifiers.

Figures [\ref=fig:routing_1] and [\ref=fig:routing_2] illustrate a simple sensor network routing problem using a graph, and the traditional spanning tree routing algorithm, respectively. To find the optimal routing paths, the network nodes have to exchange their routing information with each other. In the other side, Figure [\ref=fig:routing_3] demonstrates how machine learning reduces the complexity of a typical routing problem by only considering neighboring nodes' information that will be used to predict the full path quality. Each node will independently perform the routing procedures to decide which channels to assign, and the optimal transmission power. As we will discuss in this subsection, such mechanism is proven to provide a near optimal routing decision with a very low computational complexity.

In this subsection, a wide range of machine learning-based routing protocols developed for WSNs are described. Table [\ref=tab:Summary-ml-routing] provides a summary and comparison of these routing protocols. The column Scalability implies the solutions' capability to route data in large scale networks.

Distributed regression framework

In [\cite=1307317], Guestrin et al. introduced a general framework for sensors data modeling. This distributed framework relies on the network nodes for fitting a global function to match their own measurement. The nodes are used to execute a kernel linear regression in the form of weighted components. Kernel functions map the training samples into some feature space to facilitate data manipulation (refer to [\cite=Scholkopf:2001:LKS:559923] [\cite=1315937] for an introduction to kernel methods). The proposed framework exploits the fact that the readings of multiple sensors are highly correlated. This will minimize the communication overhead for detecting the structure of the sensor data. Collectively, these results serve as an important step in developing a distributed learning framework for wireless networks using linear regression methods. The main advantages of utilizing this algorithm are the good fitting results, and the small overhead of the learning phase. However, it cannot learn non-linear and complex functions.

Data routing using self-organizing map (SOM)

Barbancho et al. [\cite=9120642] introduced "Sensor Intelligence Routing" (SIR) by using SOM unsupervised learning to detect optimal routing paths as illustrated in Fig. [\ref=fig:sir_som]. SIR introduces a slight modification on the Dijkstra's algorithm to form the network backbone and shortest paths from a base station to every node in the network. During route learning, the second layer neurons compete with each other to reserve high weights in the learning chain. Accordingly, the weights of the winning neuron and its neighboring neurons are updated to further match the input patterns. Clearly, the learning phase is a highly computational process due to the neural network generation task. As a result, it should be performed within a resourceful central station. However, the execution phase does not incur computational cost, and can be run on the network nodes. As a result, this hybrid technique (i.e., a combination of the Dijkstra's algorithm and the SOM model) takes into account the QoS requirements (latency, throughput, packet error rate, and duty cycle) during the process of updating neurons' weights. The main obstacles of applying such an algorithm are the complexity of the algorithm and the overhead of the learning phase in the case that the network's topology and setting change.

Routing enhancement using reinforcement learning (RL)

In multicast routing, a node sends the same message to several receivers. Sun et al. [\cite=1181362] demonstrated the use of Q-learning algorithm to enhance multicast routing in wireless ad hoc networks. Basically, the Q-MAP multicast routing algorithm is designed to guarantee reliable resource allocation. A mobile ad hoc network may consist of heterogeneous nodes, where different nodes have different capabilities. In addition, it is not feasible to maintain a global, up-to-date knowledge about the whole network structure. The multicast routes are determined in two phases. The first phase is "Join Query Forward" that discovers an optimal route, as well as updates the Q-values (a prediction of future rewards) of the Q-learning algorithm. The second phase, called "Join Reply Backward", creates the optimal path to allow multicast transmissions. Using Q-learning for multicast routing in mobile ad hoc networks can reduce the overhead for route searching. However, energy efficiency is the key requirement for WSNs, so Q-MAP needs to be modified for WSNs (e.g., considering hierarchical and geographic routing).

The Federal Communications Commission (FCC) has dedicated the frequency band from 3.1 to 10.6 GHz (7,500 MHz of spectrum) for the use of unlicensed ultra-wideband (UWB) communication [\cite=1201597]. UWB is a technique for transmitting bulky data for short distances using a wide spectrum of frequency bands with relatively low power. In [\cite=4411037], Dong et al. used a similar idea as [\cite=1181362] to enhance geographic routing in UWB equipped sensor networks. "Reinforcement Learning based Geographic Routing" (RLGR) protocol considers the sensor node energy and delay as metrics for formulating the learning reward function. This hierarchical geographic routing uses the UWB technology for detecting the nodes' locations, where only the cluster heads are equipped with UWB devices. Moreover, each node uses a simple look-up table to maintain the information about its neighbors (as location and energy of the neighbors are needed during network learning). These information are exchanged between nodes using short "hello" messages to learn the best routing actions. The main benefit of using reinforcement learning in routing is that it does not require information about the global network structure to achieve an acceptable routing solution.

In [\cite=4496810], Arroyo-Valles et al. introduced "Q-Probabilistic Routing" (Q-PR), an enhanced geographic routing algorithm for WSNs that learns from previous routing decisions (e.g., to select the routing path that has the highest delivery rate over the past period of time). This protocol differs from RLGR [\cite=4411037] in the QoS support. Depending on the importance of messages, expected delivery rate, and the power constraints, Q-PR determines the optimal routes using reinforcement learning and a Bayesian decision model. This algorithm discovers the next hop during the message routing time (i.e., an on-line opeartion). A Bayesian method is used to handle the decision of transmitting the packets to the set of candidate neighbor nodes, taking into account the data importance, nodes' profiles, and expected transmission and reception energy.

Förster and Murphy [\cite=4496872] also introduced an enhancement to routing in WSN using reinforcement learning. A novel technique for exchanging node local information as a feedback response to other nodes, named "Feedback Routing for Optimizing Multiple Sinks in WSN" (FROMS) is introduced. The main advantage of FROMS is to allow efficient routing from multiple sources to multiple sinks. The Q-values are initialized based on the hop counts to every node in the network. The hop counts can be collected using short "hello messages", exchanged between the nodes at earlier stages of the network deployment. FROMS extends the basic mechanism of RLGR [\cite=4411037] by assuming that all nodes can directly communicate with their neighbors.

The key disadvantage of reinforcement learning-based routing algorithms is the limited recognition of future knowledge (i.e., inability to look ahead). Therefore, the algorithms are not suitable for highly dynamic environments as they require a long time to learn optimal routes.

Clustering and Data Aggregation

In large scale energy-constrained sensor networks, it is inefficient to transmit all data directly to the sink [\cite=4062839]. One efficient solution is to pass the data to a local aggregator (known as a cluster head) which aggregates data from all the sensors within its cluster and transmits to the sink. This will typically result in energy savings. There are several works that have discussed the optimal selection of the cluster head (i.e., cluster head election process), such as in [\cite=1630358] [\cite=4493846] [\cite=1420160]. Taxonomy and comparison of classical clustering algorithms are presented in [\cite=abbasi2007survey].

Figure [\ref=fig:data_aggregation_nodes_clustering] represents the cluster-based data aggregation from sources to a base station in WSNs. In this case, there could be some faulty nodes which must be removed from the network. Such faulty nodes may generate incorrect readings that could negatively affect the accuracy of the overall operation of the network. Principally, ML techniques improve the operation of node clustering and data aggregation as follows:

Usage of machine learning to compress data locally at cluster heads by efficiently extracting similarity and dissimilarity (e.g., from faulty nodes) in different sensors' readings.

Machine learning algorithms are employed to efficiently elect the cluster head, where appropriate cluster head selection will significantly reduce energy consumption and enhance the network's lifetime.

Table [\ref=tab:compassion_clustering_aggregation] compares data aggregation and node clustering solutions. The column Balancing energy consumption indicates whether the protocol distributes computationally intensive tasks into all nodes while considering the remaining energy information. The column "Topology aware" indicates the requirement for full network topology knowledge.

Large scale network clustering using neural network

Hongmei et al. [\cite=4840489] discussed the development of self-managed clusters using neural networks. This scheme targets the clustering problem in large scale network with short transmissions radii in which centralized algorithms may not work efficiently. However, for large transmission radii, the performance of this algorithm is close to that of centralized algorithms in terms of efficiency and quality of service.

Electing a cluster head using decision trees

Ahmed et al. [\cite=4761982] applied a decision tree algorithm to solve the cluster head election problem. This approach uses several critical features while iterating the input vector through the decision tree such as distance to the cluster centroids, battery level, the degree of mobility, and the vulnerability indications. The simulation reveals that this scheme enhances the overall performance of cluster head selections when compared to the "Low Energy Adaptive Clustering Hierarchy" (LEACH) [\cite=heinzelman2000application] algorithm.

Gaussian process models for sensor readings

Gaussian process (GP) is a combination of random variables (stochastic variables) that is parameterized using mean and covariance functions. Ertin [\cite=4301342] presented a scheme for initializing probabilistic models of the readings based on Gaussian process regression. Comparatively, Kho et al. [\cite=Kho:2009:DCA:1525856.1525857] also extended Gaussian process regression to adaptively sample sensor data depending on its importance. Focusing on energy consumption, [\cite=Kho:2009:DCA:1525856.1525857] studied a trade-off between computational cost and solution optimality. Broadly speaking, Gaussian process models are preferable in the problems with small training data sets (less than a few thousand samples) and for predicting smooth functions [\cite=Rasmussen06gaussianprocesses]. However, WSN designers must consider the high computational complexity of such methods when dealing with large scale networks.

Data aggregation using self-organizing map (SOM)

The SOM algorithm is an unsupervised, competitive learning method for mapping from high dimensional spaces to low dimensions. Lee et al. [\cite=5489603] proposed a novel network architecture called "Cluster-based self-Organizing Data Aggregation" (CODA). In this architecture, the nodes are able to classify the aggregated data using a self-organizing algorithm. The winning neuron j*, that has a weight vector w(t) closest to the input vector x(t), is defined as:

[formula]

where N represents the number of neurons in the second layer. Further, the winning node and its neighbors are updated as follows:

[formula]

where [formula] and [formula] represent the values of a neuron at time t and t + 1, respectively. In addition, [formula] is the Gaussian neighborhood function given as:

[formula]

Using CODA for data aggregation will result in enhancing the quality of data, saving network energy, and reducing the network traffic.

Applying learning vector quantization for online data compression

While the above methods require a complete knowledge about the network topology, some algorithms may not have such a restriction. For example, Lin et al. [\cite=4024710] introduced a technique called "Adaptive Learning Vector Quantization" (ALVQ) to accurately retrieve compressed versions of readings from the sensor nodes. Using data correlation and historical patterns, ALVQ uses the LVQ learning algorithm to predict the code-book using past training samples. The ALVQ algorithm minimizes the required bandwidth during transmission, and enhances the accuracy of original reading recovery from the compressed data.

The crucial disadvantage of using LVQ for online data aggregation is that dead neurons, that are far away from the training samples, will never take part in the competition. Therefore, it is important to develop algorithms that are robust against outliers. By the same token, LVQ is suitable for representing big data set by few vectors [\cite=Kohonen:2001].

Data aggregation using principal component analysis

We begin by introducing two important algorithms that are efficiently used in combination with principal component analysis (PCA) to enhance data aggregation in WSNs.

Compressive sensing (CS) has been recently explored to replace the traditional scheme of "sample then compress" with "sample while compressing". CS explores sparsity property of signals to recover the original signal from few random measurements. A simple introduction to CS is provided in [\cite=5954192].

Expectation-maximization (EM) [\cite=dempster1977maximum] is an iterative algorithm composed of two steps, i.e., an expectation (E) step and a maximization (M) step. During its E-step, EM formulates the cost function while fixing the current expectation of the system parameters. Subsequently, the M-step recomputes parameters that minimize the estimation error of the cost function.

Masiero et al. [\cite=5425458] [\cite=5345599] developed a method for estimating distributed observations using few collected samples from a WSN. This solution is based on the PCA technique to produce orthogonal components used by compressive sensing to reconstruct the original readings. Moreover, this method is independent of the routing protocol due to its ability to estimate data spatial and temporal correlations. Similarly, Rooshenas et al. [\cite=5706781] applied PCA to optimize the direct transmission of readings to a base station. PCA results in considerable traffic reduction by combining nodes' collected data into fewer packets. This distributed technique is executed in intermediate nodes to combine all the incoming packets instead of forwarding them to destinations.

Equally important, Macua et al. [\cite=5671089] introduced distributed consensus-based methods for data compression using PCA and maximum likelihood of the observed data. These methods are "Consensus-based Distributed PCA" (CB-DPCA) which relies on exploring the eigenvectors of local covariance matrices, and "Consensus-based EM Distributed PCA" (CB-EM-DPCA). The latter uses a distributed EM algorithm. These methods adopt the consensus algorithm [\cite=degroot1974reaching] to predict the probability distribution of the data, and hence calculate the global dominant eigenvectors using only local communication parameters (i.e., single hop communications). CB-DPCA and CB-EM-DPCA can be tuned to provide a trade-off between the achieved approximation quality and the communication cost by adjusting the consensus round parameter. For example, to increase the algorithm accuracy, the number of consensus rounds should be increased which will increase the computational requirements of the algorithm.

Recently, Fenxiong et al. [\cite=fenxiong2013data] have tackled the problem of data compression using PCA by transforming the data from a high dimensional space to a lower one. The data is collected over time, and then it is transmitted from each node to its corresponding cluster head. At the cluster head, the data matrix is compressed to eliminate the data redundancy. The data compression is achieved by ignoring principal components through which the data has the least variation values.

High computational requirement is the main issue of PCA-based data aggregation solutions. Other than increasing throughput, these solutions elegantly cope with the high dimensionality of collected data by keeping only important information (data dimensionality reduction).

Collaborative data processing through k-means algorithm

Li et al. [\cite=985674] addressed the fundamental concepts for distributed detection and tracking of a single target using sensor networks. "Collaborative Signal Processing" (CSP) is a framework for information gathering from the monitored environment. Additionally, this algorithm can track multiple targets using classification techniques such as SVM and k-nearest neighbors.

Classical surveillance systems have to collect massive data from surveillance cameras. Together with the requirement of highly complex computation and analysis process, this introduces the need for more practical methods. Therefore, Tseng et al. [\cite=4249813] proposed "Integrated Mobile Surveillance and Wireless Sensor System" (iMouse) which adopts powerful mobile sensors to enhance traditional surveillance systems. iMouse divides the monitored sites into a number of clusters using the k-means unsupervised learning algorithm. Each cluster will be repeatedly monitored by only one mobile sensor.

Although these ideas (using k-means for data processing) are appealing because of the straightforward implementations and low complexity, they are still sensitive to outliers and to initial seed selections.

Role-free clustering

In [\cite=5158454], Förster and Murphy introduced the WSN cluster formulation method called "Role-Free Clustering with Q-Learning for Wireless Sensor Networks" (CLIQUE). Instead of performing an election process, CLIQUE enables each node to investigate its ability to function as a cluster-head node. This is achieved through the use of Q-learning algorithm in combination with some dynamic network parameters such as energy levels.

Decentralized learning for data latency

Mihaylov et al. [\cite=5486214] addressed the problem of high data latency in random topology sensor networks using reinforcement learning. Each node executes the learning algorithm locally to optimize the data aggregation without the need for a central control station. Consequently, the efficiency of the whole network is enhanced with smaller learning transmission overhead. The approach saves the node energy budget during data gathering process, and hence prolong the network lifetime.

Event Detection and Query Processing

Event detection and query processing are considered to be functional requirements of any large scale sensor network. This introduces the need for trustworthy event scheduling and detection with minimal human intervention. Monitoring in WSNs can be classified as: event-driven, continuous, or query-driven [\cite=1368893]. Figure [\ref=fig:event_detection_query_processing] illustrates event detection and query processing operations in WSNs. Fundamentally, machine learning offers solutions to restrict query areas and assess event validity for efficient event detection and query processing mechanisms. This adoption will result in the following benefits:

Learning algorithms enable the development of efficient event detection mechanisms with limited requirements of storage and computing resources. Besides they are able to assess the accuracy of such events using simple classifiers.

Machine learning facilitates the development of effective query processing techniques for WSNs, that determine the search regions whenever a query is received without flooding the whole network.

The design of effective event detection and query processing solutions has recently received increased attention from WSNs research community. The simplest techniques rely on defining a strict threshold value for the sensed phenomenon and alarming the system manager of any violations. However, in most recent applications of WSNs, event and query processing units are often complicated and require more than a predefined threshold value. One such emerging technique is to use machine learning to develop advanced event detection and query processing solutions. Table [\ref=tab:comparison_query_processing] presents a comparison of functional aspects of different machine learning-based event detection and query processing solutions for WSNs.

Event recognition through Bayesian algorithms

Krishnamachari and Iyengar [\cite=1261832] investigated the use of WSNs for detecting environmental phenomenon in a distributed manner. Readings will be considered as faulty if their values exceed a specific threshold. This study employs decentralized Bayesian learning that detects up to 95 percent of the faults, and will result in recognizing the event region. It is important to note that Chen et al. [\cite=1471678] provided corrections to several errors related to the distributed Bayesian algorithms that have been derived in [\cite=1261832]. In summary, these corrections result in enhanced error and performance calculations for the distributed Bayesian algorithm proposed in [\cite=1261832].

Additionally, Zappi et al. [\cite=5932548] presented a real-time approach for activity recognition using WSNs that accurately detects body gesture and motion. Initially, the nodes, that are spread throughout the body, detect the organ motion using an accelerometer sensor with three axis measurements (positive, negative and null), where these measurements are used by a hidden Markov model (HMM) to predict the activity at each sensor. Sensor activation and selection rely on the sensor's potential contributions in classifier accuracy (i.e., select the sensors that provide the most informative description of the gesture). To generate a final gesture decision, a naive Bayes classifier is used to combine the independent node predictions so as to maximize the posterior probability of the Bayes theorem. The architecture of the proposed system is shown in Fig. [\ref=fig:hmm_naive_activity].

Forest fire detection through neural network

WSNs were actively used in fire detection and rescue systems (see [\cite=4017702] and references therein for requirements and challenges of such systems). Moreover, the use of WSNs for forest fire detection can achieve better performance than using satellite-based solutions while costing much less. Yu et al. [\cite=1544272] presented a real-time forest fire detection scheme based on a neural network method. Data processing will be distributed to cluster heads, and only important information will be aggregated to a final decision maker. Although the idea is creative and beneficial to the environment, the classification task and system core are hardly interpretable when introducing such systems to decision makers.

Query processing through k-nearest neighbors

K-nearest neighbor query is considered as a highly effective query processing technique in WSNs. For example, Winter et al. [\cite=1541008] developed an in-network query processing solution using the k-nearest neighbor algorithm, namely the "K-NN Boundary Tree" (KBT) algorithm. Each node that is aware of its location will determine its k-NN search region whenever a query is received from the application manager.

Correspondingly, Jayaraman et al. [\cite=5698542] extended the query processing design of [\cite=1541008]. "3D-KNN" is a query processing scheme for WSNs that adopts the k-nearest neighbor algorithm. This approach restricts the query region to bound at least k-nearest nodes deployed within a 3D space. In addition, signal-to-noise ratio (SNR) and distance measurements are used to refine the k-nearest neighbor.

The primary concerns of such k-NN-based algorithms for query processing are the requirement of large memory footprint to store every collected sample and the high processing delay in large scale sensor networks.

Distributed event detection for disaster management using decision tree

Bahrepour et al. [\cite=5702151] developed decision tree-based event detection and recognition for sensor network disaster prevention systems. The main application of this decentralized mechanism is the fire detection in residential areas. Most noteworthy, the final event detection decision is made by using a simple vote from the highest reputation nodes.

Query optimization using principal component analysis (PCA)

Malik et al. [\cite=5486201] optimized traditional query processing in WSNs using data attributes and PCA, thus reducing the overhead of such a process. PCA has been used to dynamically detect important attributes (i.e., dominant principal components) among the whole correlated data set. Figure [\ref=fig:pca_query] shows the workflow of the proposed algorithm in four fundamental steps. In Step 1, the structured query language (SQL) request, which contains the human intelligible attributes, is sent to the database management and optimization system. At the database management and optimization system, the original query is optimized where the high-variance components are extracted from historical data using the PCA algorithm (Step 2). Then, the optimized query is diffused to the wireless sensor network to extract the sensory data as shown in Steps 3 and 4, respectively. Later, the original attributes (i.e., human intelligible attributes) can be extracted from the optimized attributes by reversing the process of PCA.

As a result, this algorithm guarantees 25 percent improvement in energy saving of the network nodes while achieving 93 percent of accuracy rates. However, this enhancement is at the cost of accuracy of the collected data (as some of the data components will be ignored). Therefore, this solution may not be ideal for the applications with high accuracy and precision requirements.

Localization and Objects Targeting

Localization is the process of determining the geographic coordinates of network's nodes and components. Position awareness of sensor nodes is an important capability, since most sensor network operations are typically based on the location [\cite=4343996]. In most large scale systems, it is financially infeasible to use global positioning system (GPS) hardware in each node for this purpose. Moreover, GPS service may not be available in the observed environment (e.g., indoor). Relative location measurement is sufficient for certain uses. However, by using the absolute locations for a small group of nodes, relative locations can be transformed into absolute ones [\cite=56532056]. In order to enhance the performance of proximity based localization, additional measurements relying on distance, angle or a hybrid of them can be used. Distance measurements can be obtained by utilizing various techniques such as RSSI, TOA, and TDOA. Furthermore, angle of the received signal can be measured using compasses or special smart antennas [\cite=Nasipuri:2002:DBL:570738.570754]. A valuable introduction about the basics of different range-based localization techniques is provided in [\cite=8547966].

Sensor nodes may encounter changes in their location after deployment (e.g., due to movement). The benefits of using machine learning algorithms in sensor node localization process can be summarized as follows:

Converting the relative locations of nodes to absolute ones using few anchor points. This will eliminate the need for range measurement hardware to obtain distance estimations.

In surveillance and object targeting systems, machine learning can be used to divide the monitored sites into a number of clusters, where each cluster represents specific location indicator.

We begin by defining some terms that are widely used in WSN localization literature, as illustrated in Figure [\ref=fig:localization_problem].

Unknown node is a node that cannot determine its current location.

Beacon node (or anchor node) is any node that is able to recognize its location by using positioning hardware or from its manual placement. In most systems, the beacon node is used as a reference point to estimate the coordinates of other unknown nodes.

Received signal strength indication (RSSI) is an indicator of the received signal strength, used to represent transmission performance or distance.

Next, we discuss some seminal WSN localization techniques that use machine learning and summarize our reviews in Table [\ref=tab:summary-localization].

Bayesian node localization

Morelande et al. [\cite=4518167] used a Bayesian algorithm to develop a localization scheme for WSNs using only few anchor points. This study focuses on the enhancement of progressive correction [\cite=musso2001improving], which is a method for predicting samples from likelihoods to get closer to the posterior likelihood. The proposed algorithm is efficiently applicable for node localization in large scale systems (i.e., networks with a few thousands of nodes). The idea of using the Bayesian algorithm for localization is appealing as it can handle incomplete data sets by investigating prior knowledge and probabilities.

Robust location-aware activity recognition

Lu and Fu [\cite=5161349] addressed the problem of sensor and activity localization in smart homes. The activities of interest include using the phone, listening to the music, using the refrigerator, studying, etc. In such applications, designers need to comply with both human and environment constraints in a convenient and easily operated way. The proposed framework, named "Ambient Intelligence Compliant Object" (AICO), facilities the human interaction with the home electric devices in a more intelligent manner (e.g., automatic power supply management). At its core, AICO uses multiple naive Bayes classifiers to determine the resident's current location and evaluate the reliability of the system by detecting any malfunctioned sensors. Although this method provides a robust mechanism for localization, it is still application-dependent and the designers must predefine a set of supported activities in advance. This is because the used learning features are selected and evaluated manually depending on the activities and the domain of interest. To overcome this limitation in this centralized system, we recommend investigating unsupervised machine learning algorithms for automatic feature extraction such as the deep learning methods [\cite=bengio2009learning] and the non-negative matrix factorization algorithm [\cite=6165290].

Localization based on neural network

Shareef et al. [\cite=Shareef:2008:LUN:1361492.1361497] compared three localization schemes that are based on different types of neural networks. In particular, this study considers WSN localization using multi-layer perceptron (MLP), radial basis function (RBF), and recurrent neural networks (RNN). In summary, the RBF neural network results in the minimum error at the cost of high resource requirements. In contrast, MLP consumes the minimum computational and memory resources.

Likewise, Yun et al. [\cite=Yun:2009:SCA:1508324.1508595] adopted a similar design, in which two classes of algorithms for sensor node localization using RSSI from anchor nodes are proposed. The first class utilizes the fuzzy logic system and genetic algorithm. In the second class, the neural network is adopted to predict the sensor location by using RSSI measurements from all anchor nodes as an input vector. In the same way, Chagas et al. [\cite=6328975] applied neural networks for WSNs localization with RSSI as an input to the learning network.

The main advantage of these NNs-based localization algorithms is their ability to provide coordinates in the form of continuous-valued vectors (e.g., coordinates in 3D space). However, unlike statistical or Bayesian alternatives, neural network is a non-probabilistic method. This fact limits the designers' certainty about precision of unknown node's predicted coordinates, and hence restricts their ability to manage the cost of localization errors.

Localization using support vector machine (SVM)

The SVM technique has been widely used for node localization in WSNs, where having a self-positioning device to each sensor is infeasible. As an illustration, Yang et al. [\cite=5487032] developed a mobile node localization scheme by employing SVM and connectivity information capabilities. In its initial step, the proposed method has to detect node movement using their radio frequency oscillation such as RSSI metric. For movement detection, SVM will be executed to provide the new location.

Similar to [\cite=5487032], Tran and Nguyen [\cite=4384476] proposed "Localization Based on Support Vector Machines" (LSVM) method for node localization in WSNs. To achieve its design goals and given an appropriate training data, LSVM adopts several decision metrics such as connectivity information and indicators. Even though LSVM offers distributed localization in a fast and effective manner, its performance is still sensitive to outliers in training samples.

Localization using support vector regression (SVR)

Limited resources and high data dimensionality impede the wide adoption of SVR learning in WSNs. Therefore, Kim et al. [\cite=5506589] developed the idea of using lightweight implementation of SVR by dividing the original regression problem into several sub-problems. Basically, the algorithm starts by dividing the network into a set of sub-networks, thus a small number of data has to be processed by each regression algorithm (i.e., SVR's sub-predictors). Then, the learned hypothesis models of the sub-predictors are combined together using a customized ensemble combination technique. Thus, in addition to its low computational requirements and robustness against noisy data, this solution converges to the preferred solution with low computational requirement.

Decision tree-based localization

Based on decision tree learning, Merhi et al. [\cite=4912210] developed an acoustic target localization method for WSNs. Exact locations of targets are determined using the time difference of arrival (TDOA) metric in a spatial correlation decision tree. Also, this work proposed the design of "Event Based MAC" (EB-MAC) protocol, that enables event-based localization and targeting in acoustic WSNs. The proposed framework was implemented using a MicaZ board that supports ZigBee 802.15.4 specifications for personal area networks.

Using the GPS functionality to support localization in underwater wireless sensor network's applications may not be feasible due to the propagation limitation of the GPS signal through water [\cite=tan2011survey]. Erdal et al. [\cite=Cayirci2006431] developed a system for submarine detection in underwater surveillance systems, so that a randomly deployed node finds its location in the 3D space based on beacon node coordinates. Each monitoring unit consists of a sensor that is fixed with a cable to a surface buoy. Data is collected using the buoys, where they are transmitted to the central processing unit. At the central unit, a decision tree classifier is used to recognize any submarines in the monitored sites.

Sensor placements through Gaussian processes

Krause et al. [\cite=Krause:2008:NSP:1390681.1390689] provided an optimized solution to sensor placement in applications with spatially correlated data such as temperature monitoring systems. One interesting feature of this solution is the development of a lazy learning scheme based on Gaussian process model for the investigated phenomenon. Lazy learning algorithms store training samples and delay the major processing task until a classification request is received. Moreover, this solution aims to achieve robustness against node failures and model ambiguity when choosing optimal locations for sensors.

Spatial Gaussian process regression

Gu and Hu [\cite=6218781] developed a distributed protocol for collective node motion. This approach employs distributed Gaussian process regression (DGPR) to predict optimal locations for mobile nodes' movements. Traditional Gaussian process regression (GPR) algorithms have computational complexity of O(N3), where N is the size of samples. However, this solution adopts a sparse Gaussian process regression algorithm to reduce such computational complexity. Each node will execute the regression algorithm independently using only spatiotemporal information from local neighbors.

Localization using self-organizing map (SOM)

Given some anchor positions, Paladina et al. [\cite=4381576] introduced the SOM-based positioning solution for WSNs consisting of thousands of nodes. The proposed scheme is executed in each node with a simple SOM algorithm that consists of a 3x3 input layer connected to the 2 neurons of the output layer. In particular, the input layer is formulated using the spatial coordinates of 8 anchor nodes surrounding the unknown node. After a sufficient training, the output layer is used to represent the unknown node's spatial coordinates in a 2D space. The main disadvantage of this scheme is that the nodes should be distributed uniformly and equally spaced throughout the monitored area.

Unlike traditional methods that require absolute locations of a few nodes to find the positions of the unknown nodes, Giorgetti et al. [\cite=Giorgetti:2007:WLU:1236360.1236399] introduced a localization algorithm that is only based on connectivity information and the SOM algorithm. The developed method is highly suitable for networks with limited resources, as it does not require a GPS-enabled device. However, since this is a centralized algorithm, each node transmits the information of its neighbors to the central processing unit to determine the adjacency matrix and hence the node's location. Similarly, Hu and Lee [\cite=4648079] presented a scheme that provides node localization service in WSNs without the need for anchor nodes. The algorithm is based on SOM, and it operates efficiently for any number of nodes. The contribution of [\cite=4648079] over [\cite=Giorgetti:2007:WLU:1236360.1236399] is that the proposed algorithm distributes the computation tasks to all nodes in the network, which eliminates the needs for a central unit and minimizes the transmission overhead of the algorithm.

Path determination using reinforcement learning

Li et al. [\cite=6185486] developed a reinforcement learning-based localization method for WSNs, called "Dynamic Path determination of Mobile Beacons" (DPMB), suitable for real-time management of the mobile beacons. The mobile beacon (MB), which is aware of the physical location during its movement, will be used to determine the positions of large number of sensor nodes. In brief, the states of the Q-learning algorithm are used to represent the different positions of the MB, and the algorithm target is to cover all the sensors in the monitored area (i.e., all the sensors should hear a location update message from the MB at some stages). The entire operation will be run in the mobile beacon, and hence, this will save the resources of the unknown nodes. However, as a centralized method, the entire system will fail in the event of mobile beacon malfunctions.

Medium Access Control (MAC)

In WSNs, a number of sensors cooperate to efficiently transfer data. Therefore, designing MAC protocols for WSNs poses different challenges from typical wireless networks, as well as energy consumption and latency [\cite=6328420]. Also, the duty cycle (i.e., fraction of time that a sensor node is active) of the node has to be controlled to conserve energy. Therefore, the MAC protocols have to be modified to support efficient data transmission and reception of the sensor nodes. A comprehensive survey of MAC protocols in WSNs is provided in [\cite=5451759].

Recently, machine learning methods have been used to enhance the performance of MAC protocols in WSNs. Specifically, this is achieved through the following points:

Machine learning can be used to adaptively determine the duty cycle of a node using the transmission history of the network. In particular, the nodes, which are able to predict when the other nodes' transmissions will finish, can sleep in the meantime and wake up (to transmit data) just when the channel is expected to be idle (i.e., when no other node is transmitting). For WSNs, many factors, such as energy consumption and latency, are more important than fairness when designing MAC protocols.

Achieving secured data transmission by combining the concepts of machine learning and MAC protocols. Such MAC layer security schemes are independent of the proposed application and are able to iteratively learn sporadic attack patterns.

Table [\ref=tab:comparison-ml-mac] gives a brief comparison between MAC protocols reviewed in this subsection. The column "Synchronization" indicates whether the protocol assumes that time synchronization is achieved externally, and "Adaptivity to changes" indicates the ability to handle topology changes such as nodes failure.

Bayesian statistical model for MAC

Kim and Park [\cite=5612365] presented a contention-based MAC protocol for managing active and sleep times in WSNs. Instead of continuously sensing the medium, this scheme utilizes a Bayesian statistical model to learn when the channel can be allocated, and hence save network energy. Furthermore, in its basic design, this scheme is targeted for CSMA contention-based protocols such as "Sensor MAC" (S-MAC) [\cite=1019408], and "Timeout MAC" (T-MAC) [\cite=vanDam:2003:AEM:958491.958512].

Neural network-based MAC

Time division multiple access (TDMA)-based protocols employ periodic time frames to separate the medium access of different nodes. This process requires a central unit to broadcast a transmission schedule in case of topology changes. Shen and Wang [\cite=Shen2008900] proposed a solution to broadcast the transmission schedule in TDMA using a fuzzy hopfield neural network (FHNN) technique. Time slots are distributed among the nodes in a network while maximizing the cycle length, preventing any potential transmission collisions and reducing the processing time.

In the same way, Kulkarni and Venayagamoorthy [\cite=Kulkarni:2009:NNB:1704555.1704768] presented an innovative CSMA-based MAC solution, that can prevent denial-of-service (DoS) attacks in WSNs. Denial-of-service is a type of attacks that generates huge useless traffic (i.e., flood the network), thus preventing the delivery of useful data. In such cases, attackers exploit the limitations of WSNs such as limited bandwidth and buffering capabilities. The proposed solution is based on neural network learning to prevent flooding the network with untruthful data traffic by investigating the network properties and variations such as packet request rate and average packet waiting time. Consequently, the MAC layer will be blocked if the neural network output exceeds a predefined threshold level. More importantly, only nodes in the affected sites will be blocked, as this solution is designed to work in a distributed manner.

Duty cycle management using reinforcement learning

Liu and Elhanany [\cite=Liu:2006:RRL:1359189.1359190] employed a reinforcement learning technique to introduce RL-MAC, an adaptive MAC protocol for WSNs. Basically, RL-MAC reduces energy usage and increases throughput by optimizing the duty cycle of the network node. Similar to S-MAC [\cite=1019408] and T-MAC [\cite=vanDam:2003:AEM:958491.958512], RL-MAC synchronizes node's transmission on a common schedule in a frame-based structure. RL-MAC adaptively determines the slot length, duty cycle and transmission active time depending on the traffic load and the channel bandwidth.

Similarly, Chu et al. [\cite=6328420] integrated slotted ALOHA and Q-Learning algorithms to introduce a new MAC protocol for WSNs, called "ALOHA and Q-Learning based MAC with Informed Receiving" (ALOHA-QIR). ALOHA-QIR inherits the features of both ALOHA and Q-Learning to achieve the benefits of simple design, low resource requirements and low collision probability. During their transmission frames, nodes broadcast their future transmission allocation such that other nodes can sleep during reserved frame. The Q-value map in each node represents the willingness for slot reservation, where the node with higher Q-value will attain the right of slot allocation and hence transmission of its own data. Figure [\ref=fig:aloha_qir] demonstrates the steps of updating the Q-values over three frames of a node that is allowed to transmit a maximum of two packets in each frame. Initially, the Q-values are initialized to zero, i.e., [formula]. Upon successful transmission, the Q-value of each time slot is updated using the Q-learning update rule given by Eq. ([\ref=eq:rl_update]) where the learning rate is set to 0.1. Upon successful transmission, the reward value is equal to +1, and it is -1 for a failed transmission. Certainly, the nodes will decide to transmit data using the time slots with the maximum Q-values.

Although the idea of using reinforcement learning for duty cycle management is appealing because of its distributed operation and small memory and computational resource requirement, it may result in high collision rates during the initial exploration phases.

Adaptive MAC layer

In a variety of modern applications, such as in healthcare and assisted living systems, WSNs are used to directly share the collected data with the users' mobile phones. This introduces new design challenges that are related to the dynamic communication patterns and service requirements over time. Sha et al. [\cite=sha2013self] studied this problem at the MAC layer, hence proposing the "Self-Adapting MAC Layer" (SAML) design. SAML is composed of two main components: The "Reconfigurable MAC Architecture" (RMA) to switch between the different MAC protocols, and the MAC engine that is used to learn the suitable MAC protocol for the current network conditions. The learning process is performed using the decision tree classier as illustrated in Fig. [\ref=fig:saml_dt]. The learning features of the decision tree are: inter packet interval (IPI) and received signal strength indication (RSSI) statistical parameters (i.e., the mean and the variance), the application QoS requirements (reliability, energy usage, and latency), packet delivery rate (PDR), and the traffic pattern. The supported MAC protocols are Pure TDMA [\cite=klues2007component], Adaptive TDMA [\cite=doerr2005multimac], Box-MAC [\cite=moss2008box], RI-MAC [\cite=sun2008ri], and ZigBee [\cite=alliance2007zigbee]. Even though the SAML scheme provides an adaptive MAC solution in dynamic environments, it introduces a level of complexity and additional expense into the designed systems.

NON-FUNCTIONAL CHALLENGES

Non-functional requirements include specifications that are not related to the basic operational behavior of the system. For example, WSN designers may need to ensure that the proposed solution is always capable of providing up-to-date information about the monitored environment. This section provides a comprehensive review of recent machine learning advances that have been adopted to achieve non-functional requirements in WSNs such as security, quality of service, and data integrity. Moreover, this section also highlights some unique efforts in specialized WSN applications. Such studies could inspire researchers to a variety of WSN applications that can be improved using machine learning techniques.

Security and Anomaly Intrusion Detection

The major challenge to implement security techniques in WSNs is the limited resource constraints [\cite=5451757]. Moreover, some attack methods aim to produce unexpected, mistaken knowledge, by introducing misleading observations to the network. Figure [\ref=fig:simple-ex-anomalies] presents the general concept of anomaly detection in phenomenon monitoring sensor system using machine learning clustering and classification algorithms. In this example, machine learning techniques classify the data into two correct reading regions. Since most observations lie in these two regions, the points that are inconsistent (e.g., from an attack) with these regions are considered as anomalies.

Machine learning algorithms have been employed to detect outlying and misleading measurements. Simultaneously, several attacks could be detected by analyzing well-known malicious activities and vulnerabilities. Basically, WSN security enhancements by adopting machine learning techniques will result in the following earnings:

Save node's energy and significantly expand WSN lifetime by preventing the transmission of the outlier, misleading data.

Enhance network reliability by eliminating faulty and malicious readings. In the same way, avoiding the discovery of unexpected knowledge that will be converted to important, and often critical actions.

Online learning and prevention (without human intervention) of malicious attacks and vulnerabilities.

In this subsection, we explore various machine learning-based algorithms addressing the security issue in WSNs. Table [\ref=tab:comparison-ml-outlier] summarizes the reviewed methods in this subsection. The column "Predicting missing data" indicates the ability of the proposed solution to provide predictions for any missing sensors' readings.

Outlier detection using Bayesian belief network

Janakiram et al. [\cite=1665221] used Bayesian belief networks (BBNs) to develop an outlier detection scheme. Given that the majority of node's neighbors will have similar readings (i.e., temporal and spatial correlations), it is reasonable to use this phenomenon to build conditional dependencies among nodes' readings. BBNs infer the conditional relationships among the observations to discover any potential outliers in the collected data. Furthermore, this method can be used to evaluate missing values.

Outlier detection using k-nearest neighbors

Branch et al. [\cite=1648838] developed an in-network outlier detection method in WSNs using k-nearest neighbors. Moreover, any missing nodes' readings will be replaced by the average value of the k-nearest nodes. However, such non-parametric, k-NN-based algorithm requires large memory to store every collected readings from the monitored environment.

Detecting selective forwarding attacks using support vector machine (SVM)

In black hole attacks, malicious nodes send misleading "Routing Reply" (RREP) messages whenever the malicious nodes receive "Route Request" (RREQ) messages, indicating that routes to the destinations are found. Accordingly, source nodes will stop the process of route discovery, and will ignore other RREP messages. Therefore, malicious nodes will drop all network's messages, while the source nodes assume that their packets were delivered to the destination. Kaplantzis et al. [\cite=4496866] presented packet dropping attack prevention technique based on one class support vector machine classifier. The proposed scheme is capable of detecting black hole attacks and selective forwarding attacks. Basically, routing information, bandwidth and hop count are used to determine the malicious nodes in the network.

Outlier detection using support vector machine (SVM)

By using a quarter-sphere centered at the origin, the drawback of high computational requirements of traditional SVM could be alleviated. For instance, Rajasegarar et al. [\cite=4289308] introduced a one-class quarter-sphere SVM anomaly recognition technique. The motivation of this distributed scheme is to distinguish anomalies in data while minimizing communication overhead. In [\cite=4761978], Yang et al. tackled the design of an online outlier detection method using quarter-sphere SVM. The unsupervised learning method investigates the local data to reduce the computational complexity of traditional SVM-based outlier detection algorithms. This outlier detector is similar to the method introduced in [\cite=4289308].

Artificial immunity algorithm is a computationally intelligent algorithm for problem solving inspired by the biological immunity systems [\cite=de2002artificial]. The biological immunity systems automatically generate the immune body (antibody) against the antigen (e.g., a virus) through the cell fission. In [\cite=2563258], Chen et al. extended the basic idea of using SVM for detecting intrusion by combining it with immunity algorithm. In summary, an immune algorithm was introduced as a preprocessing step for the sensor data, that will be used by SVM to detect intruders. Furthermore, Zhang et al. [\cite=Zhang20131062] also investigated the temporal and spatial correlations of the collected readings using a one-class SVM learning algorithm to develop an outlier detection method. This study adopts an ellipsoidal one-class SVM that can be solved using linear optimization instead of the quadratic optimization problem in traditional SVM methods.

The main advantages of these SVM-based methods are their good performance (efficient learning) and ability to learn non-linear and complex problems. However, they still suffer from a scalability issue to large data set due to their high computational and large memory requirements [\cite=steinwart2008support].

Analyzing attacks with self-organizing map (SOM)

Avram et al. [\cite=4215510] addressed the issue of detecting network attacks in wireless ad hoc networks using self-organizing map unsupervised learning. Learning the weights are obtained through statistical analysis of the input data vectors. The main issue of this scheme is the complexity in determining input weights. Moreover, SOM-based algorithms are not suitable for detecting attacks in very large and complex data sets (i.e., large scale sensor network).

Quality of Service, Data Integrity and Fault Detection

Quality of service (QoS) guarantees high-priority delivery of real-time events and data. In the context of WSNs, there are potential multi-hop transmissions of data to the end user, in addition to distributing queries from a system controller to the network nodes [\cite=7845239]. WSNs suffer from energy and bandwidth constraints that limit the quantity of information to be transmitted from a source to destination nodes. Furthermore, data aggregation and dissemination in WSNs can be faulty and unreliable [\cite=451250]. These issues coupled with random network topologies introduce an important challenge for designing reliable algorithms for such networks. The state of the art and general QoS requirements in WSNs have been reviewed in [\cite=chen2004qos].

In the following, we review the latest efforts of using machine learning techniques to achieve specific QoS and data integrity constraints. In brief, this adoption results in the following advantages:

Different machine learning classifiers are used to recognize different types of streams, thus eliminating the need for flow-aware management techniques.

The requirements for QoS guarantee, data integrity and fault detection depend on the network service and application. Machine learning methods are able to handle much of this while ensuring efficient resource utilization, mainly bandwidth and power utilization.

Table [\ref=tab:comparison_qos] summarizes the methods that are reviewed in this subsection. The column "Characteristics" indicates features or qualities belonging to each study.

QoS estimation using neural network

Recently, there is growing interest in estimating and improving the performance of WSNs. For example, Snow et al. [\cite=1606090] introduced a method to estimate a sensor network dependability metric using a neural network method. Dependability is a metric that represents availability, reliability, maintainability, and survivability of a sensor network. Several attributes are used to estimate such a metric including mean time between failure (MTBF) and mean time to repair (MTTR).

Moustapha and Selmic [\cite=4429838] introduced a dynamic fault detection model for WSNs. This model captures the nodes' dynamic behavior and their effects on other nodes. In addition, neural network learning, which is trained using back-propagation method, was used for node identification and fault detection (a similar idea as in [\cite=1606090]). This study results in an effective nonlinear sensor model that suits applications with fault detection requirements.

MetricMap (link quality estimation framework)

Link quality measurement tools may provide inaccurate and unstable readings across different environments due to different conditions such as signal variations and interference [\cite=Baccour:2012:RLQ:2240116.2240123]. As a result, Wang et al. [\cite=Wang:2007:PLQ:1317425.1317434] presented MetricMap, a link quality estimation framework using supervised learning methods. MetricMap enhances the MintRoute [\cite=Woo:2003:TUC:958491.958494] protocol by adopting online and offline learning methods, such as decision tree learners, to derive link quality indicators. This framework uses several local features to build the classification tree such as the received signal strength indicator (RSSI), transmission buffer size, channel load, and the forward and backward probabilities. The forward probability pf(l) is defined as the ratio of the received to the total transmitted packets over the link l, whereas the backward probability pb(l) is calculated over the reverse path. The local features are preferred over the global one as they can be found without costly communications with far away nodes. Experiments reveal that up to three times improvement in data delivery rate over basic MintRoute method can be achieved.

Assessing accuracy and reliability of sensor nodes using multi-output Gaussian processes

Osborne et al. [\cite=Osborne:2008:TRI:1371607.1372727] presented a real-time algorithm to determine a set of nodes that are capable of handling information processing tasks such as assessing the accuracy of collected sensor readings and predicting the missing readings. This algorithm provides a probabilistic Gaussian process based iterative implementation that is trained to re-use previous experience (i.e., the historical data) and maintain a reasonable training data size. Yet, the posterior distribution of an observed environmental variable x (e.g., sea surface temperature) is calculated using the generalized multivariate Gaussian distribution given by Eq. ([\ref=eq:qp_distribution]).

[formula]

where μ, K are the prior mean and covariance of the variable x, respectively. Further, I denotes the historical data (a sequence of time-stamped samples) that is updated online to consider the new sequentially collected observations.

QoS provisioning using reinforcement learning

Ouferhat and Mellouk [\cite=4230960] introduced a QoS task scheduler for adaptive multimedia sensor networks based on Q-learning technique. This scheduler significantly enhances the network throughput by reducing the transmission delay. Comparatively, Seah et al. [\cite=4496881] considered coverage as a QoS metric in WSNs that represents how efficiently the area of interest will be observed. A Q-learning method was used to develop a distributed learner that is able to find weakly monitored sites. These sites can be resolved in future re-deployment stages.

It is important to note that energy harvesting has not been considered in the above QoS mechanisms. Conversely, Hsu et al. [\cite=5284227] introduced a QoS-aware power management scheme for WSNs with energy harvesting capabilities, namely "Reinforcement Learning based QoS-aware Power Management" (RLPM). This scheme is able to adapt to the dynamic levels of nodes' energy in systems with energy harvesting capabilities. QoS-aware RLPM employs reinforcement learning to attain QoS awareness and to manage nodes' duty cycle under the energy restriction. Furthermore, Liang et al. [\cite=4976244] designed "Multi-agent Reinforcement Learning based multi-hop mesh Cooperative Communication" (MRL-CC) to be a structure modeling tool for QoS provisioning in WSNs. Basically, MRL-CC is adopted to reliably assess the data in a cooperative manner. Moreover, MRL-CC might be used to examine the impact of traffic load and node mobility on the whole network performance.

Miscellaneous Applications

This subsection presents miscellaneous and unique research efforts that are not discussed previously.

Resource management through reinforcement learning

Shah and Kumar [\cite=4428658] presented the "Distributed Independent Reinforcement Learning" (DIRL) algorithm that utilizes local information and application constraints to optimize various tasks over time while minimizing energy consumption of the network. Each sensor node learns the minimum required resources to perform its scheduled tasks, and maximizes its future rewards by finding optimal parameters of the intended application. As a typical case, consider an object recognition application, as shown in Fig. [\ref=fig:dirl], which consists of five fundamental tasks: (a) aggregate two or more readings into a single reading, (b) transmit a message to the next hop, (c) receive incoming messages, (d) sample and take readings, and (e) put the radio into sleep mode. These tasks must be executed in some priority to maximize the network lifetime, where the network do not have such knowledge of priority as there is no static schedule for the events. For example, a node does not have the knowledge of when the object is going to move near to it to start taking samples. Here, the DIRL algorithm can be used to generate the required knowledge of priority by using the Q-learning algorithm and after specifying the set of reward and price value for each task.

Decision tree-based animals behavior classification

WSNs were applied in many applications such as environmental and habitat monitoring [\cite=Mainwaring:2002:WSN:570738.570751]. As an example, Nadimi et al. [\cite=Nadimi2008167] employed decision tree to accurately classify the behavior of a herd of animals (active or inactive) using parameters such as the pitch angle of the neck and movement velocity. The advantages of the proposed solution for animals behavior classification are the simple implementation and low complexity due to the use of a few critical features.

Clock synchronization using self-organizing map

Clock synchronization between sensor nodes is an important process, since most operations of the nodes must be consistent with each other. Moreover, the design of such methods for WSNs has to consider the limited resource constraints. For example, Paladina et al. [\cite=4689144] proposed to use SOM to ensure reliable clock synchronization for large scale networks. Nodes predict near-optimal estimation of the current time without having a central timing device and with limited storage and computing resources. However, this method assumes a uniform deployment of the nodes over the monitored area, as well as the same transmission powers for all nodes.

Air quality monitoring using neural networks

Postolache et al. [\cite=5196704] proposed a neural networks-based method for measuring air pollution levels using inexpensive gas sensor nodes, while eliminating the effects of temperature and humidity on sensor readings. This solution detects the air quality and gas concentration using neural networks implemented using JavaScript (JS). As a result, the solution is able to distribute processing between web server and end user computers (i.e., a combination of client and server side scripts).

Intelligent lighting control using neural networks

Gao et al. [\cite=gao2013wireless] introduced a new standard for lighting control in smart building using the neural network algorithm. A radial basis function (RBF) neural network is used to extract a new mathematical expression, called "Illuminance Matrix" (I-matrix), to measure the degree of illuminance in the lighted area. Fundamentally, in the field of lighting control, converting the collected data from the photosensors to a form that is suitable for digital signal processing is a crucial issue and can highly affect the performance of the developed system. The article shows that using the I-matrix scheme can achieve about 60% more accuracy compared to the standard methods.

FUTURE APPLICATIONS OF MACHINE LEARNING IN WIRELESS SENSOR NETWORKS

Although machine learning techniques have been applied to many applications in WSNs, many issues are still open and need further research efforts.

Compressive Sensing and Sparse Coding

In practice, a large number of sensor measurements are usually required to maintain a desired detection accuracy. This introduces several challenges to network designers such as network management and communication issues. Given that 80 percent of the nodes' energy is consumed while sending and receiving data [\cite=1425113], data compression and dimensionality reduction techniques can be used to reduce transmission and hence prolong the network lifetime.

Traditional data compression techniques may result in extra energy consumption due to their high computational and memory requirements. In [\cite=Barr:2006:ELD:1151690.1151692], Barr and Asanovi studied the tradeoff between energy consumption in data transmission and compression. This study approximates the efficiency threshold of data compression in WSNs to be 1 bit data reduction using 485-1267 ADD instructions.

Even though compressive sensing can be recast as a linear program, it still not applicable for on-node compression. As a result, it is important to apply and extend the basic concept of compressive sensing to meet the resource constraint of WSNs. For more on the theoretical performance of decentralized compressive sensing, please refer to [\cite=4472248] [\cite=5502565] [\cite=5707037]. Examples of similar emerging techniques include independent component analysis, dictionary learning, non-negative matrix factorization and singular value decomposition.

Distributed and Adaptive Machine Learning Techniques for WSNs

Distributed machine learning techniques suit limited resource devices such as WSNs. Compared to centralized learning algorithms, distributed learning methods require less computational power and smaller memory footprint (i.e., they do not need to consider the whole network information). The decentralized learning techniques enable the nodes to rapidly adapt their future behavior and predictions in tune with the current environment conditions. For such reasons, distributed and adaptive learning algorithms are adequate for in-network processing of data while avoiding exhausting the nodes with high computational tasks [\cite=7785419]. Examples of recent online learning algorithms include "Adaptive Regularization of Weights" (AROW) [\cite=Koby:4525469], "Improved Ellipsoid Method for Online Learning" (IELLIP) [\cite=Yang:2009:OLE:1553374.1553521] and "Soft Confidence-Weighted" (SCW) [\cite=wang2012exact]. Kotecha et al. [\cite=1413463] studied some distributed classification algorithms for WSNs.

Resource Management Using Machine Learning

Energy saving is a crucial issue in developing efficient WSNs algorithms and techniques. This design goal can be achieved using two main techniques, namely, by enhancing communication related protocols (e.g., routing and MAC protocols design) and by detecting nonfunctional and energy wasteful activities. The first technique includes physical, MAC and networking layer protocols. As it is discussed in this survey, this technique has been widely studied and enhanced using machine learning algorithms. The second technique focuses on decreasing the consumed energy in minor and nonfunctional requirements. For example, sensor nodes will consume their energy when over-listening to other nodes' transmissions [\cite=jiang2007architecture]. Accordingly, such operations unnecessarily increase the active time of the nodes (i.e., increase nodes' duty cycle). The nodes that are equipped with machine learning techniques will be able to optimize their resource management and power allocation operations under those circumstances.

Detecting Data Spatial and Temporal Correlations Using Hierarchical Clustering

Hierarchical clustering is an unsupervised learning algorithm that aims to build a hierarchy of clusters. Basically, hierarchical clustering algorithms generate decomposition of the set of objects, which could be a set of sensor nodes in WSNs. Broadly speaking, hierarchical clustering can provide an emerging clustering technique in WSNs using some clustering criteria such as spatial and temporal correlations of readings. Figure [\ref=fig:hierarchical_clustering] illustrates such hierarchically clustered network based on spatial and temporal correlations of readings in a temperature monitoring system. In this example, Cluster C is formed by combining Clusters A and B, and so on for the rest of the clusters in the network.

The study of data correlation based on hierarchical clustering method will provide simple methods for energy saving. In such formations, only one node from each cluster is activated at a time to cover and monitor the whole cluster area. Typical methods of hierarchical clustering include "Balanced Iterative Reducing and Clustering using Hierarchies" (BIRCH) [\cite=Zhang:1996:BED:235968.233324] and "Clustering Using Representatives" (CURE) [\cite=Guha:1998:CEC:276304.276312].

CONCLUSIONS

Wireless sensor networks are different from traditional network in various aspects, thereby necessitating protocols and tools that address unique challenges and limitations. As a consequence, wireless sensor networks require innovative solutions for energy aware and real-time routing, security, scheduling, localization, node clustering, data aggregation, fault detection and data integrity. Machine learning provides a collection of techniques to enhance the ability of wireless sensor network to adapt to the dynamic behavior of its surrounding environment. Table [\ref=tab:summary_publications] summarizes studies that have adopted machine learning methods to address these challenges from distinct research areas.

From the discussion so far, it became clear that many design challenges in wireless sensor networks have been resolved using several machine learning methods. In this paper, an extensive literature review over the period 2002-2013 on such studies was presented. In summary, adopting machine learning algorithms in wireless sensor networks has to consider the limited resources of the network, as well as the diversity of learning themes and patterns that will suit the problem at hand. Moreover, numerous issues are still open and need further research efforts such as developing lightweight and distributed message passing techniques, online learning algorithms, hierarchical clustering patterns and adopting machine learning in resource management problem of wireless sensor networks.