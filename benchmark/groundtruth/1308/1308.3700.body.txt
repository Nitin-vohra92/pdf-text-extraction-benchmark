Sailfish: Alignment-free Isoform Quantification from RNA-seq Reads using Lightweight Algorithms

The ability to generate genomic and transcriptomic data is accelerating beyond our ability to process it. The increasingly widespread use and growing clinical relevance (e.g. [\cite=personalizedRNASeq]) of RNA-seq measurements of transcript abundance will only serve to magnify the divide between our data acquisition and data analysis capabilities.

The goal of isoform quantification is to determine the relative abundance of different RNA transcripts given a set of RNA-seq reads. In the analysis of RNA-seq data, isoform quantification is one of the most computationally time-consuming steps, and it is commonly the first step in an analysis of differential expression among multiple samples [\cite=bmc:diffexp:2013]. There are numerous computational challenges in estimating transcript-level abundance from RNA-seq data. Mapping the sequencing reads to the genome or transcript sequences can require substantial computational resources. This often leads to complicated models that account for read bias and error during inference, further adding to the time spent on analysis. Finally, some reads, known as multireads [\cite=rescueStrategy] [\cite=cufflinks], can map to multiple, sometimes many, different transcripts. The ambiguity resulting from these multireads complicates the estimation of relative transcript abundances.

Existing approaches first use read-mapping tools, such as Bowtie [\cite=bowtie], to determine potential locations from which the RNA-seq reads originated. Given the read alignments, some of the most accurate transcript quantification tools resolve the relative abundance of transcripts using expectation-maximization (EM) procedures [\cite=cufflinks] [\cite=rsem:2011] [\cite=express]. In such procedures, reads are first assigned to transcripts, and these assignments are then used to estimate transcript abundances. The abundances are then used to re-estimate the read assignments, weighting potential matches in proportion to the currently estimated relative abundances, and these steps are repeated until convergence. In practice, both of these steps can be time consuming. For example, even when exploiting the parallel nature of the problem, mapping the reads from a reasonably sized (e.g. 100M reads) RNA-seq experiment can take hours.

Recent tools, such as eXpress [\cite=express], aim to reduce the computational burden of isoform quantification from RNA-seq data by substantially altering the EM algorithm. However, even for such advanced approaches, performing read alignment and processing the large number of alignments that result from ambiguously mapping reads remains a significant bottleneck and fundamentally limits the scalability of approaches that depend on mapping.

Sailfish, our software for isoform quantification from RNA-seq data, is based on the philosophy of lightweight algorithms, which make frugal use of data, respect constant factors, and effectively use concurrent hardware by working with small units of data where possible. Sailfish avoids mapping reads entirely (Fig. [\ref=fig:pipeline]), resulting in large savings in time and space. A key technical contribution behind our approach is the observation that transcript coverage, which is essential for isoform quantification, can be reliably and accurately estimated using counts of k-mers occurring in reads. This results in the ability to obtain accurate quantification estimates more than an order of magnitude faster than existing approaches, often in minutes instead of hours. For example, for the data described in Figure [\ref=fig:results], Sailfish is between 18 and 29 times faster than the next fastest method while providing expression estimates of equal accuracy.

In Sailfish, the fundamental unit of transcript coverage is the k-mer. This is different from existing approaches, where the fragment or read is the fundamental unit of coverage. By working with k-mers, we can replace the computationally intensive step of read mapping with the much faster and simpler process of k-mer counting. We also avoid any dependence on read mapping parameters (e.g. mismatches and gaps) that can have a significant effect on both the runtime and accuracy of conventional approaches. Yet, our approach is still able to handle sequencing errors in reads because only the k-mers that overlap the erroneous bases will be discarded or mis-assigned, while the rest of the read can be processed as if it were error-free. This also leads to Sailfish having only a single explicit parameter, the k-mer length. Longer k-mers may result in less ambiguity, which makes resolving their origin easier, but may be more affected by errors in the reads. Conversely, shorter k-mers, though more ambiguous, may be more robust to errors in the reads (Supplementary Fig. [\ref=fig:supinfo:KmerAmbiguity]). Further, we can effectively exploit modern hardware where multiple cores and reasonably large memories are common. Many of our data structures can be represented as arrays of atomic integers (see ). This allows our software to be concurrent and lock-free where possible, leading to an approach that scales well with the number of available CPUs (Supplementary Fig. [\ref=fig:supinf:CountTimeVsThreads]). Additional benefits of the Sailfish approach are discussed in Supplementary Note 1.

Sailfish works in two phases: indexing and quantification (Fig. [\ref=fig:pipeline]). A Sailfish index is built from a particular set of reference transcripts (a FASTA sequence file) and a specific choice of k-mer length, k. The index consists of data structures that make counting k-mers in a set of reads and resolving their potential origin in the set of transcripts efficient (see ). The most important data structure in the index is the minimal perfect hash function [\cite=cmph] that maps each k-mer in the reference transcripts to an index between 0 and the number of different k-mers in the transcripts such that no two k-mers share an index. This allows us to quickly index and count any k-mer from the reads that also appears in the transcripts. We find that pairing the minimum perfect hash function with an atomically updateable array of k-mer counts allows us to count k-mers even faster than with existing advanced lock-free hashes such as that used in Jellyfish [\cite=jellyfish]. The index also contains a pair of look-up tables that allow fast access to the indexed k-mers appearing in a specific transcript as well as the indexed transcripts in which a particular k-mer appears, both in amortized constant time. Because the index depends only on the set of reference transcripts and the choice of k-mer length, it only needs to be rebuilt when one of these factors changes.

The quantification phase of Sailfish takes as input the index described above and a set of RNA-seq reads and produces an estimate of the relative abundance of each transcript in the reference, measured in both Reads Per Kilobase per Million mapped reads (RPKM) and Transcripts Per Million (TPM); (see  for the definitions of these measures). First, Sailfish counts the number of times each indexed k-mer occurs in the set of reads. Owing to efficient k-mer indexing by means of the perfect hash function and the use of a lock-free counting data structure (), this process is efficient and scalable (Supplementary Fig. [\ref=fig:supinf:CountTimeVsThreads]). Sailfish then applies an expectation-maximization (EM) procedure to determine maximum likelihood estimates for the relative abundance of each transcript. Conceptually, this procedure is similar to the EM algorithm used by RSEM [\cite=rsem:2011], except that k-mers rather than fragments are probabilistically assigned to transcripts, and a two-step variant of EM is used to speed up convergence. The estimation procedure first assigns k-mers proportionally to transcripts (i.e. if a transcript is the only potential origin for a particular k-mer, then all observations of that k-mer are attributed to this transcript, whereas for a k-mer that appears once in each of n different transcripts and occurs m times in the set of reads, m / n observations are attributed to each potential transcript of origin). These initial allocations are then used to estimate the expected coverage for each transcript (Methods, Eqn. [\ref=eqn:unnormalizedMean]). In turn, these expected coverage values alter the assignment probabilities of k-mers to transcripts (Methods, Eqn. [\ref=eqn:allocation]). Using these basic EM steps as a building block, we apply a globally-convergent EM acceleration method, SQUAREM [\cite=squarem], that substantially increases the convergence rate of the estimation procedure by modifying the parameter update step and step-length based on the current solution path and the estimated distance from the fixed point (see Methods, Alg. [\ref=alg:SQUAREM]).

Additionally, we reduce the number of variables that need to be fit by the EM procedure by collapsing k-mers into equivalence classes. Two k-mers are equivalent from the perspective of the EM algorithm if they occur in the same set of transcript sequences with the same rate (more details available in ). This reduction in the number of active variables substantially reduces the computational requirements of the EM procedure. For example, in the set of reference transcripts for which we estimate abundance using the Microarray Quality Control (MAQC) [\cite=maqc] data (Fig. [\ref=fig:results]), there are k-mers (k = 20), of which appear at least once in the set of reads. However, there are only distinct equivalence classes of k-mers with non-zero counts. Thus, our EM procedure needs to optimize the allocations of k-mer equivalence classes instead of individual k-mers, a reduction by a factor of [formula].

Once the EM procedure converges, the estimated abundances are corrected for systematic errors due to sequence composition bias and transcript length using a regression approach similar to Zheng et al. [\cite=biascorrect], though using random forest regression instead of a generalized additive model. This correction is applied after initial estimates have been produced rather than at a read mapping or fragment assignment stage, requiring fewer variables to be fit during bias correction.

To examine the efficiency and accuracy of Sailfish, we compared it to RSEM [\cite=rsem:2011], eXpress [\cite=express] and Cufflinks [\cite=cufflinks] using both real and synthetic data. Accuracy on real data was quantified by the agreement between RNA-seq-based expression estimates computed by each piece of software and qPCR measurements for the same sample (human brain tissue (HBR) in Fig. [\ref=fig:results] and Supplementary Fig. [\ref=fig:supinf:qPCRHBRPlots], and universal human reference tissue (UHR) in Supplementary Fig. [\ref=fig:supinf:qPCRUHR]). These paired RNA-seq and qPCR experiments were performed as part of the Microarray Quality Control (MAQC) study [\cite=maqc]. qPCR abundance measurements are given at the resolution of genes rather than isoforms. Thus, to compare these measurements with the transcript-level abundance estimates produced by the software, we summed the estimates for all isoforms belonging to a gene to obtain an estimate for that gene. We compare predicted abundances using correlation coefficients, root-mean-square error (RMSE), and median percentage error (medPE) (additional details available in Supplementary Note 2). Figure [\ref=fig:results] shows that the speed of Sailfish does not sacrifice any accuracy.

To show that Sailfish is accurate at the isoform level, we generated synthetic data using the Flux Simulator [\cite=fluxsim], which allows versatile modeling of various RNA-seq protocols (see Supplementary Note 3). Unlike synthetic test data used in previous work [\cite=rsem:2011] [\cite=express], the procedure used by the Flux Simulator is not based specifically on the generative model underlying our estimation procedure. Sailfish remains accurate at the isoform level (Fig. [\ref=fig:results]).

The memory usage of Sailfish is comparable with that of other tools, using between 4 and 6 Gb of RAM during isoform quantification for the experiments reported here.

Sailfish applies the idea of lightweight algorithms to the problem of isoform quantification from RNA-seq reads and in doing so achieves a breakthrough in terms of speed. By eliminating the necessity of read mapping from the expression estimation pipeline, we not only improve the speed of the process but also simplify it considerably, eliminating the burden of choosing all but a single external parameter (the k-mer length) from the user. As the size and number of RNA-seq experiments grow, we expect Sailfish and its paradigm to remain efficient for isoform quantification because the memory footprint is bounded by the size and complexity of the target transcripts and the only phase that grows explicitly in the number of reads -- k-mer counting -- has been designed to effectively exploit many CPU cores.

Sailfish is free and open-source software and is available at .

Methods

Indexing.

The first step in the Sailfish pipeline is building an index from the set of reference transcripts . Given a k-mer length k, we compute an index [formula] containing four components. The first component is a minimum perfect hash function h on the set of k-mers contained in . A minimum perfect hash function is a bijection between [formula] and the set of integers [formula]. Sailfish uses the BDZ minimum perfect hash function [\cite=cmph]. The second component of the index is an array C containing a count [formula] for every [formula]. Finally, the index contains a lookup table F, mapping each transcript to the multiset of k-mers that it contains and a reverse lookup table R mapping each k-mer to the set of transcripts in which it appears. The index is a product only of the reference transcripts and the choice of k, and thus needs only to be recomputed when either of these changes.

Quantification.

The second step in the Sailfish pipeline is the quantification of relative transcript abundance; this requires the Sailfish index [formula] for the reference transcripts [formula] as well as a set of RNA-seq reads [formula]. First, we count the number of occurrences of each [formula]. Since we know exactly the set of k-mers that need to be counted and already have a perfect hash function h for this set, we can perform this counting in a particularly efficient manner. We maintain an array CR of the appropriate size , where [formula] contains the number of times we have thus far observed si in .

Sequencing reads, and hence the k-mers they contain, may originate from transcripts in either the forward or reverse direction. To account for both possibilities, we check both the forward and reverse-complement k-mers from each read and use a majority-rule heuristic to determine which of the k-mers to increment in the final array of counts CR. If the number of k-mers appearing in h from the forward direction of the read is greater than the number of reverse-complement k-mers, then we only increment the counts for k-mers appearing in this read in the forward direction. Otherwise, only counts for k-mers appearing in the reverse-complement of this read are incremented in the array of counts. Ties are broken in favor of the forward directed reads. By taking advantage of atomic integers and the compare-and-swap (CAS) operation provided by modern processors, which allows many hardware threads to efficiently update the value of a memory location without the need for explicit locking, we can stream through and update the counts of CR in parallel while sustaining very little resource contention.

We then apply an expectation-maximization algorithm to obtain estimates of the relative abundance of each transcript. We define a k-mer equivalence class as the set of all k-mers that appear in the same set of transcripts with the same frequency. In other words, let χ(s) be a vector that so that entry t of χ(s) gives how many times s appears in transcript [formula]. Then the equivalence class of a k-mer si is given by [formula]. When performing the EM procedure, we will allocate counts to transcripts according to the set of equivalence classes rather than the full set of transcripts. We will let [formula] denote the total count of k-mers in that originate from equivalence class [formula]. We say that transcript t contains equivalence class [formula] if [formula] is a subset of the multiset of k-mers of t and denote this by [formula].

Estimating abundances via an EM algorithm.

The EM algorithm (Algo. [\ref=alg:EM]) alternates between estimating the fraction of counts of each observed k-mer that originates from each transcript (E-step) and estimating the relative abundances of all transcripts given this allocation (M-step).

The E-step of the EM algorithm computes the fraction of each k-mer equivalence class' total count that is allocated to each transcript. For equivalence class and transcript ti, this value is computed by

[formula]

where i is the currently estimated relative abundance of transcript i. These allocations are then used in the M-step of the algorithm to compute the relative abundance of each transcript. The relative abundance of transcript i is estimated by

[formula]

where μi is

[formula]

The variable [formula] denotes the adjusted length of transcript i and is simply   =  li  -  k  +  1 where li is the length of transcript i in nucleotides.

However, rather than perform the standard EM update steps, we perform updates according to the SQUAREM procedure [\cite=squarem] described in Algo. [\ref=alg:SQUAREM]. [formula] is a vector of relative abundance maximum-likelihood estimates, and (  ·  ) is a standard iteration of the expectation-maximization procedure as outlined in Algo. [\ref=alg:EM]. For a detailed explanation of the SQUAREM procedure and its proof of convergence, see [\cite=squarem]. Intuitively, the SQUAREM procedure builds an approximation of the Jacobian of [formula] from 3 successive steps along the EM solution path, and uses the magnitude of the differences between these solutions to determine a step size γ by which to update the estimates according to the update rule (line [\ref=updateline]). The procedure is then capable of making relatively large updates to the [formula] parameters, which substantially improves the speed of convergence. In Sailfish, the iterative SQUAREM procedure is repeated for a user-specified number of steps (30 for all experiments reported in this paper; see Supplementary Fig. [\ref=fig:supinfo:Convergence]).

Bias Correction.

The bias correction procedure implemented in Sailfish is based on the model introduced by Zheng et al. [\cite=biascorrect]. Briefly, it performs a regression analysis on a set of potential bias factors where the response variables are the estimated transcript abundances (RPKMs). Sailfish automatically considers transcript length, GC content and dinucleotide frequencies as potential bias factors, as this specific set of features were suggested by Zheng et al. [\cite=biascorrect]. For each transcript, the prediction of the regression model represents the contribution of the bias factors to this transcript's estimated abundance. Hence, these regression estimates (which may be positive or negative) are subtracted from the original estimates to obtain bias-corrected RPKMs. For further details on this bias correction procedure, see [\cite=biascorrect]. The original method used a generalized additive model for regression; Sailfish implements the approach using random forest regression to leverage high-performance implementations of this technique. The key idea here is to do the bias correction after abundance estimation rather than earlier in the pipeline. The bias correction of Sailfish can be disabled with the -no-bias-correction command line option. Finally, we note that it is possible to include other potential features, like normalized coverage plots that can encode positional bias, into the bias correction phase. However, in the current version of Sailfish, we have not implemented or tested bias correction for these features.

Computing RPKM and TPM.

Sailfish outputs both Reads Per Kilobase per Million mapped reads (RPKM) and Transcripts Per Million (TPM) as quantities predicting the relative abundance of different isoforms. The RPKM estimate is the most commonly used, and is ideally 109 times the rate at which reads are observed at a given position, but the TPM estimate has also become somewhat common [\cite=rsem:2011] [\cite=wagner:2012]. Given the relative transcript abundances [formula] estimated by the EM procedure described above, the TPM for transcript i is given by

[formula]

Let [formula] be the number of k-mers mapped to transcript i. Then, the RPKM is given by

[formula]

where [formula] and the final equality is approximate only because we replace li with [formula].

Computing Accuracy Metrics.

Since the RPKM (and TPM) measurements are only relative estimates of isoform abundance, it is essential to put the ground-truth and estimated relative abundances into the same frame of reference before computing our validation statistics. While this centering procedure will not effect correlation estimates, it is important to perform before computing RMSE and medPE. Let X  =  {xi}Mi = 1 denote the ground-truth isoform abundances and Y  =  {yi}Mi = 1 denote the estimated abundances. We transform the estimated abundances by aligning their centroid with that of the ground-truth abundances; specifically, we compute the centroid-adjusted abundance estimates as Y'  =  {yi  -  ω}Mi = 1 where [formula]. It is these centroid adjusted abundance estimates on which we compute all statistics.

Simulated Data.

The simulated RNA-seq data was generated by the FluxSimulator [\cite=fluxsim] v1.2 with the parameters listed in Supplementary Note 3. This resulted in a dataset of 150M, 76 base-pair pared-end reads. RSEM, eXpress and Cufflinks were given paired-end alignments since they make special use of this data. Further, for this dataset, Bowtie [\cite=bowtie] was given the additional flag -X990 when aligning the reads to the transcripts, as 990 base-pairs was the maximum observed insert size in the simulated data. TopHat [\cite=tophat] was provided with the option -mate-inner-dist 198, to adjust the expected mate-pair inner-distance to the simulated average. The read files were provided directly to Sailfish without any extra information since the same quantification procedure is used whether single or paired end reads are provided.

Software Comparisons.

For all comparisons, both eXpress and RSEM were provided with the same sets of aligned reads in BAM format. All reads were aligned with Bowtie [\cite=bowtie] v0.12.9 using the parameters -aS and -v3, which allows up to three mismatches per read and reports all alignments. To prepare an alignment for Cufflinks, TopHat was run using Bowtie 1 (-bowtie1) and with options -N 3 and -read-edit-dist 3 to allow up to three mismatches per read. For RSEM, eXpress and Cufflinks, the reported times were the sum of the times required for alignment (via Bowtie for RSEM and eXpress or via TopHat for Cufflinks) and the times required for quantification. The time required for each method is further decomposed into the times for the alignment and quantification steps in Fig. [\ref=fig:results].

Choice of Software Options and Effect on Runtime.

Most expression estimation software, including RSEM, eXpress and Cufflinks, provides a myriad of program options to the user which allow for trade-offs between various desiderata. For example, the total time required by TopHat and Cufflinks is lower when Cufflinks is run without bias correction (e.g. 1.92h as opposed to 2.27h with bias correction on the SRX016366 data). However, without bias correction, Cufflinks yields slightly lower accuracy (Pearson σ  =  0.82, Spearman ρ  =  0.81) than the other methods, while still taking 16 times longer to run than Sailfish. Similarly, although aligned reads can be streamed directly into eXpress via Bowtie, we empirically observed lower overall runtimes when aligning reads and quantifying expressions separately (and in serial), so these times were reported. Also, we found that on the synthetic data, the correlations produced by eXpress improved to Pearson σ = 0.85, Spearman ρ = 0.87 and σ = 0.9, ρ = 0.92 when running the EM procedure for 10 or 30 extra rounds (-b 10 and -b 30). However, this also greatly increased the runtime (just for the estimation step) from 1.9h to 16.71h and 42.12h respectively. In general, we attempted to run each piece of software with the options that would be most common in a standard usage scenario. However, despite the inherent difficulty of comparing a set of tools parameterized on an array of potential options, the core thesis that Sailfish can provide accurate expression estimates much faster than any existing tool remains true, as the fastest performing alternatives, when even when sacrificing accuracy for speed, were over an order of magnitude slower than Sailfish.

Sailfish version 0.5 was used for all experiments, and all analyses were performed with a k-mer size of k = 20. Bias correction was enabled in all experiments involving real but not simulated data. The RPKM values reported by Sailfish were used as transcript abundance estimates.

RSEM [\cite=rsem:2011] version 1.2.3 was used with default parameters, apart from being provided the alignment file ([formula]), for all experiments, and the RPKM values reported by RSEM were used as abundance estimates.

eXpress [\cite=express] version 1.3.1 was used for all experiments. It was run with default parameters on the MACQ data, and without bias correction (-no-bias-correct) on the synthetic data. The abundance estimates were taken as the FPKM values output by eXpress.

Cufflinks [\cite=cufflinks] version 2.1.1 was used for experiments and was run with bias correction (-b) and multi-read recovery (-u) on the MACQ data, and with only multi-read recovery (-u) on the synthetic data. The FPKM values output by Cufflinks were used as the transcript abundance estimates.

All experiments were run on a computer with 8 AMD Opteron 6220 processors (4 cores each) and 256Gb of RAM. For all experiments, the wall time was measured using the built-in bash time command.

Implementation of Sailfish.

Sailfish has two basic subcommands, index and quant. The index command initially builds a hash of all k-mers in the set of reference transcripts using the Jellyfish [\cite=jellyfish] software. This hash is then used to build the minimum perfect hash, count array, and look-up tables described above. The index command takes as input a k-mer size via the -k option and a set of reference transcripts in FASTA format via the -t parameter. It produces the Sailfish index described above, and it can optionally take advantage of multiple threads with the target number of threads being provided via a -p option.

The quant subcommand estimates the relative abundance of transcripts given a set of reads. The quant command takes as input a Sailfish index (computed via the index command described above and provided via the -i parameter). Additionally, it requires the set of reads, provided as a list of FASTA or FASTQ files given by the -r parameter. Finally, just as in index command, the quant command can take advantage of multiple processors, the target number of which is provided via the -p option.

Sailfish is implemented in C++11 and takes advantage of several C++11 language and library features. In particular, Sailfish makes heavy use of built-in atomic data types. Parallelization across multiple threads in Sailfish is accomplished via a combination of the standard library's thread facilities and the Intel Threading Building Blocks (TBB) library [\cite=IntelTBB]. Sailfish is available as an open-source program under the GPLv3 license, and has been developed and tested on Linux and Macintosh OS X.

Author Contributions

R.P., S.M.M. and C.K. designed the method and algorithms, devised the experiments, and wrote the manuscript. R.P. implemented the Sailfish software.

Acknowledgments

This work has been partially funded by National Science Foundation (CCF-1256087, CCF-1053918, and EF-0849899) and National Institutes of Health (1R21AI085376 and 1R21HG006913). C.K. received support as an Alfred P. Sloan Research Fellow.

Sailfish: Alignment-free Isoform Quantification from RNA-seq Reads using Lightweight Algorithms Rob Patro, Stephen M.Mount and Carl Kingsford

Supplementary Figure : Effect of k-mer length on retained data and k-mer ambiguity

Supplementary Figure : Speed of counting indexed k-mers

Supplementary Note : Additional benefits of the Sailfish approach

An additional benefit of our lightweight approach is that the size of the indexing and counting structures required by Sailfish are a small fraction of the size of the indexing and alignment files required by most other methods. For example, for the MAQC dataset described in Figure [\ref=fig:results], the total size of the indexing and count files required by Sailfish for quantification was 2.4Gb, compared with much larger indexes and accompanying alignment files in BAM format used by other approaches (e.g., the 15.5Gb index and alignment file produced by Bowtie [\cite=bowtie]). Unlike alignment files which grow with the number of reads, the Sailfish index files grow only with the number of unique k-mers and the complexity of the transcriptome's k-mer composition and are independent of the number of reads.

Supplementary Figure : Correlation plots with qPCR on human brain tissue and synthetic data

Supplementary Figure : Correlation with qPCR on universal human reference tissue

Supplementary Note : Additional details of accuracy analysis

We compare predicted abundances using correlation coefficients (Pearson & Spearman), root-mean-square error (RMSE), and median percentage error (medPE). These metrics allow us to gauge the accuracy of methods from different perspectives . For example, the Pearson correlation coefficient measures how well trends in the true data are captured by the methods, but, because the correlation is taken in the log scale, it discounts transcripts with zero (or very low) abundance in either sample, while the RMSE includes transcripts with true or estimated abundance of zero. Both eXpress and Cufflinks produced a few outlier transcripts, with very low but non-zero estimated abundance, which significantly degraded the Pearson correlation measure. We discarded these outliers by filtering the output of these methods, and setting to zero any estimated RPKM less than or equal to 0.01, a cutoff chosen because it removed the outliers but did not seem to discard any other truly expressed transcripts.

Supplementary Note : Parameters for simulated data

The simulated RNA-seq data was generated by the FluxSimulator [\cite=fluxsim] v1.2 with the following parameters.

Supplementary Figure : Convergence of relative abundance estimates