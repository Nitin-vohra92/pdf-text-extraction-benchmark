Deriving Proper Uniform Priors for Regression Coefficients, Part II

Introduction

We, that is, the authors of this article, were in a position that we had to select from a considerable number spline models, that is, highly variate regression models. As these spline models may have hundreds of regression coefficients, we were forced to think about the most suitable bounds of the non-informative priors of the unknown parameters. Not because this would give us better parameter estimates, but simply because taking a uniform prior with overly large bounds would severely punish the larger regression models.

Grappling with this problem, we ended up with a uniform prior for the regression coefficients β, which is derived by putting k-sigma bounds on [formula], that is, the length of the error vector [formula]. Note that it is the multivariate probability distribution which we assign to the error vector that allows us to construct the likelihood function for some output vector [formula]. But, as it would seem, this multivariate probability distribution may also guide us in the construction of a parsimonious prior distribution for the unknown regression coefficients.

The evidence value, which results from this parsimonious prior is analytical and has as its sufficient statistics the sample size, N, the number of parameters used, m, the goodness of fit, [formula], and the sigma bound on the length of the error vector, k.

The structure of this paper is as follows. First we give a quick overview of the basic constructs of Bayesian statistics. Then we discuss the role of the evidence construct in Bayesian model selection. We then proceed to give a Bayesian regression analysis for the case where the spread σ is assumed to be known. This provides the pertinent context for the probabilistic prior of the unknown regression coefficients. We then proceed to derive the probabilistic prior and the corresponding evidence value, for the case where the spread σ is assumed to be known. We then use the probabilistic prior to compute the evidence value for the case, typically encountered in practice, where the spread σ is unknown. Finally, for completeness' sake, we give the Bayesian regression analysis for the case where the spread σ is assumed to be unknown.

Bayesian statistics

Bayesian statistics has four fundamental constructs, namely, the prior, the likelihood, the posterior, and the evidence. These constructs are related in the following way:

[formula]

Most of us will be intimately familiar with the prior, likelihood, and posterior. However, the evidence concept is less universally known, as most people come to Bayesianity by way of the more compact relationship

[formula]

which does not make any explicit mention of the evidence construct; see for example [\cite=Zellner71] throughout.

In what follows, we will employ in our analyses the correct, though notationally more cumbersome, relation [\eqref=eq1], and forgo of the more compact, but incomplete, Bayesian shorthand [\eqref=eq1b]. This is done so the reader may develop some feel for the evidence construct, and how this construct relates to the other three Bayesian constructs of prior, likelihood, and posterior.

Let [formula] be the prior of some parameter θ, where I is the prior information regarding the unknown θ which we have to our disposal. Let [formula] be the probability of the data D conditional on the value of parameter θ and the likelihood model M which is used; the probability of the data is also known as the likelihood of the parameter θ. Let [formula] be the posterior distribution of the parameter θ, conditional on the data D, the likelihood model M, and the prior model information I. Then

[formula]

where

[formula]

is the evidence, that is, marginalized likelihood of both the likelihood model M and the prior information model I. In the next section we will show how the evidence is used in Bayesian model selection.

Bayesian model selection

If we have a set of likelihood models Mj we wish to choose from, and just the one prior information model I, then we may do so by computing the evidence values [formula]. Let [formula] and [formula] be, respectively, the prior and posterior probability of the likelihood model Mj. Then the posterior probability distribution of these likelihood models is given as

[formula]

Note that if [formula] for all j and k, then we have that [\eqref=eq4] reduces to

[formula]

Stated differently, if we assign equal prior probabilities to our different likelihood models, the posterior probabilities of these models reduce to their normalized evidence values, that is, the models may be ranked by their respective evidence values [\cite=MacKay03].

We also may have the situation in which we have a set of prior information models to choose from. For example, in image reconstruction we have that all the artfulness goes into the construction of an informative prior, whereas the likelihood model is trivial and remains the same for all prior models considered, see for example [\cite=Skilling91]. Let [formula] be the evidence values of the prior information model Ij, and let [formula] and [formula], respectively, be their prior and posterior probabilities. Then the posterior probability distribution of the prior information models is given as

[formula]

And again, if [formula] for all j and k, we have that the prior information models may be ranked by their respective evidence values:

[formula]

In model selection for Bayesian regression analyses, we have yet another scenario, in which both the likelihood model, Mj, and the corresponding prior model, Ij, are determined by the particular choice of the N  ×  m predictor matrix X (as will be demonstrated in this paper). Let [formula] be the evidence value of the ensemble of the prior information and likelihood model, that is, IjMj, and let [formula] and [formula], respectively, be their prior and posterior probabilities. Then the posterior probability distribution of these ensembles is given as

[formula]

Again, if [formula] for all j and k, we have that the ensemble of the prior information and likelihood models may be ranked by their respective evidence values:

[formula]

Note that the right-hand sides of [\eqref=eq5], [\eqref=eq6], [\eqref=eq6c] all pertain to the same scaled evidence values, though their left-hand sides refer to different posteriors. Consequently, scaled evidences may be many different things to many different people, depending on the context of their analyses.

Bayesian regression analysis for known σ

Let the model M for the output vector [formula] be,

[formula]

where X is some N  ×  m predictor matrix, β is the m  ×  1 vector with regression coefficients, and [formula] is the N  ×  1 error vector to which we assign the multivariate normal distribution:

[formula]

or, equivalently, [formula], where I is the N  ×  N identity matrix and σ is some known standard deviation

By way of a simple Jacobian transformation from [formula] to [formula], [\eqref=eq7] and [\eqref=eq7b], we construct the likelihood function:

[formula]

We assign a uniform prior to the unknown regression coefficients β, [\cite=Zellner71],

[formula]

where C, is a yet unspecified constant and I is the prior information regarding the unknown β's, which we have at our disposal. By way of the Bayesian product rule, see also [\eqref=eq2] and [\eqref=eq3],

[formula]

we may derive the probability distribution of both vectors β and [formula]

[formula]

By integrating the unknown β out of [\eqref=eq10], we obtain the evidence of both M and I, [\eqref=eq3]:

[formula]

The evidence [\eqref=eq11] is used both to normalize [\eqref=eq10] into a posterior distribution, by way of the relation [\eqref=eq2], as well as to choose between competing regression models, [\eqref=eq5].

In order to evaluate the evidence [\eqref=eq11], we may rewrite the inner vector product in the exponential of [\eqref=eq10] as, Appendix A,

[formula]

where

[formula]

and

[formula]

Substituting the decomposition [\eqref=eq12] into [\eqref=eq10], we obtain

[formula]

which may be factored as

[formula]

The last term in [\eqref=eq18] evaluates to 1 when integrated over the β vector, as it is in the multivariate normal form, [\cite=Zellner71]. Consequently, we have, by way of the factorization [\eqref=eq18], that the evidence, that is, integral [\eqref=eq11], evaluates to

[formula]

If we then substitute [\eqref=eq18] and [\eqref=eq19] into [\eqref=eq2], we obtain the posterior of the unknown β vector,

[formula]

It can be seen that posterior of the unknown β has a mean of, [\eqref=eq13], [formula], and a covariance matrix of [formula].

Note that in parameter estimation problem, that is, the derivation of the posterior distribution [\eqref=eq20], all reference to the uniform prior C, [\eqref=eq9], has fallen away. In contrast, in the model selection problem, that is, the derivation of the evidence [\eqref=eq19], C is still very much there.

Assigning a parsimonious prior

We now try to specify the constant C in the prior [\eqref=eq9]. By way of [\eqref=eq7], we have that for a N  ×  m predictor matrix X or rank m,

[formula]

where [formula], [\eqref=eq7b]. Closer inspection of [\eqref=eq21] shows us that the parameter space of β is a-priori constrained by the error vector [formula]. We will now demonstrate this for the special case where the predictor matrix X is a N  ×  1 vector [formula].

By way of [\eqref=eq21], we have that

[formula]

where θ is the angle between the predictor vector [formula] and the error vector [formula], [formula] is the length of [formula], [formula] is the length of [formula]. Seeing that - 1  ≤   cos θ  ≤  1, we may by way of [\eqref=eq22a] put definite bounds on β

[formula]

Stated differently, if we assign a uniform distribution to the regression coefficient β, then this uniform distribution is defined on a line-piece of length [formula], and it follows that, for the case of just the one regression coefficient, the prior [\eqref=eq9] can be derived to be

[formula]

where [\eqref=eq23], is understood to be centered on [formula].

In Appendix B it is demonstrated that for the case where X is a N  ×  m predictor matrix, [\eqref=eq22] and [\eqref=eq23] generalize to the statements that β is constrained to lie in an m-dimensional ellipsoid which is centered on [formula] and has a volume of

[formula]

and that the corresponding multivariate uniform prior is the inverse of this volume:

[formula]

where [\eqref=eq25] is understood to be centered on [formula].

Seeing that [formula] has a known probability distribution, [\eqref=eq7b],

[formula]

we may derive, by way of a Jacobian transformation, the marginal probability distribution of [formula], that is, the length of the error vector [formula], Appendix B:

[formula]

This probability distribution has a mean

[formula]

and a variance

[formula]

By way [\eqref=eq28] and [\eqref=eq29], we may give a probabilistic interpretation of [formula] in [\eqref=eq25], that is, we let

[formula]

where k is some suitable sigma upper bound, for example, k  =  6. Note, that for small sample sizes N one should be careful to use in [\eqref=eq30] the exact terms of [\eqref=eq28] and [\eqref=eq29], as opposed to their approximations.

In what follows, we will assume large sample sizes N and, consequently, stick with the simpler approximation [\eqref=eq30]. Substituting [\eqref=eq30] into [\eqref=eq25], we obtain the prior of the β's we are looking for

[formula]

Note that the prior [\eqref=eq31] is conditional upon the spread parameter σ.

By way of [\eqref=eq9], we may substitute [\eqref=eq31] into [\eqref=eq19], and so obtain the evidence value of the likelihood model M and prior information I, conditional on some known σ,

[formula]

The evidence for unknown σ

By assigning the Jeffreys prior

[formula]

where A is some normalizing constant, to the evidence [\eqref=eq32], we may integrate out the unknown σ, see also [\eqref=eq3],

[formula]

where, [\eqref=eq32], [\eqref=eq33], and [\eqref=eq34],

[formula]

We may conveniently factorize [\eqref=eq35] as,

[formula]

where last term in [\eqref=eq36] evaluates to 1 when integrated over σ; as it has the form of an inverted gamma distribution, [\cite=Zellner71]. Consequently, we have, by way of the factorization [\eqref=eq36], that the evidence, that is, the integral [\eqref=eq34], evaluates to

[formula]

The evidence [\eqref=eq37] consists of an Occam Factor, which penalizes the number of parameters and which is a monotonic decreasing function in m:

[formula]

a goodness-of-fit factor, which rewards a good fit of the likelihood model M:

[formula]

and a common factor

[formula]

which is a shared by all evidence values and which cancels out as the posterior probabilities of the models are computed, [\eqref=eq4].

Note that the analytical evidence [\eqref=eq37] has as its sufficient statistics the sample size, N, the number of parameters used, m, the goodness of fit [formula], and the sigma bound k.

The Posterior of β for Unknown σ

We now will, for completeness sake derive the posterior of the β's, which is associated with the evidence value [\eqref=eq37]. We assign as priors for the β's and σ, [\eqref=eq31] and [\eqref=eq33],

[formula]

Multiplying the prior [\eqref=eq37] with the likelihood [\eqref=eq8], and dividing by the evidence [\eqref=eq37], we obtain the posterior of β and σ,

[formula]

The marginalized posterior of the β vector is

[formula]

We may factor [\eqref=eq41] as

[formula]

where the last term in [\eqref=eq43] evaluates to 1 when integrated over σ; as it has the form of an inverted gamma distribution, [\cite=Zellner71]. Consequently, we have, by way of the factorization [\eqref=eq43], that the marginalized posterior of β, that is, the integral [\eqref=eq42] evaluates to a multivariate Student-t distribution, [\cite=Zellner71]:

[formula]

where we have used [\eqref=eq12] to write

[formula]

where [\eqref=eq13] and [\eqref=eq14], [formula] and   =  X. The evidence corresponding with the marginalized posterior [\eqref=eq44] is given by [\eqref=eq37].

What is the Data?

The obvious elephant in the room is the question whether the predictor matrix X, used to derive the parsimonious prior [\eqref=eq31], is or is not a part of the data. In [\cite=vanErp10] the matrix X was deemed to be part of the data and, consequently, in order to construct the parsimonious prior, one needed to assign a minimum value to the determinant [formula], based on the prior information at hand; a non-trivial task.

This article is a second iteration of the [\cite=vanErp10] article, in which it is now suggested that the predictor matrix X is not a part of the data. And we offer up two arguments to substantiate this claim. The first argument is that in Bayesian regression analysis the predictor variables [formula] are assumed to be 'fixed nonstochastic variables', or, alternatively, 'random variables distributed independently of the [formula], with a pdf not involving the parameters βj and σ', as stated in [\cite=Zellner71]. The second argument, in the same vein, is that the likelihood, that is, the probability distribution of the data, [\eqref=eq8],

[formula]

is a probability of [formula], and not of X. This then also would imply that the predictor matrix X should not be considered a part of the data. Rather, X is part of the 'prior' problem structure, [\eqref=eq7],

[formula]

as is the assumed probability distribution of the error vector [formula].

The benefit of letting X not be a part of the data is that this allows us to derive the parsimonious prior [\eqref=eq31], without having to dub it a 'data' prior; a Bayesian oxymoron, if there ever was one.

Discussion

Using informational consistency requirements, Jaynes [\cite=Jaynes68] derived the form of maximal non-informative priors for location parameters, that is, regression coefficients, to be uniform. However, this does not tell us what the limits of this this uniform distribution should be, that is, what particular uniform distribution to use. If we are faced with a parameter estimation problem these limits of the uniform prior are irrelevant, since we may scale the product of the improper uniform prior and the likelihood to one, thus obtaining a properly normalized posterior. However, if we are faced with a problem of model selection then the value of the uniform prior is an integral part of the evidence, which is used to rank the various competing models.We have given here some guidelines for choosing a parsimonious proper uniform prior. To construct such a parsimonious prior one only needs to assign a prior maximal length to the error vector [formula]. In this paper we have treated the case that [formula], both for known and unknown σ.

Decomposing a vector product

In this appendix we will decompose the inner vector product [formula] in the sum of two inner vector products.

Let

[formula]

Making use of the identities

[formula]

and

[formula]

we have both

[formula]

and

[formula]

So, by way of [\eqref=eqC15] and [\eqref=eqC16], we may rewrite the last right-hand side of [\eqref=eqC12] as the sum of two inner vector products

[formula]

This concludes this appendix.

An ellipsoid parameter space

In this appendix we show that the transformation

[formula]

will map the vector [formula] somewhere in an ellipsoid which has a maximal volume of

[formula]

This result was first derived in [\cite=vanErp10].

Say we have m independent N  ×  1 vectors [formula] that span some m-dimensional orthogonal subspace in the N-dimensional data space. We may decompose the vector [formula] as

[formula]

where [formula] is the projection of [formula] on the m-dimensional subspace spanned by the vectors [formula] and [formula] is the part of [formula] that is orthogonal to this subspace.

Now, the projection [formula] is mapped on the orthogonal base spanned by the vectors [formula] through the regression coefficients βj, that is,

[formula]

where

[formula]

Because of the independence of the [formula], we have that [formula], for i  ≠  j. So, if we take the squared norm of [\eqref=eqB2] we find, by way of [\eqref=eqB3],

[formula]

From identity [\eqref=eqB4], it then follows that the angles cos θj in [\eqref=eqB3] must obey the constraint

[formula]

Combining [\eqref=eqB3] and [\eqref=eqB5], we see that all possible values of the coordinates βj must lie on the surface of an m-variate ellipsoid centered at the origin, having a volume of

[formula]

and with respective axes

[formula]

Since

[formula]

the axes [\eqref=eqB6] admit the upper bounds

[formula]

Consequently, the volume of the parameter space of the βj is, for given [formula], [\eqref=eqB6b],

[formula]

Because of the independence of the [formula], we have that the product of the norms [formula] is equivalent to the square root of determinant of XTX, that is,

[formula]

where [formula] is the volume of the parallelepiped defined by the [formula]. So, we may rewrite as [\eqref=eqB8]

[formula]

If the predictors [formula] are not independent, then we may transform them to an orthogonal basis, say, j, by way of an Gram-Schmidt orthogonalization process. But seeing that the volume of the parallelepiped is invariant under orthogonalization, we have that

[formula]

where [formula] is the orthogonalized predictor matrix. So, we conclude that [\eqref=eqB10] is the volume of the parameter space of the βj for both dependent and independent predictors [formula].

The probability distribution of [formula]

Let [formula] be a N  ×  1 error vector having the multivariate normal distribution

[formula]

where σ is some known standard deviation. Then we make a change of variable [\cite=Zellner71]

[formula]

The Jacobian of the transformation[\eqref=eqA2] is

[formula]

From trigonometry [\eqref=eqA2] yields,

[formula]

So, substituting [\eqref=eqA2], [\eqref=eqA3], and [\eqref=eqA4] into [\eqref=eqA1], we may rewrite the distribution [\eqref=eqA1] as

[formula]

Using, for [formula],

[formula]

and, for j = N - 1,

[formula]

We are left with the marginal distribution

[formula]

The rth moment of [\eqref=eqA6] may be computed by way of the identity

[formula]