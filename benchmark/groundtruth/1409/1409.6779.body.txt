Lemma Proposition Definition Corollary Conjecture

On estimation in the reduced-rank regression with a large number of responses and predictors

Vladislav Kargin

Abstract

Introduction

In this paper we are concerned with the reduced rank variant of the multivariate response regression model. We are given N observations of the predictors [formula] and responses [formula] which are assumed to be related by the linear regression model:

[formula]

where A is an unknown p-by-r matrix and U is a noise matrix. This model is ubiquitous in statistics, signal processing, and numerical analysis.

On methodological grounds one often postulates that the responses depend only on a small number of factors which are linear combinations of the predictors. This postulate leads to a model, in which A is assumed to be a low-rank matrix:

[formula]

where [formula] and [formula] are two fixed orthonormal vector systems. This model appeared already in Anderson (1951) [\cite=anderson51], and it was named reduced-rank regression in Izenman (1975) [\cite=izenman75]. In some contexts, this model is also known under the names simultaneous linear prediction (Fortier (1966) [\cite=fortier66]) and redundancy analysis (van den Wollenberg (1977) [\cite=wollenberg77]), both of which assume that U has the covariance matrix equal to σ2I. The reduced-rank model has been intensively studied, and many results are collected in the monograph by Reinsel and Velu (1998) [\cite=reinsel_velu98].

In this paper, we assume that U has the covariance matrix equal to σ2I, and we are interested in the situation in which all three variables, p, r, and N, grow at the same rate.

Assumption A1. It is assumed that as N  →    ∞  , [formula] 1 + λ  ≥  1 and [formula].

It is also useful to define [formula]

The studies devoted to the reduced-rank regression in this setup are relatively recent and include Bunea, She, and Wegkamp (2011) [\cite=bunea_she_wegcamp11] and Giraud (2011) [\cite=giraud11].

We address the following questions. First, is it possible to detect that the true matrix A is not zero? If yes, then how do we how do we estimate the rank and singular values of A?

Our approach to these questions is based on the study of the statistical properties of the standard least squares estimator and the matrix of fitted responses:

By using this approach, we will develop a rank-selection algorithm which performs better than the algorithm from [\cite=bunea_she_wegcamp11] in a certain range of parameters and is simpler than the algorithm in [\cite=yelm07]. In addition, we will develop tools for consistent estimation of singular values θi. The paper [\cite=bunea_she_wegcamp11] does not address this issue, since its focus is on minimizing the prediction error, in particular on bounds for [formula] where [formula] is an estimator of A and the expectation is over randomness in U.

The rest of the paper is organized as follows. Section [\ref=section_major_results] describes the major results. Section [\ref=subsection_largest_eigenvalue_nonnull] provides the details of the proofs. Section [\ref=conclusion] recapitulates the results. And Appendix [\ref=section_limiting_distribution] provides a proof for the theorem about the limiting distribution of singular values of Â.

Major results

Tests of the null hypothesis

Let X be a p-by-r real Gaussian matrix: each row is an independent observation from N(0,Σ). Then, an r-by-r matrix [formula] is said to be a Wishart matrix with distribution Wr(Σ,p).

A random m-by-m matrix X is said to belong to the (real) Jacobi ensemble with parameters α1 and α2, if its distribution is invariant with respect to orthogonal transformations and the distribution of its eigenvalues is given by

[formula]

The following result is fundamental for the analysis of matrices Â and Ŷ.

(i) Suppose that U is an N-by-r matrix with i.i.d standard real Gaussian entries, and X is an N-by-p full-rank matrix (N  ≥  p) independent of U. Then the squared singular values of [formula] are distributed as the eigenvalues of the Wishart matrix with distribution Wr(I,p). (ii) In addition, suppose that X has i.i.d standard real Gaussian entries. Let s2i be the squared singular values of [formula] and fi = s2i / (1 + s2i). Then, the positive fi are distributed as eigenvalues of the Jacobi ensemble with parameters m =  min {p,r}, α1 = (|r - p| - 1) / 2 and α2 = (N - p - 1) / 2.

Proof: The matrix [formula] is the orthogonal projection of r column vectors of U on the p-dimensional column span of X. Hence, in an appropriate basis, Ŷ is a block matrix with one block given by a p-by-r matrix with i.i.d. standard Gaussian entries and another block of N - p-by-r matrix of zeros. This proves the first part of the theorem. For the second part, note that positive eigenvalues of [formula] have the same distribution as positive eigenvalues of B- 1C, where B and C are independent Wishart matrices.

Indeed, the rank of matrices [formula] and [formula] is min {p,r}, and their positive eigenvalues are the same. Let W be an orthogonal N-by-p matrix formed by the eigenvectors of [formula] and such that the matrix [formula] is diagonal with positive eigenvalues on the diagonal. These eigenvalues coincide with positive eigenvalues of the inverse of a Wishart matrix, [formula], where the Wishart matrix has the distribution Wp(I,N). The matrix [formula] is Wishart with distribution Wp(I,r).

In addition, matrices [formula] and [formula] are independent because the eigenvalues and eigenvectors of [formula] are independent. Finally, since similarity transformations do not change eigenvalues, the distribution of positive eigenvalues of [formula] is the same as the distribution of positive eigenvalues of B- 1C for two independent Wishart matrices B and C with distributions Wp(I,N) and Wp(I,r), respectively.

Next, note that the eigenvalues of B- 1C are the same as those of [formula] where F = (B + C)- 1C. Hence the eigenvalues of B- 1C, denoted by li, are related to the eigenvalues of [formula] denoted by fi by the transformation,

[formula]

Then one can use the classical fact that the eigenvalues of [formula] are distributed as in ([\ref=pdf_Jacobi]) with α1 = (|r - p| - 1) / 2 and α2 = (N - p - 1) / 2. (See Theorem 3.3.1 in Muirhead [\cite=muirhead82]). [formula]

There are several ways to test the null hypothesis that A = 0. The simplest way is to compute the largest eigenvalue of [formula] called lY,1, and compute where

[formula]

If A = 0 and the noise matrix U has i.i.d. standard Gaussian entries, then by Theorem 1.1 in Johnstone (2001) [\cite=johnstone01] sY,1 converges in distribution to the Tracy-Widom distribution F1. Note that this test does not depend on X and that the normality assumption on the entries of U can be significantly relaxed. Indeed, by results of Pillai and Yin [\cite=pillai_yin14], the convergence to the Tracy-Widom distribution holds provided that the entries are independent, have zero mean, unit variance, and subexponential decay: [formula] for a positive κ and all t > 1. (The first universality result of this type is due to Soshnikov [\cite=soshnikov02].)

Two other methods to test the null hypothesis are conditional on X. They are based on the singular values of matrices [formula] and [formula] Let us define two statistics, sŶ,1 and sÂ,1:

where lŶ,1, is the square of the largest singular value of Ŷ and

[formula]

Similarly, where lÂ,1 is the square of the largest singular value of Â and

[formula]

and the angle parameters γ and φ are defined by

[formula]

(i) Suppose that A = 0, Assumption A1 is satisfied, U is a matrix with independent standard Gaussian entries, and X is a full-rank matrix independent of U. Then the random variable sŶ,1 converges in distribution to the Tracy-Widom distribution. (ii) In addition, suppose that X has i.i.d. standard Gaussian entries. Then sÂ,1 converges in distribution to the Tracy-Widom distribution.

Proof: Both claims follow from Theorem [\ref=theorem_fundamental] above and Johnstone's work on the largest eigenvalues of the Wishart and Jacobi ensembles (specifically, Theorem 1.1 in [\cite=johnstone01] and Theorem 1 in [\cite=johnstone08]). [formula]

Corresponding to each of these three tests, we can devise a procedure for the choice of the rank of the model ([\ref=low_rank_model]):

1. Calculate the statistics sY,i, sŶ,i, or sÂ,i for the singular values of corresponding matrices.

2. Check how many of these statistics exceed the 10% quantile of the Tracy-Widom distribution (x10% = 0.45) and take this number as the rank of the model ([\ref=low_rank_model]).

This should be compared to the procedure suggested in [\cite=bunea_she_wegcamp11]. The first part of their procedure is to compute lŶ,i. Next, they prescribe to choose the rank of the model equal to the number of statistics lŶ,i that exceed a threshold t. This is similar to our prescription. However, our choice of the threshold is different from the choice in [\cite=bunea_she_wegcamp11]. They suggest either choosing t by cross-validation or using t = 2(p + r). We use t = 2(p + r) to replicate their method in numerical experiments and call this algorithm BSW ("Bunea-She-Wegkamp"). Our choice of the threshold is based on the 10-th quantile of the Tracy-Widom distribution.

It is not guaranteed that any of these algorithms will estimate the rank of the model successfully. First of all, as we will see later, there is a certain threshold, so that if a true singular value θ is below this threshold then it cannot be estimated consistently. Second, since Theorem [\ref=theorem_TW] is about the null case, hence it is not applicable in the situation with nonzero true singular values and it does not guarantee that the largest of the remaining singular value estimates is distributed according to the Tracy-Widom law.

The good news is that if the true singular values exceed the threshold, then Theorem 2 in [\cite=onatski06] and Proposition 5.8 in [\cite=BenaychGeorges_Guionnet_Maida11] suggest that our Theorem [\ref=theorem_TW] can be extended to the non-null case by perturbative methods, and therefore the distribution of the largest of the remaining singular values of Ŷ or Â is indeed Tracy-Widom. (An analogous result also holds for deformations of Wigner matrices, see Proposition 5.3 in [\cite=BenaychGeorges_Guionnet_Maida11] and Theorem 2.7 in [\cite=knowles_yin13].) However, if one of the true singular values is at the threshold (precisely, if it is in a neigborhood of the threshold that shrinks as the matrix size grows), then the limiting distribution is a deformation of the Tracy-Widom law. For the real-valued Wishart matrices, the full description of this deformation is still an active research problem ([\cite=forrester13a]).

In order to compare the performance of the rank-selection algorithms we run several numerical experiments. Their results are summarized below.

In the first experiment, we assumed that the null hypothesis is satisfied and A = 0. The number of observations in this experiment is N = 100, and the number of predictors is p = 25.

Figure [\ref=table_rank_selection_1] displays the estimated rank [formula] averaged over many simulations of the model. The results show that for the null case A = 0 the algorithms based on the Tracy-Widom distribution falsely detect about 10% of model realizations as having rank 1 (both in the case p = r = 25 and in the case p = 25 < r = 75). This behavior should be expected since the threshold was set at the 10-percentile of the Tracy-Widom distribution. In other words, the significance level of the tests is 10%.

In contrast, the results of the BSW ("Bunea-She-Wegkamp") algorithm change from about 12% of false detections in the case when p = r = 25 to 2% in the case when p = 25 < r = 75. Hence, in our application of the BSW algorithm, the proportion of false positives varies depending on parameters. That is, one cannot easily pinpoint the significance level of the test based on the BSW algorithm.

The results for non-null case with rank s = 1 are shown in Figure [\ref=table_rank_selection_2]. In this experiment, the strength of the signal θ was chosen to make it difficult but not impossible to detect the signal. As before, N = 100 and p = 25.

The values in Figure [\ref=table_rank_selection_2] are simulation-based estimates of the expected values of the rank estimators. For our choice of parameters, in most repetitions the rank estimator takes the values 0 or 1. Hence, the numbers in this table are approximations for the power of the test, that is, for the proportion of the times when the test detects the signal when it is in fact present. (Note, however, that this is only an approximation since the rank estimator [formula] can take values greater than 1. In particular, this explain the value 1.05 in the table.)

For r = 25, the values in Figure [\ref=table_rank_selection_2] show that the estimated expected values of the rank estimators based on the BSW and the fitted values algorithms are 0.56 and 0.53, respectively. The estimated expected values for the other two rank estimators are much smaller. Since the true rank is 1, this finding suggests that the power of the tests based on the BSW and the fitted values algorithms is larger than for the tests based on the other two rank estimators.

As the number of responses, r, increases, it becomes easier to detect the signal. The best performer for larger r is the algorithm based on the distribution of the largest singular value of the fitted responses. In contrast, the algorithm based on the singular values of the coefficient matrix estimator Â detects the signal poorly. The performance of the BSW algorithm is also not very satisfactory as it appears to be too conservative and biased in favor of the null hypothesis. For example, for r = 200, the BSW algorithm is outperformed by the simple algorithm based on the singular values of responses, which does not use any information about the design matrix X.

Finally, we consider the setup, in which A has 10 nonzero singular values, each with value θ  =  0.05. The results, summarized in Figure [\ref=table_rank_selection_3], are similar to results for one nonzero singular value in Figure [\ref=table_rank_selection_2]. The best estimates are produced by the BSW algorithm and the algorithm based on fitted responses Ŷ. For small r, the signal detection is difficult and both algorithms perform roughly similar. For large r, the algorithm based on fitted responses outperform the BSW algorithm.

Estimation of singular values

Limit distribution of the squared singular values

Recall that the empirical eigenvalue distribution of a square Hermitian r-by-r matrix M is the measure [formula] where λi are eigenvalues of M and δx is the Dirac measure concentrated on x, that is, for every continuous function f, [formula]

Let [formula] and [formula] (i)Suppose that Assumption A1 is satisfied, U is a matrix with i.i.d. standard Gaussian entries, and X is a full-rank matrix independent of U. Then, as N  →    ∞  , the empirical eigenvalue distribution of the r-by-r matrix [formula] converges weakly to the Marchenko-Pastur probability measure [formula] with the density defined in ([\ref=density_MP]). (ii) In addition, suppose that the entries of X are i.i.d. standard Gaussian variables. Then the empirical eigenvalue distribution of [formula] weakly converges to the probability measure [formula] with the density defined in ([\ref=density]).

The Marchenko-Pastur measure [formula] is supported on the interval and has the density

[formula]

If 0 < β < 1, then the measure [formula] has an additional atom at 0 with mass 1 - β.

The family of probability measures [formula] is parameterized by λ  ≥  0 and β > 0. The continuous part of [formula] is supported on the interval [formula] If λ > 0, then

[formula]

where x1 and x2 correspond to the -   and +   signs before the square root, respectively. If λ = 0, then [formula] and x2  =    ∞  . In both cases, the density is

[formula]

If 0 < β < 1, then the measure [formula] has an additional atom at 0 with mass 1 - β.

Proof: Both parts immediately follow from Theorem [\ref=theorem_fundamental]. The first part uses a property of the Wishart random matrices discovered by Marchenko and Pastur [\cite=marchenko_pastur67] (and independently rediscovered by Jonnson [\cite=jonnson82] and Wachter [\cite=wachter78]).

The second part uses a property of the Jacobi ensemble shown by Wachter [\cite=wachter80] and Silverstein [\cite=silverstein85]. In appendix, we give another proof of this property which uses the S-transform from free probability.

Remarks: 1. The theorem is illustrated by a numerical example in Figure [\ref=table_limit_distr].

2. Here is what happens in some special cases:

a) If λ  →    ∞   and β is fixed, then both x1 and x2 converge to 0. This is in agreement with the intuitive notion that the true matrix AN = 0 can be estimated precisely if the number of observations is large relative to the number of variables in the model.

b) If λ  →    ∞   and β  →  0 so that λβ  →  ξ > 0, then both x1 and x2 converge to ξ- 1. Note that [formula]

c) If λ is fixed and β  →    ∞  , then both x1 and x2 converge to λ- 1.

In both b) and c), the regression will pick up spurious dependencies. In b), it is because the number of responses is very large, and in c), it is because the number of predictors is comparable to the number of observations.

d) If p = r, then Â is square and one can ask about the distribution of its eigenvalues. By using the methods from [\cite=guionnet_krishnapur_zeitouni11] and [\cite=haagerup_larsen00], the limit distribution of eigenvalues of Â can be recovered from that of its singular values. It turns out that the limit distribution is supported on the disc [formula] and has the density where dm(z) is the Lebesgue measure on the complex plane. After the stereographic projection this measure maps to the uniform measure on a Riemann sphere's cap.

This is a generalization of the result for the spherical ensemble of random matrices, which occurs when N = p = r, and therefore Â = X- 1U. For the eigenvalues of this ensemble, it is known that the limit distribution is uniform on the Riemann sphere after the stereographic projection. (See [\cite=krishnapur08], [\cite=bordenave11], [\cite=forrester_mays12] and [\cite=tikhomirov13] for more results about this ensemble).

3. In [\cite=tikhomirov13], Tikhomirov considers the spherical ensemble X- 1U and finds the limit distribution of its singular values under rather weak assumptions on the distribution of matrix entries. The basis for the results in [\cite=tikhomirov13] is the Gaussian case which is extended to non-Gaussian matrices by consecutively changing every matrix entry to a Gaussian random variable and then verifying that the total change in the Stieltjes transform of the eigenvalue distribution is negligible. We conjecture that the result in Theorem [\ref=Theorem_main] can be extended to non-Gaussian matrices in a similar way.

Consistent estimation of singular values

Now, suppose that the true matrix A is from the reduced-rank model ([\ref=low_rank_model]). We are interested to know if the singular values θi can be estimated consistently.

The estimator [formula] is an m-by-r matrix that can be written as follows: In other words, Â is the sum of the matrix A and a random matrix. This random matrix has a rotationally invariant distribution by the assumption that both X and U are Gaussian with i.i.d entries. In addition, matrix A has a fixed set of non-zero singular values [formula] and we assume that N  ≫  s. In this situation, one can apply the results by Benaych-Georges and Nadakuditi from [\cite=BenaychGeorges_Nadakuditi12].

It turns out that the singular values of Â do not converge to that of A as N  →    ∞  . We need to define a correction function, DA(x), that will map the singular values of Â to consistent estimates of the singular values of A.

Recall that the Stieltjes transform of a probability measure [formula] is an analytic function defined  as Let GA(z) denote the Stieltjes transform for the measure [formula] (from Theorem [\ref=Theorem_main]). It can be computed as

[formula]

(This follows from formula ([\ref=Stieltjes_P_lambda_beta]) in Appendix after rescaling by β- 1.)

Let and This function has no singularities for real [formula] (with x2 defined in ([\ref=support])), and it is increasing on [formula] The behavior of the function DA(x) - x for various values of parameters λ and β is illustrated in Figure [\ref=D_function].

Let Note that [formula]. (The threshold is below the upper edge of the support for the limit measure [formula].)

Let [formula] be the first s largest singular values of [formula], where Y = XA + U and [formula] with [formula]. Suppose Assumptions A1 is satisfied with λ > 0 and X and U are independent matrices with i.i.d. standard Gaussian entries. For each fixed i, if [formula] then almost surely as N  →    ∞  , If [formula], then [formula]

In other words, [formula] is a consistent estimator of θi provided that [formula] otherwise θi is hidden by spurious eigenvalues of Â.

There is a similar result that uses Ŷ instead of Â. Define where GβMP(x) is the Stieltjes transform of the Marchenko-Pastur distribution with parameter β:

[formula]

and [formula]

Define xu,Y as the upper edge of the support of the Marchenko-Pastur distribution [formula] and [formula]

Let [formula] where Y = XA + U and [formula] with [formula]. Suppose Assumption A1 is satisfied with λ > 0, and X and U are independent matrices with i.i.d. standard Gaussian entries. Let i,Ŷ be the first s largest singular values of [formula] If [formula] then almost surely as N  →    ∞  , If [formula], then [formula]

Remarks: 1. These results are similar in spirit to the results in [\cite=baik_benarous_peche05], [\cite=baik_silverstein06], and [\cite=paul07], which are concerned with the singular values of sample covariance matrices. In these papers, it was found that if the true covariance matrix has a 'spike' that exceeds a certain threshold, then it will be observed in the spectrum of the sample covariance matrix. Otherwise it will be hidden among the spurious eigenvalues.

2. The quality of the estimators is illustrated in Figures [\ref=fig_estimate] and [\ref=table_thresholds]. They show that for relatively small values of parameters λ and β, the threshold [formula] is smaller than [formula] and the estimator i,Ŷ is preferable to i,Â. For large λ and β the threshold [formula] can be smaller than [formula]. However, the difference is small and in this region the correction term D(x) - x is also small.

3. The proofs of Theorem [\ref=theorem_perturbation] and Theorem [\ref=theorem_perturbation_Y] are essentially by combining Theorem [\ref=Theorem_main] in this paper and Theorem 2.8 in [\cite=BenaychGeorges_Nadakuditi12]. We provide a detailed proof in Section [\ref=subsection_largest_eigenvalue_nonnull] below for the convenience of the reader. (The proof of Theorem 2.8 in [\cite=BenaychGeorges_Nadakuditi12] lacks some details and has annoying typos.) However, we defer to [\cite=BenaychGeorges_Nadakuditi12] for details about convergence and continuity issues.

CLT for the estimator of the singular values

Let us define the empirical version of the function GA(x):

[formula]

where si are singular values of the matrix [formula] Let

[formula]

and

[formula]

Then Theorem [\ref=theorem_perturbation] holds with [formula] instead of [formula]

Next, let [formula] where [formula] is the largest singular value of the matrix [formula] and DA,N is as defined in ([\ref=def_D_N]).

Assume that Y = XA + U with [formula]. Suppose Assumption A1 is satisfied with λ > 0, and X and U are independent matrices with i.i.d. standard Gaussian entries.

Then with ω as defined below in ([\ref=omega]), converges in distribution to a standard zero-mean Gaussian random variable.

Let σ be the largest solution of the equation Define

[formula]

Let

[formula]

Remark: We conjecture that the statement of the theorem holds true with the estimator [formula] instead of [formula] However the proof of this statement runs into some technical difficulties and might require some additional assumption on the convergence N / m  →  1 + λ and m / r  →  β.

Proofs of the main results

Proof of Theorem [\ref=theorem_perturbation]

The basic tool is the following determinantal identity. Let |X| denote the determinant of a matrix X.

Let Ψ and D be an N-by-N and s-by-s matrices, respectively, and let W1 and W2 be an N-by-s and s-by-N matrices, respectively. Assume that D is invertible. Then,

Proof: The proof is through a sequence of elementary determinantal identities:

[formula]

[formula]

We apply this lemma to understand how the singular values of a matrix Z are affected if Z is perturbed by a low-rank matrix A.

Let Â = A + Z with [formula] Let [formula] and let U and V be an p-by-s and an r-by-s matrices whose columns are vectors ui and vi, respectively. Let

[formula]

If t > 0 and [formula] then t is a singular value of Â. Conversely, if a singular value of Â is different from the singular values of Z, then it is a zero of the function [formula] defined in ([\ref=definition_Mt]) below.

Proof: We apply Lemma ([\ref=lemma_perturbation1]) to [formula] [formula] [formula] and [formula] and we use the identity: Then we get

[formula]

Hence, the positive eigenvalues of the matrix [formula] are either positive eigenvalues of the matrix [formula] or positive zeros of [formula] Recall that the singular values of matrix A + Z coincide with the positive eigenvalues of matrix and similarly for the singular values of matrix Z. This proves both claims of the Lemma. [formula]

We will apply this theorem to [formula], where U denotes the matrix of noise in the regression model [\ref=regression_model] and should not be confused with matrix U in the definition of M(t). The crucial observation is that the rotational invariance of Z and Theorem [\ref=Theorem_main] imply that the s-by-s matrix [formula] converges to the scalar matrix [formula] provided that [formula].

By using Lemma [\ref=lemma_shift], one can also check that the matrix [formula] converges to [formula]

The off-diagonal blocks of [formula] converge to zero.

Finally, we use the fact that and conclude (by continuity properties of the determinantal equation solutions proved in [\cite=BenaychGeorges_Nadakuditi12]) that if [formula] then there is a singular value of Â that converge to the positive solution of the following equation: Since the left hand side equals [formula] by definition, this convergence establishes the first part of Theorem [\ref=theorem_perturbation].

In order to establish the second part, let [formula]. Then, by monotonicity of DA(x), if i > k then for every ε > 0 and sufficiently large N the equation DA(x) = θi does not have roots in the interval [formula]. Hence, by Lemma [\ref=lemma_reduction_to_M]), the eigenvalues of matrix Â can have only k limits greater than [formula]. It can also be checked that the singular values of the matrix Â has the same limiting distribution, [formula], as in the null case. (Indeed, a deformation by a fixed-rank matrix does not affect the empirical distribution of eigenvalues). It follows that the (k + 1)-st, (k + 2)-nd, , and s-th largest singular values of Â must converge to [formula]. [formula]

Proof of Theorem [\ref=theorem_perturbation_Y]

An analogue of Lemma [\ref=lemma_reduction_to_M] holds and establishes the fact that the singular values of [formula] are zeros of [formula], where

[formula]

where [formula], and U is the matrix of noise in the regression model [\ref=regression_model]. As before, the off-diagonal blocks of MY(t) converge to -  Θ- 1. Besides, if t is sufficiently large, then the first diagonal block converges to tGβMP(t2)Is.

In addition, for sufficiently large t, the limit of the second diagonal block is the scalar matrix atIs, where a is the limit of

[formula]

In order to calculate this limit, we note that the matrices [formula] and [formula] converge in distribution (in the sense of free probability theory) to pairs of non-commutative random variables [formula] and [formula], which are free from each other. Let [formula]. This is the orthogonal projection corresponding to [formula], since [formula], P2x = Px and [formula]. Then, the limit of the expectation of ([\ref=matrix_trace]) is equal to the limit of

[formula]

where τ is the trace in the corresponding free probability space.

In order to handle this expression, we use the following lemma.

Suppose that the pair of variables [formula] and the variable b are free from each other, and suppose that Pa has the properties Paa = aPa = a and P2a = Pa. Then

Proof: On both sides we have complex-analytic functions in t (aside of singularities), and we can expand these functions in powers of t- 1. It is enough to show the equality in the region of the complex plane where the resulting series converge, since for other values of t the equality will hold by analytic continuation. Consequently, it is enough to show that the equality holds term by term in the series. In particular, we need to check that for each integer n  ≥  1. By using the properties of Pa and a, the left hand-side can be written as Next we use the property that [formula] and b are free, and write:

[formula]

where the sum is extended over all non-crossing partitions of the sequence [formula], K(π) is the Kreweras complement of the partition π, τK(π) is the multiplicative extension of the trace τ, associated with partition K(π), and κπ denotes the free cumulant functional associated with partition π. (For additional details and a proof of formula ([\ref=Nica_Speicher]), see Theorem 14.4 in [\cite=nica_speicher06].)

Let |π| denote the number of blocks in the partition π. It is a fact that |K(π)| = n + 1 - |π|. Then, by multiplicativity of τK(π), the expression in ([\ref=Nica_Speicher]) can be written as We can take τ(a) outside of the sum sign, and a similar calculation shows that Hence, [formula]

By applying this lemma to the expression in ([\ref=R_XX]), we calculate that its limit equals where [formula].

Hence, for sufficiently large t, [formula] converges to From this, we conclude that for [formula], there exists a singular value of [formula], denoted by i, such that in probability. This is equivalent to the claim in the first part of the Theorem. The second part is proved as in Theorem [\ref=theorem_perturbation]. [formula]

Proof of Theorem [\ref=theorem_clt]

Now we are going to prove the central limit theorem for the estimator [formula] Recall that we are dealing with the rank-one perturbation model, in which θ is the singular value of the perturbation and [formula] is the largest singular value of the estimator [formula] First, we note that the function [formula] concentrates better than what could be expected if si were independent.

Let si, [formula]  be the singular values of the m-by-r matrix [formula] and x2 be as defined in ([\ref=support]). Then for every ε > 0 and every t > x2, in L2.

Proof: The claim of this lemma is a consequence of a CLT for linear eigenvalue statistics. Indeed, the eigenvalues of [formula] are the transformed eigenvalues from the Jacobi ensemble of random matrices (with a smooth transformation ([\ref=relation_of_eigenvalues])). Hence we can use the results about the linear eigenvalue statistics of the Jacobi ensemble. For example, the results of Dumitriu and Paquette [\cite=Dumitriu_Paquette12] (specifically their Theorem 3.1) imply that [formula] converges to a Gaussian random variable with a finite variance. Hence, [formula] in L2. [formula]

Remark: The results of Johansson in [\cite=Johansson98] (e.g., his Theorem 2.4) suggest that one can use [formula] instead of [formula] in the above lemma. However, Johansson's results are proved under assumption of a fixed potential V, while the potential in the pdf of the Jacobi ensemble is changing with N. Hence, some additional assumptions on the convergence m / N  →  λ and m / r  →  β might be needed.

Next, we prove a CLT for the matrix [formula]

Assume that Y = XA + U with [formula]. Let [formula] be the 2-by-2 matrix defined in ([\ref=definition_Mt]), and let [formula] and [formula] be as defined in ([\ref=def_G_N]) and (t[\ref=def_G_N_tilda]), respectively. If t > x2 (with x2 defined in ([\ref=support])), then the random matrix converges in distribution to the matrix where X,Y, and Z are independent standard Gaussian random variables, and

[formula]

For the proof we rely on Theorem 7.1 in [\cite=bai_yao08] that allows one to compute the distributional limit for the forms [formula] as n  →    ∞  , for independent, identically distributed K-tuples of real or complex valued random variables [formula] under some additional assumptions on matrices [formula]. The theorem shows that the limit is Gaussian and provides a formula for the covariance matrix of the limit. This theorem is not directly applicable in our case since we will have variables ui which are the coordinates of the vector uniformly distributed on a sphere Sn and, therefore, not independent. This can be overcome either by a suitable modification of Theorem 7.1 in [\cite=bai_yao08], or by a trick that represents the uniformly distributed vector u as a normalization of a Gaussian vector (which is similar to what is done in the proof of Theorem 6.4 in [\cite=BenaychGeorges_Guionnet_Maida11]). In the following proof we concentrate on explaining how the variance coefficients are calculated.

Proof of Lemma [\ref=lemma_M_fluctuations]: By a suitable modification of Theorem 7.1 in [\cite=bai_yao08], we find that the matrix [formula] converges in distribution to a matrix that consists of independent zero-mean Gaussian entries. It remains to compute the variance of the entries. Let us start with the upper-left diagonal entry of [formula] which is [formula] Because of the rotational invariance of Z, one can take v uniformly distributed on the sphere Sr. In the basis that diagonalizes [formula] we get   where si  are singular values of Z and vi are coordinates of vector v. Hence,

[formula]

Next we use the total variance formula: The second term converges to zero by Lemma [\ref=speed_convergence], hence we need only to compute the limit of the first term. We calculate: where we used [formula] and [formula] Hence,

[formula]

In addition, Lemma [\ref=speed_convergence] shows that converges in distribution to the same variable as that is, to a zero-mean Gaussian variable with variance [formula]

Similar argument holds for [formula]: converges to a zero-mean Gaussian variable with variance

Next, [formula] where u and v are unit vectors, which are independent and uniformly distributed on Sp and Sr, respectively. In appropriate coordinates, The expectation of this term is zero and for the conditional variance we have the following sum:

[formula]

[formula]

If t > x2, then the random variable converges in distribution to a Gaussian random variable with the variance where κ1, κ2, and τ are as in Lemma [\ref=lemma_M_fluctuations].

Next, by Lemma [\ref=lemma_reduction_to_M], the largest singular value of Â satisfies the equation Let so that by Corollary [\ref=lemma_fluctuations_determinant] where W is a standard Gaussian random variable. If σ1 denote the largest root of [formula] then this implies that

[formula]

Note that our estimator is [formula] where Hence

[formula]

[formula]

Conclusion

This paper is about the reduced-rank regression in the multivariate response linear model. We found that if the number of responses and predictors is large relative to the number of observations, then the singular values of the OLS estimator of the coefficient matrix do not converge to zero even if the true coefficient matrix is zero. The same observation is true for the matrix of fitted responses. Instead, the empirical distributions of singular values are converging to some limit distributions that depend on how numerous the predictors and responses are relative to the number of observations.

In addition, under the null hypothesis A = 0 we found that the scaled largest singular value for these matrices are distributed in the limit according to the Tracy-Widom distribution. This fact can be used to test whether the true coefficient matrix is zero and to estimate the rank of the model. In numerical simulations, we found that one of these rank-selection algorithms compares favorably with the algorithm suggested by Bunea, She, and Wegkamp in [\cite=bunea_she_wegcamp11].

In the case of the low-rank A  ≠  0, we showed that the singular values of A are detectable if and only if they exceed a certain threshold. If they do, then the estimated coefficient matrix has singular values outside of the support of the limit empirical distribution.

Finally, we showed that consistent estimators of the true singular values can be obtained by shrinking the outlier singular values of Â or Ŷ appropriately. We found that the estimation based on singular values of Ŷ is preferable to the estimation based on that of Â. We have also proved a CLT for the asymptotic distribution of one of the estimators.

The limiting distribution of singular values for the coefficient matrix estimator

In this section we will prove the second part of Theorem [\ref=Theorem_main] by using the technique of S-transforms from free probability. (This technique was introduced in [\cite=voiculescu87] and generalized in [\cite=bercovici_voiculescu93] in order to study the spectra of products of infinite-dimensional operators.) Let the moments of a square N-by-N random matrix A be defined as [formula] and let the generating function for mk be denoted as The Stieltjes transform of the empirical eigenvalue distribution is and Voiculescu's S-transform is where [formula] is the functional inverse of MA.

If XN is an N-by-p random matrix with the standard Gaussian entries, and N / p  →  1 + λ, then it is known that as N  →    ∞  , the moments of matrices [formula] converge to the moments of the Marchenko-Pastur distribution with parameter 1 + λ. This distribution has the S-transform [formula]

Let ξ be a random variable with the Marchenko-Pastur distribution with parameter 1 + λ  ≥  1. Then, the S-transform of ξ- 1 is

Proof: Let [formula]. Then, a calculation of integrals gives the moments of ξ- 1: and therefore, Hence, and [formula]

Let A be an N-by-p matrix, B be an p-by-p matrix, [formula] and [formula] Then, the Stieltjes and S-transforms of matrices Z and [formula] are related as follows:

[formula]

Proof: The moments of matrices Z and [formula] are related as follows:

[formula]

The rest follows from the definitions of the Stieltjes and S- transforms. [formula]

Let [formula] As N  →    ∞  , the S-transform of [formula] converges to

Proof: By applying Lemma [\ref=lemma_shift] to matrices [formula] and [formula] we find that If λ > 1 then the moments of [formula] converge to the moments of the inverse Marchenko-Pastur distribution and therefore (by Lemma [\ref=lemma_inv_MP]) [formula] converges to λ - (1 + λ)u. Therefore, as N  →    ∞  ,

Next, the matrices [formula] converge in distribution to a multiple of the Marchenko-Pastur distribution with parameter μ, and one compute (for example, by Lemma [\ref=lemma_shift]) that the S transform of [formula] converges to [formula]

Next, we use the facts that XN and YN are asymptotically free, and that the S-transform of the product of free variables is the product of their S-transforms. Hence the S-transform of the matrix converges to φ1(u)φ2(u). Another application of Lemma [\ref=lemma_shift] shows that the S-transform of [formula] converges to [formula]

Now, by inverting[formula] we calculate [formula] and then the Stieltjes transform G(z) for the limit of the variables [formula]

[formula]

where [formula] From this we can extract the density function and the support of the limiting probability measure for [formula] The limit distribution for [formula] is obtained by scaling this distribution by β- 1. [formula]