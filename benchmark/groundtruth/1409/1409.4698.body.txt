A Mixtures-of-Experts Framework for Multi-Label Classification

1011

INTRODUCTION

Mutli-Label Classification (MLC) refers to a classification problem in which data instances are associated with multiple class variables that may reflect different views, functions or components describing the data. MLC naturally arises in many real world problems, such as text categorization [\citep=Kazawa:2005] [\citep=Zhang:2006] where a document may be associated with different topics reflecting its content; semantic scene and video classification [\citep=Boutell:2004:PR] [\citep=Qi:2007] where different images or videos are assigned to different categories or tagged based on their content; or in genomics where individual genes may be associated with multiple functions [\citep=Clare:2001:PKDD] [\citep=Zhang:2006].

Formally speaking, the MLC problem is specified by learning a function [formula] that maps each data instance, represented by a feature vector [formula], to class assignments, represented by a vector of d binary values [formula], such that yi  =  {0,1} indicates the absence or presence of the i-th class. However, the application of the model in practice raises tow important questions: how to define/represent such a function for high-dimensional feature and label spaces; and how to learn this function from data.

The problem of learning MLC classifiers from data has been studied extensively by the machine learning community in recent years. One of the key challenges in solving the problem is how to efficiently model and learn dependences among class variables given the fact that the number of possible class assignments is exponential in d. A simple solution to this is to assume that all class variables Yi for i  =  1,...,d are conditionally independent of each other and, hence, learn d functions to predict each class separately [\citep=Clare:2001:PKDD] [\citep=Boutell:2004:PR]. However, this may not suffice when dependences among labels exist. To overcome this limitation, more advanced machine learning methods that model class relations have been proposed. These include two-layer classification models [\citep=Godbole:2004:PAKDD] [\citep=Cheng:2009:ML], classifier chains [\citep=Read:2009:ECML] [\citep=Zhang:2010:KDD] [\citep=Dembczynski:2010:ICML], multi-dimensional Bayesian network classifiers [\citep=Gaag:2006:PGM] [\citep=Bielza:2011:IJAR] [\citep=Antonucci:2013:IJCAI] and output compression and coding methods [\citep=Hsu:2009:NIPS] [\citep=Tai:2010] [\citep=Zhang:2011:AISTATS] [\citep=Zhang:2012:ICML].

However, the above mentioned methods are still rather limited especially when the relations among features and labels become more complex. More specifically, if the relations tend to change across the dataset, these methods may fail to respond with correct classification, since they are designed to capture only one dependence structure from data. For example, in automated image tagging, an object can be tagged as {cat, pet} or {cat, wild animal} according to its context; Similarly, in medical informatics, patients who are suffering from the same disease may receive different sets of medications due to their medical history or allergic reactions. To address such issues, ensemble techniques have been recently adopted to MLC settings [\citep=Read:2009:ECML] [\citep=Dembczynski:2010:ICML] [\citep=Antonucci:2013:IJCAI]. However, these approaches are still limited in that they are relying on randomization for obtaining multiple dependence relations and using simple averaging to make ensemble predictions. As a result, the improvement we could obtain from them is not significant or consistent.

In this paper, we propose and study a new probabilistic MLC approach that attempts to remedy the limitations of the existing approaches. Our approach relies on the mixtures-of-experts (ME) framework [\citep=Jacobs:1991:AML] [\citep=Yuksel:2012:IEEE] and conditional tree-structured Bayesian network (CTBN) classifiers proposed recently in [\citep=batal:2013:CIKM]. Briefly, CTBN defines a multi-label classifier that is modeling [formula] where dependences among class variables for different inputs are modeled by a collection of classifiers (for example, logistic regression models) linked together in directed tree structures. The model comes with a number of computational advantages such as efficient learning, and efficient MAP inference that lets us find the best set of class assignments for a given data instance [formula].

A limitation of CTBN is that dependences among class variables are restricted to tree structures which may not reflect all existing dependences in the data. Our new framework based on the ME architecture aims to take advantage of the computational benefits of CTBNs and remedy its limitation by learning and combining multiple CTBNs, where each CTBN can cover a different region of the input space and/or can help to model the different dependences among class variables. We develop and present an EM algorithm for learning the new model from data and an algorithm for making the MAP inferences for predicting class assignments for future data instances.

The rest of the paper is organized as follows. Section [\ref=sec:problem_definition] formally defines the MLC problem. Section [\ref=sec:related_research] gives summary on related MLC research. Section [\ref=sec:preliminary] provides the necessary definitions for ME and CTBN, which are needed to understand our model. Section [\ref=sec:proposed] describes our proposed solution by going over its model representation (Section [\ref=subsec:representation]) and the supporting algorithms for parameter learning (Section [\ref=subsec:param_learn]), structure learning (Section [\ref=subsec:struct_learn]) and prediction (Section [\ref=subsec:predict]). Section [\ref=sec:experiments] presents the experimental results and evaluation. Section [\ref=sec:conclusion] concludes the paper.

PROBLEM DEFINITION

Multi-Label Classification (MLC) is a classification problem in which each data instance is associated with a subset of labels from a set of possible labels L. Let d  =  |L|. We can define d binary class variables Y1,...,Yd, where the value of Yi in instance [formula] indicates whether or not the i-th label in L is present in [formula]. We are given labeled training data [formula], where [formula] x(n)m) is the m-dimensional feature vector of the n-th instance (the input) and [formula] is its d-dimensional class vector (the output). We want to learn a function h that fits D and assigns to each instance, represented by its feature vector, a class vector:

[formula]

One way to approach this task is to model and learn the conditional joint distribution [formula], where [formula] is a random variable for the class vector and [formula] is a random variable for the feature vector. Assuming the 0-1 loss function, the optimal classifier h* assigns to each instance [formula] the maximum a posteriori (MAP) assignment of class variables:

[formula]

A key challenge for modeling and learning [formula] from data, as well as for defining the corresponding MAP classifier, is that the number of all possible class assignments for a given [formula] is exponential in d (there are 2d different assignments). Our goal is to develop a parametric model that allows us to efficiently model and learn [formula] from data.

Notation: For notational convenience, we will omit the index superscript (n) when it is not necessary. We may also abbreviate the expressions by omitting variable names; e.g., [formula].

RELATED RESEARCH

In this section, we review the related research in MLC and outline the main differences from our approach.

Earlier MLC methods ignore the relations between classes and learn to predict each class separately [\citep=Clare:2001:PKDD] [\citep=Boutell:2004:PR]. [\citet=Zhang:2007:PR] presented the multi-label k-nearest neighbor method, which predicts each class label by combining KNN with Bayesian inference. An approach that enriches the feature space by incorporating an intermediate layer of classifiers was proposed by [\citep=Godbole:2004:PAKDD] and later on by [\citep=Cheng:2009:ML]. The main drawback of these methods is that class dependences are either not represented at all, or represented indirectly in a limited way.

Several methods have been proposed to probabilistically model MLC. The classifier chains (CC) method [\citep=Read:2009:ECML] decomposes the relations among the class variables using the chain rule of probability:

[formula]

Each component in the chain is a classifier that is learned separately by incorporating the (0/1) predictions of preceding classifiers as additional features. [\citet=Zhang:2010:KDD] further studied the influence of the chain order and presented a method to learn an effective ordering from data. The main disadvantage of CC, however, is that they do not perform proper probabilistic inference for classification. Instead, they simply propagate the predictions according to the class order defined by the chain. To overcome this shortcoming, [\citet=Dembczynski:2010:ICML] presented the probabilistic classifier chains method by extending CC to estimate the entire class posterior distribution. However, this method needs to evaluate all possible 2d label configurations, which greatly limits its applicability.

Other probabilistic MLC methods are based on multi-dimensional Bayesian networks [\citep=Gaag:2006:PGM] [\citep=Bielza:2011:IJAR] [\citep=Antonucci:2013:IJCAI]. These methods attempt to build a generative model of [formula] using a restricted Bayesian network structure, which assumes all class variables are top nodes and all feature variables are their descendants. The limitation of this approach is that it must learn the structure of the full joint distribution over both features and classes, which can be very complex. Besides, it requires all features to be a priori discretized. In contrast, our approach directly learns the conditional distribution [formula] and takes advantage of modern discriminative classifiers.

An alternative approach for MLC is based on output coding. The idea is to project the output space [formula] into a lower dimensional space [formula], learn to predict [formula], and then reconstruct the original output from the noisy predictions. Methods that fall into this category use different dimensionality reduction techniques, such as compressed sensing [\citep=Hsu:2009:NIPS], principal component analysis [\citep=Tai:2010] and canonical correlation analysis [\citep=Zhang:2011:AISTATS]. The state-of-the-art in output coding utilizes a maximum margin formulation [\citep=Zhang:2012:ICML] that promotes both discriminative and predictable codes. The limitation of output coding methods is that they can only predict the single "best" output for a given input, and they cannot compute probabilities for different input-output pairs.

Several researchers proposed using ensemble methods for MLC. In general, the objective was to compensate for the restrictions the base MLC models introduce using a set of models and their combinations. [\cite=Read:2009:ECML] presented a simple method that averages the predictions of multiple randomly ordered CCs trained on random subsets of the data. [\cite=Antonucci:2013:IJCAI] proposed an ensemble of multi-dimensional Bayesian networks combined via simple averaging. In this work, we develop an approach based on the mixtures-of-experts architecture. The difference from the previous work is that our approach optimizes the structures and parameters of base classifiers in a principled way, and that our approach can help to overcome the restriction of the base MLC classifier by modeling better the relations among inputs and their labels, as well as, mutual relations among different labels.

PRELIMINARY

The MLC solution we propose in this work combines multiple base MLC classifiers using the mixtures-of-experts (ME) [\citep=Jacobs:1991:AML] architecture. The base classifiers we use are based on the conditional tree-structured Bayesian networks (CTBN) [\citep=batal:2013:CIKM]. To start with, we briefly review the basics of ME and CTBN.

ME is a mixture model that consists of a set of experts that are combined via gating (or switching) module to represent the conditional distribution [formula]. The model is defined by the following decomposition:

[formula]

where [formula] is the distribution of outputs defined by the k-th expert Ek and [formula] is the context sensitive prior of the k-th expert that is implemented by the gating function [formula]. In general, depending on the choice of the expert model, ME can be used for either regression or classification [\citep=Yuksel:2012:IEEE].

The ME model defines a soft-partitioning of the input space (via the gating module and its functions), on which the K experts represent different input-output relations. ME is especially useful when individual expert models are good in representing local input-output relations but fail to accurately capture the relations for the complete input space. The ability to switch among the experts in different regions of the input space allows to compensate for the limitation of individual experts and improve the overall model and its accuracy. In general, depending on the choice of the expert model, ME can be used for either regression or classification [\citep=Yuksel:2012:IEEE].

ME has been successfully adopted in a range of applications such as handwriting recognition [\citep=Ebrahimpour:2009:JDCTA], text classification [\citep=Estabrooks:2001], climate prediction [\citep=Lu:2006:PR] and bioinformatics [\citep=Qi:2007:BMC] [\citep=Cao:2010]. In addition, ME has been vigorously used in time series analysis, including speech recognition [\citep=Mossavat:2010], financial forecasting [\citep=Weigend:2000] and dynamic control systems [\citep=Jacobs:1993] [\citep=Weigend:1995]. Recently, ME has been used in social network analysis, in which different social behavior patterns are modeled through mixtures [\citep=Gormley:2011].

In this work, we apply the ME approach in context of MLC. We would like to note that although modeling of the joint conditional probability [formula] has been attempted in context of multiple target regression [\citep=Jordan:1995] [\citep=Waterhouse:thesis:1997], its application to MLC is new to the best of our knowledge.

In particular, we combine ME with CTBN to model individual experts. CTBN is a recently proposed probabilistic MLC method that has been shown to be competitive and efficient on a range of domains. CTBN defines [formula] using a collection of classifiers modeling relations in between features and individual labels that are tied together using a special Bayesian network structure that approximates the dependence relations among the class variables. In modeling of the dependences, it allows each class variable to have at most one other class variable as a parent (without creating a cycle) besides the feature vector [formula].

A CTBN T defines the joint distribution of class vector (y1,...,yd) conditioned on feature vector [formula] as:

[formula]

where π(i,T) denotes the parent class of class Yi in T (by convention, π(i,T) = {} if Yi does not have a parent class). For example, the conditional joint distribution of class assignment (y1,y2,y3,y4) given [formula] according to the network T in Figure [\ref=fig:CTBN-example] is defined as:

[formula]

PROPOSED SOLUTION

In this section, we develop a Multi-Label Mixtures-of-Experts (ML-ME) model, which uses the ME framework in combination with the CTBN classifiers to improve the classification accuracy of MLC tasks, and develop algorithms for its learning and predictions. Our key motivation is to exploit the divide and conquer principle, which states that a large, complex problem can be decomposed and effectively solved using simpler sub-problems. More specifically, we want to accurately model relations among inputs [formula] and class variables [formula] by learning multiple CTBN models and by improving their predictive ability by combining their outputs. In section [\ref=subsec:representation], we describe the mixture defined by the ML-ME model. In section [\ref=subsec:param_learn] through [\ref=subsec:predict], we present the learning and prediction algorithms for the ML-ME model.

REPRESENTATION

By following the definition of ME in Equation ([\ref=eq:def-me]), ML-ME defines the multivariate posterior distribution of class vector [formula] as:

[formula]

where [formula] is the joint conditional distribution defined by the k-th expert Ek and [formula] is the gating function reflecting how much the k-th expert should contribute to predict classes for input [formula]. In this work, we model the gating functions for the different experts with the help of the softmax function, which is also know as normalized exponential:

[formula]

where [formula] is a set of the softmax parameters.

To model the different experts and their probabilistic MLC predictions [formula] in Equation ([\ref=eq:def-ml-me]), we rely on the CTBN model introduced in the previous section. Employing multiple CTBN models Tk:k∈{1,...,K} as experts in the mixture, our ML-ME model defines the joint distribution of class vector [formula] as:

[formula]

Figure [\ref=fig:MLME-example] depicts an example ML-ME model, which consists of K CTBNs and whose output is probabilistically mixed with the help of the gating network.

Parameters Let [formula] denote the set of all the parameters of the ML-ME model, where [formula] are parameters of the the gating model, and [formula] are parameters of CTBNs defining individual experts. Since the gating function for each expert is defined by a linear combination of inputs, there are [formula] parameters per expert. Consequently, the total number of parameters needed to represent the full gating network is: [formula].

The parameterization of a CTBN expert is done by modeling the conditional probability distributions (CPDs) of class variable Yi conditioned on its parents. For each Yi, we learn two logistic regression classifier functions according to the parent class: [formula] and [formula]. Let [formula] denote the parameters for the k-th CTBN expert. Then, for [formula], since two different classifiers are defined on each class variable. Hence, [formula].

Table [\ref=table:notations] summarizes the notation and parameterization of our ML-ME model. In summary, the total number of parameters for our model is K(1 + 2d)(m + 1).

PARAMETER LEARNING FOR FIXED STRUCTURES

In this section, we describe how to learn the parameters of ML-ME, when the structures of individual CTBN experts are known and fixed. We return to the structure learning problem in Section [\ref=subsec:struct_learn].

[formula] denotes the set of all the parameters of the ML-ME model. Our objective is to find the parameters that optimize the log-likelihood of the training data:

[formula]

By substituting the joint probability with the definition of ML-ME (Equation ([\ref=eq:def-ml-me-ctbn])), we obtain:

[formula]

We refer to Equation ([\ref=eq:oll]) as the observed log-likelihood. However, optimizing this function is very difficult because there is a summation inside the log , which results in a non-convex function. To overcome this difficulty, we instead optimize the complete log-likelihood in the expectation-maximization (EM) framework.

The complete log-likelihood is defined by associating each instance [formula] with a hidden variable z(n)∈{1,...,K} indicating to which expert it belongs to:

[formula]

where [formula] is the indicator function that evaluates to one if the n-th instance belongs to the k-th expert and to zero otherwise.

The EM framework iteratively optimizes the expected complete log-likelihood [formula], which is always a lower bound of the observed log-likelihood [\citep=Dempster:1977]. In the E-step, the expectation is computed using the current set of parameters; in the M-step, the parameters of the model are relearned by maximizing the expected complete log-likelihood. In the following, we explain our parameter learning algorithm by deriving the E-step and M-step for ML-ME.

E-step In the E-step, we compute the expectation of the complete log-likelihood, which reduces to computing the expectation of the hidden variable.

[formula]

Notice that this becomes the posterior of the k-th expert given the observation and the current set of parameters. Let h(n)k denote [formula]. We can write h(n)k using Bayes rule as:

[formula]

M-step In the M-step, we learn the model parameters [formula] that maximize the expected complete log-likelihood. Let us first rewrite Equation ([\ref=eq:param_learn1]) using h(n)k and by switching the order of summations:

For the fixed h(n)k we can decompose Equation ([\ref=eq:mstep_1]) into two parts, each of which respectively involves the gating model parameters [formula] and the CTBN parameters [formula]:

[formula]

Notice that, in order to optimize the gate parameters [formula], we only need to optimize f1(D;ΘG); whereas to optimize the CTBN parameters [formula], we only need to optimize [formula]. We first show how to learn the parameters [formula] of the gating model. We can rewrite [formula] using Equation ([\ref=eq:def-gate]) as:

Since [formula] is concave in [formula], gradient methods are guaranteed to find its optimal solution. The derivative of the log-likelihood with respect to [formula] is calculated as:

[formula]

Note that the derivative becomes zero when [formula] and [formula] are equal.

We can solve the optimization of [formula] using any gradient method. However, in practice the dimensionality of the input space can be high which may result in model overfitting. To prevent the overfitting problem, we add L2-regularization penalty [formula] to the optimization function to penalize model complexity. In our experiments, we use the L-BFGS algorithm [\citep=Liu:1989] to optimize this regularized objective function. L-BFGS is a quasi-Newton optimization method that uses a sparse approximation to the inverse Hessian matrix to steer its search through parameter space. The algorithm is known to provide faster convergence rate and be well suited for optimization problems with a large number of variables.

To optimize the parameters of the CTBN experts [formula], we optimize [formula]. This optimization decomposes into optimization of parameters of individual CTBN Tk. Note that f2 is the weighted log-likelihood where h(n)k serves as the instance weight. Following [\citep=batal:2013:CIKM], we use the logistic regression model to represent [formula] in all our experiments. To prevent overfitting, we apply L2-regularized instance-weighted logistic regression to optimize f2. Algorithm [\ref=alg:learn-ML-ME-parameters] summarizes our parameter learning algorithm.

Complexity

E-step We compute h(n)k for each instance on every CTBN expert. This requires O(md) multiplications. Hence, the complexity of a single E-step is O(KNmd).

M-step Computing the derivative in Equation ([\ref=eq:derive_mstep1]) requires O(mN) multiplications, therefore optimizing [formula] requires O(mNl) operations, where l is the number of L-BFGS steps. To learn [formula], we optimize an instance-weighted logistic regression for every node of each of the K experts, which requires learning of O(Kd) logistic regression models.

STRUCTURE LEARNING

In the previous section, we described the parameter learning of ML-ME by assuming we have fixed the individual CTBN structures. In this section, we present how to automatically learn CTBN structures from data. In a nutshell, we apply a sequential boosting-like heuristic; i.e., on each iteration, we learn a structure that focuses on "hard instances" that previous CTBNs tend to misclassify. In the following, we first describe how to learn a single CTBN structure from instance-weighted data. After that, we describe how to re-weight the instances and incrementally add new structures to the ML-ME model.

Learning a Single CTBN Structure on Weighted Data

To learn the CTBN structure that best approximates weighted data, we find the structure that maximizes the weighted conditional log-likelihood (WCLL) on {D,Ω}, where [formula] is the data and Ω  =  {ω(n)}Nn = 1 is the instance weight. Note that we further split D into training data Dtr and hold-out data Dh.

Given a CTBN structure T, we train its parameters using Dtr, which corresponds to learning instance-weighted logistic regression using Dtr and the corresponding instance weights. On the other hand, we use WCLL of Dh to define the score that measures the quality of T.

Below we describe our algorithm for obtaining the CTBN structure that optimizes Equation ([\ref=eq:structure_score_1]) without having to evaluate all of the exponentially many possible tree structures.

Let us first define a weighted directed graph G  =  (V,E) as follows:

There is one vertex Vi for each class variable Yi:i∈{1,...,d}.

There is a directed edge Ej  →  i from each vertex Vj to each vertex Vi (i.e., G is complete). In addition, each vertex Vi has a self-loop Ei  →  i.

The weight of edge Ej  →  i, denoted as Wj  →  i, is the WCLL of class Yi conditioned on [formula] and Yj:

The weight of self-loop Ei  →  i, denoted as Wφ  →  i, is the WCLL of class Yi conditioned only on [formula].

Using the definition of edge weights (Equation ([\ref=eq:structure_score_2])) and by switching the order of the summations in Equation ([\ref=eq:structure_score_1]), we can rewrite the score of T simply as the sum of its edge weights:

[formula]

Now we have transformed the problem of finding the optimal tree structure into the problem of finding the tree in G that has the maximum sum of edge weights. The solution can be obtained by solving the maximum branching (arborescence) problem [\citep=Edmonds:1967], which finds the maximum weight tree in a weighted directed graph.

Learning Multiple CTBN Structures

In order to obtain multiple, effective CTBN structures for the ML-ME model, we apply the above described algorithm multiple times with different sets of instance weights. We assign the weights such that we give poorly predicted instances higher weights; and give well-predicted instances lower weights.

We start with assigning all instances uniform weights (i.e., all instances are equally important a priori).

[formula]

Using this initial set of weights, we find the initial CTBN structure T1 (and its parameters [formula]) and set the current model M to be T1. We then estimate the prediction error margin [formula] for each instance and renormalize such that [formula]. We repeat our structure learning algorithm with {ω(n)} and find another CTBN structure T2. After that, we set the current model to be the mixture of T1 and T2 and learn the ML-ME parameters [formula] according to Algorithm [\ref=alg:learn-ML-ME-parameters].

We incrementally add trees to the mixture by repeating this process. To stop the process, we use internal validation approach. Specifically, the data used for learning are split to internal train and test sets. The structure of the trees and parameters are always learned on the internal train set. The quality of the current mixture is evaluated on the internal test set. The mixture growth stops when the log-likelihood on the internal test set for the new mixture is worse than for the previous mixture. The trees included in the previous mixture are then fixed, and the parameters of the mixture are relearned on the full training data.

Complexity

To learn a single CTBN structure, we need to compute edge weights for the complete graph G, which requires estimating [formula] for all d2 pairs of classes. Finding the maximum branching in G can be obtained in O(d2) using [\citep=Tarjan:1977]. To learn K CTBN structures for the mixture, we repeat these steps K times. Therefore, the overall complexity is O(Kd2) times the complexity of learning logistic regression.

PREDICTION

In order to make a prediction for a new instance [formula], we want to find the MAP assignment of the class variables (see Equation ([\ref=OPT-classification])). In general, this requires to evaluate all possible assignments of values to d class variables, which is exponential in d.

One important advantage of the CTBN model is that the MAP inference can be done more efficiently by avoiding blind enumeration of all possible assignments. More specifically, the MAP inference on a CTBN is linear in the number of classes (O(d)) when it is implemented using a variant of the max-sum algorithm on a tree structure [\citep=batal:2013:CIKM]. However, our ML-ME model consists of multiple CTBNs and the MAP solution may, at the end, require enumeration of exponentially many class assignments. To address this problem, we rely on approximate MAP inference. The two commonly applied MAP approximation approaches in the literature are: convex programming relaxation via dual decomposition [\citep=Sontag:thesis:2010], and simulated annealing using a Markov chain [\citep=Yuan:2004:UAI]. In this work, we use the latter approach. Briefly, we search the space of all assignments by defining a Markov chain that is induced by local changes to individual class labels. The annealed version of the exploration procedure [\citep=Yuan:2004:UAI] is then used to speed up the search. We initialize our MAP algorithm using the following heuristic: first, we identify the MAP assignments for each CTBN in the mixture individually, and after that, we pick the best assignment from among these candidates. We have found this (efficient) heuristic to work very well and often results in the true MAP assignment.

EXPERIMENTS

DATA

We use ten publicly available MLC datasets obtained from different domains including music recognition, semantic image labeling, biology and text classification. Table [\ref=table:datasets] summarizes the characteristics of the datasets. It shows the number of instances (N), the number of feature variables (m) and the number of class variables (d). In addition, it shows two statistics: 1) label cardinality (LC), which is the average number of labels per instance and 2) distinct label sets (DLS), which is the number of distinct class configurations that appear in the data. Note that for RCV1 datasets we have used the 10 most common labels.

METHODS

We compare the performance of our proposed method, which we refer to as ML-ME, with the following MLC methods:

Binary Relevance (BR) [\citep=Boutell:2004:PR] [\citep=Clare:2001:PKDD]

Classification with Heterogeneous Features (CHF) [\citep=Godbole:2004:PAKDD]

Multi-Label k-Nearest Neighbor (MLKNN) [\citep=Zhang:2007:PR]

Instance-Based Learning by Logistic Regression (IBLR) [\citep=Cheng:2009:ML]

Classifier Chains (CC) [\citep=Read:2009:ECML]

Ensemble of Classifier Chains (ECC) [\citep=Read:2009:ECML]

Probabilistic Classifier Chains (PCC) [\citep=Dembczynski:2010:ICML]

Maximum Margin Output Coding (MMOC) [\citep=Zhang:2012:ICML]

Single CTBN (CTBN) [\citep=batal:2013:CIKM]

For all methods, we use the same parameter settings as suggested in the papers that introduced them: For MLKNN and IBLR, we use Euclidean distance to measure similarity of instances and we set the number of nearest neighbors to 10 [\citep=Zhang:2007:PR] [\citep=Cheng:2009:ML]; for CC, we set the order of classes to Y1   <   Y2,...  <   Yd [\citep=Read:2009:ECML]; for ECC, we use 10 CCs in the ensemble [\cite=Read:2009:ECML]; and for MMOC, we set λ (the decoding parameter) to 1 [\citep=Zhang:2012:ICML]. Also note that all of these methods except MMOC are considered as meta-learners because they can work with several base classifiers. To eliminate additional effects that may bias the results, we use L2-penalized logistic regression for all of these methods and choose their regularization parameters by cross validation. Lastly, for our ML-ME model, we set the number of mixture components to 5 and we use 150 iterations of simulated annealing for prediction.

EVALUATION MEASURES

Evaluating the performance of MLC methods is more challenging than that of traditional single-label classification methods. The most appropriate measure is the exact match accuracy (EMA), which computes the percentage of instances whose predicted output vectors are exactly the same as their true class vectors (i.e., all classes are predicted correctly). This measure is proper for MLC because it evaluates the success of the method in finding the mode of [formula] (see Section [\ref=sec:problem_definition]). However, EMA could be too harsh, especially when the output dimensionality is high. An alternative evaluation measure is the conditional log-likelihood loss (CLL-loss), which computes the negative conditional log-likelihood of the test instances:

[formula]

CLL-loss evaluates how much probability mass is given to the true label vectors (the higher the probability, the smaller the loss). Note that CLL-loss is only defined for probabilistic methods.

Micro F1 and macro F1 have been used for evaluating MLC methods [\citep=Tsoumakas:2009:PKDD]. Micro F1 aggregates the number of true positives, false positives and false negatives for all classes and then calculates the overall F1 score. On the other hand, macro F1 computes the F1 score for each class separately and then averages these scores. Note that both measures are not quite appropriate for MLC because they do not account for the correlations between classes [\citep=Dembczynski:2010:ICML]. However, we report them in our results as they have been used in other MLC literature.

RESULTS

Tables [\ref=table:EMA-results], [\ref=table:CLL-results], [\ref=table:microF1-results] and [\ref=table:macroF1-results] show the performance of all methods in terms of EMA, CLL-loss, micro F1 and macro F1, respectively. All results are obtained using ten-fold cross valiation. To evaluate the statistical significance in the performance measure differences, we use paired t-test at 0.05 significance level. We use markers [formula]/[formula] to indicate whether ML-ME is statistically superior/inferior to the compared method.

Note that we report the results of MMOC only on four datasets (emotions, image, scene and yeast) because it did not finish on the rest of the datasets (MMOC did not finish one round of the learning within 24 hours). Also, PCC did not finish on the enron dataset be causes it has to evaluate all 253 possible class assignments, which is clearly infeasible. Lastly, we do not report CLL-loss for for MMOC and ECC because they do not compute the probabilistic score for a giving class assignment.

In terms of EMA (Table [\ref=table:EMA-results]), ML-ME clearly outperforms the other methods on most datasets. For example, ML-ME is significantly better than BR, CHF, MLKNN, CC and ECC on all ten datasets, significantly better than IBLR on nine datasets, significantly better than PCC on six datasets and significantly better than MMOC on three datasets. In addition, ML-ME shows significant improvement over a single CTBN on six datasets, which demonstrates the ability of the mixture to compensate for the tree structure restriction of a single CTBN.

In terms of CLL-loss (Table [\ref=table:CLL-results]), ML-ME again shows consistent improvement over the other methods. This is because ML-ME is learned to optimize the log-likelihood of the data and its prediction is based on exact MAP inference. Also note that ML-ME shows a consistent improvement over CTBN because combining multiple CTBNs allows us to account for different relations in the data and, hence, improves the generalization of the model.

Lastly, Table [\ref=table:microF1-results] and [\ref=table:macroF1-results] show that ML-ME is also very competitive in terms of micro and macro F1 scores, although optimizing them was not our main objective.

In summary, the experimental results show that our ME-ML method with CTBN experts is able to outperform or match the existing state-of-the-art methods across a broad range of benchmark MLC datasets. We attribute this performance to the ability of CTBN mixtures to simultaneously compensate for the restricted dependences modeled by an individual CTBN, and for its ability to fit better the different regions of the input space with new expert models.

CONCLUSION

In this paper, we have proposed a new probabilistic approach for the multi-label classification problem. Our approach models different input-output relations using conditional tree-structured Bayesian networks, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and as a result achieves a more accurate model. We have formulated and developed the algorithms for learning the model from data and for performing multi-label predictions on future data instances. Our experiments on a broad range of datasets showed that our approach outperforms several state-of-the-art methods and produces more reliable probabilistic estimates.