=2500

Performance Analysis on Evolutionary Algorithms for the Minimum Label Spanning Tree Problem

Introduction

The minimum label spanning tree (MLST) problem is an issue arising from practice, which seeks a spanning tree with the minimum number of labels in a connected undirected graph with labeled edges. For example, we want to find a spanning tree that uses the minimum number of types of communication channels in a communication networks connected with different types of channels. The MLST problem, proposed by Chang and Leu, is proved to be NP-hard [\cite=Chang].

For this problem, Chang and Leu have proposed two heuristic algorithms. One is the edge replacement algorithm, ERA for short, the other is the maximum vertex covering algorithm, MVCA for short. Their experimental results showed that ERA is not stable, and MVCA is more efficient.

The genetic algorithm, belonging to the larger class of EAs, is a general purpose optimization algorithm [\cite=Holland] [\cite=Goldberg] [\cite=Herrera] with a strong globally searching capacity [\cite=Gallagher]. So, Xiong, Golden, and Wasil proposed a one-parameter genetic algorithm for the MLST problem. The experimental results on extensive instances generated randomly showed that the genetic algorithm outperforms MVCA [\cite=Xiong]. Nummela and Julstrom also proposed an efficient genetic algorithm for solving the MLST problem [\cite=Nummela].

Besides, many methods recently have been proposed for solving this NP-hard problem. Consoli et al. proposed a hybrid local search combining variable neighborhood search and simulated annealing [\cite=Consoli1]. Chwatal and Raidl presented exact methods including branch-and-cut and branch-and-cut-and-price [\cite=Chwatal]. Cerulli et al. utilized several metaheuristic methods for this problem, such as simulated annealing, reactive tabu search, the pilot method, and variable neighborhood search [\cite=Cerulli]. Consoli et al. still proposed a greedy randomized adaptive search procedure and a variable neighborhood search for solving the MLST problem [\cite=Consoli2].

Since both ERA and MVCA are two original heuristic algorithms for the MLST problem, the worst performance analysis of these two algorithms, especially MVCA, is a hot research topic in recent years. Krumke and Wirth proved that MVCA has a logarithmic performance guarantee of 2ln(n)  +  1, where n is the number of nodes in the input graph, and presented an instance to show that ERA might perform as badly as possible [\cite=Krumke]. Wan, Chen, and Xu further proved that MVCA has a better performance guarantee of ln(n - 1) + 1 [\cite=Wan]. Xiong, Golden, and Wasil proved another bound on the worst performance of MVCA for MLSTb problems, i.e., [formula], where the subscript b denotes that each label appears at most b times, and also called the maximum frequency of the labels [\cite=Xiong2].

The performance of MVCA on the MLST problem has been deeply investigated. However, there is still no theoretical analysis work on EAs' performance for the MLST problem.

In fact, the theoretical analysis of EAs' performance on fundamental optimization problems has received much attention from many researchers. During the past few years theoretical investigations about EAs focused on the runtime or(and) the probability of EAs for finding globally optimal solutions of fundamental optimization problems or their variants. These problems include plateaus of constant fitness [\cite=Jansen], linear function problems [\cite=He] [\cite=Droste] [\cite=He2], minimum cut problems [\cite=Neumann0], satisfiability problems [\cite=Zhou], minimum spanning tree problems [\cite=Neumann], Eulerian cycle problems [\cite=Neumann2], Euclidean traveling salesperson problems [\cite=Sutton], etc.

Nevertheless, since many fundamental optimization problems, including the MLST problem, are NP-hard, no polynomial-time algorithm can be expected to solve them unless P = NP. Fortunately, we usually only ask satisfying solutions to such NP-hard problems in practice. Thus, we are interested in whether an approximation solution with a given satisfying quality can be efficiently obtained. In fact, the approximation performance analysis of randomized heuristics, including EAs, on NP-hard problems receives many attentions.

Giel and Wegener proved that the (1+1) EA can find a (1 + ε)-approximation solution in expected runtime [formula], and concluded that EAs are good approximation algorithms for the maximum matching problem [\cite=Giel].

Subsequently, Oliveto, He, and Yao found that for minimum vertex cover problems the (1+1) EA may find arbitrary bad approximation solutions on some instances, but can efficiently find the minimum cover of them by using a restart strategy [\cite=Oliveto]. Friedrich et al. proved that the (1+1) EA may find almost arbitrarily bad approximation solution for minimum vertex cover problems and minimum set cover problems as well [\cite=Friedrich]. Witt proved that in the worst case the (1+1) EA and the randomized local search algorithm need an expected runtime O(n2) to produce a [formula]-approximation solution to the partition problem [\cite=Witt].

On the approximation performance of multi-objective EAs, Friedrich et al. revealed that the multi-objective EA efficiently finds an (ln(n))-approximation solution to the minimum set cover problem. Neumann and Reichel found that multi-objective EAs can find a k-approximation solution for the minimum multicuts problem in expected polynomial time [\cite=Neumann3]. Recently, Yu, Yao, and Zhou studied the approximation performance of SEIP, a simple evolutionary algorithm with isolated population, on set cover problems. They found that SEIP can efficiently obtain an Hn-approximation solution for unbounded set cover problems, and an [formula]-approximation solution for k-set cover problems as well [\cite=Yu2].

In this paper, we concentrate on the performance analysis of the (1+1) EA and GSEMO for the MLST problem. We analyze the approximation performances of the (1+1) EA and GSEMO on the MLST problem. For the MLSTb problem, We prove that the (1+1) EA and GSEMO are [formula]-approximation algorithms. We also reveal that GSEMO can efficiently achieve a (2ln(n))-approximation ratio for the MLST problem. Though the MLST problem is NP-hard, we show that on three instances the (1+1) EA and GSEMO efficiently finds the global optima, while local search algorithms may be trapped in local optima. Meanwhile, we construct an additional instance where GSEMO outperforms the (1+1) EA.

The rest of this paper is organised as follows. The next section describes the MLST problem, and the algorithms considered in this paper. Section analyzes the approximation performances of the (1+1) EA and GSEMO on the MLST problem, while section analyzes the performances of the (1+1) EA and GSEMO on four instances. Finally, the section presents the conclusions.

The MLST problem and algorithms

First of all, we give the concept of spanning subgraph.

Let G = (V,E) and H = (V',E') be two graphs, where V and V' are, respectively, the sets of nodes of G and G', E and E' are, respectively, the sets of edges of G and G', if V' = V and E'  ⊂  E, then H is a spanning subgraph of G.

Let G = (V,E,L) be a connected undirected graph, where V, E, and [formula] are the set of nodes, the set of edges, and the set of labels, respectively, |V| = n, |E| = m, and clearly |L| = k, each edge associates with a label by a function l:E  →  N. Thus, each edge e∈E has an unique label l(e)∈L. The MLST problem is to seek a spanning tree with the minimum number of labels in the input graph G. If the maximum frequency of the labels is b, then we denote such an MLST problem by MLSTb. Clearly, the MLSTb problem is a special case of the MLST problem.

Our goal in this paper is to seek a connected spanning subgraph with the minimum number of labels rather than a spanning tree with the minimum number of labels, since any spanning tree contained in such a spanning subgraph is a MLST. This is an alternative formulation of the MLST problem which is also adopted in papers [\cite=Xiong] [\cite=Nummela].

We encode a solution as a bit string [formula] which is used in [\cite=Xiong], where bit xi(1  ≤  i  ≤  k) corresponds to label i. If [formula], then label i is selected, otherwise it is not. Thus, a bit string X represents a label subset, and |X| represents the number of labels contained in X.

We consider the spanning subgraph H(X) of G, where H(X) is a spanning subgraph restricted to edges with labels that the corresponding bits in X are set to 1. We call a solution X such that H(X) is a connected spanning subgraph a feasible solution. A feasible solution with the minimum number of labels is a globally optimal solution.

For solving the MLST problem, the (1+1) EA uses a fitness function, which is defined as

[formula]

where c(H(X)) is the number of connected components in H(X), k is the total number of labels in L, and [formula], i.e, the number of labels conained in X and also used in H(X).

The fitness function should be minimized. The first part of it is to make sure that H(X) is a connected spanning subgraph, and the second part is to make sure that the number of labels in the connected spanning subgraph is minimized.

For a feasible solution X, since the number of connected components of H(X) is 1, the fitness value equals to the number of labels contained in it.

We also define the fitness vector for GSEMO as a vector (c(H(X),|X|), where c(H(X)) and |X| are simultaneously minimized by GSEMO.

The following algorithms are those considered in this paper.

Algorithm 1: The (1+1) EA for the MLST problem 01: Begin 02: Initialize a solution X∈{0,1}k uniformly at random; 03: While termination criterion is not fulfilled 04: Obtain an offspring Y by flipping each bit in X with probability [formula]; 05: If fit(Y)  <  fit(X) then X: = Y; 06: End while 07: End

The (1+1) EA starts with an arbitrary solution, and repeatedly uses mutation operator to generate an offspring solution from the current one. If the offspring solution is strictly better than the current one, then the (1+1) EA uses it to replace the current solution.

Another algorithm proposed by Brüggemanna, Monnot, and Woeginger is called the local search algorithm with the 2-switch neighborhood. We now describe some concepts about it.

[\cite=Bruggemann] Given an integer h  ≥  1, let X1 and X2 be two feasible solutions for some instance of the MLST problem. We say that X2 is in h-switch neighborhood of X1, denoted by X2∈h-SWITCH(X1), if and only if

[formula]

In other words, X2∈h-SWITCH(X1) means that X2 can be derived from X1 by first removing at most h labels from X1 and then adding at most h labels to it.

The local search algorithm with the 2-switch neighborhood:

In the algorithm 1, if the initial solution X is an arbitrary feasible solution, and the offspring Y is selected from the 2-switch neighborhood of X, then it is the local search algorithm with the 2-switch neighborhood [\cite=Bruggemann].

GSEMO has been investigated on covering problems [\cite=Friedrich], pseudo-Boolean functions [\cite=Giel2] [\cite=Laumanns], and minimum spanning tree problems [\cite=Neumann4] [\cite=Neumann5]. It is described as follows.

Algorithm 2: GSEMO for the MLST problem 01: Begin 02: Initialize a solution X∈{0,1}k uniformly at random; 03: P←{X}; 04: While termination criterion is not fulfilled 05: Choose a solution X from P uniformly at random; 06: Obtain an offspring Y by flipping each bit in X with probability [formula]; 07: If Y is not dominated by [formula] then 08: Q: = {X|X∈P, and Y dominates X }; 09: [formula]; 10: End if 11: End while 12: End

In algorithm 2, P is a population used to preserve those solutions which can not be dominated by any other from the population. The concept of domination is defined as follows.

Suppose the fitness vectors of solutions X and Y are (c(H(X)),|X|) and (c(H(Y)),|Y|), respectively. We say that X dominates Y, if one of the following two conditions is satisfied:

(1) c(H(X)) < c(H(Y)) and |X|  ≤  |Y|; (2) c(H(X))  ≤  c(H(Y)) and |X| < |Y|.

For the sake of completeness, another two greedy algorithms are included.

The first one is the modified MVCA. It starts with a solution containing no labels, and each time selects a label such that when this label is chosen the decrease in the number of connected components is the largest.

Algorithm 3: The modified MVCA for the MLST problem [\cite=Xiong2] Input: A given connected undirected graph G = (V,E,L), [formula]. 01: Let C be the set of used labels, [formula]; 02: Repeat 03: Let H be the spanning subgraph of G restricted to edges with labels from C; 04: For all [formula] do 05: Determine the number of connected components when inserting all edges labeled by i in H; 06: End for 07: Choose label i with the smallest resulting number of connected components: [formula]; 08: Until H is connected. Output: H

In algorithm 3, if we contract each connected component in H to a supernode after step 3, then we obtain the second greedy algorithm which is investigated in [\cite=Krumke], and we call it the modified MVCA with contraction in this paper.

Approximation performances of the (1+1) EA and GSEMO on the MLST problem

The following is the concept of approximation ratio (solution). Given a minimization problem [formula] and an algorithm A, if for an instance [formula] of [formula], the value of the best solution obtained in polynomial time by A is A(), and [formula], where OPT() is the value of the optimal solution of [formula], then we say that A achieves an r-approximation ratio (solution) for [formula]. Although the MLST problem is NP-hard, we reveal that the (1+1) EA and GSEMO guarantee to achieve an approximation ratio for the MLSTb problem in expected polynomial times of n and k, and that GSEMO guarantees to obtain an approximation ratio for the MLST problem in expected polynomial time of n and k.

The approximation guarantees of the (1+1) EA and GSEMO on the MLSTb problem

To reveal that the (1+1) EA and GSEMO guarantee to achieve a [formula]-approximation ratio in expected polynomial time of n the number of nodes and k the number of labels, we first prove that the (1+1) EA and GSEMO find a feasible solution starting from any initial solution in expected polynomial time of n and k, then prove that starting from any feasible solution the (1+1) EA and GSEMO find a [formula]-approximation solution in expected polynomial time of k by simulating the following result proved by Brüggemann, Monnot, and Woeginger [\cite=Bruggemann].

If b  ≥  2, then for any instance of the MLSTb problem, the local search algorithm with the 2-switch neighborhood can find a local optimum with at most [formula] labels, where OPT is the number of labels in the global optimum.

We partition all feasible solutions into two disjoint sets. One is S1  =  {X|X∈{0,1}k, X is a feasible solution, [formula], the other is S2  =  {X|X∈{0,1}k, X is a feasible solution, [formula].

From theorem [\ref=thrmBruggemann], we derive a property with respect to the 2-switch neighborhood for MLSTb problems.

If b  ≥  2, let G = (V,E,L) be an instance of MLSTb, which has a minimum label spanning tree with OPT labels. If X is a feasible solution, and X∈S2, then there must exist a feasible solution X'∈2-SWITCH(X) whose fitness is 1 or 2 less than that of X.

We now prove that starting with an arbitrary initial solution for an instance G = (V,E,L) of MLSTb, the (1+1) EA can efficiently find a feasible solution.

Given an instance G = (V,E,L) of MLSTb, where |V| = n, and |L| = k, the (1+1) EA starting from an arbitrary initial solution finds a feasible solution in O(nk) for G.

Then, we prove that starting with an arbitrary feasible solution on an instance G = (V,E,L) of MLSTb, the (1+1) EA can efficiently find a [formula]-approximation solution.

Given an instance G = (V,E,L) of MLSTb, where |V| = n, |L| = k, and b  ≥  2, the expected time for the (1+1) EA starting from an arbitrary feasible solution to find a local optimum with at most [formula] labels for G is O(k4).

Combining Lemma [\ref=EAConnectedSpaningSubgraph] and [\ref=EAFindApprSlut], we obtain the following Theorem.

Given an instance G = (V,E,L) of MLSTb, where |V| = n, |L| = k, and b  ≥  2, the (1+1) EA starting with any initial solution finds a [formula]-approximation solution in expected time O((n + k3)k).

It has been proved that the (1+1) EA efficiently achieves a [formula]-approximation ratio for the MLSTb problem. As we will see below, GSEMO can also efficiently achieve this approximation ratio.

Given an instance G = (V,E,L) of MLSTb, where |V| = n, |L| = k, and b  ≥  2, GSEMO starting with any initial solution finds a [formula]-approximation solution in expected time O(nk2 + k5).

The approximation guarantee of GSEMO on the MLST problem

Here we prove the approximation guarantee of GSEMO on the MLST problem by simulating the process of the modified MVCA with contraction. Similar to Lemma 2 in [\cite=Krumke], we prove the following Lemma.

Given a connected undirected graph G = (V,E,L) having a minimum label spanning tree TOPT with OPT labels, where |V| = n and n  ≥  2, there exists a label such that the number of connected components of the spanning subgraph restricted to edges with this label is not more than [formula].

Further, for a spanning subgraph H(X) of G = (V,E,L), we have the following Corollary.

If r the number of connected components of H(X) is greater than 2, then there is a label such that when it is added to X the number of connected components will be reduced to not more than [formula].

Based on Corollary [\ref=KrumkeCol], we prove that the GSEMO guarantees to find a (2ln(n))-approximate solution in expected polynomial time of n and k.

Given an instance G = (V,E,L) of MLST problems, where |V| = n and |L| = k, the expected time that GSEMO starts with any initial solution to find a (2ln(n))-approximation solution for G is O(k3ln(n) + k2ln(k)).

Table [\ref=CompareApprxmt] summarizes the approximation performances of the (1+1) EA and GSEMO for the minimum label spanning tree problem. For the MSLTb problem, the (1+1) EA and GSEMO can efficiently achieve a [formula]-approximation ratio. However, the order of the expected time of GSEMO is higher than that of the (1+1) EA, the reason is that GSEMO has to select a promising solution to mutate in a population of size O(k). For the MLST problem, GSEMO efficiently achieves a 2ln(n)-approximation ratio, but the approximation performance of the (1+1) EA is unknown.

Performances of the (1+1) EA and GSEMO on four instances

In this section, we firstly present an instance where GSEMO outperforms the (1+1) EA, then we show that the (1+1) EA and GSEMO outperform local search algorithms on three instances of the MLST problem.

An instance where GSEMO outperforms the (1+1) EA

At first, we construct an instance G' = {V,E,L} to show that GSEMO is superior to the (1+1) EA, where [formula].

Given [formula] and k the number of labels, we construct instance G' by the following steps. For simplicity, we assume that μk is an integer, thus (1 - μ)k is an integer as well. First, we construct (1 - μ)k subgraphs G'1, [formula], G'(1 - μ)k. G'i (1  ≤  i  ≤  (1 - μ)k) contains a (μk - 1)-sided regular polygon whose edges are all labeled by the same label μk + i and an inner node in the center. From the inner node, (μk - 1) edges labeled by from 1 to μk - 1 connect to the μk - 1 outer nodes v1, v2, [formula], vμk - 1. Then three edges are connected from G'i (1  ≤  i  ≤  (1 - μ)k - 1) to G'i + 1: the first one labeled by μk + i is from the inner node of G'i to outer node v1 of G'i + 1, the second one labeled by μk + i is from outer node v1 of G'i to outer node v1 of G'i + 1, the third one labeled by μk is from the the inner node of G'i to the inner node of G'i + 1. Finally, an additional edge labeled by k connects the inner node of G'(1 - μ)k with outer node v1 of G'1. Figure [\ref=figEA] shows instance G'.

For 0 < μ < 1 / 2, the global optimum of G' is [formula], and the local optimum is [formula] for local search algorithms, such as the local search algorithm with the 2-switch neighborhood, since both spanning subgraphs H(X*) and H(Xl) are connected, but |X*| = μk, |Xl| = (1 - μ)k, and μk  <  (1 - μ)k. For instance G', the expected time for the (1+1) EA to jump out of the local optimum is exponential.

For instance G', starting from the local optimum Xl, the expected time for the (1+1) EA to find the global optimum is Ω(kμk).

Though the (1+1) EA needs expected exponential time to jump out of the local optimum, GSEMO can efficiently find the global optimum for instance G'.

For instance G', GSEMO finds the global optimum in expected time O(k2ln(k)).

An instance where the (1+1) EA and GSEMO outperform ERA

ERA is a local search algorithm. It takes an arbitrary spanning tree as input, then considers each non-tree edge and tests whether the number of used labels can be reduced by adding this non-tree edge and deleting a tree edge on the induced cycle.

In this subsection, we show that the (1+1) EA and GSEMO outperform ERA on an instance proposed by Krumke and Wirth, which is denoted by G1 in this paper.

This instance can be constructed by two steps. First, we construct a star shaped graph with n - 1 distinct labels, i.e., selecting one node out of n nodes, and adding n - 1 edges from it to the other n - 1 nodes labeled by n - 1 distinct labels: 1, 2, [formula], n - 1. Second, by adding edges between each pair of nodes with the same label k. Thus, we get a complete graph G1 = (V,E,L), where |V| = n, |E| = n(n - 1) / 2, and [formula] is the set labels. It is clear that |L| = k, and k = n. Figure [\ref=instance1] shows an example with n = 5, where the dashed edges construct the spanning tree with the minimum number of labels.

For instance G1, a global optimum X* uses one a label from [formula] and label k, i.e., |X*| = 2, [formula], and xk = 1.

Krumke and Wirth used instance G1 to demonstrate that ERA might perform as badly as possible. In fact, [formula] is a local optimum for ERA, since the number of labels used in H(Xl) can not be reduced by adding any non-tree edge and deleting a tree edge on the induced cycle. The local optimum uses k - 1 labels, while the global optimum uses only 2 labels. However, the (1+1) EA and GSEMO can efficiently find a global optimum for G1.

For instance G1, the (1+1) EA finds a global optimum in expected time O(kln(k)).

For instance G1, GSEMO finds a global optimum in expected time O(k2ln(k)).

An instance where the (1+1) EA and GSEMO outperform the local search algorithm with the 2-switch neighborhood

Brüggemann, Monnot, and Woeginger proposed an instance, denoted by G2 in this paper, to show that there exists a local optimum with respect to the local search algorithm with the 2-switch neighborhood [\cite=Bruggemann].

As shown in Figure [\ref=instance2], this instance is a graph G2 = (V,E,L), where V = ( v0, x0, x1, [formula], xk - 4, y0, y1, [formula], yk - 4), L = (1, 2, [formula], k), |V| = 2k - 5, |E| = 4k - 12,|L| = k. Figure [\ref=instance2OPT] shows the minimum label spanning tree.

In this instance, the global optimum is [formula], 1, 1).

The local search algorithm with the 2-switch neighborhood might be trapped in the local optimum which contains labels 1, 2, [formula], k - 2. In fact, to jump out of this local optimum, at least three labels from [formula] should be removed and simultaneously two labels k - 1 and k should be added, but the resulting solution is not in the the 2-switch neighborhood of the local optimum. However, the (1+1) EA and GSEMO can efficiently find the global optimum of G2.

For instance G2, the (1+1) EA finds the global optimum in expected time O(k2).

For instance G2, the expected time for GSEMO to find the global optimum is O(k2ln(k)).

An instance where the (1+1) EA and GSEMO outperform the modified MVCA

In this subsection, we show that the (1+1) EA and GSEMO outperform the modified MVCA on an instance proposed by Xiong, Golden, and Wasil [\cite=Xiong2], which is denoted by G3 in this paper.

Given the bound of the labels' frequency b(b  ≥  2), and let n = b  ·  b! + 1, we construct G3 = (V,E,L) as follows, where [formula], |V| = n, and [formula].

We construct b! groups from V, each containing b + 1 nodes:

[formula], [formula], [formula] [formula], [formula] [formula].

In Vj [formula], the edges between consecutive nodes ((j - 1)b + 1,(j - 1)b + 2), [formula], (jb,jb + 1) are all labeled with one label. Thus, b! labels are needed, which constitute the label set Lopt. The edges with these b! labels construct the minimum label spanning tree Topt, so in this instance OPT = b!.

The label subset Lh [formula] is obtained as follows. We choose edge ((j - 1)b + 1,(j - 1)b + 1 + h) in each Vj, so there are b! such edges. We label the first h edges with one label, and the next h edges with a second label, etc. So, [formula] labels are needed, and they construct Lh. Hence, [formula], and the total number of labels [formula].

Figure [\ref=instance3] shows an example with b = 4, where the dashed edges construct the spanning tree with the minimum number of labels.

In this instance, the global optimum is [formula].

Xiong, Golden, and Wasil used this instance to show that the modified MVCA may obtain the worst-case solution using all labels from [formula], which is Hb-approximation solution, where [formula]. Here, we show that the (1+1) EA and GSEMO can efficiently find the global optimum.

For instance G3, the (1+1) EA finds the global optimum in expected time O(nk), where n = b  ·  b! + 1, [formula], and b is the maximum frequency of the labels.

GSEMO can also efficiently find the global optimum for G3.

For instance G3, GSEMO finds the global optimum in expected time O(k3).

Table [\ref=Compare] summarizes the upper bounds on the expected times for the (1+1) EA and GSEMO to find the global optima on all four instances. On instances G1, G2, and G3, GSEMO needs times of higher order than the (1+1) EA. The main reason is that GSEMO selects a promising solution to mutate in a population of size O(k). On G', GSEMO outperforms the (1+1) EA because GSEMO behaves greedily and optimizes solutions with different number of labels.

Conclusion

In this paper, we investigated the performances of the (1+1) EA and GSEMO on the minimum label spanning tree problem. We found that the (1+1) EA and GSEMO can guarantee to achieve some approximation ratios. We further show that the (1+1) EA and GSEMO defeat local search algorithms on some instances, and that GSEMO outperforms the (1+1) EA on an instance.

As for the approximation ratio of the (1+1) EA on the MLST problem, we still know nothing about. Apart from this, since the (1+1) EA and GSEMO are randomized algorithms, it is natural to ask whether they can achieve better approximate ratios than those guaranteed by some greedy algorithms. From our analysis process, especially from the analysis process of GSEMO, it seems possible.

Acknowledgement

This work was supported in part by the National Natural Science Foundation of China (61170081, 61165003, 61300044, 61332002), in part by the EPSRC under Grant EP/I009809/1, in part by the National High-Technology Research and Development Program (863 Program) of China No. 2013AA01A212, and in part by the NSFC for Distinguished Young Scholars 61125205.