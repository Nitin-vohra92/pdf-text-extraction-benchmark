Network calculus for parallel processing

Introduction

Multi-stage parallel data processing systems are ubiquitous in cloud computing environments. In a single parallel processing stage, a job is partitioned into tasks ( i.e.,  the job is "forked" or the tasks are demultiplexed); the tasks are then worked upon in parallel. Within parallel processing systems, there are often processing "barriers" (points of synchronization or "joins") wherein all component tasks of a job need to be completed before the next stage of processing of the job can commence. The terminus of the entire parallel processing system is typically a barrier. Thus, the latency of a stage (between barriers or between the exogenous job arrival point to the first barrier) is the greatest latency among the processing paths through it ( i.e.,  among the tasks comprising that stage). Google's multi-stage MapReduce  [\cite=DBLP:journals/cacm/DeanG08] (especially its open-source implementation Apache Hadoop [\cite=White:2009:HDG:1717298]) is a very popular such framework. However, numerous other systems exhibit similar programming patterns and our work is relevant to them as well [\cite=DBLP:conf/osdi/YuIFBEGC08] [\cite=Reiss12] [\cite=google-dataflow].

In MapReduce, jobs arrive and are partitioned into tasks. Each task is then assigned to a mapper for initial processing. The results of mappers are transmitted (shuffled) to reducers. Reducers combine the mapper results they have received and perform additional processing (a final stage after the reducers may simply combine their results). The workloads of the reducer tasks may be unrelated to those of their "tributary" mapper tasks. A barrier exists before each reducer (after its mapper-shuffler stage) and after all the reducers (after the reducer stage).

To achieve good interleaving of the principal resources consumed by the mapper (CPU/memory) and the shuffler (network bandwidth), these stages are made to work in a pipelined manner wherein the shuffler transmits partial results created by the mapper (as they are generated) rather than waiting for a mapper to entirely finish its task. Of course, the shuffler must "follow" the mapper at all times in the sense of being able to send only what results the mapper has generated so far  [\cite=DBLP:journals/pe/LinZWT13]. On the other hand, the barrier between the shuffler stage and the reducer stage is a strict one - a reducer may not begin any processing until all of the shuffler stage's work is done.

Our goal is to develop a performance model for such applications. As a first step toward this, we consider a single parallel processing stage. Our approach can be extended to create a model with separate queues for the mapper, the shuffler, and the reducer stages (or even more stages), but we restrict our attention to the interaction between the shuffler and the reducer stages. Specifically, each processor/server (and associated job queue) in our model represents a mapper stage.

In this paper, we focus on the Mapper stage, where an initial job scheduling would be in play to achieve a "bounded burstiness" of the aggregate workload. We prove two claims using "network calculus" for parallel processing systems. We then numerically evaluate the "generalized" (strong) stochastic burstiness bound (gSBB) of a publicly disseminated workload trace of a Facebook data-center.

Related Work

There is substantial prior work on fork-join queueing systems particularly involving underlying Markov chains, e.g.,  [\cite=NT88]. Our primary result is significantly simpler and somewhat more general than those based on Markovian models of workload. Of course, there is also an enormous literature on parallel processing systems in general. Typically, parallel processing systems employ robust load balancing to minimize synchronization delays at the barriers. To this end, load balancing could proactively estimate throughputs along the parallel processing paths and proportionately size the workloads from tasks fed to them. As an example of the many proposed reactive/dynamic mechanisms, "straggler" (deemed excessively delayed) tasks at barriers can be restarted or the entire job can be interrupted and restarted or additional can be allocated ( e.g.,  more parallelism). See [\cite=JianTan14] [\cite=Ead14] [\cite=DBLP:conf/osdi/AnanthanarayananKGSLSH10] [\cite=DBLP:conf/nsdi/AnanthanarayananGSS13] for recent discussions on the online management specifically for a MapReduce parallel processing system. Some recent work on MapReduce systems [\cite=Lucent12] [\cite=DBLP:journals/pe/LinZWT13] focuses on the the pipelining between the mapper and shuffler, the latter formulating a proactive scheduling problem that jointly considers individual job workloads of both mapper and shuffler (assuming the shuffler load is known a priori).

Single-stage, fork-join system

Consider single-stage fork-join (parallel processing) system, modeled as a bank of K parallel queues, with queue-k provisioned with service/processing capacity sk. Let A be the cumulative input process of work that is divided among queues so that the kth queue has arrivals ak and departures dk in such a way that [formula],

[formula]

Define the virtual delay processes for hypothetical departures from queue k at time t  ≥  0 as

[formula]

where we define inverses a- 1k of non-decreasing functions ak as continuous from the left so that [formula].

In following definition of the cumulative departures, D, the output is determined by the most lagging (straggling) queue/processor: [formula],

[formula]

Note that in the case of continuous, fluid arrivals ( e.g.,  piecewise linear A), this definition of departures D corresponds to periods of continual, possibly perpetual, barriers (synchronization times). In the case of discrete arrivals (piecewise constant A with jump discontinuities at arrival instances), then the barriers are discrete.

A queue q with service s has a (non-negative, non-decreasing) at least a service-curve smin ( i.e.,  s  ≫  smin) if for all cumulative arrivals a and all time t its cumulative departures

[formula]

Define the convolution (*  ) identity as

[formula]

and

[formula]

where:

the delay operator [formula],

ak  ≪  bin,k, i.e.,  ak conforms to the non-negative, non-decreasing burstiness curve (traffic envelope) bin,k,

[formula]

dmax ,k is the largest horizontal difference between bin,k and smin ,k [\cite=Cruz98].

If the kth queue has at least a service curve of smin ,k and arrivals ak  ≪  bin,k, then for all t  ≥  0,

[formula]

Remark: This claim simply states that the maximum delay of the whole system (from A to D) is the maximum delay among the queues. Equivalently, the service curve from A to D is at least Δdu∞, where d: =  max kdmax ,k.

By Equation ([\ref=dmax]), [formula] and [formula],

[formula]

Thus, [formula] and [formula],

[formula]

where we have used the fact that, [formula], ak are nondecreasing. Thus,

[formula]

where we have used the fact that A is nondecreasing.

We now consider a stationary stochastic model of this single-stage system. To simplify matters, we assume the workload process A has strong ("generalized") stochastically bounded burstiness (gSBB) [\cite=gSBB09], and leave to future work generalizations to non-stationary settings assuming only (weak) stochastically bounded burstiness [\cite=Staro99] or bounded log moment-generating function [\cite=Chang94].

In the stationary regime at time t  ≥  0, if

service to queue k, sk  ≫  smin ,k where

[formula]

[formula], [formula] small εk > 0 such that [formula]

[formula]

where [formula];

the total arrivals have strong stochastically bounded burstiness [\cite=gSBB09],

[formula]

where Φ decreases in x > 0;

then [formula],

[formula]

Remark: By A2, the mapper divides arriving work roughly proportional to minimum allocated service resources μk to queue k, i.e.,  strong load balancing.

[formula]

where we have used the fact that A and the ak are nondecreasing (cumulative arrivals) and the inequality is by A1. Also, we have defined non-negative random variables z and xk such that [formula]. So by using A2 then A3, we get

[formula]

Acknowledgements

This work was supported in part by a Cisco Systems URP gift and NSF CAREER grant 0953541.