A One-Line Proof of the Fundamental Theorem of Algebra with Newton's Method as a Consequence

Introduction

We assume the reader has familiarity with a complex number z = x + iy, [formula], its conjugate [formula], its modulus [formula], and Euler's formula, eiθ  =   cos θ  +  i sin θ.

To prove the fundamental theorem of algebra (FTA), that a nonconstant complex polynomial p(z) must have a zero, many proofs rely on the fact that the minimum of the modulus function

[formula]

over the complex plane is attained at a point z*. This fact can be shown by first observing that |p(z)| approaches infinity as |z| does. Thus the set S  =  {z:|p(z)|  ≤  |p(0)|} is bounded. Then, by continuity of p(z), S is also closed. Hence, the minimum of F(z) over S is attained and coincides with its minimum over the entire complex plane. If p(z*) is nonzero, it suffices to exhibit a direction of descent for F(z) at z*, i.e. a complex number d such that for some positive real number α* we have,

[formula]

This would then contradict optimality of z*. For proofs of the FTA based on a descent direction, see [\cite=Fin], [\cite=BK2011], [\cite=Kon], [\cite=Lit], [\cite=Rio]. In fact, [\cite=BK2011] gives a complete characterization of all descent and ascent directions.

Given a nonconstant complex polynomial p(z), for any z0 that is not a zero of p(z), we explicitly define a direction that is a decent direction of the modulus function. The use of this specific direction not only gives a proof of the FTA but justifies the definition of Newton's iteration for complex polynomials. It also allows defining the iterate when z0 is a critical point (i.e. p'(z0) = 0).

Newton's method for root finding is traditionally studied for real polynomials and is the best known such method. Given a real polynomial p(x), and a seed x0, Newton's iterations are defined as

[formula]

where p'(x) is the derivative of p(x). The iterate xj + 1 is undefined when p'(xj) = 0. The iterate xj + 1 can be interpreted as the root of the tangent line to p(x) at xj. Graphically, this property is taken as justification that when x0 is close to a root of p(x) the iterates converge to θ. The latter can be proved analytically. In particular, |p(xj)| converges to zero. In general Newton iterates do not necessarily decrease |p(x)| monotonically, i.e. it is possible to have |p(xj + 1)|  >  |p(xj)|.

Cayley [\cite=cayley79] is among the first to have considered Newton's iterations for a complex polynomial p(z). Given p(z) and a complex seed z0, Newton's iterates are defined as in ([\ref=eq3]), replacing xj with zj. In this case too Newton's iteration zj + 1 is the solution to the linear equation, p(zj) + p'(zj)(z - zj) = 0. However, in the next section we will give another interpretation and derivation of the method in terms of modulus minimization.

A Direction of Descent At Any Point, Critical or Not

Let u be a nonzero complex number. Given a natural number k, set

[formula]

Define the real numbers γ and δ as

[formula]

Then for any α > 0 and any real θ, we have

[formula]

In particular, given α > 0, we can select θ∈{0,π / 2k,π / k,3π / 2k}, so that Gk(αeiθu) < 0. Specifically,

[formula]

Equation ([\ref=maineq]) is easily verifiable from straightforward properties of conjugation, exponentiation and Euler's formula. Since [formula], it is easy to show γ and δ cannot both be zero. Thus from ([\ref=eq5]) we can choose θ so that for all α > 0, Gk(αeiθu) = cαk, c a negative constant.

Equation ([\ref=maineq]) together with appropriate selection of u and θ hold the essence of our proof of the FTA as they give rise to a descent direction for the modulus function.

Let p(z) be a polynomial of degree n  ≥  1. Let [formula]. Suppose [formula]. Let k  ≥  1 be the smallest index with [formula]. Let

[formula]

Then for any α > 0 and any real θ, we have

[formula]

where Gk(αeiθu) is as in ([\ref=maineq]), and q(α) is a polynomial of degree 2n having αl as a factor, with l  >  k. In particular, by selecting θ appropriately, eiθu is a descent direction for F(z) at z0.

We derive ([\ref=eq1]) by setting z = z0  +  αeiθu in the Taylor's expansion formula

[formula]

More specifically, for [formula], denote p(j)(z0) / j! by aj. Thus from ([\ref=udef]), [formula]. We have

[formula]

We can write the above as a real polynomial in α of the form [formula], where

[formula]

From ([\ref=eq0]) and ([\ref=eqaa]) it follows that bkαk coincides with Gk(αeiθu). In each of the remaining terms bjαj, j  >  k. Hence their sum gives a real polynomial q(α) of degree 2n claimed in ([\ref=eq1]). From Proposition [\ref=prop1] we can choose θ so that for all α > 0, Gk(αeiθu) < 0. Since q(α) goes to zero faster than αk does, for this θ there exists α* > 0 so that for all α∈(0,α*) the right-hand-side of ([\ref=eq1]) is negative.

As a consequence of Theorem [\ref=thm1] we have the following which justifies Newton's method for complex polynomials.

If [formula], - p(z0) / p'(z0), is a descent direction for F(z) at z0.

Since k = 1, ([\ref=udef]) gives [formula]. Equivalently, u = |p'(z0)|2p(z0)  /  p'(z0). Then ([\ref=eq0]) gives γ = 1, δ = 0. Then from Proposition [\ref=prop1] and Theorem [\ref=thm1], eiπu =  - u is a descent direction.

When u is a nonzero real number, ([\ref=eq0]) implies δ = 0 so that there is only one descent direction. However, when λ and δ are both nonzero two directions of descent are implies by ([\ref=eq5]).

Theorem [\ref=thm1] also allows defining Newton's iteration at a critical points z0. Using u as defined in the theorem, the Newton iterate can be defined as

[formula]

where θ is selected as in Proposition 1, ([\ref=eq5]), and k is as in Theorem [\ref=thm1]. As an example suppose p(z) = z2 - c, c a positive constant. Then p'(0) = 0. Thus from ([\ref=udef]) [formula]. Then from ([\ref=eq0]) γ =  - 2c < 0, δ = 0. Thus θ = 0, giving z1 = c.