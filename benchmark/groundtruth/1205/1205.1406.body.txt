=10000 =10000

Introduction

We study the prediction problem where the observation is a sequence of graphs adjacency matrices (At)0  ≤  t  ≤  T and the goal is to predict AT + 1. This type of problem arises in applications such as recommender systems where, given information on purchases made by some users, one would like to predict future purchases. In this context, users and products can be modeled as nodes of a bipartite graph, while purchases or clicks are modeled as edges.

In functional genomics and systems biology, estimating regulatory networks in gene expression can be performed by modeling the data as graphs and fitting predictive models is a natural way for estimating evolving networks in these contexts.

A large variety of methods for link prediction only consider predicting from a single static snapshot of the graph - this includes heuristics [\cite=liben2007link] [\cite=sarkar2010theoretical], matrix factorization [\cite=koren2008factorization] or probabilistic methods [\cite=taskar2003link]. More recently, some works have investigated using sequences of observations of the graph to improve the prediction, such as using regression on features extracted from the graphs [\cite=Richard10], using matrix factorization [\cite=koren2010collaborative], or in some special cases probabilistic techniques. Most techniques, however, do not explicitly take into account the inherently sparse nature of usual sequences of adjacency matrices. In this work, we extend the work of [\cite=Richard10] to address this and propose in addition a more principled way of predicting using features extracted from the sequence of graph snapshots.

We make the following assumptions about the graph sequence (represented by adjacency matrices At):

Low-rank. At has low rank. This reflects the presence of highly connected groups of nodes such as communities in social networks.

Autoregressive linear features. We assume given a linear map [formula] defined by a set of n  ×  n matrices (Ωi)1  ≤  i  ≤  d

[formula]

such that the vector time series ω(At) has an autoregressive evolution:

[formula]

where [formula] is a sparse matrix such that [formula] is stationary. An example of linear features is degrees that is a popularity measure in social and commerce networks.

Formulation of an optimization problem

In order to reflect the stationarity assumption on ω(At) we use a convex loss function

[formula]

to penalize the dissimilarity between two feature vectors at successive time steps. Let us introduce

[formula]

and

[formula]

We also use [formula] to design the elementwise extension of [formula] to [formula]s. In case of quadratic loss, we consider the following [formula] penalized regression objective :

To predict AT + 1, we propose a regression term penalized by the sum of [formula] and trace norm in the same fashion as in [\cite=Richard12] in order to predict the future graph AT + 1 given the prediction of its features [formula] should approximate ω(S) well:

The overall objective function we consider here is the sum of the two partial objectives J1 and J2, which is convex as J1 and J2 are both convex.

[formula]

Let us introduce the linear map Φ defined by

[formula]

The objective can be written as a penalized least squared regression on the joint variable (S,W):

[formula]

Oracle inequality

Define [formula], i.e.,

[formula]

and

[formula]

We define [formula], where Ωi are defined in ([\ref=eq:defOm]) and let

[formula]

We defined M and Ξ such that they verify

[formula]

The following result can be proved using the tools introduced in [\cite=Koltchinskii11].

Let (Ŝ,Ŵ) be the minimizers of L(S,W) over a convex cone S  ×  W. Suppose that

for some μ  >  0, and for any S1,S2∈S and W1,W2∈W,

[formula]

[formula], [formula], [formula] for any real number α∈(0;1);

then

[formula]

The latter inequality shows how the quality of the solution is bounded by the rank and sparsity of the future graph AT + 1, and the interplay between these two prior through the parameter α. The dependence in T quantifies the improvement of the estimation in terms of the number of observations.

Algorithms

Generalized forward-backward algorithm for minimizing L

We use the algorithm designed in [\cite=raguet2011generalized] for minimizing our objective function. Note that this algorithm outperforms the method introduced in [\cite=Richard10] as it directly minimizes L jointly in (S,W) whereas the previous method first estimates Ŵ by minimizing a functional similar to J1 and then minimizes L(S,Ŵ).

In addition to this we use the novel joint penalty from [\cite=Richard12] that is more suited for estimating graphs. The proximal operator for the trace norm is given by the shrinkage operation, if [formula] is the singular value decomposition of Z,

[formula]

Similarly, the proximal operator for the [formula]-norm is the soft thresholding operator defined by using the entry-wise product of matrices denoted by [formula]:

[formula]

The algorithm converges under very mild conditions when the step size θ is smaller than [formula], where L is the operator norm of Φ.

Non-convex Factorization Method

An alternative method to the estimation of low-rank and sparse matrices by penalizing a mixed penalty of the form [formula] as in [\cite=Richard12] is to factorize [formula] where [formula] are sparse matrices, and penalize [formula]. The objective function to be minimized is

[formula]

which is a non-convex function of the joint variable (U,V,W), making the theoretical analysis more difficult. Given that the objective is convex in a neighborhood of the solution, by initializing the variables adequately, we can write an algorithm inspired by proximal gradient descent for minimizing it.

Numerical Experiments

A generative model for graphs having linearly autoregressive features

Let [formula] be a sparse matrix, [formula] its pseudo-inverse such, that [formula]. Fix two sparse matrices [formula] and [formula] . Now define the sequence of matrices (At)t  ≥  0 for [formula] by

[formula]

and

[formula]

for i.i.d sparse noise matrices Nt and Mt, which means that for any pair of indices (i,j), with high probability (Nt)i,j = 0 and (Mt)i,j = 0.

If we define the linear feature map [formula], note that

The sequence [formula] follows the linear autoregressive relation

[formula]

For any time index t, the matrix At is close to UtV0 that has rank at most r

At is sparse, and furthermore Ut is sparse

Results

We tested the presented methods on synthetic data generated as in section ([\ref=sec:gen]). In our experiments the noise matrices Mt and Nt where built by soft-thresholding iid noise N(0,σ2), n = 50,T = 10,r = 5,d = 10,σ  =  .5. After choosing the parameters τ,γ,r by 10-fold cross-validation, we compare our methods to standard baselines in link prediction [\cite=liben2007link]. We use the area under the ROC curve as the measure of performance and report empirical results averaged over 10 runs. Nearest Neighbors (NN) relies on the number of common friends between each pair of nodes, which is given by A2 when A is the cumulative graph adjacency matrix [formula] and we denote by Shrink the low-rank approximation of [formula]. Since V0 is unknown we consider the feature map ω(S)  =  SV where [formula] is the SVD of [formula].

Discussion

The experiments suggest the empirical superiority of the proposed approaches to the standard baselines. It is very intriguing that the non-convex matrix factorization outperforms the convex rival. A possible explanation is that minimizing the nuclear norm by using the shrinkage operator results in factorizations of the solution by two orthogonal matrices, which conflicts with the sparsity of the solution. The other benefit of the non-convex formulation is its scalability, as the proximal method proposed for the convex formulation scales in O(n2) in storage and O(n3) in time. Several questions open perspectives for further investigations.

Choice of the feature map ω. In the current work we used the projection onto the vector space of the top-r singular vectors of the cumulative adjacency matrix as the linear map omega, and this choice has shown empirical superiority to other choices. The question of choosing the best measurement to summarize graph information as in compress sensing seems to have both theoretical and application potential.

Characterization of sparse and low-rank matrices. Can all the sparse and low-rank matrices S be written as [formula] where [formula] are both sparse? Or in other terms, what is the relation between the solution of problems penalized by [formula] -such as J- and those, e.g. L, penalized by [formula] ?

Preliminary Tools

Let [formula] be a rank r matrix. We can write the SVD of S in two ways: [formula] or [formula], where [formula] are orthogonal and [formula]. Let [formula] and [formula] matrices of size n  ×  (n - r) ortho-normally completing the bases of U and V, and define the projections onto the vector spaces spanned by vectors ui and vi for [formula]:

[formula]

[formula]

and define the orthogonal projection

[formula]

We highlight the fact that PS(B) can also be written as

[formula]

or

[formula]

We know that [formula] and [formula]. The two following inequalities will also be useful:

For any matrix B,

[formula]

[formula]

For any matrix B, with the same notations, we have

[formula]

and the 4 terms are pairwise orthogonal. It follows that

We have the identity

[formula]

It follows that [formula]

Proof

We have for any (S,W)∈S  ×  W, by optimality of (Ŝ,Ŵ):

[formula]

Thanks to trace-duality and [formula]-duality we have [formula] and [formula] for any X, so for any α∈[0;1],

[formula]

now using assumptions [formula], [formula], and [formula] and then triangular inequality

[formula]

For proving the other bound, we start by setting some notations. Let S∈S, and let [formula], [formula], [formula]. Let [formula] be the SVD of S and let [formula], [formula] where ΘS∈{0,  ±  1}n  ×  n, ΘW∈{0,  ±  1}d  ×  d are sign matrices of S, and W such that [formula], [formula] and [formula] is the entry-wise product. Let

[formula]

denote an element of the subgradient of the convex function [formula], so [formula] and [formula]. There exist G1 and G* such that

[formula]

We use the two standard inequalities of convex function subdifferentials 〈∂L(Ŝ,Ŵ),(Ŝ  -  S,Ŵ - W)〉  ≤  0 and 〈Ŝ  -  S,Ẑ  -  Z〉  ≥  0 and a similar inequality on subdifferentials on Ŵ and W of [formula], denoted by [formula] and Q. We get

[formula]

Therefore we obtain

[formula]

The inequality ([\ref=eq:tsybakov1]) can be written as

[formula]

Thanks to Cauchy-Schwarz

[formula]

similarly

[formula]

and

[formula]

so we have

[formula]

We need to bound 〈(δ,ε),Φ(Ŝ  -  S,Ŵ  -  W)〉. For this, note that by definition,

[formula]

and decompose for any α∈[0,1]

[formula]

We get by applying triangle inequality, Cauchy-Schwarz, Hölder inequality written for the trace-norm and [formula]-norm

[formula]

by rank and support inequalities obtained again by Cauchy-Schwarz

[formula]

Now by using

[formula]

we can rewrite the inequality ([\ref=eq:Tsybakov2]) as follows:

[formula]

the last inequality being due to the assumptions [formula], [formula] and [formula].

So finally, and by using again these assumptions,

[formula]

and [formula] gives

[formula]

and the result follows by using

[formula]

and setting (S,W)  =  (AT + 1,W0):

[formula]