{ {

Algorithmic randomness and stochastic selection function

Introduction

Von Mises [\cite=mises] seemed to try to construct a probability theory that depends on given sample sequence but does not assume probability model a priori. In other words von Mises tried to construct a probability theory from a statistical point of view. In order to achieve this program, he introduced the notion of collective (random numbers) and demand that its subsequences (selected with place-selection rule) have the same frequency of symbols of the original sequence. Since then many authors studied the properties of subsequences of random numbers, e.g., Wald (1937), Church [\cite=church], Ville (1939). In this paper we show algorithmic randomness versions of the two classical theorems on subsequences of normal numbers. One is Kamae-Weiss (KW) theorem on normal numbers [\cite=kamae73], which characterize the selection function that preserves normal numbers. Another one is the Steinhaus theorem on normal numbers [\cite=steinhaus22], which characterize the normality from their subsequences. In van Lambalgen [\cite=lambalgen87], an algorithmic analogy to KW theorem is conjectured in terms of algorithmic randomness and complexity [\cite=chaitin75] [\cite=Kol65] [\cite=martin-lof66] [\cite=solomonoff64]. In this paper we consider two types of algorithmic random sequence; one is Martin-Löf (ML)-random sequences and the other one is the set of sequences that have maximal complexity rate. Then we show algorithmic randomness versions of corresponding theorems to the above classical results.

Let Ω be the set of infinite binary sequences. For x,y∈Ω, let [formula] and [formula]. Let [formula] be a strictly increasing function such that [formula], where [formula] is the set of natural numbers. If [formula] then τ(j) is defined for 1  ≤  j  ≤  n. For x,y∈Ω let x / y be the subsequence of x selected at yi = 1, i.e., [formula]. For example, if [formula] then τ(1) = 2,τ(2) = 4 and [formula]. For finite binary strings [formula] and [formula], xn1 / yn1 is defined similarly. Let S be the set of finite binary strings and |s| be the length of s∈S. For s∈S let Δ(s): = {sω|ω∈Ω}, where sω is the concatenation of s and ω. Let (Ω,B,P) be a probability space, where B is the sigma-algebra generated by Δ(s),s∈S. We write P(s): = P(Δ(s)).

A probability P on Ω is called computable if there is a computable function A such that [formula]. For A  ⊂  S, let [formula]. A recursively enumerable (r.e.) set [formula] is called (ML) test with respect to P if 1) U is r.e., 2) Ũn + 1  ⊂  Ũn for all n, where Un  =  {s:(n,s)∈U}, and 3) P(Ũn) < 2- n. A test U is called universal if for any other test V, there is a constant c such that [formula]. In [\cite=martin-lof66], it is shown that a universal test U exists if P is computable and the set [formula] is called the set of ML-random sequences with respect to P.

Next, we introduce another notion of randomness. We say that y has maximal complexity rate with respect to P if

[formula]

i.e., both sides exist and are equal. For example, y has maximal complexity rate with respect to the uniform measure (i.e., P(s) = 2- |s| for all s) if lim n  →    ∞K(yn1) / n = 1. If y is ML-random sequences with respect to a computable ergodic P then from upcrossing inequality for the Shannon-McMillan-Breiman theorem [\cite=hochman2009], the right-hand-side of ([\ref=weak]) exists (see also [\cite=vyugin98]) and from Levin-Schnorr theorem (see ([\ref=ls]) below), we see that ([\ref=weak]) holds i.e., y has maximal complexity rate w.r.t. P.

Algorithmic version of Kamae-Weiss theorem

In Kamae [\cite=kamae73], it is shown that the following two statements are equivalent under the assumption that [formula]:

(i) h(y) = 0. (ii) [formula], where h(y) is Kamae entropy [\cite=brudno83] [\cite=lambalgen87] and N is the set of binary normal numbers.

A probability p on Ω is called cluster point if there is a sequence {ni}

[formula]

From the definition, the cluster points are stationary measures. Let V(x) be the set of cluster points defined from x. From a diagonal argument we see that [formula] for all x. Kamae entropy is defined by

[formula]

where h(p) is the measure theoretic entropy of p. If h(x) = 0, it is called completely deterministic, see [\cite=kamae73] [\cite=weiss71] [\cite=weiss00]. The part (i)⇒   (ii) is appeared in [\cite=weiss71].

As a natural analogy, the following equivalence (algorithmic randomness version of Kamae's theorem) under a suitable restriction on y is conjectured in van Lambalgen [\cite=lambalgen87],

(i) lim n  →    ∞K(yn1) / n = 0. (ii) [formula]

where K is the prefix Kolmogorov complexity and R is the set of ML-random sequences with respect to the uniform measure (fair coin flipping), see [\cite=LV2008]. Note that lim n  →    ∞K(yn1) / n = h,P - a.s., for ergodic P and its entropy h, see [\cite=brudno83]. We show two algorithmic analogies to KW theorem (the following results are appeared in Takahashi [\cite=takahashi2011] however we reproduce them for convenience). The first one is a ML-randomness analogy and the second one is a complexity rate analogy to KW theorem, respectively.

Our first algorithmic analogy to the KW theorem is the following.

Suppose that y is ML-random with respect to some computable probability P and [formula]. Then the following two statements are equivalent: (i) y is computable. (ii) [formula], where Ry is the set of ML-random sequences with respect to the uniform measure relative to y.

Proof) (i)⇒   (ii). Since [formula] we have [formula], where λ is the uniform measure. Let U be a universal test with respect to λ and y(s)  ⊂  S be a finite set such that [formula]. Then y(s) is computable from y and s, and hence Uy: = {(n,a)|a∈y(s),s∈Un} is a test if y is computable. We have x∈Ũyn  ↔  x / y∈Ũn. (Intuitively Uy is a universal test on subsequences selected by y). Then

[formula]

Since y is computable, Ry  =  R and we have (ii).

Conversely, suppose that y is a ML-random sequence with respect to a computable P and is not computable. From Levin-Schnorr theorem, we have

[formula]

where Km is the monotone complexity. Throughout the paper, the base of logarithm is 2. By applying arithmetic coding to P, there is a sequence z such that z is computable from y and [formula] for all n, where u is a monotone function and we write [formula] if s is a prefix of s'. Since y is not computable, we have lim nln  =    ∞  . From ([\ref=ls]), we see that [formula]. We show that if y∈RP then sup nln + 1 - ln  <    ∞  . Observe that if y∈RP then [formula] and

[formula]

Let Un: = {s|P(s|s|s| - 11) < 2- n}. Then P(Ũn) < 2- n and U: = {(n,s)|s∈Un} is a r.e. set. Since y∈ lim sup nŨn  ↔   lim inf nP(yn + 1|yn1) = 0, we see that if y∈RP then sup nln + 1 - ln  <    ∞  . (If U is r.e. and P(Ũn) < 2- n then RP  ⊂  ( lim sup nŨn)c, see [\cite=shen89].) Since [formula] and sup nln + 1 - ln  <    ∞  , we have [formula] and z∈R. Since z is computable from y we have z / y∉Ry.

Note that if y is computable then y is a ML-random sequence with respect to a computable measure that has positive probability at y.

Suppose that y has maximal complexity rate with respect to a computable probability and [formula]. Then the following two statements are equivalent: (i) [formula] (ii) [formula]

Proof) (i) ⇒   (ii) Let [formula] such that i = 1 if yi = 0 and i = 0 else for all i. Since

[formula]

[formula]

if lim n  →    ∞K(yn1) / n = 0 and [formula] then we have

[formula]

where [formula]. Similarly, if lim n  →    ∞K(yn1) / n = 0 and [formula] then we have [formula]. (ii) ⇒   (i) Suppose that

[formula]

for a computable P. Let ln be the least integer greater than -   log P(yn1). Then by considering arithmetic coding, there is [formula] and a monotone function u such that [formula]. By considering optimal code for zln1 we have Km(yn1)  ≤  Km(zln1) + O(1). From ([\ref=wls]), we have lim nKm(yn1) / ln  =   lim nKm(zln1) / ln = 1. For ln  ≤  t  ≤  ln + 1, we have Km(zln1) / ln + 1  ≤  Km(zt1) / t  ≤  Km(zln + 11) / ln. From ([\ref=wls]), we have lim nln + 1 / ln = 1, and hence lim nKm(zn1) / n =  lim nK(zn1) / n = 1.

Since 1) zln1 is computable from yn1, 2) lim nln / n > 0 by ([\ref=wls]), and 3) [formula], we have [formula].

Champernowne sequence satisfies the condition of the proposition and (i) holds, however its Kamae-entropy is not zero.

If y is a Sturmian sequence generated by an irrational rotation model with a computable parameter [\cite=kamaeTakahashi] [\cite=takahashiAndaihara] then y satisfies the condition of the proposition and (i) holds.

Algorithmic version of Steinhaus theorem

In Steinhaus [\cite=steinhaus22], it is shown that

x is normal number iff for all [formula], where uq is the binary i.i.d. process with parameter q, i.e., uq(1) = q,uq(0) = 1 - q.

We have an algorithmic analogies for this result.

Let q∈[0,1]. The following two statements are equivalent: (i) x∈Ruq,q (ii) [formula], where Ruq,q is the set of ML-random sequences w.r.t. uq relative to q.

Proof) By considering the ML-test on the subseqences selected by y, (i)⇒   (ii) follows. Conversely if x / y∈Ruq,q then x / y satisfies the law of large numbers, for example, see [\cite=LV2008]. Thus q is uniquely determined (in fact computable) from x / y and we have (i).

Let w be a computable probability such that (a) [formula], (b) [formula] exists for y∈Rw, and (c) [formula]. Then the following two statements are equivalent. (i) [formula]. (ii) [formula] for y∈Rw.

Proof) (i) ⇒   (ii) follows from the part of (i)⇒   (ii) of Proposition 2. (ii)⇒   (i): Observe that where [formula] is defined in the proof of Proposition [\ref=main]. From the condition, we have [formula]. Then [formula] and [formula], where the latter equality follows from the condition (a). From (ii), ([\ref=stein-A]), ([\ref=stein-B]), and ([\ref=stein-C]), we have [formula]. Since we can choose ε > 0 arbitrary, we have (i). For example, [formula], where Pρ is a probability derived from irrational rotation with parameter ρ, satisfies the condition of Prop. 2, see [\cite=takahashiAndaihara].

Discussion

Both proofs of Proposition 1 and 2 have similar structure, i.e., the part (i) →   (ii) are straightforward and in order to show the converse, we construct random sequences (in the sense of Proposition 1 and 2, respectively) by compression.

We may say that Proposition 1 is a Martin-Löf randomness analogy and Proposition 2 is a complexity rate analogy to KW theorem, respectively. These results neither prove nor disprove the conjecture of van Lambalgen. However Martin-Löf randomness and complexity rate randomness give different classes of randomness, and a strange point of the conjecture is that it is described in terms of different notions of randomness.

As stated above we proved our propositions by constructing random sequences. In [\cite=dowe2011] pp.962, a different direction is studied, i.e., a sequence that is not predicted by MML with respect to finite order Markov processes is considered. Such a sequence is called red herring sequence [\cite=dowe2011] and considered to be a non-random sequence with respect to MML and finite order Markov processes, in the sense that MML cannot find a finite order Markov model for that sequence.

Although Proposition [\ref=model-selection] is simple, it is considered to be a model selection problem from partially observed sequence, i.e., identify the model among i.i.d. model class from its subsequence. We can trivially extend the model class to the set of ergodic processes in Proposition [\ref=model-selection], e.g., choose the selection function y such that [formula]. We need further development of this problem.

Acknowledgement

The author thanks Prof. Teturo Kamae (Matsuyama Univ.) for discussions and comments.