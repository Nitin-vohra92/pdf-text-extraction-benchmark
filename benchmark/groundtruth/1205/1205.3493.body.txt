=1

Extending BEAMS to incorporate correlated systematic uncertainties

Introduction

Type Ia supernovae (SN Ia) are standardisable candles, making them one of the most reliable distance measures and a cornerstone of cosmology ever since the discovery of the late time accelerated expansion of the Universe [\cite=riess] [\cite=perlmutter]. Future surveys which will produce large amounts of photometric data, such as the Dark Energy Survey (DES) [\cite=des], Pan-STARRS [\cite=pan] and the Large Synoptic Survey Telescope (LSST) [\cite=lsst], will increase the number of SN Ia candidates by orders of magnitude. While a foolproof method of identifying a Type Ia is to analyse its observed spectrum, taking spectra is expensive and, for surveys such as those mentioned above, it will be unfeasible to perform spectroscopic follow-up for all candidates, introducing a possible bias due to contamination from Type Ib/c and Type II supernovae, which we collectively denote non-Ia supernovae (SN nIa) [\cite=hlozek] [\cite=falck]. However, using the photometric information gathered by the survey, one can fit template light curve models to the data using a template fitter such as MLCS2k2 [\cite=mlcs], SALT2 [\cite=salt] or a model fitter such as in [\cite=sako], which gives a probability for each object to be a Type Ia. In previous surveys, the relative SN Ia/nIa probabilities were used only to determine candidates for spectroscopic follow-up [\cite=sdss2]. Without spectroscopic follow-up, applying a probability cut (for example, taking all supernovae with probability greater than 0.9 to be a Type Ia) will introduce a bias in the cosmological parameters [\cite=hlozek]. To avoid such biases, one can either demand a very high purity, which excludes much of the data [\cite=gjergo] [\cite=bernstein] [\cite=campbell] or use all the data within a statistical framework that accounts for the contamination. One such method, developed by Kunz, Bassett & Hlozek [\cite=kunz] is Bayesian Estimation Applied to Multiple Species (BEAMS). BEAMS has recently been applied to the full three years of data from the SDSS-II supernova survey [\cite=sdss1] [\cite=sdss2], which reduced the Ωm  -  ΩΛ contours by a factor of three relative to the spectroscopic data alone [\cite=hlozek]. Despite this success, the current implementation of BEAMS assumes the supernovae are not correlated with each other; an approach which will not be appropriate for future surveys. In general, systematic uncertainties are correlated and, as the number of discovered supernovae increases, we are entering an era where systematic uncertainties are comparable to statistical uncertainties. This means the off-diagonal terms of the covariance matrix cannot be ignored. To analytically account for correlations between supernovae in the BEAMS posterior requires summing over mN terms, where N is the number of supernova candidates and m is the number of possible supernova types, which here we take to be two, corresponding to Ia's and nIa's. Clearly, this is computationally impossible, but in this paper we will show that if the form of the covariance matrix is known (for example, if the known sources of correlations can be parameterised), BEAMS can still be used to estimate cosmological parameters in an unbiased way, using a numerical marginalisation over supernova type. After a brief review of the theory of BEAMS in section [\ref=beams_sec], we move on to discuss possible sources of correlations in section [\ref=correlations_sec]. We introduce mock supernova datasets described by three different covariance matrices in section [\ref=cov_mat_sec]. In section [\ref=n_beams] we discuss a solution to the 'exponential catastrophe' of the correlated form of the BEAMS posterior using numerical marginalisation of types and in section [\ref=p_beams] we compare it with an alternative solution based on a perturbative expansion of the BEAMS posterior.

BEAMS

Cosmological parameter estimation usually proceeds by maximising the posterior, P(θ|D), where D is the set of redshifts and distance moduli of spectroscopically confirmed Type Ia supernovae and θ is the set of cosmological parameters, such as Ωm, ΩΛ and H0. What happens if we do not have spectroscopic confirmation of an object's type but only a probability that it is a Ia? Unbiased parameter estimation in this case can be achieved using Bayesian Estimation Applied to Multiple Species (BEAMS). BEAMS [\cite=kunz] considers all data in a given sample, appropriately weighting each data point based on its probability of being a Ia. Let τi be the type of object i and τi  =   if the object is a Type Ia and τi  =   if the object is not a Ia (for example, if it is a Type Ib/c or a Type II supernova). Then we can write the posterior, P(θ|D) (here θ, D and τ are understood to be vectors), as

[formula]

This sum marginalises over all possible combinations of types for the dataset so τ here is a length-N vector (where N is the number of objects). For example, if there were three objects in the dataset, the first term in the sum would have τ1  =  , τ2  =  , τ3  =  , the second term would have τ1  =  , τ2  =  , τ3  =   etc. Thus, in general, for the case of two distinct object types, this is a 2N summation. Applying Bayes' theorem gives that

[formula]

P(D), the Bayesian evidence, can be considered as a normalisation factor and ignored in further calculations. We will assume that P(θ,τ)  =  P(θ)P(τ) (see [\cite=kunz] for a discussion of this assumption). P(θ) is the usual prior on the parameters (probability based on prior knowledge about the parameters), and P(τ) can be written as

[formula]

This is the product of the probabilities, Pi, for all the objects typed as a Ia, τi  =  , multiplied by the product of (1 - Pi) for all the objects typed as nIa's, τi  =  . Here we assume the data and the object types are uncorrelated (see [\cite=newling] for details of BEAMS without this assumption). Thus the BEAMS posterior is given by

[formula]

As an order 2N calculation, this is computationally unfeasible. In the case of uncorrelated data, there is a simplification we can use to make the problem tractable. By decomposing the likelihood as a product of probabilities, it can be shown [\cite=kunz] that the posterior can be written as

[formula]

where L,i is the likelihood assuming the i'th object is a Ia and L,i is the likelihood assuming it is a nIa. This formula works well for uncorrelated data but is unlikely to be accurate in the case of correlated data. For the large datasets which will be available in the future, correlations among the data cannot be ignored and another solution must be found to apply BEAMS to these datasets in a computationally feasible way.

Correlated supernova data

Correlations between supernovae has only recently become an issue that must be included in cosmological parameter sets (e.g. [\cite=conley] [\cite=amanullah]) but will become progressively more important as we push on the systematics floor related to SNIa surveys. Correlated systematic uncertainties, the focus of this paper, can arise from a large number of sources and a detailed study of correlations has yet to be undertaken due to the diversity and complexity of the various contributing factors. Schematically correlations can arise from:

Peculiar velocities: When supernovae are within 50 Mpc or so of each other the peculiar velocities of their host galaxies will be correlated by large-scale bulk flows. These peculiar velocities cause correlated redshift errors. Usually redshift errors are converted into additional distance modulus errors (see e.g. [\cite=kessler]) but even if this is not done it will cause errors in the recovered cosmological parameters. See, for example, [\cite=hui] [\cite=cooray] [\cite=gordon] [\cite=davis].

Redshift-colour and redshift-stretch correlations: If there is no spectroscopic host galaxy redshift for an object, the redshift is estimated photometrically either from the host or the supernova multi-band light curves. The effect of redshift on the light curve is degenerate to some extent with the stretch and colour corrections. Hence errors on redshift will correlate with those in the colour and stretch and thus with the estimated distance modulus of the supernova [\cite=snls].

Filter errors: Transmission curves for the actual filters used on a telescope approximate true through-put. Errors in measuring these transmission curves (or time dependent changes of the filters)[\cite=doi] will tend to induce redshift-dependent correlations. For example, in [\cite=feindt], the authors consider the error in the flux calibration of the telescope which, since it affects each filter differently, correlates objects at similar redshifts. Zero-point photometry errors are a major source of uncertainty in current surveys that are common to all supernovae observed with the same telescope; e.g. [\cite=amanullah].

Template error correlations: Unaccounted for evolution of supernovae with redshift causes correlated errors due to errors in the light curves that form the training set. An example of this is the 'U-band anomaly' which causes discrepancies between the SALT2 and MLCS2k2 light curve fitters which may be related to an excess of flux in the UV at high redshift in SNIa [\cite=foley] [\cite=kessler].

Observational conditions: Bad weather will cause holes in the light curve coverage of all supernovae visible at a given time, while seeing conditions will alter photometry measurements in a correlated way. These will induce subtle correlations between objects observed on the same night in similar conditions [\cite=mandel] [\cite=lsst].

Combining data from multiple telescopes: The covariance matrix for combining data from multiple telescopes can be very complex, as discussed in the 3 year SNLS analysis [\cite=conley]. With the exception of perhaps the final LSST dataset, combining data from multiple surveys will continue to be standard.

Gravitational lensing: Supernovae that are close together on the sky, at similar redshifts will experience similar brightening or dimming due to lensing, depending on the matter distribution along the line of sight. Future large surveys will have mass maps and hence will be able to predict and remove this signal to some extent [\cite=gunnarsson] [\cite=jonsson] [\cite=amendola].

Dust: Supernovae along neighbouring lines of sight will suffer similar extinction from the Milky Way and from any intergalactic dust, which will induce correlations [\cite=zhang] [\cite=corasaniti].

Host-galaxy correlations: There is now solid evidence that dispersion in the Hubble diagram correlates with the properties of host galaxies, particularly the galaxy type, size and mass. See, for example [\cite=kelly] [\cite=gupta] [\cite=dandrea] [\cite=meyers].

Spectroscopic targeting correlations: Since spectroscopic follow-up is typically not random, there may be hidden correlations. For example, follow-up may favour candidates well-separated from the host galaxy core. Malmquist bias can also cause correlations which depend on the details of the spectroscopic survey [\cite=sdss2] [\cite=kessler]. If there is an unknown systematic that such objects are intrinsically brighter/fainter than average, this will cause a correlated systematic error [\cite=sdss2].

nIa correlations: Many of the sources of correlation listed here will also affect nIa's, causing correlations between their distance moduli. However, these correlations will typically be much smaller than the intrinsic dispersion of the nIa population.

Given the complexity of these various effects, developing a 'realistic' correlation matrix is beyond the scope of this work and will need to be laboriously built from simulations and detailed studies. Here we wish instead to develop ways of dealing with the general problem posed by the 2N 'exponential catastrophe' that correlations present. We use several toy model covariance matrices to study potential resolutions of this catastrophe, finding that numerical marginalisation over the supernova types performs best.

Mock data

In order to determine the effect that correlated data can have on parameter estimation with BEAMS, we use mock supernova datasets, the properties of which are known. This enables us to estimate the magnitude of the bias introduced by correlations between the data points and to determine the optimum way to handle this additional source of error. To simulate supernova data, we need to create a distance modulus μ(z) for each object. The distance modulus is defined as

[formula]

where m is the apparent magnitude of the object, M is the absolute magnitude of the object and dL is the luminosity distance to the object in Mpc [\cite=dodelson]. Through the distance modulus, SN Ia can be used to constrain cosmological parameters. The luminosity distance depends on the cosmological parameters (assuming a ΛCDM model) by

[formula]

where

[formula]

and H0 is the Hubble constant, Ωm is the energy density of matter, ΩΛ is the energy density of dark energy and Ωk is the energy density of curvature (all densities relative to the critical density) [\cite=dodelson]. We used a ΛCDM model to generate the mock data with the following parameters: H0 = 70.4 km/s/Mpc, Ωm = 0.272 and ΩΛ = 0.628 (values taken from [\cite=komatsu]).

SN Ia typically have very little scatter in their distribution of distance moduli, while nIa's tend to be widely scattered. SN Ia also tend to be brighter than nIa's. As such, we model the mock data as drawn from two populations: the Ia distribution is a narrow Gaussian centred on the fiducial cosmological distance modulus, μ(z), with standard deviation [formula]. The nIa distribution is a wide Gaussian, centred on μ(z) + b, where b is a constant shift. The nIa distribution has standard deviation [formula]. While in general one could model each known type of supernova as a separate population, two populations are sufficient for this work. For each object, redshifts in the range of 0.01 < z < 2.0 were drawn from a uniform random distribution, while probabilities, Pi, were drawn from a distribution that simulates the expected Dark Energy Survey probability distribution, shown in figure [\ref=des]. There were roughly equal numbers of Ia's and nIa's. Here we assume that the probabilities are correct, but it was shown in [\cite=kunz] [\cite=hlozek] that BEAMS can deal with biased probabilities in general. The type of each object was randomly chosen with probability Pi which allowed the object to be assigned a distance modulus. Residuals were drawn from the appropriate distribution, depending on type, correlated using one of the covariance matrices described below and added to the appropriate Ia or nIa mean. A typical mock dataset distance modulus diagram is shown in figure [\ref=data], where we have chosen the standard deviation of the Ia population to be [formula], that of the nIa's to be [formula] and the shift of the nIa population to be b = 2.0. In this paper we experimented with datasets varying between 200 and 1000 supernovae. Since the exact form of the true covariance matrix for supernovae is unknown, we use simple toy models for a covariance matrix for the mock data. Our main goal is to show how BEAMS can be used for correlated data, so we do not attempt to model a realistic covariance matrix. We analyse the effects of three different covariance matrices. In all cases, we order the objects according to redshift, since we expect the strongest correlations to be functions of redshift. As they will come from the same sources, we expect Ia-nIa and nIa-nIa correlations to be of similar magnitude as Ia-Ia correlations. However, since the dispersion of the nIa distribution is so large, these small correlations have little influence and only the Ia-Ia correlations affect the results. We found that when we created a dataset with correlations between all supernova types and analysed it assuming only Ia correlations, the results were identical to an analysis which included non-zero nIa-nIa and Ia-nIa correlations. Hence, we only consider Ia-Ia correlations in our analysis.

Wedding cake covariance matrix

This example matrix is based on the supernova covariance matrix in Kim et al. [\cite=kim], which is guaranteed to be positive-definite. This matrix has a wedding cake or step-like structure: an error entered at a given redshift contributes to the error of all higher redshift objects. This structure naturally arises as one loses observational features at higher redshifts thus introducing errors. We constructed a matrix based on this, in which we divided the data set into five redshift bins, adding an extra source of error in each bin. This structure is given as

[formula]

where [formula] if τi=Ia and [formula] if τi=nIa and

[formula]

where ni,j is the bin to which the object belongs. To produce the step-like structure, [formula] (where "⌊⌋" indicates the floor function, rounding down to the nearest integer). For this covariance matrix we used sk = 0.015 for k = 1 to 5, as illustrated in the top left panel of figure [\ref=correlated].

Decaying Covariance Matrix

This covariance matrix assumes positive correlations between objects which are nearby in the covariance matrix, with the correlations decaying as the distance between the indices of objects is increased. The exact form of this covariance matrix is

[formula]

This is illustrated in the middle left panel of figure [\ref=correlated], where we set x = 0.7.

Block-diagonal covariance matrix

To illustrate our method with a more realistic example, we used the covariance matrix for the Union2 sample of 557 supernovae. [\cite=amanullah] [\cite=suzuki] constructed this matrix by parameterising all known sources of correlations for the Union2 dataset, fitting these nuisance parameters simultaneously with the cosmological parameters and providing an estimate of the best-fit covariance matrix. We binned into 11 redshift bins, which we then applied to our mock data. An example for one mock data realisation can be seen in the bottom left panel of figure [\ref=correlated]. We have set nIa-nIa and nIa-Ia correlations to zero, since these should be very small compared to the intrinsic dispersion of the nIa's. We added a σ2i term to the diagonal where [formula] if object i is a Ia and [formula] if it is a nIa. The block diagonal structure arises largely from the fact that supernovae from the same survey are correlated with each other.

Numerical marginalisation over supernova type

Theory

A solution to the computational problem of handling correlated data would be to perform the marginalisation over the types numerically instead of analytically, allowing us to use the correlated BEAMS posterior without the 2N sum. In order to do this, we create N discrete nuisance parameters, the types of the supernovae, τi, and marginalise over these parameters in our Markov Chain Monte Carlo (MCMC) [\cite=metropolis] [\cite=hastings] analysis. This problem is similar to the Ising spin problem because these parameters are discrete and can only assume one of two values [\cite=ising]. In each step of the MCMC chain, we randomly select one object and set the corresponding τi  =  Ia with probability Pi, which significantly speeds up convergence when compared with varying all the types each step. To further improve convergence, we choose the initial type parameters based on the ratio of Ia to nIa uncorrelated likelihoods for each object. If the ratio is greater than one, we set the initial type for that object to a Ia, otherwise to a nIa. To compute this initial likelihood ratio, we use some fiducial values for the cosmological parameters. This initial choice for the types has little bearing on the final result, but without it, the chain typically starts with a very low likelihood because many objects have the wrong type and it takes much longer to reach the region of the peak. Thus, if θ represents the set of parameters in the MCMC chain and τ the types, the usual MCMC acceptance criterion is

[formula]

We applied this technique using MCMC to estimate cosmological parameters from the mock datasets. We ran at least three MCMC chains for each dataset produced and ensured all chains were converged using the Gelman-Rubin [\cite=gelman] criterion for convergence, with R < 1.01. Chains were, on average, 60 000 steps long after burn-in was removed. It should be noted that for all MCMC chains, the following flat, wide priors were used, to ensure unbiased parameter estimation (since small datasets were used to reduce computational time, which tend to have large contour areas): - 0.2 < Ωm < 1.2, - 0.2 < ΩΛ < 1.2 and 10 < H0  <  130. We allowed the shift, b, of the nIa population to vary with a flat prior of 1 < b < 3 and the standard deviations of the populations, [formula] and [formula] to vary in log space with priors of [formula] and [formula]. To allow for the most general case where only the form of the covariance matrix is known, but its parameters are not, we also varied the covariance parameters for each individual covariance matrix over a flat prior, except for the block covariance matrix whose off-diagonal terms remained fixed, since they were determined from the Union2 dataset.

Testing BEAMS with numerical marginalisation

We first generated uncorrelated datasets to directly compare the numerical and analytic marginalisation of BEAMS. We found that the Ωm  -  ΩΛ credible contours produced using eq.([\ref=beams]) and those produced using the numerical marginalisation technique were identical for the uncorrelated datasets, showing that the technique works as expected.

It is interesting to note that the marginalisation method is faster than uncorrelated BEAMS, since there are half the calculations to perform at each step (uncorrelated BEAMS computes the likelihood assuming the object is nIa and Ia respectively, whereas the numerical marginalisation only requires the likelihood given the current types). However, it takes longer for the correlated BEAMS chains to converge. On average, using the Gelman-Rubin [\cite=gelman] criterion for convergence, uncorrelated BEAMS chains converge within 10 000 steps whereas the correlated BEAMS chains only converge after about 40 000 steps. We tested datasets of up to 1000 data points in size and found that, although the amount of time taken to do a single likelihood calculation increases linearly, the number of steps taken to converge does not increase appreciably as the number of data points is increased. Hence, computational complexity will not be an issue for correlated BEAMS. Figure [\ref=correlated] compares the uncorrelated BEAMS approach with correlated BEAMS (using numerical marginalisation of types) in the case of correlated data, based on the three different covariance matrices from section [\ref=cov_mat_sec]. The uncorrelated approach includes no information about correlations and hence it is not surprising that it is biased at > 2σ for both the decaying covariance matrix and the wedding cake covariance matrix. In contrast, correlated BEAMS with numerical marginalisation correctly estimates the cosmological parameters without any bias. There is less of an effect from the block diagonal covariance matrix, because both the correlations and the dataset are small. Although even in this case, it is clear the uncorrelated BEAMS contours appear to be mildly biased.

Testing the Contours and Coverage Properties of the BEAMS Estimators

To test the accuracy of the correlated BEAMS contours, we created 5000 datasets, each consisting of 200 correlated supernovae, using the wedding cake covariance matrix. We then ran a 100 000 step MCMC chain for both uncorrelated and correlated BEAMS on each of the datasets, totaling about a billion MCMC steps. Figure [\ref=sim] shows scatter plots for both uncorrelated BEAMS and correlated BEAMS. Each point represents the maximum posterior values of the parameters for one of the 5000 datasets. We computed the mean squared error (MSE), which is the sample average of the squared distance between the estimates and the true value for the parameters (in this case Ωm and ΩΛ), for both uncorrelated and correlated BEAMS. We found the relative efficiency, defined to be the ratio of the MSE for uncorrelated BEAMS to that of correlated BEAMS, to be 2.3, which implies that correlated BEAMS is a much more efficient estimator than uncorrelated BEAMS for correlated data, i.e. it estimates parameters with much less scatter. We also plotted the 95% credible contours for five randomly selected datasets to show how the size and shape of the contours are underestimated by uncorrelated BEAMS but well estimated by correlated BEAMS, for these correlated datasets. To quantify this point, we computed the coverage, which we here define to be the proportion of the 5000 datasets where the true value (marked by the black circle in figure [\ref=sim]) lies within the 95% credible interval for each dataset derived from the corresponding MCMC chain for that dataset. We found that the coverage was 88.2% for correlated BEAMS and only 7.2% for uncorrelated BEAMS, showing that accounting for the correlations is crucial in getting the correct contours. Ideally, the coverage should be close to 95%. However, the reader is cautioned that coverage is a frequentist concept and only for asymptotically large datasets would we expect the frequentist coverage to coincide with the corresponding coverage from Bayesian credible intervals. As our datasets only have 200 points each, we should not be surprised if the coverage is not exactly 95%. Undercoverage occurs even for the standard χ2 method applied to supernovae datasets (see figure 8 in [\cite=march]).

The coverage properties of both estimators are illustrated in figure [\ref=bias]. For both uncorrelated and correlated BEAMS we plotted the effective coverage (on the y-axis), i.e. the fraction of datasets for which the given credible contour contains the input cosmological parameters, at that level of credibility (on the x-axis). So for example, the 0.95 credible level corresponds to a coverage of 0.882 and 0.072 for correlated and uncorrelated BEAMS respectively. Ideal coverage is the diagonal straight line across the plot, where (for example) 0.95 of the datasets would contain the input cosmology in the 0.95 credible contour. It is clear that the coverage of correlated BEAMS is close to ideal and far superior to that of uncorrelated BEAMS.

A perturbative expansion of the BEAMS posterior

Theory

For future surveys, in exactly the limit where it will be important to apply BEAMS to avoid biases from probability cuts, the light curve data for both candidates and templates will be excellent and the probability that most candidates are a Ia will therefore be close to zero or unity. This is illustrated in figure [\ref=des], which shows simulated DES probabilities using MLCS2k2 [\cite=mlcs] and SNANA [\cite=snana]. In this limit of abundant, high-quality data, we may expect we could perform a perturbative expansion of the full correlated BEAMS posterior, to find an approximation to the full posterior as an alternative to the numerical marginalisation over type. If we write eq. ([\ref=beams_eq]) as a function of the probabilities, [formula], we can Taylor expand the posterior around the point where the probabilities are rounded to either zero or one (PRnd):

[formula]

where εi = pi  -  Rnd(pi) and Rnd(pi) is the rounded value of the probability of the i'th data point (which will be either 0 or 1) and H is the Hessian matrix. Appendix [\ref=appen] contains details of the derivation. The resulting posterior, up to the second order term, is

[formula]

Results

This perturbative approximation breaks down because we expand the posterior about the rounded off probabilities (that is, if the probability is close to 1 we take the object to be a Ia and a nIa if it close to 0), but due to the extreme nature of the likelihood, this tends to lie quite far from the maximum likelihood, the point about which one would like to perform the expansion. This happens because [formula] is very small, hence if too many nIa's are mistyped as Ia's when rounding off the probabilities, these terms cause higher order terms in the Taylor expansion to dominate, however we only include terms up to second order term in this expansion in order for the analysis remain computationally viable (the second order term is an N3 calculation, the third order N4 and so on). Thus the perturbative expansion is not a good enough approximation of the true likelihood. This can be seen in figures [\ref=p-beams_uncor] and [\ref=p-beams_cor], where we show 1 and 2-σ contours in the Ωm  -  ΩΛ plane. In each case, the fiducial value is shown by a star. While the underlying data are correlated, the uncorrelated form of BEAMS assumes uncorrelated data. In addition, perturbative BEAMS cannot sufficiently correct for the mistyped terms, and thus fail to recover the simulated data model at >  3σ.

The failure of perturbative BEAMS to correct for mistyped terms can be understood further by considering figure [\ref=fail_p_beams_small]. In the left panel, we show perturbative BEAMS successfully approximates the true likelihood, as does correlated BEAMS using numerical marginalisation, for a small dataset. The right panel, however, shows this is not the case for a large dataset. The perturbative BEAMS estimate for ΩΛ is biased, because mistyped objects in the dataset cause higher order terms in the perturbative approximation to dominate. As shown previously, however, correlated BEAMS (with numerical marginalisation of types) accurately recovers the input cosmology.

Conclusion

Photometric supernova surveys with unprecedented amounts of data will provide an exciting opportunity to learn about the structure and evolution of the universe. Due to the vast number of supernova candidates and their extended redshift distribution in large future surveys, it will be difficult, if not impossible, to spectroscopically follow-up all candidates as is normally required in cosmological analyses. BEAMS is a rigorous statistical method which avoids biases while using all supernova candidates, together with the probability that a candidate is a Type Ia supernova, derived from the multicolour lightcurves of the candidate. Until now, BEAMS has been applied assuming the supernovae are uncorrelated [\cite=kunz] [\cite=hlozek] [\cite=newling], an assumption which will be inappropriate for future surveys. Without this assumption, the analytical form of the BEAMS posterior is computationally unfeasible. If the uncorrelated form of BEAMS is applied to a dataset with correlated systematic uncertainties, the posterior for the cosmological parameters can be incorrectly estimated. To deal with this 'exponential catastrophe', we have explored two different approaches. The first marginalises over all the possible combinations of object type numerically instead of analytically, by including the types as discrete nuisance parameters in our MCMC chains, making it computationally efficient. We have shown, with three separate models of covariance matrices, that this algorithm successfully recovers the input cosmology in the correlated case without bias. In addition, we have shown with 5000 mock datasets that the correlated BEAMS credible contours are reliable estimates of the true error contours. This is something that cannot be easily reproduced without using the correlated BEAMS formalism. The second approach we considered was a perturbative expansion of the BEAMS posterior which typically fails because, when too many objects are mistyped, the higher order terms of the expansion (which are neglected due to computational constraints) dominate over the lower order terms, even when the probabilities are very close to zero or one thus producing a biased posterior. However, with numerical marginalisation over types, correlations between supernovae do not appear to be an impediment to using BEAMS in analysing future photometric supernova surveys.

Perturbative BEAMS

Theory

The aim is to derive a formula for the BEAMS posterior in the case where the probabilities are all close to zero and one. We start with the BEAMS posterior in the general case:

[formula]

This results in 2N terms being calculated, in the case where there are two types. We can Taylor expand this posterior, written as a function of the probabilities [formula] (this is a length-N vector containing the probability for each object), around the point where the probabilities are rounded to either zero or one. We define the vector [formula] such that εi = pi  -  Rnd(pi), where Rnd(pi) is the rounded value of the probability of the i'th data point (which will be either 0 or 1). So for example, if pi = 0.99 then εi =  - 0.01 and if pi = 0.02 then εi = 0.02 etc. The BEAMS posterior becomes:

[formula]

where H is the Hessian matrix.

The first order term

Now we will explicitly calculate the first order derivative term. The i'th component of [formula] is given by

[formula]

This derivative is trivial since [formula] is linear in the pi's. [formula] is defined as

[formula]

Since only one of these terms is a function of pi (where i equals either j or k depending on its type), the derivative is given by

[formula]

Here, [formula] is equivalent to eq. ([\ref=P]), with the contribution to the product from i'th object's probability removed. Since Pτi is either pi or (1 - pi), its derivative is given by

[formula]

Thus:

[formula]

Once we subsitute for the rounded probabilities, many of the terms in [formula] will go to zero. In fact, each element of [formula] must equal its rounded type for [formula] in order to be non-zero, in which case, [formula]. This leaves only two terms, corresponding to the two possible types of the i'th data point. Thus:

[formula]

where [formula] is the same as [formula] (where τRndi  =   if Rnd(pi) = 1 and τRndi  =   if Rnd(pi) = 0) with the i'th element being Type Ia and [formula] is the same except the i'th element is Type nIa.

Finally, the first order BEAMS posterior is given by

[formula]

For N data points, the order of this calculation is 2N times the order of the likelihood calculation. So in the case where the likelihood is given by the usual Gaussian form e-  χ2 / 2, this would be an order 2N2 calculation.

The second order term

Our treatment can be repeated for the second order term. The second order derivative is given by

[formula]

[formula] can again be expressed as a product of one i-dependent term, one j-dependent term and one term independent of both

[formula]

if i  ≠  j. If i = j, then the second derivative of [formula] will be zero. By the product rule, this is

[formula]

Which evaluates to

[formula]

Similarly to the first order term, when we evaluate the full posterior (eq. ([\ref=post])) at the rounded probabilities, we find that only four terms remain (still for i  ≠  j):

[formula]

where

[formula]

Putting this all together, the second order term of the BEAMS posterior is

[formula]

Thus the second order term is order (4N)2 times the order of the likelihood calculation. For a Gaussian likelihood, this is an order N3 calculation.

References