Lemma Conjecture Corollary Proposition Remark Definition

Scene Grammars, Factor Graphs, and Belief Propagation

Introduction

The primary motivation of this work is that objects and scenes can be represented using hierarchical structures defined by compositional rules. For instance, faces are composed of eyes, nose, mouth. Similarly, geometric objects such as curves can be defined in terms of shorter curves that are recursively described. A hierarchical structure defined by compositional rules defines a rich description of a scene that captures both the presence of different objects and relationships among them. Moreover, compositional rules provide contextual cues for inference with ambiguous data. For example, the presence of some parts of a face in a scene provides contextual cues for the presence of other parts.

In the models we consider, every object has a type from a finite alphabet and a pose from a finite but large pose space. While classical language models generate sentences using a single derivation, the grammars we consider generate scenes using multiple derivations. These derivations can be unrelated or they can share sub-derivations. This allows for very general descriptions of scenes.

We show how to represent the distributions defined by probabilistic scene grammars using factor graphs, and we use loopy belief propagation (LBP) [\cite=MWJ99] [\cite=KFL01] for approximate inference. Inference with LBP simultaneously combines "bottom-up" and "top-down" contextual information. For example, when faces are defined using a composition of eyes, nose and mouth, the evidence for a face or one of its parts provides contextual influence for the whole composition. Inference via message passing naturally captures chains of contextual evidence. LBP also naturally combines multiple contextual cues. For example, the presence of an eye may provide contextual evidence for a face at two different locations because a face has a left and a right eye. However, the presence of two eyes side by side provides strong evidence for a single face between them.

We demonstrate the practical feasibility of the approach on two very different applications: curve detection and face localization. Figure [\ref=fig:samples] shows samples from the two different grammars we use for the experimental results. The contributions of our work include (1) a unified framework for contextual modeling that can be used in a variety of applications; (2) a construction that maps a probabilistic scene grammar to a factor graph together with an efficient message passing scheme; and (3) experimental results showing the effectiveness of the approach.

Probabilistic grammars and compositional models are widely used for parsing sentences in natural language processing [\cite=Manning99]. Recursive descriptions of objects using grammars and production rules have also been widely used in computer graphics to generate geometric objects, biological forms, and landscapes [\cite=PL91]. A variety of other compositional models have been used in computer vision. The models we consider are closely related to the Markov backbone model in [\cite=JG06]. Other related approaches include [\cite=BGP97] [\cite=TCYZ05] [\cite=FM10] [\cite=ZZ11] [\cite=GFM11] [\cite=FBL14]. These previous methods have relied on MCMC or heuristic methods for inference, or dynamic programming for scenes with single objects. The models we consider generalize part-based models for object detection such as pictorial structures [\cite=FE73] [\cite=FH05] and constellations of features [\cite=BWP98]. In particular, the grammars we consider define objects that are composed of parts but allow for modeling objects with variable structure. The models we consider also explicitly capture scenes with multiple objects.

Probabilistic Scene Grammars and a Factor Graph Representation

Our point of departure is a probabilistic scene grammar that defines a distribution over scenes. The approach is based on the Markov backbone from [\cite=JG06]. Scenes are defined using a library of building blocks, or bricks, that have a type and a pose. Bricks are generated spontaneously or through expansions of other bricks. This leads to a hierarchical organization of the elements of a scene.

A probabilistic scene grammar G consists of

A finite set of symbols, or types, Σ.

A finite pose space, ΩA, for each symbol A∈Σ.

A finite set of production rules, R. Each rule r∈R is of the form [formula] where Ai∈Σ. We use RA to denote the rules with symbol A in the left-hand-side (LHS). We use Ar,i to denote the i-th symbol in the right-hand-side (RHS) of a rule r.

Rule selection probabilities, P(r), with [formula] for each symbol A∈Σ.

For each rule [formula] we have categorical distributions gr,i(z|ω) defining the probability of a pose z for Ai conditional on a pose ω for A0.

Self-rooting probabilities, εA, for each symbol A∈Σ.

A noisy-or parameter, ρ.

The bricks defined by G are pairs of symbols and poses,

[formula]

A scene S is defined by

A set O  ⊆  B of bricks that are present in S.

A rule r∈RA for each brick (A,ω)∈O, and a pose z∈ΩAi for each Ai in the RHS of r.

Let H = (B,E) be a directed graph capturing which bricks can generate other bricks in one production. For each rule r, if gr,i(z|ω)  >  0, we include ((A0,ω),(Ai,z)) in E. We say a grammar G is acyclic if H is acyclic.

A topological ordering of B is an ordering of the bricks such that (A,ω) appears before (B,z) whenever (A,ω) can generate (B,z). When G is acyclic we can compute a topological ordering of B by topological sorting the vertices of H.

An acyclic grammar defines a distribution over scenes, P(S), through the following generative process.

Initially [formula].

For each brick (A,ω)∈B we add (A,ω) to O independently with probability εA.

We consider the bricks in B in a topological ordering. When considering (A,ω), if (A,ω)∈O we expand it.

To expand (A,ω) we select a rule r∈RA according to P(r) and for each Ai in the RHS of r we select a pose z according to gr,i(z|ω). We add (Ai,z) to O with probability ρ.

Note that because of the topological ordering of the bricks, no brick is included in O after it has been considered for expansion. In particular each brick in O is expanded exactly once. This leads to derivation trees rooted at each brick in the scene. The expansion of two different bricks can generate the same brick, and this leads to a "collision" of derivations. When two derivations collide they share a sub-derivation rooted at the point of collision. Derivations terminate using rules of the form [formula], or through early termination of a branch with probability ρ.

Factor Graph Representation

We can represent the distribution over scenes, P(S), using a factor graph with binary variables.

A scene S generated by an acyclic grammar G defines a set of random variables,

[formula]

where

X(A,ω)  =  1 if (A,ω)∈O.

R(A,ω,r)  =  1 if rule r is used to expand (A,ω).

G(A,ω,r,i,z)  =  1 when (A,ω) is expanded with rule r, and z is the pose selected for Ai.

Let G(A,ω) be the vector of variables G(B,z,r,i,ω) where Ar,i = A. We have X(A,ω)  =  0 when X(A,ω) is not generated spontaneously or by the expansion of another other brick. Therefore,

[formula]

where c is the number of variables in G(A,ω) with value 1.

Let R(A,ω) be the vector of random variables R(A,ω,r). The generative process determines R(A,ω) by selecting a rule r∈RA for expanding (A,ω) when X(A,ω) = 1, and no rule is selected when X(A,ω) = 0. Therefore,

[formula]

where I(r) is an indicator vector for r∈RA

Let G(A,ω,r,i) be the vector of random variables G(A,ω,r,i,z). The generative process selects a pose z for Ar,i if the rule r is used to expand a brick. Therefore,

[formula]

where I(z) is an indicator vector for z∈ΩAr,i.

The joint distribution, P(X,R,G), defined by an acyclic grammar can be expressed using a factored representation following the structure of the generative process defined by G,

[formula]

We can express P(X,R,G) using a factor graph over the binary variables, with a factor for each term in the product above. The factors in the factor graph representation are

[formula]

Although we have assumed an acyclic grammar in the derivation of the distribution P(X,R,G) in equation ([\ref=eqn:factored]), the factor graph construction can also be applied to arbitrary grammars. This makes it possible to define probability distributions over scenes using cyclic grammars, without relying on the generative process formulation.

Inference Using Belief Propagation

To perform approximate inference with the factor graph representation, we use loopy belief propagation (LBP) [\cite=MWJ99] [\cite=KFL01]. Here we describe how to compute LBP messages efficiently for the factor graphs that represent scene grammars.

The factors in our model are of one of two kinds: The factor Ψ1 defined in equation ([\ref=factorsPsi1]) captures a noisy-OR distribution, and the factors Ψ2 and Ψ3 defined in equations ([\ref=factorsPsi2]) and ([\ref=factorsPsi3]) capture categorical distributions in which the outcome probabilities depend on the state of a switching random variable. Figure [\ref=fig:factors] shows the local graphical representation for the two types of factors. The computation of messages from variables to factors follows the standard LBP equations. Below we describe how to efficiently compute the messages from factors to variables. The computational complexity of message updates for both kinds of factors is linear in the degree of the factor. In the derivations below we assume all messages have non-zero value.

Message passing for noisy-OR factors

Consider a factor [formula] that represents a noisy-OR relationship between binary inputs y1,...yN, and a binary output z. Suppose we have a leak in the noisy-OR with probability ε and independent failure parameter 1 - ρ. We define β  =    -   log (1 - ρ). We can write the factor F as

[formula]

The message passing equations are straightforward to derive and we simply state them here,

[formula]

Message passing for categorical factors

Consider a factor [formula] that represents a mixture of categorical distributions. The binary values [formula] specify the outcome and y controls the outcome probabilities. Concretely,

[formula]

where θiy is the probability of the i-th outcome with the mixture component defined by y.

In this case we can derive the following message passing equations,

[formula]

Learning Model Parameters

For a grammar with fixed structure we can use EM to learn the the production rule probabilities, P(r), and the self-rooting parameters, εA. The approach involves iterative updates. In each iteration, we (1) use LBP to compute (approximate) conditional marginal probabilities on training examples with the current model parameters, and (2) update the model parameters according to sufficient statistics derived from the output of LBP.

Let Qe(A,ω,r) be the marginal probability of brick (A,ω) being expanded using rule r in the training example e. In the factor graph representation, this corresponds to the marginal probability that a random variable takes a particular value, P(R(A,ω,r) = 1), a quantity that is approximated by the output of LBP. The update for P(r) is,

[formula]

The value of ZA is determined by normalizing probabilities over r∈RA. We update the self-rooting parameters, εA, in an analogous way, using approximate marginals computed by LBP.

Experiments

To demonstrate the generality of our approach we conducted experiments with two different applications: curve detection, and face localization. Previous approaches for these problems typically use fairly distinct methods. Here, we demonstrate we can handle both problems within the same framework. In particular we have used a single implementation of a general computational engine for both applications. The computational engine can perform inference and learning using arbitrary scene grammars. We report the speed of inference as performed on a laptop with an Intel i7 .5GHz CPU and 16 GB of RAM. Our framework is implemented in Matlab/C using a single thread.

Curve detection

To study curve detection we used the Berkeley Segmentation Dataset (BSD500) [\cite=AMFM11] following the experimental setup described in [\cite=FOP]. The dataset contains natural images and object boundaries manually marked by human annotators. For our experiments, we used the standard split of the dataset with 200 training images and 200 test images. For each image we use the boundaries marked by a single human annotator to define ground-truth binary contour maps J.

From a binary contour map J we generate a noisy image I by sampling each pixel I(x,y) independently from a normal distribution whose mean depends on the value of J(x,y).

[formula]

For our experiments, we used μ(0)  =  150, μ(1)  =  100, σ = 40.

To model binary contour maps we use a first-order Markov process that generates curves of different orientations and varying lengths. The grammar is defined by two symbols: C (oriented curve element) and J (curve pixel). We consider curves in one of 8 possible orientations. For an image of size

[formula]

Face Localization

To study face localization, we performed experiments on the Faces in the Wild dataset [\cite=LFW]. The dataset contains faces in unconstrained environments. Our goal for this task is to localize the face in the image, as well as face parts such as eyes, nose, and mouth. We randomly select 200 images for training, and 100 images for testing. Although the dataset comes annotated with the identity of the persons in the image, it does not come with part annotations. We manually annotate all training and test images with bounding box information for the parts: Face, Left eye, Right eye, Nose, Mouth. Examples of the manual annotation are shown in Figure [\ref=fig:lfw_res].

The face grammar has symbols Face (F), Left eye (L), Right eye (R), Nose (N), and Mouth (M). Each symbol has an associated set of poses of the form (x,y,s), which represent a position and scale in the image. We refer to the collection of {L,R,N,M} symbols as the parts of the face. The grammar has a single rule of the form F  →  {L,R,N,M}. We express the geometric relationship between a face and each of its parts by a scale-dependent offset and region of uncertainty in pose space. The offset captures the mean location of a part relative to the face, and the region of uncertainty captures variability in the relative locations.

Concretely, suppose we had a Face with pose (x,y,s). Then, for each part Z∈{L,R,N,M}, the Face would expand to a part Z somewhere in a uniform region centered at (x',y')  =  (x,y)  +  sbZ. Having a part-dependent base offset bZ allows us to express information such as "the mouth is typically near the bottom of the face" and "the nose is typically near the middle of the face". The dependence of the offset on the scale s of the Face allows us to place parts in their correct position independent of the Face size. We model the relationship between scales of a Face and a part in a similar way. Modeling the relation between scales allows us to represent concepts such as large faces tending to have large parts. We learn the geometric parameters such as the part offsets by collecting statistics in the training data.

Figure [\ref=fig:samples] shows samples of scenes with one face generated by the grammar model we estimated from the training images in the face dataset. Note the location and scale of the objects varies significantly in different scenes, but the relative positions of the objects are fairly constrained. Samples of scenes with multiple faces are included in the supplemental material.

Our data model is based on HOG filters [\cite=DT05]. We train HOG filters using publicly-available code from [\cite=voc-release4]. We train separate filters for each symbol in the grammar using our annotated images to define positive examples. Our negative examples are taken from the PASCAL VOC 2012 dataset [\cite=pascal-voc-2012], with images containing the class "People" removed.

The score of a HOG filter is real-valued. We convert this score to a probability using Platt's method [\cite=P99], which involves fitting a sigmoid. This allows us to estimate [formula] for each symbol A∈{F,L,R,N,M}. For the observation model we require a quantity that can be interpreted as [formula], up to a proportionality constant. We note that [formula]. We approximate P(X(A,ω) = 1) using the self-rooting probability, εA. To connect the data model to the grammar, the normalized scores of each filter are used to define messages into the corresponding bricks in the factor graph.

The result of inference with our grammar model leads to the (approximate) probability that there is an object of each type in each pose in the image. We show detection and localization results on images with multiple faces in the supplementary material. To quantify the performance of the model for localizing the face and its parts on images containing a single face we take the highest probability pose for each symbol. As a baseline we consider localizing each symbol using the HOG filter scores independently, without using a compositional rule.

Figure [\ref=fig:lfw_res] shows some localization results. The results illustrate the context defined by the compositional rule is crucial for accurate localization of parts. The inability of the baseline model to localize a part implies the local image evidence is weak. By making use of contextual information in the form of a compositional rule we can perform accurate localization despite locally weak image evidence.

We provide a quantitative evaluation of the grammar model the baseline model in Table [\ref=table:faceRes]. The Face localization accuracy of both models are comparable. However, when attempting to localize smaller objects such as eyes, context becomes important since the local image evidence is ambiguous. We also ran an experiment with the grammar model without a HOG filter for the face. Here, the grammar is unchanged but there is no data model associated with the Face symbol. As can be seen in the bottom row of Table [\ref=table:faceRes], we can localize faces very well despite the lack of a face data model, suggesting that contextual information alone is enough for accurate face localization. Inference using the grammar model on a (250  ×  250) test image took 2 minutes.

Conclusion

Probabilistic scene grammars define priors that capture relationships between objects in a scene. By using a factor graph representation we can apply belief propagation for approximate inference with these models. This leads to a robust algorithm for aggregating local evidence through contextual relationships. The framework is quite general and the practical feasibility of the approach was illustrated on two different applications. In both cases the contextual information provided by a scene grammar proves fundamental for good performance.

Acknowledgements

We would like to thank Stuart Geman and Jackson Loper for many helpful discussions about the topics of this research.

Contour Detection Results

Figures [\ref=fig:contour1] and [\ref=fig:contour2] show more contour detection results on images from the BSDS500 at a larger resolution so the reader can examine more details.

Multiple Faces

In Figure [\ref=fig:sample_multiFace_p1] we show unconstrained samples from the grammar model for faces. Note how the model generates scenes with multiple faces, and also generates parts that appear on their own, since every symbol has a non-zero probability of self-rooting.

In Figure [\ref=fig_supp:multiFace] we show localization results for images with multiple faces. In this case we show the top K poses for each symbol after performing non-maximum suppression, where K is the number of faces in the image.

In general we do not know in advance the number of symbols of each type that are present in the scene. In this case it is not possible to simply select the top K poses for each symbol. A different strategy is to use a threshhold, and select bricks that have marginal probabilities above the threshold to generate detections. The particular threshhold used depends on the desired trade-off between false positives and false negatives. A threshhold for a particular application can be set by examining a Precision-Recall curve. For three example images, we manually selected a single threshhold for detection that leads to good results. In Figure [\ref=fig_supp:multiFace_p1] we show all Face bricks with marginal probability above the threshold, after applying non-maximum suppression.

Contextual Influence

Figure [\ref=fig:bels] shows the results of inference when conditioning on various parts being in the image in specific poses. The grammar used here was a simplified version of the face grammar in the main paper. The symbols are Face (F), Eye (E), Nose (N), and Mouth (M). The pose of each symbol is the set of pixels in the image (there is no scale variation). The only compositional rule is F  →  {E,E,N,M}. Note that we use the same symbol to represent the left and right eyes.

As can be seen in Figure [\ref=fig:bels], when we condition on the presence of a Face at a particular position (first row), the model "expects" to see parts of the face in certain regions in the image. When we condition on the location of an Eye (second row), the model does not know whether the Eye should be a left eye or right eye, hence there are two modes for the location of the Face, and two modes for the location of another Eye. Intuitively LBP is performing the following chain of reasoning: (1) the eye that is known to be present can be a left or right eye, (2) there are two possible regions of the image in which the face can occur, depending on whether the eye that is known to be present is a left or right eye, and finally (3) given each possible pose for the face, the other parts of the face should be located in a particular spatial configuration. When we condition on more parts, LBP can infer that it is more likely for the face to be in one region of the image over another region, and the beliefs for the other face parts reflect this reasoning.