=8000 =1000

Introduction

Explanations for recommendations can have several benefits, such as: helping the user make a good decision (effectiveness), helping the user make a faster decision (efficiency), and revealing the reasoning behind the system's recommendation (transparency) [\cite=tintarev2011designing] [\cite=Zanker:2012:IKE:2365952.2366011]. As a result, users are more likely to follow the recommendation and use the system in better ways [\cite=tintarev2007survey] [\cite=herlocker2000explaining]. For instance, the Netflix recommender system justifies its movie suggestions by listing similar movies, obtained from the user's social network. Amazon's recommender system shows similar items to the ones that the user (or other similar users) have bought or viewed, when recommending a new item using neighborhood based Collaborative Filtering (CF).

CF approches provide recommendations to users based on their collective recorded interests on items, typically relying on the similarity between users or items, giving rise to neighborhood-based CF approaches, which can be user-based or item-based. Neighborhood-based CF methods are white-box approaches that can be explained based on the ratings of similar users or items.

Most accurate recommender systems are model-based methods that are black-boxes. Among model-based approaches are Restricted Boltzmann Machines (RBM) [\cite=hinton2010practical] that can assign a low dimensional set of features to items in a latent space. The newly obtained set of features capture the user's interests and different items groups; however, it is very difficult to interpret these automatically learned features. Therefore, justification of the recommendation or the reasoning behind the recommended item in these models is not clear. RBM approaches have recently proved to be powerful for designing deep learning techniques to learn and predict patterns in large datasets because they can provide very accurate results [\cite=hinton2006reducing]. However, they suffer from the lack of interpretation of the results, especially for recommender systems. Lack of explanations can result in users not trusting the suggestions made by the recommender system. Therefore, the only way for the user to assess the quality of a recommendation is by following it. This, however, is contrary to one of the goals of a recommendation system, which is reducing the time that users spend on exploring items. It would be very desirable and beneficial to design recommender systems that can give accurate suggestions, which, at the same time, facilitate conveying the reasoning behind the recommendations to the user. A main challenge in creating a recommender system is to choose an interpretable technique with moderate prediction accuracy or a more accurate technique, such RBM, which does not give explainable recommendations.

Problem Statement

Our research question is: can we design an RBM model for a CF recommender engine that suggests items that are explainable, while recommendations remain accurate? Our current scope is limited to CF recommendations where no additional source of data is used in explanations, and where explanations for recommended items can be generated from the ratings given to these items, by the active user's neighbors only (user-based neighbor style explanation), as shown in Figure [\ref=fig:example-explanation].

Related Work

Explaining CF

There are different ways of classifying explanation styles. Generally explanations can be user-based neighbor-style (Figure [\ref=fig:example-explanation]), item-based neighbor style (also known as influence-style), and keyword-style [\cite=bilgic2005explaining]. A user-based neighbor-style explanation is based on similar users, and generally is used when the CF method is also a user-based neighborhood style method. An item-based neighbor-style explanation is generally used in item-based CF methods by presenting the items that had the highest impact on the recommender system's decision. A keyword-style explanation is based on the items' features or users' demographic data available as content data and is mostly used in content based recommender systems [\cite=bilgic2005explaining].

In all styles, the explanation may, or may not reflect the underlying algorithm used by the recommender system. Also, data sources employed in the recommendation task, may be different from the data sources used in generating the explanation [\cite=vig2009tagsplanations] [\cite=herlocker2000explaining] [\cite=symeonidis2008justified] [\cite=bilgic2005explaining] [\cite=billsus1999personal]. Hence, the explanation generation is a separate module from the recommender system. However, performing the recommendation task based on the items' explainability (thus integrating recommendation and explanation) may improve the transparent suggestion of interpretable items to the user, while enjoying the powerful prediction of a model-based CF approach.

Zhang et al. [\cite=zhang2014explicit] proposed a model-based CF to generate explainable recommendations based on item features and sentiment analysis of user reviews, as data sources, in addition to the ratings data. That said, their approach is similar to our proposed method in that the recommender model suggests highly explainable items as recommendations and in that the recommendation and explanation engines are not separate. They further expanded their feature-level explanation by considering different forms such as word clouds [\cite=zhang2015incorporating]. Also, [\cite=abdollahi2016explainable] proposed explainable MF (EMF) method for explainable CF. In contrast to Zhang et al. [\cite=zhang2014explicit], EMF approach does not require any additional data such as reviews for explanation generation. Herlocker et al. [\cite=herlocker2000explaining] performed a detailed study on 21 different styles of explanation generation for neighbor-based CF methods, including content-based explanations that present a list of features from the recommended item to the user. Symeonidis et al. [\cite=symeonidis2008justified] also proposed an explanation approach based on the content features of the items. Their recommender system and the explanation approach are both content-based but they used different algorithms. Bilgic and Mooney [\cite=bilgic2005explaining] proposed three forms of explanations: keyword style, neighbor style and influence style, as separate forms of explanation approaches from their recommender system, a hybrid approach, called LIBRA, that recommends books [\cite=mooney2000content]. Billsus and Pazzani [\cite=billsus1999personal] presented a keyword style and influence style explanation approach for their news recommendation system. The system generates explanations and adapts its recommendation to the user's interests based on the user's preferences and interests. In all the reviewed existing approaches except [\cite=herlocker2000explaining], the explanations have to resort to external data such as item content or reviews, while useful when available, are outside the scope of the work in this paper that primarily aims at explainability of pure CF without resorting to external data.

RBM for CF

RBM is a two layer stochastic neural network consisting of visible and hidden units. Each visible unit is connected to all the hidden units in an undirected form. No visible/hidden unit is connected to any other visible/hidden unit. The stochastic, binary visible units encode user preferences on the items from the training data, therefore the state of every visible unit is known. Hidden units are also stochastic, binary variables that capture the latent features. A probability p(,) is assigned to each pair of a hidden and a visible vector:

[formula]

where E is the energy of the system and Z is a normalizing factor, as defined in [\cite=Hinton:2002:TPE:639729.639730].To train for the weights, a Contrastive Divergence method was proposed by Hinton [\cite=Hinton:2002:TPE:639729.639730]. Salakhutdinov et al. [\cite=Salakhutdinov:2007:RBM:1273496.1273596], proposed an RBM framework for CF. Their model assumes one RBM for each user and takes only rated items into consideration when learning the weights. They presented the results of their approach on the Netflix data and showed that their technique was more accurate than Netflix's own system. The focus of this RBM approach was only on the accuracy and predicting error and not explanation generation.

Proposed Methods

In this section, we present our explainable RBM framework. First, we present our approach for measuring explainability of each item for each user using explicit rating data. Next, we propose the explainable RBM framework.

Explainability

Explainability can be formulated based on the rating distribution within the active user's neighborhood. The main idea is that if many neighbors have rated the recommended item, then this could provide a basis upon which to explain the recommendations, using neighborhood style explanation mechanisms. For user-based neighbor-style explanations, such as the ones shown in Figure [\ref=fig:example-explanation], we can therefore define the Explainability Score of item i for user u as follows:

[formula]

where Nk(u) is the set of user u's k neighbors, rx,i is the rating of x on item i and Rmax is the maximum rating value of Nk(u) on i. Neighbors are determined based on the cosine similarity. Without loss of information, rx,i is considered as 0 for missing ratings, indicating that user x does not contribute to the user-based neighbor-style explanation of item i for user u. Given this definition, it is obvious that Explainability Score is between zero and one. Item i is explainable for user u only if its explainability score is larger than zero. When no explanation can be made, the explainability ratio would be zero.

Conditional RBM

The conditional RBM model takes explainability into account with an additional visible layer, [formula], with n nodes, where n is the number of items. Each node has a value between 0 and 1, indicating the explainability score of the relative item to the current user in the iteration, calculated as explained in Section 4. The idea is to define a joint distribution over (,), conditional on the explainability scores, [formula]. Figure [\ref=fig:Conditional-RBM] presents the conditional RBM model with explainability. Based on [\cite=hinton2010practical], the p(hj = 1|,), p(vi = 1|), and p(mi = 1|) are defined as:

[formula]

[formula]

[formula]

where a, b, and c are biases, f and n are the numbers of hidden and visible units, respectively, and W and D are the weights of the entire network. σ(x) is the logistic function [formula].

To avoid computing a model, we follow an approximation to the gradient of a different objective function called “Contrastive Divergence” (CD) [\cite=Hinton:2002:TPE:639729.639730]:

[formula]

where Wij is an element of a learned matrix that models the effect of ratings on h. Learning D, which is the effect of explainability on h, using CD, is similar and takes the form:

[formula]

Experimental results

We tested our approach on the MovieLens ratings data which consists of 100,000 ratings, on a scale of 1 to 5, for 1700 movies and 1000 users. The data is split into training and test sets such that 10% of the latest ratings from each user are selected for the test set and the remaining are used in the training set. Ratings are normalized between 0 and 1, to be used as RBM input. We compare our results with RBM, Explainable Matrix Factorization [\cite=abdollahi2016explainable], user-based top-n recommendations [\cite=herlocker1999algorithmic], and non-personalized most popular items. Each Experiments is run 10 times and the average results are reported. To assess the rating prediction accuracy, we used the Root Mean Squared Error (RMSE) metric: [formula]. Note that RMSE can only be calculated for the prediction based methods and not the top-n recommendation techniques. To evaluate the top-n recommendation task, we used the nDCG@10 metric following [\cite=herlocker2004evaluating]. The results for RMSE and nDCG, when varying the number of hidden units, f, is presented in Figure [\ref=fig:MEPMER], top row. It can be observed that Explainable RBM (E-RBM) outperforms other approaches when f > 20 for RMSE and when f < 20 for nDCG. To evaluate the explainability of the proposed approach, we used Mean Explainability Precision (MEP) and Mean Explainability Recall (MER) metrics [\cite=abdollahi2016explainable] which are higher when the recommend items are explainable as per Eq (2). The results are shown in Figure [\ref=fig:MEPMER], bottom row. It can be observed that explainable RBM outperforms other approaches in terms of explainability. For a test user, the top-3 rated movies and their genres by the user, in addition to the top-3 recommended movies and the explanations generated using the proposed method, are presented in Table [\ref=tab:example-1-1]. Explanations can be presented to the users using the format shown in Figure [\ref=fig:example-explanation].

Conclusion

We presented an explainable RBM approach for CF recommendations that achieves both accuracy and interpretability by learning an RBM network that tries to estimate accurate user ratings while also taking into account the explainability of an item to a user. Both rating prediction and explainability are integrated within one learning goal allowing to learn a network model that prioritizes the recommendation of items that are explainable.