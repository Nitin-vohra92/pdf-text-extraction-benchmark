=8000 =1000

Background

The Clock Drawing Test (CDT) - a simple pencil and paper test - has been used as a screening tool to differentiate normal individuals from those with cognitive impairment. The test takes less than two minutes, is easily administered and inexpensive, and is deceptively simple: it asks subjects first to draw an analog clock-face showing 10 minutes after 11 (the command clock), then to copy a pre-drawn clock showing the same time (the copy clock). It has proven useful in helping to diagnose cognitive dysfunction associated with neurological disorders such as Alzheimer's disease, Parkinson's disease, and other dementias and conditions. [\citep=freedman1994clock] [\citep=ashendorf2013bostonwithrandy]. The CDT is often used by neuropsychologists, neurologists and primary care physicians as part of a general screening for cognitive change [\citep=strub1985mental].

For the past decade, neuropsychologists in our group have been administering the CDT using a commercially available digitizing ballpoint pen (the DP-201 from Anoto, Inc.) that records its position on the page with considerable spatial (±  0.005 cm) and temporal (13ms) accuracy, enabling the analysis of not only the end product - the drawing - but also the process that produced it, including all of the subject's movements and hesitations. The resulting test is called the digital Clock Drawing Test (dCDT). Figure [\ref=fig:ExampleClockA] and Figure [\ref=fig:ExampleClockP] illustrate clock drawings from a subject in the memory impairment group, and a subject diagnosed with Parkinson's disease, respectively.

Existing Scoring Systems

There are a variety of methods for scoring the CDT, varying in complexity and the types of features they use. They often take the form of systems that add or subtract points based on features of the clock, and often have the additional constraint that the (n + 1)th feature matters only if the previous n features have been satisfied, adding a higher level of complexity in understanding the resulting score. A threshold is then used to decide whether the test gives evidence of impairment.

While the scoring system are typically short and understandable by a human, the features they attend to are often expressed in relatively vague terms, leading to potentially lower inter-rater reliability. For example, the Rouleau [\citep=rouleau1992quantitative] scoring system, shown in Table [\ref=table:Rouleau], asks whether there are "slight errors in the placement of the hands" and whether "the clockface is present without gross distortion".

In order to benchmark our models for the dCDT against existing scoring systems, we needed to create automated versions of them so that we could apply them to our set of clocks. We did this for seven of the most widely used existing scoring systems [\citep=souillard2015learning] by specifying the computations to be done in enough detail that they could be expressed unambiguously in code. As one example, we translated "slight errors in the placement of the hands" to "exactly two hands present AND at most one hand with a pointing error of between ε1 and ε2 degrees", where the εi are thresholds to be optimized. We refer to these new models as operationalized scoring systems.

An Interpretable Machine Learning Approach

Stroke-Classification and Feature Computation

The raw data from the pen is analyzed using novel software developed for this task [\citep=davis2014think] [\citep=davis2014method] [\citep=cohen2014digital]. An algorithm classifies the pen strokes as one or another of the clock drawing symbols (i.e. clockface, hands, digits, noise); stroke classification errors are easily corrected by human scorer using a simple drag-and-drop interface. Figure [\ref=fig:ExampleClockClassified] shows a screenshot of the system after the strokes in the command clock from Figure [\ref=fig:ExampleClockA] have been classified.

Using these symbol-classified strokes, we compute a large collection of features from the test, measuring geometric and temporal properties in a single clock, both clocks, and differences between them. Example features include:

The number of strokes; the total ink length; the time it took to draw; and the pen speed for various clock components; timing information is used to measure how quickly different parts of the clock were drawn; latencies between components.

The length of the major and minor axis and eccentricity of the fitted ellipse; largest angular gaps in the clockface; distance and angular difference between starting and ending points of the clock face.

Digits that are missing or repeated; the height and width of digit bounding boxes.

Omissions or repetitions of hands; angular error from their correct angle; the hour hand to minute hand size ratio; the presence and direction of arrowheads.

We also selected a subset of our features that we believe are both particularly understandable and that have values easily verifiable by clinicians. We expect, for example, that there would be wide agreement on whether a number is present, whether hands have arrowheads on them, whether there are easily noticeable noise strokes, or if the total drawing time particularly high or low. We call this subset the Simplest Features.

Traditional Machine Learning

We focused on three categories of cognitive impairment, for which we had a total of 453 tests: memory impairment disorders (MID) consisting of Alzheimer's disease and amnestic mild cognitive impairment (aMCI); vascular cognitive disorders (VCD) consisting of vascular dementia, mixed MCI and vascular cognitive impairment; and Parkinson's disease (PD). Our set of 406 healthy controls (HC) comes from people who have been longitudinally studied as participants in the Framingham Heart Study.

Our task is screening: we want to distinguish between healthy and one of the three categories of cognitive impairment, as well as a group screening, distinguish between healthy and all three conditions together.

We started our machine learning work by applying state-of-the-art machine learning methods to the set of all features. We generated classifiers using multiple machine learning methods, including CART [\citep=Breiman84], C4.5 [\citep=Quinlan93], SVM with gaussian kernels [\citep=Joachims/98c], random forests [\citep=breiman2001random], boosted decision trees [\citep=friedman2001greedy], and regularized logistic regression [\citep=fan2008liblinear]. We used stratified cross-validation to divide the data into 5 folds to obtain training and testing sets. We further cross-validated each training set into 5 folds to optimize the parameters of the algorithm using grid search over a set of ranges. We chose to measure quality using area under the receiver operator characteristic curve (AUC) as a single, concise statistic.

We found that the AUC for best classifiers ranged from 0.88 to 0.93. We also ran our experiment on the subset of Simplest Features, and found that the AUC ranged from 0.82 to 0.83. Finally, we measured the performance of the operationalized scoring systems; the best ones ranged from 0.70 to 0.73. Complete results can be found in Table [\ref=table:auc].

Human Interpretable Machine Learning

Definition of Interpretability

To ensure that we produced models that can be used and accepted in a clinical context, we obtained guidelines from clinicians. This led us to focus on three components in defining complexity:

Computational complexity: the models should be relatively easy to compute, requiring a small number of simple operations, similar to the existing manual scoring systems. Those systems have on average 8 to 15 rules, with each rule containing on average one or two features. We thus focus on models that use fewer than 20 features, and have a simple form.

Understandability: the rationale for a decision made by the model should be easily understandable, so that the user can understand why the prediction was made and can easily explain it. Thus if several features are roughly equally useful in the model, the most understandable one should be used.

Ease of feature measurement: Features that can be easily understood and verified by eye should be prioritized; this lead to the creation of the Simplest Features subset mentioned above.

Supersparse Linear Interpretable Models

We use a recently developed framework, Supersparse Linear Interpretable Models (SLIM) [\citep=ustun2015supersparse], designed to create sparse linear models that have integer coefficients. The framework produces models that meet many of our interpretability goals. Integer coefficients allow for models that are more easily computable, have greater expository power, and have the same form as the scoring systems already in use; hard constraints on the coefficients allow us to set a hard limit on the number of variables used in the model, thus reducing computational complexity.

To improve model understandability, we added feature preferences by introducing an understandability penalty that indicates which features would be preferred over others when their performance is similar.

Given a dataset of N examples [formula], where observation [formula] and label yi∈{ - 1,1}, and an extra element with value 1 is included within each [formula] vector to act as the intercept term, we want to build models of the form [formula], where [formula] is a vector of integer coefficients. The framework determines the coefficients of the models by solving an optimization problem of the form:

[formula]

[formula]

The Loss function is:

[formula]

where ψi is 1 if an incorrect prediction is made. It penalizes misclassifications and allows to set relative costs for accuracy on the positive examples and accuracy on the negative examples by setting C+ and C-.

The interpretability penalty function [formula] is defined as

[formula]

The first term computes the count of the number of nonzero features, encouraging the model to use fewer features. The second term allows for the prioritization of certain features, helping to ensure that the most understandable features appear in the model. In particular, we defined an understandability penalty uj for each feature j by organizing our features into trees such that the children of each feature are those it depends on. For instance "total time to draw both clocks" has as children "total time to draw command clock" and "total time to draw copy clock." The height of a given node is the number of nodes traversed from the top of the tree to the given node. We define

[formula]

which produces a bias toward simpler features, i.e., those lower in the tree. The constants C0 and C1 trade off between sparsity and understandability.

Given the above formulation, we used stratified cross-validation to divide the data into 5 folds to obtain training and testing sets and further cross-validated each training set into 5 folds to optimize the parameters (C+, C-, C0, C1) using grid search. We ran our optimization problem on the set of simplest features and all features, with a hard upper bound of 10 features, to keep the resulting models interpretable.

The resulting AUCs ranged from 0.74 to 0.78 and 0.81 to 0.83, respectively. While this is lower than the traditional machine learning methods, it still outperforms existing scoring systems and, yet remains equally interpretable. As one example, the SLIM model for Memory Impairment screening containing only 9 binary features, yet it achieves an AUC score of 0.78 (Table [\ref=table:SLIMAH]).

Conclusion

The dCDT combined with machine learning techniques allows for a significantly better screening of cognitive conditions than the existing CDT scoring systems. Traditional machine learning methods have high accuracy, but by constraining our models with formats similar to existing scoring systems, we can still obtain a significant improvement in accuracy and remove any subjectivity, while maintaining the human interpretability of the models.