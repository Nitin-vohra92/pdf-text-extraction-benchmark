Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval

Introduction

Embedding words as real-valued vectors enables the application of neural network (NN) based approaches for several supervised natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, semantic role labelling etc. [\cite=Collobert:2011]. The real-valued vectors are fed into the input layers of NNs designed for various supervised tasks. The word embeddings themselves are obtained by unsupervised pre-training from a large corpus of text, by making use of recurrent neural networks (RNNs) [\cite=Bengio:2003] [\cite=Mikolov13]. The specific technique of word embeddings, known as word2vec, uses the core idea of negative sampling which massively increases the efficiency of the embedding process so as to make it scalable for large document collections [\cite=Mikolov13]. The main idea behind negative sampling is to design an objective function that seeks to maximize the similarity of the vector representation of the current word with another word sampled from the context of it, i.e. from a word window of a preset length around the current word. The objective function also seeks to minimize the similarity of the vector representation of the current word with another word drawn randomly from outside its context; hence the name negative sampling [\cite=Mikolov13].

A useful feature of word vectors pre-trained using word2vec is that vector addition (component wise addition) of two or more word vectors results in a vector that is close to the semantic meaning of the constituent words. This property of the embeddings has been used to address the proportional analogy task in NLP, such as predicting that 'Berlin' is the capital of 'Germany' given that 'Paris' is the capital of 'France', since vec(Berlin) is close to the vector vec(Paris) + vec(France) - vec(Germany). Word embedding thus encapsulates the useful information about the context of a word and effectively represents the semantic information of a word. It is thus tempting to use the real-valued vector representations of words to represent documents and queries in information retrieval (IR) IR and defining similarity measures between them.

However, a major difficulty in applying word vector embeddings in IR is to be able to devise an effective strategy for obtaining representations of compound units of text (in comparison to the atomic words), such as passages and documents for the purpose of indexing. Adding word vectors for deriving composed meanings has been shown to be beneficial for small units of text, e.g. for predicting query intent in search sessions [\cite=Mitra:2015]. However, this approach of adding the constituent word vectors of a document to obtain the vector representation of the whole document is not likely to be useful, because the notion of compositionality of the word vectors works well when applied over a relatively small number of words. A vector addition for composition does not scale well for a larger unit of text, such as passages or full documents, because of the broad context present within a whole document. A possible solution is to use an extension of 'word2vec', commonly called 'doc2vec', to obtain distributed representations of arbitrary large units of text, such as paragraphs or documents, where in addition to learning the vector representations of words, the RNN also learns of the vector representation of the current document with a modified objective function [\cite=LeM14]. Unfortunately, training 'doc2vec' on a collection of a large number of documents (as often encountered in IR) is practically intractable both in terms of speed and memory resources.

We address this issue from a different perspective. Instead of striving for a suitable method for obtaining a single vector representation of a large document of text and a query, so as to use the inner product similarity between them for scoring documents, we seek to develop a suitable similarity (or inverse distance) metric which makes use of the each individual embedded word vector in a document and a query. More specifically, we represent each document (and a query) as a set of word vectors, and use a standard notion of similarity measure between these sets. Typically, such set based distance measures, such as the single-link, complete-link, average-link similarities etc. are used in hierarchical agglomerative clustering (HAC) algorithms [\cite=manning2009]. Generally speaking, these similarities are computed as functions of the individual similarities between each constituent word vector pair of a document and a query set. We then combine this word-vector based similarity measure with a standard text based IR similarity measure in order to rank the retrieved documents in response to a query.

The remainder of the paper is organized as follows. In Section [\ref=sec:representation], we discuss how documents and queries are represented as sets of word vectors so as to compute the similarities between them. Section [\ref=sec:indexing] describes the index construction procedure to store and make use of the additional word vectors. Section [\ref=sec:eval] evaluates our proposed approach of document ranking with the word vector based similarity. Finally, Section [\ref=sec:concl] concludes the paper with directions for future work.

Document Representation

Documents as Mixture Distributions

The 'bag-of-words' (BoW) representation of a document within a collection treats the document as a sparse vector in the term space comprised of all terms in the collection vocabulary. The vector representation of each word, obtained with a word embedding approach, makes provision to view a document as a 'bag-of-vectors' (BoV) rather than as a BoW. With this viewpoint, one may imagine that a document is a set of words with one or more clusters of words, each cluster broadly representing a topic of the document. To model this behaviour, it can be assumed that a document is a probability density function that generates the observed document terms.

In particular, it is convenient to represent this density function as a mixture of Gaussians of p dimensions so that each word vector (of p dimensions) is an observed sample drawn from this mixture distribution. With the observed document words, this probability can be estimated with an expectation maximization (EM) algorithm, e.g. K-means clustering of the observed document vectors. The observed query vectors during retrieval can be considered to be points that are also sampled from this mixture density representation of a document. Documents can then be ranked by the posterior likelihood values of generating query vectors from their mixture density representations.

More specifically, let the BoW representation of a document d be Wd  =  {wi}|d|i = 1, where |d| is the number of unique words in d and wi is the ith word. The BoV representation of d is the set Vd  =  {xi}|d|i = 1, where [formula] is the vector representation of the word wi. Let each vector representation xi be associated with a latent variable zi, which denotes the topic or concept of a term and is an integer between 1 and K, where K, being a parameter, is the total number of topics or the number of Gaussians in the mixture distribution. These latent variables, zis, can be estimated by an EM based clustering algorithm such as K-means, where after the convergence of K-means on the set Vd, each zi represents the cluster id of each constituent vector xi. Let the points Cd  =  {μk}Kk = 1 represent the K cluster centres as obtained by the K-means algorithm.

Query Likelihoods

The posterior likelihood of the query to be sampled from the K mixture model of Gaussians, centred around the μk centroids, can then be estimated by the average distance of the observed query points, q = {qi}|q|i = 1, from the centroids of the clusters.

[formula]

In Equation [\ref=eq:avgsim], qi  ·  μk denotes the inner product between the query word vector qi and the kth centroid vector μk. This measure is commonly known as the centroid similarity or average inter-similarity in the literature, where it finds application to measure how similar two sets are during hierarchical agglomerative clustering (HAC) [\cite=manning2009].

Although there are multiple set-based similarity measures, such as the single-link, complete-link similarities etc., we report the average-link similarity in our experiments because it produced the best results. A likely reason for this metric to produce the best results is that it is not largely affected by the presence of outliers like single-link or complete-link algorithms (for more details, see the discussion in Chapter 17 of the book [\cite=manning2009]).

Intuitively speaking, the notion of set-based similarity is able to make use of the semantic distances between the constituent terms of a document and a given query so as to improve the rankings of retrieved documents. Note that a standard text-based retrieval model can only make use of the term overlap statistics between documents and queries, and cannot utilize the semantic distances between constituent terms for scoring documents.

Illustrative Examples

We now demonstrate the key idea of the usefulness of the set-based similarity of the constituent word vectors of documents and queries with illustrative examples. Consider the documents of Figure [\ref=fig:case1], where for illustrative purposes, we assume that the p = 2, i.e., each word is embedded in a two dimensional space. The individual word vectors of a document are shown with blue dots, whereas the query points are shown with red triangles. Note that the document in Figure [\ref=fig:case1_a] has one cluster and all the three query points are relatively close to the centroid of this cluster. In contrast, the document in Figure [\ref=fig:case1_b] is comprised of two clusters (one blue and the other red). In this case, the query terms are relatively far away from the central theme of the document, i.e. the position of the centroid vector of the predominant topic of the document, i.e the centroid of the blue words. This indicates that the posterior query likelihood for the document in Figure [\ref=fig:case1_b] is lower than that of Figure [\ref=fig:case1_a]. This means that the document of Figure [\ref=fig:case1_a] will be ranked higher for the example query than the document on the right. Intuitively speaking, the closer the query terms are to the clusters of the constituent word vectors of a document, the higher is the similarity of the document with the query.

Figure [\ref=fig:case2] demonstrates two example cases where 4 distinct clusters of terms (topics) can be seen. The query likelihood for the document in Figure [\ref=fig:case2_a] is lower than that of the document in Figure [\ref=fig:case2_b]. This is because the observed query terms in Figure [\ref=fig:case2_b] are close to the cluster centres on the bottom-left and bottom-right, whereas for the example scenario in Figure [\ref=fig:case2_a], the query point q2 is relatively far away from all the centroids. This has the implication that the information need aspect of the query term q2 is not adequately expressed in the contents of the document in Figure [\ref=fig:case2_a].

Combining with Text Likelihood

Equation [\ref=eq:avgsim] gives a new way to compute query likelihoods in comparison to the standard method of computing P(d|q) using the BoW representation model using the standard language modeling (LM) [\cite=hiemstra00] [\cite=PonteThesis]. In LM, the prior probability of generating a query q from a document d is given by a Multinomial sampling probability of obtaining a term qi from d with prior belief λ (a parameter for LM), coupled with a probability of sampling the term qi from the collection with prior belief (1 - λ). Let this probability be represented by PLM(d|q) as shown in Equation [\ref=eq:lm].

[formula]

In order to combine the standard LM query likelihood probability PLM(d|q) of Equation [\ref=eq:lm] with the query likelihood estimated using the mixture model density estimate of the word vectors, as shown in Equation [\ref=eq:avgsim], we introduce another indicator binary random variable to denote the individual believes of the text based similarity and the word vector based similarity. If the probability of this indicator variable is denoted by α, we obtain the combined query likelihood as a mixture model of the two respective query likelihoods, one for the text and the other for the embedded word vectors.

[formula]

Index Construction Details

In our approach, as discussed in Section [\ref=sec:representation], the embedded representation of the constituent terms of a document are used for estimating the similarity between a document and a query in combination with the BoW based LM similarity. In this section, we discuss how can the constituent word vector representations be stored in the index so that the cluster centres for each document can be computed efficiently during retrieval time in order to compute its similarity with the query.

Pre-Clustering the Collection Vocabulary

Word embedding techniques ensure that semantically related words are embedded in close proximity. This means that the K-means clustering algorithm executed once for the whole vocabulary of the words in a collection is able to cluster the words into distinct semantic classes. Each semantic class represents a global topic (cluster id of a term) of the whole collection.

In order to obtain per-document topics from these global topic classes, while indexing each document, we perform a table look-up to retrieve the cluster id of each constituent term. Note that since the cluster id, c(w) of each term w is an integer in

[formula]

Evaluation

In this section, we report our experimental investigation of the proposed method. The objective of our experiments is to investigate whether incorporating semantic information (relative semantic similarities between the terms), when used in combination with the standard text based similarity can further improve retrieval effectiveness.

Experimental Setup

Our experiments were conducted on the TREC ad-hoc test collections. The topic sets used for our experiments constitute the TREC 6, 7, 8 and Robust ad-hoc task topics. The characteristics of the document and the query sets are outlined in Table [\ref=tab:trec].

As our baseline retrieval model for initial retrieval, we used the standard LM with Jelinek Mercer smoothing [\cite=hiemstra00] [\cite=Zhai2004], which is distributed as a part of Lucene, an open source IR framework. The proposed methods are also implemented as a part of Lucene. The smoothing parameter for LM λ is set to the optimal value of 0.4 after varying it in the range

[formula]

, we found out that the optimal value of this parameter is 0.4, and therefore we report the results with this particular setting of α.

To see the benefits of pseudo-relevance feedback (PRF) on the initial retrieval results, we conducted additional experiments by applying relevance model based feedback [\cite=lavrenko_croft2001] [\cite=rm3] on both the sets of initial retrieval results, i.e. the baseline LM and LM in combination with word vector based query likelihood.

Results

In this section, we report the empirical performance of the proposed method on each dataset. The results of our experiments are reported in Table [\ref=tab:res]. We note that the results with K = 100 are the best for the TREC 8 and the TREC Robust topic sets. In fact, these results are significantly better than the results obtained with only text based similarity, which in this case is the standard LM retrieval model. TREC-8 and TREC Robust results show consistent improvements in both recall and precision at top ranks.

We also note that on the TREC 8 and the Robust topic sets, the clustered representation of the documents produces much better results in comparison to the method of considering a very fine-grained representation of a document by using each constituent word vector for the similarity computation with the query. Somewhat surprisingly, the results with K = 1, i.e., representing each document by a single point (the average of each word in the document) is quite close in performance to K = 100, which makes us believe that we need to use much higher values of K for clustering the whole vocabulary set since it is likely that 100 concepts or semantic classes is too small to capture the wide diversity in the vocabulary of the TREC collection.

Another observation is that the results with K = 100 are not the best for the TREC 6 and the TREC 7 topic sets. The results with K = 100 show marginal improvements. Surprisingly, the single-point representation of the documents produces better results than with K = 100.

From Table [\ref=tab:res], we observe that the initial retrieval results obtained with word vector based likelihood are in general improved by application of RLM based PRF, e.g. the TREC 7 and the TREC 8 results. However, for TREC 6 and Robust, the best PRF results are obtained with the LM based initial retrieval.

Figure [\ref=fig:qq] investigates the effect of word vector based query likelihood on individual queries. Each vertical bar in the plots of Figure [\ref=fig:qq] denotes the difference between the average precision (AP) values of the retrieval results obtained with the combination of LM with the word vector based similarity and LM alone. Ideally, we would want the vertical bars to be higher over X axis. From Figure [\ref=fig:qq] it appears that the combination turns out to be beneficial for a number of queries while hurting the performance of others. It may be argued that the results can further be improved with selective application of the word vector based query likelihood. In order to see how much benefit we can get in the ideal situation, we assume the existence of an oracle which always correctly 'selects' the best approach, i.e. chooses the best result given two approaches, one the baseline LM and the other with the word vector based likelihood in combination with it. Table [\ref=tab:oracle] shows the best results that can, in principle, be achieved with using the word vector based similarities, on each topic set.

Conclusions and Future work

In this paper, we have proposed a novel approach to represent queries and documents as sets of embedded word vectors. A document is represented as a probability density function of a mixture of Gaussians. The posterior query likelihood is estimated by the average distance of the query points from the centroids of these Gaussians. This word vector based query likelihood is then combined with the standard LM based query likelihood for document ranking. Experiments on standard text collections showed that the combined similarity measure almost always outperforms (often significantly) the LM (text based) similarity measure.

As a possible future work, we would like to apply our proposed representation of queries and documents for relevance feedback and query expansion. We would also like to explore various notions of distance measures (e.g. [\cite=weinberger_distance]). It would also be interesting to see the performance of this set-based representation obtained with a multi-sense word embedding method, such as [\cite=neelakantan_efficient].

Acknowledgements

This research is supported by Science Foundation Ireland (SFI) as a part of the ADAPT Centre at DCU (Grant No: 13/RC/2106) and by a grant under the SFI ISCA India consortium.