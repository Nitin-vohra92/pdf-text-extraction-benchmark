=2500

Yet Another Proof of the Entropy Power Inequality

(first version June 8, 2016)

Introduction

The entropy power inequality (EPI) was stated by Shannon [\cite=Shannon48] in the form

[formula]

for any independent n-dimensional random vectors [formula] with densities and finite second moments, with equality if and only if X and Y are Gaussian with proportional covariances. Shannon gave an incomplete proof; the first complete proof was given by Stam [\cite=Stam59] using properties of Fisher's information. A detailed version of Stam's proof was given by Blachman [\cite=Blachman65]. A very different proof was provided by Lieb [\cite=Lieb78] using Young's convolutional inequality with sharp constant. Dembo, Cover and Thomas [\cite=DemboCoverThomas91] provided a clear exposition of both Stam's and Lieb's proofs. Another variation of Stam's proof for one-dimensional variables can also be found in [\cite=CarlenSoffer91]. Szarek and Voiculescu [\cite=SzarekVoiculescu00] gave a proof related to Lieb's but based on a variant of the Brunn-Minkowski inequality. Guo, Shamai and Verdú gave another proof based on the relation [\cite=GuoShamaiVerdu06] [\cite=VerduGuo06]. A similar proof based on a relation between divergence and causal MMSE was given by Binia [\cite=Binia07]. Yet another proof based on properties of mutual information was proposed in [\cite=Rioul07] [\cite=Rioul11]. A more involved proof based on a stronger form of the EPI that uses spherically symmetric rearrangements, also related to Young's inequality with sharp constant, was recently given by Wang and Madiman [\cite=WangMadiman13] [\cite=WangMadiman14].

As first noted by Lieb [\cite=Lieb78], the above Shannon's formulation [\eqref=epishannon] of the EPI is equivalent to

[formula]

for any 0 < λ < 1. All known proofs of the EPI used this form. Proofs of the equivalence can be found in numerous papers, e.g., [\cite=DemboCoverThomas91], [\cite=VerduGuo06], [\cite=Rioul11], and [\cite=MadimanMelbourneXu16].

There are a few technical difficulties for proving [\eqref=epi] which are not always explicitly stated in previous proofs. First of all, one should check that for any random vector X with finite second moments, the differential entropy h(X) is always well-defined--even though it could be equal to -    ∞  . This is a consequence of [\cite=Rioul11]; see also Appendix [\ref=A] for a precise statement and proof. Now if both independent random vectors X and Y have densities and finite second moments, so has [formula] and both sides of [\eqref=epi] are well-defined. Moreover, if either h(X) or h(Y) equals -    ∞   then [\eqref=epi] is obviously satisfied. Therefore, one can always assume that X and Y have finite differential entropies.

Another technical difficulty is the requirement for smooth densities. More precisely, as noted in [\cite=WangMadiman14] some previous proofs use implicitly that for any X with arbitrary density and finite second moments and any Gaussian Z independent of X, [formula]. This was proved explicitly in [\cite=Rioul11] and [\cite=WangMadiman14] using the lower-semicontinuity of divergence. As a consequence, it is sufficient to prove the EPI for random vectors of the form [formula] (t > 0). Indeed, letting Z' be an independent copy of Z such that (Z,Z') is independent of (X,Y), the EPI written for [formula] and [formula] reads

[formula]

where [formula] is again identically distributed as Z and Z'. Letting t  →  0 we obtain the general EPI [\eqref=epi]. Now, for any random vector X and any t > 0, [formula] has a continuous and positive density. This can be seen using the properties of the characteristic function, similarly as in [\cite=Rioul11]; see Appendix [\ref=B] for a precise statement and proof. Therefore, as already noticed in [\cite=WangMadiman14], one can always assume that X and Y have continuous, positive densities.

One is thus led to prove the following version of the EPI.

Let X,Y be independent random vectors with continuous, positive densities and finite differential entropies and second moments. For any 0 < λ < 1,

[formula]

with equality if and only if X,Y are Gaussian with identical covariances.

Previous proofs of the EPI can be classified into two categories:

proofs in [\cite=Stam59] [\cite=Blachman65] [\cite=CarlenSoffer91] [\cite=GuoShamaiVerdu06] [\cite=VerduGuo06] [\cite=Binia07] [\cite=Rioul07] [\cite=Rioul11] rely on the integration over a path of a continuous Gaussian perturbation of some data processing inequality using either Fisher's information, the minimum mean-squared error (MMSE) or mutual information. As explained in [\cite=Rioul07], [\cite=Rioul11] and [\cite=MadimanBarron07], it is interesting to note that in this context, Fisher's information and MMSE are complementary quantities;

proofs in [\cite=Lieb78] [\cite=SzarekVoiculescu00] [\cite=WangMadiman13] [\cite=WangMadiman14] are related to Young's inequality with sharp constant or to an equivalent argumentation using spherically symmetric rearrangements, and/or the consideration of convergence of Rényi entropies.

It should also be noted that all available proofs of [\eqref=epi] do not settle the equality case--that equality in [\eqref=epi] holds only for Gaussian random vectors with identical covariances.

In this paper, a simple proof of the Theorem is given that avoids both the integration over a path of a continuous Gaussian perturbation and the use of Young's inequality, spherically symmetric rearrangements, or Rényi entropies. It is based on a "Gaussian to not Gaussian" lemma proposed in [\cite=RioulCosta16] and is formally identical in one dimension (n = 1) and in several dimensions (n > 1). It also easily settles the equality case.

From Gaussian to Not Gaussian

Let [formula] be an n-dimensional random vector with continuous, positive density and let [formula] be any n-variate Gaussian vector, e.g., [formula].

There exists a diffeomorphism Φ whose Jacobian matrix is triangular with positive diagonal elements such that X has the same distribution as Φ(X*).

The essential content of this lemma is well known in the theory of convex bodies [\cite=MilmanSchechtman86], [\cite=Schneider93], [\cite=GiannopoulosMilman04], [\cite=ArtsteinGiannopoulosMilman15] where Φ is known as the Knöthe map between two convex bodies. The difference with Knöthe map is that in Lemma [\ref=NG2G], the determinant of the Jacobian matrix need not be constant. For completeness we present two proofs in the Appendix. The first proof in Appendix  [\ref=NG2G1] follows Knöthe [\cite=Knothe57]. The second proof in Appendix  [\ref=NG2G2] is based on the (multivariate) inverse sampling method. It should be noted that these proofs do not use the Gaussian assumption on X* which could be instead any n-dimensional random vector with continuous, positive density.

Let Φ' be the Jacobian (i.e., the determinant of the Jacobian matrix) of Φ. Since Φ' > 0, the usual change of variable formula reads

[formula]

A simple application of this formula gives the following.

If h(X) is finite,

[formula]

Proof of the Entropy Power Inequality

Let X*,Y* be i.i.d. Gaussian having any Gaussian density g, e.g., [formula]. For any 0 < λ < 1, [formula] has the same Gaussian density g as X* and Y* and, therefore,

[formula]

Subtracting both sides from both sides of [\eqref=epi] one is led to prove that [formula] is nonnegative.

Let Φ be as in Lemma [\ref=NG2G], so that X has the same distribution as Φ(X*). Similarly let Ψ be such that Y has the same distribution as Φ(Y*). On one hand, letting f be the density of [formula], which is the same as that of [formula], one has

[formula]

On the other hand, by Lemma [\ref=CVH],

[formula]

We now compare [\eqref=1] and [\eqref=2]. Toward this aim we make the change of variable (X*,Y*)  →  (,) where

[formula]

Again , are i.i.d. Gaussian with density g and

[formula]

To simplify the notation define

[formula]

where [formula] is the Jacobian of the transformation [formula]. By the change of variable formula [\eqref=cvf],

[formula]

hence [formula] is a density. The right-hand side of [\eqref=1] becomes

[formula]

since the divergence is nonnegative. The right-hand side of [\eqref=2] becomes

[formula]

where in [\eqref=jensen] we have used Jensen's inequality λ log a + (1 - λ) log b  ≤   log (λa + (1 - λ)b) on each component, and in [\eqref=triangle] the fact that (since Φ and Ψ have triangular Jacobian matrices with positive diagonal elements) the Jacobian matrix of [formula] is also triangular with positive diagonal elements. This proves [\eqref=epi].

The Case of Equality

Equality in [\eqref=epi] holds if and only if both [\eqref=divergence] and [\eqref=jensen] are equalities. Equality in [\eqref=jensen] holds if and only if for all [formula],

[formula]

Since X* and Y* are independent Gaussian random vectors this implies that [formula] and [formula] are constant and equal. Thus in particular [formula] is constant. Now equality in [\eqref=divergence] holds if and only if for all [formula],

[formula]

From [\eqref=deff], since [formula] is constant and f is not, this implies that [formula] does not depend on [formula]. Thus for any [formula],

[formula]

which implies

[formula]

hence [formula] and [formula] are constant and equal. Therefore, Φ and Ψ are linear transformations, equal up to an additive constant. It follows that Φ(X*) and Φ(Y*) (hence X and Y) are Gaussian with identical covariances. This ends the proof of the Theorem.

The differential entropy [formula] of a random vector X with density f is not always well-defined because the negative and positive parts of the integral might be both infinite, as in the example f(x) = 1 / (2x log 2x) for 0 < x < 1 / e and e < x <  +   ∞  , and = 0 otherwise [\cite=Rioul11]. The following result shows that if X has finite second moments, then the positive part of the integral is finite so that h(X) is well-defined and <  +   ∞  .

Let X be an random vector with density f and finite second moments. Then [formula] is well-defined and <  +   ∞  .

Let Z be any Gaussian vector with density g > 0. On one hand, since X has finite second moments, the integral [formula] is finite. On the other hand, since g never vanishes, the probability measure of X is absolutely continuous with respect to that of Z. Therefore, the divergence [formula] is equal to the integral [formula]. Since the divergence is non-negative, it follows that [formula] is well-defined and <  +   ∞  .

The following result is stated for an arbitrary random vector X. It is not required that X have a density. It could instead follow e.g., a discrete distribution. It is stated in [\cite=GengNair14] that strong smoothness properties of distributions of Y = X + Z for independent Gaussian Z are "very well known in certain mathematical circles" but it seems difficult to find a reference.

Let X be any random vector and Z be any independent Gaussian vector with density g > 0. Then Y = X + Z has a bounded, positive, indefinitely differentiable (hence continuous) density that tends to zero at infinity, whose all derivatives are also bounded and tend to zero at infinity.

Taking characteristic functions, [formula], where [formula] is the Fourier transform of the Gaussian density g. Now ĝ(t) is also a Gaussian function with exponential decay at infinity and [formula]. Therefore, the Fourier transform of the probability measure of Y (which is always continuous) also has exponential decay at infinity. In particular, this Fourier transform is integrable, and by the Riemann-Lebesgue lemma, Y has a bounded continuous density which tends to zero at infinity. Similarly, for any monomial tα, [formula] is integrable and is the Fourier transform of the αth partial derivative of the density of Y, which is, therefore, also bounded continuous and tends to zero at infinity.

It remains to prove that the density of Y is positive. Let Z1, Z2 be independent Gaussian random vectors with density φ equal to that of [formula] so that Z has the same distribution as Z1 + Z2. By what has just been proved, X + Z1 follows a continuous density f. Since Y has the same distribution as (X + Z1)  +  Z2, its density is equal to the convolution product [formula]. Now φ is positive, and for any [formula], [formula] would imply that f vanishes identically, which is impossible.

First Proof of Lemma [\ref=NG2G]

We use the notation f for densities (p.d.f.'s). In the first dimension, for each [formula], define Φ1(x*1) such that

[formula]

Since the densities are continuous and positive, Φ1 is continuously differentiable and increasing; differentiating gives

[formula]

which proves the result in one dimension: X1 has the same distribution as Φ1(X*1) where [formula] is positive.

In the first two dimensions, for each x*1,x*2 in [formula], define Φ2(x*1,x*2) such that

[formula]

Again Φ2 is continuously differentiable and increasing in x*2; differentiating gives

[formula]

which proves the result in two dimensions. Continuing in this manner we arrive at

[formula]

which shows that [formula] has the same distribution as [formula]. The Jacobian matrix of Φ has the form

[formula]

where all diagonal elements are positive since by construction each Φk is increasing in x*k.

Second Proof of Lemma [\ref=NG2G]

We use the notation F for distribution functions (c.d.f.'s). We also note [formula] and let F- 1X2|X1(  ·  |x1) be the corresponding inverse function in the argument x2 for a fixed value of x1. Such inverse functions are well-defined since it is assumed that X is a random vector with continuous, positive density.

The inverse transform sampling method is well known for univariate random variables but its multivariate generalization is not.

Let [formula] be uniformly distributed on n. The vector Φ(U) with components

[formula]

has the same distribution as X.

By inverting Φ, it is easily seen that an equivalent statement is that the random vector [formula] is uniformly distributed in n. Clearly FX1(X1) is uniformly distributed in

[formula]

. The result follows by the chain rule.

By Lemma [\ref=MITSM], X has the same distribution as Φ(U), where each [formula] is increasing in uk. Similarly X* has the same distribution as Ψ(U), where both Φ and Ψ have (lower) triangular Jacobian matrices [formula] with positive diagonal elements. Then X has the same distribution as Φ(Ψ- 1(X*)). By the chain rule for differentiation [formula] has Jacobian matrix [formula]. This product of (lower) triangular matrices with positive diagonal elements and is again (lower) triangular with positive diagonal elements.