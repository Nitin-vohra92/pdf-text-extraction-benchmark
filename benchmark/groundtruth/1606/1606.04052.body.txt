Dialog state tracking, a machine reading approach using a memory-enhanced neural network

Introduction

One of the core components of state-of-the-art and industrially deployed dialog systems is a dialog state tracker. Its purpose is to provide a compact representation of a dialog produced from past user inputs and system outputs which is called the dialog state. The dialog state summarizes the information needed to successfully maintain and finish a dialog, such as users' goals or requests. The precise definition of the state depends on the associated dialog task. In the simplest case of a so-called slot-filling schema, the state is composed of a predefined set of variables with a predefined domain of expression for each of them. As a matter of fact, in the recent context of end-to-end machine trainable dialog systems, state tracking remains a central element of such architectures [\cite=WenGMRSUVY16]. Indeed, in the most general case, effective dialog systems tend to include a tracking mechanism which is able to accurately accumulate evidence over the sequence of turns of a dialog and must adjust the dialog state according to its observations. The goal of the dialog system is to efficiently instantiate each of these variables thereby performing an associated task and satisfying the corresponding intent of the user. Because of the reasoning capability that can be required in such task, we propose to formalize the state tracking problem as a particular instance of a machine reading problem. As far as our knowledge goes, it is the first attempt to explicitly frame the task of dialog state tracking as a machine reading problem. Following this direction, we extend the current definition of dialog state dataset by assuming the true dialog state is only available at the end of each dialog. On the other hand, such formalization allows for the implementation of approximate reasoning capability that has been shown to be crucial for any machine reading tasks [\cite=WestonBCM15].

Roadmap: This paper is structured as follows, Section [\ref=sec:pb] recalls the main definitions associated to transactional dialogs and describes the associated problem of statistical dialog state tracking with both the generative and discriminative approaches. At the end of this section, the current limitations of the current models in terms of necessary annotations and reasoning capabilities are addressed. Then, Section [\ref=sec:prop] depicts the proposed machine reading model for dialog state tracking and proposes to extend a state of the art dialog state tracking dataset, DSTC-2, to several simple reasoning capabilities. Section [\ref=sec:xp] illustrates the approach with experimental results obtained using a state of the art benchmark for dialog state tracking.

Dialog state tracking

In this section, the main definitions of a dialog state tracking task are recalled and the two main learning based approaches are described and discussed.

Main definitions

A dialog state tracking task is formalized as follows: at each turn of a dyadic dialog, the dialog agent chooses a dialog act d to express and the user answers with an utterance u. In the simplest case, the dialog state at each turn of a given dialog is defined as a distribution over a set of predefined variables, which define the structure of the state as mentioned in [\cite=Williams05]. This classic state structure is commonly called slot filling or semantic frame. In this context, the state tracking task consists of estimating the value of a set of predefined variables in order to perform a procedure or transaction which is, in fact, the purpose of the dialog. Typically, a natural language understanding module processes the user utterance and generates an N-best list [formula], where di is the hypothesized user dialog act and fi is its confidence score. Various approaches have been proposed to define dialog state trackers. The traditional methods used in most commercial implementations use hand-crafted rules that typically rely on the most likely result from an NLU module as described in [\cite=YehDJRRPLTBM14] and hardly models uncertainty. However, these rule-based systems are prone to frequent errors as the most likely result is not always the correct one  [\cite=Williams14]. More recent methods employ statistical approaches to estimate the posterior distribution over the dialog states allowing them to leverage on the uncertainty of the results of the NLU module.

In the simplest case where no ASR and NLU modules are employed, as in a text based dialog system as proposed in [\cite=Henderson13a] the utterance is taken as the observation using a so-called bag of words representation. If an NLU module is available, standardized dialog act schemas can be considered as observations as in [\cite=bunt10]. Furthermore, if prosodic information is available from the ASR component of the dialog system as in [\cite=MiloneR03], it can also be considered as part of the observation definition. A statistical dialog state tracker maintains, at each discrete time step t, the probability distribution over states, b(st), which is the system's belief over the state. The general process of slot-filling, transactional dialog management is summarized by the following sequence of steps. First, intent detection is typically an NLU problem consisting of identifying the task the user wants the system to accomplish. This first step determines the set of variables to instantiate during the second step, which is the slot-filling process. This type of dialog management assumes that a set of variables are required for each predefined intention. Second, the actual slot filling process is composed of the cyclic tasks of information gathering and integration, in other words - dialog state tracking. Finally, once all the variables have been correctly instantiated, a common practice in dialog systems is to perform a last general confirmation of the task desired by the user before finally executing the requested task. In such framework, the purpose is to estimate as early as possible in the course of a given dialog the correct instantiation of each variable. In the following, we will assume the state is represented as a concatenation of zero-one encoding of the values for each variable defining the state. Furthermore, in the context of this paper, only the bag of words has been considered as an observation at a given turn but dialog acts or detected named entity provided by an SLU module could have also been incorporated as evidence.

Two statistical approaches have been considered for maintaining the distribution over a state given sequential NLU output. First, the discriminative approach aims to model the posterior probability distribution of the state at time t + 1 with regard to state at time t and observations z1:t. Second, the generative approach attempts to model the transition probability and the observation probability in order to exploit possible interdependencies between hidden variables that comprise the dialog state.

Generative Dialog State Tracking

A generative approach to dialog state tracking computes the belief over the state using Bayes' rule, using the belief from the last turn b(st - 1) as a prior and the likelihood given the user utterance hypotheses p(zt|st), with zt the observation gathered at time t. In prior works [\cite=Williams05], the likelihood is factored and some independence assumptions are made:

[formula]

A typical generative model of a dialog state tracking process using a factorial hidden Markov model proposed by [\cite=gj97]. The shaded variables are the observed dialog turns and each unshaded variable represents a single variable describing the task dependent variables. In this family of approaches, scalability is considered as one of the main issues. One way to reduce the amount of computation is to group the states into partitions, as proposed in the Hidden Information State (HIS) model of [\cite=GasicY11]. Other approaches to cope with the scalability problem in dialog state tracking is to adopt a factored dynamic Bayesian network by making conditional independence assumptions among dialog state components, and then using approximate inference algorithms such as loopy belief propagation as proposed in [\cite=ThomsonY10] or a blocked Gibbs sampling as in [\cite=RauxM11]. To cope with such limitations, discriminative methods of state tracking presented in the next part of this section aim at directly model the posterior distribution of the tracked state using a choosen parametric form.

Discriminative Dialog State Tracking

The discriminative approach of dialog state tracking computes the belief over a state via a trained parametric model that directly represents the belief b(st + 1)  =  p(ss + 1|st,zt). Maximum Entropy has been widely used in the discriminative approach as described in [\cite=MetallinouBW13]. It formulates the belief as follows:

[formula]

where η is the normalizing constant, [formula] is the history of user dialog acts, [formula], the system dialog acts, [formula], and the sequence of states leading to the current dialog turn at time t. Then, φ(.) is a vector of feature functions on x and s, and finally, w is the set of model parameters to be learned from annotated dialog data. According to this formulation, the posterior computation has to be carried out for all possible state realizations in order to obtain the normalizing constant η. This is not feasible for real dialog domains, which can have a large number of variables and possible variable instantiations. So, it is vital to the discriminative approach to reduce the size of the state space. For example, [\cite=MetallinouBW13] proposes to restrict the set of possible state variables to those that appeared in NLU results. Finally, deep neural models, performing on a sliding window of features extracted from previous user turns, have also been proposed in [\cite=Henderson14]. Of the current literature, this family of approaches have proven to be the most efficient for publicly available state tracking datasets.

Current limitations

Using error analysis [\cite=HendersonTW14], several limitations have been observed in the application of the current types of inference model. On one hand, current models tend to fail at considering long-tail dependencies that can occur on dialogs as coreferences and inter-utterances informations even with the usage of recurrent network models [\cite=Henderson2014d]. On the other hand, reasoning capabilities, as required in machine reading applications [\cite=PoonD10] [\cite=EtzioniBC07] [\cite=BerantSCLHHCM14] [\cite=WestonBCM15] remain absent in these classic formalizations of dialog state tracking. In the next section, we present a model of dialog state tracking that aims at leveraging on the current advances of memory-enhanced neural networks and their approximate reasoning capabilities that seems particularly adapted to the sequential, long range dependency equipped and sparse nature of complex dialog state tracking tasks. Furthermore, this model allows to relax the hypothesis of utterance-level annotation to dialog-level annotation that corresponds to common pratices in industrial applications of transactional conversational user interfaces. Indeed, in such contexts, annotations tend to be associated with an overall dialog scale and not at the utterance one. In that context, producing such annotation required a dedicated effort that can be tedious.

A Machine reading formulation of dialog state tracking

We propose to formalize the problem of dialog state tracking as a particular case of machine reading [\cite=EtzioniBC07] [\cite=BerantSCLHHCM14]. Indeed, this task can be understood as the capability of infering a set of latent values l associated to a set of variables v related to a given dyadic or multi-party conversation d, from direct correlation and/or reasoning, using the course of exchanges of utterances, p(l|d,v). In this section, we recall the main definitions of the task of machine reading, then we mention the principal recent models of memory-enhanced neural network architectures designed to handle such tasks. Finally, we formalized the task of dialog state tracking as a machine reading problem and propose to solve it using a memory-enhanced neural architecture of inference.

Machine reading

The task of textual understanding has recently been formulated as a supervised learning problem [\cite=KumarISBEPOGS15] [\cite=HermannKGEKSB15]. The task consists in estimating the conditional probability p(a|d,q) of an answer a to a query q where d is a document. Such an approach requires a large training corpus of {document - query - answer} triples and until now such corpora have been limited to hundreds of examples [\cite=RichardsonBR13]. On the other hand, in the context of dialog state tracking, state updates at an utterance level are rarely provided off-the-shelf from a production environment. So an additional effort of specific annotation is often needed in order to train a state of the art statistical state tracking model [\cite=HendersonTW14]. In the context of dialog state tracking challenges, the dialogs are currently limited to two to three thousands of dialogs [\cite=HendersonTW14]. In the context of human-to-human dialog systems, like personal assistance or customer care, a transactional dialog is often summarized/annotated at the end by the entire state that corresponds to the situation encounted during the dialog. Because of that, the machine reading paradigm becomes a promising formulation for the general problem of dialog state tracking. Furthermore, current approaches and available datasets for state tracking do not explicitly cover reasoning capabilities such as temporal and spatial reasoning, couting, sorting and deduction. I suggest that in the future dataset dialogs expressing such specific abilities should be developed. In this last part, several reasoning enhancements are suggested to the DSTC-2 dataset.

Memory-enhanced neural networks

Several neural network models have very recently been developed with memory capability enhancement [\cite=GravesWD14] [\cite=JoulinM15] [\cite=SukhbaatarSWF15] [\cite=KumarISBEPOGS15]. Briefly, they share the particularity of taking a discrete set of inputs [formula] that have to be stored in the memory, a query q, and outputs an answer a. Each of the xi, q, and a contains symbols coming from a dictionary with V words. The model writes all x to the memory up to a fixed buffer size, and then it finds a continuous representation for the x and q. The continuous representation is then processed via multiple hops to output a distribution over a. This allows backpropagation of the error signal through multiple memory accesses back to the input during training. As mentioned in [\cite=WestonCB14], the main point of such family of approach is to combine the successful, mainly stochastic gradient-based, learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. In that sense, the model is trained to learn how to operate effectively with a memory component and enable it to capture more complex reasoning dynamics that other model lack. In the next section, we show how the task of dialog state tracking can be formalized as machine reading task and solved using such memory enhanced model.

Dialog reading model for state tracking

In this section, we propose to formalize dialog state tracking using the paradigm of machine reading. As far as our knowledge goes, it is the first attempt to apply this approach and develop a specific dataset format, detailed in Section [\ref=sec:xp], from an existing and publicly available dialog state tracking challenge dataset to fulfill this task. Assuming (1) a dyadic dialog d, (2) a state composed with (2a) a set of variables vi with [formula]and (2b) a set of corresponding assigned values li. One can define a query qv = l that corresponds to the specific instantiation of a variable to a value in the context of a dialog p(qvi = li|d). In such context, a dialog state tracking task consists in determining for each variable v, [formula], with L the specific domain of expression of a variable vi. In additional to the actual dataset, we propose to tackle four general reasoning tasks using DSTC-2 dataset as a starting point. In such way, we leverage on the dataset of DSTC-2 to create more complex reasoning task than the ones present in the original dialogs of the dataset by performing deterministic and stochastic conversions of the corpus. Obviously, the goal is to develop resolution algorithms that are not dedicated to a specific reasoning task but inference models that will be as generic as possible.

Single Supporting Fact : This first task corresponds to the most classic view of dialog state tracking. It consists of questions where a previously given single supporting fact, potentially amongst a set of other irrelevant facts, provides the answer. This kind of task was already employed in [\cite=WestonCB14] in the context of a virtual world. At the difference of the proper state tracking task, the true variable state is given at the end of the overall sequence. The use of the supporting fact corresponds to the same information than the one given in the original dataset. In that sense, the result obtained to such task are comparable with the state of the art approaches. Table [\ref=tab:food] given an example of such dialog for the case of the food slot of the DSTC-2 dataset.

Yes/No Questions : This task tests, on some of the simplest questions possible, like confirming the value of a given slot, the ability of a model to answer true/false type questions like "Is the food italian ?". The conversion of a dialog to such format is deterministic regarding the fact that the utterances and corresponding true states are known at each utterance of a given dialog. Table [\ref=tab:yesno] is an example if such conversion.

Indefinite Knowledge : This task tests a complex natural language construction. It tests if statements can be models in order to describe possibilities rather than certainties, as proposed in [\cite=WestonCB14], in our case the answer will be "maybe" to the question "Is the price-range required moderate ?". In the case of state tracking, it will allow to seamlessly deal with unknown information about the dialog state. Table [\ref=tab:ik] gives an example of such conversion.

Counting and Lists/Sets : This last task tests the capacity of the model to perform simple counting operations, by asking about the number of objects with a certain property, e.g. "How many area are requested ?". Similarly, the ability to produce a set of single word answers in the form of a list, e.g. "What are the area requested ?". Table [\ref=tab:ct] gives an example of such conversion.

We believe more reasoning capabilities need to be explore in the future, like spacial and temporal reasoning or deduction as suggested in [\cite=WestonBCM15]. However, it will probably need the development of a new dedicated ressource. Another alternative could be to develop a question-answering annotation task based on a dialog corpus where reasoning task are present. The closest work to our proposal that can be cited is [\cite=BordesW16]. In this paper, the authors proposes to learn a so-called end-to-end learnable dialog system to infer the answer, from a finite set of eligible answers, by the dialog system w.r.t the current list of utterances of the dialog. The authors generate 5 artificial tasks of dialog however the reasoning capabilities are not explicitly addressed and the author explicitly claim that the resulting dialog system is not satisfactory yet. Indeed, we believe that having a proper dialog state tracker where a policy is built on top can guarantee dialog achievement by properly optimizing a reward function throughout a explicitly learnt dialog policy. indeed, in the case of proper end-to-end systems, the objective function is not explicitly defined [\cite=SerbanLCP15] and the resulting systems tend to be used in the context of chat-oriented and non-goal oriented dialog systems. In the next section, we present experimental results obtained on the basis of the DSTC-2 dataset and its conversion to the four mentioned reasoning tasks.

Experiments

This section is organized in two parts. In a first part, the generation of a machine reading type of dataset from the publicly available DSTC-2 corpus is presented. In a section part, experimental results are provided.

A machine reading dataset of the DSTC-2 corpus

We use the DSTC-2 dialog domain [\cite=williams13a] as a basis for evaluating the proposed approach. In this dialog corpus, the user queries a database of local restaurants by interacting with a dialog system. A dialog proceeds as follows: first, the user specifies constraints concerning the restaurant. Then, the system offers the name of a restaurant that satisfies the constraints. Finally, the user accepts the offer and requests additional information about the accepted restaurant. In this context, the dialog state tracker should be able to track several types of information that compose the state like the geographic area, the food type and the price range slots. In this paper, we restrict ourselves to tracking these variables, but our tracker can be easily setup to track others as well if they are properly specified. We adapt the dataset in order to correspond to a proper task of machine reading by only considering the annotation at a dialog-level instead of a utterance-level one. In that sense, the tracker task consists in finding the value l* as defined in Section [\ref=sec:model].We present examples of a machine reading formulation of a dialog state tracking task for the slots Food in Tables [\ref=tab:food], Area in Tables [\ref=tab:area] and Price range in Tables [\ref=tab:price] from the DSTC-2 dialog corpus that we have produced for this task. This three first examples are part of the One supporting fact set of the corpus in reference to [\cite=WestonBCM15]. In order to exibit reasoning capability of the proposed model in the context of dialog state tracking, three other dataset have been automatically generated from the DSTC-2 corpus in order to support 3 capabilities of reasoning as mentioned in [\cite=WestonBCM15], (1) Yes-No question in Table [\ref=tab:yesno] (2) Indefinite Knowledge in Table [\ref=tab:ik] (3) Counting in Table [\ref=tab:ct] (4) List in Table [\ref=tab:list].

Figures [\ref=fig:awesome_image1], [\ref=fig:awesome_image2] and [\ref=fig:awesome_image3] detail the respective distributions of dialog length for each slot-specific corpus extracted from the DSTC-2 corpus to generate the examples used for the machine reading learning tasks. Table [\ref=tab:variables] recalls the domain of expression of each variables of the DSTC-2 corpus.

Experimental results

The learning protocol follows the process described in [\cite=SukhbaatarSWF15], we decided to use the Memory Network model for the implementation of our experiment, in the future we plan to experiment and compare the same approach with Stacked-Augmented Recurrent Neural Network [\cite=JoulinM15] and Neural Turing Machine [\cite=GravesWD14] that sounds also promising for these reasoning tasks. The size the embedding has been varied for experimental purposes. Concerning the choice of hyperparameters, the number of internal hops between the memory and the decision stack has been set to 3, the learning rate λ  =  0.01, the batch size to 50 and the number of epochs to 60.

Table [\ref=tab:results] presents detailed results obtained by the proposed machine reading model for state tracking on three tracked variables of the DSTC-2 dataset formulated a so-called one supporting fact question answering task. In this context, a memory enhanced model allows to obtain competitive results with the most close, non-memory enhanced, state of the art approach of recurrent neural network.

As a second result, Table [\ref=tab:resultsreason] presents the performance obtained for the four simple reasoning tasks. The obtained results lead us to think that memory enhanced model are a competitive alternative for the task dialog state tracking. In the future, we believe new reasoning capabilities like spacial and temporal reasoning and deduction should be exploited on the basis of a specifically designed dataset.

Conclusion and further work

This paper describes a novel method of dialog state tracking based on the general paradigm of machine reading and solved using a memory-enhanced neural network architecture.In this context, a specific dataset format inspired from the current dataset format of machine reading tasks has been developed for the task of dialog state tracking. As far as our knowledge goes, it is the very first attempt to solve this classic problem of dialog management in such way. Beyond the experimental results presented in the previous section, the proposed approach offers several advantages compared to state of the art methods of tracking. First, the proposed method allows to perform tracking on the basis of dialog-level annotation instead of utterance-level one that is commonly admitted in academic datasets but tedious to produce in a large scale industrial environment where annotation is often performed afterhand for the purpose of logging, monitoring and quality assessment. Indeed, dialog-level annotations correspond to common pratices of annotation especially in personal assistance, customer care dialogs and, in a more general sense, industrial application of transactional conversational user interfaces. Second, we propose to develop reasoning capability specific dialog corpus to exibit limitations of current models of dialog state tracking when exposed to such challenges as mentioned before. Finally, the memory enhanced inference model used in this paper seems to be able to cope with reasoning capabilities that remains nearly impossible with state of art approaches of state tracking like recurrent neural networks. In this paper, we have addressed four simple reasoning capabilities. In futher work, we plan to address more complex tasks like spatial and temporal reasoning, sorting, counting or deduction and experiment with different memory enhanced inference models.

Appendix