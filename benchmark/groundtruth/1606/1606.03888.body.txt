Extending E Prover with Similarity Based Clause Selection Strategies

Josef Urban[formula]

Introduction

Many state-of-the-art automated theorem provers (ATPs) are based on the given clause algorithm introduced by Otter [\cite=mccune1994otter]. The input problem [formula] is translated into a refutationally equivalent set of clauses. Then the search for a contradiction, represented by the empty clause, is performed maintaining two sets: the set P of processed clauses and the set U of unprocessed clauses. Initially, all the input clauses are unprocessed. The algorithm repeatedly selects a given clause g from U and generates all possible inferences using g and the processed clauses from P. Then, g is moved to P, and U is extended with the newly produced clauses. This process continues until a resource limit is reached, or the empty clause is inferred, or P becomes saturated, that is, nothing new can be inferred.

The search space of this loop grows quickly. Several methods can be used to make the proof search more efficient. The search space can be narrowed by adjusting (typically restricting) the inference rules, pruned by using forward and backward subsumption, reduced by pre-selecting relevant input clauses, or otherwise simplified. One of the main sources of non-determinism affecting efficiency of the search is the selection of the given clause. Clever selection mechanism can improve the search dramatically: in principle, one only needs to do the inferences that participate in the final proof. So far, this is often only a tiny portion of all the inferences done by the ATPs during the proof search.

Clause Selection in E Prover

E [\cite=schulz2002brainiac] is a state-of-the-art theorem prover which we use as a basis for implementation. The selection of a given clause in E is implemented by a combination of priority and weight functions. A priority function assigns an integer to a clause and is used to pre-order clauses for weight evaluation. A weight function takes additional specific arguments and assigns to each clause a real number called weight. A clause evaluation function [formula] is specified by a priority function, weight function, and its arguments. Each [formula] selects the clause with the smallest pair (,) for inferences. E allows a user to select an expert heuristic on a command line in the format "(n1*[formula],, nk*[formula])", where integer ni indicates how often the corresponding [formula] should be used to select a given clause. E additionally supports an autoschedule mode where several expert heuristics are tried, each for a selected time period. The heuristics and time periods are automatically chosen based on input problem properties.

One of the well-performing weight functions in E, which we also use as a reference for evaluation of our weight functions, is the conjecture symbol weight. This weight function counts symbol occurrences with different weights based on their appearance in the conjecture as follows. Different weights [formula], [formula], [formula], and [formula] are assigned to function, constant, and predicate symbols, and to variables. The weight of a symbol which appears in the conjecture is multiplied by [formula], typically [formula] to prefer clauses with conjecture symbols. To compute a term weight, the given symbol weights are summed for all symbol occurrences. This evaluation is extended to equations and to clauses.

Similarity Based Clause Selection Strategies

Many of the best-performing weight functions in E are based on a similarity of a clause with the conjecture, for example, the conjecture symbol weight from the previous section. In this paper we try to answer the question whether or not it makes sense to also investigate a term structure. We propose, implement, and evaluate several weight functions which utilize conjecture similarity in different ways. Typically they extend the symbol-based similarity by similarity on terms. Using finer formula features improves the high-level premise selection task [\cite=kaliszyk2015efficient], which motivates this work on steering also the internal selection in E. We first describe the common arguments of our weight functions and then function-specific properties.

Common Arguments ([formula],[formula],[formula])

We implement two ways of term variable normalization, selected by the argument [formula]. Either (1) variables are Î±-normalized, naming them consistently by their appearance in the term from left to right (value "[formula]"), or (2) all variables are unified to a single variable ("[formula]"). This provides differently coarse notions of similarity. Each of our weight functions relates a term to the global set [formula]. This set [formula], controlled by the argument [formula], contains either (1) all conjecture terms ("[formula]"), (2) conjecture terms and their subterms ("[formula]"), (3) conjecture subterms and top-level generalizations ("[formula]"), or to (4) conjecture subterms and all their generalizations ("[formula]"). Each of our weight functions implements a different function [formula] which assigns a weight to a term. We use three different ways of extending [formula] to compute a term weight, selected by the argument [formula]. Either (1) [formula] value is used directly (value "[formula]"), or (2) values of [formula] for all the subterms are summed ("[formula]"), or (3) the maximal value of [formula] on all of the subterms is used ("[formula]").

Conjecture Subterm Weight ()

The first of our weight functions is similar to the standard conjecture symbol weight, counting instead of symbols the number of subterms a term shares with the conjecture. The weight function takes five specific arguments [formula], [formula], [formula], [formula] and [formula] and [formula] equals weight [formula] for functional terms, [formula] for constants, [formula] for predicates, and [formula] for variables, possibly multiplied by [formula] when [formula].

Conjecture Frequency Weight ()

Term frequency - inverse document frequency, is a numerical statistic intended to reflect how important a word is to a document in a corpus [\cite=DBLP:books/cu/LeskovecRU14]. A term frequency is the number of occurrences of the term in a given document. A document frequency is the number of documents in a corpus which contain the term. The term frequency is typically multiplied by the logarithm of the inverse of document frequency to reduce frequency of terms which appear often. We define [formula] as the number of occurrences of [formula] in [formula]. We consider a fixed set of clauses denoted [formula]. We define [formula] as the count of clauses from [formula] which contain [formula]. Out weight function takes one specific argument [formula] to select documents, either (1) [formula] for the axioms or (2) [formula] for all the processed clauses, and [formula] is as follows.

[formula]

Conjecture Term Prefix Weight ()

The above weight functions rely on an exact match of a term with a conjecture related term. The following weight function loosen this restriction and consider also partial matches. We consider terms as symbol sequences. Let [formula] be the longest prefix [formula] shares with a term from [formula]. A term prefix weight () counts the length of [formula] using weight arguments [formula] and [formula], formally, [formula].

Conjecture Levenshtein Distance Weight ()

A straightforward extension of is to employ the Levenshtein distance [\cite=levenshtein1966bcc] which measures a distance of two strings as the minimum number of edit operations (character insertion, deletion, or change) required to change one word into the other. Our weight function defines [formula] as the minimal distance from [formula] to some [formula]. It takes additional arguments [formula], [formula], [formula] to assign different costs for edit operations.

Conjecture Tree Distance Weight ()

The Levenshtein distance does not respect a tree structure of terms. To achieve that, we implement the Tree edit distance [\cite=Zhang:1989:SFA:76071.76082] which is similar to Levenshtein but uses tree editing operations (inserting a node into a tree, deleting a node while reconnecting its child nodes to the deleted position, and renaming a node label). Our weight function takes the same arguments as above and [formula] is defined similarly.

Conjecture Structural Distance Weight ()

With , a tree produced by the edit operations does not need to represent a valid term as the operations can change number of child nodes. To avoid this we define a simple structural distance which measures a distance of two terms by a number of generalization and instantiation operations. Generalization transforms an arbitrary term to a variable while instantiation does the reverse. Our weight function takes additional arguments [formula], [formula], and [formula] as penalties for variable mismatch and operation costs. The distance of a variable x to a term [formula] is the cost of instantiating x to [formula], computed as [formula]. The distance of [formula] to x is defined similarly but with [formula]. A distance of non-variable terms [formula] and [formula] which share the top-level symbol is the sum of distances of the corresponding arguments. Otherwise, a generic formula [formula] is used. Function [formula] is as for but using [formula].

Experimental Results and Evaluation

The best evaluation would be to measure how our weight functions enrich the autoschedule mode of E. This is, however, beyond the scope of this paper. Instead, we design experiments to help us estimate the quality of the new weights. For each new weight function we run all possible combinations of common arguments ("[formula]", see Section [\ref=sec:weights]) and other manually selected arguments. First, we run the weight functions on the 2078 MPTP bushy problems [\cite=abs-1108-3446] with a 5 second time limit. We compare the number of solved problems with the number of problems solved by the conjecture symbol weight (denoted [formula]) discussed in Section [\ref=sec:anl]. Second, to estimate how complementary our weight functions are with existing functions, we pick a well-performing expert heuristic from the autoschedule mode of E, and we compute how many problems were solved which the expert heuristic was not able to solve in 10 seconds (denoted [formula]). The five best-performing combinations of arguments for each weight function are presented in Table [\ref=tab:experiments]. Column speed contains an average number of processed (kilo-)clauses per second to evaluate implementation efficiency. Our implementation is available for download.

From Table [\ref=tab:experiments] we can see that the weights which rely on an exact match of a term with a related term or its part (, , and ) perform best when values of [formula] are summed for all the subterms ([formula]). On the other hand, weights which incorporate some notion of term similarity directly in [formula] do not profit so much from this. For weights , , and we have tried to experiment with operation costs (column [formula], for example, [formula] means that [formula] is increased to 5 while other costs are 1). In general, the experiments show that different arguments have an impact on performance. Finally, the experiments also reveal a higher time complexity of the and weights (Levenshtein distance of two terms is in O(n2) while is in O(n3)). However, a higher time complexity does not have to be a drawback as is still best performing.

Conclusions and Future Work

We have implemented several new weight functions for E prover based on term similarity with a conjecture. The experiments suggest that our functions have a potential to improve the autoschedule mode of E as they are reasonably complementary with existing heuristics. In order to use our weight functions with the autoschedule mode of E, we would need to (1) find the best performing parameters of our weight functions, (2) find the best combinations of our weight functions with other weight functions, and (3) find the most complementary combinations and create a scheduling strategy. As a future research, we are planning to use parameter-searching methods such as BliStr [\cite=GCAI2015:BliStr_The_Blind_Strategymaker] to achieve this task.