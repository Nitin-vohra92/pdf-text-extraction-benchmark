The Attentive Perceptron

IG1

BL

Introduction

Many Online Algorithms base their model update on the margin of each example. Passive online algorithms, such as Rosenblatt's Perceptron [\cite=rosenblatt58perceptron] and Crammer et al's online passive-aggressive algorithms [\cite=crammer06online], update the algorithm's model only if the value of the margin falls below a defined threshold. These algorithms fully evaluate the margin for each example, even if the model is not to be updated!

The running time of these algorithms is linear either in the number of features, or in the dimensionality of the input space. Contemporary models may have thousands of features making running time daunting. The budgeted learning community addresses this problem by putting a budget on the number of features a classifier can evaluate while learning and while making predictions. Our work stems from the theoretical framework suggested by Ben David and Dichterman [\cite=bendavid1998learning], and is closely related to recent work by Cesa-Bianchi et al. [\cite=cesabianchi2010efficient] as well as Reyzin [\cite=reyzin10boosting].

We differ by the fact that we do not impose a hard budget constraint on the number of features, but rather look at the probability of making decision errors. Decision error are errors that occur when the algorithm stops the feature evaluation process, predicts its outcome, and is wrong. This work extends on previous work by Pelossof et al. [\cite=curtailed10pelossof].

We propose a new method for early stopping the computation of feature evaluations for uninformative examples by connecting the Perceptron algorithm to sequential statistical tests [\cite=wald45tests] [\cite=lan82sequential] (Figure [\ref=fig:attentive-perceptron].) This connection results in a general method that makes margin based learning algorithms attentive, which means that they have the ability to quickly filter uninformative examples.

The Attentive Perceptron

The margin of each example is computed as a weighted sum of feature evaluations. Informative examples are misclassified examples, which force the Perceptron to preform a model update, whereas uninformative examples are correctly classified and therefore ignored by the perceptron.

We break up the feature evaluation for every example in the stream. The breakup of every example allows the Attentive Perceptron to make a decision after the evaluation of each feature about whether the feature evaluation should continue or be stopped. This decision making process allows us to stop the evaluation of features early on examples with a large partial margin after having evaluated only a few features. For example, examples with a large partial margin are unlikely to have a negative full margin. Therefore, rejecting these examples early achieves large savings in computation.

We define the mathematical setup to derive the stopping conditions for margin evaluation. Let X1,...,Xn be weakly dependent random variables. Let a partial sum be defined by Si = X1 + ... + Xi and the remainder sum by Sin = Sn - Si. The expectation of a sum is denoted by ESi and its standard deviation by std(Si).

The Perceptron compares the margin (a sum) to a threshold, and updates its model if the margin of the example is negative. We formulate the equivalent sequential decision making process, and drive constant stopping thresholds τ. These thresholds will essentially tell us when it's highly unlikely for the margin to end below the desired importance threshold θ.

The stopping thresholds are derived by requiring that the joint distribution of stopping (and predicting Sn  >  θ) while the actual full sum satisfies Sn  <  θ is less than a required error rate δ

[formula]

We bound the probability of making a decision error

[formula]

Equation [\ref=eqn:rw_reflection] is derived by applying the reflection principle, and equation [\ref=eqn:flat_reflection_standerdized] is its standardization.

Since we assume that X1,...,Xn are weakly independent, the sum Sn = X1 + ... + Xn is approximately normally distributed by the Central Limit Theorem. By standardizing Sn we upper bound the probability of making a decision error with the inverse normal cumulative distribution function Φ- 1. Therefore, requiring that the probability of making a decision error be less than δ we get the following equality from equation [\ref=eqn:flat_reflection_standerdized]

[formula]

The quantities ESn and std(Sn) can be approximated using the empirical data.

Finally, by solving for the stopping threshold τ we get from equation [\ref=eqn:boundary_inequality]

[formula]

Therefore, examples with partial margin calculations Si that hit this boundary should be filtered and with probability at least 1 - δ determined that their full margin satisfies Sn  >  θ.

In summary, we presented a simple test to speed up the Perceptron algorithm by quickly filtering unimportant examples without fully evaluating their features. This results in an algorithm which typically focuses on examples by the decision boundary - the Attentive Perceptron.