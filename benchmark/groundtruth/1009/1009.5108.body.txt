Improving the Space-Bounded Version of Muchnik's Conditional Complexity Theorem via "Naive" Derandomization

Supported by ANR Sycomore, NAFIT ANR-08-EMER-008-01 and RFBR 09-01-00709-a grants.

Introduction

Many statements about Kolmogorov complexity may be proven by applying some combinatorial constructions like expanders or extractors. Usually these objects are characterized by some parameters, and one may say which parameters are "better". Very often the probabilistic method allows one to obtain these objects with much better parameters than explicit constructions do. But exploiting the probabilistic method causes exponential-space brute-force search for an object satisfying the necessary property. And if this search is performed while describing some string to obtain an upper bound on its complexity then this bound cannot be repeated for a resource-bounded version of complexity, even for the polynomial-space one. On the other hand, replacing the probabilistic method by an explicit construction weakens the statement due to worse parameters.

We present a technique that combines advantages of both probabilistic and explicit construction methods. The key idea is to substitute a random object with a pseudo-random one still possessing the necessary property. The employed property of a pseudo-random generator is the indistinguishability one: its output cannot be distinguished from a random string by boolean circuits of constant depth and polynomial size. The Nisan-Wigderson generator satisfies this condition. If the necessary property can be tested by such circuits then it holds for a pseudo-random object with approximately the same probability as for a truly random object, so a search among all seeds may be perormed. Unfortunately, it is not clear how to build such a circuit for the extractor property and similar ones, so we relax the property in a way that allows both proving the theorem and building polynomial constant-depth circuits.

This "naive derandomization" idea has been recently applied by Andrei Romashchenko ([\cite=romash]) in another situation: a probabilistic bit-probe scheme with one-sided error for the membership problem is constructed.

By exploiting the new method we improve the previous result [\cite=tocs] generalizing Muchnik's conditional complexity theorem. The original theorem [\cite=muchnik-codes] states that for all a and b of length n there exists a program p of length [formula] that transforms b to a and has complexity O( log n) conditional on a. In [\cite=tocs] this result is restated for space-bounded complexity with gap rised from O( log n) to O( log 3n). The main idea was to employ a property of extractors, proven in [\cite=fortnow]: in an extractor graph in any sufficiently big subset S of the left part there are few vertices with all right-part neighbours having indegree from S twice greater than average. We refer to this property as to "low-congesting" one. Best explicit extractor constructions yield a space-bounded version of the theorem with polylogarithmic precision. In this paper we replace an explicit extractor by a pseudo-random graph, that does not necessary have the extractor property, but enjoys the low-congesting property for "relevant" subsets S. This replacement leads to decreasing the precision back to logarithmic term.

The rest of the paper is organized as follows. In Sect. [\ref=pre] we give formal definitions of all involved objects and formulate necessary results. In Sect. [\ref=muchnik] we formally state our space-bounded variant of Muchnik's theorem and specify all details of the proof.

Preliminaries

Kolmogorov complexity

Let V be a two-argument Turing machine. We refer to the first argument as to the "program" and to the second argument as to the "argument". (Plain) Kolmogorov complexity of a string x conditional on y with respect to V is the length of a minimal program p that transforms y to x, i.e.

[formula]

There exists an optimal machine U that gives the least complexity up to an additive term. Specifically, [formula]. We employ such a machine U, drop the subscript and formulate all theorems up to a constant additive term. The unconditional complexity [formula] is the complexity with empty condition [formula], or the length of a shortest program producing x.

Now we define the notion of resource-bounded Kolmogorov complexity. Roughly speaking, it is the length of a minimal program that transforms y to x efficiently. Formally, Kolmogorov complexity of a string x conditional on y in time t and space s with respect to V is the length of a shortest program p such that V(p,y) = x, and the computation of V(p,y) works in t steps and uses s cells of memory. This complexity is denoted by [formula]. Here the choice of V alters not only complexity, but also time and space bounds. However, there still exists an optimal machine in the following sense:

There exists a machine U such that for any machine V there exists a constant c such that for all x, y, s and t it is true that [formula].

This paper deals with space bounds only, so the time-bound superscript in all notations is dropped. Also, the machine subscript is dropped as before and all theorems are formulated with a constant additive term in all complexities and a constant multiplicative term in all space bounds.

Extractors

An extractor is a function that extracts randomness from weak random sources. A k-weak random source of length n is a probabilistic distribution on {0,1}n with minentropy greater than k. The last condition means that no particular string occurs with probability greater than 2- k. An extractor with parameters n, m, d, k, ε is a function [formula], such that for any independent k-weak random source x of length n and uniform distribution u on {0,1}d the induced distribution [formula] on {0,1}m is ε-close to uniform, that is, for any set Y  ⊂  {0,1}m its probability differs from its fraction by at most ε. This is interpreted as follows: an extractor receives n weakly random bits and d truly random bits independent from the first argument and outputs m almost random bits.

Like any two-argument function, an extractor may be viewed as a bipartite (multi-)graph: the first argument indexes a vertex in the left part, the second argument indexes an edge going from this vertex, and the value indexes a vertex in the right part which this edge directs to. That is, the graph has N = 2n vertices on the left, M = 2m vertices on the right, and all left-part vertices have degree D = 2d. Throughout the paper, we say that a bipartite graph has parameters (n, m, d) if the same holds for it. For the sake of clarity we usually omit these parameters in extractor specifications. The extractor property may be also formulated in terms of graphs: for any left-part subset S of size greater than K = 2k and for any right-part subset Y the fraction of edges directing from S to Y among all edges directing from S differs from the fraction of vertices from Y among all right-part vertices by at most ε. Put formally, [formula], where E(S,T) is the number of edges diredting from S to T and M slightly abusively denotes both the right part and the number of vertices in it. A proof of equivalence may be found in [\cite=extractor-bounds].

It is proven by the probabilistic method (see, for example, [\cite=extractor-bounds]) that for all n, k and ε there exists an extractor with parameters d =  log (n - k) + 2 log (1 / ε) + O(1) and m = k + d - 2 log (1 / ε) - O(1). Nevertheless, no explicit (that is, running in polynomial time) construction of such an extractor is known. Best current results ([\cite=extractor-explicit], [\cite=trevisan]) for m = k use [formula] truly random bits. A number of explicit extractors including those in [\cite=wigderson], [\cite=dvir] and [\cite=guruswami] use O( log n) truly random bits but output only (1 - α)k almost random bits. Such constructions are insufficient for our goals.

Nisan-Wigderson generators

The Nisan-Wigderson pseudo-random generator is a deterministic polynomal-time function that generates n pseudorandom bits from [formula] truly random bits. The output of such a generator cannot be distinguished from a truly random string by small circuits. Specifically, we exploit the following theorem from [\cite=NW97]. (The statement was initially proven by Nisan in paper [\cite=Nisan91]).

For any constant d there exists a family of functions Gn:{0,1}k  →  {0,1}n, where k = O( log 2d + 6n), such that two properties hold:

where x is distributed uniformly in {0,1}k and y -- in {0,1}n.

By rescaling the parameters we get the following

For any constant d there exists a family of functions Gn:{0,1}k  →  {0,1}N, where [formula] and N = 2O(n), such that two properties hold:

G is computable in workspace [formula];

For any family of circuits Cn of size 2O(n) and depth d, for any constant c and for all large enough n it holds that:

[formula]

Here all constants in [formula]- and O-notations may depend on d but do not depend on k, n and Cn.

The last corollary implies the following basic principle:

Let Cn be some set of combinatorial objects encoded by boolean strings of length 2O(n). Let P be some property satisfied for fraction at least α fraction of objects in Cn that can be tested by a family of circuits of size 2O(n) and constant depth. Then for sufficiently large n the property P is satisfied for a fraction at least α / 2 of values of Gn, where Gn is the function from the previous corollary.

Constant-depth circuits for approximate counting

It is well-known that constant-depth circuits cannot compute the majority function. Moreover, they cannot compute a general threshold function that equals 1 if and only if the fraction of 1's in its input exceeds some threshold α. Nevertheless, one can build such circuits that compute threshold functions approximately. Namely, the following theorem holds:

Let α∈(0,1). Then for any (constant) ε there exists a constant-depth and polynomial-size circuit C such that C(x) = 0 if the fraction of 1's in x is less than α  -  ε and C(x) = 1 if the fraction of 1's in x is greater than α  +  ε.

Note that nothing is promised if the fraction of 1's is between α  -  ε and α  +  ε. So, the fact that C(s) = 0 guarantees only that the fraction of 1's is at most α  +  ε, and C(s) = 1 -- that it is at least α  -  ε.

Muchnik's theorem

Subject overview

An. Muchnik's theorem [\cite=muchnik-codes] on conditional Kolmogorov complexity states that:

Let a and b be two binary strings such that [formula] and [formula]. Then there exists a string p such that

[formula];

[formula];

[formula].

The constants hidden in O( log n) do not depend on n,k,a,b,p.

Informally, this theorem says that there exists a program p that transforms b to a, has the minimal possible complexity [formula] (up to a logarithmic term) and, moreover, can be easily obtained from a. (The last requirement is crucial, otherwise the statement trivially reformulates the definition of conditional Kolmogorov complexity.)

Several proofs of this theorem are known. All of them rely on the existence of some combinatorial objects. The original proof in [\cite=muchnik-codes] leans upon the existence of bipartite graphs with specific expander-like property. Two proofs by Musatov, Romashchenko and Shen [\cite=tocs] use extractors and graphs allowing large on-line matchings. Explicit constructions of extractors provide variants of Muchnik's theorem for resource-bounded Kolmogorov complexity. Specifically, the following theorem is proven in [\cite=tocs]:

Let a and b be binary strings of length n, and k and s be integers such that [formula]. Then there exists a binary string p, such that

[formula]

[formula]

[formula]

where all constants in O- and [formula]-notations depend only on the choice of the optimal description method.

One cannot reduce residuals to O( log n) since all explicit extractors with such seed length output only (1 - α)k bits, but p is taken as an output of an extractor and should have length k. However, an application of our derandomization method will decrease conditional complexities from O( log 3n) to O( log n) + O( log  log s) at the price of increasing the space limit in the last complexity from [formula] to [formula]. If s is polynomial in n then all space limits become also polynomial and all complexity discrepancies become logarithmic.

Proof overview

Before we proceed with the detailed proof, let us present its high-level description. The main idea is the same as in all known proofs: p is a fingerprint (or hash value) for a constructed in some specific way. This fingerprint is chosen via some underlying bipartite graph. Its left part is treated as the set of all strings of length n (i.e., all possible a's) and its right part is treated as the set of all possible fingerprints. To satisfy the last condition each left-part vertex should have small outdegree, since in that case the fingerprint is described by its ordinal number among a's neighbours. To satisfy the first condition each fingerprint should have small indegree from the strings that have low complexity conditional on b (for arbitrary b). If resources are unbounded then the existence of a graph satisfying all conditions may be proven by the probabilistic method and the graph itself may be found by brute-force search. In the resource-bounded case we suggest to replace a random graph by a pseudo-random one and prove that it still does the job. The proof proceeds in several steps. Firstly, in Sect. [\ref=sect-lowcon] we define the essential graph property needed to our proof. We call this property low-congestion. It follows from the extractor property, but not vice versa. Secondly, in Sect. [\ref=sect-space-enum] we specify the instrumental notion of space-bounded enumerability and prove some lemmas about it in connection to low-congesting graphs. Next, in Sect. [\ref=muchnik-derandom] we employ these lemmas to prove that the low-congesting property is testable by small circuits. Hence, by applying the main principle (lemma [\ref=mainprinciple]) this property is satisfied for pseudo-random graphs produced by the NW generator as well as for truly random ones. Moreover, a seed producing a graph with this property may be found in polynomial space. Finally, in Sect. [\ref=sect-proof] we formulate our version of Muchnik's theorem and prove it using the graph obtained on the previous step. I.e., we describe the procedure of choosing a fingerprint in this graph and then calculate all complexities and space requirements and assure that they do not exceed their respective limitations.

Low-congesting graphs

In fact, the proof in [\cite=tocs] does not use the extractor property, but employs only its corollary. In this section we accurately define this corollary in a way that allows derandomization.

Fix some bipartite graph with parameters (n, m, d) and an integer k < n. Let there be a system S of subsets of its left part with the following condition: each S∈S contains less than 2k vertices, and the whole system S contains 2O(n) sets (note that there are [formula] different sets of required size, so the last limitation is not trivial). We refer to such systems as to "relevant" ones. Having fixed a system S, let us call the sets in it relevant also.

Let [formula]. Then the system Sk is relevant.

By a standard counting argument, each set S∈Sk contains less than 2k elements. If b is fixed then the described sets are expanding while s is rising. Since the largest set is smaller than 2k, there are less than 2k different sets for a fixed b. Since there are 2n different strings b, the total size of Sk is bounded by 2n2k = 2O(n).

Considering a modification of the upper system [formula], one may note that it is relevant as well.

Now fix some relevant system S and take an arbitrary set S in it. Call the α-clot for S the set of right-part vertices that have more than αDK / M neighbours in S (that is, at least α times more than on average). Call a vertex x∈S α-congested (for S) if all its neighbours lie in the α-clot for S. Say that G is (α, β)-low-congesting if there are less than βK α-congested vertices in any relevant S.

Following [\cite=fortnow], we prove the next lemma:

Let G be an extractor graph with parameters n, m, d, k, ε. Then for any α > 1 the graph G is (α, [formula])-low-congesting.

In fact in an extractor graph there are less than [formula] α-congested vertices in any set S of size K, not only in relevant ones. We may treat S as an arbitrary set of size exactly K: since a congested vertex in a subset is also a congested vertex in the set, an upper bound for the number of congested vertices in the set holds also for a subset.

Let Y be the α-clot for S, and |Y| = δM. Then the fraction |Y| / M of vertices in Y equals δ and the fraction E(S,Y) / E(S,M) of edges directing from S to Y is greater than αδ (by the definition of clot). A standard counting argument implies only [formula], but by the extractor property E(S,Y) / E(S,M) - |Y| / M < ε, so (α - 1)δ  <  ε, i.e. [formula]. Next, let T  ⊂  S be the set of α-congested vertices in S, and |T| = βK. All edges from T direct to vertices in Y, so at least D|T| = βDK edges direct from S to Y. In other words, the fraction of edges from S to Y is at least β. By the extractor property it must differ from the fraction of vertices in Y (that equals δ) by at most ε. So, [formula]. Putting it all together, the number of α-congested vertices in any relevant S is less than [formula], so the graph is (α, [formula])-low-congesting, q.e.d.

Space-bounded enumerability

Say that a system S is enumerable in space q if there exists an algorithm with two inputs i and j working in space q that either outputs the jth element of the ith set from S or indicates that at least one of the inputs falls out of range. Note that for a polynomial space bound enumeration is equivalent to recognition: if one may enumerate a set then one may recognize whether a given element belongs to it by sequentially comparing it to all enumerated strings, and if one may recognize membership to a set then one may enumerate it by trying all possible strings and including only those accepted by the recognition algorithm. Only small auxiliary space is needed to perform these modifications.

The system [formula] is enumerable in space [formula].

Assume firstly that a set S is given (by specifying b and s < s̄) and show how to enumerate it. Look through all programs shorter than k and launch them on b limiting the space to s and counting the number of steps. If this number exceeds cs (for some constant c depending only on the computational model) then the current program has looped. If the looping is detected or if the program exceeds the space limit it is terminated and the next one is launched. Otherwise, if the program produces an output, then a check whether it has not been produced by any previous program is performed. This check proceeds as follows: store the result, repeat the same procedure for all previous programs and compare their results with the stored one. If no result coincides then include the stored result in the enumeration, otherwise skip it and in both cases launch the next program. Emulation of a program requires O(s) space and no two emulations should run in parallel. All intermediate results are polynomial in n, so a total space limit [formula] holds.

Specifying a set S by b and s is not reasonable because a lot of values of s may lead to the same set. (And if s̄ = 2ω(n) then the number of possible indexes (b,  s) exceeds the limit 2O(n) on the size of S). Instead, call a limit s pivotal if [formula] and consider only pivotal limits in the definition of Sk,s̄. Clearly, the latter modification of definition does not affect the system itself. The advantage is that there are less than 2k pivotal limits, and the ith pivotal limit si may be found algorithmically in space [formula]. Indeed, it is sufficient to construct an algorithm recognizing whether a given limit is pivotal, and the latter is done by a procedure similar to one described in the first paragraph: try all possible programs in space s, and in case they produce an output, check whether it is produced by any program in space s - 1. If at least one result is new then s is pivotal, otherwise it is not. Putting all together, a set S in Sk,s̄ is defined by a word b and the ordinal number i of a pivotal space limit si. Knowing these parameters, one may enumerate it in space [formula]. Only O(n) additional space is needed to look over all possible values of b and i, so the declared bound is meeted.

The next lemma indicates that if a system S is enumerable in small space, then the same holds for the system of congested subsets of its members. We call a bipartite graph computable in space q if there exists an algorithm working in space q that receives an index of a left-part vertex and an index of an incident edge and outputs the right-part vertex this edge directs to.

Let S be a system of relevant sets enumerable in space s, G be a bipartite graph computable in space q and α > 1 be a (rational) number. Then the system [formula] is computable in space [formula]. Moreover, the iteration [formula] is also computable in space [formula] with constants in O- and [formula]- notations not depending on r, but possibly depending on α.

Since for space complexity enumeration and recognition are equivalent, it is sufficient to recognize that a vertex x is α-congested. The recognizing algorithm works as follows: having a set S and a vertex x∈S fixed, search through all neighbours of x (this requires space [formula]) and for each neighbour check whether it lies in the α-clot for S. If all neighbours do then x is congested, otherwise it is not. Having a neighbour y fixed, the check is performed in the following way: enumerate all members of S (using [formula] space), for each member search through all its neighbours (using [formula] space) and count the number of these neighbours coinciding with y. Finally, compare this number with the threshold αDK / M. Note that no two computations requiring space s or q run in parallel and all intermediate results need only polynomial space, so the total space requirement is [formula], as claimed. Note also that O- notation is used only due to the possibility of computational model change, not because of the necessity of looping control. If a computational model is fixed then the required space is just [formula].

The last observation is crucial for the "moreover" part of lemma. Indeed, by a simple counting argument the fraction of α-congested vertices in S is at most 1 / α. That is why after at most log α2n = O(n) iterations the set of congested variables becomes empty. Each iteration adds [formula] to the space requirement, so the overall demand is still [formula] (with greater polynomial), as claimed.

Derandomization

In this section we show that, firstly, the low-congesting property may be (approximately) recognized by 2O(n)-sized constant-depth circuits, secondly, that there are low-congesting graphs in the output of Nisan-Wigderson pseudo-random generator and, thirdly, that one can recognize in polynomial space whether the NW-generator produces a low-congesting graph on a given seed. Put together, the last two lemmas provide a polynomial-space algorithm outputting a seed on which the NW-generator produces a low-congested graph.

Let us encode a bipartite graph with parameters (n, m, d) by a list of edges. The length of this list is 2n2dm: for each of 2n left-part vertices we specify 2d neighbours, each being m-bit long.

Let G be the set of all bipartite graphs with parameters (n, m, d) encoded as described above. Let k be an integer such that 1 < k < n, and let ε be a real positive number. Then there exists a circuit C of size 2O(n) and constant depth defined on G, such that:

If G is a (k, ε)-extractor then C(G)=1;

If C(G) = 1 then G is a (2.01, 2.01ε)-low-congesting graph for Sk from lemma [\ref=relevant-sets].

We build a non-uniform circuit, so we may assume that Sk is given. We construct a single circuit approximately counting the number of congested vertices in a given set, then replicate it for each relevant set and take conjunction. Since there are less than 2n + k relevant sets, this operation keeps the size of circuit being 2O(n). We proceed by constructing a circuit for a given set S. A sketch of this circuit is presented on Fig. [\ref=circuit].

The size of the input is [formula]. We think of it as of being divided into |S| blocks of 2d segments of length m. We index all blocks by elements x∈S and index all segments of the block x by vertices y incident to x. It is easy to see that there is a constant-depth circuit that compares two segments (that is, has 2m inputs and outputs 1 if and only if the first half of inputs coincides with the second half). On the first stage we apply this circuit to every pair of segments, obtaining a long 0-1-sequence. On the second stage we employ a counting circuit with |S|D - 1 arguments that is guaranteed to output 1 if more than 2.01DK / M of its arguments are 1's and to output 0 if the number of 1's is less than 2DK / M. By theorem [\ref=ajtai] there exists such a circuit of polynomial (in the number of arguments) size and constant depth. For all segments y a copy of this circuit is applied to the results of the comparison of y to all other segments. If y lies in the 2.01-clot then the respective copy outputs 1, and if it outputs 1, then y lies in 2-clot. On the third stage we take a conjuction of all second-stage results for the segments lying in the same block x. If this conjunction equals 1 then all images of x lie in 2-clot, that is, x is 2-congested. Conversely, if x is 2.01-congested then the conjunction equals 1. Finally, we utilize another counting circuit with |S| inputs that outputs 0 if more than 2.01εK of its inputs are 1's and outputs 1 if less than 2εK of its inputs are 1's. This circuit is applied to all outputs of the third stage. If the final result is 1 then less than 2.01εK elements of S are 2.01-congested; and if less than 2εK elements of S are 2-congested then the final result is 1.

The last claim holds for any relevant S. On the very last stage we take a conjunction of results for all S. If this conjunction is 1 then less than 2.01εK elements in each S are 2.01-congested, and if G is a (k, ε)-extractor then by lemma [\ref=fortnow-new] less than 2εK elements in each S are 2-congested and hence this conjunction equals 1.

Since there are at most 2O(n) gates on every stage and each stage has constant depth, the overall circuit has also 2O(n) gates and constant depth, as claimed.

Let n, m = k, d = O( log n) and ε be such parameters that a random bipartite graph with parameters (n, d, m) is a (k, ε)-extractor with constant probability p > 0. Let q be the depth of the circuit from the previous lemma. Let NW:{0.1}l  →  {0,1}N, where l = O( log 2q + 6) and N = 2n2dm, be the Nisan-Wigderson generator from corollary [\ref=nw-modified]. Then [formula] for sufficiently large n, where C is the circuit from the previous lemma.

This is a straightforward application of lemma [\ref=mainprinciple]. By the previous lemma if a graph G is an extractor then C(G) = 1, so [formula]. Since C is a constant-depth circuit, the property C(G) = 1 is tautologically testable by a constant-depth circuit. By lemma [\ref=mainprinciple] [formula], q.e.d.

Consider the following problem R: find a string u∈{0,1}l such that the graph NW(u) is (2.01, 2.01εK)-low-congesting with respect to Sk,  s.

The problem R is solvable in space [formula].

The existence of a solution follows from the previous lemma. Since we care only about the space limit, the search problem may be replaced by the corresponging recognition problem. The space bound for the latter one arises from corollary [\ref=nw-modified], lemma [\ref=relevant-enum] and lemma [\ref=congested-enum]. Indeed, by corollary [\ref=nw-modified] the graph NW(u) is computable in polynomial space, and by lemma [\ref=relevant-enum] the system Sk,  s is enumerable in space [formula]. Hence by lemma [\ref=congested-enum] the system [formula] is also enumerable in space [formula], therefore one may easily check whether each set in [formula] contains less than 2.01εK elements, thus solving the recognition analogue of R. Only polynomial extra space is added on the last step.

Proof of the theorem

Now we proceed by formulating and proving our version of Muchnik's theorem.

Let a and b be binary strings of length less than n, and s and k be numbers such that [formula]. Then there exists a binary string p, such that

[formula];

[formula];

[formula],

where all constants in O- and [formula]-notations depend only on the choice of the optimal description method.

Basically the proof proceeds as the respective proof of theorem [\ref=muchnik-space] in [\cite=tocs]. Here we replace an explicitly constructed extractor by a pseudorandom graph described in Sect. [\ref=muchnik-derandom].

Let ε be a small constant and let d = O( log n) be such that a random bipartite graph with parameters (n, m = k, d) is a (k, ε)-extractor with probability greater than some positive constant μ. Let l be a parameter and NW be a function from lemma [\ref=lowcon-derandom]. By lemmas [\ref=lowcon-derandom] and [\ref=muchnik-circuit] the output of NW is a (2.01, 2.01εK)-low-congesting graph for Sk with probability at least μ / 2. Applying the program from lemma [\ref=finding-good-seed], one may find in [formula] space a seed u for which NW(u) is a (2.01, 2.01εK)-low-congesting graph for Sk,  s. This u has low complexity: to perform this search one needs to know parameters n, k and [formula], that is, O( log n) bits, and the space bound s, that requires log s bits. The last number may be reduced to log  log s, because the space bound s may be replaced by the least power of 2 exceeding s keeping the needed space to be [formula]. For what follows, fix this seed u and the graph G = NW(u).

By definition of the low-congesting property the number of 2.01-congested vertices in the set [formula] does not exceed 2.01εK. Clearly, a belongs to S. Firstly suppose that it is not 2.01-congested. Then it has a neighbour outside the 2.01-clot for S. This neighbour may be taken as p. Indeed, the length of p equals m = k, hence its complexity is also less than k + O(1). To specify p knowing a, one needs to construct the graph NW(u), for which only O( log n) + O( log  log s) bits are necessary, and to know the number of p among a's neighbours, which is at most d = O( log n). Finding a given b and p proceeds as follows: construct G, enumerate S and choose the specified preimage of p in S. Then a is determined by the information needed to construct G (O( log n +  log  log s) bits), information needed to enumerate S (that is, k, log s and b) and the number of a among preimages of p (since p is not in the 2.01-clot, there are not more than 2.01DK / M = 2.01D preimages, so O(d) = O( log n) bits are required). Summarizing, we get O( log n) + O( log  log s) bits and [formula] space needed. Note that the fact that m = k is crucial here: in the case m = (1 - α)k the number of required bits would increase by αk and exceed the bound.

Now we turn to the case where a is 2.01-congested. By lemma [\ref=congested-enum] the set [formula] of 2.01-congested vertices is enumerable in [formula] space. Take parameters n1 = n, m1 = k1  =   log K1  =   log (2.01εK), d1 = O( log n) and ε1  =  ε such that an extractor with these parameters exists with probability greater than μ. Lemmas [\ref=muchnik-circuit], [\ref=lowcon-derandom] and [\ref=finding-good-seed] are applicable to the new situation, so we may find a new u1 such that the graph G1 = NW(u1) is (2.01, 2.01εK1)-low-congesting for [formula] with probability at least μ / 2. By assumption, a belongs to the set [formula]. If it is not 2.01-congested in the new set then we choose p analogously to the initial situation. Otherwise we reduce the parameters again and take a low-congesting graph for [formula], and so on. By the "moreover" part of lemma [\ref=congested-enum], this reduction may be performed iteratively for arbitrary number of times keeping the space limit to be [formula].

The total number of iterations is less than log 1 / 2.01εk = O( log n). Finally p is defined as a neighbour of a not lying in the 2.01-clot in graph Gi. To find p knowing a one needs to know i and the same information as on the upper level. To find a knowing p and b also only specifying i is needed besides what has been specified on the upper level. So, all complexities and space limits are as claimed and the theorem is proven.

Acknowledgments

I want to thank my colleagues and advisors Andrei Romashchenko, Alexander Shen and Nikolay Vereshchagin for stating the problem and many useful comments. I also want to thank three anonymous referees for careful reading and precise comments. I am grateful to participants of seminars in Moscow State University and Moscow Institute for Physics and Technology for their attention and thoughtfulness.