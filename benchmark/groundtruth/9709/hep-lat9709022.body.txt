8pt 5pt 12pt 12pt 24pt

CERN 97-239

DESY 97-172

The non-perturbative O(a)-improved action for dynamical Wilson fermions

Talk given by K.J. at the International Symposium on Lattice Field Theory, 21-  27 July 1997, Edinburgh, Scotland

Introduction

The standard formulation of lattice QCD by Wilson has been used since the early days of lattice gauge theory. However, it is known that in this formulation the leading discretization errors are linear in the lattice spacing a. Moreover, by testing the PCAC relation on the lattice, it could be demonstrated that the effects of these discretization errors are most severe and can influence values of physical observables strongly [\cite=letter].

In a series of papers [\cite=paper1] [\cite=paper2] [\cite=paper3] it was shown that, by implementing Symanzik's improvement programme [\cite=symanzik] for QCD on-shell and non-perturbatively, one can reach a complete cancellation of the O(a) effects. The advantages of this procedure are obvious, and this conference has seen the improvement programme successfully at work [\cite=hartmut]. The complete improvement programme demands as a first step a computation of the parameter csw that multiplies the Sheikholeslami-Wohlert term [\cite=clover] in the improved action. In addition, also the coefficients that enter the improved operators have to be determined. By now, in the quenched approximation, a number of these parameters are known as a function of the bare gauge coupling g0 [\cite=paper3] [\cite=Marco] [\cite=Giulia].

In this contribution we want to initiate the computation of the improvement coefficients for two flavours of dynamical fermions. As a first step we will compute the coefficient csw. As is well known, dynamical fermion simulations are very demanding even with today's computers and algorithms [\cite=karl]. On the other hand, knowing the improvement coefficients will substantially reduce the computational cost, since one is allowed to choose larger lattice spacings.

The improvement condition

The idea of testing the lattice artefacts is to probe the PCAC relation, which should hold, up to O(a) corrections:

[formula]

where Aaμ(x) denotes the isovector axial current and Pa(x) the corresponding density. The quark mass that appears in eq. ([\ref=pcac]) is a bare current quark mass at scale 1 / a. The important point to notice here is that the PCAC relation is an operator identity that can be inserted into arbitrary correlation functions.

One can make use of this fact to improve the theory: one tests the PCAC relation in different correlation functions and demands to obtain always the same value of m. Using the Schrödinger functional, it was demonstrated in ref. [\cite=paper3] how this strategy can be efficiently implemented to determine csw. Here we follow ref. [\cite=paper3] closely and impose exactly the same improvement condition:

[formula]

at L / a = 8, with ΔM as introduced in ref. [\cite=paper3].

The improvement condition eq. ([\ref=condition]) can in principle be imposed at any (not too large) value of M, where M is a specific definition of the current quark mass [\cite=paper3] derived from eq. (1). In order to guarantee a smooth behaviour of csw(g0) one should, however, make a definite choice, where a natural value is M = 0. Since ΔM turns out to be a very weak function of the quark mass M [\cite=paper3] [\cite=csw_paper], one may also compute ΔM for |aM|  ≪  1. In particular, for the values of aM chosen, here, the error introduced is negligible compared with the statistical one.

The simulations

The numerical simulations are performed on 16  ×  83 lattices, with boundary conditions as detailed in [\cite=paper3]. We use the Hybrid Monte Carlo (HMC) algorithm with the Sexton-Weingarten scheme to integrate the classical equations of motion [\cite=SexWei]. Our implementation of the HMC algorithm is described in detail in ref. [\cite=sw_hmc]. All simulations are performed on the massively parallel Alenia Quadrics (APE) computers. On the two versions of these machines that we have used with 256 and 512 nodes, we ran 32 and 64 independent simulations in parallel. Combined with a jack-knife method, this allows for a realistic error estimate on our observables.

We have run simulations at eight values of β = 6 / g20 in the range 5.2  ≤  β  ≤  12.0. Each simulation has at least 1280 molecular dynamics trajectories and typically 2500. Keeping the trajectory length fixed to 1, we reach typical acceptance rates of 95%. Despite the relatively large acceptance rates, we noticed that sometimes a system can get stuck and does not accept a number of (larger than, say, 10) trajectories. The problem is easily overcome by performing every n number of trajectories one with a much smaller step size. Of course, in order to be able to show that one generates the correct distribution, the value of n has to be chosen independently of the Monte Carlo history. We simply kept n fixed in each simulation.

We applied the improvement condition eq. ([\ref=condition]) in the small quark mass region, |aM| < 0.01. With Schrödinger functional boundary conditions, simulations at such small quark masses are unproblematic, since the massless Dirac operator of the Schrödinger functional with time extent T has a lowest eigenvalue with magnitude λ min  =  const. / T  +  O(g2) + O(M).

Owing to the O(g2) terms in λ min, the simulations slow down, when β is decreased. In detail the reason for this is threefold. First, going to smaller values of β we have to decrease the step size dt from, as an example, dt = 0.066 at β = 7.4 to dt = 0.027 at β = 5.4. Second, the condition number k of the preconditioned fermion matrix 2 (see e.g. ref. [\cite=sw_hmc] for a definition of [formula]) increases with decreasing β, as can be seen in fig. 1. The increasing values of k result in a growing number of conjugate gradient (CG) iterations when going to smaller β. Third, we find an increase of the autocorrelation time τ with decreasing β for observables such as the lowest eigenvalue of 2 or quark correlation functions at a given distance. Fortunately, it turns out that the autocorrelation time for ΔM is small, τ  ≈  2-4, and shows only a weak dependence on β.

As a rule, we find that the performance of the simulation algorithm does not significantly depend on the value of csw. The only exception are the autocorrelation times τ for which there are indications that they are particularly large when both csw and β are small.

Results

We determined ΔM at fixed β for various values of csw. From the -linear- dependence of ΔM on csw we can then extract the slope s = dΔM / dcsw. We found in practice that the slope is well described by a linear function of g20. In order to extract the desired improvement coefficients cimprsw(gi0) at the eight values of gi0  ,i = 1,...,8, where the simulations are performed, we fit all our data for ΔM to the form

[formula]

where

[formula]

and s1 as well as cimprsw(gi0) are fit parameters. The results for cimprsw are displayed as the full symbols in fig. 2. The solid line is a representation of these data, given by

[formula]

As already mentioned, for small values of β the simulations become very costly, and we were not able to perform simulations at β = 5.2 and small quark masses. We therefore switched to the following strategy: we take the parameterization eq. ([\ref=pade]) to extrapolate a little bit further in β, to β = 5.2. At the value of csw determined in this way, we then select a large quark mass, aM = 0.1 and try to verify that improvement is at work. Indeed, we find for β = 5.2 and csw = 2.02 that aΔM  =   - 0.0006(9). This indicates that our final result eq. ([\ref=pade]) can safely be used for β  ≥  5.2. Preliminary studies of the hadron spectrum in the improved theory suggest that β  ≥  5.2 yields the range of lattice spacings that is of interest to computations of hadronic properties [\cite=Talevi].

We want to emphasize that although, with our values of csw, the O(a) terms are cancelled, O(a2) effects remain and are not negligible for β  ≈  5.2, as will be discussed elsewhere [\cite=csw_paper].

This work is part of the ALPHA collaboration research programme. We thank DESY for allocating computer time to this project.