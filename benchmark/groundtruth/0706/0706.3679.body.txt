Introduction

Vapnik's statistical learning theory [\cite=Vap98] deals with three types of problems: pattern recognition, regression estimation and density estimation. However, the theory of bounds has primarily been developed for the computation of dichotomies only. Central in this theory is the notion of "capacity" of classes of functions. In the case of binary classifiers, the measure of this capacity is the famous Vapnik-Chervonenkis (VC) dimension. Extensions have also been proposed for real-valued bi-class models and multi-class models taking theirs values in the set of categories. Strangely enough, no generalized VC dimension was available so far for Q-category classifiers taking their values in [formula]. This was all the more unsatisfactory as many classifiers exhibit this property, such as the multi-layer perceptrons, or the multi-class support vector machines (M-SVMs). In this paper, the scale-sensitive Ψ-dimensions are introduced to fill this gap. A generalization of Sauer's lemma [\cite=Sau72] is given, which relates the covering numbers appearing in the standard guaranteed risk for large margin multi-category discriminant models to one of these dimensions, the margin Natarajan dimension. This latter dimension is then bounded from above for the architecture shared by all the M-SVMs proposed so far. This provides us with a sharper bound on their sample complexity. The organization of the paper is as follows. Section [\ref=sec:basic_bound] introduces the basic bound on the risk of large margin multi-category discriminant models. In Section [\ref=sec:gamma_psi-dims], the scale-sensitive Ψ-dimensions are defined, and the generalized Sauer lemma is formulated. The upper bound on the margin Natarajan dimension of the M-SVMs is then described in Section [\ref=sec:M-SVMs]. For lack of space, proofs are omitted. They can be found in [\cite=Gue04].

Basic theory of large margin Q-category classifiers

We consider Q-category pattern recognition problems, with 3  ≤  Q  <    ∞  . A pattern is represented by its description x∈X and the set of categories Y is identified with the set of indices of the categories, [formula]. The link between patterns and categories is supposed to be probabilistic. X and Y are probability spaces, and X  ×  Y is endowed with a probability measure P, fixed but unknown. Let [formula] be a random pair distributed according to P. Training consists in using a m-sample [formula] of independent copies of [formula] to select, in a given class of functions G, a function classifying data in an optimal way. The criterion to be optimized, the risk, is the expectation with respect to P of a given loss function. The way the functions in G perform classification must be specified. We consider classes of functions from X into [formula]. [formula] assigns x∈X to the category l if and only if gl(x)  >   max k  ≠  lgk(x). Cases of ex æ quo are treated as errors. This calls for the choice of a loss function [formula] defined on G  ×  X  ×  Y by [formula]. The risk of g is then given by:

[formula]

This study deals with large margin classifiers, when the underlying notion of multi-class margin is the following one.

Let g be a function from X into [formula]. Its margin on (x,y)∈X  ×  Y, Mxy(g,x,y), is given by:

[formula]

Basically, the central elements to assign a pattern to a category and to derive a level of confidence in this assignation are the index of the highest output and the difference between this output and the second highest one. The class of functions of interest is thus the image of G by application of an appropriate operator. Two such "margin operators" are considered here, Δ and Δ*.

Define Δ as an operator on G such that:

[formula]

[formula]

[formula], let Mx(g,x)  =   max kΔgk(x).

Define Δ* as an operator on G such that:

[formula]

[formula]

In the sequel, Δ# is used in place of Δ and Δ* in the formulas that hold true for both operators. The empirical margin risk is defined as follows.

Let [formula]. The risk with margin γ of g, Rγ(g), and its empirical estimate on sm, Rγ,sm(g), are defined as:

[formula]

For technical reasons, it is useful to squash the functions Δ#gk as much as possible without altering the value of the empirical margin risk. This is achieved by application of another operator.

For [formula], define πγ as an operator on G such that:

[formula]

[formula]

Let Δ#γ denote [formula] and Δ#γG be defined as the set of functions Δ#γg. The capacity of Δ#γG is characterized by its covering numbers.

Let (E,ρ) be a pseudo-metric space, E'  ⊂  E and [formula]. An ε-cover of E' is a coverage of E' with open balls of radius ε the centers of which belong to E. These centers form an ε-net of E'. A proper ε-net of E' is an ε-net of E' included in E'. If E' has an ε-net of finite cardinality, then its covering number N(ε,E',ρ) is the smallest cardinality of its ε-nets. If there is no such finite cover, then the covering number is defined to be ∞  . N(p)(ε,E',ρ) will designate the covering number of E' obtained by considering proper ε-nets only.

The covering numbers of interest use the following pseudo-metric:

Let G be a class of functions from X into [formula]. For a set sXn  ⊂  X of cardinality n, define the pseudo-metric [formula] on G as:

[formula]

Let [formula]. The following theorem extends to the multi-class case Corollary 9 in [\cite=Bar98].

Let sm be a m-sample of examples independently drawn from a probability distribution on X  ×  Y. With probability at least 1  -  δ, for every value of γ in (0,1], the risk of any function g in a class G is bounded from above by:

[formula]

Studying the sample complexity of a classifier G can thus amount to computing an upper bound on N(p)∞  ,  ∞(γ / 4,Δ#γG,2m). In [\cite=GueMauSur05], we reached this goal by relating these numbers to the entropy numbers of the corresponding evaluation operator. In the present paper, we follow the traditional path of VC bounds, by making use of a generalized VC dimension.

Bounding covering numbers in terms of the margin Natarajan dimension

The Ψ-dimensions are the generalized VC dimensions that characterize the learnability of classes of [formula]-valued functions.

Let F be a class of functions on a set X taking their values in the finite set [formula]. Let Ψ be a set of mappings ψ from [formula] into [formula], where *   is thought of as a null element. A subset [formula] of X is said to be Ψ-shattered by F if there is a mapping [formula] in Ψn such that for each vector vy of [formula], there is a function fy in F satisfying

[formula]

The Ψ-dimension of F, denoted by Ψ(F), is the maximal cardinality of a subset of X Ψ-shattered by F, if it is finite, or infinity otherwise.

One of these dimensions needs to be singled out, the Natarajan dimension.

Let F be a class of functions on a set X taking their values in [formula]. The Natarajan dimension of F, (F), is the Ψ-dimension of F in the specific case where Ψ is the set of Q(Q - 1) mappings ψk,l, (1  ≤  k  ≠  l  ≤  Q), such that ψk,l takes the value 1 if its argument is equal to k, the value - 1 if its argument is equal to l, and *   otherwise.

The fat-shattering dimension characterizes the uniform Glivenko-Cantelli classes among the classes of real-valued functions.

Let G be a class of functions from X into [formula]. For [formula], [formula] is said to be γ-shattered by G if there is a vector [formula] such that, for each vector [formula], there is a function gy∈G satisfying

[formula]

The fat-shattering dimension of G, [formula], is the maximal cardinality of a subset of X γ-shattered by G, if it is finite, or infinity otherwise.

Given the results available for the Ψ-dimensions and the fat-shattering dimension, it appears natural, to study the generalization capabilities of classifiers taking values in [formula], to consider the use of capacity measures obtained as mixtures of the two concepts, namely scale-sensitive Ψ-dimensions.

Let G be a class of functions on a set X taking their values in [formula]. Let Ψ be a family of mappings ψ from [formula] into [formula]. For [formula], a subset [formula] of X is said to be γ-Ψ-shattered by Δ#G if there is a mapping [formula] in Ψn and a vector [formula] in [formula] such that, for each vector [formula] of [formula], there is a function gy in G satisfying

[formula]

The γ-Ψ-dimension of Δ#G, Ψ-dim(Δ#G,γ), is the maximal cardinality of a subset of X γ-Ψ-shattered by Δ#G, if it is finite, or infinity otherwise.

The margin Natarajan dimension is defined accordingly.

Let G be a class of functions on a set X taking their values in [formula]. For [formula], a subset [formula] of X is said to be γ-N-shattered by Δ#G if there is a set [formula] of n pairs of distinct indices in [formula] and a vector [formula] in [formula] such that, for each binary vector [formula], there is a function gy in G satisfying

[formula]

The Natarajan dimension with margin γ of the class Δ#G, (Δ#G,γ), is the maximal cardinality of a subset of X γ-N-shattered by Δ#G, if it is finite, or infinity otherwise.

For this scale-sensitive Ψ-dimension, the connection with the covering numbers of interest, or generalized Sauer lemma, is the following one.

Let G be a class of functions from a domain X into [formula]. For every value of γ in (0,1] and every [formula] satisfying [formula], the following bound is true:

[formula]

where [formula].

This theorem is the central result of the paper (and the novelty in the revised version of [\cite=Gue04]). What makes it a nontrivial Q-class extension of Lemma 3.5 in [\cite=AloBenCesHau97] is the presence of both margin operators. The reason why Δ* appears in the covering number instead of Δ is the very principle at the basis of all the variants of Sauer's lemma: two functions separated with respect to the functional pseudo-metric used (here [formula]) shatter (at least) one point in sXn. This is true for Δ*γG, or more precisely its η-discretization, not for ΔγG (see Section 5.3 in [\cite=Gue04] for details). One can derive a variant of Theorem [\ref=theorem:covering_margin_Natarajan_final] involving [formula]. This alternative is however of lesser interest, for reasons that will appear below.

Margin Natarajan dimension of the M-SVMs

We now compute an upper bound on the margin Natarajan dimension of interest when G is the class of functions computed by the M-SVMs. These large margin classifiers are built around a Mercer kernel. Let κ be such a kernel on X and [formula] the corresponding reproducing kernel Hilbert space (RKHS) [\cite=Aro50]. Let Φ be any of the mappings on X satisfying:

[formula]

where 〈.,.〉 is the dot product of the [formula] space. "The" feature space traditionally designates any of the Hilbert spaces [formula] spanned by the [formula]. By definition of a RKHS, [formula] is the class of functions [formula] from X into [formula] of the form:

[formula]

where the xik are elements of X (the βik and bk are scalars), as well as the limits of these functions when the sets [formula] become dense in X in the norm induced by the dot product. Due to ([\ref=eq:kernel_trick]), H can also be seen as a multivariate affine model on [formula]. Functions h can then be rewritten as:

[formula]

where vectors wk are elements of [formula]. They are thus described by the pair [formula] with [formula] and [formula]. Let [formula] stand for the product space HQκ. Its norm [formula] is given by [formula].

A M-SVM is a large margin multi-category discriminant model obtained by minimizing over the hyperplane [formula] of H an objective function of the form:

[formula]

where the empirical term, used in place of the empirical risk, involves a loss function [formula] which is convex.

The M-SVMs only differ in the nature of [formula]. The specification of this function is such that the introduction of the penalizer [formula] tends to maximize a notion of margin directly connected with the one of Definition [\ref=definition:functional_margin]. The formulation of the generalized Sauer lemma provided here (Theorem [\ref=theorem:covering_margin_Natarajan_final]) is the one obtained under the weakest hypotheses. Proceeding as in the bi-class case, we express below a bound on the margin Natarajan dimension of the M-SVMs as a function of the volume occupied by data in [formula] and constraints on [formula], thus restricting the study to functions with a well-defined range. In that case, a variant of Theorem [\ref=theorem:covering_margin_Natarajan_final] can be derived from Lemma 7 in [\cite=Gue04] which does not involve πγ but relates the covering numbers of Δ*G to the margin Natarajan dimension of ΔG. Its use for M-SVMs is advantageous since [formula] is easier to bound than [formula] (nonlinearity is difficult to handle). This change of generalized Sauer lemma calls for the use of an intermediate formula relating the covering numbers of Δ*γH and Δ*. It is provided by the following lemma.

Let H be the class of functions that a Q-category M-SVM can implement under the hypothesis [formula]. Let [formula] satisfy 0  <  ε  ≤  γ  ≤  1. Then

[formula]

A final theorem then completes the construction of the guaranteed risk.

Let [formula] be the class of functions that a Q-category M-SVM can implement under the hypothesis that Φ(X) is included in the closed ball of radius [formula] about the origin in [formula] and the constraints [formula] and [formula]. Then, for any positive real value ε, the following bound holds true:

[formula]

The proof follows the line of argument of the corresponding bi-class result, Theorem 4.6 in [\cite=BarSha99]. This involves a generalization of Lemma 4.2 which can only be performed for the Δ operator. The discussion on the presence of both Δ and Δ* in Theorem [\ref=theorem:covering_margin_Natarajan_final] is thus completed. Putting things together, the control term of the guaranteed risk decreases with the size of the training sample as ln (m)  ·  m- 1 / 2. This represents an improvement over the rate obtained in [\cite=GueMauSur05], m- 1 / 4.

Conclusions and future work

A new class of generalized VC dimensions dedicated to large margin multi-category discriminant models has been introduced. They can be seen either as multivariate extensions of the fat-shattering dimension or scale-sensitive Ψ-dimensions. Their finiteness (for all positive values of the scale parameter γ) is also a necessary and sufficient condition for learnability. A generalized Sauer lemma has been provided for one of these capacity measures, the margin Natarajan dimension. This latter dimension has been bounded from above in the case where the classifier is a multi-class SVM. This study provides us with new arguments to support the thesis that the theory of multi-category pattern recognition cannot be developed by extending in a straightforward way bi-class results. We are currently making use of the specificities identified here to extend new concentration inequalities to the multi-class case with the goal to obtain improved convergence rates.