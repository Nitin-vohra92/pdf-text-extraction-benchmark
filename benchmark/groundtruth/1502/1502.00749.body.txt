Data-Driven Scene Understanding with Adaptively Retrieved Exemplars

Introduction

ignificant progresses have been identified in solving the task of semantic image understanding [\cite=shotton2006textonboost] [\cite=ladicky2010graph]. However, these methods usually build upon supervised learning with fully annotated data that are expensive and sometimes limited in large-scale scenarios [\cite=lin2009grammar] [\cite=lin2012representing]. Several weakly supervised methods were proposed [\cite=zhang2013sparse] to reduce the overload of data annotating, which can be trained with only image-level labels indicating the classes presented in the images. Recently, data-driven approaches [\cite=liu2009nonparametric] [\cite=luo2012joint] receive increasing attentions, which tend to leverage knowledges from auxiliary data in weakly supervised fashions, and demonstrate very promising applications. Following this trend, one interesting but challenging problem arises for the scene understanding: How to parse the raw images in virtue of the strength of numerous unsegmented but tagged images, as the image-level tags can be achieved easier. In this work, we investigate this problem by developing a unified framework, in which the two following steps perform iteratively, as Fig. [\ref=fig:framework] illustrates.

In Step. 1, we search for similar images as the exemplars (i.e. references) matching to the target image from the auxiliary database (in Fig. 1 (b)), and these references are required to share similar semantic concepts with the target. Moreover, we enforce the representation to be semantically meaningful: The references that are selected should contain consistent tags. The tags of the target image can be also taken into account during the iteration, as they can be determined by the last label assignment step (in Step. 2). We solve this step using the proximal gradient method.

In Step. 2, we assign labels to the pixels of the target by propagating semantics from the selected references. We create a graphical model, in which the vertices are the superpixels from the target image and its references. There are two types of edges defined over the graph, which is inspired by [\cite=lin2010layered]: (i) the inner-edges connecting the spatial adjacent vertices within the target; (ii) the outer-edges connecting the vertices of the target to those of its references. The potentials are then derived into an MRF form by aggregating the two types of edge connections, which can be fast solved by the Graph Cuts algorithm [\cite=ladicky2010graph]. The two above steps are mutually conditional, providing complementary information to each other. We present a novel probabilistic Expectation-Maxima (EM) formulation making the two steps bootstrapped by each other to conduct results in a self-driven manner. In addition, the proposed framework can also be directly applied on new test image to perform multi-label image annotation. Our approach is evaluated on several benchmarks, and outperforms other state-of-the-art methods.

Related work

Traditional efforts for scene understanding mainly focused on capturing scene appearances, structures and spatial contexts by developing combinatorial models, e.g., CRF [\cite=shotton2006textonboost] [\cite=ladicky2010graph], Texton-Forest [\cite=shotton2008semantic], Graph Grammar [\cite=lin2014AOG]. These models were generally founded on supervised learning techniques, and required manually prepared training data containing labels at pixel level.

Several weakly supervised methods are proposed to indicate the classes presented in the images with only image-level labels. For example, Winn et al. [\cite=winn2005locus] proposed to learn object classes based on unsupervised image segmentation. Zhang et al. [\cite=zhang2013sparse] learned classification models for all scene labels by selecting representative training samples, and multiple instance learning was utilized in [\cite=vezhnevets2011weakly]. Some nonparametric approaches have been also studied that solve the problems by searching and matching with an auxiliary image database. For example, an efficient structure-aware matching algorithm was discussed in [\cite=liu2009nonparametric] to transfer labels from the database to the target image, but the pixelwise annotation was required for the auxiliary images.

Problem Formulation

In this section, we phrase the problem in a probabilistic formulation, and then discuss the Expectation-Maximization (EM) inference framework for optimization.

Probability Model

Let [formula] denote a set of images {Ik} with image-level labels {Lk}. Each image Ik is represented as a set of superpixels {xki}nki = 1, where nk is the number of superpixels in Ik.

Given the target image It, our task is to predict its image-level labels Lt, as well as to assign each superpixel xti a label yti∈Lt. Let Yt denote the whole label assignment, i.e., Yt  =  {yti}nti = 1, we can define the joint probability distribution of target image It and the label assignment Yt.

We also define a binary-valued correspondence variable [formula] such that αk = 1 if image Ik is selected as a reference for the target image. [formula] is treated as a hidden variable.

The complete probability model is defined as follow,

[formula]

and we further derive it by summing out [formula] as,

[formula]

Then the optimal label assignment Yt by maximizing the probability,

[formula]

and we propose to solve it iteratively under an Expectation-Maximization (EM) framework.

The EM Iterations

It has been shown that estimating Y*t from [formula] is equivalent to minimize the following energy function [\cite=neal1998view]:

[formula]

where [formula] is the posterior of the latent variable [formula].

Since the second term in Eq. ([\ref=eq:target-energy]) is a constant, the optimization iterates with two steps: (i) The E-step minimizes the energy L(Q,Yt) with respect to [formula] with Yt fixed. (ii) The M-step minimizes the energy L(Q,Yt) with respect to Yt with [formula] fixed.

(i) The E-step: Approximating [formula] :

The posterior of the latent variable [formula] is defined as,

[formula]

where Z is the normalization constant of the probability. The energy [formula] evaluates the appearance and semantics consistency, which is specified as,

[formula]

The first term ESc measures the appearance similarity between It and images in [formula], defined as,

[formula]

where β is the tradeoff parameter used to balance the sparsity and the reconstruction error. F(  ·  ) is an m-dimemsional global feature of an image, and [formula] is a matrix consisting of all the features of images in [formula].

The second term ESa in Eq. ([\ref=eq:energy-alpha]) measures semantic consistency, defined as,

[formula]

where Sij measures the semantic similarity between [formula], as,

[formula]

and A in Eq. ([\ref=eq_E_sa]) is a diagonal matrix where [formula] and L = A- 1 / 2(A - S)A- 1 / 2, in which L is the normalized Laplacian matrix.

Images with similar semantics should be encoded with similar activations. In other words, if two images have common labels, then the activations corresponding to this image pair should also be close to each other. The distance between their activation codes should be small.

D is a diagonal matrix where Dkk measures the semantic dissimilarity between [formula] and the target image It. Thus the second term [formula] penalizing the target It is reconstructed by images that are semantically dissimilar with It. We define the diagonal matrix D by

[formula]

where Lt are the latent labels of the target image, which are unknown at the beginning, and can be determined from Yt during the later iterations.

(ii) The M-step: estimating Yt :

The M-step performs to minimize the following energy function with respect to Yt:

[formula]

However, summing out [formula] for all possibilities demands very expensive computational cost, particularly to process a large number N of data. Instead, we seek a lower-bound of EM(Yt). Assume that we can infer [formula] with the maximized probability [formula] by the E-step. Then we can define the joint distribution of (It,Yt) conditioned on [formula], and we have

[formula]

It is straightforward in the context of our task, as the cumulative density of assigning labels from good references (i.e. given [formula]) is higher than that with general cases. Thus, we set the lower-bound as,

[formula]

where [formula] is fixed by the last E-step. The energy to be minimized can be further simplified as,

[formula]

where we will specify [formula] with a combinatorial graph model in Sec. [\ref=sec:label-assignment].

Inference and Implementation

Within the EM formulation, the inference algorithm iterates with two steps: (i) computing [formula] in the E-step for reference retrieval and (ii) solving the optimal labeling Y*t with the selected references in the M-step.

Adaptive Reference Retrieval

Maximizing [formula] is equivalent to minimizing the energy defined in Eq. ([\ref=eq:energy-alpha]) w.r.t. [formula]. Notice that [formula] can be regarded as a semantic-aware sparse representation, where we jointly model the appearance reconstruction with semantic consistency. Fig. [\ref=fig:label-assignment] intuitively illustrates this model, and it can be rewritten as,

[formula]

where [formula]. The semantic associated terms in Eq. ([\ref=eq_combine]) can be phrased in convex forms, thus we can use the proximal gradient method to solve this problem efficiently. The optimization process is shown in Algorithm [\ref=alg:Ea].

Given the optimized [formula], we can simply select the references according to coding co-efficiencies, e.g., select by thresholding. And we set αk = 0 if image Ik is not selected.

Aggregated Label Assignment

Given the references determined by [formula], we propagate their semantic labels to It by constructing a combinatorial graph. We extract superpixels from both It and the references as graph vertices, and connect them with probabilistic edges incorporating their affinities, as Fig. [\ref=fig:graphical-model] illustrates.

Two types of edges are considered over the graph: (i) the inner-edges ω connecting the spatial neighboring superpixles within the target (red wavy line in Fig. [\ref=fig:graphical-model]) , and (ii) the outer-edges ξ connecting the superpixels of the target to those of its references (straight green line in Fig. [\ref=fig:graphical-model]) . And each superpixel of the target connects with the q most similar superpixels of each reference.

We define [formula] in Eq. ([\ref=eq:mstep_star]) on the graphical model as follows,

[formula]

where ω is the inner edges. The optimization of Eq. ([\ref=eq:mstep_star]) becomes a tractable graphical model optimization problem. To derive the potentials of assigning labels to one vertex of the target [formula] in Eq. ([\ref=eq:mstep-simplified]), we propose the semantic-based superpixel density prior, which is defined as,

[formula]

where ρ(xti,Ik) denotes the density of superpixel xti in image Ik, which is defined as,

[formula]

where ξ denotes outer-edges, Nξ is the number of outer-edges, and f(  ·  ) is the feature vector of a superpixel. This density measures the similarity between the superpixel xti in the target and its neighboring superpixels connected by outer-edges in the reference image Ik, thus it implicitly exhibits the probability that xti sharing the same labels with its reference Ik.

The pairwise potentials, i.e. φ(yti,ytj,xti,xtj) in Eq. ([\ref=eq:mstep-simplified]), encourages the smoothness between neighboring superpixels within the target, as,

[formula]

where δ(  ·  ) is the indicator function.

Thus the approximate solutions Eq. ([\ref=eq:mstep-simplified]) can be found using alpha-beta swap algorithms of graph cuts. The sketch of our framework is shown in Algorithm [\ref=alg:overall].

Image Annotation

We propose a simple method to transfer n labels to a test image It from the query's K nearest neighbors in the training set. For a given test image It, the sparse reconstruction coefficient vector [formula] is determined by soloving the problem in Eq. ([\ref=eq_combine]), where we set λ = 0, and set other parameters as the same as described in section [\ref=sec_pars]. The optimal sparse coefficient solution denote as [formula], then let its top K largest value denote as [formula] consponding with image label indicator [formula]. The label vector probability of test image can then be obtained as:

[formula]

where i is the i-th component of vector [formula]. The labels corresponding to the top few largest values in [formula] are considered as the final annotationns of the test image.

We compare the following two annotation methods, and find out that the sparse coefficient [formula] is extremely useful for image annotation. (i) weighed: That is the annotation weighed by sparse reconstruction coefficient i. (ii) unweighed: We set [formula] in manual.

Besides, we also compared with classical works for image annotation, the proposed method here have the following characteristics: (i) the propagation process is robust and less sensitive to the image noises owing to the semantic constraints in image retrieval step. (ii) the proposed algorithm is scalable to large-scale, and retrieval images by jointly matching their appearances as well as the semantics.

Experiment

In this section, we conduct extensive experiments to validate the performance of our method and discuss the experimental analysis. We also conduct an empirical study on the effectiveness of the proposed EM iterations.

Implemenation details: Five parameters are required to be set in our framework. We set q = 20 to construct the q-nearst graph, and set p = 10 to retrieval 10 images as reference for each test image. In the experiment we also set λ = 1 empirically. The other parameters β and γ are introduced in Sec. ([\ref=sec_pars]).

Datasets

To verify the effectivenes of our method, we conduct experiments on two challenging datasets, i.e. MSRC [\cite=shotton2006textonboost] and VOC 2007 [\cite=pascal-voc-2007], by comparing with state-of-the-art. We use the standard average per-class measure (average accuracy) to evaluate the performance. For each test image, we use the training set as the auxiliary data for our framework.

Exp-I: Image Semantic Segmentation

Parameter Analysis

Specifically, we focus on the effects of β and γ which control the influence of appearance term and semantic term in Eq. ([\ref=eq_combine]), and these two parameters are crucial to our results. The range of β and γ are both set to {0,0.05,0.10,0.15,0.20,0.25,0.30}. The semantic segmentation performance is used to tune parameters.

We used MSRC dataset to finetune the parameters. The results of changing the parameter values are presented in Fig. [\ref=fig_pars], from which we can observe the following conclusions:

When β and γ increase from small values to large values, the performance varies apparently, which shows that the sparse term and semantic constraint term have great impacts on the performance.

Mean average precision (MAP) reach the peak points (0.71) when β = 0.1 and γ = 0.2 on MSRC which lie in the middle range and the precision do not increase monotonically when β and γ increase. In the following experiments, we adopt the best parameter settings on all datasets.

Experiments on MSRC dataset

Given this insight, we compare the proposed method with the following stae-of-the-art algorithms: MIM[\cite=vezhnevets2011weakly], and K. Zh[\cite=zhang2013sparse]. Table [\ref=tb:msrc_and_voc] shows that our algorithm outperforms the others. Benefit from the semantic constraints incorporated in our approach, we achieve a significant improvements for certain difficult classes, e.g., chair and cat. Serveral visualized results with the corresponding ground-truths are presented in Fig. [\ref=fig:segment_result], and more semantic segmentation results are in supplementary material as to the limited space of article.

Experiments on VOC 2007 dataset

Few performance on VOC 2007 dataset is reported, due to the 20 extremely challenging categories it contains. Here we compare with the weakly supervised STF[\cite=shotton2008semantic] by running the code provide by the author. We also compare our method with [\cite=zhang2013sparse]. Results are reported in Table [\ref=tb:msrc_and_voc], and our methods outperforms [\cite=zhang2013sparse] by 3%. It takes about 8 seconds per image with an un-optimized matlab implementation for semantic segmentation, on a 64-bit system with Core-4 3.6 GHz CPU, 4GB memory (extracting features: 1s; sparse coding with semantic constraints: 5s; optimization by GraphCuts: 2s).

Moreover, we validate the effectiveness of the proposed EM iterations from two aspects. First, we plot the energy Eα in each iteration, which is the energy of semantic-aware spare coding defined in Eq. ([\ref=eq_combine]), as shown in Fig. [\ref=fig:energy_descend]. We also present some intermediate results during the EM iterations, as Fig. [\ref=fig:iteration_result] shown, which empirically supports the effectiveness of the iterations.

Exp-II: Image Annotation on Test Image

Benchmarks and Metrics

Three popular algorithms are implemented as benchmark baselines for the image annotation task: MAHR[\cite=huang2012multi-hypothesis], MLkNN[\cite=zhang2007ml], ML-LOC[\cite=huang2012multi].

MLkNN and ML-LOC are the state-of-the-art multi-label annotation algorithms in literature. They have been reported to outperform most other multi-label annotating algorithms, such as RankSVM [\cite=elisseeff2001kernel]. Thus, we do not plan to further implement the latter two in this work. We evaluate and compare among the three algorithms over two datasets, MSRC and VOC 2007, each of which is randomly and evenly split into training and testing subset. The image annotation performance is measured by mean average precision, which is widely used for evaluating the performances of ranking related tasks.

Results and Analysis

The weighed method is outperforms the unweighed one as Table [\ref=tb:annotation] shown. It notices that the sparse coefficient [formula] is useful to improve the image annotation performance, and useful for image semantic segmentation apparently, as we do the image retrieval by jointly matching their appearance as well as the semantics. The larger [formula] means the more similar in semantics between the test image and image Ii (i.e. sharing the more common labels).

The weighed method proposed outperforms the three classical methods listed in Table [\ref=tb:annotation]. Some example image annotation results from the MSRC and VOC 2007 dataset are shown in Fig. [\ref=fig_annotation]. Here we only display the top 3 or 2 labels for MSRC and VOC 2007, since the average number of labels for each images in MSRC and VOC 2007 is 3 and 2 respectively.

Conclusions

In this paper proposes a new framework for data-driven semantic image segmentation where only image-level labels are available, and it is also useful for image annotation. Compared with the traditional supervised learning methods, our framework is more flexible for real applications such as online image retrieval. In the experiments, we demonstrate very promising results on the standard benchmarks of scene understanding. In future work, we can improve the algorithm efficiency by utilizing parallel implementation and validate our approach on larger scale datasets.