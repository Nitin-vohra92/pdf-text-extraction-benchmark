Lemma Proposition Corollary

Definition Example Remark Assumption

On the spectral norm of Gaussian random matrices

Introduction

Let X be a symmetric random matrix with independent mean zero entries. If the variances of the entries are all of the same order, this model is known as a Wigner matrix and has been widely studied in the literature (e.g., [\cite=AGZ10]). Due to the large amount of symmetry of such models, extremely precise analytic results are available on the limiting behavior of fine-scale spectral properties of the matrix. Our interest, however, goes in an orthogonal direction. We consider the case where the variances of the entries are given but arbitrary: that is, we consider structured random matrices where the structure is given by the variance pattern of the entries. The challenge in investigating such matrices is to understand how the given structure of the matrix is reflected in its spectral properties.

In particular, we are interested in the location of the edge of the spectrum, that is, in the expected spectral norm [formula] of the matrix. When the entries of the matrix are i.i.d., a complete understanding up to universal constants is provided by a remarkable result of Seginer [\cite=Seg00] which states that the expected spectral norm of the matrix is of the same order as the largest Euclidean norm of its rows and columns. Unfortunately, this result hinges crucially on the invariance of the distribution of the matrix under permutations of the entries, and is therefore useless in the presence of nontrivial structure. It is noted in [\cite=Seg00] that the conclusion fails already in the simplest examples of structured random matrices with bounded entries. Surprisingly, however, no counterexamples to this statement are known for structured random matrices with independent Gaussian entries. This observation has led to the following conjecture proposed by R.  Latała (see also [\cite=Lat05] [\cite=RS13]).

Throughout the remainder of this paper, X will denote the d  ×  d symmetric random matrix with entries Xij = bijgij, where {gij:i  ≥  j} are independent standard Gaussian random variables and {bij:i  ≥  j} are given nonnegative scalars. We write [formula] if a  ≤  Cb for a universal constant C, and [formula] if [formula] and [formula].

The expected spectral norm satisfies

[formula]

The lower bound in Conjecture [\ref=conj:1] holds trivially for any deterministic matrix: if a matrix has a row with large Euclidean norm, then its spectral norm must be large. Conjecture [\ref=conj:1] suggests that for Gaussian random matrices, this is the only reason why the spectral norm can be large. It is not at all clear, however, what mechanism might give rise to this phenomenon, particularly as the Gaussian nature of the entries must play a crucial role for the conjecture to hold.

Recently, Bandeira and the author [\cite=BvH15] proved a sharp dimension-dependent upper bound on [formula] (we refer to [\cite=BvH15] for a discussion of earlier work on this topic):

The expected spectral norm satisfies

[formula]

The combinatorial proof of this result sheds little light on the phenomenon described by Conjecture [\ref=conj:1]. Nonetheless, the right-hand side of this expression is a natural upper bound on the right-hand side of Conjecture [\ref=conj:1] [\cite=BvH15]. On the other hand, the terms in this bound admit another natural interpretation. A simple computation shows that the first term in this bound is precisely [formula], while the second term is an upper bound on [formula]. This suggests the following alternative to Conjecture [\ref=conj:1] that is also consistent with Theorem [\ref=thm:bvh].

The expected spectral norm satisfies

[formula]

Once again, the lower bound in Conjecture [\ref=conj:2] holds trivially (cf. [\cite=BvH15]): the first term follows readily from Jensen's inequality, while the second term follows as the spectral norm of any matrix is bounded below by the magnitude of its largest entry. Thus the two terms in the lower bound reflect two distinct mechanisms that control the spectral norm of any random matrix: a random matrix has large spectral norm if it is large on average (as is quantified by [formula]; note that the expectation here is inside the norm!), or if one of its entries is large (as is quantified by [formula]). Conjecture [\ref=conj:2] suggests that for Gaussian random matrices, these are the only reasons why the spectral norm can be large.

In many cases the improvement of Conjectures [\ref=conj:1] and [\ref=conj:2] over Theorem [\ref=thm:bvh] is modest, as the latter bound is already tight under mild assumptions. On the one hand, if [formula], then the first term in Theorem [\ref=thm:bvh] dominates and therefore [formula] as predicted by Conjecture [\ref=conj:2]. On the other hand, if a polynomial number [formula] of entries Xkl of the matrix have variance of the same order as the largest variance [formula], then [formula] and thus Theorem [\ref=thm:bvh] also implies Conjecture [\ref=conj:2]. These observations indicate that Theorem [\ref=thm:bvh] already implies Conjecture [\ref=conj:2] when the matrix is "not too sparse". Nonetheless, the apparent sharpness of Theorem [\ref=thm:bvh] belies a fundamental gap in our understanding of the probabilistic mechanisms that control the spectral norm of Gaussian random matrices: the phenomena predicted by Conjectures [\ref=conj:1] and [\ref=conj:2] are inherently dimension-free, while the assumptions under which Theorem [\ref=thm:bvh] is tight exhibit nontrivial dependence on dimension. The resolution of Conjectures [\ref=conj:1] and [\ref=conj:2] would therefore provide a substantially deeper insight into the structure of Gaussian random matrices than is obtained from Theorem [\ref=thm:bvh].

The aim of this paper is to develop a number of new techniques and insights that contribute to a deeper understanding of Conjectures [\ref=conj:1] and [\ref=conj:2]. While our results fall short of resolving these conjectures, they provide strong evidence for their validity and shed significant light on the geometry of the problem.

We begin by observing that Conjectures [\ref=conj:1] and [\ref=conj:2] are in fact equivalent, which is not entirely obvious at first sight. In fact, our first result provides an explicit expression for the right-hand side in Conjectures [\ref=conj:1] and [\ref=conj:2] in terms of the coefficients bij. (A much more complicated expression in terms of Musielak-Orlicz norms can be found in [\cite=RS13], but is too unwieldy to be of use in the sequel.)

Conjectures [\ref=conj:1] and [\ref=conj:2] are equivalent:

[formula]

where the matrix {b*ij} is obtained by permuting the rows and columns of the matrix {bij} such that [formula].

As the bound of Theorem [\ref=thm:bvh] appears to be tantalizingly close to the expression in Theorem [\ref=thm:main1], one might hope that the latter could be established by a refinement of the methods that were developed in [\cite=BvH15]. The proof of Theorem [\ref=thm:bvh] in [\cite=BvH15] relies heavily on the moment method, which is widely used in the analysis of random matrices. This method is based on the elementary observation that [formula] for any d  ×  d symmetric matrix X and p  ≥  1, so that

[formula]

The essential feature of the moment method is that the right-hand side of this expression is the expectation of a polynomial in the entries of the matrix, which admits an explicit expression that is amenable to combinatorial analysis. By its very nature, any proof using the moment method cannot directly bound [formula]; instead, this method bounds the larger quantity [formula], which is what is actually done in [\cite=BvH15]. For the latter quantity, however, it is readily seen that the result of Theorem [\ref=thm:bvh] is already sharp without any additional assumptions:

[formula]

The upper bound is proved in [\cite=BvH15], while the lower bound follows along the lines of Conjecture [\ref=conj:2] from the estimate [formula]. We therefore see that the moment method is exploited optimally in the proof of Theorem [\ref=thm:bvh], so that the resolution of Conjectures [\ref=conj:1] and [\ref=conj:2] cannot be addressed by the same technique that gave rise to Theorem [\ref=thm:bvh].

Nonetheless, by a slicing procedure that applies Theorem [\ref=thm:bvh] separately at different scales, we can already establish that Conjectures [\ref=conj:1] and [\ref=conj:2] hold up to a very mild dimensional factor. This is our second main result.

The expected spectral norm satisfies

[formula]

While this result still exhibits an explicit dependence on dimension, the point of Theorem [\ref=thm:main2] is that the very mild dimensional factor [formula] is of much smaller order than the natural scale [formula] that appears in the sharp dimension-dependent bound of Theorem [\ref=thm:bvh]; in this sense, Theorem [\ref=thm:main2] could be viewed as providing significant evidence for validity of Conjectures [\ref=conj:1] and [\ref=conj:2].

In the final part of this paper, we develop an entirely different approach for bounding the spectral norm of Gaussian random matrices. Unlike the methods developed so far, this approach is genuinely dimension-free and sheds significant light on the probabilistic mechanism that lies at the heart of Conjectures [\ref=conj:1] and [\ref=conj:2]. The starting point for this approach is the elementary observation that

[formula]

is the expected supremum of a Gaussian process indexed by the Euclidean unit ball B2. It is well known that such quantities are completely characterized, up to universal constants, by the geometry of the metric space (B2,d), where

[formula]

is the natural metric associated with the Gaussian process (cf.  [\cite=Tal14]). Therefore, in principle, understanding the spectral norm of Gaussian random matrices requires "only" a sufficiently good understanding of the geometry of the metric space (B2,d). To this end, we show that the geometry of (B2,d) can be related to the Euclidean geometry of certain nonlinear deformations of the unit ball. The geometric structure exhibited by this mechanism appears to almost resolve Conjectures [\ref=conj:1] and [\ref=conj:2], but we do not know how to optimally exploit this structure. Even a crude application of this idea, however, suffices to prove a nontrivial dimension-free bound.

The expected spectral norm satisfies

[formula]

It is instructive to compare this bound with the expression in Theorem [\ref=thm:main1]. Using [formula], it is readily seen that Theorem [\ref=thm:main3] implies the bound

[formula]

While this estimate falls slightly short of the conjectured optimal bound of Theorem [\ref=thm:main1] (due to the wrong power on the logarithm), it is dimension-free precisely in the expected manner. Together with the natural geometric structure exhibited in the proof, this provides further evidence for the validity of Conjectures [\ref=conj:1] and [\ref=conj:2]. The result of Theorem [\ref=thm:main3] is complementary to Theorem [\ref=thm:bvh]: while Theorem [\ref=thm:bvh] is often sharp, Theorem [\ref=thm:main3] can give a substantial improvement for highly inhomogeneous matrices. For example, Theorem [\ref=thm:main3] readily implies the dimension-free bound of Latała [\cite=Lat05], which could not be reproduced using Theorem [\ref=thm:bvh].

The statement of Theorem [\ref=thm:main3] was chosen for sake of illustration; it is in fact a direct consequence of a sharper bound that arises from the proof. This sharper bound both improves somewhat on Theorem [\ref=thm:main3] for arbitrary matrices, and is able to establish the validity of Conjectures [\ref=conj:1] and [\ref=conj:2] in certain special cases. For example, we will establish these conjectures under the assumption that the matrix of variances {b2ij} is positive definite or has a small number of negative eigenvalues. While these special cases are restrictive, they emphasize that the underlying geometric principle is not yet exploited optimally in the proof. The elimination of this inefficiency provides a promising route to the resolution of Conjectures [\ref=conj:1] and [\ref=conj:2].

The ideas described above are developed in detail in the sequel. Our main results, Theorems [\ref=thm:main1], [\ref=thm:main2], and [\ref=thm:main3], are proved in sections [\ref=sec:gaus], [\ref=sec:slice], and [\ref=sec:geom], respectively.

Gaussian estimates

The aim of this section is to prove Theorem [\ref=thm:main1]. We will, in fact, consider an additional quantity beside those that appear in Conjectures [\ref=conj:1] and [\ref=conj:2]. Let [formula] be independent standard Gaussian variables, and consider the quantity

[formula]

This quantity will appear naturally from the geometry that is to be developed in section [\ref=sec:geom] below. The maximum is taken here over random variables with the same distribution as in Conjecture [\ref=conj:1] (note that these quantities differ only in that b2ijg2j is replaced by X2ij = b2ijg2ij); however, in the above quantity these variables are dependent, while the maximum is taken over independent variables in Conjecture [\ref=conj:1]. Nonetheless, these quantities prove to be of the same order. The equivalence of the various quantities considered below indicates that the phenomena described by Conjectures [\ref=conj:1] and [\ref=conj:2] can appear in many different guises, providing us with substantial freedom in how to approach the proof of these conjectures.

The following quantities are of the same order:

[formula]

where we recall that the matrix {b*ij} is obtained by permuting the rows and columns of the matrix {bij} such that [formula].

The proof of the upper bound in Theorem [\ref=thm:main1ext] in fact yields

[formula]

for a universal constant C (that is, the constant in front of the leading term is one). This is used in section [\ref=sec:geom] to prove Theorem [\ref=thm:main3] with an optimal constant.

The proof of Theorem [\ref=thm:main1ext] is based on elementary estimates for the maxima of (sub-)Gaussian random variables with inhomogenenous variances.

Gaussian maxima

We begin by stating a standard upper bound on the maximum of sub-Gaussian random variables, cf. [\cite=Tal14]. As the proof is short and illuminating, it is included here for completeness.

Let [formula] be not necessarily independent random variables with

[formula]

where C is a universal constant and σi  ≥  0 are given. Then

[formula]

where [formula] is the decreasing rearrangement of [formula].

Let [formula]. Then

[formula]

for t  ≥  KS, where K is a universal constant and we used [formula]. So

[formula]

As the left-hand side is invariant under permutation of the variables Xi, we can optimize over permutations of σi to obtain the desired upper bound.

The essential tool in the proof of Theorem [\ref=thm:main1ext] is that the result of Lemma [\ref=lem:supg1] can be reversed when the random variables are independent and Gaussian. This is surely known to experts, but we could not locate a proof in the literature.

Let [formula] be independent with Xi  ~  N(0,σ2i). Then

[formula]

where [formula] is the decreasing rearrangement of [formula].

By permutation invariance and scaling, we can assume throughout the proof that σi are nonincreasing in i (so that σi  =  σ*i) and that [formula].

First, we recall an elementary bound: if [formula] are independent events, then

[formula]

We apply this bound with Ai  =  {|Xi| > t}. Note that

[formula]

By the assumptions at the beginning of the proof, [formula] for some index 1  ≤  m  ≤  n and for all 1  ≤  i  ≤  m. Therefore, we obtain

[formula]

In particular,

[formula]

We can therefore estimate

[formula]

As we assumed that [formula], the proof is complete.

The proof of Lemma [\ref=lem:supg2] is readily adapted to establish also that [formula] (that is, without the absolute values inside the expectation) whenever n  ≥  2. This will not be needed in the sequel.

Proof of Theorem [\ref=thm:main1ext]

Let us begin by writing

[formula]

By Jensen's inequality, we have

[formula]

On the other hand, by Gaussian concentration [\cite=BLM13], we have

[formula]

for every i  ≤  n and t  ≥  0. We therefore obtain

[formula]

by Lemma [\ref=lem:supg1], where C is a universal constant. As

[formula]

we have shown

[formula]

(this last step is irrelevant to our results and is included for cosmetic reasons only).

Next, we note that

[formula]

where we have used the Gaussian Poincaré inequality [\cite=BLM13]. Therefore,

[formula]

On the other hand, we trivially have

[formula]

Averaging these bounds gives

[formula]

In the opposite direction, for every i, choose j(i) such that bij(i)  =   max jbij. Then

[formula]

by Lemma [\ref=lem:supg2]. Putting together the above bounds, we have shown that

[formula]

This establishes the equivalence between Conjectures [\ref=conj:1] and [\ref=conj:2].

It remains to consider the second quantity in Theorem [\ref=thm:main1ext]. The upper bound

[formula]

and the lower bound

[formula]

are obtained by repeating verbatim the corresponding arguments for the first quantity in Theorem [\ref=thm:main1ext]. On the other hand, we can now estimate

[formula]

by Lemma [\ref=lem:supg2]. Averaging these bounds completes the proof.

Slicing

The aim of this short section is to prove Theorem [\ref=thm:main2]. The lower bound is trivial, and therefore by Theorem [\ref=thm:main1] it remains to prove the following.

The expected spectral norm satisfies

[formula]

This result will be established by slicing the matrix into ~   log  log d pieces at different scales, each of which is bounded separately using Theorem [\ref=thm:bvh].

It proves to be convenient for the present purposes to work with matrices with independent entries that are not symmetric (as opposed to symmetric matrices, for which Xij = Xji are not independent). To this end, let us cite the following non-symmetric variant of Theorem [\ref=thm:bvh], see [\cite=BvH15] and its proof.

Let Z be the d1  ×  d2 matrix whose entries Zij  ~  N(0,c2ij) are independent Gaussian variables. Then the expected spectral norm satisfies

[formula]

We can now proceed to the proof of Theorem [\ref=thm:main2ext].

By permuting the rows and columns of X if necessary, we can assume without loss of generality in the sequel that bij = b*ij.

We begin by decomposing the matrix [formula] into its parts above and below the diagonal: that is, [formula] and [formula]. As

[formula]

(the second bound follows by Jensen's inequality), it suffices to bound [formula].

We now decompose [formula] into N: = ⌈ log 2 log 2d⌉ horizontal slices as follows:

[formula]

with

[formula]

Each matrix X(n) has independent entries, and the only nonzero entries of this matrix are contained in its upper 22n  ×  22n block. Moreover,

[formula]

We therefore have

[formula]

We now apply Theorem [\ref=thm:bvhrect] to estimate each term [formula]. Define the quantities

[formula]

As we assumed that bij = b*ij, it follows immediately that

[formula]

In particular, this implies that for n  ≥  2

[formula]

On the other hand, the sum of the variances of the entries in any row or column of X(n) is clearly still bounded by σ2. Finally, as noted above, X(n) is a 22n   ×   22n-dimensional matrix (we can remove all vanishing rows and columns without decreasing the norm). Applying Theorem [\ref=thm:bvhrect] yields for every 2  ≤  n  ≤  N

[formula]

On the other hand, applying Theorem [\ref=thm:bvhrect] with d1 = d2 = 4 immediately yields the analogous bound for X(1). We therefore finally obtain

[formula]

which completes the proof.

The proof of Theorem [\ref=thm:main2ext] does not really contain a new idea: it follows directly from the dimension-dependent bound of Theorem [\ref=thm:bvhrect] by applying it in a multiscale fashion. While we have engineered the slices so that the dimension-dependent bound is sharp on each slice, substantial loss is incurred in the estimate

[formula]

that is, when we assemble the slices to obtain the final bound. As the matrices X(n) have vastly different dimensions and scales, it seems particularly unlikely that their norms will add up in this fashion, but it is far from clear how this idea can be made precise. It therefore appears that the residual dimension-dependence in Theorem [\ref=thm:main2] cannot be further reduced without the introduction of a genuinely new idea.

Geometry

The aim of this section is to exhibit a very useful mechanism to control the geometric structure of the Gaussian processes associated to Gaussian random matrices. A direct application of this mechanism gives rise to dimension-free bounds on the spectral norm of Gaussian random matrices that can improve significantly on Theorem [\ref=thm:bvh] for highly inhomogeneous matrices. Let us begin by formulating a general result that can be obtained by this method, from which Theorem [\ref=thm:main3] and a number of other interesting consequences will follow as corollaries.

A general result

In the sequel, we will denote by B the d  ×  d symmetric matrix of variances of the entries of X, that is, Bij: = b2ij. We denote by B+ and B- its positive and negative parts, respectively; that is, if [formula] is the spectral decomposition of B, then [formula] and [formula].

Let Y  ~  N(0,B-) be Gaussian with covariance matrix B-, and let [formula] be i.i.d. standard Gaussian variables. Then for any γ > 0

[formula]

As a first consequence, we deduce a sharp form of Theorem [\ref=thm:main3].

There is a universal constant C such that

[formula]

As B2  =  (B+)2 + (B-)2, we have

[formula]

Therefore,

[formula]

for every i, and Lemma [\ref=lem:supg1] gives

[formula]

On the other hand, by Remark [\ref=rem:main1sharp], we have

[formula]

for a universal constant C'. Now apply Theorem [\ref=thm:main3ext] with γ = 1.

Let us note that the leading term in the first inequality of Corollary [\ref=cor:main3sharp] is sharp. To see this, consider the example of a Wigner matrix where bij = 1 for all i,j. Then the first inequality yields [formula], which precisely matches the exact asymptotic [formula] as d  →    ∞   [\cite=AGZ10]. On the other hand, the second term in this inequality is suboptimal, as can be seen by considering the example where B is a band matrix with bij = 1 inside a diagonal band of width [formula] and bij = 0 outside the band (compare the conclusion of Corollary [\ref=cor:main3sharp] with that of Theorem [\ref=thm:bvh]). In cases such as the latter example where the second term dominates, Corollary [\ref=cor:main3sharp] can be improved slightly by optimizing over γ.

The expected spectral norm satisfies

[formula]

Apply Theorems [\ref=thm:main3ext] and [\ref=thm:main1] and optimize over γ > 0.

Despite the suboptimal nature of the second term in Corollaries [\ref=cor:main3sharp] and [\ref=cor:main3opt], these results can improve significantly on Theorem [\ref=thm:bvh] for highly inhomogeneous matrices. To illustrate this, let us use Corollary [\ref=cor:main3sharp] to derive a delicate (but much less sharp) result of Latała [\cite=Lat05] that could not be recovered from Theorem [\ref=thm:bvh].

The expected spectral norm satisfies

[formula]

We may assume without loss of generality that the rows and columns of X have been ordered such that [formula] is nonincreasing in i. Then we must have

[formula]

for all i, and the conclusion follows readily from Corollary [\ref=cor:main3sharp].

The above corollaries are based on a rather crude estimate on the variance of the random variables Yi that appear in Theorem [\ref=thm:main3ext] (see the proof of Corollary [\ref=cor:main3sharp]). Unfortunately, it seems that this estimate cannot be significantly improved in general, which indicates that there is genuine inefficiency in the proof of Theorem [\ref=thm:main3ext]. The apparent origin of this inefficiency will be discussed in some detail in the sequel. It is interesting to note, however, that there are certain special cases where Theorem [\ref=thm:main3ext] already provides substantially better results than is suggested by Corollary [\ref=cor:main3sharp]. For example, Theorem [\ref=thm:main3ext] immediately resolves Conjecture [\ref=conj:1] (with optimal constant!) under the strong assumption that the matrix of variances B is positive semidefinite.

If B is positive semidefinite, then

[formula]

In this case B- = 0, and the result follows from Theorem [\ref=thm:main3ext] with γ = 1.

Along similar lines, it is not difficult to see that if B has at most k negative eigenvalues, than the conclusion of Conjecture [\ref=conj:1] holds with a constant that depends on k only (so that the conjecture is established if B has O(1) negative eigenvalues). On the other hand, there are other cases where the special structure of B makes it possible to deduce Conjecture [\ref=conj:1] from Theorem [\ref=thm:main3ext]. For example, if

[formula]

where B' is a positive semidefinite matrix, then

[formula]

so that [formula] for all i; then arguing as in the proof of Corollary [\ref=cor:main3sharp] and applying Theorem [\ref=thm:main1] immediately yields Conjecture [\ref=conj:1]. All of these special cases are restrictive; however, they emphasize that the approach developed in this section can already extend significantly beyond the result of Theorem [\ref=thm:main3].

Proof of Theorem [\ref=thm:main3ext]

We begin by recalling that

[formula]

is the expected supremum of a Gaussian process indexed by the Euclidean unit ball B2. It is well known that the supremum of a Gaussian process is intimately connected with the geometry defined by the associated (semi)metric

[formula]

The difficulty we face is to understand how to control this rather strange geometry.

To motivate the device that we will use for this purpose, let us disregard for the moment the natural metric d and consider instead a simpler quantity, the variance of the Gaussian process. We can easily compute

[formula]

We now observe that this expression can be reorganized in a suggestive manner. Define the norm [formula] on [formula] and the nonlinear map [formula] as

[formula]

and consider a second Gaussian process

[formula]

where [formula] are i.i.d. standard Gaussian variables. Then

[formula]

In particular, we see that the variance of the Gaussian process {〈v,Xv〉}v∈B2 associated with our random matrix is dominated up to a constant by the variance of the Gaussian process {〈x(v),g〉}v∈B2. The latter process is precisely what we would like to obtain in our upper bound, as we immediately compute

[formula]

using the Cauchy-Schwarz inequality and the fact that the map v  ↦  (v2i) maps the Euclidean unit ball in [formula] onto the d-dimensional simplex.

Unfortunately, an inequality between the variances of Gaussian processes does not suffice to control the suprema of these processes. What is sufficient, however, is to establish such an inequality between the natural distances of these Gaussian processes: if we could show that the natural distance of the Gaussian process {〈v,Xv〉}v∈B2 is dominated by the natural distance of {〈x(v),g〉}v∈B2, that is,

[formula]

then the conclusion of Conjecture [\ref=conj:1] would follow immediately from the Slepian-Fernique lemma [\cite=BLM13]. Unfortunately, this inequality does not always hold, see section [\ref=sec:disc] below. However, it turns out that such an inequality nearly holds, and this is the key device that will be exploited in this section.

For every [formula] and γ > 0

[formula]

We first compute d(v,w):

[formula]

We can now estimate using the triangle inequality [formula]

[formula]

The elementary inequality 2ab  ≤  γa2  +  γ- 1b2 gives

[formula]

for any γ > 0. We now compute

[formula]

Combining these bounds completes the proof.

With Lemma [\ref=lem:basic] in hand, we can now easily complete the proof of Theorem [\ref=thm:main3ext].

We begin by noting that the spectral norm of a symmetric matrix is the largest magnitude of its maximal and minimal eigenvalues, that is,

[formula]

As X and - X have the same distribution, we can estimate

[formula]

where we used the Gaussian Poincaré inequality [\cite=BLM13] in the second inequality. To proceed, assume without loss of generality that Y  ~  N(0,B-) is independent of [formula], and define the Gaussian process {Zv}v∈B2 as follows:

[formula]

The natural distance of this Gaussian process satisfies

[formula]

by Lemma [\ref=lem:basic]. We therefore obtain

[formula]

by the Slepian-Fernique inequality [\cite=BLM13]. A simple application of the Cauchy-Schwarz inequality as discussed before Lemma [\ref=lem:basic] completes the proof.

Discussion

It is instructive to discuss the geometric significance of the basic principle described by Lemma [\ref=lem:basic]. The clearest illustration of this device appears in the setting of Corollary [\ref=cor:posdef] where the matrix of variances B is positive semidefinite. In this case, the second term in Lemma [\ref=lem:basic] is nonpositive, and we obtain

[formula]

This inequality maps the geometry of the metric space (B2,d) onto the Euclidean geometry of the nonlinear deformation of the unit ball

[formula]

which is much easier to understand.

The trivial case of this construction appears in the example of a Wigner matrix where bij = 1 for all i,j. In this special case, the nonlinear deformation has no effect and B* = B2 is simply the Euclidean unit ball. Applying the Slepian-Fernique inequality in this setting shows that

[formula]

This idea is not new: our approach reduces in this trivial setting to the well-known method of Gordon for estimating the norm of Wigner matrices [\cite=Ver12]. However, the crucial insight developed here is that the geometry of B* changes drastically when we depart from the simple setting of Wigner matrices, which is not captured by Gordon's method. This is illustrated in the following example.

Consider the example of a diagonal random matrix where [formula]. In this case, the nonlinear deformation transforms the Euclidean unit ball into the [formula]-ball B* = B1, whose geometry is entirely different than in the previous example. Applying the Slepian-Fernique inequality in this setting shows that

[formula]

which captures precisely the correct behavior in this setting.

In general, the deformed ball B* can take very different shapes, as is illustrated in Figure [\ref=fig:interp]. The beauty of this construction is that the manner in which the geometry of the space (B2,d) is captured by the geometry of [formula] provides a clear mechanism that gives rise to the phenomenon predicted by Conjecture [\ref=conj:1].

Unfortunately, the simple geometry exhibited above is much less clear when the matrix B is not positive semidefinite. One might hope that the inequality

[formula]

remains valid in the general setting, but this is not always true. The following illuminating example was suggested by Afonso Bandeira.

Let a,b,δ > 0 (a  ≠  b) and let

[formula]

We readily compute

[formula]

while

[formula]

Thus the ratio

[formula]

can be arbitrarily large. This example is essentially the worst possible, as optimizing γ in Lemma [\ref=lem:basic] shows that [formula] for v,w∈B2.

While the above example illustrates conclusively that the desired inequality cannot hold in general when B is not positive semidefinite, we also note that the failure point in this example appears to be very special. The vectors v and w, while far apart in the Euclidean distance, satisfy both d(v,w) = 0 and [formula] when δ = 0. These points are therefore in some sense "singular" with respect to the geometry of (B2,d) and of [formula] when δ = 0. Example [\ref=ex:afonso] shows that the comparison between the two geometries can fail near such singular points. Numerical experiments suggest that such points are rather rare and that the inequality [formula] typically fails only in a very small subset of the unit ball. We do not have a precise formulation of this idea, however.

The phenomenon that is illustrated by Example [\ref=ex:afonso] is controlled in Lemma [\ref=lem:basic] by the addition of a second term that dominates the bound at the singular points of the geometry of (B2,d). The remarkable aspect of this second term is that it has a very suggestive interpretation: if the matrix - B were positive semidefinite (which of course cannot be the case as B has nonnegative entries), this would be the natural distance corresponding to Gaussian process defined by the convex hull of random variables [formula] with U  ~  N(0, - B). By Lemma [\ref=lem:supg1], the supremum of this Gaussian process would be of the same order as the second term of the last expression in Theorem [\ref=thm:main1], which would suffice to establish Conjecture [\ref=conj:1].

While this intuition clearly cannot be implemented in this manner, it is nonetheless highly suggestive that the validity of Conjecture [\ref=conj:1] can "almost" be read off from the geometric structure described by Lemma [\ref=lem:basic]. Unfortunately, we do not know how to optimally exploit this geometric structure. In Theorem [\ref=thm:main3ext], we have crudely forced - B = B- - B+ to be positive definite by estimating it from above by B-. The problem with this approach is that the entries of B- can be much larger than the entries of B, which is the origin of the suboptimal second term in Corollary [\ref=cor:main3sharp]: there can in general be significant cancellation between B- and B+ that our approach fails to exploit. The elimination of this inefficiency in the proof of Theorem [\ref=thm:main3ext] would be a significant step towards the resolution of Conjecture [\ref=conj:1].

We conclude by noting that there is no reason, in principle, to expect that a sharp bound on the expected supremum of a Gaussian process can always be obtained using the Slepian-Fernique inequality, as we have done in the proof of Theorem [\ref=thm:main1ext]. In general, the connection between the supremum of a Gaussian process and the underlying geometry is described by the generic chaining method [\cite=Tal14]. Unfortunately, even a geometric description along these lines of the trivial behavior of the supremum of a Gaussian process over a convex hull remains a long-standing open problem [\cite=Tal14], so that a direct application of generic chaining methods in the present setting appears to present formidable difficulties.

Acknowledgments

Many of the results presented here were obtained while the author was a visitor at IMA in the spring semester of 2015 in the context of the annual program "Discrete Structures: Analysis and Applications," and while the author attended the Oberwolfach workshop "Probabilistic Techniques in Modern Statistics" in May 2015. It is a pleasure to thank IMA and Oberwolfach, and in particular the organizers of these excellent programs, for their hospitality. The author is grateful to Rafał  Latała, Afonso Bandeira, and Amirali Ahmadi for several interesting discussions, and to Afonso Bandeira for suggesting Example [\ref=ex:afonso].

An early draft of this paper was dedicated with great admiration to Evarist Giné on the occasion of his 70th birthday. I was immensely saddened to learn that Evarist unexpectedly passed away shortly after the completion of this draft, as is begrudgingly reflected in the dedication of the present version of this paper.