Definition Corollary Lemma Theorem

A lower bound on the average entropy of a function Determined up to a diagonal linear Map on [formula]

Introduction

Suppose that [formula] is an arbitrary function (where q is a prime power and [formula] is the finite field of q elements). Let A be a random variable uniformly distributed over [formula]. Clearly, f(A) may be far from uniform, while kA is uniform for all [formula]. Is f(A) + kA nearly uniform for most values of [formula]? More generally, given a positive integer n, for an arbitrary [formula] and for [formula] uniformly distributed over [formula], is [formula] nearly uniform for most values of [formula]?

Recall that the Shannon entropy H(B) of a random variable B taking values in a finite set S is defined by [formula], while the collision probability of B, [formula], is defined by [formula], where B' is an independent copy of B. The Rényi entropy of B, H2(B), is defined by [formula]. A straightforward application of Jensen's inequality shows that H2(B)  ≤  H(B).

Since both the Rényi entropy and the Shannon entropy measure randomness (where for both entropies the maximum possible value of log (|S|) is equivalent to having uniform distribution, and the minimum possible value of 0 is equivalent to being deterministic), a possible formal phrasing of the above question on [formula] is: How much smaller than log (qn) might the average over [formula] of the Rényi (or Shannon) entropy be?

The collision probability itself is yet an additional measure of randomness, where the minimum collision probability of 1 / |S| is equivalent to having uniform distribution and the maximum possible collision probability of 1 is equivalent to being deterministic. So, another possible formal phrasing of the question on [formula] is: How much larger than 1 / qn might the average over [formula] of the collision probability be?

The main motivation for this question is a certain side-information problem in information theory [\cite=Zamir]. Several neighboring questions were considered in the literature. For example, the case n = 1 of Theorem [\ref=thm:main] ahead extends Lemma 21 of [\cite=KLSS], stating that for any [formula] there exists [formula] for which [formula]. The same case of Theorem [\ref=thm:main] ahead also extends the main theorem of [\cite=Car], which states that the average over [formula] of [formula] (for f a polynomial of degree [formula]) is at least q / (2 - 1 / q). In addition, a somewhat similar question, concerning the min-entropy of [formula] for random a1 and a2 in [formula] and for large q was implicitly considered in the merger literature, see, e.g., Sec. 3.1 of [\cite=D], and Theorem 18 of [\cite=DKSS].

The main contributions of the current note are the following two theorems.

Let n  ≥  1 be an integer, let [formula] be an arbitrary function, and for [formula], let [formula] be defined by

[formula]

Suppose that a random variable [formula] is uniformly distributed over [formula]. Then

[formula]

The point of the theorem is that the average over [formula] of [formula] is at most about n bits below the entropy of a uniform distribution over [formula], regardless of q and f. Of course, since the Shannon entropy is not smaller than the Rényi entropy, we may replace H2 by H in Theorem [\ref=thm:main]. In fact, a stronger result is proven:

Using the terminology of Theorem [\ref=thm:main], we have

[formula]

with equality if for all i, [formula] depends only on xi.

Note that by Jensen,

[formula]

and hence Theorem [\ref=thm:main2] implies Theorem [\ref=thm:main].

As stated in the theorem itself, the bound of Theorem [\ref=thm:main2] is tight. The bound of Theorem [\ref=thm:main] is also tight, as seen by the following proposition.

For the function [formula] defined by [formula], we have (using the terminology of Theorem [\ref=thm:main])

[formula]

and

[formula]

Proof of Theorem [\ref=thm:main2]

The proof begins as the proof of the Leftover Hash Lemma as appearing in [\cite=CF]. Letting [formula] and [formula] be random variables uniformly distributed over [formula] such that [formula], [formula] and [formula] are jointly independent, the left-hand side of ([\ref=eq:main2]) can be written as

[formula]

It follows that Theorem [\ref=thm:main2] is an immediate consequence of the following Lemma.

Using the above notation,

[formula]

with equality if for all i, [formula] depends only on xi.

For [formula], let [formula] be the Hamming distance between [formula] and [formula] (number of coordinates i for which xi  ≠  x'i) and let [formula]. We have

[formula]

Now,

[formula]

(probability over [formula], [formula] and [formula]) is the average over pairs of vectors [formula] of Hamming distance d of expressions like

[formula]

(probability over [formula]). The last expression is either 0 (if [formula] for some i for which ai = a'i), or qn - d / qn otherwise (d entries of [formula] are determined by the equation, and the other n - d entries are free). So, in either case, the expression in ([\ref=eq:mid]) is ≤  q- d (with equality if for all i, fi depends only on the ith argument), and hence so is the average of these expressions. Substituting in ([\ref=eq:cont]), we get

[formula]

with equality if for all i, [formula] depends only on xi.

Proof of proposition [\ref=prop:tight]

The assertion regarding the average Shannon entropy will follow immediately from the chain rule for conditional Shannon entropy if we prove that for n = 1 and for the function [formula] defined by f(x) = x2, we have

[formula]

for A uniformly distributed on [formula].

Suppose first that q is even. Then g0 = (x  ↦  x2) is a permutation on [formula] (in fact, an automorphism), and so H(g0(A)) =  log (q). For [formula], let Xk,y: = g- 1k(y). We claim that for all [formula] and for all [formula] with [formula], there are exactly 2 elements in Xk,y: On one hand, there are at most two solutions to a quadratic equation, and on the other hand, for x∈Xk,y, x + k is different from x and satisfies gk(x + k) = gk(x), which means that x + k∈Xk,y. Hence in the case of characteristic 2, the average entropy is (1 / q)  ·   log (q) + (1 - 1 / q)  ·   log (q / 2), as desired.

For odd q, we claim that for all [formula], there is a single y with |Xk,y| = 1, and (q - 1) / 2 values of y with |Xk,y| = 2: Fix k, take y with [formula], and let x∈Xk,y. Clearly, gk( - k - x) = gk(x), and if x  ≠   - k / 2, then - k - x  ≠  x, which implies that |Xk,y| = 2. For y with - k / 2∈Xk,y, |Xk,y| must therefore be odd, and hence necessarily equals 1. Hence in the case of odd characteristic, the average entropy is ((q - 1) / 2)  ·  (2 / q)  ·   log (q / 2)  +  (1 / q)  ·   log (q), as in ([\ref=eq:one_enough]).

It remains to calculate the average Rényi entropy for [formula]. It follows from the above discussion on the Shannon entropy that if q is even, then for all [formula] and all i, the collision probability of the i-th entry of [formula] equals 2 / q if ki  ≠  0 (uniform distribution on q / 2 elements), and 1 / q if ki = 0. As the collision probability of a vector of jointly independent random variables is the product of the individual collision probabilities, it follows that [formula], where [formula] is the Hamming weight of [formula] (number of nonzero coordinates in [formula]).

Since [formula], we get

[formula]

as desired.

Finally, if q is odd, then it follows from the discussion in the beginning of the proof that for all [formula], the collision probability of any entry of [formula] equals

[formula]

Because the collision probability of [formula] is the product of the collision probabilities of the individual entries, it follows that for all [formula],

[formula]

which completes the proof.

Note that in Proposition [\ref=prop:tight], the components fi may be any quadratic functions xi  ↦  aix2i + bixi + ci with ai  ≠  0 for all i (eliminating ai and ci is done by an invertible function, and then the linear term is "absorbed" in the averaging over ki).

Acknowledgments

We are grateful to Avner Dor for carefully reading several earlier drafts and for his helpful comments. We would also like to thank Simon Litsyn for pointing us to [\cite=Car].