Clustering and Classification in Text Collections Using Graph Modularity

Introduction

We explore a possibility of clustering (or classification) of documents. Clustering and classification are methods for information retrieval (for a recent review see [\citet=Berry:2003]). The possibility we explore consists in combining two ideas considered previously.

The first idea is co-clustering [\citep=Dhillon:2001], [\citep=Zha:2001]. Co-clustering clusters along with the documents the words used in the documents. As an outcome, clusters of documents are generated along with corresponding clusters of words. This approach features the following advantages: Clusters of words generated as a byproduct of the approach can be used for interpretation of the clusters of documents; In the classification tasks, it is possible to use in the training sets separate words along with documents. The standard algorithm used within the co-clustering approach to reach the result is the spectral clustering [\citep=Luxburg07]. (Computationally, spectral clustering finds eigenvectors of the graph laplacian. With a number of tricks, the eigenvectors are used for clustering.)

The second idea is modularity [\citep=Newman-2006]. Modularity is a class of optimization functionals introduced in the studies of graph clustering. Let us compare modularity to other optimization functionals appearing within the widely used approach to clustering based on generative models [\citep=ZhongG05]. These optimization functionals are various "distances" between the data and the model. Optimization consists in finding parameters of the model yielding the minimal distance. In contrast, the modularity is optimal when a "distance" between the data and a null-model is maximal. The null-model is a key notion for the modularity idea. The null-model models the data without structure (the most random data). Concretely, modularity is defined as follows. A functional on graph partitionings is picked out. Modularity is an additive or multiplicative difference of the value the functional takes on the graph under study and the mean value it takes on the null-model. From the above comparison we conclude that using modularity is in a way less demanding than using generative models, because it is easier to model randomness than specific data. Comparison of modularity-based approaches and generative models approaches is attempted in [\citep=Newman-2011], [\citep=Bickel09].

Modularity has been used in text clustering [\citep=Grinev:2009]. In this attempt, a dense weighted graph has been clustered. The nodes of the graph are the documents, all the documents are potentially linked to one another, the edges have weights characterizing similarity of the linked documents.

In this paper we apply the modularity of [\citep=Newman-2006] to the bipartite word-document graph. This is a very sparse bipartite graph G whose nodes are documents and words, and edges are between documents and words contained in them. The sparsity of G makes our approach practical.

More technically, our work is based on two facts. First, the modularity can be optimized with fast and efficient algorithms [\citep=Blondel:2008] that have complexity proportional to the number of links. (Here we point out that we independently developed an algorithm similar to the so called Louvain algorithm [\citep=Blondel:2008] before the paper [\citep=Blondel:2008] appeared. We had used it in 2007 to cluster the citation graph of the papers from http://arxiv.org. The results of this clustering are accessible via http://xstructure.inr.ac.ru.) Second, the density of the graphs in our experiments was in the range from 0.0015 to 0.006 . For such graphs, [formula], where |E| is the number of edges and |V| is the number of vertexes of the graph. Also, the number of vertexes in our graph equals approximately the number of documents in the collection.

We conclude that in the case under consideration the linearity of the algorithm in the number of edges of the graph almost implies the linearity in the number of documents. In this way we obtain a very fast algorithm. It allows one to cluster (classify) tens of millions of documents in a few hours with a typical computer hardware. Presently, a clustering problem is considered to be a "large scale" if it involves up to 105 documents [\citep=Vries:2010]. With our algorithm, it is possible to raise this bar at least up to 107 documents.

The paper is organised as follows. In the next section, we outline the algorithm. In the third section, we present the results of experiments applying the new algorithm to various text collections. In the cocluding section, we briefly summarise our achievements.

The Algorithm

Clustering: The Basic Algorithm

In this section, we outline the algorithm we used to maximize the modularity of the bipartite graph G modeling a collection of documents (its vertexes are documents and words of the collection; an edge appears between a document and a word if the latter is contained in the former).

The modularity Q(P,G) is a functional defined on the set {P} whose members are partitions of the set of vertexes of the graph G [\citep=Newman-2006].

As discussed above the modularity is a difference between the fraction of the edges inside the clusters for the graph under consideration and for the null model. For example, for a simple (unweighted and undirected) graph, the value it takes on a particular partition is

[formula]

where the summation runs over the clusters of the partition, N is the number of clusters, li is the number of edges inside the ith cluster, L is the number of graph edges, and Di is the sum of degrees of vertexes inside the cluster i.

Modularity can be used to determine an invariant of the graph G--the partition P that gives the modularity its maximal value. Generally, computing this invariant is an NP-complete problem [\citep=Brandes:2006]. There is a number of algorithms for computing an approximation to this invariant [\citep=Fortunato:2010].

For our particular case, where the graph under consideration is a bipartite one, the null model should be modified allowing for the edges to appear randomly only between the two parts of the graph. Accordingly, equation ([\ref=modularity]) is transformed as follows [\citep=Barber:2007]:

[formula]

where D1i (D2i) is the sum of degrees of vertexes inside the first (second) part of the ith cluster, and L is the number of graph edges.

Our algorithm is based on a use of an operation TP to be defined below. It acts on any partition P' that can be obtained from the partition P involved in its definition by a coarsening, P'  ≥  P (this means that the subsets of P' can be obtained by merging some subsets of P). The outcome of TP acting on P' is a new partition whose modularity is not less than the one of P': Q(TPP')  ≥  Q(P'). (Here and below we omit the second argument of Q(P,G) because the graph G is fixed.) This is the basic property of the operation TP: its action "improves" the partition. The definition of TP does not use any specific property of the quality functional Q, and can be given for any particular choice of the latter. We stress that TP depends on the particular choice of the quality functional Q.

To define TP, we introduce an arbitrary numbering of the elements v, v∈P (the notation v originates from the most refined partition of G whose members are separate vertexes). After that, instead of the set of elements v of the partition P we deal with the set of their numbers, [formula].

The next step is to introduce coordinates on the set of P'  ≥  P. Each P' can be considered as a point in a space with |P| discrete coordinates; each coordinate takes an integer value from 1 to |P|. Indeed, each P' defines an equivalence relation on the numbers: v'  ~  v if v and v' belong to the same subset of P'. The vth coordinate of P' can be defined as follows:

[formula]

So, by this formula, any set v is mapped to the set inside the same cluster of |P'| with the maximal number. Inversely, any point [formula] of the discrete space [formula] can be interpreted as a partition P' whose members are obtained by merging the subsets of P whose coordinates xv∈P coincide.

Now the functional Q can be considered as a function of |P| discrete arguments:

[formula]

each argument runs from 1 to |P|. We are looking for the maximum of this function.

To approximate the maximum, we can take any starting P' and use the discrete cyclic coordinate descent method [\citep=Luenberger73] to obtain a point TPP' improving the partition P', Q(TPP')  ≥  Q(P'). This concludes our definition of the operation TP.

The operation TP can be used to describe the previously introduced Louvain algorithm [\citep=Blondel:2008]. Indeed, the Louvain algorithm yields the partition ending the sequence of partitions Pn  =  TPn - 1Pn - 1 that starts from the most refined partition P0 whose members are the vertexes.

Experimenting with classification of text collections, we have found that it is advantageous to use another sequence of partitions approaching the maximum, Pn  =  TP0TPn - 1Pn - 1. So, we start with the most refined partition P0. The first step of the process yields P1  =  TP0P0 (this is the case because T2P  =  TP for any P), the second, P2  =  TP0TP1P1, and so on. The process stops when its next step yields a partition whose modularity coincides with the one obtained on the previous step.

Comparing this algorithm to the Louvain algorithm we point out that, in contrast to the Louvain algorithm, each step of our algorithm does not necessarily coarsen the partition, i.e. our Pn is not always more coarse than Pn - 1. The results we obtain appear to be more accurate (in the sense to be defined latter on) than the ones obtained with the Louvain algorithm.

This concludes the general description of our algorithm.

Clustering: Finetuning

Handmade classifications of large text collections have a number of classification levels. For example, the online arxive arxiv.org has three classification levels (e.g. Physics--Condensed Matter--Superconductivity), and the huge collection of web sites dmoz.org has more than three classification levels (the actual number of levels depends on the subject field). Such levels are not described with the above approach employing the modularity function.

A handle on this is provided by the parametric modularity introduced in [\citep=Reichardt:2006],[\citep=Lambiotte:2010]. It is defined as follows:

[formula]

where an extra real positive parameter λ had appeared.

Let us give an example clarifying the meaning of the new parameter λ. Consider a graph GK which consists of K copies of the graph G. Let the modularity of G reach its maximum value on the partition Pmax. This GK gives a simple model of a graph with two classification levels naturally present: the upper level P2 has as its classes the separate copies of G, while the ground level P1 of the classification subdivides each copy of G on the subgraphs participating in Pmax. With this notations, Q(GK,P1)  =  Q+(G,Pmax)  -  Q-(G,Pmax) / K, where Q+ (Q-) denotes the first (second) term in the right hand side of ([\ref=modification]). Also, Q(GK,P2) = 1 - 1 / K. Because Q±  <  1, at large K, Q(GK,P2)  >  Q(GK,P1). We conclude that in this case the modularity is unable to resolve the ground level of the classification if the number of subclasses at the upper level K is large enough (practically, this takes place at K  ~  10). We can speculate that there is a "resolution limit" beyond which the modularity is unable to resolve the substructures in a graph. (For more on this see [\citep=Fortunato:2007]).

Now consider the performance of the parametric modularity on the above graph GK. In this example, the graph is not a bipartite one. So, we take as a parametric modularity the quantity [formula]. Compare this formula with the above definition of the modularity for nonbipartite graphs ([\ref=modularity]). For this case, take λ  =  K. We have Q(GK,P1,K)  =  Q(G,Pmax), and Q(GK,P2,K)  =  0. We conclude that taking λ  =  K enables the parametric modularity to see namely the ground level of the classification.

The big question in using the parametric modularity is how to find the "good values" of the parameter λ. As we have seen, λ has a meaning of the number of clusters on the upper level of clustering, and we normally do not know it beforehand. At this moment we do not give any prescription on defining λ. In what follows, we use the parametric modularity to find our classifications. We always give the value of λ with which one or another classification had been obtained.

What we can state is that varying λ is a useful tool. In our experiments, λ was varied from 1 to 300.

Clustering: Tidying up

Applying the above clustering algorithm to various large graphs we observed appearance of long tails in the distribution of the clusters in the number of vertexes: Typically, along with a few large clusters, we obtain a large number of relatively small clusters. And the smaller is the cluster, the harder to interpret it. Also, it seems that the appearance of small clusters is not infrequently caused by minor peculiarities in the data.

In the results we present below, the vertexes of the clusters belonging to the long tails are redistributed among a few large clusters. In this section, we describe the procedure of this redistribution of the "astray" vertexes.

The redistribution was obtained with an operation similar to the above TP. This operation, RN, depends on a natural number N. It acts on any partition P with number of clusters larger than N, |P| > N.

First, the redistribution operation RN orders the clusters of the partition P by their size. Next, all the vertexes not included in the first N clusters are counted. Let the number of these astray vertexes be M. A redistribution of the astray vertexes among the N largest clusters can be pointed out with the set of coordinates (x1,...,xM). The value taken by the coordinate xk equals the number of the large cluster the kth astray vertex is redistributed to.

As in the operation TP, the optimal point in the space with the above coordinates is determined by the modularity with the discrete cyclic coordinate descent method [\citep=Luenberger73]. The only undetermined ingredient in the definition of the redistribution operation RN is the starting point for the descent.

The starting point for the descent was determined with the following procedure. The value of the first coordinate x1 was determined by the optimal number of the large cluster for placing the first astray vertex in under the condition that the rest of the astray vertexes are considered as separate clusters. The value of the second coordinate x2 was determined similarly but under condition that the first of the astray vertexes is already placed into the large cluster number x1, and so on.

Previously we described the sequence of partitions Pn  =  TP0TPn - 1Pn - 1. It stops on a partition P. Our final result is Pf  =  TP0RNP, where N < |P| is the number of clusters we choose to be present in the final clustering. As before, the leftest operation TP0 improves the clustering (its action determines the optimal cluster for each vertex among the clusters obtained by the action of the redistribution operation RN).

Classification

A classification problem is given if a subset of the classification indexes is already given (the training set), and the rest should be generated. To clarify, the number of classes is preset to N. For a subset of vertexes (the training set) the correct classes are known. For the rest of vertexes (testing set) the correct classes should be determined. We attempt to solve the classification problem using its analogy to the problem of redistribution of the astray vertexes of the previous subsection.

To solve the problem using modularity, we point out that the correct classification defines a partition on the set of documents obtained from joining the training and testing sets. The members of this partition are the classes consisting from the documents of the training set with addition of the correctly attributed documents from the testing set. We assume that this partition is the one that maximizes the parameterized modularity at a certain value of the parameter λ. If λ is known, this is a problem of maximization with constraints. The constrains fix the number of clusters to N and the distribution among the clusters for the training set.

We look for approximate solution of this problem using the above redistribution operation RN. Our approximation to the optimal classification is Pc  =  RNP where P is the partition with the training set correctly distributed and each of the rest of vertexes belonging to its own cluster.

The Experiment

Four document collections have been used for testing our algorithm. Three of them are among well known classical test collections--20 Newsgroups, Reuters 21578, and WEBKB4. We used pre-processed versions of these collections [\citep=ACardoso]. The fourth collection (TripAdvisor dataset) is a collection of travelers reviews of the hotels they stayed in obtained via the popular resource tripadvisor.com [\citep=Opinion]. In this collection, all the reviews were classified into two classes--the positive and negative reviews.

Table 1 gives parameters of the collections. All four collections were used for clustering and classification.

The performance has been measured with the standard quality functionals. For clustering, the performance has been measured with the Purity [\citep=Manning:2008] and Normalized Mutual Information (NMI) [\citep=Manning:2008] (see below). For classification, it has been measured with micro and macro F1-measures [\citep=Manning:2008] (see below).

The bipartite graphs have been formed using stemming, removing stop-words (the stop-list included 770 words) and rare words involved in less than five documents. Besides the graphs representing the document-word pairs, we also constructed larger graphs representing the document-word and document-bigram pairs (the bigram is a sequence of two words involved in a document).

Table 2 gives parameters of the obtained graphs.

We used unit weights for the links in the graphs. (Experimenting with weighted links--we tested the standard tf-idf weights and weights generated via normalization by the document length in the [formula]-norm--had not shown improvement sufficient to justify the trouble of using them.)

Experiment: Clustering

The clustering was performed by the following protocol. For each testing collection, optimization of the parameterized modularity was performed for a sequence of values [formula] with the objective of finding the suboptimal value of λ.

As mentioned above, the quality of clustering was measured with the Normalized Mutual Information (NMI) and Purity functionals. These functionals are maximal when the generated clustering coincides with a given "correct" clustering. Below we give the formulas for computing these functionals. The clusters of the given "correct" clustering are called classes. The NMI functional reads

[formula]

Here summation in l is over the classes, in m over the generated clusters, N is the total number of documents, Nl (Nm) is the number of documents in class l (cluster m), Nl,m is the number of documents in the overlap between class l and cluster m, The NMI takes its values in the interval (0,1), and measures a similarity between the generated clustering and the known partitioning into classes.

For completeness, and to facilitate comparison with other algorithms, we also computed a similar quality criterion--the Purity:

[formula]

Table 3 gives the clustering results. It shows that the optimization in the value of λ, and the use of bigrams improves the quality of clustering (measured with NMI) considerably.

Table 4 compares our results with the results obtained with other algorithms. The latter were extracted from sources pointed out in the Table 4 caption.

Experiment: Classification

In the classification experiments, the suboptimal value of the parameter λ was used determined previously in the clustering experiments.

There are two standard classification quality measures [\citep=Manning:2008], micro-averaged and macro-averaged:

[formula]

Here the sum in the right hand side of the definitions runs over classes; D is the number of documents to be classified, N is the number of classes, TP(c) is the number of correctly classified documents for class c, and F(c) = 2R(c)P(c) / (R(c) + P(c)), where R(c)  =  TP(c) / N1(c), and P(c)  =  TP(c) / N2(c). In the last relations, N1(c) (N2(c)) are, respectively, the correct (actual) number of the documents from the testing set to be (have been) attributed to class c.

Table 5 gives results of our classification experiments. The same table compares our results to the results obtained with other algorithms.

Conclusions

We presented a new algorithm for clustering and classification of text collections. Our algorithm optimizes modularity computed for a fundamental object--the word-document bipartite graph.

At a competitive quality of the output, our algorithm's main boast is its speed: Using the results on the clustering of a large web-graph (about one billion of the edges) [\citep=Blondel:2008], we estimate the time complexity of the clustering task for a collection of 10 millions of documents (each document about the average size of the documents from 20 Newsgroups collection) as several hours for a typical hardware.

We conclude that our algorithm can be used for clustering very large document collections in reasonable time. With our algorithm, the size of amenable collections can be increased at least an order of magnitude.

We believe that using our algorithm opens up new possibilities for automated structuring of the enormous number of text documents available via the web.