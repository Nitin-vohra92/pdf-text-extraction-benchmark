Claim Corollary Lemma Remark Definition

Exponential Inapproximability of Selecting a Maximum Volume Sub-matrix

Introduction

Given a matrix [formula], it is of practical importance to obtain the "significant information" contained in A. It becomes especially important to have a compact representation of A when A is large and has low numerical rank, as is typical of modern data. Thus, in a broad sense, we are interested in concise representations of matrices. Besides the tremendous practical impact of linear algebraic algorithms designed to this aim, they also come up in different theoretical forms and paradigms. Specifically, the formalization of "significant information" can be done in several ways and to a great extent, it depends on how a matrix is interpreted.

From a conceptual point of view, rather than interpreting a matrix as a block of numbers, we view it as a set of vectors (specifically, column vectors) which are indivisible entities. Thus, the formalization of "significant information" is essentially related to finding a subset of columns of the matrix which satisfies some certain spectral conditions or orthogonality requirements. From a purely combinatorial perspective, treating vectors as elements of a set, one can also view subset selection in matrices as a generalization of the usual subset selection problem where the elements contain little or no information. To give a specific example, the well known Set Cover problem asks for a smallest cardinality subset of a set system which covers a universal set. Likewise, the problem we are interested in essentially asks for a small number of column vectors to "cover" the whole matrix. In this paper, we state a measure of quality for this problem, namely the volume, and we prove an exponential inapproximability result for the problem of selecting a maximum volume sub-matrix of a matrix.

Several problems in matrix analysis require to construct a more concise version of a matrix generally performed by a re-ordering of the columns [\cite=Golub], such that the new smaller matrix is as good a representative of the original as possible. One of the criteria that defines the quality of a subset of columns of a matrix is how well-conditioned the sub-matrix that they define is. To motivate the discussion, consider the set of three vectors

[formula]

which are clearly dependent, and any two of which are a basis. Thus any pair can serve to reconstruct all vectors. Suppose we choose e1,u as the basis, then [formula], and we have a numerical instability in this representation as ε  →  0. Such problems get more severe as the dimensionality of the space gets large (curse of dimensionality), and it is natural to ask the representatives to be "as far away from each other as possible". From this simple example, we see that two orthogonal vectors will capture more information about a superset of columns than two that have an acute angle between each other. Hence, in its generality, this vaguely stated problem can be stated as finding a subset of columns with the maximum volume possible or equivalently with the maximum determinant. A similar (but not equivalent) problem is to find a subset with the maximum smallest singular value. Indeed, in one of the early works studying Rank Revealing QR (RRQR) factorizations [\cite=Hong], while discussing different options on how to choose a good sub-matrix, it was noted that it turns out that "the selection of the sub-matrix with the maximum smallest singular value suggested in [\cite=Klema] can be replaced by the selection of a sub-matrix with maximum determinant", which heuristically proposes to maximize the volume of the sub-matrix instead of using more complicated functions. Several algorithms have been designed following this intuition [\cite=Chan] [\cite=Chan-Hansen] [\cite=Chandrasekaran] [\cite=Hoog] [\cite=Gu] [\cite=Hong] [\cite=Pan-Tang]. The optimization problem of finding a maximum volume sub-matrix of a matrix was only recently studied by ivril and Magdon-Ismail:

[\cite=Volume-TCS] Given a matrix [formula] of rank at least k, MAX-VOL is the problem of finding a sub-matrix [formula] of A such that the volume of the k dimensional parallelepiped defined by the column vectors in C is maximum over all possible choices.

[\cite=Volume-TCS] MAX-VOL is NP-hard. Further, it is NP-hard to approximate to within [formula] for arbitrarily small ε  >  0.

Since MAX-VOL is NP-hard, it is natural to ask for an algorithm to approximate the maximum volume. The first thing one might try is a simple greedy algorithm for approximating MAX-VOL:

The analysis of the approximation ratio of this algorithm and a lower bound was also provided in [\cite=Volume-TCS]. Specifically, let Vol(Gr) be the volume of the column vectors chosen by Greedy and let Vol(Opt) be the optimum volume. Then, we have

[\cite=Volume-TCS] [formula].

[\cite=Volume-TCS] There exists an instance of MAX-VOL for which [formula] for arbitrarily small ε > 0. Furthermore, this instance can explicitly be constructed.

Note that there is a gap between the proven approximation ratio and the lower bound implied by the explicit example. The analysis yielding the ratio 1 / k! is essentially a product of k different mutually exclusive analyses related to each step of the algorithm. However, it is not clear whether the overall contribution of these different steps to the approximation ratio is actually better than their products. Indeed, the lower bound of 1 / 2k - 1 pertains to such a peculiar construction that we have conjectured a 1 / 2k - 1 approximation ratio for the greedy algorithm. Hence, in general, proving an exponential inapproximability for this problem is an important step towards characterizing its approximability properties. It will show that the greedy algorithm is almost the best one can hope for.

This work takes a first step towards this goal and prove exponential inapproximability for MAX-VOL via a gap preserving reduction from the well known Label-Cover problem using the Parallel Repetition Theorem [\cite=Raz]. In doing so, we will establish that the greedy algorithm is asymptotically optimal up to a logarithm in the exponent. Specifically, we prove the following theorem:

There exists δ < 1 and c > 0 such that the problem MAX-VOL is not approximable within 2- ck for k  =  δn, unless P = NP.

Our reduction may also be of independent interest which can be used to prove inapproximability results for other matrix approximation problems with different objective functions.

Preliminaries and Notation

We introduce some preliminary notation and definitions. Let a matrix A be given in column notation as: [formula]. The volume of A, Vol(A) can be recursively defined as follows: if A contains one column, i.e. A  =  {v1}, then [formula], where [formula] is the Euclidean norm. If A has more than one column, [formula] for any v∈A, where πA(v) is the projection of v onto the space spanned by the column vectors of A. It is well known that π(A - {v})(v) = AvA+vv, where Av is the matrix whose columns are the vectors in A - {v}, and A+v is the pseudo-inverse of Av (see for example [\cite=Golub]). Using this recursive expression, we have

where [formula] for   ≤  i  ≤  n - 1.

We observe a simple fact about the "distance" of a vector to a subspace in the following lemma, which will be useful in the final proof. Given two sets of vectors P and [formula], let [formula] denote the distance of q∈Q to the space spanned by the vectors in P.

[formula].

We argue by induction on m. For m = 1, Q has one element and the statement trivially holds. Assume that it is true for n = k where [formula]. Then, for any qk + 1

[formula]

(a) follows because [formula] for any A, B and (b) follows by the induction hypothesis.

Related Work

The concept of volume has been closely related to matrix approximation and mainly studied from a linear algebraic perspective. There are a few results revealing the relationship between the volume of a subset of columns of a matrix and its approximation. In [\cite=Vempala], the authors introduced volume sampling to find low-rank approximation to a matrix where one picks a subset of columns with probability proportional to the volume of the simplex they define. In volume sampling, one picks a subset of columns S of size k with probability

[formula]

where the summation in the denominator is over all subsets of size k. This sampling provides an almost tight low-rank approximation of a matrix in Frobenius norm. Improving this existence result, Deshpande and Vempala [\cite=Deshpande-Vempala] provided an adaptive randomized algorithm for the low-rank approximation problem, which includes a sub-procedure that repetitively chooses a small number of columns by approximating volume sampling. This algorithm is essentially a greedy algorithm and can be regarded as a randomized version of the greedy algorithm we have analyzed for MAX-VOL [\cite=Volume-TCS]. They show that, if [formula] is the probability that this algorithm chooses a subset of columns S of size k, then

[formula]

Thus, not only is sampling larger volume columns good, but approximately sampling columns with large volume can prove useful for matrix approximation. A natural question is to ask what happens when one finds a set of columns with the largest volume (deterministic), which is our problem MAX-VOL. Note that, the last expression ([\ref=ps]) is reminiscent of the approximation ratio we have proved for MAX-VOL in [\cite=Volume-TCS], but its analysis relies on a linear algebraic identity whereas the result in [\cite=Volume-TCS] is derived via combinatorial means. MAX-VOL and volume sampling seem to be related, but they have different characteristics. MAX-VOL is proven to be intractable by using complexity theoretic tools, whereas according to a recent result by Deshpande and Rademacher [\cite=Desh-Rade], volume sampling can be exactly implemented in polynomial time. This work together with [\cite=Desh-Rade] reveals the fact that, although one can exactly sample the columns of a matrix with probability proportional to their volumes, identifying a subset with the maximum volume is hard.

Goreinov and Tyrtyshnikov [\cite=Tyr] provided explicit statements of how MAX-VOL, in particular, is related to low-rank approximations in the following theorem:

Suppose that A is an m  ×  n block matrix of the form

[formula]

where A11 is nonsingular, k  ×  k, whose volume is at least μ- 1 times the maximum volume among all k  ×  k sub-matrices. Then [formula].

This theorem implies that if one has a good approximation to the maximum volume k  ×  k sub-matrix, then the rows and columns corresponding to this sub-matrix can be used to obtain a good approximation to the entire matrix in the ∞  -norm. If σk + 1(A) is small for some small k, then this yields a low-rank approximation to A. [\cite=Tyr2] also proves a similar result to Theorem [\ref=kk].

Pan [\cite=Pan] unifies the main approaches developed for finding RRQR factorizations by defining the concept of local maximum volume and then gives a theorem relating it to the quality of approximation.

Let [formula] and C be a sub-matrix of A formed by any k columns of A. Vol(C)(  ≠  0) is said to be local μ-maximum volume in A, if μ  Vol(C)  ≥  Vol(C') for any C' that is obtained by replacing one column of C by a column of A which is not in C.

For a matrix [formula], an integer k (1  ≤  k  <  n) and μ  ≥  1, let [formula] be a permutation matrix such that the first k columns of AΠ is a local μ-maximum in A. Then, for the QR factorization

[formula]

we have [formula] and [formula].

We note that, MAX-VOL asks for a stronger property of the set of vectors to be chosen, i.e. it asks for a "good" set of vectors in a global sense rather than only requiring local optimality. Obviously, a solution to MAX-VOL provides a set of vectors with local maximum volume.

Independently of our work, there are some results in computational geometry which are related to the ability to construct large simplices embedded in V-polytopes. Essentially, the problem we consider is a more general version of finding a large simplex in a V-polytope, where the vertices of the polytope are the column vectors. The results in this area are similar to ours in spirit but using different techniques [\cite=Gritzmann] [\cite=Koutis] [\cite=Packer]. The most relevant work to ours is that of Koutis [\cite=Koutis], which shows exponential inapproximability for finding a large simplex in a V-polytope. He provides a reduction from set packing using an inapproximability result of [\cite=Hazan], whereas our reduction is directly from the Label Cover problem.

The Label-Cover Problem

Our reduction will be from the Label Cover problem. Label Cover combinatorially captures the expressive power of a 2-prover 1-round proof system for the problem Max-3SAT(5). Specifically, there exists a reduction from Max-3SAT(5) to Label Cover, so that using the well known parallel repetition technique for the specified proof system yields a new k-fold Label Cover instance. For simplicity, we prefer to state our reduction from Label Cover and for the sake of completeness, we provide a canonical reduction from Max-3SAT(5) to Label Cover.

Max-3SAT(5) is defined as follows: Given a set of 5n / 3 variables and n clauses in conjunctive normal form where each clause contains three distinct variables and each variable appears in exactly five clauses, find an assignment of variables such that it maximizes the fraction of satisfied clauses. The following result is well known [\cite=Pcp] [\cite=Pcp2]:

There is a constant ε  >  0, such that it is NP-hard to distinguish between the instances of Max-3SAT(5) having optimal value 1 and optimal value at most (1 - ε).

Although this result was proved for general 3CNF formulas, without the requirement that each variable appears exactly 5 times, there is a standard reduction from Max-3SAT to Max-3SAT(5) [\cite=Feige], which only results in a difference in the constant ε.

A Label Cover instance L is defined as follows: where

G(V,W,E) is a regular bipartite graph with vertex sets V and W, and the edge set E.

ΣV and ΣW are the label sets associated with V and W, respectively.

Π is the collection of constraints on the edge set, where the constraint on an edge e is defined as a function Πe:ΣV  →  ΣW.

A labeling is an assignment to the vertices of the graph, [formula]. It is said to satisfy an edge e = (v,w) if Πe(σ(v))  =  σ(w). The Label Cover problem asks for an assignment σ such that the fraction of the satisfied edges is maximum.

A standard reduction from Max-3SAT(5) to Label Cover reveals that

There is a constant ε'  >  0, such that it is NP-hard to distinguish between the instances of Label Cover having optimal value 1 and optimal value at most (1 - ε').

In order to amplify the gap, one can define a new Label Cover instance for which the vertex set is essentially a set Cartesian product of the original one. This instance, as follows, captures a standard 2-prover 1-round protocol with parallel repetition [formula] times applied. We first note that for a given set [formula], [formula] consists of all [formula]-tuples of the form [formula] where sij∈S and ij runs over [formula] for [formula]. Given the original Label Cover instance L  =  (G(V,W,E),(ΣV,ΣW),Π) reduced from Max-3SAT(5), let

[formula]

where [formula], [formula], [formula] and [formula] are the [formula] times Cartesian products of the sets V, W, ΣV and ΣW, respectively as defined above. Let

[formula] consist of all edges of the form e  =  (v,w) where [formula] and [formula] satisfying (vij,wij)∈E and for all [formula].

[formula] be the collection of constraints on the edge set [formula]. The constraint on an edge e = (v,w) where [formula] and [formula] is a function [formula] which is essentially an [formula]-tuple constraint ([formula]), where [formula] for [formula].

A labeling σ of the vertices [formula] and [formula] is said to satisfy an edge e = (v,w) where [formula] and [formula], if [formula]. Note that this requirement is equal to Π(vij,wij)(σ(vij))  =  σ(wij) for all [formula]. It is easy to see that, in this new Label Cover instance, [formula], [formula], [formula], [formula] and [formula]; the degrees of the vertices in V and W is [formula] and [formula], respectively. The following theorem is a well known result by Raz [\cite=Raz]:

There is an absolute constant α  >  0, such that it is NP-hard to distinguish between the case that [formula] and [formula].

Exponential Inapproximability of MAX-VOL

The Basic Gadget

At the heart of our analysis is a set of vectors with a special property. We will use a set of vectors (composed of binary entries for simplicity of construction) such that any two of them have large dot-product. We will also require that the dot product of a vector and the binary complement of any other vector is large. More specifically, we need these dot products be proportional to the Euclidean norms squared of the vectors.

Given a vector [formula] where vi∈{0,1} for m  ≥  i  ≥  1, we denote the binary complement of v by [formula] where [formula] if vi  =  0, and [formula] otherwise. We begin with the following lemma:

For m  ≥  2, there exists a set of vectors [formula] of dimension 2m with binary entries such that the following three conditions hold:

[formula] for 2m - 1  ≥  i  ≥  1

[formula] for 2m - 1  ≥  i  >  j  ≥  1.

bi  ·  bj  =  2m - 2 for 2m - 1  ≥  i  >  j  ≥  1.

Consider the Hadamard matrix H of dimension 2m  ×  2m with entries - 1 and 1, constructed recursively by Sylvester's method. Let B be the (2m - 1)  ×  2m matrix consisting of the rows of H for which we replace - 1's with 0's, excluding the all 1's row. We claim that the rows of B satisfy the requirements. Indeed, by the properties of Hadamard matrices, each row of B has exactly 2m - 1 1's which satisfies the first requirement. Note also that, for m  ≥  2, two distinct rows of H (excluding the all 1's vector) have exactly 2m - 2 element-wise dot-products of the following four types: 1  ·  1, 1  ·  ( - 1), ( - 1)  ·  1, ( - 1)  ·  ( - 1). Considering the construction of B, we have that the dot-product of any two of its rows is 2m - 2 since all the products in H involving - 1 vanishes for B. Similarly the dot-product of a row with the binary complement of another row is 2m - 2 by symmetry. Thus, the second and the third requirement also hold.

The Reduction

Lemma [\ref=existence-vector] guarantees the existence of of a set of binary vectors [formula] of dimension [formula] such that the following three conditions hold:

[formula] for [formula]

[formula] for [formula].

[formula] for [formula].

B can be constructed in time [formula]. In our reduction, [formula] will be a constant (to be exactly determined later) inversely proportional to α which is the constant in Raz' Theorem. Hence, one can construct B in constant time. For the sake of simplicity of our argument, we normalize the vectors in B, which then clearly satisfies

[formula] for [formula]

[formula] for [formula].

bi  ·  bj  =  1 / 2 for [formula].

Given a Max-3SAT(5) instance and the reduction described in the previous section, we will define a column vector for each vertex-label pair in [formula], making [formula] vectors in total. (Note that [formula], [formula], [formula] and [formula]). Each vector will be composed of [formula] "blocks" which are either vectors from the set B or the zero vector according to the adjacency information. More specifically, let Av,i be the vector for the vertex label pair [formula] and [formula]. Similarly let Aw,j be the vector for the pair w∈W and [formula]. Both of these vectors are [formula] dimensional. The block of Av,i corresponding to an edge [formula] is denoted by Av,i(e). The block of Aw,j corresponding to an edge [formula] is denoted by Aw,j(e). We define

[formula]

[formula]

In order to show how our reduction works, we present a part of a simple bipartite graph in Figure  [\ref=fig:graph] with all the edges drawn between two pairs of nodes, and the corresponding (row) vectors computed by the reduction in Figure  [\ref=fig:matrix]. Note that Av,i has exactly [formula] non-zero blocks, and Aw,j has [formula] non-zero blocks. Hence, according to the definition above, their Euclidean norm is 1. The column vector set for the MAX-VOL instance is defined as

[formula]

Note that [formula] and [formula], both having polynomial size in n for constant [formula]. From an intuitive point of view, we define mutually orthogonal subspaces for each edge, and then we "spread" the Euclidean norm of each vector to the subspaces corresponding to the edges incident to the vertex corresponding to the vector. A crucial observation for this construction is that, vectors Av1,i1 and Av2,i2 are orthogonal to each other for all [formula], and [formula], since there are no edges between the vertices in [formula]. The same result holds for the vertices in [formula]. From now on, this fact will be used frequently without explicit reference. We set the number of column vectors k to be chosen in the MAX-VOL instance to [formula]. Note that k is a constant fraction of N, the total number of columns, i.e. there exists a constant δ  <  1 such that k  =  δN.

Analysis

We start with the completeness of the reduction:

If the Label Cover instance [formula] has a labeling that satisfies all the edges, then in the MAX-VOL instance, there exist k column vectors with volume 1.

We show that there are at least k orthogonal vectors. For an edge e = (v,w), let [formula] and [formula] be the labeling of v and w assigned by the optimal labeling which satisfies all the edges. Then, in the MAX-VOL instance the dot product of the vectors Av,i and Aw,j is

[formula]

This is due to the fact that the labeling satisfies e, i.e. [formula]. Since all the edges are satisfied, and there exists a vector from each vertex corresponding to the optimal labeling satisfying the equation ([\ref=completeness]), we have [formula] orthogonal vectors, i.e. we have k orthogonal vectors.

Before proving the soundness of the reduction, which will prove hardness of approximation, we first give the intuition for the argument. According to our construction of the MAX-VOL instance, there is a set of vectors corresponding to each node in [formula] and [formula]. The set of vectors defined for a specific node has high pair-wise dot products whereas a vector from a node [formula] and another from v2 in [formula] are orthogonal to each other. The same goes for the vectors defined for [formula]. Hence, if vectors are chosen from the same set corresponding to a single node, the total volume will decrease exponentially with respect to the number of such vectors. Let us call these vectors duplicates in [formula] and [formula]. The more intricate part of the analysis is due to the dot products between the vectors defined for [formula] and [formula], which is enforced to be non-zero by the unsatisfied edges in the Label-Cover instance. We will show that, in case the Label-Cover instance has few satisfied edges, any k vectors chosen in the MAX-VOL instance should satisfy the following: either the number of duplicates in [formula] and [formula] is large enough so that the total volume is small, or the dot products between [formula] and [formula] leads to a small volume.

There exist absolute constants α and c such that, if the Label Cover instance [formula] does not have any labeling that satisfies more than [formula] of the edges, then the volume of any k vectors in the MAX-VOL instance is at most 2- ck.

Let [formula] and [formula]. Let Av be the vectors corresponding to the vertex [formula]: [formula]. Similarly, let [formula] for [formula]. Let [formula] be the set of all vectors corresponding to the nodes in [formula], and [formula] be the set of all vectors corresponding to the nodes in [formula], i.e.

[formula]

For a set of vectors C of size k, let [formula] for all [formula], [formula] and [formula]. Let [formula] and [formula] be the set of vectors for which C "selects" at least one vector from [formula] and [formula], respectively.

[formula]

For ease of notation, we let [formula]. Note that kVC and kWC denote how many vectors are chosen by C from [formula] and [formula], respectively. Whereas dVC and dWC are the total number of duplicates in [formula] and [formula], respectively. The following lemma relates the number of duplicates on one side with its volume.

[formula] and [formula].

Let P be the set of [formula] elements which contains exactly one vector of the form Av,i for each [formula]. In words, we consider the vectors of C corresponding to the nodes in the Label-Cover instance minus all the duplicates. For the duplicate vector Av,j, we have Av,i  ·  Av,j  =  1 / 2. Hence, [formula]. By the definition of dVC and by the Union Lemma, we get [formula]. The argument for [formula] is similar.

Let the constant [formula]. Recall that, our reduction will require [formula] to be inversely proportional to α in Raz' Theorem. Hence, although having an exponential dependence on α, c is a constant. We will show that Theorem [\ref=inapprox-thm] holds for this value of c; we will prove that Vol(C)  ≤  2- ck for any set C of k vectors. To this aim, we argue by contradiction. The next lemma roughly states that if the volume of C is large enough, then its vectors are almost equally distributed among the nodes of the Label-Cover instance. This condition will in turn imply a small volume completing our argument.

First, we note that [formula] since all the vectors in the MAX-VOL instance have unit norm. Similarly, [formula]. Thus, by the premise of the claim, we have [formula] and [formula]. By Lemma [\ref=exp_drop], we get

[formula]

which implies dVC  ≤  ck / (1 -  log 3 / 2)  <  5ck since log 3  <  1.6. The analysis for dWC along exactly the same lines also yields dWC  <  5ck. Noting the expressions for c and k, and following the definitions, we obtain

[formula]

Similarly,

[formula]

which proves the right hand sides of  [\eqref=eq:k_V] and  [\eqref=eq:k_W]. Noting that [formula], we get

[formula]

and

[formula]

which proves the left hand sides.

Claim [\ref=homo] ensures that if the volume of a set of k vectors exceeds 2- ck, then some certain concentration result should hold, namely Equation ([\ref=eq:k_V]) and Equation ([\ref=eq:k_W]). We will now show that, these equations imply Vol(C)  <  2- ck which is our contradiction.

Without loss of generality, let [formula], [formula]. Note that these sets contain the nodes of the Label-Cover instance from which C "selects" at least one vector. Let [formula] where Avs,is∈Cvs for [formula]. Let [formula] where Avs,is∈Cvs for [formula]. By definition,

[formula]

In words, the set of nodes from which C selects at least one vector essentially covers [formula] and [formula]. These vectors are all orthogonal. From this point of view, [formula] and [formula] play an important role in our argument. Since C "covers" [formula] and [formula] and since the Label-Cover instance has many unsatisfied edges, it means that the dot products of many vectors in [formula] with many vectors in [formula] will be large. This will lead to small volume. Hence, we are essentially interested in the number of unsatisfied edges between [formula] and [formula]. Since there are at most [formula] satisfied edges in the Label-Cover instance, and there are exactly [formula] edges incident to a node in [formula], the number of unsatisfied edges incident to [formula] is greater than [formula]. Similarly, the number of unsatisfied edges incident to [formula] is greater than [formula]. Thus, the number of unsatisfied edges whose end points are in [formula] and [formula], is greater than [formula].

We now give an upper bound for the distance of the vectors in Q to P, namely [formula] for each Avs,is∈Q. To this end, we define the set [formula]. Note that the vectors in different sets are mutually orthogonal, and by the reduction we have

[formula]

for Aw,j∈N(Avs,is) since e = (Avs,is,Aw,j) is unsatisfied. Thus, by the Pythagoras Theorem, we obtain

[formula]

Using the Union Lemma, we get

[formula]

The product in the last expression is maximized when all the factors are equal to each other. We also previously showed that [formula] and that q, the number of distinct nodes hit in [formula] satisfies, [formula]. Hence, we obtain

[formula]

To simplify, let [formula]. For [formula], we have

[formula]

Noting that log e  ≥  10 / 7, we obtain log e  ·  (1 - 2ε1)  ≥  10 / 7  ·  7 / 10  =  1. Then, we get

[formula]

where e is the base of the natural logarithm. In the second inequality, we have used the fact that Vol(P)  ≤  1 and [formula] for t > 1.

We will now provide an upper bound for t to further simplify the last expression. To this aim, let [formula] be the smallest integer such that [formula]. Taking logarithms and rearranging, it is easy to see that [formula]. Note also that for [formula], we have 2ε1  <  1 / 27 and 2ε2  <  3 / 27. Then, for [formula], we get

[formula]

Since [formula], we also have

[formula]

which yields

[formula]

which is our contradiction. Thus, the volume of a set of k vectors in a negative instance of MAX-VOL cannot exceed 2- ck for [formula].

We have shown that

if the optimal value of the Label Cover instance is 1, then the optimal value of the MAX-VOL instance is 1.

if the optimal value of the [formula]-fold Label Cover instance is less than [formula], then the optimal value of the MAX-VOL instance is less than 2- ck.

By the combination of Theorem [\ref=pcp-thm] and Theorem [\ref=raz], we know that there exists a gap producing reduction from SAT to [formula]-fold Label Cover with parameters 1 and [formula]. This means that there is a polynomial time reduction from SAT to MAX-VOL such that, given a formula φ

if φ is satisfiable , then [formula].

if φ is not satisfiable , then [formula].

Thus, unless P = NP, MAX-VOL is inapproximable within 2- ck for some constant c > 0.

Discussion

Our reduction heavily relies on the Raz' Parallel Repetition Theorem [\cite=Raz]. Indeed, it doesn't seem possible to get an exponential inapproximability result without parallel repetition. But, since the degrees of the vertices in the Label-Cover instance exponentially increases with respect to the number of repetitions, our constant c depends on the constant α in Raz' result. It might be possible to improve this constant by making use of more sophisticated parallel repetition theorems, but we did not proceed so far. Indeed, the exact analysis is irrelevant as the constant will be too small in all cases. Overall, the strength of our result is is directly related to the underlying theorems for the inapproximability of Label-Cover.

Another way of getting a stronger hardness result is to find a more sophisticated reduction. In our MAX-VOL instance, the subspaces "reserved" for each edge in the Label-Cover instance are orthogonal to each other. This dramatically simplifies the analysis, yielding perfect completeness, i.e. volume 1 in MAX-VOL. It might be possible to construct a MAX-VOL instance for which these subspaces have some pair-wise angle, so that we sacrifice the perfect completeness, but at the same time get a much smaller soundness. This would improve the inapproximability result.

The obvious open problem is whether the inapproximability can be strengthened to 2- k + 1. Recall that this is the lower bound for the greedy algorithm for MAX-VOL. Considering the multiplicative nature of the problem yielding a very small approximation ratio for the obvious greedy algorithm, a significant improvement of the upper bound would be expected to provide asymptotically better approximations in the exponent. This suggests that the inherent hardness of MAX-VOL might be very close to the performance of the greedy algorithm. However, with the techniques we have used, it is not possible to break the dependence of c on the constant in the parallel repetition theorems.

We would finally like to point out that the reduction and the analysis provided in this paper might be a good starting point for studying hardness of other matrix approximation problems in general (e.g. [\cite=Boutsidis] [\cite=Vempala]) for which no technique related to the PCP theorem have been used. Such an extension to other matrix approximation problems is not trivial. Indeed, computing the volume is already difficult although purely geometric intuition is used. Relating this to other linear algebraic functions (e.g. singular values) which continuously depend on the entries will put even more strain on the analysis.

Acknowledgments: We would like to thank Ioannis Koutis who, in the final stages of this paper, pointed out to us the relevant lines of research in V-polytope theory [\cite=Koutis].