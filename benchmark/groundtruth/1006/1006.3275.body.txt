Theorem Proposition Fact Claim Corollary Conjecture Definition Remark Example

Normalized Information Distance is Not Semicomputable

, Leen Torenvliet

, and Paul M.B. Vitányi

Introduction

The classical notion of Kolmogorov complexity [\cite=Ko65] is an objective measure for the information in a single object, and information distance measures the information between a pair of objects [\cite=BGLVZ98]. This last notion has spawned research in the theoretical direction, among others [\cite=CMRSV02] [\cite=VV02] [\cite=Vy02] [\cite=Vy03] [\cite=MV01] [\cite=SV02]. Research in the practical direction has focused on the normalized information distance (NID), also called the similarity metric, which arises by normalizing the information distance in a proper manner. (The NID is defined by [\eqref=eq.nid] below.) If we also approximate the Kolmogorov complexity through real-world compressors [\cite=Li03] [\cite=CVW03] [\cite=CV04], then we obtain the normalized compression distance (NCD). This is a parameter-free, feature-free, and alignment-free similarity measure that has had great impact in applications. The NCD was preceded by a related nonoptimal distance [\cite=LBCKKZ01]. In [\cite=KLRWHL07] another variant of the NCD has been tested on all major time-sequence databases used in all major data-mining conferences against all other major methods used. The compression method turned out to be competitive in general and superior in heterogeneous data clustering and anomaly detection. There have been many applications in pattern recognition, phylogeny, clustering, and classification, ranging from hurricane forecasting and music to to genomics and analysis of network traffic, see the many papers referencing [\cite=Li03] [\cite=CVW03] [\cite=CV04] in Google Scholar. The NCD is trivially computable. In [\cite=Li03] it is shown that its theoretical precursor, the NID, is a metric up to negligible discrepancies in the metric (in)equalities and that it is always between 0 and 1. (For the subsequent computability notions see Section [\ref=sect.prel].)

The computability status of the NID has been open, see Remark VI.1 in [\cite=Li03] which asks whether the NID is upper semicomputable, and (open) Exercise 8.4.4 (c) in the textbook [\cite=LiVi08] which asks whether the NID is semicomputable at all. We resolve this question by showing the following.

Let x,y be strings and denote the NID between them by e(x,y).

(i) The function e is not lower semicomputable (Lemma [\ref=lem.lower]).

(ii) The function e is not upper semicomputable (Lemma [\ref=lem.upper]).

Item (i) implies that there is no pair of lower semicomputable functions g,δ such that g(x,y)  +  δ(x,y) = e(x,y). (If there were such a pair, then e itself would be lower semicomputable.) Similarly, Item (ii) implies that there is no pair of upper semicomputable functions g,δ such that g(x,y)  +  δ(x,y) = e(x,y). Therefore, the theorem implies

(i) The NID e(x,y) cannot be approximated by a semicomputable function g(x,y) to any computable precision δ(x,y).

(ii) The NID e(x,y) cannot be approximated by a computable function g(x,y) to any semicomputable precision δ(x,y).

How can this be reconciled with the above applicability of the NCD (an approximation of the NID through real-world compressors)? It can be speculated upon but not proven that natural data do not contain complex mathematical regularities such as [formula] or a universal Turing machine computation. The regularities they do contain are of the sort detected by a good compressor. In this view, the Kolmogorov complexity and the length of the result of a good compressor are not that different for natural data.

Preliminaries

We write string to mean a finite binary string, and ε denotes the empty string. The length of a string x (the number of bits in it) is denoted by |x|. Thus, |ε|  =  0. Moreover, we identify strings with natural numbers by associating each string with its index in the length-increasing lexicographic ordering

[formula]

Informally, the Kolmogorov complexity of a string is the length of the shortest string from which the original string can be losslessly reconstructed by an effective general-purpose computer such as a particular universal Turing machine U, [\cite=Ko65]. Hence it constitutes a lower bound on how far a lossless compression program can compress. In this paper we require that the set of programs of U is prefix free (no program is a proper prefix of another program), that is, we deal with the prefix Kolmogorov complexity. (But for the results in this paper it does not matter whether we use the plain Kolmogorov complexity or the prefix Kolmogorov complexity.) We call U the reference universal Turing machine. Formally, the conditional prefix Kolmogorov complexity K(x|y) is the length of the shortest input z such that the reference universal Turing machine U on input z with auxiliary information y outputs x. The unconditional prefix Kolmogorov complexity K(x) is defined by K(x|ε). For an introduction to the definitions and notions of Kolmogorov complexity (algorithmic information theory) see  [\cite=LiVi08].

Let N and R denote the nonnegative integers and the real numbers, respectively. A function f:N  →  R is upper semicomputable (or Π01) if it is defined by a rational-valued computable function φ(x,k) where x is a string and k is a nonnegative integer such that φ(x,k + 1)  ≤  φ(x,k) for every k and lim k  →    ∞φ(x,k) = f(x). This means that f can be computably approximated from above. A function f is lower semicomputable (or Σ01) if - f is upper semicomputable. A function is called semicomputable (or [formula]) if it is either upper semicomputable or lower semicomputable or both. A function f is computable (or recursive) iff it is both upper semicomputable and lower semicomputable (or [formula]). Use 〈  ·  〉 as a pairing function over N to associate a unique natural number 〈x,y〉 with each pair (x,y) of natural numbers. An example is 〈x,y〉 defined by y + (x + y + 1)(x + y) / 2. In this way we can extend the above definitions to functions of two nonnegative integers, in particular to distance functions.

The information distance D(x,y) between strings x and y is defined as

[formula]

where U is the reference universal Turing machine above. Like the Kolmogorov complexity K, the distance function D is upper semicomputable. Define

[formula]

In [\cite=BGLVZ98] it is shown that the function E is upper semicomputable, D(x,y) = E(x,y) + O( log E(x,y)), the function E is a metric (more precisely, that it satisfies the metric (in)equalities up to a constant), and that E is minimal (up to a constant) among all upper semicomputable distance functions D' satisfying the mild normalization conditions [formula] and [formula]. (Here and elsewhere in this paper "log " denotes the binary logarithm.) It should be mentioned that the minimality property was relaxed from the D' functions being metrics [\cite=BGLVZ98] to symmetric distances [\cite=Li03] to the present form [\cite=LiVi08] without serious proof changes. The normalized information distance (NID) e is defined by

[formula]

It is straightforward that 0  ≤  e(x,y)  ≤  1 up to some minor discrepancies for all x,y∈{0,1}*. Since e is the ratio between two upper semicomputable functions, that is, between two Π01 functions, it is a Δ02 function. That is, e is computable relative to the halting problem [formula]. One would not expect any better bound in the arithmetic hierarchy. However, we can say this: Call a function f(x,y) computable in the limit if there exists a rational-valued computable function g(x,y,t) such that lim t  →    ∞g(x,y,t) = f(x,y). This is precisely the class of functions that are Turing-reducible to the halting set, and the NID is in this class, Exercise 8.4.4 (b) in [\cite=LiVi08] (a result due to [\cite=Ga01]).

In the sequel we use time-bounded Kolmogorov complexity. Let x be a string of length n and t(n) a computable time bound. Then Kt denotes the time-bounded version of K defined by

[formula]

Here we use the two work-tape reference universal Turing machine U' suitable for time-bounded Kolmogorov complexity [\cite=LiVi08]. The computation of U' is measured in terms of the output rather than the input, which is more natural in the context of Kolmogorov complexity.

The NID is not lower semicomputable

Define the time-bounded version Et of E by

[formula]

For every length n and computable time bound t there are strings u and v of length n such that

K(v)  ≥  n - c1,

K(v|u)  ≥  n - c2,

K(u|n)  ≤  c2,

Kt(u|v)  ≥  n - c1 log n  -  c2,

where c1 is a nonnegative constant independent of t,n, and c2 is a nonnegative constant depending on t but not on n.

Fix an integer n. There is a v of length n such that K(v|n)  ≥  n by simple counting (there are 2n strings of length n and at most 2n - 1 programs of length less than n). If we have a program for v then we can turn it into a program for v ignoring conditional information by adding a constant number of bits. Hence, K(v) + c  ≥  K(v|n) for some nonnegative constant c. Therefore, for large enough nonnegative constant c1 we have

[formula]

Let t be a computable time bound and let the computable time bound t' be large enough with respect to t so that the arguments below hold. Use the reference universal Turing machine U' with input n to run all programs of length less than n for t'(n) steps. Take the least string u of length n not occurring as an output among the halting programs. Since there are at most 2n - 1 programs as above, and 2n strings of length n there is always such a string u. By construction Kt'(u|n)  ≥  n and for a large enough constant c2 also

[formula]

where c2 depends on t' (hence t) but not on n,u. Since u in the conditional only supplies c2 bits apart from its length n we have

[formula]

This implies also that Kt'(v|u)  ≥  n - c2. Hence,

[formula]

Now we use the time-bounded symmetry of algorithmic information [\cite=Lo86] (see also [\cite=LiVi08], Exercise 7.1.12) where t is given and t' is choosen in the standard proof of the symmetry of algorithmic information [\cite=LiVi08], Section 2.8.2 (the original is due to L.A. Levin and A.N. Kolmogorov in [\cite=ZL70]), so that the statements below hold. (Recall also that for large enough f, Kf(v|u,n) = Kf(v|u) and Kf(u|v,n) = Kf(u|v) since in the original formulas n is present in each term.) Then,

[formula]

with the constant c1 large enough and independent of t,t',n,u,v. For an appropriate choice of t' with respect to t it is easy to see (the simple side of the time-bounded symmetry of algorithmic information) that

[formula]

Since Kt(v|n)  ≥  K(v|n)  ≥  n we obtain Kt(u|v)  ≥  n  -  c1 log n  -  c2.

A similar but tighter result can be obtained from [\cite=BT09], Lemma 7.7.

For every length n and computable time bound t (provided t(n)  ≥  cn for a large enough constant c), there exist strings v and w of length n such that

K(v)  ≥  n - c1,

E(v,w)  ≤  c3,

Et(v,w)  ≥  n - c1 log n - c3,

where the nonnegative constant c3 depends on t but not on n and the nonnegative constant c1 is independent of t,n.

Let strings u,v and constants c1,c2 be as in Lemma [\ref=lem.1] using 2t instead of t, and the constants c',c'',c3 are large enough for the proof below. By Lemma [\ref=lem.1], we have K2t(u|v)  ≥  n - c1 log n  -  c2 with c2 appropriate for the time bound 2t. Define w by [formula] where [formula] denotes the bitwise XOR. Then,

[formula]

where the nonnegative constant c3 depends on 2t (since u does) but not on n and the constant c' is independent of t,n. We also have [formula] so that (with the time bound t(n)  ≥  cn for c a large enough constant independent of t,n)

[formula]

where the nonnegative constants c',c'' are independent of t,n.

The function e is not lower semicomputable.

Assume by way of contradiction that the lemma is false. Let ei be a lower semicomputable function approximation of e such that ei + 1(x,y)  ≥  ei(x,y) for all i and lim i  →    ∞ei(x,y)  =  e(x,y). Let Ei be an upper semicomputable function approximating E such that Ei + 1(x,y)  ≤  Ei(x,y) for all i and lim i  →    ∞Ei(x,y)  =  E(x,y). Finally, for x,y are strings of length n let ix,y denote the least i such that

[formula]

where c is a large enough constant (independent of n,i) such that K(z)  <  n + 2 log n + c for every string z of length n (this follows from the upper bound on K, see [\cite=LiVi08]). Since the function E is upper semicomputable and the function e is lower semicomputable by the contradictory assumption such an ix,y exists. Define the function s by s(n) =  max x,y∈{0,1}n{ix,y}.

The function s(n) is total computable and Es(v,w))  ≥  n - c1 log n - c3 for some strings v,w of length n and constants c1,c3 in Lemma [\ref=lem.xor].

By the contradictory assumption e is lower semicomputable, and E is upper semicomputable since K(  ·  |  ·  ) is. Recall also that e(x,y)  >  E(x,y) / (n + 2 log n + c) for every pair x,y of strings of length n. Hence for every such pair (x,y) we can compute ix,y  <    ∞  . Since s(n) is the maximum of 22n computable integers, s(n) is computable as well and total. Then, the claim follows from Lemma [\ref=lem.xor]. (If s(n) happens to be too small to apply Lemma [\ref=lem.xor] we increase it total computably until it is large enough.)

The string v of length n as defined in the proof of Lemma [\ref=lem.1] satisfies K(v|n)  ≥  n. Hence v is incomputable [\cite=LiVi08]. Similarly this holds for [formula] (defined in Lemma [\ref=lem.xor]). But above we look for a function s(n) such that all pairs x,y of strings of length n (including the incomputable strings v,w) satisfy [\eqref=eq.eE] with s(n) replacing ix,y. Since the computable function s(n) does not depend on the particular strings x,y but only on their length n, we can use it as the computable time bound t in Lemmas [\ref=lem.1] and [\ref=lem.xor] to define strings u,v,w of length n.

For given strings x,y of length n, the value Eix,y(x,y) is not necessarily equal to Es(x,y). Since s(n) majorises the ix,y's and E is upper semicomputable, we have Es(x,y)  ≤  Eix,y(x,y), for all pairs (x,y) of strings x,y of length n.

Since K(v)  ≥  n - c1 we have E(v,w)  ≥  e(v,w)(n - c1). By the contradictory assumption that e is lower semicomputable we have e(v,w)  ≥  es(v,w). By [\eqref=eq.eE] and the definition of s(n) we have

[formula]

Hence,

[formula]

But E(v,w)  ≤  c3 by Lemma [\ref=lem.xor] and Es(v,w)  ≥  n - c1 log n - c3 by Claim [\ref=claim.es], which yields the required contradiction for large enough n.

The NID is not upper semicomputable

The function e is not upper semicomputable.

It is easy to show that e(x,x) (and hence e(x,y) in general) is not upper semicomputable. For simplicity we use e(x,x)  =  1 / K(x). Assume that the function 1 / K(x) is upper semicomputable Then, K(x) is lower semicomputable. Since K(x) is also upper semicomputable, it is computable. But this violates the known fact [\cite=LiVi08] that K(x) is incomputable.

Open Problem

A subset of N is called n-computably enumerable (n-c.e.) if it is a Boolean combination of n computably enumerable sets. Thus, the 1-c.e. sets are the computably enumerable sets, the 2-c.e. sets (also called d.c.e.) the differences of two c.e. sets, and so on. The n-c.e. sets are referred to as the difference hierarchy over the c.e. sets. This is an effective analog of a classical hierarchy from descriptive set theory. Note that a set is n-c.e. if it has a computable approximation that changes at most n times.

We can extend the notion of n-c.e. set to a notion that measures the number of fluctuations of a function as follows: For every n  ≥  1, call f:N  →  R n-approximable if there is a rational-valued computable approximation φ such that lim k  →    ∞φ(x,k)  =  f(x) and such that for every x, the number of k's such that φ(x,k + 1)  -  φ(x,k) < 0 is bounded by n - 1. That is, n - 1 is a bound on the number of fluctuations of the approximation. Note that the 1-approximable functions are precisely the lower semicomputable (Σ01) ones (zero fluctuations). Also note that a set A  ⊆  N is n-c.e. if and only if the characteristic function of A is n-approximable.

Conjecture For every n  ≥  1, the normalized information distance e is not n-approximable.

Acknowledgement

Harry Buhrman pointed out an apparent circularity in an early version of the proof of Claim [\ref=claim.es]. A referee pointed out that an early version of Lemma [\ref=lem.upper] was incorrect and gave many other useful comments that improved the paper.