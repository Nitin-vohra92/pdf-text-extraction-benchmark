Email:

L-DIVERGENCE CONSISTENCY FOR A DISCRETE PRIOR

Introduction

Walker [\cite=W1] has recently considered consistency of posterior distribution in Hellinger distance, for strictly positive prior over a countable set of continuous data-sampling distributions. By means of his martingale approach [\cite=W2], Walker developed a sufficient condition for the Hellinger consistency of posterior density in the above mentioned setting. Via a simple large-deviations approach we show that in this setting posterior density is always consistent in L-divergence. The consistency holds also under misspecification. If there are multiple 'concentration points' (L-projections) the posterior spreads among them equally.

Bayesian nonparametric consistency

Let there be countable set [formula] of probability density functions with respect to the Lebesgue measure; sources, for short. On the set a Bayesian puts his strictly positive prior probability mass function π(  ·  ). Let r be the true source of a random sample [formula]. Provided that r∈M, as the sample size grows to infinity, the posterior distribution π(  ·  |Xn = xn) over M is expected to concentrate in a neighborhood of the true source r. Whether and under what conditions this indeed happens is a subject of Bayesian nonparametric consistency investigations. Surveys of the subject can be found at [\cite=GGR], [\cite=WLP] among others.

Ghosal, Ghosh and Ramamoorthi [\cite=GGR] define consistency of a sequence of posteriors with respect to a metric or discrepancy measure d as follows: The sequence {π(  ·  |Xn),n  ≥  1} is said to be d-consistent at r, if there exists a [formula] with r(Ω0)  =  1 such that for ω∈Ω0, for every neighborhood U of r, π(U|Xn)  →  1 as n goes to infinity. If a posterior is d-consistent for any r∈M then it is said to be d-consistent. There, two modes of convergence are usually considered: convergence in probability and almost sure convergence.

Obviously, in the definition the set of sources is not restricted to be countable. The present work is concerned with the countable M case.

Sanov's Theorem for Sources, L-consistency

Let [formula] be support of the prior pmf. In what follows, r is not necessarily from Me. Thus we are interested also in Bayesian consistency under misspecification; i.e., when π(r)  =  0. The problem is the same as in the case of standard Bayesian consistency (cf. Sect. 2): to find the source(s) upon which the posterior concentrates.

For two densities p,q with respect to the Lebesgue measure λ, the I-divergence [formula]. The L-divergence L(q||p) of q with respect to p is defined as [formula]. The L-projection [formula] of p on Q is [formula]. There Q is a set of probability densities defined on the same support. The value of L-divergence at an L-projection of p on Q is denoted by L(Q||p).

The following Sanov's Theorem for Sources (LST) will be needed for establishing the consistency in L-divergence. The Theorem provides rate of the exponential decay of the posterior probability.

LST  Let N  ⊂  Me. As n  →    ∞  ,

[formula]

with probability one.

Proof Let [formula], [formula], and [formula], [formula]. In this notation [formula]. The posterior probability is bounded above and below as follows:

[formula]

where [formula], [formula].

[formula] converges with probability one to L(Me||r)  -  L(N||r). The same is the 'point' of a.s. convergence of [formula] of the lower bound. [formula]

Let for ε  >  0, [formula]. Let [formula].

Corollary Let there be a finite number of L-projections of r on Me. As n  →    ∞  , π(q∈NCε(Me)|xn)  →  0, with probability one.

Standard Bayesian consistency follows as a special π(r)  >  0 case of the Corollary.

Posterior Equi-concentration of Sources

If there is more than one L-projection of r on Me, how is the posterior probability asymptotically spread among them? This issue is 'in probability' answered by the next Theorem. Let N1ε  ⊂  Nε(Me) contain (among other sources) just one L-projection of r on Me.

Theorem Let there be [formula] L-projections of r on Me. Then for n going to infinity, [formula], in probability.

Proof For any ε  >  0, there exists such n0 that for n  >  n0, r{xn:S(λ)  =  S(L)}  =  1, where [formula], L is L-projection of r on Me, and S(  ·  ) stands for 'set of all'. Consequently, π(L|xn)  ≥  π(q|xn) for all q∈Me. Posterior π(q∈N1ε|xn) can be expressed as [formula], where [formula], [formula]; [formula], [formula]. Markov's inequality implies that π(q|xn) / π(L|xn) converges to zero, in probability. Slutsky's Theorem then implies that A, B converges to zero, in probability. [formula]

EndNotes

In order to place this note in context let us make a few comments.

1) An inverse of Sanov's Theorem has been established by Ganesh and O'Connell [\cite=GO] for the case of sources with finite alphabet, by means of formal large-deviations approach. Unaware of their work, the present author developed in [\cite=g] an inverse of Sanov's Theorem for n-sources, for both discrete and continuous alphabet and applied it to conditioning by rare sources problem and criterion choice problem; cf. also [\cite=GJ].

2) At [\cite=g] the concepts of L-divergence and L-projection were introduced. See [\cite=g] for a short discussion on why or why not the 'new' divergence.

3) The present form of Sanov's Theorem for Sources (LST) as well as its proof are new.

4) Bayesian consistency under misspecification has already been studied by Kleijn and van der Vaart [\cite=KV] for general setting of continuous prior on a set of continuous sources, using a different technique. The authors developed sufficient conditions for somewhat related consistency (cf. Corollary 2.1 and Lemma 6.4 of [\cite=KV]) as well as rates of convergence. The equi-concentration was not considered there.

Acknowledgements

Supported by VEGA grant 1/3016/06.

With a typo in statement and proof of LST (Me and N were interchanged) this note appeared as: M. Grendar, L-divergence consistency for a discrete prior, J. Stat. Res., 40(1), 73-76, 2006.