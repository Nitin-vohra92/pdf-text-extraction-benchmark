Asymptotics for regression models under loss of identifiability

introduction

This paper discusses the asymptotic behavior of regression models under general conditions. Let F be the family of possible regression functions and suppose that we observe a random sample

[formula]

from the distribution P of a vector (X,Y), with Y a real random variable, that follows the regression model

[formula]

In our model, the function f0 will be the best regression function among the set F:

[formula]

where

[formula]

is the L2 norm for an square integrable function g.

For simplicity, we assume that the best function f0 is unique.

A natural estimator of f0 is the least square estimator (LSE) [formula] that minimizes the sum of square errors (SSE):

[formula]

[formula] should be expected to converge to the function f0 under suitable conditions. If F is a parametric family and Θ is a set of possible parameters, [formula], the LSE is the parameter [formula] that minimizes

[formula]

Let us write Θ0 the set of parameters realizing the best regression function f0: [formula], fθ = f0. If the set F is large enough, it may be possible that the dimension of the interior of the set Θ0 is larger than zero and various difficulties arise in analyzing the statistical properties of estimators of f0. This is for example the case if F contains multilayer neural networks with redundant hidden units (see [\cite=Fukumizu2003]).

Under loss of identifiability of the parameters, the asymptotics for likelihood functions has been studied by [\cite=Liu] who improve the method of [\cite=Dacunha] and [\cite=Dacunha2]. The authors establish a general quadratic approximation of the log-likelihood ratio in a neighborhood of the true density which is valid with or without loss of identifiability of the parameter of the true distribution. In this paper, we will use a similar idea, but here we are interested in regression functions, not in density functions, so we will introduce generalized derivative functions:

[formula]

Under some general regularity conditions, this paper shows that

[formula]

where D is the L2 limits of the generalized derivative function df as ||f(X) - f0(X)||2  →  0. Such result allows for example, to fully explicit the asymptotic behavior of the SSE when regression functions are multilayer neural networks, even if F is too big and contains neural networks with redundant hidden units.

This result is a consequence of the very general inequality: For all regression function f∈F, f  ≠  f0,

[formula]

and the fact that the empirical process :

[formula]

converges in distribution to some Gaussian process. For instance, when [formula] is a Donsker class, [formula] converges uniformly to some zero-mean Gaussian process.

Note that, even if the set F is a regular parametric family, the function θ  ↦  dfθ(x) may be not extendable by continuity in θ0∈Θ0, hence the Donsker property of the set of generalized derivative functions has to be carefully studied. This problem occurs also for the generalized score functions Sθ of [\cite=Liu], although the authors did not mention it.

The paper is organized as follows: Section 2 establishes the asymptotic distribution of the SSE for regression models if the set of generalized derivative functions S is Donsker. In the next section, we show how to get the Donsker property for S in the parametric case but under loss of identifiability. As an example, section 4 characterizes the asymptotic distribution of regression using neural networks with redundant hidden units.

Asymptotic distribution of the SSE

For sake of simplicity we consider identically distributed independent variables, but all the following results can be easily generalized to geometrically mixing stationary sequence of random variables as in [\cite=Olteanu] or [\cite=Gassiat]. For example, our results may be applied to non-linear autoregressive models using multilayer neural neural networks as in [\cite=Yao]. Under fairly general condition the LSE is consistent and the regularity conditions of this paper imply consistency, so the asymptotic distribution of SSE is determined by the local properties of the regression function in a small L2-neighborhood of the best regression function f0.

First we begin with some definitions.

The envelope function of a class of functions F is defined as

[formula]

We will use the abbreviation [formula] for an integrable function f and a probability measure P. A family of random sequences

[formula]

is said to be uniformly OP(1) if for every δ > 0, there exist constants M > 0 and N(δ,M) such that

[formula]

for all n  ≥  N(δ,M).

A family of random sequences

[formula]

is said to be uniformly oP(1) if for every δ > 0 and ε > 0 there exists a constant N(δ,ε) such that

[formula]

for all n  ≥  N(δ,ε).

Upper bound for the SSE

We prove this lemma which gives a very general upper bound for the sum of square errors.

For all regression function f∈F with f  ≠  f0:

[formula]

We have

[formula]

Now, let us write

[formula]

then remark that 2AZ - A2  ≤  Z2 implies that

[formula]

Approximation of the SSE

Define the limit-set of derivatives D as the set of functions d∈L2(P) such that one can find a sequence (fn)∈F satisfying [formula] and [formula]. With such (fn), define, for all t∈[0,1], ft = fn, where [formula]. We thus have that, for any d∈D, there exists a parametric path (ft)0  ≤  t  ≤  α such that for any t∈[0,α], ft∈F, t  ↦  ||ft(X) - f0(X)||2 is continuous, tends to 0 as t tends to 0 and ||d - dft||2  →  0 as t tends to 0. Using the reparameterization ||fu(X) - f0(X)||2 = u, for any d∈D, there exists a parametric path (fu)0  ≤  u  ≤  α such that:

[formula]

Now, let us state the following theorem:

If the set of generalized derivative function S is a Donsker class and for any d in the limit-set of derivatives D, a reparameterization (fu)0  ≤  u  ≤  α exists so that ||d - dfu||2  →  0 as u tends to 0 and the map

[formula]

admits a second-order Taylor expansion with strictly positive second derivative [formula] at u = 0, then

[formula]

We have

[formula]

As soon as [formula],

[formula]

and

[formula]

Since, S is Donsker

[formula]

and S admits an envelope function F such that P(F2) <   ∞  , so S2 is Glivenko-Cantelli and

[formula]

Then, one may apply inequality ([\ref=ineg2]) to obtain

[formula]

By lemma [\ref=lemmeineg],

[formula]

Using ([\ref=compar2]), we obtain that

[formula]

Let [formula]. Using ([\ref=compar3]), we obtain that

[formula]

Now, [formula], thus for a sequence un decreasing to 0, and with

[formula]

we obtain that

[formula]

But, using the Donsker property, the definition of Δn and the property of asymptotic stochastic equicontinuity of empirical processes indexed by a Donsker class, we get:

[formula]

and

[formula]

Moreover, since S admits a square integrable envelope function, a function m exists such that for u1 and u2 belonging to a parametric path converging to a limit function d:

[formula]

and since, along a path, the map

[formula]

admits a second-order Taylor expansion with strictly positive second derivative [formula] at u = 0, we can use classical normal asymptotic theorem for M-estimators (see theorem 5.23 of [\cite=Vandervaart]) along this parametric paths, to obtain a sequence of finite subsets Dk increasing to D such that

[formula]

for any k, therefore, equality holds in ([\ref=inegscore]).

Define [formula] the centered Gaussian process with covariance the scalar product in L2(P), an immediate application of theorem [\ref=asymptotM] gives:

Assume that S is a Donsker class,

[formula]

converges in distribution to

[formula]

As we see, the Donsker property of the set of generalized derivatives functions S is fundamental for the results above. In the next section we will show how to get it for parametric models under loss of identifiability.

Donsker property for S

This section will give a framework for the demonstration of Donsker property for the set of generalized derivative functions S for parametric models and under loss of identifiability. Note that this framework could be easily adapted to likelihood ratio test and generalized score functions of [\cite=Liu].

First, we recall the notion of bracketing entropy. Consider the set S endowed with the norm [formula]. For every η > 0, we define an η-bracket by [formula] such that [formula]. The η-bracketing entropy is

[formula]

where [formula] is the minimum number of η-brackets necessary to cover S.

With the previous notations if

[formula]

then, according to [\cite=Vandervaart], the set S is Donsker. Note that, if the number of η-brackets necessary to cover S, [formula], is a polynomial function of [formula], S will be Donsker. If a class of function

[formula]

is parametric and regular, in general, for any θ1, θ2∈Θ there exists a function G∈L2(P) such that

[formula]

and according to [\cite=Vandervaart] a constant K exists such that,

[formula]

Hence, the set F is Donsker. However, even if the set F is parametric and regular, the set [formula] is not regular, since θ  ↦  dfθ(x) is, in general, not extendable by continuity in θ0 a parameter realizing the best regression function f0. Note that, it is also the case for the generalized score function Sθ of [\cite=Liu], in particular in the case of finite mixture models under loss of identifiability. Hopefully, we can show the Donsker property of the set S by an other method which can be applied also to generalized score function in the framework of likelihood ratio test as in [\cite=Olteanu].

For proving that [formula], is a polynomial function of [formula], we have to split S into two sets of functions: A set in a neighborhood of the best regression function f0 and a second one at a distance at least η of f0. For a sufficiently small η > 0, we consider Fη  ⊂  F, a L2-neighborhood of f0: [formula]. S is split into [formula] and [formula].

On [formula], it can be easily seen that

[formula]

for every [formula]. By ([\ref=modulusnum]), if ||θ1  -  θ2||  ≤  η3, a constant C exists such that

[formula]

Then, by the definition of Sη,

[formula]

and, a constant M exists so that

[formula]

Finally, we get:

[formula]

where D is the dimension of parameter vector of the model.

It remains to prove that the bracketing number is a polynomial function of ([formula]) for Sη. The idea is to reparameterize the model in a convenient manner which will allow a Taylor expansion around the identifiable part of the true value of the parameters, then, using this Taylor expansion, we can show that the bracketing number of Sη is a polynomial function of [formula]. Indeed, in many applications, there exists a reparameterization [formula] such that fθ = f0 is equivalent to the condition that φ  =  φ0 for all ψ. Then, positive integers (q0,q1) and linearly independent functions gβ0i, [formula], [formula], i = 1,...,q0, [formula] exist so that the difference of regression functions can be written:

[formula]

where [formula] are fixed parameter, αiνi are real parameters and δiγi are parameter vectors with sizes compatible with functions [formula] and [formula], [formula] are in a compact set and inequality ([\ref=modulusnum]) is true for the regular parametric functions [formula]. Moreover ||f(φ,ψ) - f0||22  ≤  η2 on Sη.

We will see an example of such expansion in the next section. Note that similar expansion is also possible for the likelihood ratio framework (see [\cite=Liu], section 4). We get then the next result:

If an expansion like ([\ref=lrts]) exists, a positive integer d exists so that the number of η-brackets [formula] covering Sη is [formula].

The idea is to bound [formula] by the number of η-brackets covering a wider class of functions. For every fθ∈Fη, we will consider the reparameterization [formula] which allows to get a second-order development of the density ratio like ([\ref=lrts]).

Now, using the linear independence of functions gβi, gβ0i, [formula], [formula], for every vector [formula] of norm 1,

[formula]

Using the compacity of sets

[formula]

m > 0 exists so that for all (βi)1  ≤  i  ≤  q1 and v∈V,

[formula]

At the same time, since

[formula]

the Euclidean norm of coefficients [formula] in the development of [formula] is upper bounded by [formula]. This fact implies that Sη can be included in

[formula]

and a positive integer d exists so that [formula].

Note that, since the [formula], is a polynomial function of [formula], the Donsker property of S may be easily extended to β-mixing observations with respect to the norm ||.||2,β (see [\cite=Doukhan]).

Application to regression with neural networks

Feedforward neural networks or multilayer perceptrons (MLP) are well known and popular tools to deal with non-linear regression models. [\cite=White] reviews the statistical properties of MLP estimation in detail, however he leaves an important question pending: The asymptotic behavior of the estimator when the MLP in use has redundant hidden units. When the noise of the regression model is assumed Gaussian, [\cite=Amari] give several examples of the behavior of the likelihood ratio test statistic (LRTS) in such cases. [\cite=Fukumizu2003] shows that, for unbounded parameters, the LRTS can have an order lower bounded by O( log (n)) with n the number of observations instead of the classical convergence property to a χ2 law. [\cite=Hagiwara] investigate relation between LRTS divergence and weight size in a simple neural networks regression problem.

However, if the set of possible parameters of the MLP regression model are bounded the behavior of LRTS and more generally the SSE is still unknown. In this section, we derive the distribution of the SSE if the parameters are in a compact (bounded and closed) set.

The model

Let [formula] be the vector of inputs and [formula] be the parameter vector of the hidden unit i. The MLP real function with k hidden units can be written :

[formula]

with [formula] the parameter vector of the model. The transfer function φ will be assumed bounded and two times derivable. We assume also that the first and second derivatives of the transfer function φ: φ' and φ'' are bounded like for sigmoid functions, the most used tranfer functions. Moreover, in order to avoid a symmetry on the signs of the parameters, we assume that, for 1  ≤  i  ≤  k, ai  ≥  0. Let Θ  ⊂  R  ×  R + k  ×  Rk  ×  (d + 1) be the compact set of possible parameters, the regression model ([\ref=Regression]) is then:

[formula]

with X is a random input variable with probability law Q and

[formula]

a parameter such that fθ0 = f0. Note that the set of parameters Θ0 realizing the best regression function f0 may belong to a non-null dimension sub-manifold if the number of hidden units is overestimated. Suppose, for example, we have a multilayer perceptron with two hidden units and the true function f0 is given by a perceptron with only one hidden unit, say f0 = a0 tanh (w0x), with x∈R. Then, any parameter θ in the set:

[formula]

realizes the function f0. Hence, classical statistical theory for studying the LSE can not be applied because it requires the identification of the parameters (up to some permutations and sign symmetries) so that the Hessian matrix of mean square error with respect to the parameters will be definite positive in a neighborhood of the parameter vector realizing the true regression function. Let us denote k0 the minimal number of hidden units to realize the best regression function f0, we will compare the SSE of over-determined models against the true model :

[formula]

when unidentifiability occurs (i.e. when k > k0).

Asymptotic distribution of the difference of SSE

Let us give simple sufficient conditions for which the Donsker property of the generalized derivatives functions condition holds. Note that assumption H-1 allows that, for any accumulation sequence of parameter θn leading to f0, the regression functions [formula] are in a L2-neighborhood of f0, in the same spirit of locally conic models of [\cite=Dacunha]. Moreover, if the distribution Q of the variable X is not degenerated, it may be shown that the assumption H-3 is true for the sigmoid transfer function:

[formula]

with straightforward extension of the results of [\cite=Fukumizu1996].

are linearly independent in the Hilbert space L2(Q).

We then get the following result:

Let the map Ω:L2(Q)  →  L2(Q) be defined as [formula] Under the assumptions H-1, H-2 and H-3, a centered Gaussian process {W(d),d∈D} with continuous sample paths and a covariance kernel [formula] exists so that

[formula]

The index set D is defined as [formula], the union runs over any possible [formula] with [formula] and

[formula]

δ(i) = 1 if a vector [formula] exists so that: qj  ≥  0, [formula], [formula] and [formula], otherwise δ(i) = 0.

If the set of generalized derivative functions [formula] is a Donsker class, we can apply the theorem [\ref=asymptotM] to conclude. So, in order to show this Donsker property, we will get an asymptotic development of the generalized derivative function and apply proposition ([\ref=polycov]).

Reparameterization.

The idea is similar of the reparameterization of finite mixture models in [\cite=Liu]. Under assumption H-3, the writing of f0 with a neural network with k0 hidden units is unique, up to some permutations:

[formula]

Then, for a θ∈Θ, if fθ = f0, a vector t = (ti)1  ≤  i  ≤  k0 + 1 exists so that [formula] and, up to permutations, we have [formula] if t1 > 0, [formula], [formula], [formula] and [formula] if t1 > 0 else β  =  β0.

For 1  ≤  i  ≤  k0, let us define [formula] and, if [formula], let us write [formula]. If [formula], qj will be set at 0. Moreover, let us write [formula], if t1 > 0 else γ  =  β  -  β0.

We get then the reparameterization [formula] with

[formula]

With this parameterization, for a fixed t, Φt is an identifiable parameter and all the non-identifiability of the model will be in ψt. Namely, fθ will be equal to:

[formula]

So, for a fixed t, [formula] if and only if

[formula]

Now, the second derivative of the transfer function is bounded and a constant C exists so that we have the following inequalities:

[formula]

So, thanks to assumption H-2, the second order derivative of the function [formula] with respect to the components of Φt will be dominated by a square integrable function. Then, by the Taylor formula around the identifiable parameter Φ0t, we get the following expansion for the numerator of generalized derivative function:

For a fixed t, in the neighborhood of the identifiable parameter Φ0t, we get the following approximation:

[formula]

with

[formula]

and

[formula]

This development is obtained by a straightforward calculation of the derivatives of f(Φt,ψt) - f0 with respect to the components of Φt up to the second order.

So, the numerator of generalized derivative function can be written like ([\ref=lrts]), the proposition [\ref=polycov] can be applied to this model and the polynomial bound for the growth of bracketing number shows the Donsker property of generalized derivative functions. Finally, the lemma [\ref=dev] and the next section show that for any d in the limit-set of derivatives D a sequence of vector [formula] exists so that ||d - df(Φn,ψn)||2  →  0 as Φn tends to a Φ0t and since the map

[formula]

admits a second-order Taylor expansion with strictly positive second derivative [formula] at Φt  =  Φ0t , one can apply theorem [\ref=asymptotM] and corollary [\ref=corollasymp].

Asymptotic index set

The set of limit score functions D is defined as the set of functions d so that one can find a sequence [formula] satisfying ||f(Φn,ψn) - f0||2  →  0 and ||d - df(Φn,ψn)||2  →  0. This limit function depends on the development obtained in lemma [\ref=dev].

Let us define the two principal behaviors for the sequences f(Φn,ψn) which influence the form of functions d :

If the second order term is negligible with respect to the first one:

[formula]

If the second order term is not negligible with respect to the first one:

[formula]

In the first case, a set [formula] exists so that the limit function of df(Φn,ψn) will be in the set:

[formula]

In the second case, an index i exists so that :

[formula]

otherwise, the second order term will be negligible compared to the first one. So

[formula]

Hence, a set [formula] exists so that the set of functions d will be:

[formula]

where δ(i) = 1 if a vector [formula] exists so that: [formula], [formula], [formula] and [formula], otherwise δ(i) = 0.

Hence, the limit index set functions will belong to D.

Conversely, let d be an element of D, since function d is not null, one of its component is not equal to 0. Let us assume that this component is γ, but the proof would be similar with any other component. The norm of d is the constant 1, so any component of d is determined by the ratio: [formula].

Then, since Θ contains a neighborhood of the parameters realizing the true regression function f0, we can chose

[formula]

so that:

[formula]