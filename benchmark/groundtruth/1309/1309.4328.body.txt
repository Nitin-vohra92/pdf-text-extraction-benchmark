=1

The Beta-MANOVA Ensemble with General Covariance

The Beta-MANOVA Ensemble with General Covariance

Introduction

The first β-ensembles were introduced by Dumitriu and Edelman [\cite=Dumitriu2002], the β-Hermite ensemble and the β-Laguerre ensemble. A β- ensemble is defined to be a real random matrix with a nonrandom continuous tuning parameter β  >  0 such that when β  =  1,2,4, the β- ensemble has the same joint eigenvalue distribution as the real, complex, or quaternionic -ensemble. For β not equal to 1,2,4 its eigenvalue distribution interpolates naturally among the β  =  1,2,4 cases. A β-circular ensemble and four β-Jacobi ensembles shortly followed [\cite=Lippert2003], [\cite=Killip2004], [\cite=Forrester2005], [\cite=Edelman2008]. The extreme eigenvalues of the β-Jacobi ensembles were characterized by Dumitriu and Koev in [\cite=Dumitriu2008]. More recently, Forrester [\cite=Forrester2011] and Dubbs-Edelman-Koev-Venkataramana [\cite=Dubbs2013] separately introduced a β-Wishart ensemble with diagonal covariance, which generalizes the β-Laguerre ensemble by adding the covariance term.

This paper introduces the β-MANOVA ensemble with diagonal covariance, which generalizes the β-Jacobi ensembles by adding the covariance term. When β  =  1 this amounts to finding the distribution of the cosine generalized singular values of the pair (Y,XΩ), where X is m  ×  n Gaussian, Y is p  ×  n Gaussian, and Ω is n  ×  n diagonal pds. Note that forcing Ω to be diagonal does not lose any generality; using orthogonal transformations, were Ω not diagonal we could replace it with its diagonal matrix of eigenvalues and preserve the model. Our β-MANOVA ensemble also generalizes the real, complex, and quaternionic MANOVA ensembles (the last of which has never been studied). [\cite=Fisher1939], [\cite=Hsu1939], and [\cite=Roy1939] independently solved the β  =  1 identity-covariance case, [\cite=Constantine1963] solved our problem in the β  =  1, general-covariance case, and [\cite=James1964] solved our problem in the β  =  2, general-covariance case. We find the joint eigenvalue distribution of the β-MANOVA ensemble, and generalize Dumitriu and Koev's results in [\cite=Dumitriu2008] by finding the distribution of the largest generalized singular value of the β-MANOVA ensemble. We also set the covariance to the identity to add a fourth β-Jacobi ensemble to the literature in Theorem 3.1. Our β-MANOVA ensemble is unique in that it is not built on a recursive procedure, rather it is sampled by calling the sampler for the β-Wishart ensemble.

Generalizations of our results exist in the β  =  1,2 cases by adding a mean matrix to one of the Wishart-distributed parameters. The β  =  1 case is from [\cite=Constantine1963] and the β  =  2 case is from [\cite=James1964].

The sampler for the β-Wishart ensemble of Forrester [\cite=Forrester2011] and Dubbs-Edelman-Koev-Venkataramana [\cite=Dubbs2013] is the following algorithm:

in in

The elements of Σ are distributed according to the following theorem of [\cite=Forrester2011] and [\cite=Dubbs2013]:

The distribution of the singular values [formula], [formula], generated by the above algorithm is equal to:

[formula]

where [formula] and K(β)m,n are defined in the upcoming section, Preliminaries.

To get the generalized singular values of the β-MANOVA ensemble with general covariance, in diagonal C, we use the following algorithm which calls BetaWishart(m,n,β,D). Let Ω be an n  ×  n diagonal matrix.

in in

Our main theorem is the joint distribution of the elements of C,

The distribution of the generalized singular values [formula], [formula], generated by the above algorithm for m,p  ≥  n is equal to:

[formula]

where [formula] and K(β)m,n are defined in the upcoming section, Preliminaries.

We also find the distributions of the largest generalized singular value in certain cases:

If [formula],

[formula]

where the Jack function C(β)κ and Pochhammer symbol (  ·  )(β)κ are defined in the upcoming section, Preliminaries.

These expressions can be computed by Edelman and Koev's software, mhg, [\cite=Koev2006].

It is actually intuitive that BetaMANOVA(m,n,p,β,Ω) should generalize the real, complex, and quaternionic MANOVA ensembles with diagonal covariance using the "method of ghosts." The method of ghosts was first used implicitly to derive β-ensembles for the Laguerre and Hermite cases in [\cite=Dumitriu2002], was stated precisely by Edelman in [\cite=Edelman2010], and was expanded on in [\cite=Dubbs2013]. To use the method of ghosts, assume a given ensemble is full of β-dimensional Gaussians, which generalize real, complex, and quaternionic Gaussians and have some of the same properties: they can be left invariant or made into a χβ's under rotation by a real orthogonal or "ghost orthogonal" matrix. Then apply enough orthogonal transformations and/or ghost orthogonal transformations to the ghost matrix to make it all real.

In the β-MANOVA case, let X be m  ×  n real, complex, quaternion, or ghost normal, Y be p  ×  n real complex, quaternion, or ghost normal, and let Ω be n  ×  n diagonal real pds. Let ΩX*XΩ have eigendecomposition UΛU*, and ΩX*XΩ(Y*Y)- 1 have eigendecomposition VMV*. We want to draw M so we can draw [formula]. Let ~   mean "having the same eigenvalues."

[formula]

which we can draw the eigenvalues M of using BetaWishart(p,n,β,Λ- 1)- 1. Since Λ can be drawn using BetaWishart(m,n,β,Ω2), this completes the algorithm for BetaMANOVA(m,n,p,β,Ω) and proves Theorem 1.1 in the β  =  1,2,4 cases.

The following section contains preliminaries to the proofs of Theorems 1.1 and 1.2 in the general β case. Most important are several propositions concerning Jack polynomials and Hypergeometric Functions. Proposition 2.1 was conjectured by Macdonald [\cite=Macdonald] and proved by Baker and Forrester [\cite=Baker1997], Proposition 2.3 is due to Kaneko, in a paper containing many results on Selberg-type integrals [\cite=Kaneko1993], and the other propositions are found in [\cite=Forrester2010].

Preliminaries

We define the generalized Gamma function to be

[formula]

for [formula].

[formula]

[formula]

If X is a diagonal matrix,

[formula]

As in Dumitriu, Edelman, and Shuman, if [formula], [formula] is nonnegative, ordered non-increasingly, and it sums to k. Let α  =  2 / β. Let [formula]. We define l(κ) to be the number of nonzero elements of κ. We say that μ  ≤  κ in "lexicographic ordering" if for the largest integer j such that μi  =  κi for all i  <  j, we have μj  ≤  κj.

As in Dumitriu, Edelman and Shuman, [\cite=Dumitriu2007] we define the Jack polynomial of a matrix argument, C(β)κ(X), as follows: Let [formula] be the eigenvalues of X. C(β)κ(X) is the only homogeneous polynomial eigenfunction of the Laplace-Beltrami-type operator:

[formula]

with eigenvalue ραk + k(n - 1), having highest order monomial basis function in lexicographic ordering (see Dumitriu, Edelman, Shuman, Section 2.4) corresponding to κ. In addition,

[formula]

We define the generalized Pochhammer symbol to be, for a partition [formula]

[formula]

As in Koev and Edelman [\cite=Koev2006], we define the hypergeometric function [formula] to be

[formula]

The best software available to compute this function numerically is described in Koev and Edelman, mhg, [\cite=Koev2006]. [formula].

We will also need two theorems from the literature about integrals of Jack polynomials and hypergeometric functions.

Conjectured by MacDonald [\cite=Macdonald], proved by Baker and Forrester [\cite=Baker1997] with the wrong constant, correct constant found using Special Functions [\cite=Andrews1999] (Corollary 8.2.2):

Let X be a diagonal matrix.

[formula]

where [formula].

From [\cite=Forrester2010],

If X  <  I is diagonal,

[formula]

Kaneko, Corollary 2 [\cite=Kaneko1993]:

Let [formula] be nonincreasing and X be diagonal. Let a,b >  - 1 and β  >  0.

[formula]

From [\cite=Forrester2010],

Let X be diagonal,

[formula]

From [\cite=Forrester2010],

If X is n  ×  n diagonal and a or b is a nonpositive integer,

[formula]

From [\cite=Forrester2010],

[formula]

Main Theorems

Proof of Theorem 1.1. Let m,p  ≥  n. We will draw M by drawing Λ  ~  P(Λ)  =  BetaWishart(m,n,β,Ω2), and compute M by drawing M  ~  P(M|Λ)  =  BetaWishart(p,n,β,Λ- 1)- 1. The distribution of M is [formula]. Then we will compute C by [formula]. We use the convention that eigenvalues and generalized singular values are unordered. By the [\cite=Dubbs2013] BetaWishart described in the introduction, we sample the diagonal Λ from

[formula]

[formula]

Likewise, by inverting the answer to the [\cite=Dubbs2013] BetaWishart described in the introduction, we can sample diagonal M from

[formula]

To get P(M) we need to compute

[formula]

[formula]

Expanding the hypergeometric function, this is

[formula]

[formula]

Using Proposition 2.1,

[formula]

[formula]

Cleaning things up, By the definition of the hypergeometric function, this is

[formula]

Converting to cosine form, [formula], this is

[formula]

If we set Ω  =  I and ui  =  c2i, [formula] obey the standard β-Jacobi density of [\cite=Lippert2003], [\cite=Killip2004], [\cite=Forrester2005], and [\cite=Edelman2008].

[formula]

Proposition 2.2 works from the statement of Theorem 1.1 because C2(C2 - I)- 1  <  I (we know that M > 0 from how it is sampled, so 0  <  C2  =  (M + I)- 1  <  I, likewise C2 - I).

[formula]

or equivalently

[formula]

If we substitute ui  =  c2i, by the change-of-variables theorem we get the desired result.

Proof of Theorem 1.2. Let [formula]. Changing variables from (3.1) we get

[formula]

Taking the maximum eigenvalue, following mvs.pdf,

[formula]

Letting [formula], changing variables again we get

[formula]

Expanding the hypergeometric function we get

[formula]

Using Proposition 2.3,

[formula]

Now

[formula]

Therefore,

[formula]

Using (3.4) and the definition of the hypergeometric function we get

[formula]

Rewriting the constant we get

[formula]

Commuting some terms gives

[formula]

The left fraction in parentheses is

[formula]

Hence

[formula]

Now H  =  M- 1 and C  =  (M + I)- 1 / 2, so equivalently,

[formula]

Remark. Using [formula] and setting Ω  =  I this is

[formula]

so by using Proposition 2.4, this is

[formula]

which is familiar from Dumitriu and Koev [\cite=Dumitriu2008].

Now back to the proof of Theorem 1.2. If we use Proposition 2.4 on (3.6) we get

[formula]

Using the approach of Dumitriu and Koev [\cite=Dumitriu2008], let t  =  (m - n + 1)β / 2 - 1 in [formula]. We can prove that the series truncates: Looking at (3.7), the hypergeometric function involves the term

[formula]

which is zero when i  =  1 and j - 1  =  t, so the series truncates when any κi has κi - 1  ≥  t, or just κ1  =  t + 1. This must happen if k  >  nt. Thus (3.7) is just a finite polynomial,

[formula]

Let Z be a positive-definite diagonal matrix, and ε a real with |ε|  >  0. Define

[formula]

Using Proposition 2.5,

[formula]

Using the definition of the hypergeometric function and the fact that the series must truncate,

[formula]

Now the limit is obvious

[formula]

Plugging this expression into (3.8)

[formula]

Cancelling via Proposition 2.6 gives

[formula]

Numerical Evidence

The plots below are empirical cdf's of the greatest generalized singular value as sampled by the BetaMANOVA pseudocode in the introduction (the blue lines) against the Theorem 1.2 formula for them as calculated by mhg (red x's).

Acknowledgments

We acknowledge the support of the National Science Foundation through grants SOLAR Grant No. 1035400, DMS-1035400, and DMS-1016086. Alexander Dubbs was funded by the NSF GRFP.