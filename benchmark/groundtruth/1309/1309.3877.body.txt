A Metric-learning based framework for Support Vector Machines and Multiple Kernel Learning

Introduction

Support Vector Machines [\cite=Taylor2000] have been an active research area for more than two decades. They have been widely used not only because of their excellent predictive performance but also because their generalization ability is supported by solid generalization error bounds defined over the radius-margin ratio. Mahalanobis metric learning has started attracting significant attention rather recently [\cite=Lanckriet2004] [\cite=Ong2003] [\cite=Sonnenburg2006a] [\cite=rakotomamonjy2008] [\cite=Cortes2010l2] [\cite=weinberger2009distance] [\cite=goldberger2005nca] [\cite=globerson2006mlc]. However, relying mostly on intuition, it still lacks theoretical support. Very recently SVM has been reformulated in the metric learning context and has been shown to be equivalent to a Mahalanobis metric learning problem [\cite=Huyen2011]. This new interpretation of SVM brings the worlds of SVM and metric learning together into a single unified view. This allows us to exploit the advantages of each one to develop for example hybrid algorithms or to derive theoretical error bounds for metric learning problems exploiting the SVM error bounds.

In this paper we build on the ideas presented in [\cite=Huyen2011] to develop a novel metric-learning-based SVM framework and equip SVM with a metric learning bias. More precisely we will define new SVM optimization problems that will make use of both the between-and within-class distances. Under the metric learning view of SVM, the margin plays the role of the between-class distance. However, SVM ignores the within-class distance. In the metric-learning-based SVM framework that we present here, we maximize the SVM margin and minimize some measure of the within-class distance. We can use different measures of the within-class distance with SVM and we will define a new such measure that is more appropriate for SVM. We will give a new SVM algorithm that optimizes both the margin and the new within-class distance measure that we define. The resulting optimization problem is convex and can be directly kernelized. Moreover we will follow the same approach with MKL and show that it, also, can be formulated as a Mahalanobis metric learning problem. As a result we develop a novel family of MKL methods that incorporate the metric learning bias. We experiment with the developed algorithms on a number of benchmark datasets and saw that the incorporation of the within-class distance measures in the SVM learning problem brings significant performance improvements.

Finally, we give a unified view of SVM, metric-learning-based SVM, metric learning algorithms and Fisher Discriminant Analysis ( FDA) using the concepts of between-class and within-class distances. This view provides new insights to the existing algorithms and unveils some unexpected relations.

The rest of the paper is organized as follows. In the next section we briefly describe the basic concepts of SVM, FDA and Mahalanobis metric learning. In section [\ref=related] we summarize the metric learning view of SVM. In section [\ref=sec:emkl] we propose the metric-learning-based SVM framework which relies on the use of the between-class and within-class distance measures. In section [\ref=sec:unifiedview] we provide a common view of SVM, metric-learning based SVM, FDA and metric learning algorithms. In Section 5 we employ the metric learning perspective in the context of MKL and develop the metric-learning-based MKL framework. We report experimental results in section [\ref=sec:expr] and conclude with section [\ref=sec:conclusion].

Preliminary

Consider a binary classification problem in which we are given a set of n learning instances [formula], where yi is the class label of the [formula] instance and yi∈{ + 1, - 1}. Let [formula] be the samples of class Ci.

Support Vector Machines and MKL

SVM s learn a hyperplane [formula] which maximizes the margin between the two classes. The SVM margin is defined informally as the distance of the nearest instances from [formula] [\cite=Taylor2000]. The SVM optimization problem is:

[formula]

The performance of SVM strongly depends on the choice of kernel. MKL addresses that problem by selecting or learning the appropriate kernel(s) for a given problem [\cite=Lanckriet2004] [\cite=Cristianini2002] [\cite=Ong2003] [\cite=rakotomamonjy2008] [\cite=Cortes2010] [\cite=Sonnenburg2006a] [\cite=Argyriou05]. Given a set of basis kernel functions, [formula], MKL learns a kernel combination, usually convex, so that some cost function, e.g margin, kernel alignment, is optimized. The cost function that is most often used in MKL is the margin-based objective function of SVM [\cite=rakotomamonjy2008] [\cite=Cortes2010] [\cite=Sonnenburg2006a] [\cite=Argyriou05] [\cite=Lanckriet2004] [\cite=Kloft2011]. We denote by MKLγ the MKL method which learns linear kernel combinations and uses as its cost function that of standard SVM, i.e. it finds a linear kernel combination that maximizes the margin. Its optimization problem is:

[formula]

Mahalanobis metric learning

The squared Mahalanobis distance between two instances [formula] and [formula] is: where [formula] is a positive semi-definite matrix, [formula]. Learning a Mahalanobis metric parametrized by [formula] ([formula]) is equivalent to learning a linear transformation [formula] where [formula].

Typical metric learning methods try to bring instances of the same class close while pushing instances of different classes far away. They do so by optimizing a cost function of the Mahalanobis distance, while globally or locally satisfying some constraints on the pairwise distances. This is equivalent to minimize some measure of the within-class distances while maximizing the between-class distances [\cite=Huyen2011].

A typical Mahalanobis metric learning optimization problem has the following form:

[formula]

where F and Constraints are the cost function and the constraints respectively, parametrized either by [formula] or [formula]. The constraints can be local, i.e applied to instance pairs that are in the same neighborhood, or global, i.e applied for all instance pairs. They usually have the following form [\cite=xing2003dml] [\cite=globerson2006mlc] [\cite=davis2007itm] [\cite=weinberger2009distance]:

[formula]

for some constants f1,f2. These constraints reflect the primal bias of most metric learning algorithms, maximizing the between-class distance and minimizing the within-class distance, or in other words, instances of the same class should have small distances while those of different classes should have large distances. If we take two simple measures of the within and between-class distances such as the sum of the pairwise distances of the same-class instances and the sum of the pairwise distances of the different-class instances respectively, the pairwise constraints in ([\ref=metric.learning.constr]) will ensure that our between-class distance, denoted by dB, is bigger than f2  ×  t1, and our within-class distance, denoted by dW, is smaller than f1  ×  t2, where t1,t2 are the numbers of instance pairs of the same and different class respectively over which we impose the constrains..

Fisher Discriminant Analysis

FDA, although usually not considered a metric learning method, it also learns linear projections in the same manner as metric learning methods. It uses a similar learning bias as metric learning algorithms: the samples are well separated if their between-class distance is large and their within-class distance is small. These quantities are defined as follows [\cite=Duda2001]: let the sample mean of class Ci be [formula], then the within-class distance (or within-class scatter) of two classes C1,C2, is defined as [formula], and the between-class distance (or between-class scatter) is defined as the squared distance between the means of the two classes, [formula]. In the case of two-class problems FDA seeks for a projection line with a direction vector [formula] which optimizes the ratio of the between-class over the within-class distances in the projected space, its cost function is:

[formula]

where [formula] and [formula] are the within-class and between-class scatter matrices, respectively. Another measure often used in the different variants of FDA is the total scatter matrix, [formula], i.e a multiply of the covariance matrix which quantifies the total data spread.

Related work

Do et al [\cite=Huyen2011] recently show SVM can be formulated as a Mahalanobis metric learning problem in which the transformation matrix is diagonal [formula], [formula]. In the metric learning jargon SVM learns a diagonal linear transformation [formula] and a translation b which maximize the margin and place the two classes symmetrically in the two different sides of the hyperplane [formula]. In the standard view of SVM, the space is fixed and the hyperplane is moved around to achieve the optimal margin. In the metric view of SVM, the hyperplane is fixed to [formula] and the space is scaled, [formula], and then translated, b, so that the instances are placed optimally around [formula] [\cite=Huyen2011].

[\cite=Huyen2011] proposed a measure of the within-class distance for SVM. This measure is inspired by the relation, developed in that paper, between SVM and LMNN--Large Margin Nearest Neighbor [\cite=weinberger2009distance]--a popular metric learning algorithm. It is defined as the sum of the distances of the instances from the margin hyperplane and for the class Ci, it is given by: [formula]. The authors then proposed an SVM variant, called ε-SVM, which optimizes the margin and the above within-class distance measure, essentially combining both the SVM and the LMNN learning biases. As we will see below ε-SVM turns out to be a special case of the SVMm which we will describe in section [\ref=subsec:within_band]. The optimization problem of ε-SVM is:

[formula]

which is equivalent to:

[formula]

where ηi is the distance of the ith instance from its margin hyperplane, and ξi are the SVM slack variables which allow for the soft margin. This problem is convex and can be kernelized directly as a standard SVM.

[\cite=Shivaswamy2010] proposed to maximize the margin and to constrain the outputs of SVM, their optimization problem thus optimizes the margin and some measure of the data spread. This approach also falls within our general metric-learning-based SVM framework that we will present right away.

A metric-learning-based SVM framework

Since SVM can be seen as a metric learning algorithm, we can interpret it in terms of the within- and between-class distances, as is done with typical metric learning algorithms using ([\ref=metric.learning.opt]), ([\ref=metric.learning.constr]). The SVM margin can be seen as a measure of the between-class distance. Unlike FDA where the between-class distance is defined via the distance of the class means, in SVM s, it is defined as the minimum distance of different-class instances from the hyperplane, i.e. twice the SVM margin. Unlike other metric learning algorithms, where the between-class distance takes into account all pairs of instances of different classes, the SVM between-class distance only focuses on pairs of instances of different classes which lie on the margin (in [\cite=Huyen2011], the SVM margin is reformulated as: [formula]). In other words, SVM between-class distance, i.e the margin, is a simple example of ([\ref=metric.learning.constr]).

Similar to most metric learning algorithms, SVM maximizes the between-class distance; however, unlike them, it ignores the within-class distance. In other words, instances of different classes are pushed far away by the margin, but there is no constraint on instances of the same class.

Interpreting SVM as a metric learning problem allows us to fully equip it with the metric learning primal bias, i.e to maximize some between-class distance measure and minimize some within-class distance measure. We propose a learning framework in which in addition to the standard SVM margin maximization, we also minimize some measure of the within-class distance. We will call the resulting learning algorithms metric-learning-based SVM. In the next sections we will give different general functions of the within and between class distances which can be the target of optimization. We will then propose SVM specific within-class distance measures, and finally formulated the full learning problem which now will also include some measure of the within class distance.

Within- and between-class distances cost functions

There are several ways to maximize the between-class distances dB while minimizing the within-class distances dW. Bellow we give some simple and widely used cost functions that include these two terms: Depending on the exact measures of the within- and between-class distances, these cost functions will lead to convex or non-convex optimization problems. An example of the first cost function F1 of ([\ref=ratio.form]) is FDA ([\ref=fld]) where the resulting optimization problem is convex and easy to solve. F1 places equal importance on the between- and within-class distances. On the other hand F2,F3, and F4 allow us to better control their trade-off through the introduction of an additional hyperparameter λ. Within our metric-learning-based SVM framework, dB will denote the margin, and dW will denote some measure of the within-class distance

Metric-learning based SVM

In this section we start by presenting a new measure of the within-class distance dW which is appropriate for SVM. We will then use this measure to define one instantiation of our metric-learning-based SVM framework. In addition, we discuss a number of other within-class distance measures which can be used to define other instantiations of our framework.

A bandwidth-based within-class distance measure

One way to control the within-class distance is by forcing the learning instances to stay close to their class margin hyperplane, confining them within a band defined by their margin hyperplane and a hyperplane parallel to it. The width of this band can be seen as a measure of the within-class distance. This bandwidth is equal to the maximum distance of the instances to their class margin given by [formula]. To avoid the effect of outlier instances we add slack variables that allow some of them to lie outside the band.

We introduce now a new SVM variant, which optimizes both the margin and the measure of the within-class distance described above. Its cost function trades-off the margin maximization and the bandwidth minimization. Let diW2  =  εiγ, εi states how many times is the bandwidth of the Ci class larger than the margin. To simplify the optimization problem we use the same ε  =  ε1  =  ε2 for both bandwidths. Using the cost function given in equation ([\ref=minus.form]), we get the following optimization problem:

[formula]

Problem ([\ref=eq:eSVM]) is not convex, however, it can be easily kernelized as standard SVM. If we fix ε then it becomes convex. Fixing ε is equivalent to fixing a specific value for the margin-bandwidth ratio and then maximizing the margin; this is described by the following optimization problem:

[formula]

where [formula] and [formula] are the slack variables that allow for the margin and bandwidth violations, the latter is needed to alleviate the effects of outlier instances. Intuitively the hyperparameter C2, which controls the bandwidth slack variable, should be bigger than C1 which controls the margin slack variable, since we can tolerate more instances outside the band than inside the margin.

Problem ([\ref=eq:eSVM_fixed_e]) is convex, quadratic and can be solved similarly to standard SVM. We will call it SVMm. Its dual form is:

[formula]

It can be also kernelized directly as SVM, as the term [formula] that appears in the dual form ([\ref=dual.form.svmmetric]) can be replaced by a kernel function.

Using the F4 cost function ([\ref=plus2.form]) we get the following non-convex optimization problem which also "maximizes the margin while keeping the bandwidth diW1 small":

[formula]

However, this problem is non convex even for a fixed ε; therefore, we do not explore it further in this paper and we leave it for future work.

Interestingly, we note that if in the optimization problem ([\ref=eq:eSVM]) we set ε = 0 then this problem reduces to the optimization problem ([\ref=eq:eSVMTotal]); therefore, we can also solve ε-SVM of [\cite=Huyen2011] as a special case of SVMm using the SVMm solver.

We note that SVMm is a general formulation for both SVM and ε-SVM. In the limit when ε  →    ∞  , SVMm reduces to standard SVM, and when ε  →  0, SVMm becomes ε-SVM. From the optimization point of view we remark that the bigger value of ε is, the smaller the number of the active constraints will be (see the second set of constraints in ([\ref=eq:eSVM_fixed_e])). Thus in terms of running time, ε-SVM is the slowest, followed by SVMm and then SVM. However, all are quadratic optimization problems and can be solved efficiently.

One may also think of the FDA within-class distance as an appropriate measure, i.e. [formula]. Xiong et al. [\cite=Tao2005] combined SVM and FDA, optimizing like that the margin and the FDA within-class distance. However, we note that they simply introduced this combination without putting it in the metric learning context that we described here, and they provided no interpretation on the use of the within- and between-class distances. Still their work falls into our metric-learning-based SVM framework as a special case. Using the cost function of ([\ref=plus.form]) to optimize the margin and the FDA within-class distance, we can formulate the following optimization problem:

[formula]

([\ref=eq:eSVMLDA]) is convex and is equivalent to the standard SVM problem in the transformed space [formula], where [formula] is the FDA within-class scatter matrix and [formula] is the identity matrix [\cite=Tao2005]. However, it is not straight forward to kernelize and [\cite=Tao2005] solved it only in the original feature space.

We note that it is an interesting question to develop more measures for the within- and between-class distances, or the data spread. For example the radius of the smallest sphere containing the data is also a measure of the data spread. Therefore, the different variants of the radius-margin based SVM s [\cite=Weston2000] [\cite=Rakotomamonjy2003] [\cite=Huyen2009b], which maximize the margin and minimize the radius, also fall to our metric-learning-based SVM framework. A more challenging problem is to determine which measure is the best in some specific situations.

A unified view of FDA, SVM, metric-learning-based SVM and metric learning

We first analyze FDA from the metric learning perspective as it is done with SVM in [\cite=Huyen2011]. We focus only on binary classification problems. Similar to SVM, we can also formulate FDA as a Mahalanobis metric learning problem, where the transformation matrix is diagonal [formula], [formula]. We reformulate the FDA between-class, [formula], and within-class, [formula], distances with respect to the [formula] hyperplane as the standard FDA between and within-class distances in the transformed space [formula], projected to the norm vector of [formula]:

[formula]

The FDA learning problem then can be stated in the metric learning jargon as follows: we learn a diagonal transformation [formula] so that in the transformed space [formula], the FDA between-class distance, with respect to the H1 hyperplane, is maximized and the FDA within-class distances, with respect the same hyperplane, is minimized. This learning problem leads to the standard FDA given in ([\ref=fld]). Thus, we see that from the metric learning perspective both FDA and SVM learn a diagonal transformation [formula]. However, the way they define their between and within-class distances is different, although all these distances are based on distances from the fixed hyperplane H1 and are defined globally. The FDA between-class distance is the squared distance between the two class means, with respect to the hyperplane [formula], while the SVM between-class distance is the minimum distance between two instances of the two classes, with respect to H1. On the other hand, FDA defines its within-class distances as the total sum of distances between each instance and its corresponding class mean, while standard SVM ignores the within-class distances, i.e there is no constraint on the pairwise distances of the same class instances. Similar to SVM, metric-learning based SVM defines its between-class distance as the margin; in addition, it defines its within-class distance in different ways. Radius-margin based SVM s [\cite=Weston2000] [\cite=Rakotomamonjy2003] [\cite=Huyen2009b], which are some instantiations of metric-learning based SVM, use the radius of the smallest sphere as a measure of data spread instead of explicitly defining the within-class distance. Figure [\ref=fig:svmfda] graphically depicts the difference between standard SVM, metric-learning based SVM, and FDA.

The metric-learning-based SVM algorithms optimize both the margin and some measures of the within-class distances. Still the constraints on the pairwise distances of the metric-learning-based SVM are a simple version of ([\ref=metric.learning.constr]). Their within-class distances are defined globally, with respect to a hyperplane. Their between-class distance, the margin, only takes into account the closest instances of the two classes.

Metric learning algorithms in general follow the within and between-class distances optimization. However they focus more on details, i.e pairwise distances are computed locally, unlike FDA or SVM which focus on global properties of the pairwise distances using the class means or the instances on the margin.

The derivation of generalization error bounds for the presented metric learning problems is an open issue. To the best of our knowledge no such bounds exist. Through the unified view of metric learning and SVM we want to use the error bounds of the latter to derive new bounds for the methods that we present here.

A metric-learning based MKL framework

In this section we will show how to exploit the metric-learning approach in the context of MKL. We first show how to formulate MKLγ as a metric learning problem with the help of two linear transformations. We proceed by developing a metric-learning-based MKL framework which optimizes both the margin and some measure of the within-class distance using multiple kernels.

MKL from a metric learning perspective

In MKL we learn linear combinations of kernels of the form [formula]. The feature space [formula] that corresponds to the learned kernel [formula] is given by the mapping:

[formula]

where [formula] is the mapping to the Hk feature space associated with the Kk kernel, [formula], and [formula] is block diagonal matrix with block diagonal elements [formula].

Similar to SVM, we can also use the fixed hyperplane H1 and view MKLγ as learning a block diagonal linear transformation [formula] in the concatenation feature space H, followed by a diagonal linear transformation [formula] and a translation b, such that the margin with respect to the [formula] hyperplane is maximized and the two classes are placed symmetrically in the two different sides of [formula]. The linear transformation [formula] associated with MKLγ is given by [formula]. So MKLγ is also a Mahalanobis metric learning problem where the transformation matrix is [formula].

From a metric learning perspective SVM uses a single diagonal matrix transformation given by [formula] and MKLγ uses two diagonal matrix transformations given by [formula]; both optimize the same cost function (i.e the margin). A formal comparison of the two methods under the metric-learning view can be found in the Appendix.

MKL and the optimization of the within-class distance

Similar to SVM, standard MKLγ optimizes only the margin, i.e the between-class distances but ignores the within-class distances. As before with SVM we will now develop a metric-learning-based MKL framework in which we will optimize both the margin and some measure of the within-class distances. We will give two examples of metric-learning-based MKL algorithms, using the SVMm, section [\ref=subsec:within_band], and ε-SVM [\cite=Huyen2011].

The SVMm optimization problem, equation ([\ref=eq:eSVM_fixed_e]), in the MKL context becomes:

[formula]

which is equivalent to:

[formula]

This optimization problem is the counterpart of SVMm in the MKL context; we will call it MKLm. As it was the case with SVMm where setting ε to zero leads to ε-SVM, here too, if in the MKLm optimization problem ([\ref=softMarginMetricMKL]), we set ε = 0 the resulting optimization problem, which we denote by ε-MKL, will correspond to the coupling of ε-SVM with MKL. ε-MKL thus learns kernel combinations that maximize the margin and minimize the within-class distance as the latter is measured by the sum of the distances from the margin hyperplanes.

The MKLm optimization problem, equation ([\ref=softMarginMetricMKL]), is convex and equivalent to:

[formula]

where: We can solve it by a two step algorithm, similar to the one used in SimpleMKL [\cite=rakotomamonjy2008]. At the first step of the algorithm we fix [formula]. With fixed [formula], problem ([\ref=softMarginMetricMKL]) becomes a SVM-like optimization problem. In the second step, with the optimal values computed by the SVM-like problem in the first step, we optimize the whole problem by gradient descent with respect to [formula]. The gradient of [formula] is: [formula] where α*,β* are the solution of the first step.

Experiments

We performed two sets of experiments. In the first we examined the performance of three metric-learning-based SVM algorithms, namely SVMm ([\ref=eq:eSVM_fixed_e]), ε-SVM ([\ref=eq:eSVMTotal_org]) [\cite=Huyen2011], and SVM-FDA ([\ref=eq:eSVMLDA]) [\cite=Tao2005]. Note that ε-SVM is a special case of SVMm when ε is set to zero. We used as baseline the performance of standard SVM and FDA. We used the stprtool toolbox [\cite=stprtool] for FDA. In the second set, we compared the two metric-learning-based MKL algorithms, i.e. MKLm and ε-MKL, with SimpleMKL [\cite=rakotomamonjy2008], a state-of-the-art MKLγ algorithm.

We experimented with UCI benchmark datasets. We first standardized the data to a zero mean and one variance. We examine the performance of the different SVM based methods over the four following kernels: linear, polynomial with degree 2 and 3, and a Gaussian kernel with σ  =  1. Kernel [formula] was normalized as follows: [formula]. For the MKL methods, we learned combinations of the following 20 kernels: 10 polynomial with degree from one to ten, ten Gaussian with bandwidth σ∈{0.5,1,2,5,7,10,12,15,17,20}, the same set of basic kernels as the one used in SimpleMKL [\cite=rakotomamonjy2008].

Note that SVM-FDA  can be only applied on the original feature space since it does not have a kernelized version. For SVMm, ε-SVM, MKLm, ε-MKL we set C2 to C1 / 3, as we can tolerate more bandwidth than margin violations. For SVMm  ε is set to three, i.e. the allowed band is three times wider than the margin. The optimal parameter C or C1 of the margin slack variables is chosen by an inner 10-fold cross-validation, from the set of [formula]. For SVM-FDA we choose C and λ from the same set of {0.1,1,10,100,1000} using the 10-fold inner cross-validation. We estimated the classification error using 10-fold cross validation. Folds are the same for all algorithms.

The results for the SVM algorithms are given in Table [\ref=tab:svm] and for the MKL ones in Table [\ref=tab:mkl]. We can see that SVMm has the lowest error in most of the cases. Among the 28 different experiments performed for each algorithm (7 datasets ×   4 kernels), SVMm has the lowest error 19 times, ε-SVM 10 times, SVM 5 times and FDA never. We should note that the second constraint of both ε-SVM ([\ref=eq:eSVMTotal]) and SVMm ([\ref=eq:eSVM_fixed_e]) will not be triggered if the data points are already very close to the margin; in such case SVMm and ε-SVM have the same performance as SVM. We also compared the statistical significance of the performance difference using a McNemar's test with the significance level set at 0.05. For each experiment of a given algorithm, i.e. kernel and dataset, the algorithm was credited with one point if it was significantly better than another algorithm for the same experiment, half a point if there was no significance difference between the two algorithms, and no point if it was significantly worse than the other algorithm. Under this ranking schema, in the original feature space, SVMm got a total score of 20.5 points over the seven datasets, out of a total possible maximum of 28, followed by SVM-FDA with 17, SVM with 15, ε-SVM with 14; FDA is by far the worst with only 3.5 points. In the polynomial kernel space of degree two and three, SVMm and ε-SVM rank first with 14.5 points but their advantage over SVM which got 13 points is not so pronounced; FDA got zero points. With the gaussian kernel, both SVMm and ε-SVM have a significant advantage over SVM and FDA; the former two got 14.5 points while SVM and FDA got only six and seven points respectively. Overall SVMm has a consistent advantage over the different datasets and the different kernels we experimented with. We should note here that the incorporation of the within-class distance in the optimization problem seems to bring the largest benefit when the kernel that is used is not that appropriate for the given problem, e.g. the Gaussian kernel for the datasets we experimented with here, this advantage is not so pronounced when the chosen kernel is good, e.g. polynomial of degree two and three. In other words incorporating the within-class distance seems to have a corrective effect when a mediocre or poor kernel is used.

The poor performance of FDA can be explained by the between-class distance measure it deploys. Using the class means as a between-class distance measure can lead to class overlapping. ε-SVM, SVMm and SVM-FDA do not have this problem since they use the margin as the between-class distance. Unlike SVM-FDA which can be applied only in the original feature space, since it cannot be kernelized, both ε-SVM and SVMm can be easily kernelized. In Figure [\ref=fig:time] we give the running times of the different methods. These are in agreement with our remarks in [\ref=subsec:measures]. ε-SVM is the slowest since in general the closer ε is to zero, the more constraints are active and the slower the algorithm will be. FDA is the fastest one but as we saw its predictive performance is quite poor.

In Table  [\ref=tab:mkl] we give the results for the MKL experiments. ε-MKL and MKLm  have now only a slight advantage compared to SimpleMKL. ε-MKL has the lowest error in six datasets while MKLm is the best in two datasets. In terms of the McNemar score, MKLm got 8.5 points, followed by ε-MKL with eight points and SimpleMKL with 7.5 points. Unlike standard SVM the incorporation of the within-class distance in the MKL cost function does not seem to deliver a significant performance improvement. This is somehow in agreement with our previous observation, i.e. that the incorporation of the within-class distance measure seems to have a strong positive effect when the kernel that is used is not appropriate. By learning the kernel as MKL does we overcome the problem of a possible poor kernel selection, provided that within the kernel set over which we learn there are appropriate kernels for the given problem.

From the results, it is apparent that one does not need only to control for the between-class distance, as standard SVM does, but also for the within-class distance, since the incorporation of some measure of the latter in the optimization problem considerably improves the performance over the standard SVM. When it comes to determining which within-class distance measure is more appropriate then the width of the band containing the instances that we deployed in SVMm has a clear advantage since it does not only lead to the best performance but it results in a convex optimization problem which is easy to kernelize and in addition includes as a special case ε-SVM.

Conclusion

Inspired by recent work that investigated the relations of SVM and metric learning [\cite=Huyen2011], we present here a novel framework that equips SVM, as well as MKL, with metric learning concepts. The new algorithms that we propose optimize not only the standard SVM margin, which can be seen as a measure of the between-class distance, but in addition they also optimize measures of the within-class distance. For the latter we propose a new measure, the width of the band along the margin hyperplane that contains the learning instances, and we derive new SVM and MKL variants that exploit. Our experimental results show that we achieve important predictive performance improvements if we include measures of the within-class distance in the optimization problem. In addition the new algorithms that we derived are convex and easy to kernelize.

There are a number of additional issues we plan to examine. The most challenging one is the derivation of generalization error bounds for the presented metric learning problems. To the best of our knowledge no such bounds exist for metric learning. Through the unified view of SVM and metric learning we want to relate the SVM error bound with the presented metric learning problems. It is also a challenge to determine which measures of between and within-class distances are the best for a specific problem. Additionally, and similar to [\cite=Bach2005] and [\cite=Efron2004], we want to solve for the full regularization path, for ε or λ.

Appendix

We here present a formal comparison of SVM and MKLγ  under the metric-learning view.

From a metric learning perspective SVM uses a single diagonal matrix transformation given by [formula] and MKLγ a two diagonal matrix transformation given by [formula]; both optimize the same cost function (i.e the margin). We will compare SVM and MKLγ by comparing the use of one and two diagonal transformation matrices.

For MKLγ the [formula] matrix is block diagonal, while if we use that decomposition with SVM it is a simple diagonal matrix. We denote by [formula], [formula] and [formula]. The optimization problems of SVM and MKLγ can be formulated as follows:

[formula]

[formula]

We now try to highlight relationship of these two optimization problems. Given some [formula] vector such that [formula], [formula], and [formula], we define a new norm [formula] as follows: [formula]. It is easy to prove that [formula] satisfies all the properties of a norm, which means it is a valid norm.

Using this new norm, we can rewrite problem ([\ref=eq:norm2]) as:

[formula]

Let [formula] be the feasible set of [formula], obviously [formula] is also the feasible set [formula]; [formula] is the feasible set of [formula]. Let [formula]. For a given value of [formula] there is a one-to-one mapping from [formula] to [formula]. The cardinality of the [formula] feasible set is much smaller than the cardinality of [formula] therefore using this new norm gives more flexibility in finding a solution of the optimization problem, which could potentially lead to a better solution.

In Figure [\ref=fig:norms] we illustrate the relations of the different norms just discussed in the two dimensional space. We compare the p=1 and p=2 norms of [formula] to its [formula] norm. We fix [formula], its feasible set [formula] is the diamond, [formula], its feasible set [formula] is the outer circle, and [formula], [formula], for which we have the set of feasible sets, [formula], that correspond to the different values of [formula] that satisfy [formula], which are given by the inner ellipsoids.