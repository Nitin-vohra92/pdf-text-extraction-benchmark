Sparse regularization for fiber ODF reconstruction: from the suboptimality of [formula] and [formula] priors to [formula]

Introduction

Fiber-tracking is probably one of the most fascinating applications in diffusion MRI (dMRI), gathering a lot of attention since its introduction because of its ability to reconstruct the main neuronal bundles of the brain from the acquired data. In fact, the random movement of the molecules in the white matter can be exploited for mapping brain connectivity, and structures otherwise invisible with other imaging modalities can be highlighted. The study of this structural connectivity is of major importance in a fundamental neuroscience perspective, for developing our understanding of the brain, but also in a clinical perspective, with particular applications for the study of a wide range of neurological disorders.

The most powerful acquisition modality is diffusion spectrum imaging (DSI) [\citep=Wedeen:2005aa]. It relies on cartesian signal sampling and is known to provide good imaging quality, but it is too time-consuming to be of real interest in a clinical perspective. Diffusion tensor imaging (DTI) [\citep=Basser:1994aa] is always preferred instead. DTI is a very fast model-based technique providing valuable diagnostic information but, on the contrary, it is unable to model multiple fiber populations in a voxel. In a global connectivity analysis perspective, this constitutes a key limiting factor. Accelerated acquisitions relying on a smaller number of samples while providing accurate estimations of the intra-voxel fiber configuration thus represent an important challenge.

Recently, an increasing number of high angular resolution diffusion imaging (HARDI) approaches have been proposed for tackling this problem. In particular, spherical deconvolution (SD) based methods formed a very active area in this field [\citep=Tournier:2004aa] [\citep=Alexander:2005aa] [\citep=Tournier:2007aa] [\citep=Dellacqua:2007aa]. These methods rely on the assumption that the signal attenuation acquired with diffusion MRI can be expressed as the convolution of a given response function with the fiber orientation distribution (FOD). The FOD is a real-valued function on the unit sphere [formula] giving the orientation and the volume fraction of the fiber populations present in a voxel. The response function, or kernel, describes the dMRI signal attenuation generated by an isolated single fiber population; it can be estimated from the data or represented by means of parametric functions. SD approaches represented a big step in reducing the acquisition time of diffusion MRI, but are known to suffer heavily from noise and intrinsic instabilities in solving the deconvolution problem. For this reason, a regularization scheme is normally employed. A variety of approaches have been proposed, which are generally based on the two assumptions that the FOD is (i) a non-negative function and (ii) sparse, i.e. with only a few nonzero values, either explicitly or implicitly. In fact, at the imaging resolution available nowadays, diffusion MRI is sensitive only to the major fiber bundles and it is commonly accepted that it can reliably disentangle up to 2-3 different fiber populations inside a voxel [\citep=Jeurissen:2010aa] [\citep=Schultz:2012aa]. Hence, the FOD can reasonably be considered sparse in nature. In particular, the state-of-the-art Constrained Spherical Deconvolution (CSD) approach of [\citet=Tournier:2007aa] resorts to Tikhonov regularization, based on an [formula]-norm prior. While its primary purpose is to ensure the positivity of the FOD, it actually also implicitly promotes sparsity, but only a weak version of it.

The recent advent of compressed sensing (CS) theory [\citep=Donoho:2006aa] [\citep=Candes:2006aa] [\citep=Baraniuk:2007aa] provided a mathematical framework for the reconstruction of sparse signals from under-sampled measurements mainly in the context of convex optimization. CS has inspired new advanced approaches in the last few years for solving the reconstruction problem in diffusion MRI and allowed a further dramatic reduction in the number of samples needed to accurately infer the fiber structure in each voxel, by promoting sparsity explicitly. For instance, [\citet=Tristan-Vega:2011aa] and [\citet=Michailovich:2011aa] recovered the orientation distribution function (ODF) by using different representations for the response function, while [\citet=Merlet:2011aa] and [\citet=Rathi:2011aa] focused on the full ensemble average propagator (EAP) of the diffusion process. In this work, however, we focus on spherical deconvolution based-methods and the quantity of interest is the FOD. In general these methods are based on [formula] minimization, where the [formula] norm is defined as [formula] for any vector [formula], and the common goal is to recover the FOD with fewest non-zeros that is compatible with the acquired dMRI data [\citep=Ramirez-Manzanares:2007aa] [\citep=Pu:2011aa] [\citep=Landman:2012aa] [\citep=Mani:2012aa]. However, a minimum [formula]-norm prior is inconsistent with the physical constraint that the sum of the volume fractions of the compartments inside a voxel is intrinsically equal to unity. In this paper, we propose to exploit the versatility of compressed sensing and convex optimization to solve what we understand as the [formula] inconsistency, while simultaneously exploiting sparsity more optimally than the approaches based on the [formula] prior, and improve the quality of FOD reconstruction in the white matter. Our approach is as follows. Strictly speaking, the FOD sparsity is the number of fiber populations, thus identified by the [formula] norm of the FOD. [formula]-norm problems are generally intractable as they are non convex, which explains the usual convex [formula]-norm relaxation in the framework of compressed sensing. To this end, some greedy algorithms have been proposed to approximate the [formula] norm through a sequence of incremental approximations of the solution, such as Matching Pursuit [\citep=Mallat:1993aa] and Orthogonal Matching Pursuit [\citep=Pati:1993aa]. However, the greedy and local nature of these algorithms, i.e. in the sense that compartments are identified sequentially, makes them suboptimal as compared to more robust approaches based on convex optimization, which are global in nature. In particular, a reweighted [formula] minimization scheme was developed by [\citet=Candes:2008aa] in order to approach [formula] minimization by a sequence of convex weighted-[formula] problems. We thus solve the [formula] minimization problem by making use of a reweighting scheme and evaluate the effectiveness of the proposed formulation in comparison with state-of-the-art aprroaches based on either [formula] or [formula] priors. We report results on both synthetic and real data.

Materials and methods

Intra-voxel structure recovery via spherical deconvolution

As shown by [\citet=Jian:2007aa], spherical deconvolution methods can be cast into the following computational framework:

[formula]

where f is the FOD to be estimated, [formula] the response function rotated in direction [formula] and the integration is performed over the unit sphere with [formula] and dΩ  =   sin φ  dφ  dθ. [formula] represents the dMRI signal measured on the q-space shell acquired with b-value b in direction [formula], while S0 is the signal acquired without diffusion weighting. The FOD f is normally expressed as a linear combination of basis functions, e.g. spherical harmonics, as [formula]. The measurement process can thus be expressed in terms of the general formulation:

[formula]

where [formula] are the coefficients of the FOD, [formula] is the vector with the dMRI signal measured in the voxel with [formula] for [formula], η represents the acquisition noise and [formula] is the observation matrix modeling explicitly the convolution operator with the response function R, with [formula]. Several choices for the convolution kernels and basis functions exist in the literature; more details will be provided on the specific Φ used with each algorithm considered in this work.

prior

In the original formulation of [\citet=Tournier:2004aa], the FOD [formula] and the measurements [formula] were expressed by means of spherical harmonics (SH), and the deconvolution problem was solved by a simple matrix inversion. To reduce noise artifacts, a low-pass filter was applied for attenuating the high harmonic frequencies. The method was improved in [\citet=Tournier:2007aa] by reformulating the problem as an iterative procedure where, at each iteration, the current solution [formula] is used to drive to zero the negative amplitudes of the FOD at the next iteration with a Tikhonov regularization [\citep=Tikhonov:1977aa]:

[formula]

where ||  ·  ||p are the usual [formula] norms in [formula], the free parameter λ controls the degree of regularization and L(t) can be understood as a simple binary mask preserving only the directions of negative or small values of x(t). The [formula]-norm regularization term therefore tends to send these values to zero, as probably spurious, hence favoring large positive values. Interestingly, beyond the claimed purpose of enforcing positivity , the operator L thus also implicitly promotes a weak version of sparsity. However, this [formula] prior does not explicitly guarantee either positivity or sparsity in the recovered FOD. In [\citet=Alexander:2005aa] a maximum-entropy regularization was proposed to recover the FOD as the function that exhibits the minimum information content. The method showed higher robustness to noise than previous approaches, but was limited by the very high computational cost and did not promote sparsity. Other regularization schemes have been proposed in the literature, but FOD sparsity has never been addressed with a rigorous mathematical formulation.

prior

Compressed sensing provides a powerful mathematical framework for the reconstruction of sparse signals from a low number of data [\citep=Donoho:2006aa] [\citep=Candes:2006aa], mainly in the context of convex optimization. According to this theory, it is possible to recover a signal from fewer samples than the number required by the Nyquist sampling theorem, provided that the signal is sparse in some sparsity basis Ψ. Let [formula] be the signal to be recovered from the m  ≪  n linear measurements [formula] and [formula] a sparse representation of [formula] through [formula]. If the observations [formula] are corrupted by noise and Φ obeys some randomness and incoherence conditions, then the signal [formula] can be recovered by solving the convex [formula] optimization problem:

[formula]

where ε is a bound on the noise level. Assuming Gaussian noise, the square [formula] norm of the residual represents the log-likelihood of the data and follows a χ2 distribution. For a sufficiently large number of measurements, this distribution is extremely concentrated around its mean value. This fact is related to the well-known phenomenon of concentration of measure in statistics. Consequently, ε can be precisely defined by the mean of the χ2.

In the context of FOD reconstructions, the sparsity basis Ψ boils down to the identity matrix, thus [formula]. In [\citet=Ramirez-Manzanares:2007aa] and [\citet=Jian:2007aa] the sensing basis Φ, also called dictionary, is generated by applying a set of rotations to a given Gaussian kernel (i.e. diffusion tensor) and the sparsest coefficients [formula] of this linear combination best matching the measurements [formula] are recovered by solving the following constrained minimization problem:

[formula]

where the positivity constraint on the FOD values was directly embedded in the formulation of the convex problem. For [formula] the noise in the magnitude dMRI images can be assumed Gaussian-distributed [\citep=Gudbjartsson:1995aa]. However, a statistical estimation of ε is not reliable, precisely because the number of measurements is very small and the χ2 is not really concentrated around its mean value. Thus, ε becomes an arbitrary parameter of the algorithm. At very low SNR, one can also extrapolate the choice of the [formula] norm as a simple penalization term independent of statistical considerations.

The reconstruction problem can also be re-formulated as a regularized (as opposed to constrained) [formula] minimization as in [\citet=Landman:2012aa] and [\citet=Pu:2011aa]:

[formula]

where the free parameter β controls the trade-off between the data and the sparsity constraints. In general, β depends on the acquisition scheme and the noise level and it must be empirically optimized. Following the general CS approach, problems [\eqref=eqn:BPDNProblem] and [\eqref=eqn:1-normProblemRegularized] consider an [formula]-norm prior on the FOD [formula]. However, in the dMRI context, a minimum [formula]-norm prior is inconsistent with the physical constraint that the sum of the volume fractions of the compartments inside a voxel is intrinsically equal to unity, i.e. [formula]. For this reason, we reckon that also these [formula]-based formulations are intrinsically suboptimal. Fig. [\ref=fig:SumX] illustrates this inconsistency by reporting the [formula] norm of reconstructed FODs as a function of the amplitude of measurement noise.

Our main goal in this work is to demonstrate the suboptimalities of the approaches based on [formula] and [formula] priors and to suggest a new formulation, based on an [formula] prior, adequately characterizing the actual sparsity lying in the FOD.

prior

In the aim of adequately characterizing the FOD sparsity, we re-formulate the reconstruction problem as a constrained [formula] minimization problem:

[formula]

where ||  ·  ||0 explicitly counts the number of nonzero coefficients and k represents an upper bound on the expected number of fiber populations in a voxel.

As already stated, the [formula] problems as such are intractable. The reweighting scheme proposed by [\citet=Candes:2008aa] proceeds by sequentially solving weighted [formula] problems of the form [\eqref=eqn:0-normProblem], where the [formula] norm is substituted by a weighted [formula] norm defined as [formula], for positive weights [formula] and where i indexes vector components. At each iteration, the weights are set as the inverse of the values of the solution of the previous problem, i.e. [formula]. At convergence, this set of weights makes the weighted [formula] norm independent of the precise value of the nonzero components, thus mimicking the [formula] norm while preserving the tractability of the problem with convex optimization tools. Of course, it is not possible to have infinite weights for null coefficients; so a stability parameter τ must be added to the coefficients in the selection of the weights.

The main steps of the reweighted scheme are reported in the algorithm [\ref=alg:ReweightedL1]; in the remaining of the manuscript we will refer to it as L2L0, as it is based on a [formula] prior. We empirically set τ  =  10- 3 and the procedure was stopped if [formula] between two successive iterations or after 20 iterations. At the first iteration the weighted [formula] norm is the standard [formula] norm given [formula], and therefore the constraint [formula] is a weak bound on the sum of the fiber compartments and does not constitute a limitation in the procedure.

The proposed [formula] approach thus strongly promotes sparsity (by opposition with the [formula] approach) and circumvents the [formula] inconsistency. It is noteworthy that our formulation at least partially addresses the problem of arbitrary parameters such as ε in [\eqref=eqn:BPDNProblem] and β in [\eqref=eqn:1-normProblemRegularized], or λ in [\eqref=eqn:SD-Problem]. Our parameter k indeed explicitly identifies an upper bound on the number of fibers. As discussed before and largely assumed in the literature, we can expect to have at maximum 2-3 fiber compartments in each voxel. The algorithm was found to be quite robust to the choice of k, and differences were not observed for values up to k  =  5.

Finally, an explicit constraint [formula] might have been added, as it represents the physical property that the volume fractions must sum up to unity. For the sake of simplicity, in this work this constraint was not included (as it is always the case), assuming it is carried over by the data and well-designed bases as pointed out by [\citet=Ramirez-Manzanares:2007aa]. In sections [\ref=VolumeFractionsAndL1] and [\ref=RealDataVolumeFractionsAndL1] we will provide evidence that actually this physical constraint is not met when using [formula] or [formula] priors, whereas it is correctly satisfied with our proposed [formula] formulation. This might have severe consequences on the reconstruction quality.

Comparison framework

We compared our [formula] approach based on problem [\eqref=eqn:0-normProblem] against state-of-the-art [formula] and [formula] approaches respectively based on problems [\eqref=eqn:SD-Problem] and [\eqref=eqn:1-normProblemRegularized], and referred to as L2L2 and L2L1. To run L2L2 reconstructions we made use of the original mrtrix implementation of [\cite=Tournier:2012aa], setting the optimal parameters as suggested by the software itself. To solve the L2L1 and L2L0 problems we used the SPArse Modeling Software (SPAMS), an open-source toolbox written in C++ for solving various sparse recovery problems. SPAMS contains a very fast implementation of the LARS algorithm [\citep=Efron:2004aa] for solving the LASSO problem and its variants as the L2L1 problem in equation [\eqref=eqn:1-normProblemRegularized] and the weighted [formula] minimizations required for our L2L0 approach in equation [\eqref=eqn:0-normProblem]. Numerical simulations on synthetic data were performed to quantitatively assess the performance of L2L2, L2L1 and L2L0 under controlled conditions. The effectiveness of the three priors was also assessed in case of real human brain data.

Numerical simulations

Independent voxels with two fiber populations crossing at specific angles ([formula] range) and with equal volume fractions were synthetically generated. The signal S corresponding to each voxel configuration was simulated by using the exact expression given in [\citet=Soderman:1995aa] for the dMRI signal attenuation from particles diffusing in a restricted cylindrical geometry of radius ρ and length L with free diffusion coefficient D0. The following parameters were used [\citep=Ozarslan:2006aa] [\citep=Jian:2007aa]: [formula], [formula], [formula], [formula], [formula]. The signal S was contaminated with Rician noise [\citep=Gudbjartsson:1995aa] as follows:

[formula]

where ξ1,ξ2  ~  N(0,σ2) and [formula] corresponds to a given signal-to-noise ratio on the S0 image. We assumed S0  =  1 without loss of generality. Because of this assumption, we have implicitly considered a constant echo-time for acquisitions with different b-values, thus ignoring the fact that higher b-values normally require longer echo-times and therefore the images have a lower signal-to-noise ratio. The study of the impact of the echo-time on different regularization priors is beyond the scope of our investigation.

For each voxel configuration, the signal was simulated at different b-values, [formula], and seven q-space sampling schemes were tested, respectively with 6, 10, 15, 20, 25, 30 and 50 samples equally distributed on half the unit sphere using electrostatic repulsion [\citep=Jones:1999aa] assuming antipodal symmetry in diffusion signal. Six different noise levels were considered, [formula]. For every SNR, 100 repetitions of the same voxel were generated using different realizations of the noise. In our experiments, the actual signal-to-noise ratio in the simulated signal was always in a range where the Gaussian assumption on the noise holds. In the extreme setting with a   =  5 on the S0 and [formula] the actual signal-to-noise ratio in the diffusion weighted signal was about 1.4.

Evaluation criteria

As one of the aims of this work is to improve SD reconstructions, we adopted standard metrics widely used in the literature [\citep=Ramirez-Manzanares:2008aa] [\citep=Landman:2012aa] [\citep=Michailovich:2011aa] to assess the quality of the reconstructions with respect to number and orientation of the fiber populations:

Probability of false fiber detection. This metric quantifies the correct assessment of the real number M of populations inside a voxel:

[formula]

where [formula] is the estimated number of compartments. As [formula] does not distinguish between missed fibers and extra compartments found by the reconstruction, we also make use of the following two quantities where needed, [formula] and [formula], explicitly counting the number of under- and over-estimated compartments, respectively.

Angular error. This metric quantifies the angular accuracy in the estimation of the directions of the fiber populations in a voxel:

[formula]

where [formula] is a true direction and [formula] is its closest estimate. The final value is an average over all fiber compartments by first matching the estimated directions to the ground-truth without using twice the same direction.

Peaks detection was performed using a local maxima search algorithm on the recovered FOD, considering a neighborhood of orientations within a cone of [formula] around every direction. For this reason, evaluation metrics are not sensitive for small crossing angles and results are reported in a conservative range [formula]. To filter out spurious peaks, values smaller than 10% of the largest peak were discarded; in the case of L2L2 we had to increase this threshold to 20%, as suggested in [\citet=Tournier:2007aa], in order to compare with the other methods.

Real data

The human brain data have been acquired from 3 young healthy volunteers on a 3T Magnetom Trio system (Siemens, Germany) equipped with a 32-channel head coil using standard protocols routinely used in clinical practice. Each dataset corresponds to a distinct subject. Two DTI scans (referred in the following as [formula] and [formula]) were acquired at [formula] using 30 and 20 diffusion gradient directions, respectively, uniformly distributed on half the unit sphere using electrostatic repulsion [\citep=Jones:1999aa]. Other acquisition parameters were as follows: [formula] and spatial resolution = [formula] for dataset [formula], while [formula] and spatial resolution = [formula] for dataset [formula]. One HARDI dataset (referred as [formula]) was acquired at [formula] using 256 directions uniformly distributed on half the unit sphere [\citep=Jones:1999aa], [formula] and spatial resolution = [formula]. To study the robustness of the three algorithms to different under-sampling rates, the [formula] dataset has been retrospectively under-sampled and two additional datasets ([formula] and [formula]) have been created, consisting of only 50 and 20 diffusion directions, respectively. These subsets of directions were randomly selected in order to be as much equally distributed on half the unit sphere as possible. The actual SNR in the b  =  0 images, computed as the ratio of the mean value in a region-of-interest placed in the white matter and the standard deviation of the noise estimated in the background, was about 60 in [formula], 30 in [formula] and 30 in [formula].

Implementation details

In all our experiments, the response function was estimated from the data following the procedure described in [\citet=Tournier:2007aa]. A different response function was estimated for every combination of experimental conditions (number of samples, b-value, SNR), which was then used consistently in the three reconstruction methods. Specifically, the 300 voxels with the highest fractional anisotropy were selected as expected to contain only one fiber population, and a tensor was fitted from the dMRI signal in each. In the case of numerical simulations, an additional set of data containing 300 voxels with a single fiber compartment was generated for this scope. The estimated coefficients were then averaged to provide a robust estimation of the signal profile for the response function. As we used the tool estimate_response of mrtrix for these operations, the estimated kernel was already suitable to be fed into the L2L2 algorithm. Note that the fiber directions rely on a maxima identification from the SH coefficients, which can take any continuous position on the sphere. Conversely, in the case of both L2L1 and L2L0, the estimated kernel was used to create the dictionary Φ by rotating it along 200 orientations uniformly distributed on half the unit sphere. Because of this discretization, the resulting grid resolution is about [formula] and thus the intrinsic average error when measuring the angular accuracy is about [formula]. In other words, the precision of both L2L1 and L2L0 is limited by the resolution of the grid used to construct the dictionary. For this reason differences between methods below this threshold will be considered not significant. Note that, to improve the precision it would be sufficient to increase the number of directions of the discretization which, however, would have serious consequences on the efficiency and stability of the minimization algorithm. Interestingly, recent works of [\citet=Tang:2012aa] and [\citet=Candes:2012aa] explored a novel theory of CS with continuous dictionaries, in the context of which FOD peaks could be thought to be located with infinite precision. This topic will be the subject of future research. Finally, in order to model adequately any partial/full contamination with cerebrospinal fluid (CSF) that may occur in real data, an additional isotropic compartment has been considered by adding a column to Φ. This compartment was estimated by fitting an isotropic tensor in voxels within the lateral ventricles. The free parameter controlling the degree of regularization had to be estimated for both L2L2 and L2L1 algorithms. For the former we used the default values suggested in the original implementation available in the mrtrix software. For the latter, the regularization parameter β was empirically estimated following the guidelines of [\citet=Landman:2012aa], in order to place the method in its best conditions. In numerical simulations, we created an additional training dataset for every combination of experimental conditions (number of samples, b-value, SNR) and 50 reconstructions were performed varying the parameter β from 10- 4  β* to β*, with [formula] computed independently in each voxel. The value providing the best reconstructions (according to the above metrics) was then used to run L2L1 on the actual data used for the final comparison. We did not observe any improvement in the reconstructions outside this range. In real data, we tested different values for β but, as the ground-truth is unknown, the optimal value was chosen on the basis of a qualitative inspection of the reconstructions considering their shape, spatial coherence and adherence to anatomy. Nonetheless, we found that the algorithm was quite robust to the choice of β and the value providing visually the best results was always very close to β  =  0.1  ·  β*, as suggested in the same work. Therefore this value was used in all real data experiments. This stability might be probably due to the adaptive strategy of estimating β* in each voxel from the signal [formula]. As already emphasized, L2L0 does not require any free parameter to be tuned. In fact, in numerical simulations k can be fixed in all iterations to 3 while we can safely assume k = 5 in real data, hence larger than the 2-3 fibers normally assumed.

Results and discussion

Numerical simulations

We quantitatively compared the three approaches on synthetic data with the aim of assessing the impact on the reconstructions of each regularization scheme (i.e. [formula], [formula] and [formula] priors) under controlled conditions. In particular, the quality of the reconstructions was evaluated using the metrics introduced above and selectively varying (i) the number of samples and (ii) the b-value of the acquisition scheme, (iii) the noise level and (iv) the crossing angle between the fiber compartments. Results are reported independently for each experimental condition.

Volume fractions and

As previously stated, the physical constraint that the volume fractions sum to unity is normally omitted in every problem formulation, as it is expected to be carried over by the data and properly designed bases [\citep=Ramirez-Manzanares:2007aa]. In Fig. [\ref=fig:SumX] we explicitly tested whether this property is actually satisfied by the algorithms considered in this work. A more detailed analysis of the performance of each prior is performed in the following sections.

The figure reports the average value for the sum of the volume fractions of the reconstructed FODs (i.e. [formula]), as a function of the noise level, for two acquisition schemes with 30 samples at [formula] and [formula], respectively. The impact on the reconstructions is shown by means of the normalized mean-squared error [formula] [\citep=Michailovich:2011aa] between the measured signal [formula] and its estimate [formula]. The image clearly demonstrates that both L2L2 and L2L1 reconstructions do not fulfill the [formula] physical constraint, as the sum of the recovered volume fractions always tends to be over-estimated by L2L2 and under-estimated by L2L1. This is a clear effect of the weakness of the sparsity constraint in the L2L2 approach and of the inconsistency of the [formula] prior in L2L1. On the contrary our L2L0 approach appears to correctly satisfy the constraint, with deviations from unity only with very high noise levels (SNR ≈   5). With high quality data this over/under-estimation behavior is fairly mild (at SNR = 30, [formula] for L2L1 and [formula] for L2L2), but it progressively intensifies as the noise level increases. The trend is even amplified with high b-value data, in which case the [formula] can be as high as ≈  2.1 for L2L2 and as low as ≈  0.25 for L2L1.

Despite showing quite different behaviors with respect to the [formula], L2L2 and L2L0 exhibit very similar NMSE values. On the contrary, L2L1 shows significantly higher reconstruction errors than both L2L2 and L2L0, pointing to the aforementioned [formula] inconsistency. Debiasing methods [\citep=Zou:2006aa] have been proposed with the aim to correct the magnitude of the recovered coefficients and mitigate this effect. Nonetheless, a critical step for applying these techniques consists in the proper identification of the support of the solution, otherwise this procedure can lead to really bad results. As we will show in the next sections, this is the case in this work, as the three methods differ significantly in their ability to estimate the number of fiber populations. As the very same data and reconstruction basis have been used for all the methods, we can conclude that any deviation from the unit sum has to be attributed to the different regularization employed in each algorithm. In the following we will investigate the consequences on the reconstructions of using different regularization schemes.

Comparison as a function of the number of samples

Fig. [\ref=fig:performances-vs-nS] reports the performance of the three reconstruction methods as the number of samples changes. We considered seven acquisition schemes from 6 to 50 samples and results are reported for a standard scenario, specifically a shell at [formula] with a [formula]. The dependence on the b-value and the robustness to noise will be investigated in detail in the following sections. The quality metrics are reported here as the average value computed over all simulated crossing angles ([formula]).

Looking at the plots the benefits of using an [formula] prior are clear: L2L0 always outperforms both L2L2 and L2L1 in identifying the correct number of fiber populations (Pd) and results are consistent for all number of samples considered. The main benefit of L2L0 seems to be the drastically decreased number of missed fibers (smaller n-), even though also the number of over-estimated compartments (n+) is significantly reduced. Concerning the angular accuracy of the recovered fiber populations (εθ), reconstructions with L2L0 always resulted in smaller errors as compared to both L2L2 and L2L1. Although the difference with respect to L2L1 is not significant as always within the intrinsic grid precision, both methods showed a substantial improvement over L2L2, which appeared to suffer from a sudden and significant deterioration of the reconstructions ([formula]) for less than 30 samples. This can be explained with the SH representation used internally by L2L2. In fact, even though the FOD is a function on the sphere containing high-resolution features by definition, a maximum SH order lmax = 4 (or less) can be used for acquisitions with less than 30 samples, hence drastically reducing the intrinsic angular resolution of the recovered FOD. At least 30 to 60 samples are normally advised for using L2L2, so in our experiments we have actually tested L2L2 beyond its applicability range. On the contrary, L2L1 and L2L0 do not make use of SH and the reconstruction quality degrades more smoothly with the under-sampling rate of the dMRI data. In the following we will focus on two acquisition schemes to further analyze the performance of three methods: (i) in a normal setting with 30 samples and (ii) in a regime of high under-sampling with only 15 samples.

Comparison as a function of the crossing angle

In Fig. [\ref=fig:performances-vs-CA] the performances of L2L2, L2L1 and L2L0 are plotted in detail as a function of the crossing angle between the fiber populations. Results are shown for two acquisitions with 30 and 15 samples, both simulated at [formula] and [formula].

With 30 samples, the major source of errors for both L2L2 and L2L1 is represented by under-estimation (n-), although spurious orientations are not negligible (n+  ≈  0.2). In particular, both methods start to severely miss fibers for crossing angles below [formula], where they tend to recover a single peak lying between the two real fiber directions. In these situations, the maximum angular error for the sole estimated peak is generally upper bounded by half the angle separating the two fibers; for this reason the overall εθ performances of L2L2 and L2L1 do not differ significantly from L2L0 despite the drastic improvement in terms of Pd, n- and n+. On the other hand, in an under-sampling scenario with 15 samples L2L2 and L2L1 exhibit much higher Pd values and a stronger tendency to over-estimate compartments, usually in completely arbitrary orientations not even close to the true fiber directions. The overall improvement in the angular accuracy of L2L0 is more evident, with an average enhancement up to [formula] with respect to L2L1, whereas L2L2 exhibits a severe drop of the performance mainly due to modeling limitations, as previously pointed out.

These differences can have dramatic consequences for fiber-tracking applications. In fact, tractography algorithms are particularly prone to these estimation inaccuracies, i.e. number and orientation of fiber populations, because the propagation of these (perhaps locally small) errors can lead to completely wrong final trajectories. For instance, a missed compartment might stop prematurely a trajectory, while a spurious peak might lead to create an anatomically incorrect fiber tract. Hence, the ability to accurately recover the intra-voxel fiber geometry is of utmost importance.

Comparison as a function of the b-value

So far L2L2, L2L1 and L2L0 have been compared for given acquisition schemes at a fixed [formula]. Fig. [\ref=fig:performances-vs-b] reports the quality of the reconstructions with the three approaches as a function of the b-value. The results are shown for 30 and 15 samples with a [formula].

L2L2 tends to miss compartments for low b-values and over-estimate them at higher b (n+ and n- are not shown here for brevity). This is even more apparent when decreasing the number of samples in the acquisition to 15, where L2L2 estimates a lot of spurious peaks at high b-values (high n+) and thus the angular accuracy of the estimated fiber directions drops considerably. Interestingly, L2L1 shows the opposite behavior, under-estimating at high b and over-estimating at low b, although at a smaller rate thus preventing the performance to degrade significantly. Again, in comparison, L2L0 shows a very stable estimation of the number of fibers. Concerning the angular accuracy, all methods showed a minimum for εθ corresponding to [formula], representing a sort of trade-off between the loss in angular resolution happening at small b-values and the stronger noise influence at higher b. In fact, as in this work we report the noise level as the SNR of the S0 dataset, images at high b-values will have lower actual signal-to-noise ratio, and thus the noise effects will be inherently stronger. Overall, L2L0 always results in smaller angular errors than the other two methods. The improvement with respect to L2L1 is not significant, while the difference with L2L2 is much more pronounced (up to [formula]) especially as the b-value increases.

Comparison as a function of the SNR

Finally, Fig. [\ref=fig:performances-vs-SNR] compares the robustness to noise of the three methods. Six noise levels have been considered, with the SNR of the S0 dataset varying from 5 to 30. The comparison is reported for 30 and 15 samples at [formula]. The results show that L2L0 clearly outclasses the other two methods concerning the estimation of the number of compartments (Pd) and results are consistent as the SNR changes, both with 30 and 15 samples. In terms of angular accuracy, L2L0 and L2L1 have very similar εθ performances, almost indistinguishable from one another. On the contrary, L2L2 systematically obtains significantly higher εθ values at all considered SNRs (up to [formula] with 30 samples). In a high under-sampling regime (right plots), the angular accuracy drastically degrades in the case of L2L2 and it appears almost independent of the noise level. This is again consistent with the limitations of the SH representation for acquisitions with very few samples.

Real data

Qualitative evaluation on DTI data

Fig. [\ref=fig:RealData] compares the reconstructions obtained with the three regularization schemes in the case of real data acquired with a typical DTI protocol. Subplots A, B and C correspond to the [formula] dataset acquired using 30 samples. Even though the acquisition scheme used for this dataset is not the setting where our numerical simulations highlighted the most substantial differences between the three methods, important conclusions can be drawn in favor of L2L0. Looking at the regions in the white circles, the ability of both L2L1 and L2L0 to properly model the isotropic compartment in voxels with full or partial contamination with CSF is clearly visible. On the contrary, as L2L2 does not explicitly model any CSF compartment, it appears unable to adequately characterize the signal in these cases, but it rather approximates any isotropic contribution with a set of random and incoherent fiber compartments. Besides, comparing B and C we can observe that L2L0 successfully differentiates gray matter (light gray regions) from CSF voxels with pure isotropic and fast diffusion (very bright areas), whereas L2L1 appears unable to distinguish them.

The yellow frames highlight the corona radiata, a well-known region in the white matter containing crossing fibers. As expected from our simulations at this still relatively high number of samples, differences are not obvious between the three methods. However, we observe that L2L0 clearly results in sharper and more defined profiles than L2L1, whereas the improvements with respect to L2L2 are confined only to few voxels. The not so good performance of L2L1 might be related to the value chosen for β. In contrast, no free parameter has to be empirically optimized in our approach. When decreasing the acquisition samples to 20 (subplots D, E and F corresponding to [formula] dataset), fiber directions are definitely much better resolved with L2L0 than with both L2L2 and L2L1. In fact L2L2 clearly breaks, missing many fiber compartments probably due to the aforementioned limitations of the SH representation. The same happens to L2L1, whose reconstructions appear very blurred and noisy.

Qualitative evaluation on HARDI data

The comparison with high b-value data is reported in Fig. [\ref=fig:RealDataWithHighBValue]. The figure shows also the robustness to different under-sampling rates of each scheme. Subplots A, B and C correspond to the fully-sampled dataset [formula]. In this situation, no evident differences between the three approaches can be observed as they perform essentially the same. With moderate under-sampled data (subplots D, E and F corresponding to [formula]) both L2L2 and L2L0 do not show any significant difference in the quality of the reconstructions, so far exposing neat and sharp profiles. On the other hand, the FODs reconstructed by L2L1 show some signs of progressive degradation, appearing a little more blurred as compared to those reconstructed from fully-sampled data (compare subplots E and B). The situation changes drastically with highly under-sampled data, as easily noticeable by comparing the subplots G, H and I, which correspond to the reconstructions performed with only 8% of the original data. In fact, while L2L0 does not show yet any significant degradation of the FODs, both L2L2 and L2L1 clearly do not provide as sharp and accurate reconstructions as in the case of fully-sampled data (compare G to A and H to B). In addition, in the case of L2L2 we can observe a higher incidence of negative peaks (identified in the plots by small yellow spikes), a clear sign of augmented modeling errors.

Quantitative comparison: volume fractions and norm

In Fig. [\ref=fig:RealDataSumx] we tested whether the physical constraint of unit sum is satisfied also in case of real data. The images confirm the observations previously made with synthetic data (cf. Fig [\ref=fig:SumX]). In fact, the sum of the recovered volume fractions tends to be over-estimated by L2L2 (subplots A and B) and under-estimated by L2L1 (subplots C and D), whereas L2L0 reconstructions (subplots E and F) appear to meet the property of unit sum as expected. All methods coherently show a mild over-estimation in the corpus callosum, compatible with the highly-packed axonal structure in this region. Finally, L2L2 seems to suffer from over-estimation more with fully- than with under-sampled data, which might be related to the SH order employed for different number of samples.

Quantitative comparison: fully- vs under-sampled data

We compared the reconstructions obtained from under-sampled data (i.e. [formula] and [formula]) to those with fully-sampled data (i.e. [formula]), considering this latter as ground-truth, as done by [\citet=Yeh:2013aa]. In agreement with the results from numerical simulations, no significant difference was found between the three approaches in terms of angular accuracy. The average error using 50 samples was [formula] (mean ±   standard deviation) for L2L2, [formula] for L2L1 and [formula] for L2L0. The reconstructions using 20 samples clearly showed higher angular errors. The differences between L2L1 and L2L0 were below the resolution of the sphere discretization used in this study: [formula] and [formula] respectively. L2L2 revealed significantly higher εθ values: [formula]. On the other hand, results definitely confirmed the superior performance of L2L0 in terms of Pd that was previously observed in synthetic experiments. With 50 samples L2L0 had an average Pd  =  4.0%  ±  13.9% as opposed to sensibly higher values for L2L2 and L2L1, respectively 17.8%  ±  32.6% and 17.3%  ±  24.3%. For 20 samples, the performance of L2L2 and L2L1 visibly deteriorated, 42.1%  ±  43.6% for the former and 21.3%  ±  27.7% for the latter. L2L0 reconstructions appeared very stable with an average Pd  =  5.5%  ±  15.8%. These enlightening results are illustrated in Fig. [\ref=fig:RealDataQuantitative].

Limitations and future work

Our proposed formulation represents an extension of classical spherical deconvolution and sparse reconstruction methods [\citep=Tournier:2007aa] [\citep=Landman:2012aa] and, as such, it also inherits all the intrinsic limitations and shortcomings of this class of techniques. Like all its predecessors, in fact, our method is based on the assumption that the response function can be estimated from the data and especially that it is adequate for characterizing the diffusion process in all the voxels of the brain. Moreover, the validity of these approaches has yet to be properly assessed with more critical intra-voxel configurations [\citep=Sotiropoulos:2012aa] or pathological brain conditions. Yet, as these methods are currently widely used in this field, we have shown in this work that by expressing adequately the regularization prior used for promoting sparsity the quality of the reconstructions can significantly be improved, with no additional cost.

Some of the aforementioned limitations might be addressed by enhancing the estimation of the dictionary accounting for more complex configurations, such as using different response functions for different brain regions and/or pathological tissues and including specific kernels which explicitly model fiber fanning/bending. In addition, even though we focused here on single voxel experiments, future work will be devoted to study the applicability and the effectiveness of our approach in more sophisticated frameworks exploiting the spatial coherence of the data. Finally, future research will investigate the use of the recently proposed continuous CS theory [\citep=Tang:2012aa] [\citep=Candes:2012aa] with the aim of further improving the accuracy of the reconstructions and reducing the acquisition time.

Conclusion

In this paper we focused on spherical deconvolution methods currently used in diffusion MRI for recovering the FOD and estimating the intra-voxel configuration in white matter. In particular, we investigated the effectiveness of state-of-the-art regularization schemes based on [formula] and [formula] priors and provided evidence that these formulations are intrinsically suboptimal: the former because it does not explicitly promote sparsity in the FOD, the latter because it is inconsistent with the fact that the fiber compartments must sum up to unity. We proposed a formulation that rather places a strict bound on the number of expected fibers in the voxel through a bound on the [formula] norm of the FOD, relying on a reweighted [formula] scheme. We compared our L2L0 approach with the state-of-the-art L2L2 and L2L1 methods, both on synthetic and real human brain data. Results showed that our proposed formulation significantly improves single-voxel FOD reconstructions, with no additional overheads. This evolution is most remarkable in a high q-space under-sampling regime, thus driving the acquisition cost of HARDI closer to DTI.

Acknowledgments

This work was supported by the Center for Biomedical Imaging (CIBM) of the Geneva and Lausanne Universities, EPFL, the Leenaards and Louis-Jeantet foundations, the EPFL-Merck Serono Alliance award and the Swiss National Science Foundation (SNSF) under grant PP00P2-123438.

References