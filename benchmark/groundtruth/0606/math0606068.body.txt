The knee-jerk mapping

Introduction

We give the definitive theory of the knee-jerk mapping, to be defined below. This mapping has been investigated by many people, most notably Baum ([\cite=baumEagon], [\cite=baumPetrie], [\cite=baumSell], [\cite=baumEtAl], [\cite=baum:inequality] ).

We begin with an example, taken from [\cite=dempsterEtAl:jerk]. Suppose you want to locate the maximum of the function

[formula]

on the 1-simplex (a fancy name for a line segment)

[formula]

One way you can find it is by iterating the knee-jerk mapping

[formula]

This maps the simplex Σ to itself, and what is notable about the mapping is that it increases the value of the objective function Z.

The one true explanation of this ratcheting property of the knee-jerk map, the explanation that lays bare once and for all what is going on here, is as follows: Like any polynomial with only positive coefficients, the function Z is log-log-convex; that is, log Z is convex as a function of ( log x, log y); that is,

[formula]

is convex as a function of (u,v). We're trying to find the maximum of W on the set

[formula]

Since W is convex, if we fix a point (u,v), the graph of W lies above its tangent plane at (u,v,W(u,v)):

[formula]

Now ideally we'd like to move from (u,v) directly to the point of T where W(u,v) is greatest. What the knee-jerk mapping does is move instead to the point where the lower bound on the right hand side of the inequality above is maximized. This can't help increasing the objective function, right?

One remarkable fact should be pointed out, though it won't be gone into below: While the function Z is log-log-convex, it is nevertheless log-concave; that is, log Z is concave as a function of (x,y). (This is true because Z is a product of homogeneous linear functions with positive coefficients.) Because Z is log-concave, it has a unique maximum on the simplex Σ. While all polynomials with positive coefficients are log-log-convex, only very special polynomials are simultaneously log-concave.

A class of log-concave examples fundamentally more exciting than products of linear functions can be obtained as follows: Take a connected graph G, think of its edges as variables, form for each spanning tree of G a monomial (of degree one smaller than the number of vertices of G), and form a polynomial DG--the discriminant of G--by adding up the monomials corresponding to all spanning trees of G. For example, if G is a triangle with edges x,y,z,

[formula]

Discriminants of graphs are always log-concave. (If you know what a matroid is, let me add that the discriminant of a regular matroid is log-concave, but I don't know if the discriminant of a general matroid always is; my guess is that it isn't.)

Discriminants of graphs are particular cases of the diagonal discriminants of Bott and Duffin; these are always log-concave (because the determinant function is log-concave when restricted to the set of positive-definite matrices) as well as being log-log-convex (because they are polynomials with positive coefficients).

Knee-jerk functions

In real n-space, we will denote the positive orthant by Π and the closed standard simplex by Σ:

[formula]

[formula]

We denote their closures by [formula] (the non-negative orthant) and [formula] (the closed standard simplex).

We say that a function [formula] from Π to the positive real numbers is log-log-convex if log Z is a convex function of [formula]. The name comes from the fact that in the case n = 1 a log-log-convex function is one whose graph appears convex when drawn on log-log graph paper. We say that Z is a knee-jerk function if Z is increasing (which we take to mean what some would call 'non-decreasing') and log-log-convex. For pedantry's sake we require in addition that Z be smooth, and extend continuously to [formula].

Properties and examples.

There are many characterizations of convex functions, but for our purposes the most important is that a function is convex if and only if its graph lies above all of its tangent planes. Thus a smooth function Z is log-log-convex if and only if for any two points [formula] and [formula],

[formula]

where   =  Z(x̄) and ( log Z)u1 denotes the derivative of log Z with respect to u1, etc.

Using this characterization of log-log-convexity and Jensen's inequality--which states that for a concave function like log  the weighted average of the values is littler than the value of the weighted average--we get a proof that the function [formula] is log-log-convex, and hence a knee-jerk function:

[formula]

Once we know that [formula] is a knee-jerk function, we can easily produce a wealth of other examples by observing that the class of knee-jerk functions is closed under a variety of operations. The coordinate functions [formula] are knee-jerk functions, as is any positive constant function. Products, positive scalar multiples, and positive (possibly fractional) powers of knee-jerk functions are knee-jerk functions. So is the composition [formula] of a knee-jerk function [formula] with knee-jerk functions [formula], because the composition of increasing convex functions is increasing and convex. And since [formula] is a knee-jerk function, it follows that sums of knee-jerk functions are knee-jerk functions. Thus any non-zero polynomial with non-negative coefficients is a knee-jerk function.

The knee-jerk mapping

If [formula] is a knee-jerk function, we define the knee-jerk mapping

[formula]

(If [formula], we define [formula]--or just pretend we didn't notice.) Note that when Z is homogeneous of (possibly fractional) degree d, Euler's identity

[formula]

implies that

[formula]

TZ maps the positive orthant Π to the closed simplex [formula], and thus restricts to a mapping of Σ to [formula]. It is easy to see that a point [formula] is fixed by TZ if and only if it is a critical point of Z on Σ. The great thing about the knee-jerk mapping is that if [formula] is not a critical point of Z on Σ then [formula]; this will be proven in the next section. This makes the knee-jerk mapping a natural to iterate if you are interested in finding the maximum of Z on Σ. The name 'knee-jerk' is partly meant to suggest the automatic way in which the mapping increases the objective function Z.

The knee-jerk inequality

Write

[formula]

and

[formula]

The knee-jerk inequality.

[formula]

Proof. From the characterization of log-log-convexity above, we have

[formula]

Substituting [formula] yields the knee-jerk inequality. [formula]

Recall (if you don't already know) that for probability vectors [formula] the I-divergence [formula] is defined to be

[formula]

This quantity is always ≥  0, with equality if and only if [formula]. (This follows from an application of Jensen's inequality similar to that used above to show that [formula] is a knee-jerk function.)

Corollary. If [formula] then

[formula]

In particular, Z'  >  Z unless the point [formula] is fixed by TZ, which happens if and only if [formula] is a critical point of Z on Σ. [formula]

What is going on here?

Say our goal is to maximize Z over Σ. We're sitting at some point [formula], and we want to pick a new point x̄∈ so as to increase the objective function Z as much as possible. Since Z is log-log-convex we know that

[formula]

The knee-jerk idea is to choose x̄∈ so as to make the lower bound on the right of this inequality as large as possible. That is, we want to do as well as possible using only the value of Z and its derivatives at [formula] and the knowledge that Z is a knee-jerk function. So we want to choose x̄ so as to maximize

[formula]

subject to the constraint

[formula]

The maximum occurs where

[formula]

is proportional to

[formula]

that is, where

[formula]

Ruminations. When [formula], the fact that [formula] maximizes the lower bound for [formula] implies right away that Z'  ≥  Z, independently of the hocus-pocus with the I-divergence. Indeed, the positivity of the I-divergence can now be seen as a consequence of the fact that [formula] is a knee-jerk function. This is not so surprising, perhaps, since both facts followed from very similar applications of Jensen's inequality. But now it appears that [formula] is somehow the most important of all knee-jerk functions. And why should it be so distinguished? Because it crops up in the definition of the simplex Σ.

Generalizations

Given [formula], [formula], define

[formula]

and define

[formula]

[formula]

Then the knee-jerk inequality becomes

[formula]

When [formula] this becomes

[formula]

More interesting, we can replace the simplex Σ with a product of simplices: Let

[formula]

Let

[formula]

and define

[formula]

by

[formula]

Then

[formula]

and when [formula],

[formula]