Introduction

Consider a sequence [formula] of i.i.d. random vectors (i.e. identically distributed and independents). So, each couple [formula] has the same law that a generic variable [formula].

The model

Assume that the model can be written

[formula]

where

FW0 is a function represented by a one hidden layer MLP with parameters or weights W0 and sigmoidal functions in the hidden unit.

The noise, [formula], is sequence of i.i.d. centered variables with unknown invertible covariance matrix Γ(W0). Write ε the generic variable with the same law that each εt.

Notes that a finite number of transformations of the weights leave the MLP functions invariant, these permutations form a finite group (see [\cite=Sussman]). To overcome this problem, we will consider equivalence classes of MLP : two MLP are in the same class if the first one is the image by such transformation of the second one, the considered set of parameter is then the quotient space of parameters by the finite group of transformations.

In this space, we assume that the model is identifiable, this can be done if we consider only MLP with the true number of hidden units (see [\cite=Sussman]). Note that, if the number of hidden units is over-estimated, then such test can have very bad behavior (see [\cite=Fukumizu]). We agree that the assumption of identifiability is very restrictive, but we want emphasize the fact that, even in this framework, classical test of the number of parameters in the case of multidimensional output MLP is not satisfactory and we propose to improve it.

testing the number of parameters

Let q be an integer lesser than s, we want to test "[formula]" against "[formula]", where the sets Θq and Θs are compact. H0 express the fact that W belongs to a subset of Θs with a parametric dimension lesser than s or, equivalently, that s - q weights of the MLP in Θs are null. If we consider the classic cost function : [formula] where ||x|| denotes the Euclidean norm of x, we get the following statistic of test :

[formula]

It is shown in [\cite=Yao], that Sn converges in law to a ponderated sum of χ21

[formula]

where the χ2i,1 are s - q i.i.d. χ21 variables and λi are strictly positives values, different of 1 if the true covariance matrix of the noise is not the identity. So, in the general case, where the true covariance matrix of the noise is not the identity, the asymptotic distribution is not known, because the λi are not known and it is difficult to compute the asymptotic level of the test.

To overcome this difficulty we propose to use instead the cost function

[formula]

we will show that, under suitable assumptions, the statistic of test :

[formula]

will converge to a classical χ2s - q so the asymptotic level of the test will be very easy to compute. The sequel of this paper is devoted to the proof of this property.

Asymptotic properties of Tn

In order to investigate the asymptotic properties of the test we have to prove the consistency and the asymptotic normality of Ŵn  =   arg  min W∈ΘsUn(W). Assume, in the sequel, that ε has a moment of order at least 2 and note

[formula]

remark that these matrix Γn(W) and it inverse are symmetric. in the same way, we note Γ(W) =  lim n  →    ∞Γn(W), which is well defined because of the moment condition on ε

Consistency of Ŵn

First we have to identify contrast function associated to Un(W)

Proof :

By the strong law of large number we have

[formula]

where Id denotes the identity matrix of [formula]. So, the lemme is true if Γ(W) - Γ(W0) is a positive matrix, null only if W = W0. But this property is true since

[formula]

We deduce then the theorem of consistency :

If [formula],

[formula]

Proof

Remark that it exist a constant B such that

[formula]

because Θs is compact, so FW(Z) is bounded. For a matrix [formula], let ||A|| be a norm, for example [formula]. We have

[formula]

and since the function :

[formula]

is uniformly continuous, by the same argument that example 19.8 of [\cite=Vandervaart] the set of functions Un(W), W∈Θs is Glivenko-Cantelli.

Finally, the theorem 5.7 of [\cite=Vandervaart], show that Ŵn converge in probability to W0 [formula].

Asymptotic normality

For this purpose we have to compute the first and the second derivative with respect to the parameters of Un(W). First, we introduce a notation : if FW(X) is a d-dimensional parametric function depending of a parameter W, write [formula] (resp. [formula]) for the d-dimensional vector of partial derivative (resp. second order partial derivatives) of each component of FW(X).

First derivatives :

if Γn(W) is a matrix depending of the parameter vector W, we get from [\cite=Magnus]

[formula]

Hence, if we note

[formula]

using the fact

[formula]

we get

[formula]

Second derivatives :

We write now

[formula]

and

[formula]

We get

[formula]

Now, [\cite=Magnus], give an analytic form of the derivative of an inverse matrix, so we get

[formula]

so

[formula]

Asymptotic distribution of Ŵn :

The previous equations allow us to give the asymptotic properties of the estimator minimizing the cost function Un(W), namely from equation ([\ref=first_deriv]) and ([\ref=second_deriv]) we can compute the asymptotic properties of the first and the second derivatives of Un(W). If the variable Z has a moment of order at least 3 then we get the following lemma :

Assume that [formula] and [formula], let ΔUn(W0) be the gradient vector of Un(W) at W0 and HUn(W0) be the Hessian matrix of Un(W) at W0.

Write finally

[formula]

We get then

[formula]

[formula]

[formula]

where, the component (k,l) of the matrix I0 is :

[formula]

proof :

We can show easily that, for all [formula], we have :

[formula]

Write

[formula]

and U(W): =  log  det (Y - FW(Z)).

Note that the component (k,l) of the matrix 4I0 is:

[formula]

and, since the trace of the product is invariant by circular permutation,

[formula]

Now, the derivative [formula] is square integrable, so ΔUn(W0) fulfills Lindeberg's condition (see [\cite=Hall]) and

[formula]

For the component (k,l) of the expectation of the Hessian matrix, remark first that

[formula]

and

[formula]

so

[formula]

Now, since [formula] and [formula], by standard arguments found, for example, in [\cite=Yao] we get

[formula]

[formula]

Asymptotic distribution of Tn

In this section, we write Ŵn  =   arg  min W∈ΘsUn(W) and Ŵ0n  =   arg  min W∈ΘqUn(W), where Θq is view as a subset of [formula]. The asymptotic distribution of Tn is then a consequence of the previous section, namely, if we have to replace nUn(W) by its Taylor expansion around Ŵn and Ŵ0n, following [\cite=Vandervaart] chapter 16 we have :

[formula]

Conclusion

It has been show that, in the case of multidimensional output, the cost function Un(W) leads to a test for the number of parameters in MLP simpler than with the traditional mean square cost function. In fact the estimator Ŵn is also more efficient than the least square estimator (see [\cite=Rynkiewicz]). We can also remark that Un(W) matches with twice the "concentrated Gaussian log-likelihood" but we have to emphasize, that its nice asymptotic properties need only moment condition on ε and Z, so it works even if the distribution of the noise is not Gaussian. An other solution could be to use an approximation of the covariance error matrix to compute generalized least square estimator :

[formula]

assuming that Γ is a good approximation of the true covariance matrix of the noise Γ(W0). However it take time to compute a good the matrix Γ and if we try to compute the best matrix Γ with the data, it leads to the cost function Un(W) (see for example [\cite=Gallant]).

Finally, as we see in this paper, the computation of the derivatives of Un(W) is easy, so we can use the effective differential optimization techniques to estimate Ŵn and numerical examples can be found in [\cite=Rynkiewicz].