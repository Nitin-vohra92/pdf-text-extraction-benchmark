Using Filter Banks in Convolutional Neural Networks for Texture Classification

Keywords: Texture classification, Convolutional Neural Network, dense orderless pooling, filter banks, energy response

Introduction

Texture, together with colour is a key component in the analysis of images. A texture image or region obeys some statistical properties and exhibits repeated structures. Texture analysis approaches such as statistical, structural, model-based or transform-based explore these properties to classify, segment or synthesize textured images [\cite=bharati2004image]. Pioneered, among others, by Fukushima [\cite=fukushima1980neocognitron], Lecun [\cite=lecun1998gradient] and Hinton [\cite=krizhevsky2012imagenet], Convolutional Neural Networks (CNNs) have been generalized since the breakthrough in the 2012 ImageNet Large Scale Visual Recognition Challenge [\cite=russakovsky2014imagenet] of Krizhevsky et al. [\cite=krizhevsky2012imagenet], and improved the state of the art of many machine vision tasks. Basic CNN architectures have been applied to texture recognition such as [\cite=tivive2006texture], in which a simple four layers network was used in the early stage of deep learning (2006) to classify the Brodatz database. More recently, Hafemann et al. [\cite=hafemann2014forest] have applied CNN to a forest species classification, similar to a texture classification problem. While more complex and more accurate than [\cite=tivive2006texture], this approach still does not take the properties of texture images into consideration as it is a simple application of a standard CNN to a texture dataset. The complexity of the features trained by CNNs increases with the depth of the network. Therefore the last convolution layer extracts complex features which respond to objects such as a nose, a face or a human body. The fully connected layers use the response to these features to obtain information about the overall shape of the image and calculate a probability distribution over the different classes in the last fully connected layer. This design is suitable for exploring the arrangement of less complex features from the previous layers and their sparse spatial response for an object recognition scheme. This overall shape analysis and the sparsity and complexity of features is less adequate in texture analysis as we mainly seek for repeated patterns of lower complexity. However, the dense orderless extraction of features by the intermediate layers using weight sharing is of high interest in texture analysis. The first layer extracts edge-like features and can be thought of as a filter bank approach such as Gabor [\cite=fogel1989gabor] or Maximum Response filters [\cite=caenen2004maximum], widely used in texture analysis. Intermediate convolution and pooling layers are alike filter banks extracting features of increasing complexity. It is possible that a classic CNN architecture could learn how to explore the properties of texture images most efficiently without modifying its structure when trained on a very large texture dataset (such as ImageNet). This assumption can not be verified as, to the best of our knowledge, such a database does not exist. Cimpoi et al. [\cite=cimpoi2014deep] have demonstrated the relevance of densely extracting texture descriptors from a CNN with the Fisher Vector CNN (FV-CNN). They obtain impressive results on both texture recognition datasets and texture recognition in clutter. This approach is well suited to region recognition as it requires computing the convolution layer output once and pooling regions with FV separately. However, the CNN part of the FV-CNN does not learn from the texture dataset. A pre-trained network is finetuned to extract the output of the convolution layers. In the finetuning process, only the weights of the fully-connected layers significantly vary whereas pre-trained weights of the previous layers will mostly remain unchanged. The learning part for the texture database is then performed by the GMM, FV and SVM, also making this approach computationally expensive. We build up on [\cite=cimpoi2014deep] to create a CNN designed to learn and classify texture images. We are interested in creating a CNN that fully incorporates the learning of texture descriptors and their classification as opposed to the FV-CNN. Therefore, we pool simple energy descriptors from convolution layers that are used inside the CNN and allow forward and backward propagation to learn texture features. While Cimpoi et al. [\cite=cimpoi2014deep] insist on the domain transferability of CNNs by demonstrating that a network pre-trained on ImageNet can be used to accurately classify texture images, we try to analyze a texture specific domain and the result of training a network only on texture images. Therefore we evaluate trainings from scratch as well as networks pre-trained on texture and object datasets. We demonstrate that simple networks with reduced number of neurons and weights are able to achieve similar or better results on texture recognition datasets. One of the major trends in the community of deep neural networks is to use more and more complex networks, using the increasing power and memory of computers and GPUs to train very deep and computationally expensive networks. However, the interest of CNNs is not limited to powerful desktop computers and designing efficient networks while restraining their size is important for mobile and embedded computing as mentioned in [\cite=szegedy2014going]. In particular, we are interested in training networks from scratch on datasets of various sizes which is more computationally expensive than using pre-trained CNNs. In consequence, this research is not focused on competing with the state of the art in texture recognition but rather designing a simple architecture which explores the ideas introduced in this section as well as gaining insight on how CNNs learn texture feature. To summarize, the main contributions of the paper are as follows: (a) We create a simple, computationally reduced CNN which extracts, learns and classifies texture features; (b) We evaluate the performance of networks from scratch and pre-trained, as well as the domain transferability of the latter; (c) We experiment various depth networks when applied to texture and object datasets; (d) We combine texture and shape anaylisis within a new network architecture.

Method description

This section describes our T-CNN architecture and its combination with a classic deep neural network approach.

Texture CNN (T-CNN)

CNNs naturally pool dense orderless features by the weight sharing of the convolution layers. These layers can be compared to filter banks widely used in texture analysis which extract the response to a set of features. While these features are pre-designed in fiter banks methods, the power of CNNs is to be able to learn appropriate features through forward and backward propagation. These learned features can be combined within the network to classify new unknown images. We develop a simple network architecture which explores this attribute. As explained in [\cite=cimpoi2014deep], the global spatial information is of minor importance in texture analysis as opposed to the necessity of analyzing the global shape for an object recognition scheme. Therefore, we want to pool dense orderless texture descriptors from the output of a convolution layer. Our network is derived from AlexNet [\cite=krizhevsky2012imagenet]. We develop a new energy layer as described in the following and use multiple configurations with varying number of convolution layers. Each feature map of the last convolution layer is simply pooled by calculating the average of its rectified linear activation output. This results in one single value per feature map, similar to an energy response to a filter bank where we use learned features of varying complexity instead of fixed filters. A configuration with two convolution layers C1 and C2 is shown in Figure [\ref=fig:TCNN_color_C2]. The forward and backward propagation of the energy layer E2 are similar to an average pooling layer over the entire feature map, i.e. with a kernel size equal to the size of the feature map. The vector output of the energy layer is simply connected to a fully connected layer FC1, followed by two other fully connected layers FC2 and FC3. Similar to other networks, FC3 is a vector of probabilities of size equal to the number of classes. We use linear rectification, normalization and dropout in the same way as AlexNet. We experiment with five configurations T-CNN-1 to T-CNN-5 with one to five convolution layers respectively. The energy is consistently pooled from the output of the last convolution layer. It is important to notice that the number of parameters in our network is significantly lower than in a classic CNN such as AlexNet. Not only are convolution layers removed (in T-CNN-1 to T-CNN-4), but also the number of connections between the energy layer E2 and the fully connected layer FC3 is much smaller than the classic full connection of the last convolution layer. Also, the size of the fully connected layers can be drastically reduced without deteriorating the performance as they encode a much smaller amount of neurons from E2. Therefore, the number of parameters to train is reduced, as is the training and testing computational time as well as the required memory.

Combining texture and classic CNNs

The design of the T-CNN enables a simple and efficient integration into a classic CNN architecture. Figure [\ref=fig:TCNN_Original_C3] illustrates this new architecture in which the energy layer of the T-CNN is extracted within the classic CNN and its output is concatenated to the flattened output of the last convolution layer. This combination resembles the Multi-Stage features developed in [\cite=sermanet2012convolutional]. In this way, the network analyzes the texture and the overall shape of the image, likewise [\cite=cimpoi2014deep] in which the FV-CNN for texture analysis is combined to the FC-CNN for the overall shape analysis. However our method incorporates the two approaches within the same network which learns and combines texture and shape features. It is important to note that the texture and shape analysis share the same previous and following layers (C1, C2, C3 and FC6, FC7, FC8), thus keeping the computation time and memory consumption close to the original AlexNet .

Results and analysis

Details of the network

We use Caffe [\cite=jia2014caffe] to implement our network, where our architecture is derived from AlexNet. We keep the same number of feature maps, kernel sizes etc. for comparison. However it is possible to reduce the size of the fully connected layers of the T-CNN by a factor greater than two in average without loss of accuracy. In general the base learning rate is 0.001 for networks that learn from scratch and 0.0001 for finetuning; the weight decay is 0.0005. Yet, this is not a fixed rule and even though it is relatively stable, we must adapt the hyperparameters to the experiments (number of training samples, depth, finetuning or from scratch). The results are also robust to small variations of the batch size but we also adapt the latter to the training sizes. We use a batch size of 32 for the smallest training sets (kth-tips-2b, Kylberg) and up to 256 for ImageNet. Finally, we keep the cropping of the input images to 227x227 for the sake of comparison.

Datasets

We use a total of six datasets; three are texture datasets, the other three are object datasets as described below. The ImageNet 2012 dataset [\cite=russakovsky2014imagenet] contains 1000 classes. The training set contains 1,281,167 images and the validation set 50,000 images (50 images/class). The images are of size 256x256. Subsets of Imagenet: We want to compare networks pre-trained on texture and object databases of the same size when it comes to finetuning these networks on another texture dataset. Therefore, we create three subsets of ImageNet. For each subset, we select 28 classes from ImageNet. ImageNet-T is a subset which retains texture classes such as "stone wall", "tile roof" and "velvet". ImageNet-S1 is another subset with chosen object-like classes such as "chihuahua", "ambulance" and "hammer". Finally, in ImageNet-S2 the classes are chosen randomly. kth-tips-2b [\cite=hayman2004significance] contains 11 classes of 432 texture images. Each class is made of four samples (108 images/sample). Each sample is used once as training set while the remaining three samples are used for testing. The images are of size 256x256. Kylberg [\cite=Kylberg2011c] is a texture database containing 28 classes of 160 images of size 576x576. We randomly choose one of 12 available orientations for each image and split the images into four subimages which results in 8960 images of size 288x288. We use half of the obtained set for training and the other half for testing.

Results

Table [\ref=tab:scratch1] shows the results of training the T-CNN from scratch using one to five convolution layers. Table [\ref=tab:finetune1] presents the classification rates using the networks pre-trained on the ImageNet database. In both cases, our approach performs almost always better than AlexNet on the texture datasets: Kylberg, kth-tips-2b and ImageNet-T. The only unexpected result is that the original AlexNet outperforms T-CNN-3 when using the ImageNet pretrained network (see Table [\ref=tab:finetune1]). T-CNN-3 also performs best from scratch on the object-like subsets of ImageNet: ImageNet-S1 and ImageNet-S2. We believe that our network can learn from the texture regions present in these images. One can notice that T-CNN-1 performs better on ImageNet-T which contains texture images (42.7%) and ImageNet-S2 (42.14%) which is a random selection of classes than on ImageNet-S1 which contains selected object classes (34.86%). This is due to the fact that the first convolution layer extracts extremely simple features (mainly edges) and acts like a Gabor filter which is not robust at classifying object datasets. However, when using pre-trained networks, the original AlexNet expectedly performs better on non-texture images as it is designed for object recognition and can learn more object-like features from the large database. One could think that the reason we obtain good results is that small networks are well suited to small datasets as the low number of parameters prevents overfitting while being sufficient to learn appropriate features. Whereas very large networks are generally preferred to train large training sets using dropout to avoid overfitting. However, using a shallow network based on AlexNet without using our energy layer does not perform as well as our method. We have tested a shortened version of AlexNet with only three convolution layers and notice a drop of accuracy of 71.07% to 68.64% for ImageNet-T and 49.62% to 49.15% for kth-tips-2b when compared to T-CNN-3. This shallow network also requires more parameters between the last convolution layer and the fully connected layers, thus more mermory and computation. Therefore our contribution is more than solely using a small network for small datasets as the results show that our method is a good adaptation of CNN to texture analysis.

Tables [\ref=tab:scratch1] and [\ref=tab:finetune1] show that our T-CNN performs best with three convolution layers with T-CNN-4 being closely second best. Averaging the output of convolution layers is like measuring the response to a set of filters. The more convolution layers we use, the more complex the features sought by these filters are. In our method, using five layers performs worse as the fifth layer extracts sparse and complex object-like features (nose, wheel etc.). Averaging the response of these complex features over the entire feature map is not appropriate as such features may be very sparsely found in the input images. Intuitively one could think that, for this deep architecture, a maximum energy layer would achieve better results than an averaging energy, especially in an object recognition task. To compare, we test a maximum energy layer which simply outputs the maximum response of each entire feature map, thus measuring whether a certain feature was found in the input image, neither taking into account its location nor the number of occurrences. The results in Table [\ref=tab:avemax2] confirm this idea as one can see that shallow T-CNNs perform better with an averaging energy layer while deeper ones are more accurate with a maximum energy layer. On the other extreme, features in the first layer are too simple (mainly edges) and the network does not have enough learnable parameters to perform as well as T-CNN-3 as can be seen in Tables [\ref=tab:scratch1] and [\ref=tab:finetune1]. Table [\ref=tab:finetunes3] shows the classification results on kth-tips-2b of networks that were pre-trained on different datasets. Finetuning a network that was pre-trained on a texture database (ImageNet-T) achieves better results on another texture dataset (kth-tips-2b) as compared to a network pre-trained on an object dataset of the same size (ImageNet-S1 and ImageNet-S2). T-CNN-3 pre-trained on ImageNet-T results in 61.47% whereas it drops to 57.29% when pre-trained on ImageNet-S1. However the size of the dataset on which T-CNN is pre-trained predominantly influences the performance of the finetuning as we can see on Table [\ref=tab:finetunes3]. The network pre-trained on ImageNet significantly outperforms the other pre-trained network (72.36% with T-CNN-3). These two observations suggest that a very large texture dataset could bring a significant contribution to CNNs applied to texture analysis.

Results combining texture and shape analysis

Table [\ref=tab:combined] shows the improvements obtained by combining the T-CNN to a classic AlexNet network. The networks are pre-trained on the ImageNet dataset and finetuned on kth-tips-2b. First the voting scores of the pre-trained T-CNN-3 and AlexNet (outputs of the last fully connected layers) are summed to provide an averaged classification of the two networks. An increase of 0.75% is obtained in the classification of kth-tips-2b as compared to the T-CNN-3 and 1.65% as compared to AlexNet. Finally the combined network that we name TS-CNN (Texture and Shape CNN) described in part 2.2 obtains the best results with 73.68%. It is therefore possible to combine both approaches within the same network to learn texture and overall shape information.

Conclusion

In this paper we have developed a new CNN architecture for analyzing texture images. Inspired by classic neural networks and filter banks approaches, we introduce an energy measure which allows us to discard the overall shape information analyzed by classic CNNs. We showed that with our T-CNN architecture, we can increase the performance in texture recognition, while reducing the complexity, memory requirements and computation time. Finally we developed a network that incorporates our texture CNN approach into a classic deep neural network architecture, demonstrating their complementarity with a clear improvement of accuracy.

Future research

Our energy pooling approach enables inputing images of various dimensions, while keeping the exact same structure. Thus, it is possible to implement a multi-scale analysis by using multiple rescaled images as inputs like in [\cite=cimpoi2014deep]. Alternatively, the multiscale analysis and computation reduction of the GoogLeNet [\cite=szegedy2014going] could be incorporated to our network.