true cm true cm

Notation Problem

Group Sparse Recovery via the [formula] Penalty: Theory and Algorithm

Introduction

In recent years, sparsity has emerged as one of the most prominent tools for data acquisition, signal transmission, storage and processing in signal processing and for simultaneous variable/feature selection and estimation in statistics. Mathematically the problem is often formulated as

[formula]

where the vector [formula] denotes the target signal to be recovered, the vector [formula] denotes measurement errors with a noise level [formula], and the sensing matrix [formula] models the system response mechanism. Under the assumption that the observational data [formula] is generated from the linear combination of a few basis vectors ψj, the sparsity approach has proven very effective to estimate the underlying sparse signal x† in the highly under-determined case n  ≪  p. A natural approach is the following [formula] optimization

[formula]

where [formula] denotes the Euclidean norm of a vector, and [formula] the number of nonzero entries in a vector. However, problem [\eqref=eqn:l0reg] is generally hard to solve due to discontinuity of the [formula] penalty, and in practice, convex relaxations, such as Lasso [\cite=Tibshirani:1996] [\cite=Chen:1998], and greedy methods [\cite=TroppGilbert:2007] [\cite=NeedellTropp:2009] [\cite=DaiMilenkovic:2009] [\cite=BlumensathDavies:2008] [\cite=BlumensathDavies:2009] [\cite=Foucart:2011] [\cite=NeedellVershynin:2009] have been proven very successful.

In many applications, the underlying signal x† exhibits a natural group structure: not only the signal x† is sparse but also a priori known subsets of the components are all equal to zero. For example, in electroencephalography, each group encodes the information about the direction and strength of the dipoles of each discrete voxel representing the dipole approximation [\cite=OuHamalainenGolland:2009]. Other examples include multi-task learning/multivariate regression, wavelet image analysis, and gene analysis with biological pathways, to name a few. Such group structure represents an important piece of knowledge about the problem statement, and should be properly accounted for in the solution procedure in order to improve the interpretability and accuracy of the recovered signal.

These applications have motivated developing regularization methods at group level. Bakin [\cite=Bakin:1999] proposed group Lasso as an extension of Lasso and adopted group thresholding in regularized wavelet estimation. Malioutov et al [\cite=MalioutovCetinWillsky:2005] proposed a model to enforce the sparsity pattern of spatial-temporal signals. This idea was further developed by Yuan and Lin [\cite=YuanLin:2006], where the sum of the [formula] norms of the group coefficients is penalized, instead of individual coefficients. This piece of work has sparked many further research activities on group-wise estimation and variable selection; see the overview [\cite=HuangBrehenyMa:2012] and references therein. A number of theoretical studies have shown that many desirable properties of group Lasso, and the advantages of group Lasso over Lasso for recovering group sparse signals [\cite=HuangZhang:2010] [\cite=BaraniukCevher:2010] [\cite=LouniciPontil:2011] [\cite=Bajwa:2015] [\cite=ErenVidyasagar:2015]. Very recently, [formula] and [formula] error estimates for the were derived for a class of group recovery models, including group Lasso [\cite=ErenVidyasagar:2015]. However, group Lasso suffers from the same drawbacks as Lasso: it requires stringent conditions for exact support recovery, and statistically, the estimator is biased and lacks the oracle property [\cite=FanLi:2001] [\cite=ZhangZhang:2012]. In the context of Lasso, nonconvex penalties have been proposed to remedy these drawbacks, e.g., the bridge, smoothly clipped absolute deviation, and minmax concavity penalty; and their nice properties do extend to the group case [\cite=WangLiHuang:2008] [\cite=HuangMaZhang:2009] [\cite=HuangBrehenyMa:2012]. A number of efficient algorithms [\cite=YuanLin:2006] [\cite=MeierVandegeerPeter:2008] [\cite=VandDenBerg:2008] [\cite=TsengYun:2009] [\cite=ChenLinKim:2011] [\cite=She:2012] [\cite=QinScheinberGoldfarb:2013] [\cite=BrehenyHuang:2015] have been proposed for the convex and nonconvex group sparse recovery models; see also [\cite=Eldar:2010] [\cite=BenElda:2011] for group greedy methods. However, in these interesting existing works, the small submatrices of Ψ (i.e., ΨGi) are assumed to be well conditioned to get estimation errors; see [\cite=HuangZhang:2010] [\cite=LouniciPontil:2011] [\cite=HuangBrehenyMa:2012] [\cite=ErenVidyasagar:2015] [\cite=Eldar:2010] [\cite=BenElda:2011]. While this assumption is reasonable in some applications, it excludes the practically important case of a general sensing matrix, especially possible strong correlation within groups. For example, in microarray gene analysis, where the columns in the matrix Ψ correspond to gene expression values, it was observed that genes in the same pathway produce highly correlated values [\cite=SegalDahlquist:2004]. One remedy is the A-norm group lasso, which penalizes a weighted [formula] norm of the group coefficient vector, with the weights dependent on Ψ [\cite=SimonTibs:2012]. However, a rigorous justification seems not yet available.

In this work, we propose a novel approach for recovering group sparse signals:

[formula]

where the [formula] penalty [formula] (with respect to a given group partition {Gi}Ni = 1) is defined below in [\eqref=eqn:l0l2], and the scalar λ > 0, known as regularization parameter, controls the group sparsity level of the regularized solution. The motivation of the [formula] penalty follows the same spirit of the [formula] penalty in [\eqref=eqn:l0reg], and it is the genuine penalty for recovering groupwise sparse signals. To the best of our knowledge, this model has not been studied in the literature. We shall show that it has a number of salient features in comparison with existing approaches. First, the regularized solution is invariant under full rank column transformation, and thus it does not depend on the specific parametrization within the groups. Second, it allows strong inner-group correlation and merits a built-in decorrelation effect, cf. Remarks [\ref=rmk:correlation1], [\ref=rmk:correlation2] and [\ref=rmk:correlation3], and thus admits theoretical results under very weak conditions. Third, both global minimizer and the block coordinatewise minimizer merit several desirable properties, e.g., support recover and oracle property.

In this work, we provide a thorough mathematical and numerical study on the model [\eqref=eqn:groupl0]. First, we establish a number of fundamental theoretical properties, e.g., existence of a global minimizer, local optimality, necessary optimality condition, and transformation invariance. The model [\eqref=eqn:groupl0] can be equivalently transformed into a problem with orthogonal columns within each group, and thus its property is independent of the conditioning of the inner-group columns, which is in sharp contrast to existing group sparse recovery models. Second, we develop an efficient numerical algorithm for solving the model, which is of primal dual active set (PDAS) type. It represents a nontrivial extension of the PDAS algorithm for the [formula] and [formula] penalties [\cite=FanJiaoLu:2014] [\cite=JiaoJinLu:2014]. Numerically, at each iteration, the algorithm involves only solving a least-squares problem on the active set and exhibits a fast local convergence. When coupled with a continuation strategy along λ, the algorithm is extremely efficient and merits a global convergence in finite steps. The MATLAB package of the algorithm is provided online at .

The rest of the paper is organized as follows. In Section [\ref=sec:prelim], we describe the problem setting, and derive useful estimates. Then in Section [\ref=sec:cwm], we investigate fundamental properties of the model [\eqref=eqn:groupl0], e.g., the existence of a global minimizer, invariance property, and necessary optimality condition. In Section [\ref=sec:pdasc], we develop a group primal dual active set algorithm, and establish its global convergence. Finally, in Section [\ref=sec:numer], several numerical examples are provided to illustrate the mathematical theory and the efficiency of the algorithm. The technical proofs are given in the appendix.

Preliminaries

In this section, we describe the problem setting, and derive useful estimates.

Problem setting and notations

Throughout, we assume that the sensing matrix [formula] with n  ≪  p has normalized columns [formula] for i = 1,...,p, and the index set S  =  {1,...,p} is divided into N non-overlapping groups {Gi}Ni = 1 such that 1  ≤  si  =  |Gi|  ≤  s and [formula]. For any index set B  ⊆  S, we denote by xB (respectively ΨB) the subvector of x (respectively the submatrix of Ψ) which consists of the entries (respectively columns) whose indices are listed in B. All submatrices ΨGi, [formula], are assumed to have full column rank. The true signal x† is assumed to be group sparse with respect to the partition {Gi}Ni = 1, i.e., x†  =  (x†G1,...,x†GN), with T nonzero groups. Accordingly, the group index set [formula] is divided into the active and inactive set by

[formula]

The measurement vector y in [\eqref=model], possibly contaminated by noise, can be recast as Given the true active set A† (as provided by an oracle), we define the oracle solution xo by the least squares solution on A† to [\eqref=model], i.e.,

[formula]

The oracle solution xo is uniquely defined provided that [formula] has full column rank. It is the best approximation for problem [\eqref=model], and will be used as the reference solution.

For any vector [formula], we define an [formula]-penalty (with respect to the group partition {Gi}Ni = 1) for [formula] and q > 0 by

[formula]

When r = q > 0, the [formula] penalty reduces to the usual [formula] penalty. The choice r = 0 (or r =   ∞  ) and q = 2 is frequently used below. Further, we shall abuse the notation [formula] for any vector that is only defined on some sub-groups (equivalently zero extension).

For any [formula], the [formula] penalty defines a proper norm, and it was studied in detail in [\cite=Kowalski:2009]. For any r,q > 0, the [formula] penalty is continuous. The [formula] penalty, which is of major interest in this work, is discontinuous, but still lower semi-continuous.

The [formula] penalty is lower semicontinuous.

Let [formula] be a convergent sequence to some [formula]. By the continuity of the [formula] norm, [formula] converges to [formula], for [formula]. Now the assertion follows from the lower semicontinuity of the [formula] function, i.e., [formula] [\cite=ItoKunisch:2014].

Now we derive the associated hard-thresholding operator x*∈Hλ(g) for one single group for an s-dimensional vector [formula] as where the [formula] penalty is given by [formula] if x  ≠  0, and [formula] otherwise. Then it can be verified directly For a vector [formula], the hard thresholding operator Hλ (with respect to the partition {Gi}Ni = 1) is defined groupwise. For s = 1, it recovers the usual hard thresholding operator, and hence called a group hard thresholding operator.

Blockwise mutual coherence

Now we develop several useful estimates for the analysis of and algorithm for the group [formula] model in Sections [\ref=sec:cwm] and [\ref=sec:pdasc], using the concept of blockwise mutual coherence. We first introduce some notation:

[formula]

Since ΨGi has full column rank, Gi is symmetric positive definite and invertible.

The main tool in our analysis is the blockwise mutual coherence (BMC) μ on the matrix Ψ with respect to the group partition {Gi}Ni = 1, which is defined by

[formula]

where Ni is the subspace spanned by the columns of ΨGi, i.e., [formula]. Geometrically, the quantity μi,j is the cosine of the minimum angle between two subspaces Ni and Nj. Hence the BMC μ is a natural generalization of the concept mutual coherence (MC) ν, which is defined by ν  =   max i  ≠  j|〈ψi,ψj〉| [\cite=DonohoHuo:2001], and has been widely used in analyzing sparse recovery algorithms [\cite=TroppGilbert:2007] [\cite=CaiWang:2011] [\cite=JiaoJinLu:2014]. In linear algebra, one often uses principal angles to quantify the angles between two subspaces [\cite=BjorckGloub:1973], i.e., given [formula] with   U  =  si and   V = sj, the principal angles θl for l  =  1,2,..., min {si,sj} are defined recursively by

By the definition of principal angles, μi,j  =   cos (θ1) for (U,V)  =  (Ni,Nj); see Lemma [\ref=equdef] below. Principal angles (and hence the BMC) can be computed efficiently by QR and SVD [\cite=BjorckGloub:1973], unlike the restricted isometry property or its variants [\cite=Bandeira:2013].

Let [formula] and [formula] be two matrices whose columns are orthonormal basis of Ni and Nj, respectively, and {θl}min (si,sj)l = 1 be the principal angles between Ni and Nj. Then, μi,j  =   cos (θ1)  =  σmax(UtiVj).

The proof can be found in [\cite=BjorckGloub:1973].

The next result indicates that the BMC μ can be bounded from above in terms of the MC ν. In particular, it implies that the BMC is generally sharper than a direct extension of the MC, since the BMC μ does not depend on the correlation of the inner-group columns.

Let the MC ν of Ψ satisfy (s - 1)ν  <  1. Then for the BMC μ of Ψ, there holds

Let N1  =  {p1,...,ps1} and N2  =  {q1,...,qs2} be two subspaces spanned by two distinct groups, where pi, qj are column vectors of unit length. By the definition of the MC ν, |〈pi,qj〉|  ≤  ν for any [formula] and [formula]. For any u∈N1 and v∈N2, let [formula] and [formula]. Then with c  =  (c1,...,cs1) and d  =  (d1,...,ds2), and similarly [formula]. Hence we have

[formula]

in view of the inequality where the second last inequality follows by the Cauchy-Schwarz inequality.

In later discussions we assume the following condition on the BMC μ of the matrix Ψ.

Assumption [\ref=assump:mu] ensures the uniqueness of the oracle solution xo; see Corollary [\ref=cor:oracle]. We have a few comments on Assumption [\ref=assump:mu].

First, if the group sizes do not vary much, then the condition μ  <  1  /  3T holds if [formula]. The latter condition with C∈(2,7) is widely used for analyzing Lasso [\cite=zhang:2009sharp] and OMP [\cite=Tropp:2004] [\cite=CaiWang:2011]. Hence, the condition in Assumption [\ref=assump:mu] generalizes the classical condition. Second, the assumption allows strong inner-group correlations (i.e., ill-conditioning of ΨGi), for which the MC ν can be very close to one, and thus it has a built-in mechanism to tackle inner-group correlation. This differs essentially from existing approaches, which rely on certain pre-processing techniques, e.g., clustering [\cite=Buhlmann:2013] [\cite=Witten:2014].

A similar block MC, defined by [formula], was used for analyzing group greedy algorithm [\cite=Eldar:2010] [\cite=BenElda:2011] and group Lasso [\cite=Bajwa:2015] (without scaling s). If each submatrix ΨGi is column orthonormal, i.e., ΨtGiΨGi  =  I, then μB and μ are identical. However, to obtain the error estimates in [\cite=Eldar:2010] [\cite=BenElda:2011], the MC ν within each group is still needed, which excludes possible correlations within each group. The estimates in [\cite=Bajwa:2015] were obtained under the assumption [formula], which again implies that ΨGi are well conditioned [\cite=Bajwa:2015]. Group restrict eigenvalue conditions [\cite=HuangZhang:2010] [\cite=LouniciPontil:2011] and group restricted isometry property [\cite=ErenVidyasagar:2015] were adopted for analyzing the group Lasso. Under these conditions, strong correlation within groups is not allowed.

Now we give a few important preliminary estimates. These estimates are very useful for analyzing the model and algorithm.

For any i,j, there hold

First, recall that for any matrix A, AtA and AAt have the same nonzero eigenvalues. Upon letting A  =  - 1GiΨtGi, we have AAt  =  I, and and likewise giving the first two estimates. If i = j, Di,j is an identity matrix, and thus [formula]. For i  ≠  j, [formula], [formula], then we have Thus by Lemma [\ref=equdef], there holds

[formula]

showing the last inequality.

For any distinct groups [formula], 1  ≤  M  ≤  T, let There holds

Since Di,i  =  I, we have By Lemma [\ref=lem:est-G], [formula] for any k  ≠  ij. Let k* be the index such that [formula]. Then To show the other inequality, let j* be the index such that [formula]. Then by Lemma [\ref=lem:est-G], we deduce This completes the proof of the lemma.

An immediate consequence of Lemma [\ref=lem:est-D] is the uniqueness of the oracle solution xo.

If Assumption [\ref=assump:mu] holds, then the oracle solution xo is uniquely defined.

Since ΨGi has full column rank, problem [\eqref=eqn:oracle] is equivalent to where x̄Gi  =  GixGi. The normal matrix involved in the least-squares problem on [formula] is exactly the matrix D in Lemma [\ref=lem:est-D], with {i1,...,iM}  =  A†. Then the uniqueness of the oracle solution xo follows from Lemma [\ref=lem:est-D].

Theory of the [formula] optimization problem

Now we study the [formula] model [\eqref=eqn:groupl0], and analyze its analytical properties, e.g., the existence of a global minimizer, invariance property, support recovery, and block coordinatewise minimizers.

Existence and property of a global minimizer

First we show the existence of a global minimizer to problem [\eqref=eqn:groupl0].

There exists at least one global minimizer to problem [\eqref=eqn:groupl0].

Let [formula]. Then the set [formula] is finite. For any nonempty [formula], the problem [formula] has a minimizer x*(B). Let [formula], and for [formula], let [formula] and x*(B) = 0. Then we denote [formula], with the minimizing set B*, and accordingly x* = x*(B*). We claim that Jλ(x*)  ≤  Jλ(x) for all [formula]. Given any [formula], let [formula] be the smallest superset of [formula]. Then [formula], and further by construction [formula] and hence Jλ(x)  ≥  Jλ(x*(B))  ≥  Jλ(x*).

It can be verified directly that the [formula] penalty is invariant under group full-rank column transformation, i.e., [formula] for nonsingular Gi, [formula]. Thus problem [\eqref=eqn:groupl0] can be equivalently transformed into

[formula]

with x̄Gi  =  GixGi. This invariance does not hold for other group sparse penalties, e.g., group lasso and group SCAD. Further, the BMC μ is invariant under the transformation, in view of the identity [formula].

Most existing works do not distinguish inner- and inter-group column vectors, and thus require incoherence between the columns in each group in order for the theory to hold. For strong inner-group correlation, first a clustering step is often employed to decorrelate the sensing matrix Ψ [\cite=Buhlmann:2013] [\cite=Witten:2014]. In contrast, our approach has a built-in decorrelation mechanism: it is independent of the conditioning of the submatrices {ΨGi}Ni = 1.

The next result shows that for properly chosen λ, the global minimizer has nice properties. In particular, the proposed [formula] model allows the exact support recovery for small noise and the [formula] estimate has the oracle property.

Let Assumption [\ref=assump:mu] hold, x be a global minimizer of [\eqref=eqn:groupl0] with an active set A, and x̄†Gi  =  Gix†Gi.

Let [formula]. If λ  >  ε2  /  2, then

Let the noise η be small in the sense that [formula] Then for any [formula], the oracle solution xo is the only global minimizer to Jλ.

Since x* is a global minimizer of Jλ, we have which together with the choice of λ implies |A|  ≤  T. Since any global minimizer is also a block coordinatewise minimizer (see Section [\ref=ssec:bcwm] below for the definition), by Theorem [\ref=thm:support](i) below, we deduce This gives part (i). Next, for [formula], there holds A†  ⊆  A and hence A†  =  A. Hence the only global minimizer is the oracle solution xo.

Necessary optimality condition

Since problem [\eqref=eqn:groupl0] is highly nonconvex, there seems no convenient characterization of a global minimizer that is amenable with numerical treatment. Hence, we resort to the concept of a block coordinatewise minimizer (BCWM) with respect to the group partition {Gi}Ni = 1, which is minimizing along each group coordinate xGi [\cite=Tseng:2001]. Specifically, a BCWM x* to the functional Jλ satisfies

We have the following necessary and sufficient condition for a BCWM x*. It is also the necessary optimality condition of a global minimizer x*.

The necessary and sufficient optimality condition for a BCWM [formula] of problem [\eqref=eqn:groupl0] is given by

[formula]

where x̄*Gi  =  Gix*Gi, and the dual variable d* is d*  =  Ψt(y - Ψx*) and *Gi  =  - 1Gid*Gi.

By simple computation, a BCWM x* is equivalent to the following: for [formula] Recall the matrices Gi = (ΨtGiΨGi)1 / 2 from [\eqref=eqn:notation]. Using the elementary identities the BCWM x* can be equivalently characterized by Now recalling x̄Gi  =  GixGi, x̄*Gi  =  Gix*Gi, and *Gi  =  - 1Gid*Gi etc., we deduce Using the hard-thresholding operator Hλ, we obtain the optimality condition [\eqref=eqn:opt].

The optimality system is expressed in terms of the transformed variables x̄ and [formula] only, instead of the primary variables x and d. This has important consequences for the analysis and algorithm of the [formula] model: both should be carried out using the transformed variables. Clearly, [\eqref=eqn:opt] is also the optimality system of a BCWM x̄* for problem [\eqref=eqn:groupl0-orth], concurring with the invariance property.

Notation. In the discussions below, given a primal variable x and dual variable d, we will use (x̄,) for the transformed variables, i.e., x̄Gi  =  GixGi and Gi  =  - 1GidGi, i = 1,...,N.

Using the group hard-thresholding operator Hλ, we deduce Combining these two relations gives the following simple observation

[formula]

Next we discuss some interesting properties of a BCWM. Our first result states that a BCWM x* is always a local minimizer, i.e., Jλ(x* + h)  ≥  Jλ(x*) for all small [formula].

A BCWM x* of the functional Jλ is a local minimizer. Further, with its active set A, if [formula] has full column rank, then it is a strict local minimizer.

It suffices to show Jλ(x* + h)  ≥  Jλ(x*) for all small [formula]. Let [formula]. Then

[formula]

Now consider a small perturbation [formula] to x*. If [formula], since [formula] for small h, by [\eqref=eqn:min-chara], the assertion holds. Otherwise, if [formula], then

[formula]

which is positive for small h, since [formula], cf. [\eqref=eqn:xtd]. This shows the first assertion. Now if ΨB has full column rank, then problem [\eqref=eqn:min-chara] is strictly convex. Hence, for small [formula] with [formula] This and [\eqref=eqn:J-positive] show the second assertion.

To further analyze a BCWM x*, we derive crucial estimates on one-step primal-dual iteration. The proof is lengthy and deferred to Appendix [\ref=app:05]. Here the energy E associated with an active set A is the largest group of transformed variables [formula] outside A, i.e.,

[formula]

These estimates bound the errors in the primal variable x̄ on A by the energy E and the noise level ε, and similarly the dual variable [formula] on I.

Let Assumption [\ref=assump:mu] hold, and A be a given index set with |A|  ≤  T, and I  =  Ac. Consider the following one-step primal-dual update (with [formula])

[formula]

where Ψ†B  =  (ΨtBΨB)- 1ΨtB is the pseudo-inverse of ΨB. Then with [formula], [formula] and [formula], E = E(A), for the transformed primal variable x̄, there holds

[formula]

and for the transformed dual variable [formula], there holds for any i∈I

[formula]

Given the active set A of a BCWM x*, if |A| is controlled, then A provides information about the true active set A†; see Theorem [\ref=thm:support] below. For example, if the noise η is small, with a proper choice of λ, then A  ⊆  A†.

Let Assumption [\ref=assump:mu] hold, and x* be a BCWM to the model [\eqref=eqn:groupl0] with a support A and |A|  ≤  T. Then the following statements hold.

The inclusion [formula] holds.

The inclusion A  ⊆  A† holds if the noise η is small in the sense

[formula]

If the set [formula] is empty, then A  ⊆  A†.

First we derive two preliminary estimates using the notation P, Q and R from Lemma [\ref=lem:update-in-A]. Since |A|  ≤  T and |Q|  ≤  T, Lemma [\ref=lem:update-in-A] and the triangle inequality yield

[formula]

Likewise, using the inequality we deduce from Lemma [\ref=lem:update-in-A]

[formula]

Now we can proceed to the proof of the theorem. For [formula], A  =  A† and assertions (i) and (ii) are trivially true. Otherwise, let [formula]. Then [formula]. By [\eqref=eqn:est-d-new] and inequality [\eqref=eqn:xtd] with i  =  i*, we have Consequently, by Assumption [\ref=assump:mu], we deduce

[formula]

i.e., assertion (i) holds. Next we show assertion (ii) by contradiction. If [formula], we can choose [formula], and apply [\eqref=eqn:est-x-new] and [\eqref=eqn:est-d-new], together with [\eqref=eqn:xtd], to obtain which contradicts [\eqref=assump:noise], thereby showing assertion (ii). Last, we show assertion (iii). Assume that [formula]. Then [\eqref=eqn:E-upper] holds. Meanwhile, since [formula], using [\eqref=eqn:est-x-new] and [\eqref=eqn:est-d-new] (by choosing x̄Gi by [formula] and Gi*) and inequality [\eqref=eqn:xtd], we have Under Assumption [\ref=assump:mu], simple computation gives [formula] and [formula]. This contradicts with the assumption in (iii), and thus the inclusion A  ⊆  A† follows.

Group Primal-Dual Active Set Algorithm

Now we develop an efficient, accurate and globally convergent group primal dual active set (GPDAS) algorithm for problem [\eqref=eqn:groupl0]. It generalizes the PDAS algorithm for the [formula] and [formula] regularized problems [\cite=FanJiaoLu:2014] [\cite=JiaoJinLu:2014] to the group case. The starting point is the necessary and sufficient optimality condition [\eqref=eqn:opt] for a BCWM x* from Proposition [\ref=prop:necopt]. The following two observations from [\eqref=eqn:opt] form the basis of the derivation. First, given a BCWM x* (and its dual variable d*  =  Ψt(y - Ψx*)), one can determine the active set A* by and the inactive set I* its complement, provided that the set [formula] is empty. Second, given the active set A*, one can determine uniquely the primal and dual variables x* and d* by (with [formula]) By iterating these two steps alternatingly, with the current estimates (x,d) and (A,I) in place of (x*,d*) and (A*,I*), we arrive at an algorithm for problem [\eqref=eqn:groupl0].

The complete procedure is listed in Algorithm [\ref=alg:gpdasc]. Here [formula] is the maximum number of inner iterations, λ0 is the initial guess of the regularization parameter λ. The choice [formula] ensures that x0 = 0 is the only global minimizer, cf. Proposition [\ref=prop:lam0] below, with a dual variable d0  =  Ψty. The scalar ρ∈(0,1) is the decreasing factor for λ, which determines the length of the continuation path.

The algorithm consists of two loops: an inner loop of solving problem [\eqref=eqn:groupl0] with a fixed λ using a GPDAS algorithm, and an outer loop of continuation along the regularization parameter λ by gradually decreasing its value.

In the inner loop, the algorithm involves solving a least squares problem on Ak only: which is equivalent to solving a (normal) linear system of size [formula]. Hence, this step is very efficient, if the active set Ak is of small size, which is the case for group sparse signals. Further, the inner iterates are essentially of Newton type: for the convex [formula] penalty, the iterates are identical with that for the semismooth Newton method [\cite=FanJiaoLu:2014]. Hence, its local convergence is expected to be very fast. However, in order to fully exploit this nice feature, a good initial guess of the primal and dual variables (x,d) is required. To this end, we employ a continuation strategy along λ. Specifically, given a large λ0, we gradually decrease its value by λs  =  ρλs - 1, for some decreasing factor ρ∈(0,1), and take the solution x(λs - 1) to the λs - 1-problem Jλs - 1 to warm start the λs-problem Jλs.

There are two stopping criteria in the algorithm, at steps 8 and 13, respectively. In the inner loop, one may terminate the iteration if the active set Ak does not change or a maximum number Kmax of inner iterations is reached. Since the stopping criterion Ak  =  Ak - 1 for convex optimization may never be reached in the nonconvex context [\cite=JiaoJinLu:2014], it has to be terminated after a maximum number Kmax of iterations. Our convergence analysis holds for any [formula], including Kmax = 1, and we often choose Kmax∈[1,5] in practice. The stopping criterion at step 13 is essentially concerned with the proper choice of λ. The choice of λ stays at the very heart of any regularized model, and it is also the case for the model [\eqref=eqn:groupl0]. In the literature, many rules, e.g., discrepancy principle, balancing principle and Bayesian information criterion, have been developed [\cite=ItoJin:2014]. In Algorithm [\ref=alg:gpdasc], we give only the discrepancy principle [\eqref=eqn:discprin], provided that a reliable estimate on the noise level ε is available. The rationale behind the principle is that the reconstruction accuracy should be comparable with the data accuracy [\cite=ItoJin:2014]. Note that the use of the discrepancy principle (and other rules) does not incurred any extra computational overheads, since the sequence of solutions {x(λs)} is already generated along the continuation path.

Now we justify the choice of λ0: for large λ, 0 is the only global minimizer to Jλ.

The following statements hold.

For any λ > 0, x* = 0 is a strict local minimizer to Jλ;

For any [formula], x* = 0 is the only global minimizer of problem [\eqref=eqn:groupl0].

Recall the identity with [formula]. Also for any x  ≠  0, [formula]. Hence, for any [formula], where Br(0) denotes a ball centered at the origin with a radius [formula], there holds [formula] This shows the first assertion. For λ  >  λ0, for any nonzero x, we have [formula], and thus [formula] i.e., x* = 0 is the only global minimizer.

Last we state a finite-step global convergence of Algorithm [\ref=alg:gpdasc].

Let Assumption [\ref=assump:mu] and [\eqref=assump:noise] hold. Then for a proper choice of ρ∈(0,1), and for any Kmax  ≥  1, Algorithm [\ref=alg:gpdasc] converges to the oracle solution xo in a finite number of iterations.

Since the proof is lengthy and technical, we only sketch the main ideas here, and defer the complete proof to Appendix [\ref=app:07]. The most crucial ingredient of the proof is to characterize a strict monotone decreasing property of the "energy" during the iteration by some auxiliary set Γs, defined by

[formula]

The inclusion Γs1  ⊆  Γs2 holds trivially for s1 > s2. If Ak is the active set at the k-th iteration, the corresponding energy Ek is defined by

[formula]

Then with properly chosen s1  >  s2, one can show This inclusion relation characterizes precisely the evolution of the active set Ak during the GPDAS iteration and provides a crucial strict monotone decreasing property of the energy Ek. This observation is sufficient to show the convergence of the algorithm to the oracle solution xo in a finite number of steps; see Appendix [\ref=app:07] for details.

The convergence in Theorem [\ref=thm:main] holds for any [formula], including the choice Kmax = 1. According to the proof in Appendix [\ref=app:07], the smaller are the factor μT and the noise level ε, the smaller is the decreasing factor ρ that one can choose and consequently Algorithm [\ref=alg:gpdasc] takes fewer outer iterations to reach convergence on the continuation path. In our implementation, we often taken ρ  ≈  0.7.

Numerical results and discussions

Now we present numerical results to illustrate distinct features of the proposed [formula] model and the efficiency and accuracy of the group PDAS algorithm. All the numerical experiments were performed on a four-core desktop computer with 3.16 GHz and 8 GB RAM. The MATLAB code (GPDASC) is available at .

Experimental setup

First we describe the problem setup of the numerical experiments. In all the numerical examples, the group sparse structure of the true signal x† is encoded in the partition {Gi}Ni = 1, which is of equal group size s, with p = Ns, and x† has T = |A†| nonzero groups. The dynamic range (DR) of the signal x† is defined by In our numerical experiments, we fix the minimum nonzero entry at min {|x†i|:x†i  ≠  0} = 1. The sensing matrix Ψ is constructed as follows. First we generate a random Gaussian matrix [formula], n  ≪  p, with its entries following an independent identically distributed (i.i.d.) standard Gaussian distribution with a zero mean and unit standard deviation. Then for any i∈1,2...,N, we introduce correlation within the ith group Gi by: given [formula] by setting [formula], [formula] and where the correlation parameter θ  ≥  0 controls the degree of inner-group correlation. The larger is the parameter θ, the stronger is the inner-group correlation. Finally, we normalize the matrix [formula] to obtain the sensing matrix Ψ such that each column of Ψ is of unit length. The observational data y is formed by adding noise η to the exact data y†  =  Ψx† componentwise, where the entries ηi follow an i.i.d. Gaussian distribution N(0,σ2) with mean zero and standard deviation σ. Below we shall denote by the tuple (n,p,N,T,s,DR,θ,σ) the data generation parameters. All the results reported below are the average of 100 independent simulations of the experimental setting.

The benefit of group sparsity

First, we illustrate the benefit of incorporating the group structure in the sparse recovery by the proposed model [\eqref=eqn:groupl0] over the now classical Lasso [\cite=Tibshirani:1996] [\cite=Chen:1998] (solved by the homotopy method [\cite=DonohoTsaig:2006]) and the state-of-art greedy methods, e.g., OMP [\cite=TroppGilbert:2007] and HTP [\cite=Foucart:2011]. We note that all the benchmark methods do not take into account the group structure in the recovery procedure. To this end, we consider the following problem settings (n,p,N,T,s,DR,θ,σ) = (500,103,250,10:10:100,4,10,0,10- 3) and (n,p,N,T,s,DR,θ,σ) = (500,103,250,10:10:100,4,10,3,10- 3). The notation N1:d:N2 denotes the sequence of numbers starting with N1 and less than N2 with a spacing d. The probability of exact support recovery is shown in Fig. [\ref=fig:gsvs].

For the parameter θ  =  0, the inner-group columns are almost uncorrelated, confirmed by an O(1) condition number for the submatrices ΨGi, and the numerical results are shown in Fig. [\ref=fig:gsvs](a). All methods under consideration can exactly recover the underlying true support A† for very sparse signals, e.g., T = 10. However, as the group sparsity level T increases, Lasso, OMP, and HTP start to fail when T  =  20, 40, and 60, respectively. In contrast, the proposed [formula] model still works fairly well for T up to T = 80. The case θ = 3 corresponds to strong correlation within each group, and the condition number of the submatrices ΨGi is O(100). In the presence of strong inner-group correlation, we observe that Lasso, OMP and HTP fail completely to recover the true support A† even for very sparse signals. In sharp contrast, the proposed [formula] model [\eqref=eqn:groupl0] (together with the gpdasc algorithm) still performs well, which is attributed to the use of group structure (and the built-in decorrelation mechanism of the proposed model). These experiments show clearly the significant benefit of building group structure into the recovery algorithm.

Comparison with existing group sparse models

Now we compare the proposed [formula] model [\eqref=eqn:groupl0] (and the GPDASC algorithm) with three state-of-the-art group sparse recovery models and algorithms, e.g., group basis pursuit (GBP) model (solved by the group SPGl1 method [\cite=VandDenBerg:2008]), the group MCP (GMCP) model [\cite=WangLiHuang:2008] [\cite=HuangMaZhang:2009] [\cite=HuangBrehenyMa:2012] (solved by the group coordinate descent (GCD) method [\cite=BrehenyHuang:2015]), and the greedy group OMP (GOMP) [\cite=Eldar:2010] [\cite=BenElda:2011]. We refer to the cited references for the implementation details about these existing approaches. For the comparison, we shall examine separately support recovery, and computing time and reconstruction error.

First, to show exact support recovery, we consider the following problem settings: [formula] and [formula], for which the condition numbers of the submatrices ΨGi are O(1) and O(102), respectively, for the case θ = 0 and θ = 3, respectively. The numerical results are summarized in Fig. [\ref=fig:exrgm], where the exact recovery is measured by A*  =  A†, with A† and A* being the true and recovered active sets, respectively. We observe that as the (group) sparsity level T and inner-group correlation parameter θ increase, the [formula] model and GMCP are the best performers in the test. A closer look shows that the group SPGL1 tends to choose a larger active set than A† in the noisy case.

Next we compare the approaches by computing time and reconstruction error on the following problem settings [formula] and [formula], for which the condition number of the submatrices ΨGi within each group is of O(1) and O(103), respectively. The case θ = 10 involves very strong inner-group correlation, since the condition number O(103) of the submatrices ΨGi is huge in view of its size s = 4, and thus it is numerically very challenging for many existing approaches. The numerical results are summarized in Figs. [\ref=fig:timeerror1] and [\ref=fig:timeerror2].

It is observed from Fig. [\ref=fig:timeerror1] that for small correlation (i.e., θ = 0), the proposed [formula] model is the fastest one, which is about three to four times faster than GMCP and GOMP, and group Lasso is equally efficient. Meanwhile, the reconstruction error of the [formula] model is the smallest one, and the accuracy of GOMP and GMCP are close. The convex relaxation - group Lasso - significantly compromises the accuracy with computational efficiency. In the case of strong inner-group correlation (i.e., θ = 10), the computing time of the proposed algorithm does not change much, but that of other algorithms has increased by a factor of two. Further, the reconstruction quality by the [formula] model does not deteriorate with the increase of the correlation parameter θ, due to its built-in decorrelation mechanism, cf. Section [\ref=sec:cwm], and thus its reconstruction error is far smaller than that by other methods, especially when the sparsity level T is large. In summary, these experiments show clearly that the proposed [formula] model is competitive for group sparse recovery.

Superlinear local convergence of Algorithm [\ref=alg:gpdasc]

Last we examine the continuation strategy and local superlinear convergence behavior of Algorithm [\ref=alg:gpdasc], using the following problem settings (n,p,N,T,s,DR,θ,σ) = (500,103,250,50,4,100,0,10- 3) and (n,p,N,T,s,DR,θ,σ) = (500,103,250,50,4,100,3,10- 3). To examine the local convergence, we show the number of iterations for each regularization parameter λs along the continuation path in Fig. [\ref=fig:locsup]. It is observed that the stopping criterion at the inner iteration is usually reached with one or two iterations. Hence, the PADS algorithm converges locally supperlinearly and the continuation strategy can provide an excellent initial guess for each inner iteration. Also this observation corroborates the convergence theory in Theorem [\ref=thm:main], i.e., the algorithm converges globally even if each inner loop takes one iteration.

Conclusions

In this work we proposed and analyzed a novel approach for recovering group sparse signals based on the regularized least-squares problem with an [formula] penalty. We provided a complete theoretical study on the approach, e.g., the existence of global minimizers, invariance property, support recovery, and characterization and properties of block coordinatewise minimizers. One salient feature of the approach is that it has built-in decorrelation mechanism, and can handle very strong inner-group correlation. Furthermore, these nice properties can be numerically realized efficiently by a primal dual active set solver, for which the global convergence is also established. Extensive numerical experiments are presented to illustrate salient features of the [formula] model, and the efficiency and accuracy of the algorithm, and the comparative study with existing approaches show that it is competitive in terms of support recovery, reconstruction errors and computing time.

There are several avenues deserving further study. First, when the column vectors in each group are ill-posed in the sense that they are highly correlated / nearly parallel to each other, which are characteristic of most inverse problems, the propose [formula] model [\eqref=eqn:groupl0] may not be well defined or the involved linear systems in the group PDAS algorithm can be challenging to solve directly. One possible strategy is to apply an extra regularization. This motivates further study on related theoretical issues. Second, in practice, the true signal may have extra structure within the group, e.g., smoothness or sparsity. It remains to study how to use such extra a priori information.

Acknowledgements

The research of Y. Jiao is partially supported by National Science Foundation of China No. 11501579, B. Jin is partially supported by EPSRC grant EP/M025160/1, and X. Lu is supported by National Science Foundation of China No. 11471253.

Proof of Lemma [\ref=lem:update-in-A]

First, the least squares update step in [\eqref=eqn:update] can be rewritten as Hence, there holds

[formula]

Let m = |P|  ≤  T, [formula], then k = |Q| = T - m. Further, we denote the sets P, Q and R by [formula], [formula] and [formula]. Then [\eqref=eqn:primal-diff] can be recast blockwise, using the notation Di,j  =  - 1GiΨtGiΨGj- 1Gj etc, cf. [\eqref=eqn:notation], as Next we estimate the two terms in the curly bracket. By Lemma [\ref=lem:est-G], we deduce

[formula]

For the first term [formula], we denote its rows by [formula], for any [formula]. Since [formula], we have for [formula]

[formula]

By the definition of the "energy" E, [formula]. Hence,

[formula]

By Lemma [\ref=lem:est-D], [\eqref=eqn:iteeta] and [\eqref=eqn:itex] and the triangle inequality, we arrive at

[formula]

Notice that |P| + |R|  =  |A|, we show [\eqref=eqn:estx]. Next we turn to the transformed dual variable [formula]. By the definition, d  =  Ψt(y  -  Ψx), and thus for any i∈I, we have which upon some algebraic manipulations yields For any [formula], by Lemma [\ref=lem:est-G] and [\eqref=eqn:errx], we have Likewise, for any [formula], we have This shows the assertion on the dual variable [formula], and completes the proof of the lemma.

Proof of Theorem [\ref=thm:main]

The lengthy proof is divided into four steps.

Step 1. First we give the proper choice of the decreasing factor ρ. By [\eqref=assump:noise], we have Then for any s1∈((1 - μT) / (1 - 2μT - t),(1 - μT) / (μT  +  t)), letting [formula], we deduce (1  -  μT) / (1  -  2μT  -  t)  <  s2  <  s1 < (1  -  μT) / (μT + t). Combining with the monotonicity of the function f(s1) = s2 / s1 over the interval ((1 - μT) / (1 - 2μT - t),(1 - μT) / (μT  +  t)), it implies that for any ρ∈((2μT + 2t)2 / (1 - μT)2,1), we can find such s1 with [formula]. Next we will choose ρ∈((2μT + 2t)2 / (1 - μT)2,1).

Step 2. Next we show an important monotonicity relation:

[formula]

For short, we denote by A  =  Ak, I  =  Ik, and [formula]. By the assumption A  ⊆  A†, we have [formula] in Lemma [\ref=lem:update-in-A]. Then it follows from [\eqref=eqn:errx] in the proof of Lemma [\ref=lem:update-in-A] that the updates x̄k + 1 and k + 1 satisfy

[formula]

By the assumption Γs21λ  ⊆  Ak, we deduce [formula]; and by assumption [\eqref=assump:noise], [formula]. Hence, using [\eqref=eqn:update-dinI], we deduce for any i∈I† where the last inequality follows from the choice of s1. This and the relation [\eqref=eqn:xtd] imply that i∈Ik + 1, and thus Ak + 1  ⊆  A†. Meanwhile, by [\eqref=eqn:update-dinQ], for any [formula], we have which by the relation [\eqref=eqn:xtd] yields i∈Ak + 1. It remains to show [formula]. Clearly, if [formula], the assertion is true. Otherwise, for any [formula], by [\eqref=eqn:update-xinA], there holds Like before, this and [\eqref=eqn:xtd] also imply i∈Ak + 1. Hence the inclusion Γs22λ  ⊆  Ak + 1 holds.

Step 3. Now we prove that the oracle solution xo is achieved along the continuation path, i.e., A(λs) = A† for some λs. To this end, for each λs-problem Jλs, we denote by As,0 and [formula] the active set for the initial guess and the last inner step (i.e., A(λs) in Algorithm [\ref=alg:gpdasc]) of the sth iterate of the outer loop, respectively. Since s1 > s2, the inclusion Γs21λs  ⊆  Γs22λs holds. Next we claim that the following important inclusion by mathematical induction holds for the sequence active sets A(λs) from Algorithm [\ref=alg:gpdasc]. From [\eqref=eqn:key], for any index s before the stopping criterion at step 13 of Algorithm [\ref=alg:gpdasc] is reached, there hold

[formula]

Note that for s = 0, by the choice of λ0, [formula], and thus [\eqref=eqn:inclusion] holds. Now for s > 0, it follows by mathematical induction and the relation [formula]. By [\eqref=eqn:inclusion], during the iteration, the active set [formula] always lies in A†. This shows the desired claim. For large s, we have Γs21λs  =  A†, and hence A(λs)  =  A†, and accordingly x(λs) is the oracle solution xo.

Step 4. Last, at this step we show that if [formula], then the stopping criterion at step 13 of Algorithm [\ref=alg:gpdasc] cannot be satisfied. Let [formula] and [formula], and denote by [formula] and [formula]. Then with the notation Gi and Di,j etc. from [\eqref=eqn:notation], we deduce Now recall the elementary identities and then appealing to Lemma [\ref=lem:est-D], we arrive at By repeating the proof of Lemma [\ref=lem:update-in-A], we deduce Now by the condition ε  ≤  tE from assumption [\eqref=assump:noise], it suffices to show

[formula]

which implies that the stopping criterion [\eqref=eqn:discprin] at step 13 of Algorithm [\ref=alg:gpdasc] cannot be satisfied. The left hand side of [\eqref=eqn:energy-iter] is a function monotonically decreasing with respect to the length |Q|, and when |Q|  =  T, we have 1  -  μ(T - 1)  -  2t  >  t  >  t2, which completes the proof.