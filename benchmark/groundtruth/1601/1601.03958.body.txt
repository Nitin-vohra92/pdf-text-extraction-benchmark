Real-Time Association Mining in Large Social Networks

Introduction

In recent years, digital social networks have experienced unprecedented growth. Twitter, Facebook, Instagram and YouTube are household names on six continents and form a daily part of billions of people's lives. Facebook, the world's most pervasive network, commands a valuation exceeding 200 billion dollars, while smaller networks are often valued in the tens of billions. Despite market capitalisations that place them amongst the world's most valuable companies, monetisation remains challenging. One of the central reasons for this is a changing attitude towards traditional advertising. Over-saturation and new uses of digital media have depleted the effectiveness of long-established models like web banners and TV adverts. In their place new monetisation channels are needed. Our work demonstrates how social media data can be used in multiple industries for decision support. Applications are in security where analysts explore a network looking for groups of potential adversaries, social sciences where queries can establish the important relationships between individuals of interest, e-commerce where queries reveal related products or users and in marketing where companies seek to optimise advertising channels or celebrity endorsers. To meet these emergent needs social media analysts are required to explore the social graph.

Existing methods to analyse local community structure in large graphs either rely on distributed computing facilities or incur excessive run-times making them impractical for exploratory work [\citep=Clauset2005] [\citep=Bahmani2011]. We have developed a real-time tool for analysis of large graphs. Performance is of vital importance as the number of possible queries grows exponentially with the size of the network, and the results of previous queries are combined with human knowledge to inform future queries. Currently no tool exists that provides this important functionality. We define the association strength using the Jaccard similarity of neighbourhood graphs. To get real-time performance we store a minhash fingerprint for every account of interest. The fingerprint encodes the Jaccard similarity to all other accounts more compactly than storing all pairwise Jaccard coefficients explicitly. The fingerprints allow rapid querying of associated accounts using Locality Sensitive Hashing (LSH). The LSH query outputs a weighted sub-graph onto which we run the WALKTRAP community detection algorithm before visualising the results [\citep=Pons2005]. In our work with Twitter we calculate the minhash signatures for the 675,000 accounts with greater than 10,000 followers (though all 700 million Twitter accounts are used to build the signatures) because accounts that are smaller than this are unlikely to be of interest for endorsement or as media channels and it is trivial to add additional accounts on an ad-hoc basis by simply appending new signatures.

Our contribution is to first establish that robust associations between social media users can be determined by means of the Jaccard similarity of their neighbourhood graphs. To make practical use of the implicit associations we demonstrate a compact representation of the neighbourhood graph and an efficient query procedure by applying mild approximations, which do not degrade performance for our use case. Finally we show how our work can be applied to a range of problems in graph analysis such as understanding the structure of industries, allegiances within political parties or public image of a brand.

Data and Preliminaries

Our work is independent of the type of social network. For simplicity we use Twitter data (the follower graph of roughly one billion vertices and 30 billion edges) to explain our method. To show generalisation we present some results using a Facebook Pages engagement graph containing 450 million vertices and 700 million edges in our experimental evaluation.

To generate Twitter data we crawled the network identifying every account with more than 10,000 followers and used Twitter's REST API to gather their complete follower list. Our data set contains 675,000 accounts with a total of 1.5  ×  1010 followers, of which 7  ×  108 were unique. Our Facebook data set was generated by matching the Twitter accounts to Facebook Page accounts. To optimize data throughput while remaining within rate limits imposed by the data owners we developed an asynchronous distributed network crawler using Python's Twisted library [\citep=Wysocki2011]. The crawler consisted of a server responsible for token and work management and a set of clients making distributed calls to the social networks.

We used ground truth associations to verify that our method finds associated entities. To get these we matched Twitter accounts with Wikipedia profiles and extracted the associated Wikipedia tags. Not all shared tags imply closely related communities and so we measured five community quality attributes for each tag. The specific attributes were taken from a study of good ground truth communities by [\cite=Yang2012], but adapted to apply them to weighted networks. We looked for tag communities that were strongly interconnected (high density), well separated from the rest of the network (high separability), having more internal than external edges (low conductance) and with many closed triangles (high clustering). The specific tags used, together with the number of accounts per tag and the structural properties of the tag community are shown in Table  [\ref=tab:communities]. Food and drink was not used but is shown as an example of a Wikipedia tag that does not imply a community.

Related Work

It is a common task in many branches of science to understand networks in terms of their structural sub-units (often called communities). Such methods can be divided into local and global methods.

Global methods assign every vertex in a network to a community. Common approaches include modularity detection [\citep=Newman2004b], information theoretic methods [\citep=Rosvall2008] and the random walk based WALKTRAP method [\citep=Pons2005]. All scale poorly and so can not be used for graphs above 1 million vertices. We use the WALKTRAP algorithm in the final stage of our process. It produces empirically good communities on the Jaccard similarity weighted graphs that we apply it to while preserving real time performance for graphs of less than 1000 nodes (anything above this and visualisations become difficult to interpret).

Local methods do not assign every vertex to a community. Instead they find vertices that are in the same community as a set of input vertices (seeds). For this reason they are normally faster than global methods. Local community detection methods were originally developed as crawling strategies to cope with the rapidly expanding web-graph [\citep=Flake2000]. Following the success of the PageRank algorithm [\citep=page1998a] for web search, many local random walk algorithms have been developed, the best known of which is Personal PageRank (PPR) [\citep=Haveliwala2002]. These methods typically require distributed computing resources to run on large graphs. While systems are improving all of the time, such resources are not always available to analysts and typically have an overhead of several minutes per query and so can't deliver the real time performance that we seek. [\citet=Kloumann2014] have conducted a comprehensive assessment of the most popular methods for local community detection on large graphs. In their study, PPR was the clear winner. We follow their experimental design in order to demonstrate that our approach leads to faster performance and better associations than PPR. There is a great deal of community detection literature. See [\cite=fortunato2010community] or [\cite=Newman2003] for a more detailed overview of both global and local community detection methods.

In order to gain real-time performance we use the minhashing compression technique to store a compact feature representation of the graph's edges. Minhashing was initially developed by [\cite=broder1997]. Their implementation dealt with sets that could be represented as binary vectors. This has since been extended to counts (integer vectors) [\citep=Charikar2002] and continuous variables [\citep=Philbin2008]. We make use of efficient algorithms for generating the hashes, which are discussed by [\cite=Manasse2010]. Locality Sensitive Hashing was originally developed by [\citet=Indyk1998] for nearest neighbour search.

Real-Time Association Mining

In the remainder of the paper we use the following notation. A user account (or interchangeably a vertex of the network) is denoted by A and N(A) gives all accounts neighbouring A. The set of accounts used as input are S (seeds) and the output accounts are C (a community).

Our method for real-time association mining has two stages. In stage one, we run a very fast nearest neighbour search against a set of input vertices. The nearest neighbour search stops when our stopping criteria is reached. In stage two, we apply a global community detection algorithm to the subgraph returned by stage one and visualise the results.

Our algorithm uses the aggregate behaviour of 700 million individuals to create robust associations. We quantify these associations using the Jaccard similarity coefficient between neighbourhood graphs. Two vertices are neighbours if they are connected by an edge irrespective of the direction of the edge. The Jaccard similarity is given by

[formula]

where N(Ai) is the set of neighbours of ith account. The Jaccard similarity is related to the Jaccard distance by D(.)  =  1  -  J(.), which satisfies all requirements of a distance metric and so by working with Jaccard similarities we are effectively embedding the graph in a metric space.

We initially experimented with using the directly observed edges instead of deriving implicit weighted edges through shared neighbours. The results were very poor due to high levels of noise in the edge-generation process. As an illustrative example, the pop stars Eminem and Rihanna have collaborated on four records and a stadium tour. Despite this clear association, Eminem is not one of Rihanna's 40 million Twitter followers. However, Rihanna and Eminem have a Jaccard similarity of 18%, making Rihanna Eminem's 6th strongest connection.

Rapid Jaccard Estimation via Minhash Fingerprints

Evaluating Equation  [\eqref=eq:3] is very expensive as each set can have up to 108 members. The minhashing compression technique of [\cite=broder2000min] generates unbiased estimates of the Jaccard Ĵ in O(K), the number of hash functions making up the fingerprint. The estimation error scales as [formula] and for K = 1000 this equates to a 10,000 times speed-up and 1000 times reduction in space. Using the largest 675,000 Twitter accounts with ≈  4  ×  1010 neighbours minhashing reduces processing time from approximately 8,000 seconds to one second and storage from 1TB to 1GB (compared to storing every Jaccard coefficient). It is vital to providing real-time performance.

The estimator is fully efficient, i.e., the variance is given by the Cramér-Rao lower bound [formula] where we have dropped the Jaccard arguments for brevity. An alternative method would be to pre-calculate and cache all pairwise Jaccards, eliminating the need for approximations. However for our data set this would require caching [formula] floating point values, which is approximately 1TB and so not possible using commodity hardware. A further advantage of our method is that new accounts can be added very quickly by simply calculating one additional minhash fingerprint without needing to add the pairwise similarity to all other accounts.

Generating Minhash Fingerprints

Minhash fingerprints allow for rapid estimation of Jaccard similarities. Generating the fingerprints is expensive and (re)generated monthly. Our Python/C implementation allows for an efficient computation of the fingerprints in an hour.

Locality Sensitive Hashing (LSH)

LSH is a technique for rapid (constant time) lookup of similar sets [\citep=Indyk1998]. Each account is treated as the set of its neighbours and when used with minhash signatures, the similarity metric is the Jaccard similarity. In our implementation, we use 500 bands, each containing two minhashes. As most accounts share no neighbours, the LSH step dramatically reduces the number of candidate accounts and the algorithm runtime by a factor of roughly 100. Without LSH our algorithm would not have sub-second run times.

Sorting Similarities

When using LSH we treat every account in the input set independently, producing a set of candidate accounts that may only be related to one of the input accounts. We experimented with two schemes to select the most relevant candidate accounts: Minhash Similarity (MS) and Agglomerative Clustering (AC). Both sequentially select the current best account as is common in local community detection methods [\citep=Kloumann2014]. They can best be understood through the Jaccard distance D  =  1  -  J instead of the Jaccard similarity J. Using the Jaccard distance we can define the centre [formula] of any set of vertices. At each step AC and MS augment the results set C with the closest account A* to [formula] that is not already in C. The difference between the two methods is that MS uses a constant value of [formula] based on the input vertices while AC updates [formula] after each step. Formally, the centre of the input vertices used for MS is defined by

[formula]

In AC at each iteration S and [formula] get updated by adding the closest account given by

[formula]

leading to

[formula]

The new centre Xn + 1 is most efficiently calculated using the recursive online update equation

[formula]

where n is the size of St.

Stopping Criteria

Both AC and MS are sequential processes and will return every candidate account unless a stopping criteria is applied. Many criteria have been used to terminate seed expansion processes. The simplest method is to terminate after a fixed number of inclusions. Alternative methods use local maxima in modularity [\citep=Lancichinetti2009] and conductance [\citep=Leskovec2010].

We were motivated to develop a new stopping criteria by the problem of optimising marketing strategies and the specific question: "Given a set of athletes, what is the smallest number that have influence on over half of the users of Twitter?" We refer to the number of unique neighbours of a set of accounts as the coverage of that set. We exploited the properties of minhash signatures to incorporate the coverage stopping condition into our work without losing real time performance.

Efficient Coverage Computation

The coverage y is given by

[formula]

the number of unique neighbours of the output vertices. Every time a new account A* is added we need to calculate [formula] to update the coverage. This is a large union operation and so is expensive to perform on each addition. Lemma [\ref=lemma] allows us to rephrase this expensive computation equivalently by using the Jaccard coefficient (available cheaply via the minhash fingerprints), which we subsequently use for a real time iterative algorithm.

For a community [formula] and a new account A*∉C, the number of Neighbours of the union [formula] is given as

[formula]

Once A* is determined according to [\eqref=eq:next_S_opt], we use Lemma [\ref=lemma] to update the unique Neighbour count as

[formula]

The RHS of [\eqref=eq:_lemma_res] contains three terms: |N(Ct)| is what we started with, |N(A*)| is the neighbour count of A*, which is easily obtained from Twitter or Facebook metadata and J(Ct,A*) is a Jaccard calculation that can rapidly be approximated with minhash signatures . Using this relationship we are able to calculate the coverage with negligible additional calculations.

Visualisation

The final stage of the process is to visualise the community structure and association strengths in the region of the input seeds. We experimented with several global community detection algorithms. These included INFOMAP, Label Propagation, Spectral Methods, edge betweenness and modularity [\citep=Rosvall2008] [\citep=Raghavan2007] [\citep=Newman2006] [\citep=Newman2004c]. The Jaccard similarity graph is weighted and almost fully connected and most community detection algorithms are written for binary sparse graphs. As a result, all methods with the exception of label propagation and WALKTRAP were too slow. Label Propagation had a tendency to select a single giant cluster and so we chose WALKTRAP.

Experimental Evaluation

We report results on three experiments: (1) We assess the sensitivity of the Jaccard similarity estimates with respect to the number of hash functions used to generate the fingerprints. This will justify the use of the minhash approximation. (2) We assess the robustness of using the neighbourhood overlap to determine associations in social data. (3) We show the output of our work and that association maps for social networks using minhashing and LSH produce intuitively interpretable maps of the Twitter and Facebook graphs.

Quality of the Jaccard Estimates

We measured the difference between the minhash approximation and the true Jaccard as a function of the number of hashes comprising the minhash signature. Figure [\ref=fig:minhash] shows the results of this experiment using 400,000 data points. Standard error bars are just visible up until 400 hashes. The graph shows an expected error in the Jaccard of just 0.001 at 1,000 hashes. This degree of accuracy is adequate to separate entities in our work and an indication of this is given in Table [\ref=tab:Nike] where Jaccard differences are an order of magnitude greater.

Table [\ref=tab:Nike] shows the ten most associated accounts to the sports brand 'Nike'. They are ranked in order of Jaccard similarity for different length signatures. Our method adds accounts in Jaccard order and so preserving the true rank is an important property. From the close correspondence of the rank vector using signatures of length 1,000 (column R̂1000) and the true rank (column R) in Table [\ref=tab:Nike] and the diminishing accuracy improvements shown in Figure [\ref=fig:minhash] we identified a fingerprint length of K  =  1,000 as optimal. This value provides an appropriate balance between accuracy and performance (both runtime and memory scale linearly with K). These results justify our use of minhash fingerprints to approximate the true Jaccard similarities between accounts.

Assessing Association Quality

It is qualitatively clear from Table [\ref=tab:Nike] that the Jaccard similarity between neighbourhood graphs defines associations that are consistent with human intuition. The 675,000 Twitter accounts with greater than 10,000 followers include sports people, musicians, actors, politicians, educational institutions, media platforms and businesses from all sectors of the economy. Of these our approach identified four of Nike's biggest competitors, five Nike sub-brands and a major retailer of their products as the top 10 most associated accounts. This is consistent with our use of Jaccard similarity of neighbourhood sets as a robust similarity measure between accounts.

To quantitatively assess the associations that our algorithm makes we used Wikipedia tags given in Table [\ref=tab:communities] to define groups of associated entities. The input to each experiment was a random sample of 30 accounts from a common tag community of size n. We then performed n - 30 additions to the output in Jaccard similarity order, measuring the recall at fixed intervals. Under this method a perfect process would pass through (1,1) in Figure [\ref=fig:results] and have an area under the curve in Table [\ref=tab:AUC] of 0.5. Formally, the recall is given by

[formula]

with [formula] as the initial seed set, [formula] as the ground truth community and C as the set of accounts added to the output.

Figure [\ref=fig:results] shows how the recall varies with the ratio [formula]. We show results comparing Minhash Similarity (MS) and Personal PageRank (PPR) across eight communities. Agglomerative Clustering (AC) was the worst performing method on both recall and run time and so is omitted for clarity. PPR was identified as the state of the art method for growing communities from seeds by [\citet=Kloumann2014]. Our PPR implementation follows theirs; we run PPR for three iterations and return a ranked list of candidates. The plots show standard errors over five randomly chosen input sets of 30 accounts from Ctrue. It is apparent that from each plot MS is superior to PPR. In Table [\ref=tab:AUC], we show the area under each recall curve.

In addition to variable recall, the methods have significantly different runtimes, which impact upon their practical use. Table [\ref=tab:runtimes] gives the mean and standard deviation of clustering runtimes averaged over the eight communities. MS is the fastest method by about two orders of magnitude, delivering an instantaneous user experience. Both PPR and AC are fast, but exceed ten seconds, regarded as the threshold for losing user attention in interactive software [\citep=Hewett1992].

Visualisation for Graph Analysis

Having demonstrated that robust associations between social media entities can be reached through real-time computations we demonstrate how this work is applied to graph analysis. Figures [\ref=fig:fb_graphs], [\ref=fig:twit_graphs] and [\ref=fig:twit_graphs1] show the output of the MS algorithm for the Facebook Pages engagement network and the Twitter Followers network, respectively. The edge thickness depicts the pairwise Jaccard similarity, which has been thresholded for image clarity. The vertex size represents the average Jaccard similarity to the input seeds, but is logarithmically scaled to be between 1 and 50 pixels. The vertex colour represents communities as determined by running the WALKTRAP algorithm for 4 iterations on the weighted Jaccard similarity graph output by the MS algorithm.

There are some key differences between the Facebook Pages engagement network and the Twitter Followers network: The most significant one is that the Twitter data set contains significantly more data, with a higher edge density. Therefore, it can be used to answer more questions than the Facebook data. However, the Facebook data is higher fidelity than the Twitter data. This is principally because there is no market in Facebook page likes, while Twitter followers are often purchased. Moreover, the Twitter graph corresponds to actions occurring as far back as 8 years (relatively few edges are ever deleted), while the Facebook graph corresponds only to events over the past two years when we began collecting data.

We demonstrate how our work can be used to provide insight for a broad range of questions and provide the following illustrative examples:

How would you describe the factions and relationships within the US Republican party? Figure [\ref=fig:fb_rep] shows a densely connected core group of active politicians with Donald Trump at the periphery surrounded by a largely disconnected set of right-wing interest bodies.

Which factions exist in global pop music? Gender and racial factions exist within the industry (Figure [\ref=fig:fb_pop]).

How are the major social networks used? Figure [\ref=fig:twit_social] shows that Google is highly associated with other technology brands while Instagram is closely related to celebrity and YouTube and Facebook are related to sports and politics.

How is the brand RedBull perceived by Twitter users? RedBull has strong associations with motor racing, sports drinks, extreme sports, gaming and football (Figure [\ref=fig:twit_redbull]).

How does sports brand marketing differ between the USA and Europe? Figures [\ref=fig:twit_euro_sport] and [\ref=fig:twit_us_sport] show the enormous importance of football (soccer) to European sports brands whereas US sports brands are associated with a broad range of sports including hunting, NFL, basketball, baseball and mixed martial arts (MMA).

Conclusion and Future Work

We have demonstrated how robust association between social media accounts can be derived from the overlap of neighbourhood graphs. By applying some mild approximations the networks of associations can be explored in real time. Furthermore, we have shown that the approximations used do not degrade performance for our application. Our work focusses on social network data, but our method is applicable to all large networks, including bipartite networks where similar problems are currently approached using recommender systems.

Currently, our approach treats every member of the neighbourhood graph as equal. In many settings, information is available to weight the edges. This might include message counts, the time since a connection was made or the type of connection. Efficient methods already exist for working with minhashes of weighted sets [\cite=Manasse2010], and so we see the natural progression of this work to be extensions to data with edges that can contain counts, weights and categorisations.

Acknowledgements

This work was partly funded through a Royal Commission for the Exhibition of 1851 Industrial Fellowship.