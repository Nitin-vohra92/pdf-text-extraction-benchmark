Lemma Definition Problem Proposition Corollary Remark Example Question

On multiplier processes under weak moment assumptions

Department of Mathematics, Technion, I.I.T., Haifa, Israel and Mathematical Sciences Institute, The Australian National University, Canberra, Australia, Email: shahar@tx.technion.ac.il Supported in part by the Israel Science Foundation.

Introduction

The motivation for this work comes from various problems in Learning Theory, in which one encounters the following random process.

Let X = (x1,...,xn) be a random vector on [formula] (whose coordinates (xi)ni = 1 need not be independent) and let ξ be a random variable that need not be independent of X. Set (Xi,ξi)Ni = 1 to be N independent copies of (X,ξ), and for [formula] define the centred multiplier process

[formula]

Multiplier processes are often studied in a more general context, in which the indexing class need not be a class of linear functionals on [formula]. Instead, one may consider an arbitrary probability space (Ω,μ) and in which case F is a class of functions on Ω. Let X1,...,XN be independent, distributed according to μ, and the multiplier process indexed by F is

[formula]

Naturally, the simplest multiplier process is when [formula] and [\eqref=eq:multi-general-def-intorduction] is the standard empirical process.

Controlling a multiplier process is relatively straightforward when ξ∈L2 and is independent of X. For example, one may show (see, e.g., [\cite=vanderVaartWellner], Chapter 2.9) that if ξ is a mean-zero random variable that is independent of X1,...,XN then

[formula]

where here and throughout the article, (εi)Ni = 1 are independent, symmetric { - 1,1}-valued random variables that are independent of (Xi,ξi)Ni = 1, and C is an absolute constant.

This estimate and others of its kind show that multiplier processes are as 'complex' as their seemingly simpler empirical counterparts. However, the results we are looking for are of a different nature: estimates on multiplier processes that are based on some natural complexity parameter of the underlying class F, and that exhibits the class' geometry.

It turns out that chaining methods lead to such estimates, and the structure of F may be captured using the following parameter, which is a close relative of Talagrand's γ-functionals [\cite=MR3184689].

For a random variable Z and p  ≥  1, set

[formula]

Given a class of functions F, u  ≥  1 and s0  ≥  0, put

[formula]

where the infimum is taken with respect to all sequences (Fs)s  ≥  0 of subsets of F, and of cardinality |Fs|  ≤  22s. πsf is the nearest point in Fs to f with respect to the (u22s) norm.

Let

[formula]

To put these definitions in some perspective, [formula] measures the local-subgaussian behaviour of Z, and the meaning of 'local' is that [formula] takes into account the growth of Z's moments up to a fixed level p. In comparison,

[formula]

implying that for 2  ≤  p  <    ∞  , [formula]; hence, for every u  ≥  1 and s  ≥  s0,

[formula]

and Λ̃0,u(F)  ≤  cγ2(F,ψ2) (see [\cite=MR3184689] for a detailed study on generic chaining and the γ functionals).

Recall that the canonical gaussian process indexed by F consists of centred gaussian random variable Gf, and the covariance structure of the process is endowed by the inner product in L2(μ). Let

[formula]

and note that if the class F  ⊂  L2(μ) is L-subgaussian, that is, if for every [formula],

[formula]

then Λ̃s0,u(F) may be bounded using the canonical gaussian process indexed by F. Indeed, by Talagrand's Majorizing Measures Theorem [\cite=MR906527] [\cite=MR3184689], for every s0  ≥  0,

[formula]

As an example, let [formula] and set [formula] to be the class of linear functionals endowed by V. If X is an isotropic, L-subgaussian vector, it follows that for every [formula],

[formula]

Therefore, if G = (g1,...,gn) is the standard gaussian vector in [formula], [formula] and [formula], one has

[formula]

As the following estimate from [\cite=shahar_multi_pro] shows, Λ̃ can be used to control a multiplier process in a relatively general situation.

For q > 2, there are constants c0, c1,c2,c3 and c4 that depend only on q for which the following holds. Let ξ∈Lq and set ξ1,...,ξN to be independent copies of ξ. Fix an integer s0  ≥  0 and w,u > c0. Then, with probability at least

[formula]

[formula]

It follows from Theorem [\ref=thm:multiplier-intro] that if

[formula]

then with probability at least

[formula]

[formula]

There are other generic situations in which Λ̃s0,u(F) may be controlled using the geometry of F (for example [\cite=MR2899978] [\cite=shahar_multi_pro] when F is a class of linear functionals on [formula] and X is an unconditional, log-concave random vector). However, there is no satisfactory theory that describes Λ̃s0,u(F) for an arbitrary class F; such results are highly nontrivial.

Moreover, because the definition of Λs0,u(F) involves [formula] for every p, class members must have arbitrarily high moments for Λs0,u to be well defined.

In the context of classes of linear functionals on [formula], one expects an analogous result to Theorem [\ref=thm:multiplier-intro] to be true even if the functionals [formula] do not have arbitrarily high moments. A realistic conjecture is that if for each t∈Sn - 1

[formula]

then a subgaussian-type estimate like [\eqref=eq:multi-subgaussian-intro] should still be true.

In what follows we will not focus on such a general result that is likely to hold for every [formula]. Rather, we will concentrate our attention on situations where a subgaussian estimate like [\eqref=eq:multi-subgaussian-intro] is true, but linear functionals only satisfy

[formula]

The obvious example in which only ~   log n moments should suffice is V = Bn1 (or similar sets that have ~  n extreme points). Having said that, the applications that motivated this work require a broader spectrum of sets that only need that number of moments to exhibit a subgaussian behaviour as in [\eqref=eq:multi-subgaussian-intro].

Let X = (x1,...,xn) be an isotropic random vector and assume that [formula] for every 2  ≤  q  ≤  p. If ξ∈Lq0 for some q0 > 2, how small can p be while still having that

[formula]

We will show p  ~   log n suffices for a positive answer to Question [\ref=qu:main] if the norm [formula] satisfies the following unconditionality property:

Given a vector x = (xi)ni = 1, let (x*i)ni = 1 be the non-increasing rearrangement of (|xi|)ni = 1.

The normed space [formula] is K-unconditional with respect to the basis {e1,...,en} if for every [formula] and every permutation of {1,...,n}

[formula]

and if [formula] and x*i  ≤  y*i for 1  ≤  i  ≤  n then

[formula]

This is not the standard definition of an unconditional basis, though every unconditional basis (in the classical sense) on an infinite dimensional space satisfies Definition [\ref=def:K-unconditional] for some constant K (see, e.g., [\cite=MR2192298]).

There are many natural examples of K-unconditional norms, including all the [formula] norms. Moreover, the norm [formula] is 1-unconditional. In fact, if [formula] is closed under permutations and reflections (sign-changes), then [formula] is 1-unconditional. Finally, since the maximum of two K-unconditional norms is K-unconditional, it follows that if [formula] is K-unconditional, so is the norm [formula].

We will show the following:

There exists an absolute constant c1 and for K  ≥  1, L  ≥  1 and q0 > 2 there exists a constant c2 that depends only on K, L and q0 for which the following holds. Consider

If (Xi,ξi)Ni = 1 are independent copies of (X,ξ) then

[formula]

The proof of Theorem [\ref=thm:main-formulation] is based on the study of a conditioned Bernoulli process. Indeed, a standard symmetrization argument (see, e.g., [\cite=LT:91] [\cite=vanderVaartWellner]) shows that if (εi)Ni = 1 are independent, symmetric, { - 1,1}-valued random variables that are independent of (Xi,ξi)Ni = 1 then

[formula]

for an absolute constant C; a similar bound hold with high probability, showing that it suffices to study the supremum of the conditioned Bernoulli process

[formula]

Put [formula] and set [formula], which is a sum of iid random variables. Therefore, if Z = (Z1,...,Zn) then

[formula]

The proof of Theorem [\ref=thm:main-formulation] follows by showing that for a well-chosen constant C(L,q) the event

[formula]

is of high probability, and if the norm [formula] is K-unconditional then

[formula]

Before presenting the proof of Theorem [\ref=thm:main-formulation], let us turn to one of its outcomes - estimates on the random Gelfand widths of a convex body. We will present another application, motivated by a question in the rapidly developing area of Spare Recovery in Section [\ref=sec:sparse].

Let [formula] be a convex, centrally symmetric set. A well known question in Asymptotic Geometric Analysis has to do with the diameter of a random m-codimensional section of V (see, e.g., [\cite=MR827766] [\cite=MR941809] [\cite=MR845980] [\cite=MR3331351]). In the past, the focus was on obtaining such estimates for subspaces selected uniformly according to the Haar measure, or alternatively, according to the measure endowed via the kernel of an m  ×  n gaussian matrix (see, e.g. [\cite=MR1036275]). More recently, there has been a growing interest in other notions of randomness, most notably, generated by kernels of other random matrix ensembles. For example, the following was established in [\cite=MR2373017]:

Let X1,...,Xm be distributed according to an isotropic, L-subgaussian random vector on [formula], set [formula] and put

[formula]

Then, with probability at least 1 - 2 exp ( - c1(L)m)

[formula]

for constants c1 and c2 that depends only on L.

A version of Theorem [\ref=thm:MPT] was obtained under a much weaker assumption: the random vector need not be L-subgaussian; rather, it suffices that it satisfies a weak small-ball condition.

The isotropic random vector X satisfies a small-ball condition with constants κ > 0 and 0 < ε  ≤  1 if for every t∈Sn - 1,

[formula]

The analog of gaussian parameter rG for a general random vector X is

[formula]

Clearly, if X is L-subgaussian then rX(V,γ)  ≤  rG(V,cLγ) for a suitable absolute constant c.

[\cite=MR3364699] [\cite=shahar_general_loss] Let X be an isotropic random vector that satisfies the small-ball condition with constants κ and ε. If X1,...Xm are independent copies of X and [formula], then with probability at least 1 - 2 exp ( - c0(ε)m)

[formula]

Theorem [\ref=thm:main-formulation] implies that if the norm [formula] is K-unconditional, and the growth of moments of the coordinate linear functionals [formula] for 1  ≤  i  ≤  n is L-'subgaussian' up to the level ~   log n, then the small-ball condition depends only on L and rX(V,c1(L))  ≤  rG(V,c2(L,K)). Therefore, with probability at least 1 - 2 exp ( - c0(L)m) one has the gaussian estimate:

[formula]

even though the choice of a subspace has been made according to an ensemble that could be very far from a subgaussian one.

We end this introduction with a word about notation. Throughout, absolute constants are denoted by c,c1..., etc. Their value may change from line to line or even within the same line. When a constant depends on a parameter α it will be denoted by c(α). [formula] means that A  ≤  cB for an absolute constant c, and the analogous two-sided inequality is denoted by A  ~  B. In a similar fashion, [formula] implies that A  ≤  c(α)B, etc.

Proof of Theorem [\ref=thm:main-formulation]

There are two substantial difficulties in the proof of Theorem [\ref=thm:main-formulation]. First, Z1,...,Zn are not independent random variables, not only because of the Bernoulli random variables (εi)Ni = 1 that appear in all the Zi's, but also because the coordinates of X = (x1,...,xn) need not be independent. Second, while there is some flexibility in the moment assumptions on the coordinates of X, there is no flexibility in the moment assumption on ξ, which is only 'slightly better' than square-integrable.

As a starting point, let us address the fact that the coordinates of Z need not be independent.

There exist absolute constants c1 and c2 for which the following holds. Let β  ≥  1 and set p = 2β log (en). If (Wj)nj = 1 are random variables and satisfy that [formula], then for every t  ≥  1, with probability at least 1 - c1t- 2β,

[formula]

Proof.   Let [formula] and by the convexity of t  →  tq,

[formula]

Thus, given (ai)ni = 1, and taking the maximum over subsets of {1,...,n} of cardinality k,

[formula]

When applied to aj  =  Wj, it follows that point-wise,

[formula]

Since [formula] it is evident that [formula] for 2q  ≤  p. Hence, taking the expectation in [\eqref=eq-in-covering-estimate],

[formula]

for q  =  β log (en / k) (which does satisfy 2q  ≤  p). Hence, by Chebyshev's inequality, for t  ≥  1,

[formula]

Using [\eqref=eq:2-in-proof] for k = 2j and applying the union bound, it is evident that with probability at least 1 - 2t- 2β, for every 1  ≤  k  ≤  n,

[formula]

Recall that q0 > 2 and set η = (q0 - 2) / 4. Let u  ≥  2 and consider the event

[formula]

A standard binomial estimate combined with Chebyshev's inequality for |ξ|q0 shows that Au is a nontrivial event. Indeed, and by the union bound for 1  ≤  i  ≤  n, Pr(Au)  ≤  2 / uq0.

The random variables we shall use in Lemma [\ref=lemma:W-j-union-bound] are

[formula]

for u  ≥  2 and 1  ≤  j  ≤  n.

The following lemma is the crucial step in the proof of Theorem [\ref=thm:main-formulation].

There exists an absolute constant c for which the following holds. Let X be a random variable that satisfies [formula] for some p > 2 and set X1,...,XN to be independent copies if X. If

[formula]

then [formula].

The proof of Lemma [\ref=lemma:(p)-estimates] requires two preliminary estimates on the 'gaussian' behaviour of a monotone rearrangements of N copies of a random variable.

There exists an absolute constant c for which the following holds. Assume that [formula]. If X1,...,XN are independent copies of X, then for every 1  ≤  k  ≤  N and 2  ≤  q  ≤  p,

[formula]

Proof.   The proof follows from a comparison argument, showing that up to the p-th moment, the 'worst case' is when X is a gaussian variable.

Let V1,....,Vk be independent, nonnegative random variables and set [formula] to be independent and nonnegative as well. Observe that if [formula] for every 1  ≤  q  ≤  p and 1  ≤  i  ≤  N, then

[formula]

Indeed, consider all the integer-valued vectors [formula], where αi  ≥  0 and [formula]. There are constants [formula] for which

[formula]

and an identical type of estimate holds for [formula]. [\eqref=eq:comp-lemma-1] follows if

[formula]

and the latter may be verified because [formula] for 1  ≤  q  ≤  p.

Let G = (gi)ki = 1 be a vector whose coordinates are independent standard gaussian random variables. If Vi  =  X2i and [formula], then by [\eqref=eq:comp-lemma-1], for every 1  ≤  q  ≤  p,

[formula]

It is standard to verify that

[formula]

and therefore,

[formula]

By a binomial estimate,

[formula]

and if q  ≥  k log (eN / k) and [formula] for u  ≥  1 then

[formula]

Hence, setting q  =  k log (eN / k), tail integration implies that

[formula]

and if q  ≥  k log (eN / k), one has

[formula]

as claimed.

The second preliminary result we require also follows from a straightforward binomial estimate:

Assume that [formula] and let X1,...,XN be independent copies of X. Consider s  ≥  1, 1  ≤  q  ≤  p and 1  ≤  k  ≤  N that satisfies that k log (eN / k)  ≥  q. Then

[formula]

for a constant c(s) that depends only on s.

Proof.   Clearly, for every 1  ≤  i  ≤  N and 2  ≤  r  ≤  p, Hence, if [formula] for u  ≥  2 and r = 3 log (eN / i), then

[formula]

Applying the union bound for every i  ≥  k, it follows that for u  ≥  4, with probability at least 1 - (u / 2)- 3k log (eN / k),

[formula]

On that event

[formula]

and since k log (eN / k)  ≥  q, tail integration shows that

[formula]

Proof of Lemma [\ref=lemma:(p)-estimates]. Recall that q0 = 2 + 4η, that ξ∈Lq0 and that

[formula]

Note that for every [formula] and any integer 0  ≤  k  ≤  N,

[formula]

where the two extreme cases of k = 0 and k = N mean that one of the terms in [\eqref=eq:bernoulli] is 0.

Set r = 1 + η and put θ = 1 / q0. Since (εi)Ni = 1 are independent of (Xi,ξ)Ni = 1 and using the definition of the event Au,

[formula]

By the Cauchy-Schwarz inequality,

[formula]

and

[formula]

Therefore,

[formula]

Also, by Hölder's inequality for r = 1 + η and its conjugate index [formula],

[formula]

and

[formula]

Hence,

[formula]

Let k∈{0,...,N} be the smallest that satisfies k log (eN / k)  ≥  q (and without loss of generality we will assume that such a k exists; if it does not, the modifications to the proof are straightforward and are omitted).

Applying Lemma [\ref=Lemma:com-with-gaussians] for that choice of k,

[formula]

Turning to (**), set [formula] and one has to control

[formula]

for the choice of k as above. By Lemma [\ref=lemma:moments-small-coordinates],

[formula]

Therefore,

[formula]

Combining the two estimates,

[formula]

implying that [formula].

Proof of Theorem [\ref=thm:main-formulation]. By Lemma [\ref=lemma:(p)-estimates], for every 1  ≤  j  ≤  n, [formula], and thus, by Lemma [\ref=lemma:W-j-union-bound], with probability at least 1 - c1t- 2β,

[formula]

Moreover, Pr(Au)  ≥  1 - 2 / uq0; therefore, with probability at least 1 - c1t- 2β - 2u- q0, for every 1  ≤  j  ≤  n,

[formula]

Hence, on that event and because the norm [formula] is K unconditional,

[formula]

for a fixed vector Z0 whose coordinates are [formula]. Observe that [formula], and thus

[formula]

Therefore, by Jensen's inequality, with probability at least 1 - t- 2β - 2u- q0,

[formula]

And, fixing β and integrating the tails,

[formula]

as claimed.

Applications in Sparse Recovery

Spare recovery is a central topic in modern statistics and signal processing, though the problem we describe below is far from its most general form. Because a detailed description of the subtleties of sparse recovery would be unreasonably lengthy, some statements may appear a little vague. For more information on sparse recovery we refer the reader to the books [\cite=MR2807761] [\cite=MR2829871] [\cite=MR3100033], which are devoted to this topic.

The question in sparse recovery is to identify, or at least approximate, an unknown vector [formula], and to do so using relatively few linear measurements. The measurements one is given are 'noisy', of the form

[formula]

X1,...,XN are independent copies of a random, isotropic vector X and ξ1,...,ξN are independent copies of a random variable ξ that belongs to Lq for some q > 2.

The reason for the name "sparse recovery" is that one assumes that v0 is sparse: it is supported on at most s coordinates, though the identity of the support itself is not known. Thus, one would like to use the given random data (Xi,Yi)Ni = 1 and select [formula] in a wise way, leading to a high probability estimate on the error rate [formula] as a function of the number of measurements N and of the 'degree of sparsity' s.

In the simplest recovery problem, ξ = 0 and the data is noise-free. Alternatively, one may assume that the ξi's are independent of X1,...,XN, or, in a more general formulation, very little is assumed on the ξi's.

The standard method of producing [formula] in a noise-free problem and when v0 is assumed to be sparse is the basis pursuit algorithm. The algorithm produces [formula], which is the point with the smallest [formula] norm that satisfies [formula] for every 1  ≤  i  ≤  N.

It is well known [\cite=MR2373017] that if X is isotropic and L-subgaussian, v0 is supported on at most s coordinates and one is given

[formula]

random measurements [formula], then with high probability, the basis pursuit algorithm has a unique solution and that solution is v0.

Recently, it has been observed in [\cite=LM_compressed] that the subgaussian assumption can be relaxed: the same number of measurements as in [\eqref=eq:optimal-gauss-est] suffice for a unique solution if

[formula]

And, the estimate of p  ~   log n happens to be almost optimal. There is an example of an isotropic vector X with iid coordinates for which

[formula]

but still, with probability 1 / 2 the basis pursuit algorithm does not recover even a 1-sparse vector v0 given the same number of random measurements as in [\eqref=eq:optimal-gauss-est].

Since 'real world' data is not noise-free, some effort has been invested in producing analogs of the basis pursuit algorithm in a 'noisy' setup. The most well known among these procedures is the LASSO (see, e.g. the books [\cite=MR2807761] [\cite=MR2829871] for more details) in which [formula] is selected to be the minimizer in [formula] of the functional

[formula]

for a well-chosen of λ.

Following the introduction of the LASSO, there have been many variations on the same theme - by changing the penalty [formula] and replacing it with other norms. Until very recently, the behaviour of most of these procedures has been studied under very strong assumptions on X and ξ - usually, that X and ξ are independent and gaussian, or at best, subgaussian.

One may show that Theorem [\ref=thm:main-formulation] can be used to extend the estimates on [formula] beyond the gaussian case thanks to two significant facts:

then the key to controlling [formula] is the behaviour of

which is precisely the type of process that Theorem [\ref=thm:main-formulation] deals with.

It follows from Theorem [\ref=thm:main-formulation] that if ξ∈Lq for some q > 2, the expectation of [\eqref=eq:multi-in-sparse] is the same as if ξ and X were independent and gaussian. Thus, under those conditions, one can expect the 'gaussian' error estimate in procedures like [\eqref=eq:reg-procedures]. Moreover, because of [\eqref=eq:BP-nec], the condition that linear forms exhibit a subgaussian growth of moments up to p  ~   log n is necessary, making the outcome of Theorem [\ref=thm:main-formulation] optimal in this context.

The following is a simplified version of an application of Theorem [\ref=thm:main-formulation]. We refer the reader to [\cite=LM_reg_comp1] for its general formulation, as well as for other examples of a similar nature.

Let X be an isotropic measure on [formula] that satisfies [formula] for p  ≤  c0 log (n). Set ξ∈Lq for q > 2 that is mean-zero and independent of X and put [formula].

Given an independent sample (Xi,Yi)Ni = 1 selected according to (X,Y), let [formula] be the minimizer of the functional [\eqref=eq:LASSO].

Assume that v0 is supported on at most s coordinates and let 0 < δ < 1. If [formula], then with probability at least 1 - δ, for every 1  ≤  p  ≤  2

The proof of Theorem [\ref=thm:intro-LASSO-est] follows by combining Theorem 3.2 from [\cite=LM_reg_comp1] with Theorem [\ref=thm:main-formulation].