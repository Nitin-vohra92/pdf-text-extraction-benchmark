Random walks with k-wise independent increments

Introduction

Consider a simulation of a simple random walk on a graph. How will the simulation be affected if the source of randomness is not truly random but only pseudo random in the following specific sense, the random bits are k-wise independent for some k > 1 and not independent as a family? The first question to ask is, does the pseudo walk converge to the same stationary measure? The most simple graph to consider might be a cycle of size m. This suggests the following problem: given a random walk [formula], where the Xi are random signs, plus or minus 1 with equal probability, and the Xi's are k-wise independent, what can be said about the behavior of the partial sums and in particular modulo some fixed number m? It turns out that there is a fundamental difference between the cases k = 2 and k > 2.

Examine first the case k = 2. For this case we shall show

There exists a sequence of random variables [formula] taking values ±  1, pairwise independent, such that Sn is bounded almost surely.

This result should be contrasted against the fact that we know that [formula] and [formula], just like in the completely independent case. In other words, Sn for large n must have extreme "fat tail" behavior. Naturally, M: =  max nSn satisfies [formula]. The example we will demonstrate is essentially the Walsh system, which is pairwise independent. We will discuss this in section [\ref=sec:2in].

Such behavior cannot occur when k  ≥  4 for the simple reason that in this case we know that [formula] and this gives

[formula]

We could not settle k = 3,

Is there a sequence of random variables [formula] taking values ±  1 with equal probability 3-wise independent, such that Sn is bounded almost surely.

The higher k is, the more moments we know and we can approximate the large scale shape of Sn. However, this does not necessarily mean we can say something about Sn mod m. And indeed, in section [\ref=sec:Proofkm] we show the following

Let m and k be some natural numbers, and let ε > 0. Then there exists a sequence of random variables [formula] taking values ±  1, k-wise independent, and a sequence Ij such that

[formula]

Notice that the requirement that the condition holds only for some Ij is unavoidable, since, say, if k  ≥  10m2 then the distribution of SIj + 10m2 is approximately uniform, since [formula] are independent.

Explicit constructions of k-wise independent 0-1 probability spaces and estimates on their sizes are the focus of several papers in combinatorics and complexity, see e.g. [\cite=Jo] for the first construction and [\cite=AS] for a recent discussion with additional references. Sums of pairwise independent random variable were extensively studied, see e.g. [\cite=Ja] for some interesting examples. Thus there are already many interesting examples of pairwise independent processes. But it seems behavior modulo m was not studied.

pairwise independent processes

We term the construction we use a "gray walk", as it is based on the well-known Gray code construction in combinatorics. The nth Gray code is a Hamiltonian path on the discrete n-cube, or a listing of all 2n binary strings of length n, starting with the string 00...0, where every string appears exactly once, and any two consecutive strings differ in exactly one place. The construction (and hence also proof that this is possible) is done recursively, namely: To construct the nth Gray code, write down two copies of the (n - 1)th code, where the order of the strings in the second copy is reversed, and add to the first 2n - 1 strings a zero at the nth place, and to the second 2n - 1 strings a one at the nth place.

The Gray codes of all orders n can be combined into an infinite Gray code, by listing them sequentially for increasing n's, and converting all strings into infinite strings by padding with zeros. The first few strings in the infinite code are:

To construct the gray walk, we now consider each string Ai as specifying a finite subset (also denoted Ai) of the natural numbers [formula], where a 1 in the jth place signifies that j∈Ai, and define

[formula]

where ξ1,ξ2,... is a seed sequence of independent ±  1 variables. It is easy to verify that the Xi's are pairwise independent. Therefore to finish theorem [\ref=thm:2bnd] we only need

For simplicity we add the element [formula], and prove that for [formula] we have sup Sn' =  -  inf Sn' = 2min {j  ≥  1:ξj =  - 1} - 1. The recursive definition of the Gray code is reflected in the path: Assume we have defined the first steps S0',S1',S2',...,S2j - 1 - 1' of the walk, then the next 2j - 1 steps will be the previous steps listed in reverse order, and multiplied by the random sign ξj. This implies that the path up to time 2j - 1, where j is the first value for which ξj =  - 1, is 0,1,2,3,4,...,2j - 1,2j - 1 - 1,2j - 1 - 2,...,3,2,1,0, and all the subsequent values are bounded between - 2j and 2j.

Proof of theorem [\ref=thm:oneL]

Before embarking on the general proof, let us demonstrate the case m = 4 which is far simpler. Let L > k satisfy [formula] mod 4. Let [formula] be a sequence of i.i.d ±  1 variables. Next define new variables by conditioning the ξi-s:

[formula]

The fact that L > k clearly shows that the Xi's are k-independent. The fact that [formula] shows that in each block of size L the number of - 1's is even, and since 4|L we get that [formula] mod 4 for every block. Therefore if we define Ij = jL then the condition in ([\ref=eq:thm]) is actually satisfied combinatorially and not only in high probability.

How can we generalize this to m  ≠  4? The remaining ingredient in the proof is based on the following well known fact:

The algorithm is simple: take two such variables: if they give 11, "output" 1, if they give 1, - 1 "output" 2 and if they give - 1,1, "output" 3. If they give - 1, - 1, take a new set of two variables and repeat the process. The output is 1, 2 or 3, each with probability [formula]. The time required for each step is unbounded but the probability that it is large is exponentially small.

The proof below combines these two ideas, which we nickname "generating hidden symmetries" and "simulating uniform variables" to get the result.

We may assume without loss of generality that m is even (otherwise take 2m instead). Let λ  =  λ(k,ε) be some sufficiently large parameter which we shall fix later. Let [formula] where ⌈  ·  ⌉ stands, as usual, for the upper integer value.

Define [formula]. The first step is to divide {  ±  1}N according to the sum modulo m, and trim the resulting sets a little so that they all have the same size. Precisely, let

[formula]

We note that the distribution of [formula] modulo m is uniform on the set [formula] with an error of Ce- cλ. Therefore [formula]. For [formula], let Gj be arbitrary sets satisfying

[formula]

and let [formula].

So far we have constructed some general objects having very little to do with probability. Next, let [formula] be i.i.d variables taking values in ±  1, and let [formula] be a division of the ξi's into blocks of size N: [formula]. Let

[formula]

We define

[formula]

Properties [\ref=enu:Xmularge] and [\ref=enu:sumxmu] are obvious from the construction. Property [\ref=enu:LmuL] is a direct consequence of the estimate [formula], if only λ is large enough. Define λ so as to satisfy this condition. Therefore we need only prove property [\ref=enu:Xmukindep].

Let therefore [formula] and [formula] and examine the events

[formula]

We know that [formula] and we need to show that [formula]. Let b be some number and let [formula] be a set with #  σ = b - (k + 1) and examine the event

[formula]

The point of the proof is that the event Y is independent of [formula]. This is because X depends only on k places, but Y depends on k + 1 Ξi's each of which is distributed uniformly. In other words

[formula]

or

[formula]

which gives

[formula]

Returning to the proof of the theorem, we let

[formula]

be an independent family of couples of variables constructed using the lemma. We now construct our sequence Xi inductively as follows: assume that at the νth step we have defined [formula] where ρ  =  ρ(ν) is random. Define [formula] mod m and then define [formula] using

[formula]

and ρ(ν + 1) = ρ(ν) + Lμ,ν. This creates a sequence of ±  1 variables. To see ([\ref=eq:thm]), define r(a) =  max {ν:ρ(ν) < aL} and properties [\ref=enu:LmuL] and [\ref=enu:sumxmu] of the lemma will give that

[formula]

Therefore we need only show that the Xi's are k-wise independent. While being strongly related to the k-wise independence of the yμ,νi it is not an immediate consequence of it because of the dependence between the yμ,νi and the Lμ,ν's.

We prove that the Xi's are k-wise independent inductively. Let l and I be some integers and assume that we have shown that [formula] are independent for every m < l and for m = l and i1 < I. Let [formula] and let [formula]. Examine L0,1 and define

[formula]

The induction hypothesis, together with the independence of the various Yμ,ν's shows that [formula] are i.i.d ±  1 variables: if n > 0 then this follows from the induction hypothesis for m': = m - n while if n = 0 then it follows from the induction hypothesis for m' = m and I': = I - L0,1. Property [\ref=enu:Xmularge] of the lemma shows that [formula] are i.i.d ±  1 variables. Below n we have simple equality:

[formula]

Therefore

[formula]

and summing over L0,1 we get the required result:

[formula]

which finishes the proof of the theorem.

Let m and k be some natural numbers. Then there exists a sequence of random variables [formula] taking values ±  1, k-wise independent, and a sequence Ij such that

[formula]

The proof is similar to that of theorem [\ref=thm:oneL], but using a different L in each step. However there are some additional technical subtleties. Essentially, if in the main lemma of the previous theorem we defined a variable Yμ where the parameter μ was used to "return to sync" the series if ever the congruence is no longer 0, here we would need variables Yμ,τ where τ is some additional parameter needed to "return to sync" in the L domain. While being only moderately more complicated, we felt it better to present the simpler proof of the previous theorem.

Further related problems

We end by suggesting further problems related to k-wise independent random variables.

The random sign Pascal triangle

Let ξ+n,k,ξ-n,k be a family of i.i.d random (1/2-1/2) signs for - 1  ≤  k  ≤  n + 1. Define random variables Xn,k for - 1  ≤  k  ≤  n + 1 by the recurrence

[formula]

[formula]

Problems: Study the behavior of the central coefficient X2n,n. Find an interpretation of the triangle as "percolation with interference" where two closed gates cancel each other out ("quantum percolation"?) Study the behavior of the non-central coefficients.

2D Percolation

What about other functions of k-wise independent random variables? For example, instead of considering Bernoulli percolation, the random variables determining the configuration are only k-wise independent. Here is an "unimpressive" example: k-wise independent bond percolation on an n  ×  n(k + 1)- box in [formula] such that the probability to have a crossing of the narrow side is 1. Of course, even in usual percolation the probability is 1 - e- ck so that the improvement is not exciting.

The construction is as follows: divide into k + 1 disjoint boxes of size n  ×  n. Take the usual independent percolation, conditioned such that the number of boxes with crossings is odd. This is like conditioning k + 1 Bernoulli variables to have an odd sum, so that you get a k-wise independent structure. If the number is odd, then it is non zero. Can it be improved, say to guarantee crossing of the n  ×  n-box?