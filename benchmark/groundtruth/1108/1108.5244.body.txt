8mm

Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions

Shuichi Kawano

8mm

Introduction

In recent years, with the wide availability of fast and high-powered computers, high-throughput data of unexampled size and complexity have frequently been seen in the contemporary statistics and machine learning. Examples involve data from genomics, proteomics, natural language processing, and signal processing. For the huge amount of data, it is difficult to label data by a human operator, since its work requires vast times and efforts. Only small labeled data set may, therefore, be available, while an unlabeled data set can be more easily obtained. Under such a circumstance, a classification method that combines both labeled and unlabeled data, called semi-supervised learning, has received an enormous amount of attention in the late machine learning and statistical literature (see, e.g., Chapelle et al., 2006; Liang et al., 2007). For overviews of semi-supervised learning methods, we refer to Zhu (2008), and references given therein.

Many classification techniques for semi-supervised learning have been proposed by various researchers, e.g., Amini and Gallinari (2002), Basu et al. (2004), Bennett and Demiriz (1998), Chen and Wang (2007), Dean et al. (2006), Kawano and Konishi (2011), Kawano et al. (2012), Lafferty and Wasserman (2007), and Zhou et al. (2004). Most of these semi-supervised methods implicitly assumes that a density function for labeled data is the same as that for unlabeled data. On the other hand, we, here, consider the case that the densities for labeled data and unlabeled data are different, since the densities are not always same in practical situations. In such a case, several semi-supervised methods have been presented, e.g., Jiang and Zhai (2007), Wu et al. (2009), and Zadrozny (2004). However, for these methods, there remains a problem of evaluating constructed semi-supervised models, which is a crucial issue in the model building process. Cross validation (CV) is often used in evaluating models constructed by semi-supervised procedures. An advantage of CV lies in its independence from probabilistic assumptions. The computational time of the procedures is, however, very large, and the high variability and tendency to undersmooth in CV are not negligible in the analysis of complex or high-dimensional data, since the selectors are repeatedly applied.

In this paper, we propose a logistic model for the semi-supervised classification problem by using statistical methods under covariate shift (Shimodaira, 2000) in the case that the density function for labeled data is different from that for unlabeled data. The unknown parameters in the model are estimated by the regularization method with the help of EM algorithm. A crucial issue in our modeling strategy is to choose values of some tuning parameters included in semi-supervised logistic models, which corresponds to evaluating models determined by our proposed procedures. In order to objectively select optimal values of tuning parameters, we then introduce a model selection criterion based on an information-theoretic approach (Konishi and Kitagawa, 1996) that evaluates the semi-supervised logistic models estimated by the regularization method. Some numerical examples demonstrate that the proposed procedure works well and performs better than competing methods.

This paper is organized as follows. In Section 2, we present a semi-supervised logistic model for classification problem based on covariate shift adaptation and its estimation procedure by the regularization method. Section 3 provides a model selection criterion derived from an information-theoretic viewpoint to select some tuning parameters in semi-supervised logistic models. In Section 4, Monte Carlo simulations and benchmark data analysis are given to assess the performances of our proposed semi-supervised logistic discrimination. Some concluding remarks are given in Section 5.

Semi-supervised logistic modeling from different sampling distributions

Linear logistic modeling for semi-supervised learning

We review here semi-supervised linear logistic models developed by early researchers (e.g., Amini and Gallinari, 2002; Vittaut et al., 2002). Suppose that we have an n1 labeled data set [formula] and an (n  -  n1) unlabeled data set [formula], where [formula] denotes a p-dimensional explanatory variable and Yα is a random variable taking values 0 or 1 with probabilities

[formula]

Note that logistic models are first constructed by only labeled data set, while the unlabeled data set is used in estimating the parameters involved in the logistic models.

Using conditional probabilities in Equation ([\ref=posterior_probability]) and the labeled data set, a linear logistic model (see, e.g., Hastie et al., 2009) is formulated by

[formula]

where [formula] is an unknown parameter vector and [formula]. Hereafter, we denote conditional probabilities by [formula], since the conditional probabilities depend on the parameter vector [formula]. It follows from Equation ([\ref=linear_logistic_model]) that conditional probabilities can be rewritten as

[formula]

Also, a probability function of the random variable Yα is the Bernoulli distribution in the form

[formula]

Under the linear logistic model, the log-likelihood function for yα in terms of [formula] is induced into

[formula]

The unknown parameter [formula] included in the logistic model is usually estimated by maximizing the log-likelihood function with respect to the parameter. The procedure is known as the supervised learning, i.e., the parameter is determined by using only labeled data set. Since we have an additional unlabeled data set, the parameter should be estimated by both labeled and unlabeled data set, which is called the semi-supervised learning. Thereby, Amini and Gallinari (2002) proposed a log-likelihood function with additional unlabeled data given by

[formula]

where [formula] is a latent variable coded as 0 or 1. Amini and Gallinari (2002) estimated the parameter by maximizing the Equation ([\ref=linear_semi-supervised]) with the technique of EM algorithm, while Kawano and Konishi (2011) employed the Equation ([\ref=linear_semi-supervised]) with a regularization term in estimating the parameter in the context of nonlinear logistic models based on basis expansions.

Given the estimate [formula], we assign a future observation [formula] into class j (j  =  0,1) that has the maximum conditional probability in the Equation ([\ref=posterior_prob_2]).

Semi-supervised logistic model for different distributions

Logistic models for semi-supervised learning described in Section [\ref=Linear_logistic_modeling_for_semi-supervised_learning] usually assumes that a density function for the labeled data set is the same as that for the unlabeled data set, i.e., when we denote that [formula] is a probability density function of explanatory variables for the labeled data and [formula] is that for the unlabeled data, [formula]. Our aim in this section is to construct logistic models under the situation that a density for the labeled data set is different from that for the unlabeled data set, i.e., [formula].

We recall the log-likelihood function for logistic models with unlabeled data in Equation ([\ref=linear_semi-supervised]). For the log-likelihood function, we propose a weighted log-likelihood function with unlabeled data in the form

[formula]

where γ1,γ2∈[0,1] are tuning parameters. If both γ1 and γ2 are 0, the log-likelihood function in Equation ([\ref=semi_cov_lambda]) coincides with that in Equation ([\ref=linear_semi-supervised]). Note that the weight on the first term, [formula], is bigger near high densities of unlabeled data compared to those of labeled data, while that on the second term, [formula], is strengthen near high densities of labeled data compared to those of unlabeled data. Hence, the log-likelihood function on the first term is highly weighted near high densities of unlabeled data compared to those of labeled data, while that on the second term has high weighting near high densities of labeled data compared to those of unlabeled data. An idea of the weight, the ratio of [formula] and [formula], arises from a statistical inference under covariate shift (Shimodaira, 2000). In the semi-supervised learning, employing a ratio of densities in log-likelihood functions is not new. For example, Kawakita and Kanamori (2012), Sokolovska et al. (2008), and Zou et al. (2007) use a ratio of densities in the semi-supervised inference. However, the Equation ([\ref=semi_cov_lambda]) is a novel formulation in the semi-supervised context.

The Equation ([\ref=semi_cov_lambda]) includes unknown values of ratios, [formula] and [formula], which are to be estimated. Various researchers address the problem of estimating the ratios by using several methods of statistics or machine learning (Bickel et al., 2009; Huang et al., 2007; Kanamori et al., 2009; Sugiyama et al., 2008; Sugiyama and Kawanabe, 2012; Sugiyama et al., 2012). In this paper, we employ a uLSIF method proposed by Kanamori et al. (2009) in determining values of the ratios, where the determination is performed before estimating the parameter [formula]. Also, a source code of the method uLSIF is available in http://www.math.cm.is.nagoya-u.ac.jp/~kanamori/software/LSIF. We do not follow details of the density ratio estimation procedure by the uLSIF method, since these are not our focus in this paper. For readers that are interested in the topics, we refer to Kanamori et al. (2009), and Sugiyama and Kawanabe (2012).

Parameter estimation via regularization

In estimating parameters in logistic models, the log-likelihood function often diverges to infinity when the maximum likelihood method is applied (Konishi and Kitagawa, 2008). Hence, the parameter vector [formula] in Equation ([\ref=semi_cov_lambda]) is estimated by the regularization method. The regularization method is to maximize a following regularized log-likelihood function

[formula]

where λ is a regularization parameter that has positive values and K  =  diag(0,Ip) is a (p + 1)  ×  (p + 1) matrix. Here, the matrix Ip is a p-dimensional identity matrix.

It is not easy to optimize the parameter involved in Equation ([\ref=regularized_weighted_likelihood]), since the latent variables [formula] are unobserved. Hence, we employ an EM-based algorithm developed by Kawano and Konishi (2011) as follows:

is satisfied, where (k) is the value of [formula] after the k-th EM iteration and ε is an arbitrary small number (e.g., 10- 5).

It follows from these procedures that we obtain a statistical model in the form

[formula]

Note that the statistical model is constructed by using both labeled data and unlabeled data.

Model selection criterion

The statistical model in Equation ([\ref=statistical_model]) contains some adjusted parameters including two tuning parameters γ1,γ2 in the weighted log-likelihood function and the regularization parameter λ. Regarding the selection of these adjusted parameters as that of candidate models, we introduce a model selection criterion from an information-theoretic approach.

Let [formula] be n1 observations drawn randomly from an unknown probability distribution function G(y|x) having a density function g(y|x). On the other hand, we assume that n1 observations for explanatory variables [formula] are non-random; i.e., [formula] are fixed (for details of this assumption, we refer to Konishi and Kitagawa, 2008). Under these settings, we derive a model selection criterion from the viewpoint of information theory.

Suppose that [formula] are future observations for the response variable generated from g(y|x). Let [formula] and [formula], where G is an estimator of the parameter by any estimation procedures, [formula], and [formula] are weights that depend on explanatory variables [formula], which satisfy [formula]. Note that the weights [formula] are fixed, since we assume that [formula] are non-random. Then Irizarry (2001) implicitly proposes a following Kullback-Leibler information in order to measure the divergence of the statistical model with weights from the true distribution:

[formula]

The best model can be regarded as the best minimizer of the Kullback-Leibler information (Irizarry, 2001). Since the first term of Equation ([\ref=KLdiv]) does not depend on the models with the estimator G, we have only to consider the second term of Equation ([\ref=KLdiv]). Therefore, we focus on maximizing the second term of Equation ([\ref=KLdiv]) which leads to the minimization of the Kullback-Leibler information.

By introducing an estimator of the second term of Equation ([\ref=KLdiv]), a model selection criterion is, generally, given by

[formula]

where IC stands for information criterion and (G) is an estimator of the bias b(G) in the following:

[formula]

Suppose that the estimator M of the parameter is an M-estimator defined as the solution of the following implicit equation:

[formula]

with [formula] being referred to as [formula]-function (e.g., see, Huber, 2004). Using the idea of Konishi and Kitagawa (1996), we derive a model selection criterion for the statistical models with the M-estimator M in the form

[formula]

where Q(M) and R(M) are given by

[formula]

In our models, the estimator [formula], which maximizes the regularized log-likelihood function in Equation ([\ref=regularized_weighted_likelihood]), can be regarded as an M-estimator. Here, we set the [formula]-function of the estimator into

[formula]

Note that the [formula]-function in Equation ([\ref=psi-function]) is actually incorrect since the estimator [formula] is obtained by maximizing the Equation ([\ref=regularized_weighted_likelihood]) with respect to the parameter; i.e., the estimator are constructed by using both labeled and unlabeled data. However, [formula]-functions in the context of model selection criteria must be given by a regularized or non-regularized log-likelihood function with incomplete data; i.e., the functions does not include latent variables (for details, see, Hirose et al., 2008). Hence, we employ the [formula]-function in Equation ([\ref=psi-function]) in order to derive a model selection criterion.

By using the [formula]-function in Equation ([\ref=psi-function]) and substituting [formula] for the weights [formula], we introduce a generalized information criterion (GIC) for evaluating our proposed semi-supervised logistic models estimated by the regularization method. The model selection criterion is given by

[formula]

where the matrices Q() and R() are

[formula]

Here, [formula] is an n1-dimensional vector of which the elements are all one, and In1 is an n1-dimensional identity matrix. Also, X,Ŵ,Λ̂, and Π̂ are, respectively, given by

[formula]

Note that the GIC in Equation ([\ref=GIC]) seemingly appears not to depend on all adjusted parameters (in particular, γ2). However, the GIC implicitly includes the adjusted parameters (λ,γ1,γ2), since the estimator [formula] depends on all adjusted parameters.

We choose the adjusted parameters from the minimizer of the GIC in Equation ([\ref=GIC]).

Numerical studies

We studied some numerical examples to show the efficiency of our proposed modeling strategy. Two types of Monte Carlo simulations and benchmark data analysis are given to illustrate the proposed semi-supervised logistic discrimination.

Simulation 1

We investigated the effectiveness of the proposed modeling procedures through Monte Carlo simulations. In this simulation study, we generated data sets [formula] as labeled data and [formula] as unlabeled data. In labeled data, (x1α,x2α) were generated by a normal distribution [formula], and yα was generated according to a following conditional probability

[formula]

Meanwhile, unlabeled data (x1α,x2α) were obtained by a normal distribution N(( - 0.4,1 -  sin ( sin (0.42π)))T,diag(0.05,1)). Test data [formula] were generated as follows. First, (x1α,x2α) were derived by a mixture of labeled and unlabeled data, where the mixing rate is equal (that is, 0.5). Second, for the (x1α,x2α), yα was obtained according to the conditional probability in Equation ([\ref=conditional_probability]). We assumed that labeled data sizes (n) were 25, 50, 100, 150, 200, and 250.

We fitted our semi-supervised logistic regression model to the data sets. Note that the density ratio estimation procedure by the uLSIF method described in Section 2.2 is not performed in these simulation trials, since the density ratio is exactly calculated. The simulation results were obtained by averaging over 50 repeated Monte Carlo trials. For each data set, we computed averages of prediction error rates (PE) for 50 iterations. The tuning parameters in our models were selected by using the GIC in Equation ([\ref=GIC]). For 50 trials, we computed averages of selected adjusted parameters. The results are summarized in Table [\ref=Simulation1]. From the table, in the selection of adjusted parameters, the values of the tuning parameter γ1 are 0.10 in all cases, while those of the parameter γ2 increase with the increasing numbers of labeled data. The regularization parameter λ takes smaller values according to the increasing numbers of labeled data.

We compared the performances of the proposed semi-supervised methodologies (SSLRCS: semi-supervised logistic regression under covariate shift) with those of semi-supervised method proposed by Amini and Gallinari (2002) (LSSLR: linear semi-supervised logistic regression), which is developed under the condition that density functions for labeled and unlabeled data are same, and supervised linear logistic discriminant analysis (SLR: supervised logistic regression). Note that the SLR is constructed by using only labeled data. Semi-supervised and supervised logistic modeling strategies were applied into the data sets. The LSSLR and the SLR include a tuning parameter, respectively, where we denote the tuning parameters as ξ1 and ξ2, respectively. The parameter is determined by the GIC, where the GIC for LSSLR is obtained by setting [formula] in Equation ([\ref=GIC]) and that for SLR is given by Ando et al. (2008). For these methods, we also computed averages of prediction error rates and selected tuning parameters. It may be seen from Table [\ref=Simulation1] that SSLRCS is superior to other methods (LSSLR and SLR) in all cases in the sense that the proposed method gives smaller prediction error rates.

Simulation 2

We simulated three data sets given in Chakraborty (2011) to examine the performances of our proposed modeling strategy. For each of the simulation cases, we generated 100 data points in the labeled data set, 1000 data points in the unlabeled data set, and 1000 data points in the test data set. Using the data sets, we constructed the SSLRCS, the LSSLR, and the SLR. We repeated the procedure 50 times. Our simulation settings are given as follows (for details, see, Chakraborty (2011, p. 76)):

Case 1 : In the labeled data set, generate [formula] given by xi  ~  N(2,1) (i = 1,2) for Class 1 and xi  ~  N( - 2,1) (i = 1,2) for Class 2. In the unlabeled data set, xi  ~  N(2,2) (i = 1,2) for Class 1 and xi  ~  N( - 2,2) (i = 1,2) for Class 2. In the test data set, xi  ~  0.5N(2,1)  +  0.5N(2,2) (i = 1,2) for Class 1 and xi  ~  0.5N( - 2,1)  +  0.5N( - 2,2) (i = 1,2) for Class 2.

Case 2 : Generate [formula] given by [formula] for Class 1 and [formula] for Class 2.

Case 3 : Generate [formula] given by xi  ~  N(5,2) (i = 1,2) for Class 1 and xi  ~  N(8,2) (i = 1,2) for Class 2 in the labeled data set. In the unlabeled data set, xi  ~  N(6,2) (i = 1,2) for Class 1 and xi  ~  N(9,2) (i = 1,2) for Class 2. In the test data set, xi  ~  0.5N(5,2)  +  0.5N(6,2) (i = 1,2) for Class 1 and xi  ~  0.5N(8,2)  +  0.5N(9,2) (i = 1,2) for Class 2.

The results from the simulation studies are in Table [\ref=Simulation2]. We obtained the values in the table by averaging over 50 trials. The optimal tuning parameters selected by our model selection criterion were 1.00 for γ1 in all situations, 0.102 and 0.106 for γ2 in Case 1 and Case 2, 3, respectively, and 10- 2.50 and 10- 1.98 for λ Case 1 and Case 2, 3, respectively. From the simulation results, we observe that our proposed procedure performs well in all cases with respect to minimizing prediction error rates even though Case 2 is an ordinary setting of semi-supervised learning, i.e., the density function for labeled data is same as that for unlabeled data. Hence, we conclude that our proposed method may be useful even if the densities for labeled and unlabeled data are same.

Benchmark data analysis

Thorough analyzing the g10 data set (Chapelle and Zien, 2005), the ionosphere data set (Sigillito et al., 1989), and the pima data set (Ripley, 1996), we illustrated the effectiveness of the proposed semi-supervised methodology. The g10 data set includes 550 data points with 10 predictors, and we prepared 250 training data points and 300 test data points. The ionosphere data set consists of 356 data points with 33 predictors, and we split the whole 356 data points into 150 training data points and 206 test data points. The pima data set, which consists of 300 training data points and 232 test data points, is a binary classification with 7 predictors. In order to implement the semi-supervised procedure, the training data points were randomly split into two halves with labeled data points and unlabeled data points, where labeled data points were assigned as 5%, 10%, 20%, 30%, 40%, and 50% for training data points, respectively. We repeated the random splitting 50 times. We also compared our proposed method (SSLRCS) with the LSSLR and the SLR, which are described in Section 4.1.

Table [\ref=benchmark] shows the summary of the prediction errors and selected adjusted parameters for the benchmark data sets. The values in the table were averaged over 50 repetitions. From the results, we observe that the tuning parameter γ1 provides the largest values (i.e., 1.00) in almost all cases, while the parameter γ2 gives relatively smaller values (i.e., from 0.10 to 0.40). We also find that our proposed procedure outperforms the previously proposed methods in almost all situations, although it is unclear that whether densities for labeled and unlabeled data are different. In particular, the proposed method seems to work well when the number of labeled data points is small.

Concluding remarks

We proposed a semi-supervised logistic classification methodology for different density functions of labeled and unlabeled data along with the technique of covariate shift adaptation and regularization. A crucial point for our semi-supervised modeling processes includes the choices of some tuning parameters in our proposed models. We introduced a model selection criterion from the viewpoint of information theory in order to select the values of the adjusted parameters. Through Monte Carlo simulations and the benchmark data analysis, we showed that our modeling strategy is effectiveness in practical situations in the viewpoints of yielding relatively lower prediction errors than previously developed methods. Our modeling procedure may be applied into the problem of constructing a nonlinear semi-supervised classification method based on basis expansions, which will be discussed in another paper.

Acknowledgement This work was supported by the Ministry of Education, Science, Sports and Culture, Grant-in-Aid for Young Scientists (B), #  24700280, 2012-2015.