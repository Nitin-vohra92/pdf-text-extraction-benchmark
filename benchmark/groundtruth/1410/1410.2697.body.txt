A Fast and Memory Efficient Sparse Solver with Applications to Finite-Element Matrices

Introduction

In many engineering applications, we are interested in solving a set of linear equations:

[formula]

where A is a symmetric positive definite stiffness matrix arising from a finite element discretization of an elliptic PDE, and b is a forcing vector, associated with the inhomogeneity in the PDE. Iterative methods are widely popular in solving such equations. However, the main difficulty with these methods is that they require a preconditioner and convergence is not guaranteed. Direct methods on the other hand are very robust but are generally slower and more memory demanding. In this article, we present an accelerated multifrontal solver that we use as a preconditioner to a generalized minimal residual (GMRES [\cite=saad1986gmres]) iterative scheme to achieve a desired accuracy. This approach combines the robustness of direct solvers with the speed of iterative solvers to arrive at a fast overall solver for sparse finite element matrices.

Accelerating the multifrontal direct solve algorithm has been the subject of many recent research articles [\cite=xia2009superfast] [\cite=MFGeneralMesh] [\cite=GeneralizedMF] [\cite=randomizedMF] [\cite=BLRMF]. For a detailed summary and overview of such algorithms see [\cite=BlackBox_HODLR]. The general idea behind most of these methods is approximating dense frontal matrices arising in the multifrontal elimination process with an off-diagonal low-rank matrix structure. The off-diagonal low-rank property leads to more efficient factorization and storage compared to dense BLAS3 operations if the rank is sufficiently small. The methods described in [\cite=xia2009superfast] [\cite=MFGeneralMesh] [\cite=GeneralizedMF] [\cite=randomizedMF] approximate the frontal matrix with a hierarchically semiseparable (HSS) matrix, while [\cite=BLRMF] approximates the frontal matrix with a block low-rank (BLR) matrix.

In this article, we accelerate the multifrontal algorithm by approximating dense frontal matrices as hierarchically off-diagonal low-rank (HODLR) matrices. Compared to HSS structures which have been widely used in approximating dense frontal matrices, HODLR matrices are much simpler as they lack the nested off-diagonal basis. For 3D PDEs, we find that the rank used to approximate the off-diagonal blocks increases with the size of the block with [formula], where r is the rank and n the size of the block. This results in a geometric increase of the rank with the HODLR level. As a result of this increase, as we demonstrated in [\cite=BlackBox_HODLR], the factorization cost is the same for both HODLR and HSS structures, namely O(r2n), where r is the rank at the top of the tree. This is despite the fact that HSS uses a more data-sparse format. The reason why the difference in the basis does not affect the asymptotic cost is because the cost is dominated by the computation at the root of the HODLR tree, for the largest block.

In addition, HODLR is advantageous compared to HSS during the low-rank approximation phase, since it does not need to produce a nested basis, which simplifies many steps in the algorithm. Hence, in most practical applications, HSS may not have a clear advantage over HODLR.

Furthermore, we will demonstrate that the combination of HODLR and the boundary distance low-rank approximation method (BDLR) [\cite=BlackBox_HODLR] leads to a very fast and simple extend-add algorithm, which results in an overall fast multifrontal solver.

At the time of writing this article, only Xia [\cite=randomizedMF] has demonstrated a fast and memory efficient multifrontal solver for general sparse matrices, with an asymptotic cost in O(N4 / 3 log N) where N is the size of the sparse matrix. In contrast, the method in this paper leads to an overall cost of O(N4 / 3). This cost may be compared with an LU factorization with nested dissection, with cost O(N2) in 3D.

In this article, we introduce a fast multifrontal solver that is much simpler compared to [\cite=randomizedMF], and demonstrate its performance for large and complicated test cases. The method is shown to be advantageous compared to traditional preconditioners like ILU.

Review of Important Concepts

We now review two concepts that are central to the fast sparse solver algorithm. Namely, hierarchical off-diagonal low-rank (HODLR) matrices and the boundary distance low-rank approximation method (BDLR).

Hierarchically Off-Diagonal Low-Rank (HODLR) Matrices

Hierarchical matrices are data sparse representation of a certain class of dense matrices. This representation relies on the fact that these matrices can be sub-divided into a hierarchy of smaller block matrices, and certain sub-blocks can be efficiently represented as a low-rank matrix. We refer the readers to [\cite=hackbusch1999sparse] [\cite=hackbusch2000sparse] [\cite=grasedyck2003construction] [\cite=hackbusch2002data] [\cite=borm2003hierarchical] [\cite=ULV] [\cite=chandrasekaran2006fast1] [\cite=BlackBox_HODLR] for more details. Ambikasaran at al. [\cite=ambikasaran2013thesis] provides a detailed description of these different hierarchical structures. In this article, we use the simplest hierarchical structure, namely the hierarchically off-diagonal low-rank matrix (HODLR), to approximate the dense frontal matrices that arise during the sparse elimination process. As shown in [\cite=BlackBox_HODLR], the HODLR structure reduces the dense factorization and storage cost from O(n3) and O(n2) to O(r2n) and O(rn) respectively, where n is the size of the dense matrix and r is the off-diagonal rank.

An HODLR matrix has low-rank off-diagonal blocks at multiple levels. As described in [\cite=SivaFDS], a 2-level HODLR matrix, [formula], can be written as shown in Eq. [\eqref=eq:HODLR2]:

[formula]

where for a p-level HODLR matrix, [formula], U(p)2i - 1, U(p)2i, V(p)2i - 1,2i, [formula] and r  ≪  n. Further nested compression of the off-diagonal blocks will lead to an HSS structure [\cite=SivaFDS].

Boundary Distance Low-Rank Approximation Method (BDLR)

In order to take advantage of the off-diagonal low-rank property, we need a fast and robust low-rank approximate method. More precisely, we need a low rank approximation method that has the following properties:

We want our method to be applicable to general sparse matrices. Hence, we need a low-rank approximation scheme that is purely algebraic (black-box). That is, we can not use analytical low-rank approximation methods like Chebyshev, multipole expansion, analytical interpolation, etc.

In order to obtain speedup compared to conventional multifrontal solvers, we need a fast low-rank approximation scheme that has a computational cost of O(rn) where n and r are the size and rank of a dense low-rank matrix respectively. Hence, we cannot use traditional low-rank approximation methods like SVD, rank revealing LU or rank revealing QR as they have a computational cost of O(n3), O(n2) and O(n2) respectively.

We need a robust and efficient low-rank approximation method that is applicable to a wide variety of problems.

One possible option is to use randomized algorithms [\cite=RndSummary] [\cite=Rnd1] [\cite=Rnd2] [\cite=Rnd3] similar to Xia [\cite=randomizedMF]. However, such algorithms require the implementation of a fast matrix-vector product. For our purpose, as we demonstrated in [\cite=BlackBox_HODLR], the boundary distance low-rank approximation method (BDLR) is a fast and robust scheme that results in very fast solvers for both structured and unstructured meshes.

BDLR is a pseudoskeleton [\cite=pseudoSkeleton] like low-rank approximation scheme that picks rows and columns based on the corresponding interaction graph of a dense matrix, which in the case of frontal matrices, is the graph corresponding to the sparse separator. That is, for an off-diagonal block in the frontal matrix, it chooses a subset of rows and columns based on the corresponding separator graph. The criteria for choosing these rows and columns is based on the location of their respective nodes in the sparse separator graph. Figure [\ref=fig:BDLR_Full] shows an example of an interaction graph corresponding to the interaction of a set of row and column indices in an off-diagonal block of a sample frontal matrix. Figure [\ref=fig:BDLR_LR] shows that the BDLR method chooses row indices and column indices corresponding to nodes that are closer to the boundary (blue line).

Let R and C be the matrices containing all the selected rows and columns. In other words:

[formula]

where B is the off-diagonal low-rank matrix and I and J are the set of row and columns indices chosen by the BDLR algorithm respectively. Defining B̂  =  B(I,J), we perform a full pivoting LU factorization :

[formula]

where P and Q are permutation matrices. Let r be the chosen rank for B̂. Define [formula] and [formula] as:

[formula]

We then have: (U(1:r,1:r))- 1 and (L(1:r,1:r))- 1 correspond to lower-triangular solves. The inverse matrices are not explicitly computed. The approximation rank r is chosen based on the desired final accuracy such that |ur + 1,r + 1  /  u11|  <  ε where ur + 1,r + 1 and u11 correspond to (r + 1)th and the first pivots respectively and ε is the desired accuracy.

The final rank r may be significantly smaller than the number of originally selected rows and columns. This higher compression results in higher efficiency both in terms of memory and runtime.

An Iterative Solver with Direct Solver Preconditioning

In this article, we investigate using an accelerated multifrontal sparse direct solver as a preconditioner to the generalized minimal residual (GMRES) [\cite=saad1986gmres] method. In this case, we use a relatively low accuracy for the direct solver. We will show that this approach is much faster and more memory efficient than a conventional multifrontal sparse solver. We should also mention that this preconditioning method can be applied to any iterative solvers (conjugate gradient (CG) [\cite=hestenes1952methods], etc.).

A Fast Multifrontal Solver

Overview of a Conventional Multifrontal Algorithm

We do not intend to give a detailed explanation of the multifrontal solve process in this article. We refer the reader to the available literature (see for example [\cite=MFReview]) for an in-depth explanation of the algorithm.

In the multifrontal method, the unknowns are eliminated following the ordering imposed by the elimination tree. That is, each node of the elimination tree corresponds to a set of unknowns, and these unknowns cannot be eliminated until all the unknowns corresponding to the children of this node are eliminated.

The multifrontal algorithm is an algorithm to calculate the Cholesky or LU factorization of a sparse matrix [\cite=MFReview], with special optimizations that take advantage of the sparsity. Moreover (and this is specific to a multifrontal elimination), during the elimination, information is propagated only from a child node to its parent (in the so-called elimination tree [\cite=duff1986direct]). This is what distinguishes for example a multifrontal elimination from a supernodal elimination.

We note that in this paper we describe our method in the context of a multifrontal elimination; however, the same method can be applied to a supernodal elimination. No fundamental change is required to our algorithm.

Factorization

Consider now a node p in the elimination tree. Let Ip be the set of indices of unknowns associated with node p: where np is the number of unknowns corresponding to node p, and i(p)j is the global index of the jth unknown associated with node p. We denote a specific child node of p as ck (ck∈Cp, [formula] where Cp is the set of all children and ncp is the number of children of node p).

Define the set Sp as the set of unknowns j  >  i(p)np that are connected to any of the unknowns in Ip, in the graph of A. More precisely:

[formula]

where aij is the entry at the ith row and jth column of the original sparse matrix A. Describing the details of a multifrontal elimination requires the definition of the matrix Uck, which we call the update matrix corresponding to the kth child of node p by recurrence. The set of indices corresponding to unknowns associated with Uck (update matrix of children nodes) is denoted IUck.

If p is a leaf node in the elimination tree, the matrix Uck is not defined and hence, [formula]. We define the set of frontal indices Ifp as follows: We now define the matrix p as the sub-matrix of A associated with [formula].

[formula]

The symbol ×   schematically denotes a nonzero entry in the matrix. In p, we set the entries for the block Ifp  ×  Ifp to 0.

The frontal matrix for node p, Fp, is defined as follows: The symbol [formula] denotes the extend-add operation and Uck denotes the update matrix corresponding to the kth child of node p. The extend-add is basically an addition. The "extend" part corresponds to the fact that there is a size mismatch between a Uck and Fp, and an index mapping from Uck to Fp must be used. In the special case where node p is a leaf node in the elimination tree, Fp  =  p.

Note that, after the extend add operations, the frontal matrix Fp is (nearly) a fully dense matrix. We then divide Fp into four sub-blocks: Factorizing Fpp, we are left with the Schur complement. This is by definition the update matrix associated with node p which will be used in the extend-add operation of its parent:

[formula]

Repeating the operations described in Eqns. [\eqref=eq:couplingSet] to [\eqref=eq:update] for all nodes in the elimination tree starting from the leaf nodes and going up to the root node, constitutes the factorization phase of the conventional multifrontal algorithm.

Solve

The solve phase constitutes of an upward pass (L solve) and a downward pass (U solve) in the elimination tree. In the upward (downward) pass, we traverse the elimination tree upward (downward) from leaves to root (root to leaves) and traverse the right hand side vector b downwards (upwards). Hence, the upward and downward passes correspond to the L and U solve phases in a conventional LU solver respectively.

In the upward pass (L solve) phase, we first construct the upward pass solution matrix bu which is initially equal to the right hand side b. Then, moving upward in the elimination tree, we construct the upward solution bup for each node p, which is basically the elements of the upward pass solution bu corresponding to the unknowns in Ip and Ifp.

[formula]

Now, update the upward pass solution using:

[formula]

After completing the upward pass (L solve), we must perform a downward pass (U solve) to arrive at our final solution. The final solution x is initially an empty vector. We traverse the elimination tree from root to leaves (downward). For each node p, we construct the final solution vector (Eq. [\eqref=eq:U_Soln]). The corresponding solution for each node can be calculated as follows: Note that since we're traversing the elimination tree downward (traversing bup upward), xfp has already been calculated by the time we reach p.

HODLR Accelerated Multifrontal Algorithm

Looking at the procedure described in Section [\ref=sec:conventional], one can observe that dense BLAS3 operations like the one described by Eq. [\eqref=eq:update], which involves both a factorization and an outer product update, can become time and memory consuming as the front size increases. In order to accelerate the multifrontal elimination process, we replace large dense matrices with HODLR structures.

Accelerated Factorization

In the factorization phase, we want to represent the frontal and update matrices for each node p as HODLR matrices. In order to be able to construct an HODLR structure, we need to utilize a suitable low-rank approximation method. Our previous results [\cite=BlackBox_HODLR] show that the boundary distance low-rank approximation scheme (BDLR) is a suitable algorithm for our purposes. Furthermore, as we will show, a priori knowledge of rows and columns with BDLR leads to a very fast extend-add operation.

To construct the HODLR representation of the frontal and update matrices of p, we first assemble p as described by Eq. [\eqref=eq:fbar]. As described in [\cite=BlackBox_HODLR], the BDLR algorithm requires an interaction graph that describes the interaction between the rows and columns of the matrix, which, in this case, is the graph that is constructed from the submatrix of the original matrix A corresponding to the interaction of rows and columns with indices in the set [formula].

Using the interaction graph for [formula], we create the HODLR representation of FHODLRp using the BDLR algorithm:

[formula]

Using the extend-add notation [formula], FHODLRp is given by:

[formula]

For simplicity, we've assumed that all the update matrices associated with node p are HODLR matrices. In some cases, the update matrices might be small dense matrices, in which case the extend-add operations described for the child HODLR updates will become almost trivial for the dense child updates. Looking at Eq. [\eqref=eq:update], we notice that every HODLR update matrix is composed of two components: an HODLR matrix and an outer product update.

[formula]

where the subscript k denotes the update from the kth child of p. Since we only utilize UHODLRck in the extend-add operation of its parent, we will save the two contributions for the extend-add operation. That is, Eq. [\eqref=eq:HODLR_EA] now becomes:

[formula]

Equation [\eqref=eq:HODLR_EA_Expand] requires that we perform an extend-add operation into a target HODLR structure.

Before going into the details of this operation, we should first emphasize the importance of this operation both in terms of computational cost and memory saving compared to a conventional extend-add operation. Consider the outer product W1VT1 in Eq. [\eqref=eq:HODLR_EA_Expand]. Let n̂p be the size of the matrix FHODLRp. In order to perform the extend-add operation, we must extend W1 and V1 to arrive at matrices We1 and We2, of size n̂p  ×  rp. In the conventional algorithm, we had to perform the outer product We1VeT1, which has a computational cost of O(n̂2prp) and a storage cost of O(n̂2p). For a 3D mesh with N degrees of freedom, n̂p, corresponding to the root node in the elimination tree, grows as O(N2 / 3), and rp for this node roughly scales as O(N1 / 3). This will result in a computational cost of O(N2) and a storage cost of O(N4 / 3). Hence, in practice, the extend-add operation dominates the computational cost of the conventional multifrontal algorithm.

As shown in Eq. [\eqref=eq:HODLR_EA_Expand], the extend-add process involves two different operations. The first operation is updating the frontal matrix (p) with an HODLR structure (FHODLRffk) to arrive at a target HODLR structure (FHODLRp). What makes this operation difficult is the fact that FHODLRffk and FHODLRp typically have different structures. See Figure [\ref=fig:HODLR_EA] for an illustration. That is, the diagonal block sizes and the number of HODLR levels might differ between the two matrices.

A key feature of the BDLR algorithm is that for each target HODLR structure (FHODLRp), we know a priori the rows and columns needed to construct the off-diagonal low-rank approximation. Hence, in order to perform the extend-add operation, we traverse the target HODLR structure and for each off-diagonal block, we extract the rows and columns determined by the BDLR algorithm from the child update HODLR matrices (FHODLRffk).

The second extend add-operation is adding a low-rank matrix (WkVTk) to the frontal matrix (p) to arrive at a target HODLR structure (FHODLRp). This process is very similar to the one described for adding FHODLRffk to p. The only difference is that instead of reconstructing rows and columns from an HODLR structure (FHODLRffk), we reconstruct the required rows and columns from the low-rank outer product (WkVTk).

After reconstructing and extracting the rows and columns selected by BDLR and adding them to the corresponding rows and columns in the target structure, we perform the partial pivoting LU factorization. As described in Section [\ref=sec:BDLR], we arrive at a final rank r which is much smaller compared to the number of originally selected rows and columns. Given that the factorization of a hierarchical matrix of size n scales as O(r2n), a reduction in the rank has a significant effect on the resulting speedup.

Assuming the off-diagonal rank of the target HODLR structure corresponding to the node p in the elimination tree is rp, we need to extract rp rows and columns from the HODLR matrices and the outer products. Hence, we need to perform O(r2pn̂p) operations in order to construct rp rows from the outer product updates. This translates to a computational cost of O(N4 / 3) for the root node in the elimination tree (rp scales as O(N1 / 3)) and is much more efficient compared to the O(N2) scaling of the conventional extend-add algorithm. Moreover, we used no additional memory in order to perform the accelerated extend-add operation.

Now that we have constructed the HODLR representation of the frontal matrix FHODLRp, we factorize FHODLRpp using an HODLR solver (see [\cite=BlackBox_HODLR] for example). Next, we store the update matrix UHODLRp as an HODLR matrix and an outer product update:

[formula]

Accelerated Solve

The solve phase of the accelerated method is very similar to the solve phase of the conventional method described in Section [\ref=sec:convSolve]. The only difference is that F- 1pp is now replaced by (FHODLRpp)- 1, which simply represents an HODLR solve instead of a conventional solve. Furthermore, the matrices Fpf and Ffp are now represented as low-rank products which results in a more efficient matrix-vector multiplications in Eqns. [\eqref=eq:upward] and [\eqref=eq:downward].

Application to Finite-Element Matrices

In order to demonstrate the effectiveness of our method, we benchmark our solver for two classes of problems. We first apply our solver to a finite-element stiffness matrix that arises from a complicated 3D geometry. Next, we benchmark the performance of our solver for sparse matrices arising from the FETI method [\cite=FETI_DP1] [\cite=FETI_DP2].

Elasticity Problem for a Cylinder Head Geometry

We apply the iterative solver with the accelerated multifrontal preconditioner to a stiffness matrix corresponding to the finite-element discretization of the elasticity equation in a cylinder head geometry:

[formula]

where [formula] is the displacement vector and λ and μ are Lamé parameters. The cylinder head mesh consists of a mixture of 8-node hexahedral, 6-node pentahedral and 4-node tetrahedral solid elements, and also 3-node shell elements. Figure [\ref=fig:cylinderMesh] shows a sample mesh for the cylinder head geometry.

FETI-DP Solver for a 3D Elasticity Problem

FETI methods [\cite=FETI_DP1] [\cite=FETI_DP2] are a family of domain decomposition algorithms with Lagrange multipliers that have been developed for the fast sequential and parallel iterative solution of large-scale systems of equations arising from the finite-element discretization of partial differential equations [\cite=FETI_DP1]. In this article, we investigate the solution of sparse matrices arising from a FETI-DP solver applied to the elasticity equation Eq. [\eqref=eq:NavierCauchy].

We consider two classes of problems within the FETI-DP framework. The first class of matrices, called local matrices, corresponds to solving the problem on a subdomain of the original mesh. The other class of matrices is called coarse problem matrices and corresponds to the corner DOFs of all the subdomains.

We benchmark our code for FETI-DP local matrices in various mesh structures. We consider a structured and an unstructured mesh in a cube geometry geometry. The structured cube mesh uses an 8 node cube element while the unstructured cube mesh uses a 4 node tetrahedral element to discretize the elasticity equation Eq. [\eqref=eq:NavierCauchy]. Figures [\ref=fig:structuredMesh] and [\ref=fig:unstructuredMesh] show a sample mesh for the structured and the unstructured cubes respectively.

We also apply our solver to FETI-DP coarse problem matrices arising from solving the elasticity equation in a unit cube geometry. Factorization of the coarse matrix for problems where the coarse matrix is large is expensive and might become the bottleneck of the FETI-DP solver. As a result, we are interested in accelerating the solve process and decreasing the memory footprint of factorizing such matrices. Figure [\ref=fig:subdomains] show a typical subdomain configuration in the unit cube.

Numerical Results

In this section we show numerical benchmarks for the matrices described in Sections  [\ref=sec:cylinderResults] and [\ref=sec:FETIResults] respectively. As described in Section [\ref=sec:directIterative], we use the accelerated multifrontal solver at low accuracies as a preconditioner to the GMRES iterative method. We compare this approach to conventional preconditioners, namely the diagonal and the incomplete LU (ILU) preconditioner as well as the conventional multifrontal algorithm.

We implemented our code in C|++| and used the Eigen C|++| library for linear algebra operations. The incomplete LU algorithm is the incomplete LU with the dual-thresholding implementation from the SPARSEKIT package [\cite=ILUCode].

Elasticity Problem for a Cylinder Head Geometry

Figure [\ref=fig:cHeadIter] shows the convergence of the accelerated multifrontal preconditioner against the conventional diagonal and ILU preconditioners for the cylinder head geometry. As can be seen, since the problem is relatively difficult, the diagonal preconditioners fails to converge and one needs to work with the parameters of the ILU preconditioner in order to achieve convergence. Furthermore, the accelerated multifrontal preconditioner converges much faster compared to both diagonal and ILU preconditioners.

Figures [\ref=fig:cHeadTime] and [\ref=fig:cHeadMem] show the run time and memory consumption comparison between the conventional multifrontal, the accelerated multifrontal and the incomplete LU iterative schemes respectively. The accelerated multifrontal algorithm has a lower runtime compared to the conventional multifrontal and the ILU algorithm.

FETI-DP Solver for a 3D Elasticity Problem

FETI-DP Local Problems

Figure [\ref=fig:structCubeIter] compares the convergence of the accelerated multifrontal method with traditional preconditioners for the structured cube mesh local problem. As this problem is a relatively simple problem, both the diagonal and ILU preconditioners converge without too many iterations. However, the accelerated multifrontal method still has the highest convergence rate amongst the benchmarked algorithm. Figures [\ref=fig:structCubeTime] and [\ref=fig:structCubeMem] show the runtime and memory consumption comparison for the structured cube local problem. The fast multifrontal algorithm has a significantly lower runtime and memory consumption compared to the conventional multifrontal algorithm. However, because of the relative simplicity of this problem, the ILU algorithm is competitive in terms of factorization time.

Figure [\ref=fig:unstructCubeIter] shows the convergence rate of the accelerated multifrontal, diagonal and the ILU preconditioner for the unstructured cube mesh local problem. As can be seen, this problem is the most complicated and difficult. Not only does the diagonal preconditioner fail to converge, but the input parameters of ILU need to be increased significantly in order to achieve convergence. Figure [\ref=fig:unstructCubeTime] shows that the ILU preconditioner is significantly slower than both the multifrontal and accelerated multifrontal solver. The accelerated multifrontal solver is the fastest among all conventional algorithms, and Figure [\ref=fig:unstructCubeMem] shows that it also reduces the memory requirements.

FETI-DP Coarse Problems

Figure [\ref=fig:elasticitySIter] shows the convergence rate of the accelerated multifrontal method and the conventional preconditioners for a coarse FETI-DP problem in a cube geometry. Figure [\ref=fig:elasticitySTime] shows that the accelerated multifrontal method is faster than both the conventional multifrontal and the ILU algorithms. Furthermore, as can be seen in Figure [\ref=fig:elasticitySMem], the memory consumption of the accelerated multifrontal method is significantly lower compared to the conventional algorithm.

Figure [\ref=fig:elasticityStIter] compares the convergence rates of the accelerated multifrontal method with ILU and diagonal preconditioning schemes for a coarse FETI-DP problem that only includes translational degrees of freedoms at the corner of the subdomains. As can be seen in Figure [\ref=fig:elasticityStTime], the ILU algorithm is significantly slower compared to both the conventional and the accelerated multifrontal schemes. Figures [\ref=fig:elasticityStTime] and [\ref=fig:elasticityStMem] show that not only the accelerated multifrontal method is faster, but also it consumes much less memory compared to the conventional multifrontal algorithm.

Summary

Table [\ref=table:summaryNum] shows a detailed summary of all the benchmark cases. As can be seen in all cases, the GMRES solver with the accelerated multifrontal preconditioner converges after few iterations which shows the effectiveness of the developed algorithm. Furthermore, in almost all cases, we observe a speedup and memory saving of up to 3x compared to the conventional multifrontal solver. We were not able to benchmark larger cases due to memory limitations. However, one can observe that both the speedup and memory saving become more significant as the matrix size grows. That is, a very high speedup and memory saving can be achieved for very large matrices.

Figures [\ref=fig:numIterStruct] and [\ref=fig:numIterUnstruct] compare the number of iterations for the ILU and the accelerated multifrontal preconditioner for the benchmarked structured and unstructured meshes respectively. As both figures show, not only the accelerated multifrontal preconditioner has less number of iterations compared to ILU, but also the number of iterations does not grow significantly with matrix size. This is another important advantage of the accelerated multifrontal preconditioner that makes it more favorable for parallel implementations compared to ILU. This is because fewer number of iterations results in fewer matrix vector products which requires less communication between the nodes which ultimately results in a higher speedup.

Figure [\ref=fig:numIterAcc] shows that one can significantly decrease the number of iterations required by the accelerated multifrontal preconditioner by simply decreasing the accuracy parameter (and in turn, increasing the depth parameter). However, this results in an increase in the off-diagonal rank and the decrease of speedup and memory savings. This shows that one can fine-tune the code parameters based on their available resources and their desired convergence rates.

Conclusion

We have developed a black-box fast and robust linear solver for finite element matrices arising from the discretization of elliptic PDEs. As we've shown, this solver is advantageous both in terms of running time and memory consumption compared to both the conventional multifrontal direct solve algorithm and the ILU preconditioner. Furthermore, not only our solver is faster in terms of factorization time, but also it results in less number of iterations compared to ILU.

The examples presented here were run on a single core machine, and are limited in size by the amount of memory available on a single computer node. A parallel implementation would have allowed us to run much larger test cases. Since the speed-up improves with N, this would have allowed us to demonstrate even greater speed-ups, in particular compared to ILU. This will be, however, done in a future publication.

This solver can be used at low-accuracy as a preconditioner, or as a standalone direct solver at high accuracy. The current scheme relies on the assumption that all off-diagonal blocks are low-rank. In practice, this implies that the rank required to reach the desired accuracy may become somewhat large, leading to a loss in efficiency. This can be remedied using more complex algorithms such as [\cite=ambikasaran2014ifmm]. Such algorithms are currently under development for the case of sparse matrices. Despite this limitation, the class of methods presented does yield significant improvements over the current state of the art.

One advantage of the method presented here is its relative simplicity. For example, by removing the requirement to form a nested low-rank basis across levels, we can simplify the implementation and algorithm significantly. This is in contrast with the HSS class of methods for example [\cite=sheng2007algorithms]. Despite this simplification, the HODLR scheme has a computational cost in O(N4 / 3), whereas HSS-based schemes scale like O(N4 / 3 log N) [\cite=randomizedMF].

We finally point out that the algorithms presented here are very general and robust. They can be applied to a wide-range of problems in a black-box manner. This was demonstrated in part in this manuscript.

Acknowledgments

The authors would like to acknowledge Prof. Charbel Farhat, Dr. Philip Avery and Dr. Jari Toivanen for providing us with the FETI test matrices. We also want to thank Profs. Pierre Ramet and Mathieu Faverge for their useful input and suggestions in this project.

Part of this research was done at Stanford University, and was supported in part by the U.S. Army Research Laboratory, through the Army High Performance Computing Research Center, Cooperative Agreement W911NF-07-0027. This material is also based upon work supported by the Department of Energy under Award Number DE-NA0002373-1.