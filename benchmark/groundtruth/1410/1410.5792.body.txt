GENERALIZED COMPRESSION DICTIONARY DISTANCE AS UNIVERSAL SIMILARITY MEASURE

Andrey Bogomolov SKIL Telecom Italia Lab, University of Trento, Via Sommarive, 5, I-38123, Trento, Italy E-mail: andrey.bogomolov@unitn.it Bruno Lepri, Fabio Pianesi Fondazione Bruno Kessler Via Sommarive, 18, I-38123, Trento, Italy E-mail: {lepri, pianesi}

=10000 = 10000 =4991

Introduction

The similarity measure between objects is fundamental for machine learning tasks. Most similarity measures require prior assumptions on the statistical model and/or the parameters limits.

For most applications in computational social science, economics, finance, human dynamics analysis this implies a certain risk of being biased or not to account for partially observed source signal fundamental properties [\cite=taleb2012antifragile].

For more technical applications such as digital signal processing, telecommunications and remote sensing, given that the signal could be observed and modelled, we face the problems of noise, features representation and algorithmic efficiency.

We easily agree with the following outcome of 20 trials of fair coin toss "01111011001101110001", but we do not accept the result "00000000000000000000". However, both results have equal chances given that the fair coin model assumption holds. This is a common example of paradoxes in probability theory, but our reaction is caused by the belief that the first sequence is complicated, but the second is simple [\cite=Muchnik1998263]. A second example of human-inspired limitations is "Green Lumber Fallacy" introduced by Nassim Nicholas Taleb. It is a kind of fallacy that a person "mistaking the source of important or even necessary knowledge, for another less visible from the outside, less tractable one". Mathematically, it could be expressed as we use an incorrect function which, by some chance, returns the correct output, such that g(x) is mixed with f(x). The root of the fallacy is that "although people may be focusing on the right things, due to complexity of the thing, are not good enough to figure it out intellectually" [\cite=taleb2012antifragile].

Despite the psychological limitations and fallacies by which we, human, reason and develop the models, during the last few decades there were developed a number of robust methods to enhance model generalization properties and resistance to noise, such as filtering, cross validation, boosting, bootstrapping, bagging, random forests [\cite=hastie01statisticallearning]. The most promising approach to the challenging paradigm of approaching antifragility uses Kolmogorov complexity theory [\cite=Kolmogorov1998387] and concepts of Computational Irreducibility or the Principle of Computational Equivalence, introduced by Steven Wolfram [\cite=DBLP:books/daglib/0006578]. Unfortunately Kolmogorov complexity is uncomputable for a general case. For practical applications we have to implement the algorithms which run on computers having Turing machine properties.

Methodology

For defining similarity measure which uses Kolmogorov complexity researchers introduced Information Distance measure, which is defined as a distance between strings x and y as the length of the shortest program p that computes x from y and vice versa. The Information Distance is absolute and to obtain a similarity metric Normalized Information Distance (NID) was introduced:

[formula]

Unfortunately, Normalized Information Distance is also uncomputable for a general case, as it is dependent on uncomputable Kolmogorov complexity measure [\cite=1412045]. For approximating NID in a practical environment - Normalized Compression Distance (NCD) was developed, based on based on a real-world lossless abstract compressor C [\cite=1362909]:

[formula]

Daniele Cerra and Mihai Datcu introduced another approximation metric - Fast Compression Distance (FCD), which is applicable to medium-to-large datasets:

[formula]

where [formula] and [formula] are the sizes of the relative dictionaries, represented by the number of entries they contain, and [formula] is the number of patterns which are found in both dictionaries. FCD accounts for the number of patterns which are found in both dictionaries extracted during compression by Lempel-Ziv-Welch algorithm, and reduces the computational effort by computing the intersection between dictionaries, which represents the joint compression step performed in NCD [\cite=Cerra:2012:FCS:2109696.2109997].

We found that the number of patterns which exist in the both dictionaries is dependent on the compression algorithm used. Also the dictionary of x and y set intersection could be coded with different symbols, as the frequencies of strings could be different in x, y and [formula], which leads to less accurate approximation. The size of a compression dictionary does not account for the symbol frequency properties of the dictionary and the size of possible algorithmic "description of the string in some fixed universal description language", which is the essence of Kolmogorov complexity. That means that we loose a lot of information about x and y if we compute only the size of a compression dictionary.

In order to overcome this problem we introduce Generalized Compression Dictionary Distance (GCDD) metric, which is defined as:

[formula]

where [formula] is functional characteristics of the compression dictionary extracted from concatination of x and y byte arrays. GCDD returns an n-dimensional vector, which characterizes the conditional similarity measure between x and y. Each dimension of GCDD represents a real valued function.

The algorithmic complexity of the proposed solution is proportional to:

[formula]

where mx and my is the dictionary size of x and y, and k is the constant dependent on the dimensionality of the resulting vector.

In comparison, the algorithmic complexity of the other measures are:

[formula]

[formula]

which shows asymptotically small increase in computational time for GCDD but preserving informational gain through additional x and y characteristics transfer.

Experimental Results and Discussion

The implementation of Generalized Compression Dictionary Distance prototype was done in The Java Platform, Standard Edition 8 for double precision 64-bit input vectors and working with the Huffman Coding and Lempel-Ziv-Welch compression for byte array case applying the approach of binary-to-string encoding.

The experiments were run on "Synthetic Control Chart Time Series" - a well known dataset published in UCI Machine Learning Repository[\cite=Asuncion+Newman:2007], which contains control charts synthetically generated by Alcock and Manolopoulos [\cite=Alcock99time-seriessimilarity] for the six different classes of time series: normal, cyclic, increasing trend, decreasing trend, upward shift and downward shift.

Experimental results show that Normalized Compression Dictionary Size and Normalized Compression Dictionary Entropy, as examples of GCDD, give more stable and accurate results for time-series clustering problem when tested on heterogeneous input vectors, than NCD and other traditional distance (e.i. dissimilarity) measures, such as euclidean distance.

Experimental results shown in the Fig.[\ref=fig:dissimilarity] are produced from abovementioned collection of I time series vectors, on which 4 distance function are defined (GCDD, NCD, L2-norm, Pearson correlation).

Applying a distance function for each pairwise vectors, the dissimilarity matrix is consructed, such that:

[formula]

Then multidimensional scaling is performed. Given dissimilarity matrix Î”, we find I vectors [formula] such that [formula] for all [formula], where [formula] is a vector norm.

On the plots we use the following symbols to encode vectors of time series trend types: N - normal, C - cyclic, IT - increasing trend, DT - decreasing trend, US - upward shift and DS - downward shift.

From the Fig.[\ref=fig:dissimilarity] we see, that GCDD based distance metric efficiently groups time series on a hyperplane thus increasing separation ability.

It has similar properties as NCD, and much better than L2-norm and Pearson correlation based, where the time series vectors are mixed.

Then we run the experiments with computationally intensive state-of-the-art methods for time series clustering, such as: (1) autocorrelation based method, (2) Linear Predictive Coding based as proposed by Kalpakis, 2001 [\cite=Kalpakis2001], (3) Adaptive Dissimilarity Index based [\cite=Chouakria] and (4) ARIMA based (Piccolo, 1990) [\cite=piccolo1990distance].

From the Fig.[\ref=fig:dissimilarity2] we see, that numerically intensive methods do not enhance much the separation ability, which is computed applying GCDD based distance metric. Also these distance methods require much more computation time and are not applicable for big data problems.

Further more, the result proposed in this paper could be used for unsupervised clustering, supervised classification and feature representation for deep learning tasks given the nice properties of GCDD, such as (1) it scales linearly with exponential growth of the input vector size and (2) it is content independent, as the semantics is coded inside the extracted dictionary itself.

The future research steps include testing the concept on diverse data sets, including image processing data and using the GCDD output for different machine learning tasks.