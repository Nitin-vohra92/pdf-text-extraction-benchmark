Air quality prediction using optimal neural networks with stochastic variables

Introduction

Urban air pollution is a complex mixture of toxic components with considerable impact on the inhabitants of urban regions, particularly those belonging to sensitive groups, such as children and people with previous heart and respiratory insufficiency[\cite=kolehmainen2001]. Therefore, forecasting the temporal evolution of air pollution concentrations in specific urban locations emerges as a priority for guaranteeing life quality in urban and metropolitan centers. With this aim, in order to identify and predict in advance episodes of low air quality at regional and local scales, air quality forecasting models have been developed, considering the characteristics of atmospheric pollution and its consequent impact on people’s health and life quality.

Straightforward approaches such as Box models[\cite=middleton1997], Gaussian plume models[\cite=reich1999], persistence and regression models[\cite=shi1999] are commonly applied to characterize and forecast air pollutants’ dispersion. These models are easy to implement and allow for the rapid calculation of forecasts. However, they include significant simplifications[\cite=luecken2006] and usually do not describe the processes and interactions that control the transport and chemical behavior of pollutants in the atmosphere[\cite=luecken2006], important for instance for secondary pollutants[\cite=sokhi2006]. Improvements have been made with deterministic dispersion models and statistical-based approaches, which however, being highly non–linear[\cite=binbhu2012], require a large amount of accurate input data and are considerably expensive from the computational point of view[\cite=dutot].

A promising alternative to all these models are artificial neural networks (ANN)[\cite=binbhu2012] [\cite=nejadkoorki2012] [\cite=gardner1997]. Several ANN models have already been used for air quality forecast, in particular for forecasting hourly averages[\cite=kolehmainen2001] [\cite=perez2000] [\cite=kukkonen2003] and daily maxima[\cite=perez2002]. Further, several authors compared already the potential of different approaches when applied to different pollutants and prediction time lags[\cite=kukkonen2003] [\cite=YiPrybutok1996] [\cite=GardnerDorling2000] [\cite=Hooyberghs2005]. Still, though successful in many situations and having considerably less restrictions on the input data, large training data sets are usually required to improve accuracy and minimize uncertainty in the output data, which up to now has been a significant disadvantage of these models.

Recently, we applied methods from stochastic data analysis and statistical physics for deriving variables with reduced stochastic fluctuations[\cite=vitor] to empirical data in sets of NO2 concentration measurements[\cite=no2]. Such methods were introduced in the late nineties[\cite=friedrich97] [\cite=physrepreview] for analyzing measurements on complex stochastic processes, aiming for a quantitative estimation of drift and diffusion functions from sets of measurements that fully define the evolution equation of the underlying stochastic variables. The framework has already been applied successfully, for instance to describe turbulent flows[\cite=friedrich97] and the evolution of climate indices[\cite=lind05] [\cite=lind07], performance curves of wind turbines[\cite=wind], stock market indices[\cite=friedrich00], and oil prices[\cite=ghasemi07]. At the same time, the basic method has been refined in particular for data with low sampling frequency[\cite=kleinhans05] [\cite=lade09] and subjected to strong measurement noise[\cite=boettcher06] [\cite=lind10] [\cite=carvalho2010].

In this paper we present an important application of such variables: using them as input for training ANN enables one to reduce considerably the amount of input data needed for achieving a given accuracy. We argue that this reduction in the number of input variables is possible because the derived variables incorporate temporal correlations between independent and spatially separated monitoring stations. Moreover, as we quantitatively show below, when using this reduced amount of information that includes the derived variables, the predictive power of the ANN is not significantly changed, which is a major advantage when working with observational data which might include missing values. Combining a faster ANN training with the same predictive power may improve the ability and capability of alert system for air quality in large urban centers. We start in Sec. [\ref=sec:methods] by briefly describing ANN models as well as the main points of the stochastic data analysis procedure used. In Sec. [\ref=sec:data] the empirical data is described, comprising two different data sets of NO2 concentration measures in the city of Lisbon, Portugal (see Fig. [\ref=fig1]). In Sec. [\ref=sec:results] the results are discussed in the light of predictive power measures, and Sec. [\ref=sec:conclusions] concludes the paper.

Methods

The Neural Network framework

Artificial neural network models are mathematical models inspired by the functioning of nervous systems[\cite=gardner1997] [\cite=Cobourn2000] [\cite=Agirre-Basurko2006], which are composed by a number of interconnected entities, the artificial neurons (see Fig. [\ref=fig2]).

These neurons may be associated in many different ways[\cite=Agirre-Basurko2006] [\cite=Haykin1999], depending on the characteristics of the proposed problem. To construct an ANN model the air pollution system is considered as a system that receives information from n distinct sets of inputs Xi ([formula]), namely weather parameters and air pollution properties, and produces a specific output, in our case the concentration of the NO2 pollutant[\cite=gardner1997]. No prior knowledge about the relationship between input and output variables is assumed. The input variables should be independent from each other and each one is represented by its own input neuron [formula]. Each neuron computes a linear combination of the weighted inputs ωij, including a bias term bi, from the links feeding into it and the corresponding summed value [formula] is transformed using a function f, either linear or non-linear such as log-sigmoid or hyperbolic tangent. The bias term is included in order to allow the activation functions to be offset from zero and it can be set randomly or set to a desired value (e.g. dummy input with a magnitude equal to 1). The output obtained is then passed as a new input j  =  f(Cj) to other nodes in the following layer, usually named hidden layer. Though one is allowed to use several neurons in this hidden layer, it is generally advantageous to somehow minimize the number of hidden neurons, in order to improve the generalization capabilities of the model and also to avoid over-fitting. In particular, a simple one-layer ANN structure with just one neuron employing a linear activation function reduces to the well-known linear regression model[\cite=Weisberg1985]. The ANN models used here are based on a feed-forward configuration of the multilayer perceptron that has been used by several authors [\citep=Hooyberghs2005] [\citep=Papanastasiou]. A large number of architectures has been tested. The use of two layers was verified to be sufficient. The use of more layers was concluded to be redundant, and therefore we only use two layers, one input layer and one hidden layer.

Having such a framework of input variables and sets of functions, the ANN has to be trained in order to obtain the best estimate for each weight ω. The weight values are determined by an optimization procedure, the so-called learning algorithm[\cite=Haykin1999], which in our case uses a cross-validation procedure[\cite=wilks2006] to ensure stability of the model. The cross-validation was applied dividing the available period into four sets and completing the calibration-validation procedure four times independently, i.e., from the 4 years of data available, data from 3 years were used to build the model and data from 1 year for validation. The validation year was then cycled through the 4 year period. For each validation year, a set of performance measures were computed between the observed real values and the ANN forecasts, namely, the Pearson correlation coeﬃcient (PC); the root mean square error (RMSE); and the skill against persistence (Sp). The outcomes of each resulting performance measure were analyzed for each year and then averaged for the complete period, resulting on an average value for each monitoring station.

There are several learning algorithms, depending on whether the ANN model is linear or non-linear. For linear ANN models, the learning algorithm is typically based on the Widrow-Hoff learning-rule, also known as the least mean square rule, which produces a unique solution corresponding to the absolute minimum value of the error surface[\cite=TrigoPalutikof1999]. For non-linear models, the back-propagation (BP) is one of the most popular and common training procedures used, which is described in depth in the literature [\cite=Haykin1999] [\cite=TrigoPalutikof1999]. It has been shown in literature that BP training algorithms have two caveats: convergence may be slow and the final weights may be trapped in local minima over the highly complex error surface [\cite=TrigoPalutikof1999]. As an alternative, the Levenberg-Marquardt method[\cite=numrecip] minimizes an error function in "damped" procedures, i.e. selecting steps proportional to the gradient of the error function. The Levenberg-Marquardt method requires more memory [\cite=Haykin1999] [\cite=TrigoPalutikof1999] than the Widrow-Hoff rule, but has the advantage of converging faster and with a higher effective robustness than most BP schemes[\cite=TrigoPalutikof1999] because it avoids having to compute second-order derivatives. The Levenberg-Marquardt method was appliedfor this study. Together with the persistence model, which is the simplest way of producing a forecast and assumes that the conditions at the time of the forecast will not change, i.e., the forecast for each time step simply corresponds to the value of the previous time step, the linear regression model constitutes the baseline against which the performance of non-linear ANN models are usually compared. Due to a certain level of memory that characterizes air pollutants, persistence corresponds to a benchmark model considerably more difficult to beat than climatology [\cite=demuzere].

Deriving optimal stochastic variables

In this section we briefly describe how from a number K of sets of measurements, one is able to derive a set of few stochastic variables containing information from all of them.

This procedure assumes that corresponding to each measurement variable there is a property that evolves according to some stochastic equation. More precisely, the K-dimensional state vector [formula] characterizes the set of K measurements at each time-step and evolves according to the Itô-Langevin equations[\cite=fpeq] [\cite=gard]:

[formula]

where [formula] is a set of K independent stochastic forces with Gaussian distribution fulfilling 〈Γi(t)〉  =  0 and 〈Γi(t)Γj(t')〉  =  2δijδ(t - t'). On the right hand side of Eq. ([\ref=Lang2DVect]) the term with function [formula] describes the determinist part, which drifts the system, while the term with [formula] account for the amplitude of the stochastic contributions characterized through the properties of [formula][\cite=physrepreview].

The heart of the method lies in the fact that the coefficients [formula] and [formula] are closely related to the drift vectors and diffusion matrices describing the evolution of the joint probability density function of the vector state [formula] by means of the corresponding Fokker-Planck equation[\cite=fpeq] [\cite=gard]. As has been shown previously in other contexts[\cite=physrepreview] [\cite=lind05] [\cite=friedrich00] [\cite=ghasemi07] [\cite=kleinhans05] [\cite=boettcher06] [\cite=lind10], the drift vector and the diffusion matrix can therefore be extracted directly from the data set [formula][\cite=no2] for [formula] and where [formula] symbolizes conditional averaging over all measurements [formula] that fulfill the condition [formula].

The last equation in both Eqs. ([\ref=M1]) and ([\ref=M2]) yields the operational definition of the first and second conditional moments[\cite=physrepreview] [\cite=lind10], respectively. The limit in Eq. ([\ref=DefCoefKM]) is typically approximated by the slope of a linear fit of the corresponding conditional moments at small τ. When this linear fit is not possible, as it is the case for the NO2  measurements under consideration, an alternative estimate[\cite=kleinhans05] is to consider the first value of M(τ) / τ at the lowest value of τ. Additional analysis has been carried out[\cite=no2], namely confirming the Markovian properties of the data sets, which is a prerequisite for assuming a Langevin process, Eq. [\eqref=Lang2DVect]. In case Markovian properties are not observed, it should be noted that this method may be still applied with alternative procedures[\cite=boettcher06] [\cite=lind10] [\cite=carvalho2010]. Furthermore, in case a Fourier spectrum shows periodicities, those should be filtered out by a proper detrending procedure. More details about how to apply Eqs. ([\ref=DefCoefKM]) can be found in Ref. [\cite=no2].

We apply this framework to the two-dimensional system of detrended NO2  concentration measurements taken in two stations in Lisbon situated in Chelas and Avenida da Liberdade. We consider therefore a vector [formula]. Both sets of measurements do exhibit Markovian properties[\cite=no2], and therefore both drift and diffusion functions are properly derived.

To arrive to the optimal variables, we next determine the eigensystem of the diffusion matrix and investigate its principal directions[\cite=vitor] [\cite=gradisek_eigenvectors] [\cite=vanMourik_eigenvectors]. These principal directions are computed for each mesh point previously defined in phase space[\cite=no2]. The aim is to obtain the transform of the original coordinates [formula] into new ones [formula], such that the the diffusion matrix is diagonalized. The transformation [formula] is a two-times continuously differentiable function. In general, the eigenvalues of the diffusion matrix indicate the amplitude of the stochastic force and the corresponding eigenvector indicates the phase space direction towards which such force acts.

In this way, the stochastic contribution is decoupled for each new variable. Consequently, if the eigenvalues in the transformed coordinates are significantly different, we are able to restrict our investigation to the coordinates with lower stochastic sources, i.e. lower eigenvalues. For such eigenvalues, the vector field of their eigenvectors defines the path in phase space towards which the fluctuations are minimal. Additionally, if these j eigenvalues are very small compared to all the others, the corresponding stochastic forces can be neglected and the system can be assumed to have only K - j independent stochastic forces, reducing the number of stochastic variables in the system. For all the details see Ref. [\cite=no2].

Data

Target data

We consider hourly measurements of NO2 concentrations in the metropolitan region of Lisbon, Portugal, namely at the monitoring stations of Chelas (C) and Avenida da Liberdade (AL) (see Fig. [\ref=fig1]). The data were recorded from 2002 to 2006, corresponding to ~  3x104 measurement points with roughly 1% of discarded values, due to incomplete or erroneous measurements. The stations are located at a distance of 4.8km from each other. In the following, the NO2 concentrations at the stations of Chelas and Avenida da Liberdade will be designated as YC(t) and YAL(t), respectively, omitting the temporal dependency when not necessary. Figure [\ref=fig3] shows the Fourier spectrum of both these data series.

Input data for ANN training

The ANN input data sets consist of the aforementioned hourly NO2 concentration measurements, and of concentrations of two more pollutants, namely NO, and CO, also measured at the monitoring stations of Chelas and Avenida da Liberdade, from January 1st 2002 until December 31st of 2006.

The first four years are used to construct the models and year 2006 is used for independent evaluation. More specifically the prediction is done in two steps. In the first step, we consider only the period 2002-2005. For this four years we take the first three, 2002-2004, for training the ANN and derive the respective parameter values of the ANN model. Using that ANN we predict year 2005. Then we consider 2002, 2003 and 2005 for training the ANN, and predict 2004. Similar procedure is done for predicting 2002 and 2003. In the second step, we use the parameter values obtained in the first step, namely, weights and biases, for predicting 2006.

Besides pollutant concentrations, we also consider daily maximum temperature, daily mean wind direction and speed, daily humidity, daily radiance, hourly mean temperature, hourly pressure and hourly relative humidity, boundary layer height (BLH) from the European Center for Medium Weather Forecast (ECMWF), circulation weather type (CWT) at the regional scale determined for Portugal according to Trigo and DaCamara[\cite=trigocamara], North Atlantic Oscillation (NAO) index from NCEP-NOAA, two weekly cycles and two yearly cycles. Specifically, BLH fields were retrieved from the 3 hourly ECMWF 40 years reanalysis (http://data-portal.ecmwf.int/data) for the 2002-2006 period. Afterward, we extracted the 00:00 UTC (BLH1), 03:00 UTC (BLH2), 9:00 (BLH3) and 21:00 UTC (BLH4) data from the retrieved BLH fields. Together with these variables we consider the two transformed variables derived through the procedure described in Sec. [\ref=subsec:optimal], before we suggest a method for reducing the number of input variables. In total there are 48 variables that are available as input data for the ANN model (See Tab. [\ref=fig_tabela_var]).

The two transformed variables are obtained from the original NO2 measurements YC and YAL by first detrending them. As shown in Fig. [\ref=fig3] both sets of measurements YC and YAL have periodic contributions, which must be filtered out. These periodicities describes daily, weekly, seasonal and yearly variations of the concentration due to anthropogenic routines and to periodic atmospheric processes[\cite=kolehmainen2001]. In particular, the 24 hours and one week cycles are both traffic related and mirror daily and weekly cycles. By filtering out these cycles, through a proper detrending, one is reduced to the stochastic contribution solely, which can be modeled through a stochastic differential equation[\cite=no2] [\cite=fpeq], having both the deterministic forcing in the drift vector [formula], and the diffusive fluctuation, [formula].

The detrended series derived from the original measurements YC and YAL, represented henceforth as XC and XAL, respectively, are obtained as follows. The data is partitioned into segments of length N, multiple of all relevant periodic modes. Our simulations have shown that two such partitions are needed in the present case, as one partition alone is not sufficient to remove the periodicities completely. First, averages over N = 52 weeks are performed, and afterward a second detrending with N = 1 day follows on consecutive periods of 14 days. Next, a mean segment is calculated by averaging measurements with the same relative position in the periodic segment. Finally, the detrended data set is obtained by subtracting the respective values of the mean segment from the measured data[\cite=no2]. Whereas the Markovian framework can strictly only be applied after removing all the periodicities by detrending (or a similar procedure), we have verified that the estimates of the drift and diffusion coefficients and of the mean orientation angle are not altered significantly by the second step of the detrending procedure, i. e. after removing the periodicities only partially, which confirms that our detrending method does not introduce artifacts.

As indicated by Fig. [\ref=fig3], the detrended data do not show patterns of periodicity, satisfying both Markov properties as well as the delta-correlated signature of the Gaussian-like noise[\cite=no2]. One therefore may consider the series XC and XAL as a set described by two coupled Langevin Equations.

Having obtained the detrended data, the method described in Sec. [\ref=subsec:optimal] is then applied, yielding a two-dimensional drift vector and diffusion matrix. The diffusion matrix is diagonalized, yielding two eigenvalues and their normalized eigenvectors, orthogonal to each other. Multiplying them by their corresponding eigenvalues yields two orthogonal vectors that define an ellipse in phase space (XC,XAL). These diffusion ellipses have a specific orientation defined through an angle whose absolute value quantifies the relative off-diagonal contribution that describes the coupling of the noise terms by the diffusion matrix[\cite=no2]. Rotating all the ellipses by the orientation angle, aligns the largest eigenvector along one of the coordinate axis and the smallest eigenvector along the other one. At each mesh point we construct from the numerical values of the two detrended time series, and the respective functional dependency of the eigenvalues on the original variables, two transformed time series, V+O and V-0, which we take as additional input data for the ANN model. We then test the resulting improvement of the forecasts. The optimal variable corresponding to the largest eigenvalue is henceforth symbolized by V+O while the other will be represented by V-O. As reported below, the variable V+O shows a higher rank of importance than the variable V-O when selecting the most important variables for ANN training.

To select a reduced set of the Ms + input variables we conducted a forward stepwise regression (FSR) between the meteorological and the air quality variables for each monitoring station independently. The FSR allows variables to be optimized in order to predict NO2 at each monitoring station. This procedure starts with the variable most correlated with the target, and adds one new variable which, together with the previous one, most accurately predicts the target, i.e. the variable that reduces the predicting error the most. One adds iteratively new variables in this way, ordering the set of Ms + variables by descending order of correlation with the target. The procedure stops when any new variable does not significantly reduce the prediction error. The corresponding significance of error reduction is measured by a partial F-test[\cite=numrecip]. With this approach, given an initial set of variables for one monitoring station, one is able to select the best subset of variables for predicting the evolution of NO2 concentration at that station.

In this study, we consider four distinct sets of variables for the input data, defining therefore four distinct ANN models.

The first set is the complete set of (up to, depending on the lag investigated) initial variables, neglecting the transformed variables V+O and V-O, and is used for training the ANN, which then defines the standard ANN model, henceforth represented by Ms. Another set corresponds to the first set with the transformed variables V+O and V-O, yielding the model Ms +.

The third set is extracted from the Ms + variables by FSR, yielding the model Ms,FSR.

The forth set is a subset of this extended set and uses the correlation ordering made for all the variables, where it considers only the variable V+O and the variables having stronger correlation with the target. This is our optimal model MO.

Table [\ref=fig_tabela_y] shows the number of variables used for each model and for five different time lags. Depending on this time lag, namely t - 1, t - 3, t - 6, t - 12 and t - 24 hours, the input data includes hourly data from the previous [formula] hours, [formula] hours, [formula] hours, [formula] hours and t = 24 hours, respectively.

While model Ms + uses always two more variables than the standard model Ms, the number of variables for the optimal model MO is typically smaller then the number of variables for Ms,FSR.

In the last column of Tab. [\ref=fig_tabela_y], we indicate the relative reduction on the number of input variables when using model MO instead of model Ms,FSR. The reduction of the number of variables used for training the ANN model is typically above 50%, with only three exceptions, raising up to 90% for the largest time-lag predictions. Notice that by construction the number in column MO indicates the rank of the variable V+O when ordering the variables according to their correlation with the target. This result also shows the strong correlation between our transformed variable V+O and the target.

In all cases, the variable V+O shows a higher correlation with the target than the variable V-O, therefore V-O does not appear in the subsets for MO. The reason for this higher correlation is associated with the strength of the corresponding fluctuations. Since the variable V+O is the transformed variable corresponding to the largest eigenvalue of the diffusion matrix for the coupled system of both NO2 concentrations, XC and XAL, the pair of variables fluctuate stronger along this direction and therefore contains a larger part of the correlations than the other transformed variable V-O. This situation is somehow comparable to the assumptions of the principal component analysis model [\cite=Pearson].

Stochastic variables as optimal input for neural networks

In the previous section we concluded that one of our transformed variables is more correlated with the target than most of the meteorological and air quality variables. Collecting only variables with an equal or higher correlation than the one observed for V+O, one obtains the model MO, which must now be compared with the standard model MS,FSR in its predictive power. In this section we present such comparison between both models. Using different measures of predictive power, we conclude that, in general, the optimal model most frequently evidences the same predictive power as model MS,FSR. Results are summarized in Tab. [\ref=fig_tabela_resultados].

The overall conclusion is that, despite the significant decrease of input data, the predictive power of model MO is not worse than the one of the models MS,FSR.

To evaluate the efficiency and performance of our method four other quantities are computed. The first such quantity is the well-known Pearson correlation coefficient ("PC")[\cite=Pearson],

[formula]

where yi denotes the respective model forecast at time i and oi denotes the real observed values at time i.

From Tab. [\ref=fig_tabela_resultados] one sees that the difference in the Pearson correlation between both models, MS,FSR and MO, is almost nonexistent, except for the largest time-lag, namely t - 24. The PC results for the independent sample (2006) are relatively lower than for the calibration-validation period as expected, with the exception of the PC for AL (t=24 hours).

Another important quantity is the skill against persistence ("Skillp") which can be interpreted as the percentage of improvement that our model can provide when compared with the persistence model, i.e. the forecast for a given hour is the observed value of the previous time lag, i.e., 1h, 3h, 6h, 12h or 24h hours before. It is given by

[formula]

where ŷi is the value of the time series variable y 1, 3, 6, 12 or 24 hours before.

From Tab. [\ref=fig_tabela_resultados] one sees that the decrease in the skill against persistence between both models, MS,FSR and MO, is not significant ([formula]) for short time-lags except for the largest time-lag, namely t - 24. The skill against persistence results for the independent sample (2006) are higher than for the calibration-validation period for all the time lags.

Another quantity is the root mean square error ("RMSE") which represents the difference between the pairs of forecast and observation values and is given by

[formula]

While for shorter time-lags there is almost no difference between both models, a slight increase ([formula]) of RMSE is observed in the optimal model for the t=24 time lag. The RMSE for the independent sample are usually lower for the MO, except for the AL t=3 case.

Finally, the mutual information evaluates the dependence between observations and predictions and is given by

[formula]

where p(y,o) is the joint probability of observations and predictions for the same time-steps and p(y) and p(o) are the corresponding marginal distributions, computed within the range of admissible values, Ybins and Obins, properly discretized. With one single exception in the 2002-2005 period and one in the 2006 period, no deviations are observed when comparing the mutual information in both models.

Enabling a significant reduction of input data - less than one half - and simultaneously presenting a predictive power at least as good as the one given by the standard model, one can conclude that the model MO using our stochastic may be seen as a good alternative for establishing ANN models in air quality prediction. Figure [\ref=fig4] shows for both Chelas and Avenida da Liberdade the scatter plot between observations (NO2 measurements) and the corresponding predictions from a ANN model training with the optimized model MO, indicating a consistently good agreement.

Conclusions

In this paper we applied a method for deriving eigenvariables in systems of coupled stochastic variables, which were then used as input variables to considerably reduce the amount of input data needed for training the ANN, typically for than a factor of two and for large time-lags by a factor of ten. The predictive power is maintained. Indeed, the introduction of the stochastic variables as input data for training the ANN model allows to preserve the predictive power with considerable less input information.

This is because the variable incorporates temporal correlations between independent and spatially separated monitoring stations.

In the particular context of atmospheric environment, the reduction of the amount of input data, optimizes the ANN models in the sense that enables faster predictive outcome without changing significantly the predictive power. Thus, our stochastic variables together with the findings of this study can be taken as a first step for improving alarm systems, making them more efficient in predicting periods of lower levels in the air quality. Moreover, being a general numerical procedure for any given set of measurements, our finding can be easily adapted to other ANN models in weather or geophysical forecast. An extension of this work to take into account the correlations between a higher number of measurement stations is planned.

Acknowledgments

The authors thank DAAD and FCT for financial support through the bilateral cooperation DREBM/DAAD/03/2009. FR (SFRH/BPD/65427/2009) and PGL (Ciência 2007) thank Fundaão para a Ciência e a Tecnologia for financial support, also with the support Ref. PEst-OE/FIS/UI0618/2011. The authors would like to acknowledge Agência Portuguesa do Ambiente and European Centre for Medium Weather Forecast for providing the environmental and meteorological data, respectively.