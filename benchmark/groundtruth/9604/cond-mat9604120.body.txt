[formula] and the Kink Soliton

Bruce Bassett

International School for Advanced Studies, Via Beirut 2-4, 34014 Trieste, Italy

Keywords: Error function, numerical approximation, kink soliton.

Introduction

There is an inherent asymmetry between integration and differentiation which makes integration somewhat of an art form, and which is perhaps best exemplified by the lack of an elementary indefinite integral of the celebrated Gaussian:

[formula]

The fact that such an integral does not in fact exist follows from the work of Laplace [\cite=rosen72] [\cite=rosen68]. However, the Gaussian integral is fundamental, finding applications in statistics, error theory and many branches of physics. In fact, anywhere one has Gaussian distributions, cummulatives of these distributions will involve the above integral. Only special case definite integrals of e- x2 are known, the most famous being:

[formula]

In addition there is the series expansion [\cite=GR80]:

[formula]

Now in practise one can evaluate the integral accurately by numerical methods or tables, but in many cases it would be preferable to have an analytical solution, even if it were not exact, as long as the maximum error were very small and the approximation were simple .

It turns out that there exists a function well known in the analysis of nonlinear partial differential equations whose derivative is very close to Gaussian - the kink soliton:

[formula]

with derivative:

[formula]

where A,b,c, and β are all real constants. The graphs of e- x2 and χ(x) are shown in figure (1). The kink soliton is the positive, time-independent, topological solution to the non-linear 1 + 1 dimensional partial differential equation:

[formula]

where a subscript denotes partial derivative with respect to that variable. The solution to this equation is topological because the boundary conditions at x  =    ±    ∞   are different. Leaving the physical origin of φ behind, it is interesting to examine the series expansion of tanh (x):

[formula]

which should be compared with eq. ([\ref=eq:series1]) for [formula]. Here Bk are the Bernoulli numbers with generating function t / (et - 1). We see that although the coefficients differ in each case, the powers of x in the expansions are identical. Further both χ(x) and e- x2 have the property that their derivatives can be re-expressed in terms of themselves and φ(x) or powers of x respectively. These observations shed some light on the foundations of the approximation.

Details of the approximation

Turning to practical issues, we are left with choosing the constants, A,b,c to optimise the approximation of eq. ([\ref=eq:gauss]). We need three constraints to fix the three parameters. First we require that the Gaussian and χ(x) have the same symmetry axis. This requires the argument of tanh  to vanish at x*  =  β which immediately implies from eq. ([\ref=eq:family]) that c  =  b.

At this stage we have a choice, dependent on whether we are interested in an approximate solution for small or large x. For large x, a constraint is obviously that our new approximation, φ(x), must give exactly the same result as eq. ([\ref=eq:infint]) when differenced at infinity and the origin. This will ensure convergence of our approximation. Since tanh (x)  →  1 as x  →    ∞  , and tanh (0)  =  0, this implies from eq. ([\ref=eq:kink]) that:

[formula]

Finally we can impose that χ(x)  =  e- (x - β)2  /  σ2 at some point, i.e, we match the derivatives. We will choose x  =  β as the simplest. This gives:

[formula]

In fact the two are equal at another point as can be seen from figure (1). Our analytical approximation, which is very accurate for large x, is therefore:

[formula]

where in this paper ≃ is understood as meaning asymptotic convergence, as x  →    ∞   and bounded error [formula]. From figures (1,2) we see that the kink derivative underestimates the Gaussian at small (x  -  β)2 and overestimates it at large (x  -  β)2.

Alternatively if one is interested in [formula] where u  ≤  4σ say, then this will not be good enough, since the error in our approximation is strongly confined to small x. Instead we can impose that φ(x) must give the exact result, not at infinity, but at the end of the interval, i.e. at u. Thus we impose:

[formula]

In addition we need to match the derivatives χ(x*)  =  e- (x*  -  β)2  /  σ2) at some point x* as before, and then solve the equations for A,b. It is an open question which matching point yields the best results. For illustrative purposes we choose x  =  β and again find A  =  1 / b, so that substituting in eq.([\ref=eq:smallx]) gives us a nonlinear root-finding problem for A. The right-hand side can be found for example, from tables of the error function, (x). This yields an approximation which is exact at x  =  u and hence a much better approximation for small x, but which is invalid for x  ≫  u. The extension to cases with variable lower limit of integration is obvious and will not be considered.

One might be tempted to generalise eq. ([\ref=eq:kink]) to a one-parameter family of approximations to the error function:

[formula]

which have derivative:

[formula]

However, since for [formula], Δp'(0)  =  0, they are not really suitable as approximations to a Gaussian. Rather they are skewed distributions with maxima at x  >  0. It turns out however, that they will be useful later.

For testing our approximation we will use the φ(x) valid for large x, denoted φ(x)L, given by eq. ([\ref=eq:integral]). The crucial question is of course, how good is this approximation ? It turns out that it is very good in most cases, as can be seen from figures (3) and (4). The maximum error from using φ(x)L is 3.91% at x  =  1.12. However as discussed earlier, if one is interested in the result for small x, and x1 is small, then this is not the best approximation to use. In practise, the error drops off very quickly due to the exponential nature of tanh (x). For example, the error in estimating (x) drops below 1% for x  ≥  2.3 and at x  =  5 the error is 2.51  ×  10- 5. The error as a function of x is plotted in figure (4).

Improving the approximation

The shape of figure (4) is, in fact, rather startling because it is a very simple shape. From the graph it has a single local maximum and hence two points where the concavity changes. Hence although it cannot be written down explicitely in terms of elementary functions [\cite=rosen68], it can be approximated very closely. Several fitting shapes were tried, such as the log-normal and Poisson distributions, but the best was found to be a generalised Maxwell-distribution:

[formula]

For the case used in the figures, that of (x), the best parameters for reducing the maximum error (i.e. minimising w.r.t. the sup-norm ||  ·  ||∞) were (see figure (5)):

[formula]

which reduced the maximum error to 0.15%. It is also likely that our choice of function and parameters for E(x) is not optimal, since formal optimisation was not used, but was based rather on a numerical investigation of the parameter space {α1,α2,n}.

Further, since the required E(x) is a skewed Gaussian with maximum at non-zero x we can profitably employ the functions given by eq. ([\ref=eq:derivdel]), originally introduced to model the Gaussian, as fits for the error. In this case our approximation becomes:

[formula]

where ' denotes derivative w.r.t. x. For α3  =  0.23 and p  =  9.7 the error is at most 9  ×  10- 3. By suitable generalisation of the second term it is possible to increase the accuracy to the level of the generalised Maxwell distribution, but for simplicity and because of its suggestiveness, we leave it in the above form.

In the case of the error function we have explicitely that (β  =  0):

[formula]

where [formula] is the error function. Similarly the complementary error function is given by: [formula].

Moments of the soliton

A fundamental feature of a Gaussian distributed random variable is that all moments above the second, such as the skewness, are zero. From this it follows that the sum of error distributed random variables is itself error distributed. A natural question to ask is how well the soliton approximation preserves this feature.

To make this more precise: given the distribution P(x), we may define the partition function Z(J) via:

[formula]

From the "free energy" F(J)  =   ln Z(J) we may now define the n-th moment, Mn, of P(x) as:

[formula]

Thus in the case of a Gaussian distribution with zero mean, it is easy to show that the free energy is a quadratic function of J. Hence the only non-zero moment is the second, i.e. the variance, as claimed above. In the case of the soliton approximant we have:

[formula]

which is unfortunately not known analytically, so we resort to numerical analysis. Using the Gaussian case as a testbed we approximated the free energy with an 8-th degree polynomial:

[formula]

For a Gaussian αn  =  0, ~  ~ n  ≥  3. Using a least-squares method, the error, i.e. the largest αn coefficient which is zero in the exact case but non-zero in the fit, was α3  =  2.535  ×  10- 8. Each subsequent coefficient was roughly an order of magnitude smaller than the preceding one.

In the case of the soliton approximation, given by eq. ([\ref=eq:family]), the error was 2.309  ×  10- 3 again for the cubic term, and again with roughly αn + 1  ~  αn / 10.

Table (1) shows a comparison between the coefficients of the free-energy polynomials for the terms higher than cubic for the exact Gaussian and the soliton approximant χ(x). An interesting thing to note is that, although the accuracy is at the level one might expect, i.e. ~  10- 3, the pattern of the terms is identical; namely both the signs and the decrease in the coefficients have the same behaviour in both cases. This suggests that numerical errors will be "coherent", i.e. the errors one has from numerical integration of the Gaussian will be of the same nature as those one obtains from the soliton approximation. This is perhaps obvious given the similarity of their power series (see eq.s ([\ref=eq:series1],[\ref=eq:series2])) but will not be true for other approximants in terms of e.g. rational functions [\cite=AS65].

We leave this discussion by noting that inclusion of E(x), via e.g. eq. ([\ref=eq:differr]), in the calculation of moments will reduce the above errors considerably, presumably by a factor of at least 102.

Applications

Let us now consider a small sample of applications. A primary example is in the theory of statistics. If we have a uniformly distributed random variable χ and we desire a random variable y with statistics given by a distribution f, first define the integral [formula]. Then y  =  F- 1(x) will have the same distribution as f, where F- 1 denotes the inverse of F, on the interval

[formula]

Conclusions

In this Letter we have presented a function approximating (x) to better than [formula], with exponential convergence as x  →    ∞  . This solution is simply the kink soliton, [formula] and can be optimised for accuracy if the error function at small values of the argument is required.

Further we have found a solution with maximum error of 0.15% by adding a generalised Maxwell distribution to the kink soliton, equations ([\ref=eq:differr]), ([\ref=eq:tot]). Future work should be aimed at finding truly optimal solutions. Finally a few applications were discussed, particularly to diffusion dynamics and to the generation of Gaussian random fields.

The author would like to thank Prof. Domb, Claudio Scrucca and Lando Caiani for illuminating discussions and Stefano Bianchini for a very useful critical reading of the manuscript.