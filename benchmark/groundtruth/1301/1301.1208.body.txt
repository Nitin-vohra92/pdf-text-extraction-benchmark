Lemma Corollary Definition Observation Remark

Significance testing without truth

Introduction

As pointed out in the above quotation of G. E. P. Box, many interesting models are false (that is, not exactly true), yet are useful nonetheless. Significance testing helps measure the usefulness of a model. Testing the validity of using a model for virtually any purpose requires knowing whether observed discrepancies are due to inaccuracies or inadequacies in the model or (on the contrary) could be due to chance arising from necessarily finite sample sizes. Significance tests gauge whether the discrepancy between the model and the observed data is larger than expected random fluctuations; significance tests gauge the size of the unavoidable random fluctuations.

A traditional approach, along with its modern formulation in statistical decision theory, tries to decide whether a hypothesized model is likely to be true (or false). However, in many practical circumstances, a significance test need only gauge the consistency (or inconsistency) of the observed data with the assumed hypothesis/model -- without ever enquiring as to whether the assumption is likely to be true (or false), or whether some alternative is likely to be true (or false). In this practical formulation, a significance test rejects a hypothesis/model only if the observed data is highly improbable when calculating the probability while assuming the hypothesis being tested. Whether or not the assumption could be exactly true in reality is irrelevant.

An illustrative example may help clarify. When testing the goodness of fit for the Poisson regression where the distribution of Y given x is the Poisson distribution of mean exp (θ(0)  +  θ(1)x + θ(2)x2  +  θ(3)x3), the conventional Neyman-Pearson null hypothesis is

[formula]

where

[formula]

for k  =  1, 2, , n, and the observations (x1,y1), (x2,y2), , (xn,yn) are ordered pairs of scalars (real numbers paired with nonnegative integers). A related but perhaps simpler null hypothesis is

[formula]

where

[formula]

for k  =  1, 2, , n, with [formula] being a maximum-likelihood estimate. Needless to say, even if the observed data really does arise from Poisson distributions whose means are exponentials of a cubic polynomial, the particular values (0), (1), (2), (3) of the parameters of the fitted polynomial will almost surely not be exactly equal to the true values. Even though the estimated values of the parameters may not be exactly correct, it still makes good sense to enquire as to whether the fitted cubic polynomial is consistent with the data up to random fluctuations inherent in using a finite amount of observed data.

In fact, since subsequent use of the model usually involves the particular fitted polynomial -- whose specification includes the observed parameter estimates -- analyzing the model including the estimated values of the parameters makes more sense than trying to decide whether the data really did come from Poisson distributions whose means are exponentials of some unspecified cubic polynomial. For instance, any plot of the fit (such as a plot of the means of the Poisson distributions) must use the estimated values of the parameters, and any statistical interpretation of the plot should also depend explicitly on the estimates; a significance test can gauge the consistency of the plotted fit with the observed data, without ever asking whether the plotted fit is the truth (it is almost surely not identical to the underlying reality) and without making some decision about an abstract family of polynomials which may or may not include both the plotted fit and the underlying reality.

A popular measure of divergence from the null hypothesis is the log-likelihood-ratio

[formula]

A P-value (see, for example, Section [\ref=Pvalues] below) quantifies whether this divergence is larger than expected from random fluctuations inherent in using only n data points. It is not obvious how to calculate an exact P-value for HNP0 from ([\ref=theirs]) and ([\ref=theirsaux]), which refers to cubic polynomials with undetermined coefficients. In contrast, H0 from ([\ref=ours]) and ([\ref=oursaux]) refers explicitly to the particular fitted value [formula]; H0 concerns the particular fit displayed in a plot, and is natural for the statistical interpretation of such a plot.

Thus, when calculating significance, the assumed model should include the particular values of any parameters estimated from the observed data. Such parameters are known as "nuisance" parameters. As illustrated with H0 from ([\ref=ours]) and ([\ref=oursaux]), the assumed hypothesis will be "simple" in the Neyman-Pearson sense, but will depend on the observed values of the parameters -- that is, the hypothesis will be "data-dependent"; the hypothesis will be "random." Including the particular values of the parameters estimated from the observed data replaces the "composite" hypothesis of the conventional Neyman-Pearson formulation with a "simple" data-dependent hypothesis. As discussed in Section [\ref=distribution] below, fully conditional tests also incorporate the observed values of the parameters, but make the extra assumption that all possible realizations of the experiment -- observed or hypothetical -- generate the same observed values of the parameters. The device of a "simple data-dependent hypothesis" such as H0 incorporates the observed values explicitly without the extra assumption.

For most purposes, a parameterized model is not really operational -- that is, suitable for making precise predictions -- until its specification is completed via the inclusion of estimates for any nuisance parameters. The results of the significance tests considered below depend on the quality of both the models and the parameter estimators. However, the results are relatively insensitive to the particular observed realizations of the parameter estimators (that is, to the parameter estimates) unless specifically designed to quantify the quality of the parameter estimates. To quantify the quality of the parameter estimates, we recommend testing separately the goodness of fit of the parameter estimates, using confidence intervals, confidence distributions, parametric bootstrapping, or significance tests within parametric models, whose statistical power is focused against alternatives within the parametric family constituting the model (for further discussion of the latter, see Section [\ref=many] below).

The remainder of the present article has the following structure: Section [\ref=Bayesvsfreq] very briefly discusses Bayesian-frequentist hybrids, referring for details to the definitive work of [\cite=gelman]. Section [\ref=Pvalues] defines P-values -- also known as "attained significance levels" -- which quantify the consistency of the observed data with the assumed models. Section [\ref=distribution] details several approaches to testing the goodness of fit for distributional profile. Section [\ref=many] discusses testing the goodness of fit for various properties beyond just distributional profile.

[\cite=cox] details many advantages of interpreting significance as gauging the consistency of an assumption/hypothesis with observed data, rather than as making decisions about the actual truth of the assumption. However, significance testing is meaningless without any observations, unlike purely Bayesian methods, which can produce results without any data, courtesy of the prior (the prior is the statistician's system of a priori beliefs, accumulated from prior experience, law, morality, religion, etc., without reference to the observed data). Significance tests are deficient in this respect. Those interested in what is to be considered true in reality and in making decisions more generally should use Bayesian and sequential (including multilevel) procedures. Significance testing simply gauges the consistency of models with observed data; generally significance testing alone cannot handle the truth.

Bayesian versus frequentist

Traditionally, significance testing is frequentist. However, there exist Bayesian-frequentist hybrids known as "Bayesian P-values"; [\cite=gelman] sets forth a particularly appealing formulation. Bayesian P-values test the consistency of the observed data with the model used together with a prior for nuisance parameters. In contrast, the P-values discussed in the present paper test the consistency of the observed data with the model used together with a parameter estimator. In the Bayesian formulation, a P-value depends explicitly on the choice of prior; in the formulation of the present paper, a P-value depends explicitly on the choice of parameter estimator. Thus, when there are nuisance parameters, the two types of P-values test slightly different hypotheses and provide slightly different information; each type is ideal for its own set-up. Of course, if there are no nuisance parameters, then Bayesian P-values and the P-values discussed below are the same.

P-values

A P-value for a hypothesis H0 is a statistic such that, if the P-value is very small, then we can be confident that the observed data is inconsistent with assuming H0. The P-value associated with a measure of divergence from a hypothesis H0 is the probability that D  ≥  d, where d is the divergence between the observed and the expected (with the expectation following H0 for the observations), and D is the divergence between the simulated and the expected (with the expectation following H0 for the simulations, and with the simulations performed assuming H0). When taking the probability that D  ≥  d, we view D as a random variable, while viewing d as fixed, not random. For example, when testing the goodness of fit for the model of i.i.d. draws from a probability distribution p0(θ), where θ is a nuisance parameter that must be estimated from the data, that is, from observations x1, x2, , xn, we use the null hypothesis

[formula]

The P-value for H0 associated with a divergence δ is the probability that D  ≥  d, where

d  =  δ(p̂,p0()),

p̂ is the empirical distribution of x1, x2, , xn,

[formula] is the parameter estimate obtained from the observed draws x1, x2, , xn,

D  =  δ(P̂,p0()),

P̂ is the empirical distribution of i.i.d. draws X1, X2, , Xn from p0(), and

[formula] is the parameter estimate obtained from the simulated draws X1, X2, , Xn.

If the P-value is very small, then we can be confident that the observed data is inconsistent with assuming H0. Examples of divergences include χ2 (for categorical data) and the maximum absolute difference between cumulative distribution functions (for real-valued data).

To compute the P-value assessing the consistency of the experimental data with assuming H0, we can use Monte-Carlo simulations (very similar to those of [\cite=clauset-shalizi-newman]). First, we estimate the parameter θ from the n given experimental draws, obtaining [formula], and calculate the divergence between the empirical distribution and p0(). We then run many simulations. To conduct a single simulation, we perform the following three-step procedure:

we generate n i.i.d. draws according to the model distribution p0(), where [formula] is the estimate calculated from the experimental data,

we estimate the parameter θ from the data generated in Step 1, obtaining a new estimate [formula], and

we calculate the divergence between the empirical distribution of the data generated in Step 1 and p0(), where [formula] is the estimate calculated in Step 2 from the data generated in Step 1.

After conducting many such simulations, we may estimate the P-value for assuming H0 as the fraction of the divergences calculated in Step 3 that are greater than or equal to the divergence calculated from the empirical data. The accuracy of the estimated P-value is inversely proportional to the square root of the number of simulations conducted; for details, see Remark [\ref=error-bars] below.

The standard error of the estimate from Remark [\ref=MonteCarlo] for an exact P-value P is [formula], where [formula] is the number of Monte-Carlo simulations conducted to produce the estimate. Indeed, each simulation has probability P of producing a divergence that is greater than or equal to the divergence corresponding to an exact P-value of P. Since the simulations are all independent, the number of the [formula] simulations that produce divergences greater than or equal to that corresponding to P-value P follows the binomial distribution with [formula] trials and probability P of success in each trial. The standard deviation of the number of simulations whose divergences are greater than or equal to that corresponding to P-value P is therefore [formula], and so the standard deviation of the fraction of the simulations producing such divergences is [formula]. Of course, the fraction itself is the Monte-Carlo estimate of the exact P-value (we use this estimate in place of the unknown P when calculating the standard error [formula]).

Goodness of fit for distributional profile

Given observations x1, x2, , xn, we can test the goodness of fit for the model of i.i.d. draws from a probability distribution p0(θ), where θ is a nuisance parameter, via the null hypothesis

[formula]

The Neyman-Pearson formulation considers instead the null hypothesis

[formula]

The fully conditional null hypothesis is

[formula]

That is, whereas H0 supposes that the particular observed realization of the experiment happened to produce a parameter estimate [formula] that is consistent with having drawn the data from p0(), HFC0 assumes that every possible realization of the experiment -- observed or hypothetical -- produces exactly the same parameter estimate. Few experimental apparatus constrain the parameter estimate to always take the same (a priori unknown) value during repetitions of the experiment, as HFC0 assumes. Assuming HFC0 amounts to conditioning on a statistic that is minimally sufficient for estimating θ; computing the associated P-values is not always trivial. Furthermore, the assumption that HFC0 is true seems to be more extreme, a more substantial departure from HNP0, than H0. Finally, testing the significance of assuming H0 would seem to be more apropos in practice for applications in which the experimental design does not enforce that repeated experiments always yield the same value for p0(). We cannot recommend the use of HFC0 in general. Unfortunately, HNP0 also presents problems.

If the probability distributions are discrete, there is no obvious means for defining an exact P-value for HNP0 when HNP0 is false; moreover, any P-value for HNP0 when HNP0 is true would depend on the correct value of the parameter θ, and the observed data does not determine this value exactly. The situation may be more favorable when measuring discrepancies with divergences that are "approximately ancillary" with respect to θ, but quantifying "approximately" seems to be problematic except in the limit of large numbers of draws. (Some divergences are asymptotically ancillary in the limit of large numbers of draws, but this is not especially helpful, as any asymptotically consistent estimator [formula] converges to the correct value in the limit of large numbers of draws; θ is almost surely known exactly in the limit of large numbers of draws, so there is no benefit to being independent of θ in that limit.) Section 3 of [\cite=robins-wasserman] reviews these and related issues.

[\cite=romano], [\cite=henze], [\cite=bickel-ritov-stoker], and others have shown that the P-values for H0 converge in distribution to the uniform distribution over

[formula]

The surveys of [\cite=agresti1] and [\cite=agresti2] discuss exact P-values for contingency-tables/cross-tabulations, including criticism of fully conditional P-values. [\cite=gelman] provides further criticism of fully conditional P-values. [\cite=ward] numerically evaluates the different types of P-values for an application in population genetics. Section 4 of [\cite=bayarri-berger2] and the references it cites discuss the menagerie of alternative P-values proposed recently.

Goodness of fit for various properties

For comparative purposes, we first review the null hypothesis of the previous section for testing the goodness of fit for distributional profile, namely

[formula]

with θ being the nuisance parameter. The measure of discrepancy for H0 is usually taken to be a divergence between the empirical distribution p̂ and the model p0() (in the continuous case in one dimension, a common characterization of the empirical distribution is the empirical cumulative distribution function; in the discrete case, a common characterization of the empirical distribution is the empirical probability mass function, that is, the set of empirical proportions). One example for p0 is the Zipf distribution over m bins with parameter θ, a discrete distribution with the probability mass function

[formula]

for j  =  1, 2, 3, , m, where the normalization constant is

[formula]

and θ is a nonnegative real number.

When testing the goodness of fit for parameter estimates, we use the null hypothesis

[formula]

with θ being the nuisance parameter and φ being the parameter of interest (and with φ0 being the value of φ assumed under the model). Please note that H0 and H'0 are actually equivalent, via the identification p0(θ)  =  p0(φ0,θ). However, the measure of discrepancy for H'0 is usually taken to be a divergence between [formula] and φ0 rather than the divergence between p̂ and p0() that is more natural for H0. Also, if φ is scalar-valued, then confidence intervals, confidence distributions, and parametric bootstrap distributions are more informative than a significance test. A significance test is appropriate if φ is vector-valued. One example for p0 is the sorted Zipf distribution over m bins with θ being the power in the power law and with the maximum-likelihood estimate [formula] being a permutation that sorts the bins into rank-order, that is, p0 is the discrete distribution with the probability mass function

[formula]

for j  =  1, 2, 3, , m, where the normalization constant Cθ is defined in ([\ref=normalization]) with θ being a nonnegative real number, and φ is a permutation of the numbers 1, 2, , m. The choice for φ0 that is of widest interest in applications is the identity permutation (that is, the "rearrangement" of the bins that does not permute any bins: φ0(j)  =  j for j  =  1, 2, , m).

When testing the goodness of fit for the standard Poisson regression with the distribution of Y given x being the Poisson distribution of mean [formula], we use the null hypothesis

[formula]

where θ is the nuisance parameter and [formula] is a maximum-likelihood estimate. The measure of discrepancy for H''0 is usually taken to be the log-likelihood-ratio (also known as the deviance)

[formula]

where k is the mean of the Poisson distribution associated with yk in H''0, namely,

[formula]

One example is the cubic polynomial

[formula]

for k  =  1, 2, , n, which comes from the choice m  =  3 and

[formula]

for k  =  1, 2, , n, given observations as ordered pairs of scalars (x1,y1), (x2,y2), , (xn,yn). Of course, there are similar formulations for other generalized linear models, such as those discussed by [\cite=mccullagh-nelder].

Acknowledgements

We would like to thank Alex Barnett, Andrew Barron, Gérard Ben Arous, James Berger, Tony Cai, Sourav Chatterjee, Ronald Raphael Coifman, Ingrid Daubechies, Jianqing Fan, Jiayang Gao, Andrew Gelman, Leslie Greengard, Peter W. Jones, Deborah Mayo, Peter McCullagh, Michael O'Neil, Ron Peled, William H. Press, Vladimir Rokhlin, Joseph Romano, Gary Simon, Amit Singer, Michael Stein, Stephen Stigler, Joel Tropp, Larry Wasserman, Douglas A. Wolfe, and Bin Yu. This work was supported in part by Alfred P. Sloan Research Fellowships, an NSF Postdoctoral Fellowship, a Donald D. Harrington Faculty Fellowship, and a DARPA Young Faculty Award.