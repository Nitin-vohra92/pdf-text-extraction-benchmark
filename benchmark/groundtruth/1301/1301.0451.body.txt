Corollary Lemma Proposition Definition Remark Notation Notations Remarks Example Claim Assumption

=10 pt

General limit value in Dynamic Programming

Introduction

We consider a dynamic programming problem with arbitrary state space Z and bounded rewards. Is it possible to define in an unique way a possible limit value for the problem, where the "patience" of the decision-maker tends to infinity ?

For each evaluation (probability distribution over positive integers) θ = (θt)t  ≥  1, we consider the value function vθ of the problem where the initial state is arbitrary in Z and the weight of any stage t is given by θt. The total variation of θ, that we also wall the impatience of θ, is defined by [formula] For instance, for each positive integer n the evaluation θ = (1 / n,...,1 / n,0,...,0,...) induces the value function n corresponding to the maximization of the mean payoff for the first n stages; and for any λ in (0,1] the evaluation θ = (λ(1 - λ)t - 1)t induces the discounted value function vλ.

A well known theorem of Hardy and Littlewood (see e.g. Lippman, 1969) implies that for an uncontrolled problem, the pointwise convergence of (n)n, when n goes to infinity, and of (λ)λ, when λ goes to 0, are equivalent, and that in case of convergence both limits are the same. However, Lehrer and Sorin (1992) provided an example of a dynamic programming problem where (n)n and (λ)λ have different pointwise limits. But they also proved that the uniform convergence of (n)n and of (λ)λ are equivalent, with equality of the limit in case of convergence. And Sorin and Monderer (1993) extended this result to families of evaluations satisfying some conditions. Mertens and Neyman (1982) proved that when the family (λ)λ not only uniformly converges but has bounded variation, then the dynamic programming problem has a uniform value, in the sense that for all initial state z and ε > 0, there exists a play with mean payoffs from stage 1 to stage T at least v - ε provided T is large enough (see also Lehrer Monderer 1994 and Sorin Moderer 1993 for proofs that the uniform convergence of (λ)λ or (n)n does not imply the existence of the uniform value of the problem). In this case of existence of a uniform value, one can show that all value functions vθ are close to v*, whenever θ is a non increasing evaluation with small θ1. The reason is that whenever θ is non increasing, the θ-payoff of a play can be expressed as a convex combination of the Ces�ro values (n)n.

In the present paper, we investigate the uniform convergence of sequences (vθk)k when the "impatience" of the evaluations vanishes, in the sense that [formula]. We will prove in theorem [\ref=thm1] that this uniform convergence happens if and only if the metric space {vθk,k  ≥  1} (with the distance between functions given by the sup of their differences), is totally bounded. Moreover the uniform limit, whenever it exists, can only be the following function, which is independent of the particular chosen sequence (θk)k:

[formula]

where for each evaluation θ = (θt)t  ≥  1, the evaluation m,θ is defined as the evaluation with weight 0 for the first m stages and with weight θt - m for stages t > m. Consequently, while speaking of uniform convergence of the value functions when the patience of the decision-maker tends to infinity, v* can be considered as the unique possible limit value. We also give simple conditions on the state space, the payoffs and the transitions (mainly compactness, continuity and non expansiveness) implying the uniform convergence of such value functions.

The paper is organized as follows: section 2 contains the model and the main results, which are shown to extend to the case of stochastic transitions. Section 3 contains a few examples and counterexamples and section 4 contains the proof of theorem [\ref=thm1]. In the last section we formulate the following conjecture, which is shown to be true for uncontrolled problems: does the uniform convergence of (n)n, or equivalently of (λ)λ, implies the general convergence of the value functions, in the sense that: [formula] ?

Model and results

General values in dynamic programming problems

We consider a dynamic programming problem given by a non empty set of states Z, a correspondence F with non empty values from Z to Z, and a mapping r from Z to

[formula]

v (z)= sup ( θ r(z')+ (1-θ)   v(z')),

[formula]

|v(z) -sup v (z')|≤ θ+ | θ- θ|.

[formula]

TV(θ)= |θ-θ|.

[formula]

(1-θ) |θ- θ|≤ θ + TV(θ),

[formula]

|θ -(T)|≤ |θ-θ|≤ TV(θ).

[formula]

v(z)=inf   sup   v(z).

[formula]

Main results

We now state the main result of this paper. Recall that a metric space is totally bounded, or precompact, if for all ε > 0 it can be covered by finitely many balls with radius ε.

Let (θk)k  ≥  1 be a sequence of evaluations such that [formula] We have for all z in Z:

[formula]

Moreover, the sequence (vθk)k uniformly converges if and only if the metric space ({vθk,k  ≥  1},d∞) is totally bounded. And in case of convergence, the limit value is v*.

This theorem generalizes theorem 3.10 in Renault, 2011, which was only dealing with Cesàro evaluations. In particular, there is a unique possible limit point for all sequences (vθk)k such that [formula], and consequently any (uniform) limit of such sequence is v*. Notice that this is not true if we replace uniform convergence by pointwise convergence: even for uncontrolled problems, it may happen that several limit points are possible. As an immediate corollary of theorem [\ref=thm1], when Z is finite the sequence (vθk)k is bounded and has a unique limit point, so converges to v*.

Assume that Z is endowed with a distance d such that: a) (Z,d) is a precompact metric space, and b) the family (vθ)θ∈Θ is uniformly equicontinuous. Then there is general uniform convergence of the value functions to v*, i.e.

[formula]

The proof of corollary [\ref=cor1] from theorem [\ref=thm1] follows from 1) Ascoli's theorem, and 2) the fact that the convergence of (vθk)k to v* for each sequence of evaluations such that [formula] is enough to have the general uniform convergence of the value functions to v*.

Assume that Z is endowed with a distance d such that: a) (Z,d) is a precompact metric space, b) r is uniformly continuous, and c) F is non expansive, i.e. [formula] Then we have the same conclusions as corollary [\ref=cor1], there is general uniform convergence of the value functions to v*, i.e.

[formula]

Proof of corollary [\ref=cor2]. One can proceed as in the proof of corollary 3.9 in Renault, 2011. Given two states z and z', one can construct inductively from each play s = (zt)t  ≥  1 at z a play s = (z't)t  ≥  1 at z' such that d(zt,z't)  ≤  d(z,z') for all t. Regarding payoffs, we introduce the modulus of continuity [formula] of r by:

(α) =  sup z,z's.t.  ≤  α|r(z) - r(z')| for each α  ≥  0.

So |r(z) - r(z')|  ≤  (d(z,z')) for each pair of states z, z', and [formula] is continuous at 0. Using the previous construction, we obtain that for z and z' in Z, for all k  ≥  1, |vθk(z) - vθk(z')|  ≤  (d(z,z')). In particular, the family (vθk)k  ≥  1 is uniformly continuous, and corollary [\ref=cor1] gives the result. [formula]

A completely different proof of corollary [\ref=cor2], with another expression for the limit value v*, can be found in theorem 3.9 of Renault Venel 2012.

Extension to stochastic transitions

We generalize here theorem [\ref=thm1] to the case of stochastic transitions. We will only consider transitions with finite support, and given a set X we denote by Δf(X) the set of probabilities with finite support over X. We consider now stochastic dynamic programming problems of the following form. There is an arbitrary non empty set of states X, a transition given by a multi-valued mapping [formula] with non empty values, and a payoff (or reward) function r:X  →  [0,1]. The interpretation is that given an initial state x0 in X, a decision-maker has to choose a probability with finite support u1 in F(x0), then x1 is selected according to u1 and there is a payoff r(x1). Then the player has to select u2 in F(x1), x2 is selected according to u1 and the player receives the payoff r(x2), etc...

Following Maitra and Sudderth (1996), we say that Γ = (X,F,r) is a Gambling House. We assimilate an element x in X with its Dirac measure δx in Δ(X), we write Z = Δf(X) and an element in Z is written [formula]. In case the values of F only consist of Dirac measures on X, we are in the previous case of a dynamic programming problem.

We linearly extend r and F to Δf(X) by defining for each u in Z, the payoff [formula] and the transition [formula]. A play at x0 is a sequence σ = (u1,...,ut,...)∈Z∞ such that u1∈F(x0) and ut + 1∈F(ut) for each t  ≥  1, and we denote by Σ(x0) the set of plays at x0. Given an evaluation θ, the θ-payoff of a play σ = (u1,...,ut,...) is defined as: [formula], and the θ-value at x0 is:

[formula]

vθ is by definition a mapping from X to

[formula]

v(x)=inf   sup   v(x).

[formula]

x ∈ X,     v(x)= inf sup v(x).

[formula]

ṽ(z)=inf   sup   ṽ(z).

[formula]

v(x)= inf sup v(x).

[formula]

Examples

The first very simple example shows that, even when the set of states is finite, it is not possible to obtain the conclusions of theorem [\ref=thm1] or corollaries [\ref=cor1] and [\ref=cor2] with sequences of evaluations satisfying the weaker convergence condition: sup t  ≥  1θkt  →  k  →    ∞0.

Consider the following dynamic programming problem with 2 states: Z = {z0,z1}, F(z0) = {z1}, F(z1) = {z0}, with payoffs r(z0) = 0 and r(z1) = 1. We have a deterministic Markov chain, so that any play alternates forever between z0 and z1. Define for each k the evaluations [formula] and [formula]. We have vθk(z0) = vθ'k(z1) = 1, and vθk(z1) = vθ'k(z0) = 0 for all k. Define now νk as θk when k is even, and θ'k when k is odd. The evaluation νk satisfies [formula], however (vνk(z0))k and (vνk(z1))k do not converge. [formula]

Lehrer and Sorin (1992) proved that the uniform convergence of the Cesàro values [formula] was equivalent to the uniform convergence of the discounted values (vλ)λ∈(0,1]. The following example shows that this property does not extend to general evaluations: given 2 sequences of TV- vanishing evaluations (θk)k  ≥  1 and (θ'k)k  ≥  1, the uniform convergence of (vθk)k and (vθ'k)k are not equivalent.

In this example, (vn̄)n will pointwise converges to the constant 1/2 whereas for a particular sequence of evaluations (θk)k with total variation going to zero, we will have (vθk)k(z) = 1 for all k and z.

We construct a dynamic programming problem defined via a rooted tree T without terminal nodes (as in Sorin Monderer 1992 or Lehrer Monderer 1994). T has countably many nodes, and the payoff attached to each node is either 0 or 1.

We first construct a tree T1, with countably many nodes and root z0. Each node has an outgoing degree one, except the root which has countably many potential successors z1, z2,..., zn... On the nth branch starting from zn, each node has a unique successor and the payoffs starting from zn are successively 0 for n stages, then 1 for n stages, then 0 until the end of the play.

0,7mm

We now define T inductively from T1. T2 is obtained from T1 by attaching the tree T1 to each node of [formula]. This means that for each node z of [formula] we add a copy of the tree T1 where z plays the role of the root of T1. And for each l, the tree Tl is obtained by attaching the tree T1 to each node of [formula]. Finally, T is defined as the union [formula].

Starting from z0, any sequence of n consecutive payoffs of 1 has to be preceeded by n consecutive payoffs of 0, so n(z0)  ≤  1 / 2 for each n  ≥  1, and for each node z and even integer n it is possible to get exactly n / 2 payoffs of 0 followed by n / 2 payoffs of 1. Consequently one can deduce that (vn(z))n converges to 1/2 for each state z. But sup z∈Zvn(z) = 1 for each n, and the convergence is not uniform.

Consider now for any k, the evaluation [formula] [formula] We have vθk(z) = 1 for all k and z, so (vθk)k uniformly converges to v* = 1. [formula]

The condition ({vθ,θ∈Θ},d∞) totally bounded is satisfied with the hypotheses of corollary [\ref=cor1] or corollary [\ref=cor2], and is sufficient to obtain the general uniform convergence of the value functions. This condition turns out to be stronger than having ({vθk,k  ≥  1},d∞) totally bounded for every sequence of evaluations with vanishing TV.

In the following example, there is no control and the state space Z is the set of all integers, with transition given by the shift: F(z) = {z + 1}. The payoffs are given by r(0) = 1 and r(z) = 0 for all z  ≠  0.

For all evaluations θ = (θt)t  ≥  1, we have sup z∈Zvθ(z) =  sup tθt, so we have general uniform convergence of the value functions to v* = 0.

For all positive t, we can consider the evaluation given by the Dirac measure on t. We have vδt( - t) = 1, and vδt(z) = 0 if z  ≠   - t. The set {vδt,t  ≥  1} is not totally bounded. [formula]

Proof of theorem [\ref=thm1]

We start with a few notations and definitions. We define inductively a sequence of correspondences (Fn)n from Z to Z, by F0(z) = {z} for every state z, and [formula], [formula] (the composition being defined by [formula]). Fn(z) represents the set of states that the decision maker can reach in n stages from the initial state z. We also define for every state z, [formula] and [formula]. The set G∞(z) is the set of states that the decision maker, starting from z, can reach in a finite number of stages.

For all θ in Θ, m  ≥  0 and initial state z, we clearly have:

[formula]

In the sequel, we fix a sequence of evaluations (θk)k  ≥  1 such that [formula]

For all m0  ≥  0 and z in Z,

[formula]

Proof: For each k, we have vθk(z)  ≥  v1,θk(z)  -  θk1  -  TV(θk) by lemma [\ref=lem1], so vθk(z)  ≥  v1,θk(z)  -  2TV(θk). Iterating, we obtain that:

vθk(z)  ≥   sup m  ≤  m0vm,θk(z)  -  2m0TV(θk). [formula]

A key result is the following proposition, which is true for all evaluations θ.

Proof of proposition [\ref=pro1] z and θ being fixed, put β  =   sup z'∈G∞(z)vθ(z'). Fix ε∈(0,1], there exists T0 such that [formula], and fix T1  ≥  T0  /  ε.

For any play s = (z1,...,zt,...) in S(z), we have by definition of β that for all T, [formula]. Let m be a non negative integer, we define:

[formula]

[formula]

We obtain:

[formula]

and

[formula]

We now consider γθk(s) for k large. We compute [formula] by dividing the stages into blocks of length T1. For each m  ≥  0, let [formula] be the Cesàro-average of θkt, where t ranges from mT1 + 1 to (m + 1)T1. Notice that for all such t, we have [formula] ≤   [formula]. We have:

[formula]

where the last inequality follows from equation ([\ref=eq1]). Summing up over m, we obtain:

[formula]

Consequently, [formula], and this is true for all ε. [formula]

[formula]

Proof: Consider an initial state z, and write α  =   inf k sup mvm,θk(z). It is clear that α  ≥   inf θ∈Θ sup m  ≥  0vm,θ(z). Now for each k  ≥  1 there exists m(k), such that vm(k),θk(z)  ≥  α - 1 / k, and we define the evaluation [formula]. We have [formula], so by proposition [\ref=pro1] we obtain that for all evaluations θ, sup z'∈G∞(z)vθ(z')  ≥   lim sup kvθm(k),k(z) ≥  α. [formula]

From lemma [\ref=lem2] and proposition [\ref=pro1], one can easily deduce the following corollary.

For all m0  ≥  0 and z in Z,

[formula]

And we can now conclude the proof the theorem [\ref=thm1], proceeding as in the proof of theorem 3.10 in Renault, 2011.

End of the proof of theorem [\ref=thm1]. Define d(z,z') =  sup k  ≥  1|vθk(z) - vθk(z')| for all states z and z'. The space (Z,d) is now a pseudometric space (may not be Hausdorff). By assumption, there exists a finite set of indices I such that for all k  ≥  1, there exists i in I satisfying d∞(vkθ,vi)  ≤  ε. Consider now the set {(vi(z))i∈I,z∈Z}, it is a subset of the compact metric space I with the uniform distance, so it is itself precompact and we obtain the existence of a finite subset C of states in Z such that:

[formula]

We have obtained that for each ε > 0, there exists a finite subset C of Z such that for every z in Z, there is c∈C with d(z,c)  ≤  ε. The pseudometric space (Z,d) is itself precompact. Equivalently, any sequence in Z admits a Cauchy subsequence for d. Notice that all value functions vθk are clearly 1-Lipschitz for d.

Fix z in Z, and consider now the sequence of sets (Gm(z))m  ≥  0. For all m, Gm(z)  ⊂  Gm + 1(z) so using the precompacity of (Z,d) it is not difficult to show (see, e.g. step 2 in the proof of theorem 3.7 in Renault, 2011) that (Gm(z))m  ≥  0 converges to G∞(z), in the sense that:

[formula]

We now use corollary [\ref=cor4] to conclude. We have for all m :

[formula]

Fix finally ε > 0, and consider k  ≥  1 and m  ≥  0 given by equation ([\ref=eq2]). Let z' in G∞(z) be such that vθk(z')  ≥   sup z'∈G∞(z)vθk(z') - ε. Let z'' in Gm(z) be such that d(z',z'')  ≤  ε. Since vθk is 1-Lipschitz for d, we obtain vθk(z'')  ≥   sup z'∈G∞(z)vθk(z') - 2ε. Consequently,  sup z'∈Gm(z)vθk(z')  ≥   sup z'∈G∞(z)vθk(z') - 2ε for all k, so

[formula]

We obtain  lim inf k  ≥  1vθk(z)  ≥   lim sup k  ≥  1vθk(z) - 2ε, and so (vθk(z))k converges. Since (Z,d) is precompact and all vθk are 1-Lipschitz, the convergence is uniform. [formula]

An open question

We know since Lehrer and Sorin (1992) that the uniform convergence of the Cesàro values [formula] is equivalent to the uniform convergence of the discounted values (vλ)λ∈(0,1]. Example [\ref=exa2] shows that is possible to have no uniform convergence of the Cesàro values (or equivalently of the discounted values) but uniform convergence for a particular sequence of evaluations with vanishing TV. Could it be the case that the Cesàro values and the discounted values have the following "universal" property ?

Assuming uniform convergence of the Cesàro values, do we have general uniform convergence of the value functions, i.e. is it true that (vθk)k uniformly converges for every sequence of evaluations (θk)k  ≥  1 such that [formula] ?

The above property is true in case of an uncontrolled problem (zero-player), i.e. when the transition F is single-valued.

Proof: Fix ε > 0. By assumption there exists N such that for all states z in Z, |N(z) - v*(z)|  ≤  ε. Consider an arbitrary evaluation θ and an initial state z0. For each positive t we denote by zt the state reached from z0 in t stages, we have [formula] and v*(z0) = v*(zt) for all t.

Divide the set of stages into consecutive blocks of length N: B0  =  {1,...,N},..., Bm  =  {mN + 1,...,(m + 1)N},... Denote by (m) the mean of θ over Bm, we have [formula]. We also write (m) for the mean [formula]. We have (m) = N(zmN), so |(m) - v*(z0)|  ≤  ε for all m.

Computing payoffs by blocks, we have

[formula]

So we obtain:

[formula]

and

[formula]

If [formula], we get |vθ(z0) - v*(z0)|  ≤  2ε, hence the result. [formula]

References

Blackwell, D. (1962): Discrete dynamic programming. The Annals of Mathematical Statistics, 33, 719-726.

Lehrer, E. and D. Monderer (1994): Discounting versus Averaging in Dynamic Programming. Games and Economic Behavior, 6, 97-113.

Lehrer, E. and S. Sorin (1992): A uniform Tauberian Theorem in Dynamic Programming. Mathematics of Operations Research, 17, 303-307.

Lippman, S. (1969): Criterion Equivalence in Discrete Dynamic Programming. Operations Research 17, 920-923.

Maitra, A.P. and Sudderth, W.D. (1996): Discrete gambling and stochastic games, Springer Verlag

Mertens, J-F. and A. Neyman (1981): Stochastic games. International Journal of Game Theory, 1, 39-64.

Monderer, D. and S. Sorin (1993): Asymptotic properties in Dynamic Programming. International Journal of Game Theory, 22, 1-11.

Renault, J. (2011): Uniform value in Dynamic Programming. Journal of the European Mathematical Society, vol. 13, p.309-330.

Renault, J. and X. Venel (2012): A distance for probability spaces, and long-term values in Markov Decision Processes and Repeated Games. preprint hal-00674998