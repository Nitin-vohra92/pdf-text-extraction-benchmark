The Fundamental Theorem on Symmetric Polynomials: History's First Whiff of Galois Theory

Evariste Galois' (1811-1832) short life is one of the classic romantic tragedies of mathematical history. The teenage Galois developed a revolutionary theory of equations, answering more fully than ever before a centuries-old question: why can't we find a formula for solving quintic polynomials analogous to the quadratic, cubic and quartic formulas? Then he died in a duel, apparently over the honor of a woman [\cite=stillwell] before his twenty-first birthday. His discoveries lay in obscurity, ignored by the French academy, for 15 years, until Joseph Liouville encountered them, recognized their importance, and made them known. From that point they quickly transcended the problem they were designed to solve, and reshaped the landscape of modern mathematics, initiating the study of both groups and fields. These two concepts are now utterly foundational in algebra as well as in many other fields of math: number theory, topology, complex function theory, and geometry, to name a few.

This story is told and retold in popularizations of mathematics. Less frequently discussed is the actual content of Galois' discoveries. These are usually reserved for a second-semester course in advanced undergraduate or graduate algebra. This article is intended to give the reader a little whiff of the flavor of Galois' work through a theorem that plays a unique role in it. This theorem appears to have been understood, or at least intuited and used, by Newton, as early as 1665. By the turn of the nineteenth century it was regarded as well known. For Galois himself, it was the essential lemma on which his entire theory rested. However, it was not properly proven or even precisely stated until the nineteenth century. This theorem is now known as the Fundamental Theorem on Symmetric Polynomials (FTSP).

In this article we wish to give a brief look into the beginnings of the theory of Galois by examining this theorem and its proof. We approach the theorem from a pedagogical point of view, and in doing so we hope to showcase the pleasure of mathematical discovery, as well as provide a classroom module for other instructors and students. Our narrative arose out of an informal inquiry-based course in group theory and the historical foundations of Galois theory. During one session, we posed the problem of trying to prove the theorem "from scratch" before learning the standard proof. Our work revealed a way of looking at the problem that was new to us, and we realized that it could be used to construct a different proof that gave new insight into the standard one.

Galois theory has been substantially reformulated since Galois' time to make its arguments more elegant and more general. The modern body of results known as Galois theory is thus very different from what Galois himself developed, and it is this modern formulation that is usually treated in university classes. For example, Galois' own reliance on the FTSP has been replaced with the elementary theory of vector spaces over a field, a theory unavailable in the 1820's. For our purposes, what makes the theorem important is not only the historical fact of its centrality in Galois' original formulation of his theory, but also something else. The theory's essential insight is that there is a connection between symmetry and expressibility. (We will make our meaning clear in the next section.) The FTSP is the earliest known theorem to hint at this connection. By explaining and proving it, we hope to give the reader a taste of what Galois theory is all about.

The back story

The FTSP states that any polynomial in n variables [formula] that is invariant under all permutations of the variables (i.e., symmetric) is representable in a unique way as a polynomial in the n elementary symmetric polynomials,

[formula]

Formally:

Any symmetric polynomial in n variables [formula] is representable in a unique way as a polynomial in the elementary symmetric polynomials [formula].

For example, since the polynomial d = (x1 - x2)2 is unchanged by transposing the two variables, the theorem guarantees an expression for d in terms of σ1 = x1 + x2 and σ2 = x1x2. In this case the expression is easy to find: d = (x1 + x2)2 - 4x1x2  =  σ21 - 4σ2.

The importance of the theorem to the theory of equations stems from the fact known as Vieta's theorem, that the coefficients of a single-variable polynomial are precisely the elementary symmetric polynomials in its roots:

Let p(z) be an [formula] degree monic polynomial with roots [formula]. Let [formula] be the n elementary symmetric polynomials in the αi. Then

[formula]

The proof is a straightforward computation, but its significance belies its ease. With this fact in hand, the FTSP becomes the fact that given any polynomial equation p(z) = 0, any symmetric polynomial in its roots is actually a polynomial in its coefficients, and consequently its value can be written down without (in fact, on the way to) solving the equation. Continuing the example from above, if x1 and x2 are the roots of a monic quadratic polynomial, then that polynomial is p(z) = z2  -  σ1z + σ2 and d is its discriminant. The theorem guaranteed that the discriminant (defined as the square of the difference between the roots) would have an expression in terms of the coefficients. This of course is key to the quadratic's solution: [formula] is the difference between the roots and σ1 is the sum of the roots; and the roots themselves can be deduced from these two values. Since d can be expressed in terms of the coefficients, it follows that the roots can be too.

This is the form in which the theorem played its seminal historical role. It was treated as well-known by the time of Galois, even though published proofs (or even precise statements of the theorem in its general form) did not appear until the mid-nineteenth century [\cite=edwards]. For a discussion of some of its historical applications, see Greg St. George's delightful essay "Symmetric Polynomials in the Work of Newton and Lagrange" in Mathematics Magazine [\cite=greg]. It continues to form the basis of modern research, for instance into questions of complexity regarding a symmetric polynomial's representation in terms of elementary symmetric polynomials [\cite=gst].

The way of looking at this theorem that we wish to highlight is, as we mentioned above, the way it intimates the central insight of Galois theory, the connection between symmetry and rational expressibility. We have a polynomial p(z), whose coefficients we know. Even if we don't know the roots, the FTSP tells us that symmetric expressions in the roots are rationally expressible in the coefficients. As a corollary, if the coefficients of p(z) are rational numbers, then every symmetric expression in the roots (for instance the sum of their squares) is rational as well. Symmetry guarantees rational expressibility. In the last section we will indicate how this fits into the bigger picture of Galois theory.

In our course on Galois theory, we did not approach the FTSP directly, but rather sidled up to it by considering some problems of historical significance that implicitly depend on it. The first was a problem of Newton: given two polynomials f,g, how can one determine whether they have a root in common without finding the roots? (This problem is discussed at length in Greg St. George's essay.) The second was posed by Gauss in his Disquisitiones Arithmeticae: given a polynomial f, without finding its roots determine a polynomial g whose roots are the squares, or cubes, etc., of the roots of f.

Participants solved both of these problems for polynomials of low degree. The solutions were accomplished by writing desired expressions in the roots, which turned out to be symmetric, and then expressing these in terms of the coefficients instead. For example, they considered Gauss' problem for a quadratic: if f(z) = z2  -  σ1z + σ2, write down g whose roots are the squares of f's. In this case, if α1,α2 are the roots of f, then α21,α22 are the roots of g, so that

[formula]

To write down this polynomial without actually solving f, it would be necessary to have expressions for the coefficients α21  +  α22,α21α22 in terms of f's coefficients σ1,σ2. You may enjoy looking for them yourself before reading the next line.

[formula]

These activities called attention to the fact that such expressions exist in every case we considered. Thus the participants began to suspect that something like the FTSP would be true. It was clear that any expression in the roots of a polynomial would have to be symmetric to be expressible in terms of the coefficients (since the coefficients are already symmetric). But it was much less clear that any symmetric expression in the roots would be expressible in the coefficients.

The two and three variable case

In this section we start to tackle the question of why any symmetric expression in the roots is expressible in the coefficients from the naïve point of view of the participants. It is natural to begin with the special cases in which the polynomial has just two and then three variables. The participants were able to cobble together proofs in these two cases over the course of two meetings.

To start, let p(x,y) be a polynomial which is symmetric in x and y. We want to show that it can be expressed as a polynomial in σ1 = x + y and σ2 = xy. Taking an arbitrary monomial xmyn which appears in p(x,y), we will try to eliminate it by expressing it in terms of σ1 and σ2. Renaming the variables if necessary, we can suppose that m  ≥  n. If n > 0, then we can already write xmyn as σn2xm - n, so it suffices to deal with monomials of the form xn. For this, note that the symmetry of p(x,y) implies its conjugate monomial yn is also a term of p(x,y), so we can deal with xn + yn together. Now, we immediately recognize xn + yn as the first and last terms of σn1 = (x + y)n. Hence, we have that

[formula]

where q(x,y) is a polynomial of degree n - 2. In other words, we have shown that an induction on the degree of p(x,y) will succeed.

In the case of three variables, let p(x,y,z) be a polynomial which is symmetric in x,y,z. We wish to express p(x,y,z) as a function of σ1 = x + y + z, σ2 = xy + xz + yz, and σ3 = xyz. Again consider an arbitrary monomial xmynzp in p(x,y,z), where for convenience we assume that m  ≥  n  ≥  p. If p > 0 then we can write xmynzp as σp3xm - pyn - p, leaving a monomial with just two variables to deal with. In other words, we only need to treat monomials of the form xmyn. Now, all of the conjugate monomials xnzm, xmzn, xnzm, ymzn and ynzm are also found in p(x,y,z). In analogy to the two variable case, we now recognize that these are all terms of

[formula]

Thus, we can write

[formula]

But unlike the two variable case, not all of the leftover terms (which we denoted q(x,y,z)) have a common factor. However, it is easy to see that any of the terms from q(x,y,z) which does involve just two variables must be a conjugate of xkyl, where m > k  ≥  l > n and k + l = m + n. So while we have not reduced the degree in every case, in cases where we have not we have improved the situation in one key way: we have reduced the spread between the exponents. In other words, this time we will succeed using an induction which takes into account both the degree and the spread between the exponents in the case of monomials with just two variables.

It is natural to try to generalize this method to four and more variables, but there are some difficulties. For starters, it is not clear what the "spread between the exponents" would mean when there are more than two variables in play! While it would have been nice to let the discussion unfold and try to turn this into a general proof, the instructor (Ben) decided in the interest of time to wrap up the FTSP by presenting one of the standard arguments.

A classical proof

In this section we present the proof of the FTSP found in the beautiful presentation of Sturmfels [\cite=sturmfels], who attributes the proof to Van der Waerden. In the next section, we will return to the participants' proof idea.

Let f be the symmetric polynomial to be represented. The set of f's terms of a given degree is itself a symmetric polynomial and if we can represent each of these as a polynomial in the σi, we can represent f; thus nothing is lost by assuming that f is homogeneous.

Now, order the terms of f lexicographically. That is, put the term with the highest power of x1 first, and if there is a tie, decide in favor of the term with the most x2, and so on. Formally, define [formula] if i1 > j1, or if i1 = j1 and i2 > j2, or if i1 = j1, i2 = j2 and i3 > j3, etc., and then order the terms of f so that the first term is >   the second which is >   the third, and so on.

Because f is symmetric, for every term [formula] in it, it also contains all possible terms that look like this one except with the exponents permuted (its "conjugates"). It follows that the leading term of f, say [formula], has [formula]. We let

[formula]

Then g1 is symmetric, and it is easy to see that it has the same leading term as f. Thus f - g1 is symmetric with a "lower" leading term, which we denote [formula]. As before, it follows from the symmetry that [formula]. Thus we can let [formula], so that g2 has the same leading term as f - g1, and f - g1 - g2 has a leading term that is lower still.

Continue in like manner. The algorithm must eventually terminate with no terms remaining, because there are only finitely many possible monomials [formula] of a given degree in the first place. Thus we must come to a point where we have [formula]. Then [formula] is the desired representation of f as a polynomial in the σi.

To prove its uniqueness, it is sufficient to show that the zero polynomial in [formula] is representable uniquely as the zero polynomial in [formula]. This is so because no two distinct products of elementary polynomials [formula] have the same leading term. (The leading term of [formula] is [formula], and the map [formula] is injective.) Thus the leading terms in a sum of distinct products of elementary symmetric polynomials cannot cancel; so such a sum cannot equal zero unless it is empty.

While this lexicographic-order argument is both elegant and simple, there is something conceptually opaque about it (something that troubled the first author on the train ride home from class.) It conjures in one's mind an image of the terms of f neatly ordered lexicographically and then picked off one-by-one, left to right, by our careful choice of [formula]. However, since f and [formula] are all symmetric, the terms are not really being picked off one at a time. When we form f - g1 not only is the leading term [formula] getting canceled, but so are all of its conjugates (for instance, the "trailing term" [formula]). Somehow, the lexicographic ordering is obscuring the symmetry between all the conjugates by picking out one as the leading term, even while it exploits this symmetry to make the proof work. It is this unsatisfying situation that led us to return to our idea of "spread between the exponents" from the class.

Spreadiness proof

We now return to the ideas of our proof in the two and three variable case and develop it into a complete argument. Recall that to generalize our ideas, we first need to overcome the difficulty of deciding what the "spread between the exponents" means when there are a larger number of variables. Indeed, finding this definition is the linchpin of our strategy. Once it is done, it allows us to prove the theorem by building an algorithm that picks off the monomials with the most spread-out exponents first. The algorithm is identical in spirit and similar in practice to the standard one, but uses spread-out-ness rather than lexicographic order to determine which monomials to cancel out first. So while we avoid the arbitrary choices imposed by the lexicographic ordering, our proof still highlights the true key feature of the standard algorithm.

Our first idea for defining "spread" was to use the highest exponent minus lowest. Unfortunately, a simple computation shows this will not work in general. To get a deeper insight, notice that the leading terms in the lexicographic ordering are also the ones where the exponents are in some sense the most spread out. Our definition should therefore take this into account.

Given a monomial [formula] define its spreadiness to be the sum [formula].

This is equivalent to (in the sense that for terms of a given degree it is an increasing function of) both the variance of the set of exponents in a given monomial [formula], and the height of the center of gravity of the monomial when it is pictured as a pile of bricks, with a stack of ik bricks corresponding to each xk. (We will show this below.) Moreover, it has the advantage of being a nonnegative integer, allowing us to use it as the basis of an induction.

The key fact to establish is that just as [formula], with [formula], is the leading term of [formula] when the terms are ordered lexicographically, it and all its conjugates also have strictly greater spreadiness than the rest of the terms of this latter product.

Given [formula] with [formula], the terms of [formula] with maximum spreadiness are precisely [formula] and its conjugates.

In this argument we identify a monomial [formula] with a sequence of stacks of heights [formula] of identical bricks. We first compute that for terms taken from [formula], the spreadiness is an increasing linear function of the vertical coordinate (y) of the center of gravity of its corresponding brick configuration. Supposing that each brick has unit mass, then the vertical coordinate of the center of gravity is given by the sum over the bricks of each brick's height, divided by the number of bricks. If we suppose the first brick of each stack lies at a height of 1 and each brick has unit height, then the stack of height j1 contributes [formula] to the sum. The full vertical coordinate y of the center of gravity is then given by

[formula]

where d is the number of bricks (i.e., the degree), and s is the spreadiness. So s = 2dy - d and since d is fixed, s is an increasing linear function of y as claimed.

Next, we observe that all of the terms of [formula] can be obtained from [formula] by moving some bricks horizontally (and dropping them onto the top of the stack below if necessary). The conjugates of [formula] are precisely those terms in which each layer of bricks rests completely on top of the layer below it before any dropping takes place. Thus bricks will fall for precisely those terms that are not conjugates of [formula]. See Figure [\ref=fig:bricks].

Finally, we appeal to the simple fact that given any physical configuration of bricks, moving some bricks to lower positions decreases the center of gravity.

Once this is established, the proof of the fundamental theorem follows the outline of the standard argument given above.

Let f be the symmetric polynomial to be represented. As above, we lose nothing by assuming f is homogeneous.

The algorithm proceeds as in the standard proof except with spreadiness playing the role of lexicographic order. Pick any term of f with maximum spreadiness s1 and consider it and its conjugates. Form the product of elementary symmetric polynomials g1 that has these terms as its terms of maximum spreadiness. (If the terms of f have coefficient c1 and exponents [formula], then [formula] as always.) Then since these terms are the only terms of g1 with spreadiness as high as s1 by the Spreadiness Lemma, f - g1 contains fewer terms of spreadiness s1 than f does, possibly zero.

Continuing in like manner beginning with f - g1, forming g2 and then f - g1 - g2, etc., we get an algorithm that must terminate because at each stage, either the maximum spreadiness or the number of terms with this spreadiness has been decreased.

The uniqueness of the representation follows exactly as it did in the standard proof. Distinct products of elementary symmetric polynomials will have distinct terms of maximum spreadiness because of the injectivity of the map [formula]. Therefore complete cancellation is impossible: any nonzero polynomial in the elementary symmetric polynomials will be nonzero when multiplied out.

As an aside, we mentioned above that spreadiness is also equivalent to variance. To see this, we compute that for terms taken from [formula], the spreadiness s is an increasing linear function of the variance σ2 of the set [formula]. Indeed,

[formula]

Here n is fixed and so is the mean μ, being a function of just n and the degree d. Thus, s = nσ2 + nμ2 is an increasing linear function of σ2.

The FTSP in Galois' work

We promised earlier to contextualize the FTSP in the bigger picture of Galois theory by showing how it is an example of a larger phenomenon. FTSP says that expressions that are completely symmetric are completely rationally expressible. In his seminal essay Mémoire sur les conditions de résolubilité des équations par radicaux, Galois proved a series of results that tie partial types of symmetry to partially rational types of expressibility as well. First, we justify our up-till-now flip use of phrases like "rationally expressible" (since the FTSP is only a statement about polynomials; no division allowed) extending the FTSP to rational functions:

Any rational function in [formula] that is symmetric in [formula] is a rational function of the elementary symmetric polynomials [formula].

Let f be such a function. It is a quotient of polynomials f = P / Q. Let [formula] be the result of permuting the variables in Q in every possible way. Then

[formula]

The denominator of this expression is invariant under all permutations of the xi's by construction, and f is as well by assumption. It follows that the numerator is also invariant (symmetric). Thus f is here expressed as a quotient between symmetric polynomials, which are polynomials in the σi by the FTSP.

What Galois did was to prove a series of similar statements about rational functions of the xi that are partially rather than fully symmetric. We state them without proof:

If f is a rational function of [formula] that is symmetric under all permutations of the xi's that fix x1, then it is expressible as a rational function of [formula] and x1.

If V is a rational function of [formula] that is not left fixed by any nontrivial permutation of the xi's, then every rational function of the xi's is expressible as a rational function of the σi's and V.

We can summarize the connections between symmetry and rational expressibility in a table as follows.

To culminate, letting the xi's represent elements of a given field (Galois himself was generally thinking of subfields of [formula]) rather than abstract symbols, and therefore letting the σi's represent the coefficients of the polynomial that has the xi's as roots, Galois asserted his famous

Let f be a polynomial with coefficients [formula]. Let [formula] be its roots. Let [formula] be some numbers that are rational functions of the xi's that we suppose are known to us. Then, there exists a group G of permutations of the xi's such that the rational functions of the xi's that are fixed under all the permutations in G are exactly those that are rationally expressible in terms of [formula] and [formula].

If you have studied Galois theory, this formulation may feel unfamiliar to you. To see that it is really the same thing you have seen before, consider that the set of quantities that are rational functions of [formula] forms a field (f's coefficient field); similarly for the set of rational functions of [formula] (f's splitting field). The set of rational functions of [formula] is some extension of the coefficient field contained in the splitting field. So a modern way to state Galois' Proposition I is that given a polynomial f and a given extension K of its coefficient field contained in its splitting field, there is a permutation group G acting on the roots of f such that the fixed field of G is K. This powerful and famous result ties in a very precise way that which is rationally expressible (the elements of a field) to a given type of symmetry (the group).

We hope to have shown you that the FTSP contains the first whisper of this connection. If you are interested to learn more, Harold Edwards' recent article in the Notices of the AMS [\cite=edwards12] explicates some of Galois' own proofs of the above propositions modern language. This article is perhaps best appreciated by reading it alongside Galois' original essay, which is printed in English translation in several sources (see for instance [\cite=edwards], [\cite=hawking], or [\cite=neumann]).