Claim Fact

A simple randomized algorithm for sequential prediction of ergodic time series (Appeared in: IEEE Trans. Inform. Theory 45 (1999), no. 7, 2642-2650.)

Introduction

We address the problem of sequential prediction of a binary sequence. A sequence of bits [formula] is hidden from the predictor. At each time instant [formula], the predictor is asked to guess the value of the next outcome yi with knowledge of the past [formula] (where y01 denotes the empty string). Thus, the predictor's decision, at time i, is based on the value of yi - 11. We also assume that the predictor has access to a sequence of independent, identically distributed (i.i.d.) random variables [formula], uniformly distributed on

[formula]

A simple universal algorithm

In this section we present a simple prediction strategy, and prove its universality. It is motivated by some recent developments from the theory of the prediction of individual sequences (see, e.g., Vovk [\cite=Vov90], Feder, Merhav, and Gutman [\cite=FeMeGu92], Littlestone and Warmuth [\cite=LiWa94], Cesa-Bianchi et al. [\cite=CeFrHaHeScWa97]). These methods predict according to a combination of several predictors, the so-called experts.

The main idea in this paper is that if the sequence to predict is drawn from a stationary and ergodic process, combining the predictions of a small and simple set of appropriately chosen predictors (the so-called experts) suffices to achieve universality.

First we define an infinite sequence of experts [formula] as follows: Fix a positive integer k, and for each n  ≥  1, s∈{0,1}k and y∈{0,1} define the function P̂kn:{0,1}  ×  {0,1}n - 1  ×  {0,1}k  →  [0,1] by

[formula]

where 0 / 0 is defined to be 1 / 2. Also, for n  ≤  k + 1 we define P̂kn(y,yn - 11,s) = 1 / 2. In other words, P̂kn(y,yn - 11,s) is the proportion of the appearances of the bit y following the string s among all appearances of s in the sequence yn - 11.

The expert h(k) is a sequence of functions h(k)n:{0,1}n - 1  →  {0,1}, [formula] defined by

[formula]

That is, expert h(k) is a (nonrandomized) prediction strategy, which looks for all appearances of the last seen string yn - 1n - k of length k in the past and predicts according to the larger of the relative frequencies of 0's and 1's following the string. We may call h(k) a k-th order empirical Markov strategy.

The proposed prediction algorithm proceeds as follows: Let [formula] be a nonnegative integer. For 2m  ≤  n  <  2m + 1, the prediction is based upon a weighted majority of predictions of the experts [formula] as follows:

[formula]

where wn(k) is the weight of expert h(k) defined by the past performance of h(k) as

[formula]

where [formula]. Recall that

[formula]

is the average number of mistakes made by expert h(k) between times 2m and n - 1. The weight of each expert is therefore exponentially decreasing with the number of its mistakes on this part of the data.

Remarks. 1. The above-mentioned estimator of Morvai, Yakowitz, and Algoet [\cite=MoYaAl97] selects a value of k in a certain data-dependent manner, and uses the corresponding estimate P̂kn. The new estimate, however, takes a mixture (weighted average) of all possible values of k, with exponential weights depending on the past performance of each component estimator. As Lemma [\ref=expert] below suggests, this technique guarantees a number of errors almost as small as that of the best expert (i.e., best value of k).

2. Ryabko [\cite=Rya88] proposed an estimator somewhat similar in spirit to the predictor defined here. Ryabko used a mixture of empirical Markov predictors, and proved its universality for all stationary and ergodic processes in a sense related to the Kullback-Leibler divergence. The idea of diversifying Markov strategies also appears in Algoet [\cite=Alg92].

3. Each time n equals a power of two, all weights are reset to 1, and a simple majority vote is taken among the experts. This is necessary to make the algorithm sequential and to be able to incorporate more and more experts in the decision. If the total length of the sequence to be predicted was finite (say n) and known in advance, then no such resetting would be necessary, one could just use the first n experts as Lemma [\ref=expert] below describes. However, to achieve universality, an infinite class of experts is necessary. As the first part of the proof of Theorem [\ref=cons] below shows, we do not loose much by such a resetting of the weights.

4. Related prediction schemes have been proposed by Feder, Merhav, and Gutman [\cite=FeMeGu92] for individual sequences. Their computationally quite simple methods are shown to predict asymptotically as well as any finite-state predictor.

The main result of this section is the universality of this simple prediction scheme:

The prediction scheme g defined above is universal.

In the proof we use a beautiful result of Cesa-Bianchi et al. [\cite=CeFrHaHeScWa97]. It states that, given a set of K experts, and a sequence of fixed length n, there exists a randomized predictor whose number of mistakes is not more than that of the best predictor plus [formula] for all possible sequences yn1. The simpler algorithm and statement cited below is due to Cesa-Bianchi [\cite=Ces97]:

Let [formula] be a finite collection of prediction strategies (experts), and let η > 0. Then if the prediction strategy [formula] is defined by

[formula]

[formula], where for all [formula]

[formula]

then for every n  ≥  1 and yn1∈{0,1}n,

[formula]

In particular, if N is a positive integer, and [formula], then

[formula]

Proof of Theorem [\ref=cons]. Taking K = 2m + 1 and N = 2m in Lemma [\ref=expert], we have that the expected number of errors committed by g on a segment [formula] is bounded, for any y2m + 1 - 12m∈{0,1}2m, as

[formula]

where the last equality follows from the fact that for all i  <  2m + 1, all experts h(k) with k  ≥  2m + 1 predict identically to h(2m + 1). (Note that since the predictors h(k) are deterministic, for every m, L̂2m + 1 - 12m(h(k))  =  L2m + 1 - 12m(h(k)).)

Similarly, denoting [formula], and invoking Lemma [\ref=expert] with [formula] and [formula],

[formula]

Therefore, for any sequence [formula],

[formula]

Denoting μ  =  ⌊ log 2n⌋, we may write

[formula]

where

[formula]

Thus, we obtain

[formula]

Noting that for any fixed sequence yn1, Ln1(g,Un1) is a sum of

[formula]

(h) = 1 n P{Y≠ h(Y)|Y}.

[formula]

lim | L(h)-(h) | = 0   almost surely.

[formula]

limP{Y=1|Y} =P{Y=1|Y}   almost surely,

[formula]

limP(D)=0.

[formula]

Markov processes

In this section we assume that the process to predict {Yn}∞-    ∞ is (in addition to being stationary and ergodic) m-th order Markov, that is, for any binary sequence [formula],

[formula]

where m is a positive integer. We show that the proposed predictor achieves a nearly optimal performance for any m and for any such process, even though the predictor does not use the knowledge that the process is m-th order Markov. The intuitive reason for such a behavior is the following: we have seen it in the previous section that for any sequence,

[formula]

On the other hand, if the sequence is m-th order Markov, then there exists an expert, namely h(m) with very good performance.

In order to simplify our analysis, we modify the experts somewhat. They are defined as before but the probability estimates of ([\ref=freq]) are now replaced by

[formula]

In other words, the simple empirical frequency counts are now replaced by the corresponding Laplace estimates. It is easy to see that all results of Section [\ref=alg] remain valid for the modified predictor.

Remark. The reason for this modification is that this way we can appeal to a result of Rissanen [\cite=Ris86] which simplifies our analysis. We believe that similar performance bounds are true for the original predictor of Section [\ref=alg].

In the next theorem we compare the performance of our predictor to the universal lower bound L*. The statement only gives information about the expected loss, but we believe this result already illustrates the good behavior of the proposed predictor for Markov processes.

If the process to be predicted is a stationary and ergodic m-th order Markov process, then the cumulative loss Ln1(g) = Ln1(g,Un1) of the prediction strategy of Section [\ref=alg] (with the modified estimates of ([\ref=laplace])) satisfies

[formula]

where c > 0 is a universal constant.

Proof.    First note that ([\ref=conc]) implies

[formula]

(see, e.g., [\cite=DeGyLu95]), and therefore it suffices to investigate L̂n1(g). Recall also from the proof of Theorem [\ref=cons] that for any input sequence,

[formula]

and, in particular,

[formula]

Thus, it suffices to show that for m-th order Markov processes the performance of the m-th expert h(m) satisfies

[formula]

for some constant c. To this end, observe that, on the one hand,

[formula]

and on the other hand, by the Markov property,

[formula]

where h(m, * ) is the Bayes decision, given, for any s∈{0,1}m, by

[formula]

(Note that the optimal predictor, that is, the one which minimizes the probability of error at every step predicts according to h(m, * ).)

The above equalities imply that

[formula]

where the second inequality follows by [\cite=DeGyLu95]. In the rest of the proof we simply apply some known results from the theory of universal prediction. First, by applications of Jensen's and Pinsker's inequalities (see Merhav and Feder [\cite=MeFe98]) we obtain

[formula]

Observe that on the right-hand side, under the square root sign, we have the normalized Kullback-Leibler divergence between the probability measure of Yn1 and its estimate constructed as a product of the Laplace estimates ([\ref=laplace]). But this divergence, for m-th order Markov sources, is well-known to be bounded by

[formula]

see Rissanen [\cite=Ris86]. This concludes the proof. [formula]

Remarks. 1. As Theorem [\ref=markov] shows, by exponential weighting of the empirical Markov strategies, the predictor automatically adapts to the unknown Markov order. Similar results, though in different setup, are achieved by Modha and Masry [\cite=MoMa96],[\cite=MoMa98] by complexity regularization.

2. Merhav, Feder, and Gutman, [\cite=MeFeGu93] showed that if the process is m-th order Markov, then the randomized predictor h̃(m) defined by

[formula]

achieves [formula], where C is a constant depending of the distribution of the process. However, in an interesting contrast, the best distribution-free upper bound for all m-th order Markov processes is of the order of n- 1 / 2. To illustrate this, consider the case m = 0, that is, when {Yn} is an i.i.d. process with [formula], and the predictor h̃(0) is based on a majority vote of the bits appeared in the past. In this case, for every n,

[formula]

where c1 is a universal constant. (This is straightforward to see by considering θ  =  cn- 1 / 2 for some small constant c, and writing

[formula]

Finally, invoke the Berry-Esséen theorem (see, e.g., [\cite=ChTe88]) to deduce that there exists a universal constant c2 such that [formula] for every 2  ≤  i  ≤  n.) Thus, even though for every single value of θ, [formula] converges to zero at a rate of O(1 / n), the minimax rate of convergence is, in fact, [formula]. Since the upper bound in Theorem [\ref=markov] is independent of the distribution, we see that, in this sense, (ignoring logarithmic factors) the order of magnitude of the bound is the best possible.

Prediction with side information

In this section we apply the same ideas to the seemingly more difficult classification (or pattern recognition) problem. The setup is the following: let {(Xn,Yn)}∞-    ∞ be a stationary and ergodic sequence of pairs taking values in Rd  ×  {0,1}. The problem is to predict the value of Yn given the data (Xn,Dn - 1), where we denote Dn - 1 = (Xn - 11,Yn - 11). The prediction problem is similar to the one studied in Section [\ref=alg] with the exception that the sequence of Xi's is also available to the predictor. One may think about the Xi's as side information.

We may formalize the prediction problem as follows. A (randomized) prediction strategy is a sequence g = {gi}∞i = 1 of decision functions

[formula]

so that the prediction formed at time i is gi(yi - 11,xi1,Ui). The normalized cumulative loss for any fixed pair of sequences xn1,yn1 is now

[formula]

We also use the short notation Rn1(g) = Rn1(g,Un1). Denote the expected loss of the randomized strategy g by

[formula]

We assume that the randomizing variables [formula] are independent of the process {(Xn,Yn)}.

Just like in the case of prediction without side information, the fundamental limit is given by the Bayes probability of error:

For any prediction strategy g and stationary ergodic process {(Xn,Yn)}∞n =  -   ∞,

[formula]

where

[formula]

The proof of this lower bound is similar to that of Theorem [\ref=bayes], the details are omitted. It follows from results of Morvai, Yakowitz, and Györfi [\cite=MoYaGy96] that there exists a prediction strategy g such that for all ergodic processes, Rn1(g)  →  R* almost surely. (We omit the details here.) The algorithm of Morvai, Yakowitz, and Györfi, however, has a very slow rate of convergence even for i.i.d. processes. The main message of this section is a simple universal procedure with a practical appeal. The idea, again, is to combine the decisions of a small number of simple experts in an appropriate way.

We define an infinite array of experts [formula], [formula] as follows. Let [formula] be a sequence of finite partitions of the feature space Rd, and let [formula] be the corresponding quantizer:

[formula]

With some abuse of notation, for any n and [formula], we write [formula] for the sequence [formula]. Fix positive integers [formula], and for each s∈{0,1}k, [formula], and y∈{0,1} define

[formula]

0 / 0 is defined to be 1 / 2. Also, for n  ≤  k + 1 we define [formula].

The expert [formula] is now defined by

[formula]

That is, expert [formula] quantizes the sequence xn1 according to the partition [formula], and looks for all appearances of the last seen quantized strings [formula] of length k in the past. Then it predicts according to the larger of the relative frequencies of 0's and 1's following the string.

The proposed algorithm combines the predictions of these experts similarly to that of Section [\ref=alg]. This way both the length of the string to be matched and the resolution of the quantizer are adjusted depending on the data. The formal definition is as follows: For any [formula], if 2m  ≤  n  <  2m + 1, the prediction is based upon a weighted majority of predictions of the (2m + 1)2 experts [formula], k,l  ≤  2m + 1 as follows:

[formula]

where [formula] is the weight of expert [formula] defined by the past performance of [formula] as

[formula]

where [formula].

To prove the universality of the method, we need some natural conditions on the sequence of partitions. We assume the following:

(a) the sequence of partitions is nested, that is, any cell of [formula] is a subset of a cell of [formula], [formula];

(b) each partition [formula] is finite;

(c) if [formula] denotes the diameter of a set, then for each sphere S centered at the origin

[formula]

Remark. The next theorem states the universality of the proposed pattern recognition scheme. The definition of the algorithm is somewhat arbitrary, we just chose one of the many possibilities. In this version, at time n, only partitions with indices at most n are taken into account. It is easy to see that the universality property remains valid if the number of partitions considered at time n is an arbitrary, polynomially increasing function of n. The conditions for the sequence of partitions again give a lot of liberty to the user. In applications, the partitions may be chosen to incorporate some prior knowledge about the process. In this paper we merely prove universality of the scheme. Performance bounds in the style of Section [\ref=mark] for special types of proceses may be derived, thanks to the powerful individual sequence bounds. Here, however, the analysis may be substantially more complicated.

Assume that the sequence of partitions [formula] satisfies the three conditions above. Then the pattern recognition scheme g defined above satisfies

[formula]

for any stationary and ergodic process {(Xn,Yn)}∞n =  -   ∞.

Proof. As in the proof of Theorem [\ref=cons], we obtain that for any stationary and ergodic process {(Xn,Yn)}∞n =  -   ∞,

[formula]

Thus, it remains to show that

[formula]

To prove this, we use the following lemma, whose proof is easily obtained by copying that of Lemma [\ref=fststat]:

For each [formula], there exists a positive number [formula] such that for any fixed [formula], [formula] and

[formula]

where

[formula]

Now we return to the proof of Theorem [\ref=patt]. Since the sequence of partitions [formula] is nested, and by (c), the sequences

[formula]

are martingales and they converge almost surely to

[formula]

Thus, it follows from Lebesgue's dominated convergence theorem that

[formula]

Now it follows easily that

[formula]

and the proof of the theorem is finished. [formula]

Appendix

Here we describe two results which are used in the analysis. The first is due to Breiman [\cite=Bre60], and its proof may also be found in Algoet [\cite=Alg94].

Breiman's generalized ergodic theorem [\cite=Bre60]. Let Z = {Zi}∞-    ∞ be a stationary and ergodic time series. Let T denote the left shift operator. Let fi be a sequence of real-valued functions such that for some function f, fi(Z)  →  f(Z) almost surely. Assume that [formula]. Then

[formula]

almost surely.

The second is the Hoeffding-Azuma inequality for sums of bounded martingale differences:

Hoeffding [\cite=Hoe63], Azuma [\cite=Azu67].   Let [formula] be a sequence of random variables, and assume that [formula] is a martingale difference sequence with respect to [formula]. Assume furthermore that there exist random variables [formula] and nonnegative constants [formula] such that for every i > 0 Zi is a function of [formula], and

[formula]

Then for any ε > 0 and n

[formula]

and

[formula]

Acknowledgement. We thank Nicoló Cesa-Bianchi for teaching us all wee needed to know about prediction with expert advise. We are also grateful to Sid Yakowitz for illuminating discussions and to the referees for a very careful reading of the manuscript and for valuable suggestions. We also thank Márta Horváth for useful conversations.