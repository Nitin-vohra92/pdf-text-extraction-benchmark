Weakly-supervised Discriminative Patch Learningvia CNN for Fine-grained Recognition

Introduction

The task of fine-grained object recognition involves distinguishing sub-categories under the same super-category (, birds [\cite=cub2011], dogs [\cite=dog], cars [\cite=car196] and aircrafts [\cite=fgvc_air]), and solutions find and utilize the information from localized regions to capture the subtle differences. The most effective methods employ convolutional neural networks (CNN) and can be roughly separated into two categories. The first category is a traditional multistage framework built upon CNN features, which finds discriminative regions or semantic parts and constructs an image-level representation out of them [\cite=krause15] [\cite=2attention] [\cite=neural_act] [\cite=deepresp] [\cite=tripmine]; the second one typically consists of a recognition network assisted by a localization network trained with additional part annotations to detect semantic parts [\cite=fg_rcnn] [\cite=deeplac] [\cite=spda_cnn] [\cite=partstack] [\cite=maskcnn].

While recent combinations of multistage framework and CNN features achieves good performances - they not only outperform by a large margin their baseline of finetuning the same CNN used for feature extraction [\cite=krause15] [\cite=deepresp] but also find high-quality discriminative regions or parts without part annotations [\cite=tripmine] - the multistage nature of the framework limits their potential. This is because some stages in their methods depend on a CNN model pretrained with generic data ( ImageNet) without further tuning, which might not be optimal. It is also reported that finetuning the network might even hurt performance in these stages [\cite=krause15].

On the other hand, the second category of approaches [\cite=spda_cnn] [\cite=partstack] [\cite=maskcnn] depends on extra semantic part annotations, making them more expensive. Also, the training process usually involves separately tuning the localization and recognition networks followed by joint training, and such a multistage training strategy might make the integrated network tricky to tune. Furthermore, the motivation for using semantic parts for classification is to find the corresponding parts and then compare their appearance. The former requires the semantic parts ( head and body of birds) to be shared across object classes, encouraging the representations of the parts to be similar, but the latter encourages the part representations to be different across categories in order to be discriminative. This subtle conflict implies a trade-off between the recognition ability and localization ability, which might make it difficult for a single integrated network to achieve optimal classification performance.

We address the issues of both categories. Our main contribution is to explicitly learn discriminative patches within a CNN framework without part annotations. Conceptually, our discriminative patches differ from semantic parts in that the former are not necessarily shared across classes as long as they have discriminative appearance. Therefore, a trade-off between recognition and localization when using semantic parts is avoided so that the network can fully focus on classification. Technically, regarding a convolutional filter as a patch detector, a discriminative patch only gives a high response at a certain region in one class. In the rest of the paper, we demonstrate that such discriminative filters can be learned through an asymmetric two-stream network architecture with filter supervision and proper layer initialization. The resulting framework learns to find high-quality discriminative patches as well as obtaining good classification performance; this is achieved in an end-to-end fashion without the cost of semantic part annotations, , it preserves the advantages of both categories.

Related Work

Fine-grained recognition  Trending research in fine-grained recognition is gradually shifting to deep learning from multistage frameworks [\cite=fgtemplate] [\cite=bmvccar] [\cite=ptrans] [\cite=dog2] [\cite=dog1] [\cite=yenliang] [\cite=ningzhang1] [\cite=ningzhang2] [\cite=berg1] [\cite=berg2] [\cite=bangpeng] [\cite=symbiotic] [\cite=fgmultask] [\cite=fgalign] [\cite=ningzhang3] [\cite=fgvcfisher] based on hand-crafted features such as utilizing DPM [\cite=ningzhang3] or Fisher Vector [\cite=fgvcfisher]. As discussed in great details in Section [\ref=sec1], a large portion of current approaches is under the spirit of finding semantic parts or discriminative regions, which can be roughly categorized as "multistage framework with CNN feature" [\cite=krause15] [\cite=2attention] [\cite=neural_act] [\cite=deepresp] [\cite=tripmine] or "localization-recognition integrated network" [\cite=fg_rcnn] [\cite=deeplac] [\cite=spda_cnn] [\cite=partstack] [\cite=maskcnn] whose localization network is usually R-CNN and its variants [\cite=rcnn] [\cite=fast_rcnn] or FCN (Fully Convolutional Network) [\cite=fcn]. Besides these approaches, researchers have explored many other directions such as introducing more effective layers to replace fully-connected layers [\cite=b_cnn] [\cite=c_b_cnn], utilizing label structures [\cite=fzhou1] or joint embedding spaces of visual (images) and textual(class names) information [\cite=fzhou2] [\cite=embed_fg], introducing human in the loop [\cite=human1] [\cite=human2] [\cite=human3], and collecting larger amount of data [\cite=har] [\cite=large_car] [\cite=unreason]. Among them, it is worth mentioning that [\cite=b_cnn] uses a symmetric two-stream network architecture and a bilinear module, , taking the outer product over the outputs of the two streams followed by a series of normalization. [\cite=c_b_cnn] further observed that the symmetric two-stream architecture is not necessary and same performance can be achieved by taking the outer product over a single-stream output and itself. Therefore, improvements are actually obtained by replacing the traditional fully-connected layers with this novel bilinear module. In Section [\ref=sec4], we test our network using both the fully-connected layers and the bilinear module for fair comparison.

Intermediate representations in CNN  Via effective visualization [\cite=fergus14], it is widely-known that the intermediate layers of CNN learn human-interpretable patterns from edges and corners to parts and objects. Regarding the discriminativeness of such patterns, there are two hypothesis. The first is that some neurons in these layers behave as "grandmother cells" which only fire at certain categories, and the second is that the neurons forms a distributed code where the firing pattern of a single neuron is not distinctive and the discriminativeness is distributed among all the neurons. As empirically observed by [\cite=grandmacell], classical CNN learns a combination of "grandmother cells" and a distributed code. This observation is further supported by [\cite=bolei], which found that by taking proper weighted average over all the feature maps produced by a convolutional layer, one can effectively visualize all the regions in the input image used for classification. Note that both [\cite=grandmacell] and [\cite=bolei] are based on the original CNN structure and the quality of representation learning remains the same or slightly worse for the sake of better localization. On the other hand, [\cite=dsn] [\cite=lcnn] [\cite=cite_lcnn] aimed to learn more discriminative representations by putting supervision on intermediate layers, usually by transforming the layer output through a fully-connected layer followed by a loss layer. These works more or less adopt a theoretical perspective which, to some degree, makes their methods difficult to visualize. In contrast, the effectiveness of our approach is very easy to visualize since we regard the convolutional filters as patch detectors from an intuitive perspective. Detailed comparison can be found in Section [\ref=sec3_3]

Learning Discriminative Patch Detectors within CNN

Motivation and Overview

Our discriminative patch learning CNN framework regards a [formula] convolutional filter as a small patch detector. Specifically, referring to Figure [\ref=fig1], if we pass an input image through a series of convolutional and pooling layers to obtain a feature map of size C  ×  H  ×  W, we can regard each C  ×  1  ×  1 vector across channels at fixed spatial location as the representation of a small patch at a corresponding location in the original image. Suppose we have learned a [formula] filter which has high response at a certain discriminative region; by convolving the feature map with this filter we obtain a heatmap. Therefore the discriminative patch can be found simply by picking the location with the maximum value in the entire heatmap. The operation of spatially pooling the entire feature map into a single value is defined as Global Max Pooling (GMP) [\cite=bolei].

Practically, two requirements for the feature map are needed to make this idea suitable for fine-grained recognition. Firstly, since the discriminative regions in fine-grained categories are usually highly localized, we need a relatively small receptive field, , each [formula] vector represents a relatively small patch in the original image. Secondly, since fine-grained recognition involves accurate localization of these regions, the corresponding stride in the original image between adjacent patches should also be small. In earlier network architectures, the size and stride of the convolutional filters and pooling kernels are large. As a result, the receptive field of a single neuron in later convolutional layers is large, so is the stride for adjacent fields. For example, in AlexNet [\cite=alexnet], the minimum receptive field of the 5th convolutional layer conv5 is as large as [formula] with stride 32, which is not fine enough for the task. Fortunately, the evolution of network architectures [\cite=vgg] [\cite=googlenet] [\cite=resnet] is making the filter size and pooling kernel smaller as the networks go deeper. For example, in a 16-layer VGG network (VGG-16), the output of the 10th convolutional layer can represent patch as small as [formula] with stride 8, which is small and dense enough for our task given a standard original image size of [formula].

In the rest of Section [\ref=sec3], we will demonstrate how a set of discriminative patch detectors can be effectively learned as a [formula] convolutional layer in a network specifically designed for this task. An overview of our framework is displayed in Figure [\ref=fig2], which is based on VGG-16. There are three key components in our design: the asymmetric two-stream structure to learn discriminative patches as well as global features (Section [\ref=sec3_2]), the convolutional filter supervision to ensure the discriminativeness of the patch detectors (Section [\ref=sec3_3]) and the non-random layer initialization to accelerate the network convergence (Section [\ref=sec3_4]). Note that, though we use VGG-16 to illustrate our approach, the ideas are not limited by the network architecture.

Asymmetric Two-stream Architecture

The core component of the network responsible for discriminative patch learning is a [formula] convolutional layer followed by a GMP layer, as displayed in Figure [\ref=fig1] and discussed in detail in Section [\ref=sec3_1]. The component followed by a classifier (, several fully-connected layers and a softmax layer) forms the discriminative patch stream (P-Stream) of our network. The P-Stream uses the output of conv4-3 and the minimum receptive field in this feature map corresponds to a patch of size [formula] with stride 8.

In practice, however, the recognition of some fine-grained categories might depend more on global shape and appearance instead of a few discriminative patches. To give the network flexibility to learn global shape and appearance, in another stream we preserve the further convolutional and pooling layers followed by a classifier. The last pooling layer, then, has a minimum receptive field of [formula], which is almost as large as [formula] network intput cropped out of a [formula] image and represent a set of global features. Therefore, this stream focuses more on global features; and we refer to it as the G-Stream. We merge the two streams in the end.

Convolutional Filter Supervision

Using the network architecture described above, the [formula] convolutional layer in P-Stream is not guaranteed to fire at discriminative patches as desired. For the framework to learn class-specific discriminative patch detectors, we impose supervision directly at the [formula] filters by introducing a Cross-Channel Pooling layer followed by a softmax loss layer, the details of which are displayed in Figure [\ref=fig3] and the integration of which into the whole framework is shown in Figure [\ref=fig1].

The filter supervision works as follows. Suppose we have M classes and each class has k discriminative patch detectors; then the number of [formula] filters required is kM. After obtaining the max response of each filter through GMP, we get a kM-dimensional feature vector. By Cross-Channel Pooling, we average the values of this vector from dimension (ki  +  1) to dimension (k  +  1)i as the averaged response of discriminative patch detectors from Class (i + 1), resulting in an M-dimensional vector. By feeding the pooled vector into an M-class softmax loss, we encourage the filters from any class to find discriminative patches from training samples of that class, such that their averaged filter response is large. The reason to use average pooling instead of max pooling is that we want all the filters from a given class to have balanced responses. Average pooling tends to affect all pooled filters during back propogation, while max pooling only affects the filter with the maximum response. Similar considerations are discussed in [\cite=bolei].

Using this form of supervision, since there is no learnable parameter between the softmax loss and the [formula] convolutional layer, by taking the partial derivatives of the loss w.r.t. the filter weights, we can directly adjust the filter weights via the loss function. We believe this is a key difference from previous approaches which introduce intermediate supervision [\cite=dsn] [\cite=lcnn] [\cite=cite_lcnn]. Unlike us, [\cite=dsn] [\cite=lcnn] [\cite=cite_lcnn] have learnable weights (usually a fully-connected layer) between the side loss and the main network, which essentially learn the weights of a classifier unused at test time. The main network is only effected by back-propogating the gradients of these weights. In our approach, the loss directly effects the main network and its effect is much easier to visualize by checking the top patches found by these filters.

Layer Initialization

In Section [\ref=sec3_3], the side loss can directly effect the [formula] convolutions. In practice, we found that if the [formula] convolutional layer is initialized randomly, it converges to bad local minima. For example, the output vector of the Cross-Channel Pooling might approach all-zero to reduce the side loss during training, which is useless. To overcome this problem, we introduce a method for non-random initialization.

The non-random initialization is motivated by our interpretation of a [formula] filter as a patch detector. The patch detector of Class i is initialized by patch representations from the samples in that class. This is done in a weakly-supervised way without part annotations. Concretely, we extract the conv4-3 features from the ImageNet pretrained model and compute the energy at each spatial location (, l2 norm of each [formula] vector in a feature map). As can be seen from the first row of Figure [\ref=fig7], though not perfect, the heatmap of energy distribution acts as a reasonable indicator of useful patches. After choosing non-overlapping high-energy regions from training samples from Class i, we perform k-means clustering over the representations of the chosen patches and pick the cluster centers as the initialization for filters from Class i. To increase their discriminativeness, we further whiten the initializations using the technique from [\cite=ldadet] and do l2 normalization. In practice this simple method provides reasonable initializations which are further refined during end-to-end training. Also, in Section [\ref=sec4] we will see that the energy distribution used for initialization becomes much more discriminative after training.

As long as the layer is properly initialized, the whole network can be trained in an end-to-end fashion just once, which is more efficient compared with the multistage training strategy of previous works [\cite=deeplac] [\cite=spda_cnn] [\cite=partstack].

Experiments

Datasets

Stanford Cars dataset [\cite=car196] has 16,185 images from 196 classes. We follow the standard data split of 8,144 training images and 8,041 test images provided by [\cite=car196], where each class has approximately 40 training images and 40 test images. No part annotations are provided in this dataset.

CUB-200-2011 dataset [\cite=cub2011] has 11,788 images of 200-class fine-grained bird species. We follow the standard data split of 5,994 training images and 5,794 test images provided by [\cite=cub2011], where each class has roughly 30 training images and 30 test images. Part annotations are provided but unused in our experiments. Note that recent work [\cite=unreason] obtains the best result of 92.8% on test set with off-the-shelf 42-layer Inception V3 [\cite=inceptionv3], using (Class Name, Image) pairs filtered from 5 million Google Image Search results such that each class has 800 more training samples on average. Since our goal is to demonstrate that our method works for both rigid and non-rigid fine-grained domains, we follow the original training set in our experiments.

Implementation Details

Network Structure  As displayed in Figure [\ref=fig2], our network is based on VGG-16 [\cite=vgg], which has 16 layers with learnable parameters. Regarding the input, we crop an image to its bounding box, resize it to [formula] and feed a random crop of [formula] into the network. Notice that this is a relatively economic setting, as recent end-to-end approaches have input size of [formula] [\cite=b_cnn] [\cite=c_b_cnn] or [formula] [\cite=spda_cnn] or multi-scales [\cite=partstack]. Discriminative patches are learned via the [formula] convolution (denoted as conv6) layer following conv4-3. The size (C  ×  H  ×  W) of conv4-3 output is [formula], therefore the size of each convolutional filter is [formula]. We set the number of filters per class to be 10, resulting in a total of 1960 filters for cars and 2000 for birds. During Cross-Channel average pooling, the maximum responses of each group of 10 filters are pooled into one dimension. The network is built and trained with Caffe [\cite=caffe].

Layer Initialization  As discussed in Section [\ref=sec3_4], to initialize conv6, we extract conv4-3 features using ImageNet pretrained model; each image provides 7 patch representations at locations with highest energy (non-maximum suppression is used). For each class, we perform k-means clustering over the features of all the training samples in that class and their horizontal mirrors with the number of clustering centers set to 15. Since each class only requires 10 filter initializations, we select the top 10 centers with the largest number of positive samples in their top activations. To fit the scale of the network, we rescale the selected centers such that the norm of each initialized filter is 0.045. Other convolutional layers are initialized from an ImageNet pretrained model directly (compared with "indirect" initialization of conv6) and the fully-connected layers are randomly initialized.

Training/Testing Configurations  After the layers are properly initialized, a single-stage end-to-end training is conducted. The learning rate is initialized at 10- 3 and drops by a factor of 10 every 14,000 iterations. The total number of iterations is 30,000 with a batch size of 16. At test time, for each image we average the 10 classification results from the center and 4 corners with their mirrors.

Results

For convenience, we denote our approach as DPL-CNN, which is an abbreviation for Discriminative Patch Learning within a CNN.

We compare our approach with the corresponding baseline, which involves finetuning a VGG-16 network [\cite=vgg] on the datasets. For fair comparison, we cite previously published baseline results [\cite=fzhou1]. Specifically, the settings in [\cite=fzhou1] named "VGG-SM/with BBox/MV" is exactly the same as ours; this stands for "VGG-16 with SoftMax, Bounding Boxes and Multi-View testing (, 10 crops testing as discussed in Section [\ref=sec4_2])". Our results using the baseline are similar to theirs but slightly inferior.

Besides the baseline, we compare our approach to the methods with the best results on either dataset using the given amount of data. To the best of our knowledge, the best result on Stanford Cars dataset is reported in [\cite=krause15] (denoted as CoSeg), which is a multistage framework built upon 19-layer VGG-19 features, involving segmentation, part selection, part representation generation and SVM. Interestingly, the current best result on CUB-200-2011 using the original training set is obtained using an end-to-end method [\cite=b_cnn], which is the VGG-16 based bilinear network discussed in Section [\ref=sec2] (denoted as B-CNN). In addition to having the best results, these two methods serve as ideal references since (i) both have been evaluated on both datasets (which in fact is not quite common in current literature); (ii) neither of them uses part annotations, but still outperform those using them; (iii) the best results of both are obtained when bounding boxes are presented; (iv) both methods are VGG-Net based and there is no significant difference in performance between VGG-16 and VGG-19.

The results are displayed in Table [\ref=tb1].

Though the results using our reproduction of the VGG-16 baseline is slightly lower than the one reported in [\cite=fzhou1], we believe this is within reasonable range since it is already slightly better than the VGG-19 baseline reported in [\cite=krause15]. As can be seen, our method consistently outperforms the baseline by a significant margin even compared with the version in [\cite=fzhou1]. This is strong evidence that the discriminative patch learning occurred within the network helps the final classification. Another observation is that it is almost equally effective for rigid (cars) and non-rigid (birds) fine-grained objects. One explanation is that our (relatively small) discriminative patches are only determined by local appearance, which is robust to deformation. The appearance of semantic parts, in constrast, can change severely due to deformation or pose variation.

The CoSeg [\cite=krause15] method automatically finds a set of parts and the classification is based on an ensemble of classifiers trained on the representations of each part. Therefore, without part annotations, it still follows the "finding parts and comparing appearance" idea discussed in Section [\ref=sec1]. For rigid objects like cars, alignment is relatively easy; but for non-rigid object like birds, there is too much appearance variation due to deformation and pose variation, which increases the difficulty of classification based on these deformed parts. This, to some degree, explains why CoSeg outperform all end-to-end network on cars so far, but we outperform it on birds.

The B-CNN [\cite=b_cnn] method is highly effective on birds, but its advantages is diminished when dealing with rigid objects, so we outperform it on cars. The motivation of [\cite=b_cnn] is to expect one stream of the network to focus on "where" while the other focuses on "what". During the experiments, the author observed that the role of the two streams are not well separated and their neurons have similar firing patterns.

To summarize, our approach obtains balanced performance on both rigid and non-rigid fine-grained objects that are comparable to state-of-the-art.

Visualization and Analysis

Visualization Overview

Insights into the behavior of our approach can be obtained by visualizing the effects of conv6, the [formula] convolutional layer. To thoroughly understand its behavior, the visualizations are constructed from three perspectives.

Visualize patch activations. Since we regard each filter as a discriminative patch detector, we can identify the learned patches by applying conv6 filters to images and looking at top activations. We will see we do find high-quality discriminative regions.

Visualize a forward pass. Since the max responses of these filters are directly used for classification, by visualizing the output of conv6's next layer, pool6, we will see that it produces discriminative representations which have high responses for certain classes.

Visualize back propagation. During training, conv6 can affect its previous layer, conv4-3, through back propagation. By comparing the conv4-3 features before and after training, we will see that the spatial energy distributions of previous feature maps are changed in a discriminative fashion.

Stanford Cars

The visualization of top activations for some of the [formula] filters is displayed in Figure [\ref=fig5]. Unlike previous filter visualizations which pick human interpretable results randomly among the filter activations, we have imposed supervision on conv6 filters and can identify their top activations with a certain class. From Figure [\ref=fig5], we see that the top activations are very consistent with human perception and cover a diverse of regions. For instance, the 1st filter belonging to Class 1 (AM General Hummer SUV) activates on the squared side windows of an SUV, the 271st filter captures the discriminative head light of Class 27 (BMW 1 Series Coupe), the 1610th filter focuses on the frontal face of Class 160 (Mercedes-Benz 300-Class Convertible) and the 1843rd belonging to Class 184 (Tesla Model S) captures the distinctive tail of this type. The reasons for the network's ability to localize these subtle discriminative regions are that a) [formula] filters correspond to a small patch detector in original image b) the filter supervision and c) the inclusion of more than 2 / 3 of the cluster centers as initialization which promotes diversity.

The visualization of pool6 features are shown in Figure [\ref=fig6]. We plot the averaged representations over all test samples from a certain class. Since we have learned a set of discriminative filters, the representations should have high responses at one class or only a few classes. Figure [\ref=fig6] indicates that our approach works as expected, resulting in peaky layer output which is fed into the classifier consisting of fully-connected layers and a softmax in a forward pass.

Most interesting is the effect of conv6 on the previous convolutional layer conv4-3 through back propagation. As discussed in Section [\ref=sec3_4], we use the energy distribution of conv4-3 as a hint to provide layer initialization. After training, we observed that the energy distribution is refined by conv6 and becomes more discriminative. Figure [\ref=fig7] provides visualizations of this observation. We map every spatial location in the feature map back to the corresponding patch in the original image, and the value of each pixel is determined by the max energy patch covering that pixel. From the first line of Figure [\ref=fig7], the features extracted from an ImageNet pretrained model tend to have high energy at round patterns such as wheels, some unrelated background shape, a person in the image and some texture patterns, which are common patterns in generic models found in [\cite=fergus14]. After training, the energy shifts from these patterns to discriminative regions of cars. For example, in the 2nd column, after training the energy over the brick patterns is reduced. In the 6th column, the feature map has high energy initially at both the wheel and the frontal light; after training, the network has determined that a discriminative patch for that class (Volkswagen Beetle) is the light rather than the wheels. In the 7th column, before training the energy is focused mostly at the air grill, and training adds the discriminative fog light into the high energy region. Therefore, the discriminative patch detectors have beneficial effects on their previous layer during training.

CUB-200-2011

The examples of the discriminative patches found by our approach is displayed in Figure [\ref=fig8], which include the texture and spots with bright color as well as specific shape of beak or webbed. Compared with the visualizations of previous works not using part annotations ( [\cite=krause15] [\cite=b_cnn]), our approach is able to localize such patches more accurately due to the facts that our patch detectors operate over denser and smaller patches and are not necessary to be shared across categories.

Similar to cars, the features from the next GMP layers are peaky at certain categories (Figure [\ref=fig9]). And the energy distributions of previous convolutional features are effected such that the high energy at background regions like branches is reduced and the discriminative regions become more focused or diversed according to different categories (Figure [\ref=fig10]).

Conclusion

In this paper, we aim to combine the advantages of previous multistage and end-to-end frameworks for fine-grained recognition. Specifically we learn a set of discriminative patch detectors within a CNN framework in an end-to-end fashion without part annotation, which is done via an asymmetric two-stream network structure with convolutional layer supervision and non-random layer initialization. Experimental results suggest that our approach can learn high-quality discriminative patches as well as obtaining comparable results to state-of-the-art on both rigid and non-rigid fine-grained datasets. Possible future directions include utilizing multi-scale patches represented in different convolutional layers.

Acknowledgements  This research was supported in part by funds provided from the Office of Naval Research under grant N000141612713 entitled "Visual Common Sense Reasoning for Multi-agent Activity Prediction and Recognition."