A Bayesian optimization approach to find Nash equilibria

INTRODUCTION

Game theory arouse from the need to model economic behavior, where multiple decision makers (MDM) with antagonistic goals is a natural feature. It was further extended to broader areas, where MDM had however to deal with systems governed by ordinary differential equations, the so-called differential games. See e.g., [\citep=Gibbons-Game-92] for a nice introduction to the general theory and [\citep=MR0210469] for differential games. Recently, engineering problems with antagonistic design goals and with real or virtual MDM were formulated by some authors within a game-theoretic framework. See e.g., [\citet=JAD2014-HELICO] for aerodynamics, [\citet=Habbal2004-A1] for structural topology design, [\citet=HabbalSICON2013] for missing data recovery problems. The study of multi-agent systems or games such as poker under this setting is also quite common in the AI and machine learning communities, see e.g., [\citet=Johanson2009] [\citet=Lanctot2012] [\citet=Brown2015].

Solutions to games are called equilibria. Contrarily to classical optimization, the definition of an equilibrium depends on the game setting (or rules). Within the static with complete information setting, a relevant one is the so-called Nash equilibrium (NE). Shortly speaking, a NE is a fixed-point of iterated many single optimizations (see the exact definition in section-[\ref=sec:background] below). Its computation generically carries the well known tricks related to computing a fixed-point, as well as those related to intensive optimizations notably when cost evaluations are expensive, which is mostly the case for engineering applications. There is an extensive literature related to theoretical analysis of algorithms for computing NE [\citep=MR942837] [\citep=MR899829] [\citep=Rubinstein], but very little -if any- on black-box models (i.e., non convex utilities) and expensive-to-evaluate ones; to the best of our knowledge, only home-tailored implementations are used. On the other hand, Bayesian optimization [\citep=mockus1989] is a popular approach to tackle black-box problems. Our aim is to investigate the extension of such approach to the problem of computing game equilibria.

BO relies on Gaussian processes, which are used as emulators (or surrogates) of the black-box model outputs based on a small set of model evaluations. Posterior distributions provided by the Gaussian process are used to design acquisition functions that guide sequential search strategies that balance between exploration and exploitation. Such approaches have been transposed to frameworks other than optimization, such as uncertainty quantification [\citep=bect2012sequential] or optimal stopping problems in finance [\citep=Gramacy2015].

In this paper, we show that the BO apparatus can be applied to the search of game equilibria, and in particular the classical Nash equilibrium (NE). To this end, we propose two complementary acquisition functions, one based on a greedy search approach and one based on the Stepwise Uncertainty Reduction paradigm [\citep=fleuret1999graded]. Our proposal is designed to tackle derivative-free, expensive models, hence requiring very few model evaluations to converge to the solution.

The rest of the paper is organized as follows. Section [\ref=sec:background] reviews the basics of game theory and present our Gaussian process framework. Section [\ref=sec:sequential] presents our main contribution, with the definition of two acquisition functions, along with computational aspects. Finally, Section [\ref=sec:experiments] demonstrates the capability of our algorithm on several challenging problems.

BACKGROUND

Nash games and Nash equilibria

For non-cooperative static games with complete information and no leadership/followers features, a relevant game solution is Nash equilibrium [\citep=Gibbons-Game-92]. Let us consider a finite number, say p  ≥  2, of decision makers (i.e., players). Player (i) has an action space [formula], and a specific cost function yi. We denote [formula], and [formula]. We shall use the convention [formula] when we need to emphasize the role of [formula]. We warn the reader that it is only notation, no effective permutation is done. Then, one has [formula]. That is, clearly each player's cost depends on others choices.

Definition.

A Nash equilibrium [formula] is a strategy such that, for any i, 1  ≤  i  ≤  p,

[formula]

In other words, when all players have chosen to play a NE, then no single player (i) has incentive to move from his [formula]. Let us however mention by now that, generically, Nash equilibria are not efficient, i.e., do not belong to the underlying set of best compromise solutions, called Pareto front, of the objective vector [formula].

Remark.

In this work, we focus essentially on continuous games (i.e., with infinite sets [formula]) or on large finite games (i.e., with large finite sets [formula]), and consider solely pure-strategy Nash equilibria [\citep=Gibbons-Game-92].

From a BO perspective, the costs, issued from a black-box model, are expressed as functions of decision (or design) variables which we denote [formula], with d  ≥  p, rather than actions [formula]. If a classical setting is to consider a single decision variable by player (xi=si), there are many cases where the strategies encompass a set of variables: this is the case of the differential game example described in Section [\ref=ssec:differential]. Hence, [formula] carries the splitting of [formula] between players; e.g., if two players share a decision variable [formula] as follows: player (1) controls x3 and player (2) controls x1,x2 and x4, then [formula] with [formula] and [formula].

As we show in the next section, such a distinction becomes critical when it comes to GP modeling, which may only be done with respect to [formula]. Hence, we write the costs as a single function [formula], with real vectorial inputs and outputs:

[formula]

[formula] is the definition space of the decision variables [formula], typically a hyper-rectangle. Note that in the following, we also use the notation [formula] when it does not bring any confusion.

As we show in Section [\ref=sec:sequential], our approach requires [formula] to be discrete. If [formula] is originally continuous, we assume that a representative discretization is available so that the equilibrium of the corresponding finite game is similar to the one of original problem. To account for the particular form of NE, the discretization must realize a full-factorial design in the [formula] space: given each action space [formula] of size mi, [formula] consists of all the combinations [formula] (1  ≤  i  ≠  j  ≤  p, 1  ≤  k  ≤  mi, 1  ≤  l  ≤  mj), and we have [formula].

If a regular grid over [formula] always satisfies this constraint, many alternatives are possible: going back to the example [formula] and [formula], one may discretize for instance x3 over a regular grid and (x1,x2,x4) using a Latin hypercube design.

Gaussian process regression

The idea of replacing an expensive function by a cheap-to-evaluate surrogate is not new, with initial attempts based on linear regression. Gaussian process regression, or kriging, extends the versatility and efficiency of surrogate-based methods in many applications, such as in optimization or reliability analysis. Among alternative non-parametric models such as radial basis functions or random forests, see e.g., [\citet=Wang2007] [\citet=Shahriari2016] for a discussion, GP priors are attractive in particular for their tractability, since they are simply characterized by their mean m and covariance (or kernel) k functions, see e.g., [\citet=Cressie1993] [\citet=Rasmussen2006]. In the following, we consider zero-mean processes (m  =  0) and no observational noise for the sake of conciseness, but our approach straightforwardly adapts to those cases.

Briefly, for a single objective y, conditionally on n observations at [formula], the predictive distribution is another GP, with mean and covariance functions given by:

[formula]

where [formula] and [formula].

Commonly, k belongs to a parametric family of covariance functions such as the Gaussian and Matérn kernels, based on hypothesis about the smoothness of y. Corresponding hyperparameters are often obtained as maximum likelihood estimates, see e.g., [\citep=Rasmussen2006] or [\citep=Roustant2012] for the corresponding details.

With several objectives, while a joint modeling is possible (see e.g., [\cite=Alvarez2011]), it is more common practice to treat them separately. In the following, we assume that we have a statistical emulator for [formula], conditioned on a set of observations [formula], in the form of a multivariate Gaussian process [formula] ([formula]):

[formula]

with [formula], [formula], such that {μi(.),σ2i(.,.)} is the predictive mean and covariance, respectively, of a kriging model of the objective yi.

This predictive distribution is used either to derive quantities based on its moments or to draw samples of [formula] indexed by a discrete set. For the latter, the simple approach that we follow here is to cast the simulation as a standard Gaussian vector simulation problem, thus relying on a Cholesky decomposition of the covariance matrix [\citep=Diggle2007].

SEQUENTIAL DESIGN

A brief review of Bayesian optimization concepts

Bayesian optimization methods are usually outlined as follows: a first set of observations [formula] is generated using a space-filling design to obtain a first predictive distribution of [formula]. Then, observations are performed sequentially by maximizing a so-called acquisition function (or infill criterion) [formula], that represents the potential usefulness of a given input [formula]. That is, at step n  ≥  n0,

[formula]

In unconstrained optimization, the canonical choice for [formula] is the so-called Expected Improvement [\citep=jones1998efficient], which offers an efficient trade-off between exploration of unsampled regions (high posterior variance) and exploitation of promising ones (low posterior mean).

Improvement-based approaches have many advantages (among which a convenient closed form expression for the EI), however it cannot apply to our case, as it requires to measure an (expected) progress over a current best solution, which does not exist here (there is no such thing as a "current best NE").

Hence, we propose in the following two acquisition functions based on alternative concepts, respectively the probability of achieving equilibrium and stepwise uncertainty reduction. Both aim at providing an efficient trade-off between exploration and exploitation.

Maximizing the probability of equilibrium

Given a predictive distribution of [formula], a first natural metric to consider is the probability of achieving the NE. In case of finite games, this probability writes:

[formula]

where [formula] denotes the alternatives for [formula], and [formula] is fixed to its value in [formula]. Since our GP model assumes the independence of the posterior distributions of the objectives, we have:

[formula]

As exploited recently by [\citet=Chevalier2012] in a multi-point optimization context, each Pi can be expressed as

[formula]

where mi is the number of alternatives for [formula] with fixed [formula] and [formula]. [formula] amounts to compute the cumulative distribution function (CDF) of [formula], the (Gaussian) vector of size q: = mi  -  1 with mean [formula] and covariance [formula], i.e., [formula]. See [\citet=Chevalier2012] for calculation details.

Several fast implementations of the multivariate Gaussian CDF are available, for instance in the R packages mnormt (for q  <  20) [\citep=Azzalini2016] or, up to q  =  1000, by Quasi-Monte-Carlo with mvtnorm [\citep=Genz2009] [\citep=Genz2016].

Alternatively, this quantity can be computed using Monte-Carlo methods by drawing R samples [formula] of [formula] to compute

[formula]

[formula] denoting the indicator function. This latter approach may be preferred when the number of alternatives mi is high (say > 20), which makes the CDF evaluation overly expensive while a coarse estimation may be sufficient. Note that on both cases, a substantial computational speed-up can be achieved by removing from the [formula]'s the non-critical strategies. This point is discussed in Section [\ref=sec:numerical].

Using [formula] as an acquisition function (to maximize) defines an intuitive sequential sampling strategy (i.e., sampling at designs most likely to achieve NE).

Note that its counterpart in global optimization, called probability of improvement, is usually discarded for not being exploratory enough [\citep=jones2001taxonomy]. Here, [formula] is expected to perform better, as both solutions with low mean and those with high variance may realize the minimum.

Still, maximizing [formula] is a myopic approach (i.e., favoring an immediate reward instead of a long-term one), which are often sub-optimal [\citep=Ginsbourger2010] [\citep=Gonzalez2016]. Instead, other authors have advocated for the use of an information gain from a new observation instead, see e.g., [\citet=villemonteix2009informational] [\citet=Hennig2012], which motivated the definition of an alternative acquisition function, which we describe next.

Stepwise uncertainty reduction

Stepwise Uncertainty Reduction (SUR) has recently emerged as an efficient approach to perform sequential sampling, with successful applications in optimization [\citep=villemonteix2009informational] [\citep=picheny2014stepwise] [\citep=hernandez2014predictive] or uncertainty quantification [\citep=bect2012sequential] [\citep=jala2016sequential]. Its principle is to perform a sequence of observations in order to reduce as quickly as possible an uncertainty measure related to the quantity of interest (in the present case: the equilibrium).

Acquisition function definition

Let us first denote by [formula] the application that associates a NE with a multivariate function. If we consider a random process [formula] in lieu of the deterministic objective [formula], the equilibrium [formula] is a random vector of [formula] with unknown distribution. Let Γ be a measure of variability (or residual uncertainty) of [formula]; we use here the determinant of its second moment:

[formula]

The SUR strategy aims at reducing Γ by adding sequentially observations [formula] on which [formula] is conditioned. An "ideal" choice of [formula] would be:

[formula]

where [formula] is the process conditioned on the observation [formula]. Since we do not want to evaluate [formula] for all [formula] candidates, we consider the following criterion instead:

[formula]

with [formula] following the posterior distribution (conditioned on the n current observations) and [formula] denoting the expectation over [formula]. The SUR strategy is then:

[formula]

In practice, computing [formula] is a complex task, as no closed-form expression is available. The next subsection is dedicated to this question.

Approximation using conditional simulations

Let us first focus on the measure Γ when no new observation is involved. Due to the strong non-linearity of Ψ, no analytical simplification is available, so we rely on conditional simulations of [formula] to evaluate Γ.

Let [formula] be independent drawings of [formula] (each [formula]). The following empirical estimator of [formula] is available:

[formula]

with [formula] the sample covariance of [formula].

Now, let us assume that we evaluate the criterion for a given candidate observation point [formula]. Let [formula] be independent drawings of [formula]. For each [formula], we can condition [formula] on the event [formula] in order to generate [formula] drawings of [formula], from which we can compute the empirical estimator [formula]. Then, an estimator of [formula] is obtained using the empirical mean:

[formula]

Numerical aspects

The proposed SUR strategy has a substantial numerical cost, as the criterion requires a double loop for its computation: one over the K values of [formula] and another over the M sample paths. The two computational bottlenecks are the sample path generations and the searches of equilibria, and both are performed in total K  ×  M times for a single estimation of J. Thankfully, several computational shortcuts allow us to limit the cost of our approach.

First, fast formulae are used to obtain drawings of [formula] based on drawings of [formula]. We refer to [\cite=Chevalier2014] for the detailed algebra and complexity analysis. In our case, we generate a unique set of drawings [formula], prior to the search for [formula], which we update quickly when necessary depending on the pair [formula].

Second, we discard points in [formula] that are unlikely to provide information regarding the equilibrium prior to the search of [formula], as we detail below. By doing so, we reduce substantially the sample paths size and the dimension of each finite game, which drastically reduces the cost. We call [formula] the retained subset.

Finally, Ĵ is evaluated only on a small, promising subset of [formula]. We call [formula] this set.

To select the subsets, we rely on a fast-to-evaluate score functions C, which can be seen as a proxy to the more expensive acquisition function. The subset of [formula] is then chosen by sampling randomly with probabilities proportional to the scores [formula], while ensuring that the subset retains a factorial form. We propose three scores, of increasing complexity and cost, which can be interleaved:

[formula]: the simplest score is the posterior density at a target TE in the objective space, for instance the NE of the posterior mean (hence, it requires one NE search). [formula] reflects a proximity to an estimate of the NE. We use this scheme for the first iteration to select [formula].

[formula]: once conditional simulations have been performed, the above scheme can be replaced by the probability for a given strategy to fall into the box defined by the extremal values of the simulated NE (i.e., [formula]). We use this scheme to select [formula] for all the other iterations.

[formula]: since [formula] is faster (in particular in its Monte Carlo setting with small R) than [formula], it can be used to select [formula].

Note that in our experiments, [formula] and [formula] are also used with the [formula] acquisition function.

Last but not least, this framework enjoys savings from parallelization in several ways. In particular, the searches of NE for each sample [formula] can be readily distributed.

An overview of the full SUR approach is given in pseudo-code in Algorithm [\ref=alg:SUR]. Note that by construction, SUR does not necessarily sample at the NE, even when it is well-identified. Hence, as a post-processing step, the returned NE estimator is the design that maximizes the probability of achieving equilibrium.

NUMERICAL EXPERIMENTS

These experiments have been performed in R [\citep=R2016] relying on the DiceKriging package [\citep=Roustant2012], while Scilab [\citep=Scilab2012] has been used for fixed point methods. The code to reproduce the experiments will be made available.

A classical multi-objective problem

We first consider a classical optimization toy problem (P1) as given in [\cite=Parr2012], with two variables and two cost functions, defined as:

[formula]

with x1∈[ -  5,10] and x2∈[0,15]. Both functions are non-convex. We set s1  =  x1 and s2  =  x2 (note that choosing s1  =  x2 and s2  =  x1 does not change the NE here). The actual NE is attained at x1 = 0.08 and x2 = 1.

Our strategies are parameterized as follow. [formula] is first discretized over a 31  ×  31 regular grid. Since the size of [formula] is relatively small, we use [formula], and K  =  M  =  20. We use n0 = 6 initial observations from a Latin hypercube design, and observations are added sequentially using both acquisition functions. As a comparison, we ran a standard fixed-point algorithm [\citep=MR942837] based on finite differences. This experiment is replicated five times with different initial sets of observations for the GP-based approaches and different initial points for the fixed-point algorithm. The results are reported in Table [\ref=tab:P1results].

In addition, Figure [\ref=fig:P1] provide some illustration for a single run. In the initial state (top left), the simulated NEs form a cloud in the region of the actual NE. A first point is added, although far from the NE, that impacts the form of the cloud (top right). After adding 7 points (bottom left) the cloud is smaller and centered on the actual NE, and after 14 additions (bottom right), all simulated NE but two concentrate on the actual NE. The observed values have been added around the actual NE, but not only, which indicates that some exploration steps have been performed.

Both GP approaches consistently find the NE very quickly: the worst run required 14 cost evaluations. In contrast, the classical fixed-point algorithm required hundreds of evaluations, which is only marginally better than an exhaustive search on the 961-point grid. Besides, two out of five runs converged to the stationary point (1,1), which is not a NE. On this example, [formula] performed slightly better than SUR.

An open loop differential game

Differential games model a huge variety of competitive interactions, in social behavior, economics, biology among many others [\citep=MR0210469]. As a toy-model, let us consider p players who compete to drive a dynamic process to their preferred final state. Let denote T > 0 the final time, and t∈[0,T] the time variable. Then we consider the following simple process, whose state [formula] obeys the first order dynamics :

[formula]

The time-dependent action [formula] is the player (i)'s strategy, and the parameter αi(t)  =  e-  θit models the lifetime range of each player's influence on the process (θi is a measure of player (i)'s preference for the present). The source term [formula] is given, and does not depend on players actions.

All players have their own preferred final states [formula], and a limited energy to dispense in playing with their own action. The cost yi of player (i) is then the following :

[formula]

The game considered here is an open loop differential game (the decisions do not depend on the state), and belongs to the larger class of dynamic Nash games.

The time-dependent actions are discretized using polynomial approximations, as:

[formula]

where [formula] is the spline basis of degree κ - 1.

Now, the decision variables for player (i) is the array [formula] (the spline coefficients), and [formula]. We used κ = 1 (1  ×  2 constant decision variable per player) and κ = 2 (linear case, 2  ×  2 decision variables per player). The state equation ([\ref=edostate]) is solved by means of an explicit Euler scheme.

When the αi's do not depend on i, and the points [formula] are located on some -any- circle, then the Nash solution of the game puts the final state [formula] on the center of the circle (provided T is large enough to let the state reach this center starting from z0). In our setup, we have p  =  4 players, with targets positioned on the four corners of the 2 square, as illustrated by Figure [\ref=fig:DGsetting]. For the explicit Euler integration, T is set to 4, with 40 time steps. As for additional parameters, we chose [formula], [formula] and [formula], which makes the search of NE non-trivial.

The initial design space is set as d (with d = 8 or d = 16). It is discretized as follow: for each set of design variables belonging to a player (i.e., for κ = 1, x1 and x2, x3 and x4 and so on), we generate a 17-point hypercube design (hence, in dimension either two or four); then we take all the combinations of strategies, ending with a total of N = 174 = 83,521 possible strategies. On both cases, we followed Algorithm [\ref=alg:SUR] with n0  =  10d design points, K = M  =  20, selecting [formula] simulation points and [formula] candidates.

As for (P1),a fixed-point algorithm based on finite differences is also ran, and the experiment is replicated five times with different algorithm initializations.

We show the results for κ  =  1 and κ  =  2 in Table [\ref=tab:diffresultsK1]. Note that, here, all runs (including the fixed-point algorithm) converged to the actual NE. For the lower dimension problem (κ = 1, d = 8), all runs found the solution with less than 100 evaluations (that is, about 1% of the total number of possible strategies is evaluated). In this case, SUR appears as slightly more efficient than [formula]. For the higher dimension problem (κ = 2, d = 16), more evaluations are needed, yet all runs converge with less than 240 evaluations. As a comparison, on both cases the fixed-point algorithm is more than 60 (resp. 20) times more expensive.

CONCLUDING COMMENTS

We have proposed here a novel approach to solve Nash games with drastically limited budgets of evaluations based on GP regression, taking the form of a Bayesian optimization algorithm. Experiments on challenging synthetic problems demonstrate the potential of this approach compared to classical, derivative-based algorithms.

On the test problems, the two acquisition functions performed similarly well. [formula] has the benefit of not relying on conditional simulation paths, which makes it simpler to implement and less computationally intensive in most cases. Still, the SUR approach has several decisive advantages; in particular, it does not actually require the new observations to belong to the grid [formula], such that it could be optimized continuously. Moreover, it lays the groundwork for many extensions that may be pursued in future work.

First, SUR strategies are well-suited to allow selecting batches of points instead of only one, a key feature in distributed computer experiments [\citep=Chevalier2012]. Second, other games and equilibria may be considered: the versatility of the SUR approach may allow its transposition to other frameworks, such as mixed-strategies or Bayesian games. In particular, our framework transposes directly to the case of noisy evaluations, as it can be directly modeled by the GPs without affecting the acquisition functions. Finally, investigating convergence properties, in light of recent advances on SUR approaches [\citep=bect2016supermartingale], may be investigated.