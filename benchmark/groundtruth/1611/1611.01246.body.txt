Notation Corollary Lemma Proposition

Remark

Example

Probabilistic Condition Number Estimates for Real Polynomial Systems I: A Broader Family of Distributions

Introduction

When designing algorithms for polynomial system solving, it quickly becomes clear that complexity is governed by more than simply the number of variables and degrees of the equations. Numerical solutions are meaningless without further information on the spacing of the roots, not to mention their sensitivity to perturbation. A mathematically elegant means of capturing this sensitivity is the notion of condition number (see, e.g., [\cite=bcss] [\cite=cond] and our discussion below).

A subtlety behind complexity bounds incorporating the condition number is that computing it -- even within a large multiplicative error -- is provably as hard as computing the numerical solution one seeks in the first place (see, e.g., [\cite=demmel] for a precise statement in the linear case). However, it is now known that the condition number admits probabilistic bounds, thus enabling its use in average-case analysis, high probability analysis, and smoothed analysis of numerical algorithms. In fact, this probabilistic approach has revealed (see, e.g., [\cite=beltran] [\cite=derand] [\cite=lairez]) that, in certain settings, numerical solving can be done in polynomial-time on average, in spite of numerical solving having exponential worst-case complexity.

The numerical approximation of complex roots provides an instructive example of how one can profit from randomization.

First, there are classical reductions showing that deciding the existence of complex roots for systems of polynomials in [formula] is already [formula]-hard. However, classical algebraic geometry (e.g., Bertini's Theorem and Bézout's Theorem [\cite=shafa]) tells us that, with probability 1, the number of complex roots of a random system of homogeneous polynomials, [formula] (with each pi having fixed positive degree di), is 0, [formula], or infinite, according as m   >   n - 1, m   =   n - 1, or m   <   n - 1. (Any probability measure absolutely continuous with respect to Lebesgue measure will do in the preceding statement.)

Secondly, examples like [formula], which has affine roots [formula] and [formula], reveal that the number of digits necessary to distinguish the coordinates of roots of P may be exponential in n (among other parameters). However, it is now known via earlier work on discriminants and random polynomial systems (see, e.g., [\cite=pardo]) that the number of digits needed to separate roots of P is polynomial in n with high probability, assuming the coefficients are rational, and the polynomial degrees and coefficient heights are bounded. More simply, a classical observation from the theory of resultants (see, e.g., [\cite=cannyphd]) is that, for any positive continuous probability measure on the coefficients, P having a root with Jacobian matrix possessing small determinant is a rare event. So, with high probability, small perturbations of a P with no degenerate roots should still have no degenerate roots. More precisely, we recall below a version of the condition number used in [\cite=SS] [\cite=beltran] [\cite=lairez]. Let us also recall that the singular values of a matrix [formula] are the (nonnegative) square roots of the eigenvalues of [formula], where [formula] denotes the transpose of T.

Given [formula] and [formula], let [formula] and, for any homogenous polynomial [formula] with deg pi   ≤   di, note that the number of monomial terms of pi is at most Ni. Letting [formula] and [formula], let ci,α denote the coefficient of xα in pi, and set [formula]. Also, let us define the Weyl-Bombieri norms of pi and P to be, respectively, [formula] and [formula]. Let [formula] be the diagonal matrix with diagonal entries [formula] and let DP(x)|TxSn - 1 denote the linear map defined by the Jacobian matrix of the polynomial system P, evaluated at a point x, and restricted to the tangent space of the (real) (n - 1)-sphere Sn - 1 at x. Finally, when m   =   n - 1, we define the (normalized) local condition number (for solving [formula]) to be [formula] or [formula], according as DP(x) is invertible or not, where [formula] is the largest singular value of a matrix A. [formula]

Clearly, [formula] as P approaches a system possessing a degenerate root [formula] and x approaches ζ. The intermediate normalizations in the definition are useful for geometric interpretations of [formula] in terms of discriminant varieties (see, e.g., [\cite=M2] and Theorem [\ref=thm:dist] below). In particular, the preceding condition number (in the special case m   =   n - 1) was a central ingredient in the recent positive solution to Smale's 17th Problem [\cite=beltran] [\cite=lairez]: For the problem of numerically approximating a single complex root of a polynomial system, a particular randomization model (independent complex Gaussian coefficients with specially chosen variances) enables polynomial-time average-case complexity, in the face of exponential deterministic complexity.

It is natural to seek similar speed-ups for the harder problem of numerically approximating real roots of real polynomial systems. However, an important subtlety one must consider is that the number of real roots of n - 1 homogeneous polynomials in n variables (of fixed degree) is no longer constant with probability 1, even if the probability measure for the coefficients is continuous and positive. In particular, some systems can be perturbed infinitesimally from having no real roots at all to having many real roots, and vice-versa. A condition number for real solving that takes these subtleties into account was developed and applied in the seminal series of papers [\cite=M1] [\cite=M2] [\cite=M3]. In these papers, the authors performed a probabilistic analysis assuming the coefficients were centered Gaussian distribution with very specially chosen variances and independent coordinates.

[\cite=M1] Let [formula] and [formula]. We respectively call (P,x) and (P) the local and global condition numbers for real solving. [formula]

Note in particular that a large condition number can be caused not only by a root with small Jacobian determinant but also by the existence of a critical point for P with small corresponding critical value. So a large condition number for real solving is meant to capture the spontaneous creation of real roots, as well as the bifurcation of a single degenerate root into multiple distinct real roots, arising from small perturbations of the coefficients.

Our main results, Theorems [\ref=gcondition] and [\ref=expectation] in Section [\ref=sec:main1], show that useful condition number estimates can be derived for a broader class of probability measures than considered earlier. Our results thus allow dependence and non-Gaussian distributions and, unlike the existing literature, our methods do not use any additional algebraic structure, e.g., invariance under the unitary group acting linearly on the variables (as in [\cite=SS] [\cite=M1] [\cite=M2] [\cite=M3]). This aspect also allows us to begin to address sparse polynomials (in the sequel to this paper), where linear changes of variables would destroy sparsity.

To compare our results with earlier estimates, let us first recall a central estimate from [\cite=M3].

[\cite=M3] Let [formula] be a random system of homogenous n-variate polynomials where n   ≥   3 and [formula] with the ci,α independent real Gaussian random variables having mean 0 and variance 1. Then, letting [formula], d: =  max idi, [formula], and [formula], we have: 1. [formula] 2. [formula].

The expanded class of distributions we allow for the coefficients of P satisfy the following more flexible hypotheses:

Any random vector [formula] with a bounded density function f supported on a compact set satisfies the above assumptions. In this case the constants c0 and K will depend on the density f. We are mainly interested in distributions which satisfies the above assumptions with universal constants c0 and K. Nevertheless, to maintain generality we kept track of the dependency on c0 and K, hence our theorems hold for any distribution satisfying the above assumptions.

The standard Gaussian distribution is a typical example of a random vector satisfying our assumptions with universal constants. Note that it is easy to create one dimensional random variables satisfying the assumptions above. For example, any random variable Xi with density function [formula] where cp is the appropriate normalization and p  >  2 would do the assumptions with universal constants. Now for a random vector X = (Xi)Nii = 1 with independent coordinates, if Xi satisfy the above assumptions with universal constants then X satisfies the assumptions with universal constants as well. This is neither obvious nor trivial, it is a significant fact recently proved by Rudelson and Vershynin [\cite=RV-1].

A non-trivial example of a random vector satisfying our assumptions is the uniform measure on BNip, p > 2, where [formula]. In this case the subgaussian assumption follows from ([\cite=BK], Section 6) and the Small Ball Assumption is a direct consequence of the well-known fact that BNip satisfies the Hyperplane Conjecture. Another important example is the uniform distribution on the unit sphere.

A simplified summary of our main results (Theorems [\ref=gcondition] and [\ref=expectation] from Section [\ref=sec:main1]), in the special case of square dense systems, is the following:

There is an absolute constant A   >   0 with the following property: Let [formula] be a random system of homogenous n-variate polynomials where [formula] and [formula] are random vectors satisfying the Centering, Sub-Gaussian and Small Ball assumptions, with underlying constants c0 and K. Then, letting d: =  max idi, [formula], and [formula], the following bounds hold: 1. [formula] 2. [formula].

Corollary [\ref=cor:main1] is proved in Section [\ref=sec:main1]. Our main theorem in Section [\ref=sec:main1] includes stronger estimates, we preferred simplicity in corollary [\ref=cor:main1] with the expense of loosening our bounds.

Theorem [\ref=CKMW] is restricted to a particular family of Gaussian distributions, and assumes m   =   n - 1. For the case m   =   n - 1, our estimates in Section [\ref=sec:main1] are comparable to Theorem [\ref=CKMW]: for t  ≤  (ed)n - 1 our bounds are slightly better than the bounds of Theorem [\ref=CKMW], and for the other phase t  ≥  (ed)n - 1 bounds of Theorem [\ref=CKMW] are slightly better than ours. Our estimates on [formula] are quite similar to the Theorem [\ref=CKMW] with the exception that we loose a factor 2.

Our techniques enable better bounds for arbitrary m  ×  n systems when m is larger than n - 1: See the next section for the definition of a condition number enabling m  > n - 1, and the statements of Theorems [\ref=gcondition] and [\ref=expectation] for our most general condition number bounds. There appear to have been no probabilistic condition number estimates for the case m   >   n - 1 until now.

To the best of our knowledge, the only other result toward estimating condition numbers of non-Gaussian random polynomial systems is due to Nguyen [\cite=Ng]. However, in [\cite=Ng] the degrees of the polynomials are assumed to be bounded by a small fraction of the number of variables, and the quantity analyzed in [\cite=Ng] is not the condition number considered in [\cite=SS] or [\cite=M1] [\cite=M2] [\cite=M3].

It would certainly be nice to know the correct decay rate for the probability that the condition number is large: This is not even known in the restricted Gaussian case of Cucker, Malajovich, Krick, and Wschebor. So we also prove lower bounds for the condition number of a random polynomial system. To establish these bounds, we need one more assumption on the randomness.

If the vectors Ci have independent coordinates satisfying the Centering and Small Ball Assumptions, then Lemma [\ref=smallBallTensorize] from Section [\ref=sub:smallball] implies that the Euclidean Small Ball Assumption holds as well. Moreover, if the Ci are uniformly distributed on a convex body X and satisfy our Centering and Sub-Gaussian assumptions, then a result of Jean Bourgain [\cite=Bou] (see also [\cite=DP] or [\cite=KM] for alternative proofs) implies that both the Small Ball and Euclidean Small Ball Assumptions hold, and with 0 depending only on the Sub-Gaussian constant K (not the convex body X). [formula]

Suppose n,d  ≥  3, m = n - 1, and dj = d for all [formula]. Also let [formula] be a random polynomial system satisfying our Centering, Sub-Gaussian, Small Ball, and Tiny Ball assumptions, with respective underlying constants K and [formula]. Then there are constants A2   ≥   A1   >   0 such that

[formula]

Corollary [\ref=getlog] follows immediately from a more general estimate: Lemma [\ref=lower-bound] from Section [\ref=sub:smallball]. It would certainly be more desirable to know bounds within a constant multiple of (P) instead. We discuss more refined estimates of the latter kind in Section [\ref=sub:lower], after the proof of Lemma [\ref=lower-bound].

As we close our introduction, we point out that one of the tools we developed for our main theorems may be of independent interest: Theorem [\ref=Kellogsys] of the next section extends, to polynomial systems, an earlier estimate of Kellog [\cite=kellog] on the norm of the derivative of a single multivariate polynomial.

Technical Background

We start by defining an inner product structure on spaces of polynomial systems. For n-variate degree d homogenous polynomials [formula] and [formula], their Weyl-Bombieri inner product is defined as

[formula]

It is known (see, e.g., [\cite=kostlan]) that for U∈O(n) we have

[formula]

Let [formula] and let HD denote the space of (real) m  ×  n systems of homogenous n-variate polynomials with degrees respectively bounded from above by [formula]. Then for [formula] and [formula] the Weyl-Bombieri inner product for two polynomial systems is defined as follows:

[formula]

A geometric justification for the definition of the condition number [formula] can then be derived as follows: For x∈Sn - 1 we define the set of polynomial systems with singularity at x as [formula] We then define [formula] (the real part of the disciminant variety) to be: [formula]. Using the Weyl-Bombieri inner-product to define the underlying distance, we point out the following important geometric characterization of [formula]:

[\cite=M2] When m   =   n - 1 we have [formula] for all P ∈ HD. [formula]

We call a polynomial system [formula] with m   =   n - 1 (resp. m   ≥   n) square (resp.  over-determined). Newton's method for over-determined systems was studied in [\cite=DS1]. So now that we have a geometric characterization of the condition number for square systems it will be useful to also have one for over-determined systems.

For any system of homogeneous polynomials [formula] let us set [formula], where [formula] is the smallest singular value of a matrix A. Let us then define [formula] and [formula]. [formula]

The quantity [formula] thus plays the role of [formula] in the more general setting of m   ≥   n - 1. We now recall an important observation from Section 2 of [\cite=M2]: Setting Dx(P): = DP(x)|TxSn - 1 we have

[formula]

when m   =   n - 1 and Dx(P) is invertible. So by the definition of [formula] we have

[formula]

and thus our more general definition agrees with the classical definition in the square case.

Since the W-norm of a random polynomial system has strong concentration properties for a broad variety of distributions (see, e.g., [\cite=V]), we will be interested in the behavior of L(P,x). So let us define [formula]. It follows directly that [formula].

We now recall a classical result of Kellog:

[\cite=kellog] Let [formula] have degree d and set [formula] and [formula]. Then:

[formula].

If p is homogenous then we also have [formula]. [formula]

For any system of homogeneous polynomials [formula] define [formula]. Let DP(x)(u) denote the image of the vector u under the linear operator DP(x), and set

[formula]

Let [formula] be a polynomial system with pi homogeneous of degree di for each i and set d : =   max idi. Then:

[formula].

If deg (pi) = d for all [formula] then we also have [formula].

Proof. Let (x0,u0) be such that [formula] and let [formula] where [formula]. Note that [formula]. Now define a polynomial [formula] of degree d via [formula] and observe that

[formula]

[formula]

and [formula]. In particular, for our chosen x0 and u0, we have

[formula]

Using the first part of Kellog's Theorem we have

[formula]

Now we observe by the Cauchy-Schwarz Inequality that

[formula]

So we conclude that [formula]. We also note that in the case deg (pi) = d for all i, q(x) is a homogenous polynomial of degree d. So for this special case, the second part of Kellog's Theorem directly implies [formula]. [formula]

Using our extension of Kellog's Theorem to polynomial systems, we develop useful estimates for [formula] and [formula]. In what follows, we call a subset N of a metric space X a δ-net on X if and only if the maximal distance between a point of X and a point of N is δ. A basic fact we'll use repeatedly is that, for any δ > 0 and compact X, one can always find a finite δ-net for X.

Let [formula] be a system of homogenous polynomials, N a δ-net on Sn - 1, and set d: =  max idi. Let [formula]. Similarly let us define [formula], and set [formula]. Then:

[formula].

If deg (pi) = d for each [formula] then we have

[formula]

We first prove Assertion (2). Toward this end, first observe that the Lipschitz constant of P on Sn - 1 is bounded from above by [formula]: This can be seen by taking x,y∈Sn - 1 and considering the integral

[formula]

Since [formula] for all t∈[0,1], the homogeneity of the system P implies

[formula]

Using the integral formula above we conclude

[formula]

Now, when the degrees of the pi are identical, let the Lipschitz constant of P be M. By Assertion (2) of Theorem [\ref=Kellogsys] we have

[formula]

Now let x0∈Sn - 1 be such that [formula] and let y∈N satisfy [formula]. Then [formula], and thus

[formula]

To bound the norm of [formula] let us consider the net [formula] on [formula]. Let [formula] and [formula] be such that [formula] for all i. Clearly, [formula]. Since x was arbitrary, this argument proves Nk + 1 is an [formula]-net. Note also that [formula] is a homogenous polynomial system with (k + 1)n variables and degree d. The desired bound then follows from the inequality obtained above.

To prove Assertion (1) of our current theorem, the preceding proof carries over verbatim, simply employing Assertion (1) (instead of Assertion (2)) from Theorem [\ref=Kellogsys].

Condition Number of Random Polynomial Systems

Introducing Randomness

Now let [formula] be a random polynomial system where [formula]. In particular, recall that [formula] and we let [formula] be a random vector in [formula] satisfying the Centering, Sub-Gaussian, and Small Ball assumptions from the introduction. Letting [formula] we then have pj(x) = 〈Cj,Xj〉. In particular, recall that the Sub-Gaussian assumption is that there is a K   >   0 such that for each θ∈SNj - 1 we have [formula] for all t > 0. Recall also that the Small Ball assumption is that there is a c0 > 0 such that for every vector [formula] we have [formula] for all ε > 0. In what follows, several of our bounds will depend on the parameters K and c0 underlying the random variable being Sub-Gaussian and having the Small Ball property.

Let ξ be a random variable on [formula]. We denote the median of ξ by [formula]. Now, if [formula], then setting t : =  2K in the Sub-Gaussian assumption for Cj yields [formula], i.e., [formula]. On the other hand, setting [formula] in the Small Ball assumption for Cj yields [formula], i.e., [formula]. Writing [formula] we then easily obtain

[formula]

In what follows we will use Inequality ([\ref=ineq:kc]) several times.

The Sub-Gaussian Assumption and Bounds Related to Operator Norms

. We will need the following inequality, reminiscent of Hoeffding's classical inequality [\cite=hoeffding].

[\cite=V] There is an absolute constant c   >   0 with the following property: If [formula] are sub-Gaussian random variables with mean zero and underlying constant K, and [formula] and t  ≥  0, then

[formula]

Let [formula] be a random polynomial system where, as before, [formula] and the the coefficient vectors Cj are independent random vectors satisfying the Centering, Sub-Gaussian, and Small Ball assumptions from the introduction, with underlying constants K and c0. Then, for N a δ-net over Sn - 1 and t  ≥  2, we have the following inequalities:

If deg (pj) = d for all [formula] then

[formula]

In particular, there is a constant c1   ≥   1 such that for [formula] and t = s log (ed) with s  ≥  1 we have [formula].

If d: =  max j deg pj then

[formula]

In particular, there is a constant c2   ≥   1 such that for [formula], t = s log (ed) with s  ≥  1, we have [formula].

We prove Assertion (2) since the proofs of the two assertions are virtually identical. First observe that the identity [formula] implies [formula] for all j  ≤  m. Using our sub-Gaussian assumption on the random vectors Cj, and the fact that pj(x) = 〈Cj,Xj〉, we obtain that [formula] for every x∈Sn - 1.

Now we need to tensorize the preceding inequality. By Theorem [\ref=Bernstein], we have for all a∈Sm - 1 that [formula]. Letting M be a δ-net on Sm - 1 we then have [formula], where we have used the classical union bound for the multiple events defined by the (finite) δ-net M. Since [formula], an application of Lemma [\ref=net-norm] for the linear functional 〈  .  ,P(x)〉 gives us [formula].

It is known that for any δ   >   0, Sm - 1 admits a δ-net M such that [formula] (see, e.g, [\cite=V]). So for t  ≥  1 and [formula] we have [formula] for some suitable constant c2   ≥   c. We have thus arrived at a point-wise estimate on [formula]. Doing a union bound on a δ-net N now on Sn - 1 we then obtain:

[formula]

Using Lemma [\ref=net-norm] once again completes our proof.

Theorem [\ref=Kellogsys] and Lemma [\ref=operatornorm] then directly imply the following:

Let P be a random polynomial system as in Lemma [\ref=operatornorm]. Then there are constants c1,c2   ≥   1 such that the following inequalities hold for s  ≥  1:

If deg (pj) = d for all [formula] then both [formula] and [formula] are bounded from below by 1 - 2e- c1s2m log (ed).

If d: =  max j deg pj then both [formula] and [formula] are bounded from below by 1  -  2e- c2s2m log (ed). [formula]

The Small Ball Assumption and Bounds for L(P,x)

We will need the following standard lemma (see, e.g., [\cite=RV] or [\cite=NZ]).

Let [formula] be independent random variables such that, for every ε > 0, we have [formula]. Then there is a constant  > 0 such that for every ε > 0 we have [formula]. [formula]

We can then derive the following result:

Let [formula] be a random polynomial system, satisfying the Small Ball assumption with underlying constant c0. Then there is a constant  > 0 such that for every ε  >  0 and x∈Sn - 1 we have [formula].

By the Small Ball assumption on the random vectors Ci, and observing that pi(x) = 〈Ci,Xi〉 and [formula] for all x∈Sn - 1, we have [formula]. By Lemma [\ref=smallBallTensorize] we are done.

The next lemma is a variant of [\cite=Ng].

Let n  ≥  3, let [formula] be a system of n-variate homogenous polynomials, and assume [formula] and d: =  max idi. Let x,y∈Sn - 1 be such that [formula] and L(x,y)  ≤  α, and let r∈[ -  1,1]. Then for every w with w = x + βry  +  βz for some z∈Sn - 1 with [formula] and [formula], we have the following inequalities:

If [formula] then [formula].

If deg (pi) = d for all [formula] and [formula] then [formula].

We will prove just Assertion (2) since the proof of Assertion (1) is almost the same. We start with some auxiliary observations on [formula]: First note that Theorem [\ref=Kellogsys] tells us that [formula] implies [formula] and, similarly, [formula] for every k  ≥  1. Also, for any w and ui∈Sn - 1 with [formula], [formula] and the homogeneity of the pi implies [formula]. These observations then yield the following inequalities for w = x + βry  +  βz with z∈Sn - 1, |r|  ≤  1, β  ≤  d- 1, k = 3, and u1,u2,u3∈Sn - 1: [formula] and [formula], where [formula].

Setting [formula] we then have

[formula]

Note that [formula]. Applying the Cauchy-Schwarz Inequality to the vectors [formula] and [formula] then implies the following inequality: [formula].

Summing over all [formula], using the assumption [formula] and our earlier observations, we have:

[formula].

Clearly [formula]. Hence we have: [formula]. Since β  ≤  d- 1 and [formula], we then have

[formula]

Let n  ≥  3 and let [formula] be a system of random homogenous n-variate polynomials such that [formula] where [formula] are random vectors satisfying the Small Ball assumption with underlying constant c0. Let α,γ > 0. Then we have the following inequalities:

If d: =  max j deg (pj) and [formula] then

[formula]

If deg (pj) = d for all [formula] and [formula] then

[formula]

We first assume the hypotheses of Assertion (1): Let α,γ > 0 and [formula]. Let [formula] and let [formula]. Let Γ: = 5(α2  +  19β2d4γ2) and let Bn2 denote the [formula] unit ball in [formula]. Lemma [\ref=Taylor] implies that, if the event [formula] occurs, then there exists a set

[formula]

such that [formula] for every w in this set. Let V: = |Vx,y|. Note that for w∈Vx,y, [formula]. Since [formula], we have showed that the event [formula] is included in the event [formula]. Using Markov's Inequality, Fubini's Theorem, and Lemma [\ref=smallball], we can estimate the probability of this event. Indeed, [formula]

[formula]

Now recall that [formula]. Then [formula] for some constant c' > 0. We also assume that [formula] which yields (1 + 2β2)n  ≤  e2, and we compute that

[formula]

for some absolute constant c > 0. Write [formula] for any x  ≠  0. Then for z∉Bn2 we have that [formula], which implies that for every [formula] we have

[formula]

where we have used Lemma [\ref=smallball]. So we conclude that [formula]. Recall that Γ  =  5(α2  +  19β2d4γ2). We choose [formula] and observe that, under our assumption that α  ≤  γ, our choice of β implies that Γ  =  100α2. So we obtain

[formula]

and thus we have proved Assertion (1).

Applying our preceding argument under the assumptions of Assertion (2), we obtain Γ: = 5(α2  +  19β2d2γ2). In this case, choosing [formula] implies

[formula]

By adjusting constants we are done.

The Condition Number Theorem and its Consequences

We will now need bounds for the Weil-Bombieri norm of polynomial systems. Note that, with [formula], we have [formula] for [formula]. The following lemma providing large deviation estimates for the Euclidean norm is standard and follows, for instance, from Theorem [\ref=Bernstein].

There is a universal constant c'   >   0 such that for any a random n-variate polynomial system P satisfying the Centering and sub-Gaussian assumptions with underlying constant K, [formula], [formula], [formula], and t  ≥  1, we have:

[formula].

[formula]. [formula]

We are now ready to prove our main theorem on condition numbers of random polynomial systems.

There are universal constants A,c   >   0 such that the following hold: Let [formula] be a system of homogenous random polynomials with [formula] and let each [formula] be an independent random vector satisfying the Sub-Gaussian and Small Ball assumptions, with respective underlying constants K and c0. Then:

If d =  deg (pj) for all [formula] then, setting [formula], we have that [formula] is bounded from above by

[formula]

If d =  max j deg pj then, setting [formula], we have two cases:

If N  ≥  m log (ed) then [formula] is bounded from above by

[formula]

If N  ≤  m log (ed) then [formula] is bounded from above by

[formula]

We first consider Assertion (2). Recall that [formula]. Note that if u   >   0, and the inequalities [formula] and [formula] hold, then we clearly have (P)   ≥   tM. In particular, u   >   0 implies that

[formula]

To estimate the last two probabilities, we apply Theorem [\ref=L-theorem] with [formula] and [formula], Lemma [\ref=operatornorm], and Lemma [\ref=AG]. Now let us note our restrictions: We have that s  ≥  1, u  ≥  1, and (since [formula]) we have

([formula]) [formula].

Note that ( [formula]) is true if u  ≤  s and t  ≥  1. Under the above restrictions we have that if [formula] then

[formula]

or,

[formula]

for some suitable c2   >   0. We consider first the case where N  ≥  m log (ed). If [formula] then we take u = s = 1 (note that ( [formula]) is satisfied and that c2  ≥  1) and we then obtain [formula] for t  ≥  1 and u  ≥  s  ≥  1. In the case where [formula] we choose u = 1 and [formula]. (Note that u  ≤  s). These choices then yield

[formula]

In the case where [formula], we choose [formula] and [formula]. (Note that u  ≤  s also in this case). In this case we get that

[formula]

[formula]

We consider now the case where N  ≤  m log (ed). In the case [formula] we choose s = 1 and u = 1 and we get

[formula]

as before. In the case [formula], we choose [formula]. Note that again ( [formula]) is satisfied and with these choices we get

[formula]

[formula]

When dj = d for all [formula] the proof is similar: One has just to observe that in this case we have N  ≥  m log (ed).

Let P be a random polynomial system as in Theorem [\ref=gcondition] and let M be as defined in Theorem [\ref=gcondition]. Set

[formula]

[formula]

Then we have the following estimates:

If d = dj for all [formula] and q ∈ (0,m - n + 2) then we have

[formula]

In particular, if [formula], then [formula], and if [formula] then [formula]. Furthermore, [formula].

If max j deg (pj) = d then consider the following two cases:

If N  ≥  m log (ed) and q ∈ (0,m - n + 2) then

[formula]

In particular, if [formula], then [formula], and if [formula] then [formula]. Furthermore, [formula].

If N  ≤  m log (ed) then [formula]. In particular, if [formula], then [formula], and if [formula] then [formula]. Furthermore, [formula].

We first consider the case where d = dj for all [formula]. Set

[formula]

[formula]

Note that we have assumed that r  ≥  1. Using the formula

[formula]

(which follows easily from the definition of expectation), and Theorem [\ref=gcondition], we have that [formula] or  [formula].   We will give upper bounds for the last three integrals. First note that

[formula]

Also, we have that

[formula]

[formula]

Finally, we check that

[formula]

[formula]

[formula]

Note that if [formula] then δ1,δ2  ≤  1.

The proof for the case max jdj  =  d and N  ≥  m log (ed) is identical. For the case max jdj  =  d and N  ≤  m log (ed), working as before, we get that

[formula]

In the case N  ≤  m log (ed) we have [formula]. In particular, for this case, it easily follows that [formula] implies δ2  ≤  1.

Note that if m = n - 1 then N  ≥  m log (ed), and in then easily follows that [formula] still holds even if reduce M by deleting its factor of [formula]. So then, for the important case m = n - 1, our main theorems immediately admit the following refined form:

There are universal constants A,c   >   0 such that if P is any random polynomial system as in Theorem [\ref=gcondition], but with m = n - 1, then the following hold:

If d =  deg (pi), 1  ≤  i  ≤  m then, setting [formula], we have [formula] and, for all [formula], we have [formula]. Furthermore, [formula].

If max i deg (pi) = d then, setting [formula], we have [formula], and, for all [formula], we have [formula]. Furthermore, [formula]. [formula]

We are now ready to prove Corollary [\ref=cor:main1] from the introduction.

Proof of Corollary [\ref=cor:main1]: From Corollary [\ref=square], Bound (2) follows immediately, and Bound (1) is clearly true for the smaller domain of t. So let us now consider t = xe(n - 1) log (ed) with x  ≥  1. Clearly, [formula], and thus [formula]. Since [formula] we thus obtain

[formula]

Renormalizing the pair (M,t) (since the M from Corollary [\ref=square] is larger than the M from Corollary [\ref=cor:main1] by a factor of A), we are done. [formula]

On the Optimality of Condition Number Estimates

As mentioned in the introduction, to establish a lower bound we need one more assumption on the randomness. For the convenience of the reader, we recall the Euclidean small ball assumption.

(Euclidean Small Ball) There is a constant 0 > 0 such that for each [formula] and ε   >   0 we have [formula].

We will need an extension of Lemma [\ref=smallBallTensorize]: Lemma [\ref=Aniso-p] below (see also [\cite=RV-1]). Toward this end, for any matrix T: = (ti,j)1  ≤  i,j  ≤  m, write [formula] for its Hilbert-Schmidt norm of T and [formula] for its operator norm, i.e.,

[formula]

Let [formula] be independent random variables satisfying the following Small Ball assumption: For all [formula] and ε   >   0 we have [formula]. Let [formula]. Then there is a constant c   >   0 such that for any m  ×  m matrix T and ε   >   0 we have [formula].

Our main lower bound for the condition number is then the following:

Let [formula] be a homogeneous n-variate polynomial system with  deg(pj)  =  dj for all j. Then [formula]. Moreover if [formula] is a random polynomial system satisfying our Sub-Gaussian and Tiny Ball assumptions, with respective underlying constants K and [formula], then we have

[formula]

[formula]

where [formula] are absolute constants. In particular we have [formula] when d = dj for all [formula].

First note that Theorem [\ref=kellog] implies that for every x,y∈Sn - 1 we have

[formula]

So we have [formula]. Now recall that [formula]. So we get [formula], which in turn implies that

[formula]

The proof for the case where dj = d for all [formula] is identical.

We now show that under our Tiny Ball Assumption the following holds: For every ε > 0

[formula]

Indeed, recall that [formula]. Then for any fixed ε > 0,

[formula]

where [formula] is such that Nj0: =  min jNj. Let [formula] for any [formula]. Set [formula] and [formula]. Note that [formula], [formula], and [formula]. Then Lemma [\ref=Aniso-p] implies

[formula]

Recall that Lemma [\ref=operatornorm] implies that for every t  ≥  1,

[formula]

So, using our lower bound estimate for the condition number, we get

[formula]

[formula]

and

[formula]. We may choose [formula] and, by adjusting constants, we get our result. The case where dj = d for all [formula] is similar. The bounds for the expectation follow by integration.

Observe that the dominant factor in the very last estimate of Lemma [\ref=lower-bound] is [formula], which is the normalization coming from the Weil-Bombieri norm of the polynomial system. So it makes sense to seek the asymptotic behavior of [formula]. When m = n - 1, the upper bounds we get are exponential with respect to n, while the lower bounds are not. But when m = 2n - 4 we have the following estimates when d = dj for all [formula]:

[formula]

where A1,A2 are constants depending on (K,c0). This suggests that our estimates are closer to optimality when m is much larger than n.

There are similarities between our probability tail estimates and the older estimates in the linear case studied in [\cite=RV-2]. In particular our estimates in the quadratic case d = 2, when m is a constant multiple of n and n  →    ∞  , are quite similar to the optimal result (for the linear case) appearing in [\cite=RV-2]. This indicates that, in the proportional case (i.e., when m is a constant multiple of n and n  →    ∞  ), our tail bounds are close to optimal. [formula]