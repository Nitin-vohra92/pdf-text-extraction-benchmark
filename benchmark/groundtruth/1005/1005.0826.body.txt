Clustering processes

Introduction

Given a finite set of objects, the problem is to "cluster" similar objects together. This intuitively simple goal is notoriously hard to formalize. Most of the work on clustering is concerned with particular parametric data generating models, or particular algorithms, a given similarity measure, and (very often) a given number of clusters. It is clear that, as in almost learning problems, in clustering finding the right similarity measure is an integral part of the problem. However, even if one assumes the similarity measure known, it is hard to define what a good clustering is [\cite=Kleinberg:02] [\cite=Zadeh:09]. What is more, even if one assumes the similarity measure to be simply the Euclidean distance (on the plane), and the number of clusters k known, then clustering may still appear intractable for computational reasons. Indeed, in this case finding k centres (points which minimize the cumulative distance from each point in the sample to one of the centres) seems to be a natural goal, but this problem is NP-hard [\cite=Mahajan:09].

In this work we concentrate on a subset of the clustering problem: clustering processes. That is, each data point is itself a sample generated by a certain discrete-time stochastic process. This version of the problem has numerous applications, such as clustering biological data, financial observations, or behavioural patterns, and as such it has gained a tremendous attention in the literature.

The main observation that we make in this work is that, in the case of clustering processes, one can benefit from the notion of ergodicity to define what appears to be a very natural notion of consistency. This notion of consistency is shown to be satisfied by simple algorithms that we present, which are polynomial in all arguments. This can be achieved without any modeling assumptions on the data (e.g. Hidden Markov, Gaussian, etc.), without assuming independence of any kind within or between the samples. The only assumption that we make is that the joint distribution of the data is stationary ergodic. The assumption of stationarity means, intuitively, that the time index itself bares no information: it does not matter whether we have started recording observations at time 0 or at time 100. By virtue of the ergodic theorem, any stationary process can be represented as a mixture of stationary ergodic processes. In other words, a stationary process can be thought of as first selecting a stationary ergodic process (according to some prior distribution) and then observing its outcomes. Thus, the assumption that the data is stationary ergodic is both very natural and rather weak. At the same time, ergodicity means that, in asymptotic, the properties of the process can be learned from observation.

This allows us to define the clustering problem as follows. N samples are given: [formula]. Each sample is drawn by one out of k different stationary ergodic distributions. The samples are not assumed to be drawn independently; rather, it is assumed that the joint distribution of the samples is stationary ergodic. The target clustering is as follows: those and only those samples are put into the same cluster that were generated by the same distribution. The number k of target clusters can be either known or unknown (different consistency results can be obtained in these cases). A clustering algorithm is called asymptotically consistent if the probability that it outputs the target clustering converges to 1, as the lengths ([formula]) of the samples tend to infinity (a variant of this definition is to require the algorithm to stabilize on the correct answer with probability 1). Note the particular regime of asymptotic: not with respect to the number of samples N, but with respect to the length of the samples [formula].

Similar formulations have appeared in the literature before. Perhaps the most close approach is mixture models [\cite=Smyth:97] [\cite=Zhong:03]: it is assumed that there are k different distributions that have a particular known form (such as Gaussian, Hidden Markov models, or graphical models) and each one out of N samples is generated independently according to one of these k distributions (with some fixed probability). Since the model of the data is specified quite well, one can use likelihood-based distances (and then, for example, the k-means algorithm), or Bayesian inference, to cluster the data. Clearly, the main difference from our setting is in that we do not assume any known model of the data; not even between-sample independence is assumed.

The problem of clustering in our formulation generalizes two classical problems of mathematical statistics. The first one is homogeneity testing, or the two-sample problem. Two samples [formula] and [formula] are given, and it is required to test whether they were generated by the same distribution, or by different distributions. This corresponds to clustering just two data points (N = 2) with the number k of clusters unknown: either k = 1 or k = 2. The second problem is process classification, or the three-sample problem. Three samples [formula] are given, it is known that two of them were generated by the same distribution, while the third one was generated by a different distribution. It is required to find out which two were generated by the same distribution. This corresponds to clustering three data points, with the number of clusters known: k = 2. The classical approach is of course to consider Gaussian i.i.d. data, but general non-parametric solutions exist not only for i.i.d. data [\cite=Lehmann:86], but also for Markov chains [\cite=Gutman:89], and under certain mixing rates conditions. What is important for us here, is that the three-sample problem is easier than the two-sample problem; the reason is that k is known in the latter case but not in the former. Indeed, in [\cite=Ryabko:10discr] it is shown that in general, for stationary ergodic (binary-valued) processes, there is no solution to the two-sample problem, even in the weakest asymptotic sense. However, a solution to the three-sample problem, for (real-valued) stationary ergodic processes was given in [\cite=Ryabko:103s].

In this work we demonstrate that, if the number k of clusters is known, then there is an asymptotically consistent clustering algorithm, under the only assumption that the joint distribution of data is stationary ergodic. If k is unknown, then in this general case there is no consistent clustering algorithm (as follows from the mentioned result for the two-sample problem). However, if an upper-bound αn on the α-mixing rates of the joint distribution of the processes is known, and [formula], then there is a consistent clustering algorithm. Both algorithms are rather simple, and are based on the empirical estimates of the so-called distributional distance. For two processes ρ1,ρ2 a distributional distance d is defined as [formula], where wk are positive summable real weights, e.g. wk = 2- k, and Bk range over a countable field that generates the sigma-algebra of the underlying probability space. For example, if we are talking about finite-alphabet processes with the binary alphabet A = {0,1}, Bk would range over the set [formula]; that is, over all tuples [formula] (of course, we could just as well omit, say, 1 and 11); therefore, the distributional distance in this case is the weighted sum of differences of probabilities of all possible tuples. In this work we consider real-valued processes, so Bk have to range through a suitable sequence of intervals, all pairs of such intervals, triples, etc. (see the formal definitions below). This distance has proved a useful tool for solving various statistical problems concerning ergodic processes [\cite=Ryabko:103s] [\cite=Ryabko:101c].

Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated. For the case of a known number of clusters, the proposed algorithm (which is shown to be consistent) is as follows. (The distance in the algorithms is a suitable empirical estimate of d.) The first sample is assigned to the first cluster. For each j = 2..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign it to the cluster j. Thus we have one point in each of the k clusters. Next, assign each of the remaining points to the cluster that contains the closest points from those k already assigned. For the case of an unknown number of clusters k, the algorithm simply puts those samples together that are not farther away from each other than a certain threshold level, where the threshold is calculated based on the known bound on the mixing rates. In this case, besides the asymptotic result, finite-time bounds on the probability of outputting an incorrect clustering can be obtained. Each of the algorithms is shown to be at most quadratic in each argument.

Therefore, we show that for the proposed notion of consistency, there are simple algorithms that are consistent under most general assumptions. While these algorithms can be easily implemented, we have left the problem of trying them out on particular applications, as well as optimizing the parameters, for future research. It may also be suggested that the empirical distributional distance can be replaced by other distances, for which similar theoretical results can be obtained. An interesting direction, that could preserve the theoretical generality, would be to use data compressors. These were used in [\cite=BRyabko:06a] for the related problems of hypotheses testing, leading both to theoretical and practical results. As far as clustering is concerned, compression-based methods were used (without asymptotic consistency analysis) in [\cite=Cilibrasi:05], and (in a different way) in [\cite=Bagnall:05]. Combining our consistency framework with these compression-based methods is a promising direction for further research.

Preliminaries

Let A be an alphabet, and denote A* the set of tuples [formula]. In this work we consider the case [formula]; extensions to the multidimensional case, as well as to more general spaces, are straightforward. Distributions, or (stochastic) processes, are measures on the space (A∞,FA∞), where FA∞ is the Borel sigma-algebra of A∞. When talking about joint distributions of N samples, we mean distributions on the space ((AN)∞,F(AN)∞).

For each [formula], let Bk,l be the partition of the set Ak into k-dimensional cubes with volume hkl = (1 / l)k (the cubes start at 0). Moreover, define [formula] and [formula]. The set [formula] generates the Borel σ-algebra on [formula]. For a set B∈B let |B| be the index k of the set Bk that B comes from: |B| = k:B∈Bk.

We use the abbreviation X1..k for [formula]. For a sequence [formula] and a set B∈B denote [formula] the frequency with which the sequence [formula] falls in the set B.

[formula]

A process ρ is stationary if ρ(X1..|B| = B) = ρ(Xt..t + |B| - 1 = B) for any B∈A* and [formula]. We further abbreviate ρ(B): = ρ(X1..|B| = B). A stationary process ρ is called (stationary) ergodic if the frequency of occurrence of each word B in a sequence [formula] generated by ρ tends to its a priori (or limiting) probability a.s.: ρ( lim n  →    ∞ν(X1..n,B) = ρ(B)) = 1. Denote E the set of all stationary ergodic processes.

The distributional distance is defined for a pair of processes ρ1,ρ2 as follows (e.g. [\cite=Gray:88])

[formula]

where wj = 2- j.

(The weights in the definition are fixed for the sake of concreteness only; we could take any other summable sequence of positive weights instead.) In words, we are taking a sum over a series of partitions into cubes of decreasing volume (indexed by l) of all sets Ak, [formula], and count the differences in probabilities of all cubes in all these partitions. These differences in probabilities are weighted: smaller weights are given to larger k and finer partitions. It is easy to see that d is a metric. We refer to [\cite=Gray:88] for more information on this metric and its properties.

The clustering algorithms presented below are based on empirical estimates of the distance d:

[formula]

where [formula], ρ∈S, Xi1..ni∈Ani.

Although the expression ([\ref=eq:emd]) involves taking three infinite sums, it will be shown below that it can be easily calculated.

Let ρ1,ρ2∈E and let two samples [formula] and [formula] be generated by a distribution ρ such that the marginal distribution of Xi1..n1 is ρi, i = 1,2, and the joint distribution ρ is stationary ergodic. Then

[formula]

The idea of the proof is simple: for each set B∈B, the frequency with which the sample [formula] falls into B converges to the probability ρ1(B), and analogously for the second sample. When the sample sizes grow, there will be more and more sets B∈B whose frequencies have already converged to the probabilities, so that the cumulative weight of those sets whose frequencies have not converged yet, will tend to 0.

For any ε > 0 we can find an index J such that [formula]. Moreover, for each m,l we can find such elements [formula], for some [formula], of the partition Bm,l that [formula]. For each Bm,lj, where m,l  ≤  J and j  ≤  tm,l, we have [formula] a.s., so that

[formula]

for all n1  ≥  u, for some [formula]; define Um,lj: = u. Let U: =  max m,l  ≤  J,j  ≤  tm,lUm,lj (U depends on the realization [formula]). Define analogously V for the sequence [formula]. Thus for n1 > U and n2 > V we have

[formula]

which proves the statement.

Main results

The clustering problem can be defined as follows. We are given N samples [formula], where each sample [formula] is a string of length ni of symbols from A: [formula]. Each sample is generated by one out of k different unknown stationary ergodic distributions [formula]. Thus, there is a partitioning [formula] of the set {1..N} into k disjoint subsets Ij,j = 1..k

[formula]

such that [formula], 1  ≤  j  ≤  N is generated by ρj if and only if j∈Ij. The partitioning I is called the target clustering and the sets Ii,1  ≤  i  ≤  k, are called the target clusters. Given samples [formula] and a target clustering I, let [formula] denote the cluster that contains [formula].

A clustering function F takes a finite number of samples [formula] and an optional parameter k (the target number of clusters) and outputs a partition [formula] of the set {1..N}.

Let a finite number N of samples be given, and let the target clustering partition be I. Define [formula]. A clustering function F is strongly asymptotically consistent if [formula] from some n on with probability 1. A clustering function is weakly asymptotically consistent if [formula]

Note that the consistency is asymptotic with respect to the minimal length of the sample, and not with respect to the number of samples.

Known number of clusters

Algorithm [\ref=alg:1] is a simple clustering algorithm, which, given the number k of clusters, will be shown to be consistent under most general assumptions. It works as follows. The point [formula] is assigned to the first cluster. Next, find the point that is farthest away from [formula] in the empirical distributional distance [formula], and assign this point to the second cluster. For each j = 3..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign it to the cluster j. Thus we have one point in each of the k clusters. Next simply assign each of the remaining points to the cluster that contains the closest points from those k already assigned. One can notice that Algorithm [\ref=alg:1] is just one iteration of the k-means algorithm, with so-called farthest-point initialization [\cite=Katsavounidis:94], and a specially designed distance.

For two samples [formula] and [formula] the computational complexity (time and space) of calculating the empirical distributional distance [formula] ([\ref=eq:emd]) is O(n2 log s- 1min), where n =  max (n1,n2) and

[formula]

First, observe that for fixed m and l, the sum

[formula]

has not more than n1 + n2 - 2m + 2 non-zero terms (assuming m  ≤  n1,n2; the other case is obvious). Indeed, for each i = 0,1, in the sample [formula] there are ni - m + 1 tuples of size k: [formula]. Therefore, the complexity of calculating Tm,l is O(n1 + n2 - 2m + 2) = O(n). Furthermore, observe that for each m, for all l >  log s- 1min the term Tm,l is constant. Therefore, it is enough to calculate [formula], since for fixed m (that is, we double the weight of the last non-zero term). Thus, the complexity of calculating [formula] is O(n log s- 1min). Finally, for all m > n we have Tm,l = 0. Since [formula], the statement is proven.

Let [formula] and suppose that the samples [formula] are generated in such a way that the joint distribution is stationary ergodic. If the correct number of clusters k is known, then Algorithm [\ref=alg:1] is strongly asymptotically consistent. Algorithm [\ref=alg:1] makes O(kN) calculations of (  ·  ,  ·  ), so that its computational complexity is O(kNn2max log s- 1min), where nmax  =   max ki = 1ni and

[formula]

Observe that the samples are not required to be generated independently. The only requirement on the distribution of samples is that the joint distribution is stationary ergodic. This is perhaps one of the mildest possible probabilistic assumptions.

By Lemma [\ref=th:dd], [formula], i,j∈{1..N} converges to 0 if and only if [formula] and [formula] are in the same cluster. Since there are only finitely many samples [formula], there exists some δ > 0 such that, from some n on, we will have [formula] if [formula] belong to the same target cluster ([formula]), and [formula] otherwise ([formula]). Therefore, from some n on, for every j  ≤  k we will have [formula] and the sample [formula], where [formula], will be selected from a target cluster that does not contain any [formula], i < j. The consistency statement follows.

Next, let us find how many pairwise distance estimates [formula] the algorithm has to make. On the first iteration of the loop, it has to calculate [formula] for all i = 1..N. On the second iteration, it needs again [formula] for all i = 1..N, which are already calculated, and also [formula] for all i = 1..N, and so on: on jth iteration of the loop we need to calculate [formula], i = 1..N, which gives at most kN pairwise distance calculations in total. The statement about computational complexity follows from this and Proposition [\ref=th:cd]: indeed, apart from the calculation of [formula], the rest of the computations is of order O(kN).

Complexity-precision trade-off. The bound on the computational complexity of Algorithm [\ref=alg:1], given in Theorem [\ref=th:cons], is given for the case of precisely calculated distance estimates (  ·  ,  ·  ). However, precise estimates are not needed if we only want to have an asymptotically consistent algorithm. Indeed, following the proof of Lemma [\ref=th:dd], it is easy to check that if we replace in ([\ref=eq:emd]) the infinite sums with sums over any number of terms mn, ln that grows to infinity with n =  min (n1,n2), and if we replace partitions Bm,l by their (finite) subsets Bm,l,n which increase to Bm,l, then we still have a consistent estimate of d(  ·  ,  ·  ).

Let mn,ln be some sequences of numbers, Bm,l,n  ⊂  Bm,l for all [formula], and denote n: =  min {n1,n2}. Define

[formula]

-100

Assume the conditions of Lemma [\ref=th:dd]. Let ln and mn be any sequences of integers that go to infinity with n, and let, for each [formula], the sets Bm,l,n, [formula] be an increasing sequence of subsets of Bm,l, such that [formula]. Then

[formula]

It is enough to observe that

[formula]

and then follow the proof of Lemma [\ref=th:dd].

If we use the estimate [formula] in Algorithm [\ref=alg:1] (instead of (  ·  ,  ·  )), then we still get an asymptotically consistent clustering function. Thus the following statement holds true.

Assume the conditions of Theorem [\ref=th:cons]. For all sequences mn,ln of numbers that increase to infinity with n, there is a strongly asymptotically consistent clustering algorithm, whose computational complexity is O(kNnmaxmnmaxlnmax).

On the one hand, Proposition [\ref=th:as2] can be thought of as an artifact of the asymptotic definition of consistency; on the other hand, in practice precise calculation of (  ·  ,  ·  ) is hardly necessary. What we get from Proposition [\ref=th:as2] is the possibility to select the appropriate trade-off between the computational burden, and the precision of clustering before asymptotic.

Note that the bound in Proposition [\ref=th:as2] does not involve the sizes of the sets Bm,l,n; in particular, one can take Bm,l,n = Bm,l for all n. This is because, for every two samples X11..n and X21..n, this sum has no more than 2n non-zero terms, whatever are m,l. However, in the following section, where we are after clustering with an unknown number of clusters k, and thus after controlled rates of convergence, the sizes of the sets Bm,l,n will appear in the bounds.

Unknown number of clusters

So far we have shown that when the number of clusters is known in advance, consistent clustering is possible under the only assumption that the joint distribution of the samples is stationary ergodic. However, under this assumption, in general, consistent clustering with unknown number of clusters is impossible. Indeed, as was shown in [\cite=Ryabko:10discr], when we have only two binary-valued samples, generated independently by two stationary ergodic distributions, it is impossible to decide whether they have been generated by the same or by different distributions, even in the sense of weak asymptotic consistency (this holds even if the distributions come from a smaller class: the set of all B-processes). Therefore, if the number of clusters is unknown, we have to settle for less, which means that we have to make stronger assumptions on the data. What we need is known rates of convergence of frequencies to their expectations. Such rates are provided by assumptions on the mixing rates of the distribution generating the data. Here we will show that under rather mild assumptions on the mixing rates (and, again, without any modeling assumptions or assumptions of independence), consistent clustering is possible when the number of clusters is unknown.

In this section we assume that all the samples are

[formula]

γ(δ)=(2e+11(1+4/δ)qα),

[formula]

m l b(e+δqα)=o(1),

[formula]

Y:= I -ρ(X∈ B).

[formula]

ρ(|ν(X,B)-ν(X,B)|>ε)≤ 2γ(ε).

[formula]

Conclusion

We have proposed a framework for defining consistency of clustering algorithms, when the data comes as a set of samples drawn from stationary processes. The main advantage of this framework is its generality: no assumptions have to be made on the distribution of the data, beyond stationarity and ergodicity. The proposed notion of consistency is so simple and natural, that it may be suggested to be used as a basic sanity-check for all clustering algorithms that are used on sequence-like data. For example, it is easy to see that the k-means algorithm will be consistent with some initializations (e.g. with the one used in Algorithm [\ref=alg:1]) but not with others (e.g. not with the random one).

While the algorithms that we presented to demonstrate the existence of consistent clustering methods are computationally efficient and easy to implement, the main value of the established results is theoretical. As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of [\cite=BRyabko:06a] [\cite=Cilibrasi:05] [\cite=BRyabko:09].

Another direction for future research concerns optimal bounds on the speed of convergence: while we show that such bounds can be obtained (of course, only in the case of known mixing rates), finding practical and tight bounds, for different notions of mixing rates, remains open.

Finally, here we have only considered the setting in which the number N of samples is fixed, while the asymptotic is with respect to the lengths of the samples. For on-line clustering problems, it would be interesting to consider the formulation where both N and the lengths of the samples grow.