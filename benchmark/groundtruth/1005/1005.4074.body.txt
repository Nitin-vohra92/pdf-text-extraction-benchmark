=1 0.0in

Lemma Proposition Corollary

Definition

Remark

Example

Assumption

Synchronization of coupled limit cycles

Introduction

Synchronization of coupled oscillators has been studied extensively due to its importance in diverse problems in science and engineering [\cite=Blekhman] [\cite=mmp] [\cite=pr01] [\cite=STR03]. Technological applications of synchronization include coordination of activity in power, sensor, and communication networks [\cite=DB10]; and control of the groups of mobile agents [\cite=Olfati07] [\cite=Ren07]. Many experimental systems exhibit synchrony: electrical circuits [\cite=AVR86], coupled lasers [\cite=RT94], and Josephson junctions [\cite=WCS98], to name a few. In population dynamics, the theory of synchronization is used to study collective dynamics [\cite=balerini] [\cite=sumpter] [\cite=STR03]. Synchronization plays a prominent role in physiology and in neurophysiology, in particular. It is important for information processing in the brain [\cite=Sin93], attention, arousal [\cite=UCS99], and regulation of the sleep-wake cycles [\cite=GR]. In addition, synchronization underlies several common neurodegenerative pathologies such as epilepsy [\cite=TWB] and Parkinson's Disease [\cite=LHL]. This list can be continued.

A large body of mathematical and physical literature is devoted to different aspects of synchronization in differential equation models. For weakly coupled systems, very effective techniques have been developed [\cite=KU75] [\cite=KE88] [\cite=EK91] [\cite=BMH] [\cite=CK00] [\cite=GH07] [\cite=HI97] [\cite=LR03] [\cite=PJ04] (see also Chapter 10 in [\cite=IZH07] for a survey of available methods). Synchronization in networks with symmetries has been studied in [\cite=SGP03] (see also [\cite=GS06] and references therein). Chaotic synchronization has been a subject of intense research [\cite=AVR86] [\cite=ACH97] [\cite=BR97] [\cite=BR97a] [\cite=FY83] [\cite=Jos00] [\cite=PC98]. For strongly coupled systems, a number of studies explain synchronization in specific physical and biological models [\cite=AVR86] [\cite=BH84] [\cite=Coo09] [\cite=GH07] [\cite=MK] [\cite=steur09] [\cite=SR86] and for certain canonical network topologies such as nearest-neighbor coupling on a lattice [\cite=ACH97] [\cite=hale97].

Networks arising in applications feature a rich variety of oscillatory mechanisms and coupling architectures. Therefore, analytical results elucidating synchronization in systems under general assumptions on the local dynamics and coupling operators are important. For systems with strong coupling, such results are rare. The exception is the work by V. Belykh, I. Belykh, and Hasler [\cite=BBH04] [\cite=BBH06], where sufficient conditions for synchronization are given in terms of the properties of the graph of the network. The pioneering work of Afraimovich, Verichev, and Rabinovich [\cite=AVR86] already identified dissipation produced by the coupling operator as a principal ingredient in a common mechanism of synchronization. Dissipativity of the coupling was shown to be responsible for synchronization in many systems of coupled differential equations [\cite=AVR86] [\cite=ACH97] [\cite=hale97] [\cite=steur09]. It is also relevant to stability of spatially homogeneous states in reaction-diffusion systems [\cite=hale_infty]. However, the dissipation produced by the coupling alone is often not sufficient for synchronization. The analysis of forced Duffing oscillators coupled through position variables in [\cite=hale97] shows that the intrinsic dissipativity of the individual (local) subsystems is equally important. In fact, synchronization is achieved by the interplay of the dissipativity of the coupling and the intrinsic properties of the local systems. The analysis in [\cite=hale97] describes an important mechanism of synchronization using the Lyapunov functions that are specific to the Duffing systems coupled via a discrete Laplacian. The goal of this paper is to study this mechanism under general assumptions on the local oscillatory dynamics and for a broad class of coupling schemes. Through a careful analysis of the variational equation of the coupled system, we derive a sufficient condition for synchronization in terms of the geometric properties of the local limit cycles and the coupling operator. To achieve this, in the vicinity of the periodic solution of the coupled system, we construct a moving frame of reference. After a series of coordinate transformations, we arrive at a system of equations that reveals the interplay of coupling and local dynamics, and shows their combined contribution to the stability of the synchronous solution of the coupled system.

The key step in this analysis is finding a suitable transformation for the coupling operator in the moving coordinates. As a by-product, we develop a rigorous reduction of the coupled system to the system of equations for the phase variables. In approximate form, this system of phase equations was obtained in [\cite=medvedev10]. Similar to Kuramoto's phase reduction for weakly coupled systems, we expect that these phase equations will be useful in studies of physical and biological coupled oscillator models in the strong coupling regime. For analytical convenience and because the coupled limit cycle oscillators are common in applications, in this paper we consider local systems whose dynamics are generated by limit cycles. Synchronization of chaotic systems as considered in [\cite=AVR86] [\cite=ACH97] [\cite=hale97] is admittedly more appealing and physically less intuitive than synchronization of coupled limit cycles. However, from an analytical point of view, the latter problem contains many of the ingredients responsible for synchronization of systems with more complex dynamics. For a discussion of how the results of the present study can be extended to chaotic synchronization and for related extensions for systems with time-dependent and nonlinear coupling schemes and randomly perturbed local systems, we refer the interested reader to [\cite=medvedev10].

The paper proceeds as follows. In Section [\ref=section.assumptions] we list our assumptions and state the main result. Section [\ref=example] explains the assumptions made in the previous section in the context of three examples. This is followed by the proof of the main theorem in Section [\ref=proof].

Assumptions and the main result

The local dynamics

We start by discussing the assumptions on a local system:

[formula]

where [formula] is continuous together with partial derivatives up to second order. We assume that [formula] is a periodic solution of ([\ref=local]) of period 1 with a nonvanishing time derivative [formula] Denote the corresponding periodic orbit [formula]. Near O, one can introduce an orthonormal moving coordinate frame (cf. Theorem VI.1.1, [\cite=hale_odes]):

[formula]

The change of variables

[formula]

in a sufficiently small neighborhood of O, defines a smooth transformation [formula] [\cite=hale_odes].

In new coordinates ([\ref=localvar]), near O local system ([\ref=local]) has the following form

[formula]

where

[formula]

where [formula] stands for symmetric part of matrix [formula], [formula].

Notational convention. To simplify notation, in the calculations through out this paper, we will often suppress θ and [formula] as arguments of [formula] etc, when the expression of the argument is clear from the context. We continue to denote the differentiation with respect to t and θ by dot and prime respectively.

Proof.  The proof follows the lines of the proof of Theorem VI.1.1 [\cite=hale_odes]. By plugging ([\ref=localvar]) into ([\ref=local]), we have

[formula]

where

[formula]

By multiplying both sides of ([\ref=localvarinloc]) by [formula], we obtain

[formula]

For small [formula], ([\ref=thetadot]) can be rewritten as

[formula]

By differentiating both sides of [formula], we have

[formula]

By plugging in ([\ref=byparts]) into ([\ref=thetainterm]), we arrive at ([\ref=ltheta]). Next, we multiply ([\ref=localvarinloc]) by [formula], and using [formula] we derive ([\ref=locrho]). [formula]

Next we formulate our assumption on the stability of the local limit cycle. Assumption [\ref=localcycle] implies exponential stability of the trivial solution of the linear periodic system

[formula]

For convenience of the future reference, we formulate this statement as a lemma.

Suppose [formula] is a continuous periodic matrix of period 1. Then ([\ref=unistable]) implies that for any 0 < ε  <  μ,

[formula]

where C1 > 0 and R(t) stands for the principal matrix solution of ([\ref=persyst]) [\cite=hale_odes].

Proof.  Let

[formula]

By periodicity of μ1(θ) and ([\ref=unistable]),

[formula]

Further,

[formula]

and, by Gronwall's inequality,

[formula]

This in turn implies that for large t  ≫  1 and arbitrary 0 < ε <  - μ

[formula]

On the other hand, by the Floquet theorem,

[formula]

where P(t) and [formula] are periodic and constant matrices respectively. By comparing ([\ref=asympt]) and ([\ref=Floc]), we conclude that all eigenvalues of [formula] must have negative real parts. This yields ([\ref=estfund]). [formula]

The coupling operator

By the coupled system, we call a collection of N local dynamical systems ([\ref=local]) interacting via a linear coupling operator [formula]

[formula]

where [formula] and g  ≥  0 is a parameter controlling the strength of interactions. In this paper, we consider separable schemes [\cite=medvedev10]:

[formula]

where [formula], [formula], and [formula] stands for the Kronecker product [\cite=harville]. From the modeling point of view, separable coupling is natural as it reflects two levels of the network organization. The global network architecture of interconnections between local systems is captured by [formula]. Matrix [formula] reflects the organization of the coupling at the level of a local system: it shows what combination of local variables participates in the coupling. The separable structure of the coupling translates naturally into the stability analysis of the synchronous solution, revealing what features of the global and local organization of the coupling are important for stability. Next we specify the structure of the separable coupling operator D (cf. ([\ref=separable])). There are two (sets of) assumptions. The first simpler assumption ensures that the coupled system admits a synchronous solution. The second set of assumptions guarantees the stability. As far as existence is concerned, we need to postulate that [formula]. We further assume that [formula] is one-dimensional to limit our study to connected networks. Thus, we assume

[formula]

Next, we turn to assumptions that ensure stability of the synchronous solution. To this end, we define an (N - 1)  ×  N matrix

[formula]

In the stability analysis of the synchronous solution, we will use matrix [formula] defined by the following relation

[formula]

For any [formula], [formula] is well-defined (cf. [\cite=medvedev10b], see also Appendix in [\cite=medvedev09]). The spectrum of [formula] is important for synchronization. This motivates the following definition.

Finally, we state our assumptions on matrix [formula] describing the local organization of the coupling . The following definition distinguishes two important cases that come up in the stability analysis of the synchronous solutions of ([\ref=coupled]) and ([\ref=separable]).

In a slightly different form, the full and the partial coupling schemes were introduced in [\cite=hale97].

Dissipative matrices yield synchronization when the coupling is full and sufficiently strong. Synchronization in the partially coupled systems requires an additional hypothesis.

Assumption [\ref=assumptions] captures the geometric properties of the local limit cycle through matrix [formula] (cf. ([\ref=B1s])), which depends on the basis functions [formula] defined in the vicinity of the limit cycle and the Jacobian [formula] evaluated along the periodic orbit and the properties of the coupling operator through the basis for the kernel of [formula], [formula], entering the definition of [formula] (cf. ([\ref=matrixG])). Condition ([\ref=intlambda]), therefore, reflects the interplay of the intrinsic properties of the limit cycle and the coupling operator and their contribution to the stability of the periodic solution of the coupled system. Having reviewed the assumptions, we state the main result of this paper.

Suppose a periodic solution of local system ([\ref=local]) is stable in the sense of Assumption [\ref=localcycle] (cf. ([\ref=unistable])) and the coupling operator in ([\ref=coupled]) is separable (cf. ([\ref=separable])) and such that [formula] and [formula] is positive semidefinite. If the coupling is partial, in addition, we assume that ([\ref=intlambda]) holds. Then for sufficiently large g > 0, the coupled system ([\ref=coupled]) has an exponentially stable synchronous limit cycle.

Discussion and examples

We precede the proof of Theorem [\ref=thm.full] with a discussion of the assumptions and the implications of the theorem. We illustrate our techniques with three examples. In the first example, we use a planar vector for which Assumption [\ref=assumptions] can be checked by a simple explicit calculation. The second example is used to explain the assumption that the coupling operator [formula] is dissipative and to elucidate the range of networks covered by this assumption. The third example is meant to illuminate the distinction between the full and partial coupling schemes in the context of a biophysical model of a neuron, and to show how one verifies the assumptions of the Theorem [\ref=thm.full] in practice.

Coupled radially symmetric oscillators

Consider radially symmetric local vector field:

[formula]

For coupled local systems ([\ref=rad]) Theorem [\ref=thm.full] yields the following sufficient condition for synchronization.

Coupled system ([\ref=coupled]) with separable coupling ([\ref=separable]) and local vector field ([\ref=rad]) has an exponentially stable synchronous limit cycle if  [formula] and nonzero matrix [formula] is positive semidefinite.

Proof.  If [formula] is positive definite then Corrolary [\ref=uni] follows from Theorem [\ref=thm.full]. Suppose that [formula] has a 1D kernel spanned by

[formula]

Let

[formula]

Then

[formula]

Further, [formula] and

[formula]

and

[formula]

[formula]

Let [formula] be as in ([\ref=rad]) and consider

[formula]

In this example,

[formula]

is positive semidefinite. Suppose hij = hji  ≥  0 and denote

[formula]

If [formula] then [formula], by Gershgorin's Theorem. In general, [formula] does not have to be symmetric. For a more complete description of dissipative matrix we refer the reader to [\cite=medvedev10b] (see also Theorem [\ref=QLthm] below).

The consensus protocol

To elucidate what features of the coupling are important for synchronization we choose consensus protocols, a framework used for modeling coordination in the groups of dynamic agents [\cite=Ren07]. The continuous time variant for this problem deals with N agents whose states are given by x(i)(t), [formula] and are governed by the following system of differential equations:

[formula]

Here, weights dij,i  ≠  j (which for simplicity we take constant) describe the interactions between two distinct agents x(i) and x(j). After setting [formula], we rewrite ([\ref=ConsEqn]) in a vector form

[formula]

The problem data can be conveniently represented by a weighted graph G = ({dij}), where vertex set [formula] lists all agents, the pairs of interacting agents are recorded in the edge set E, and the weights {dij} quantify the intensity of interactions. If G is connected [formula]. Both positive and negative weights {dij} (corresponding to synergistic and antagonistic interactions respectively) are admissible. Likewise, the interactions do not have to be symmetric, i.e., dij may differ from dji. In designing consensus protocols, one is interested in weighted graphs yielding convergence to spatially homogeneous state

[formula]

The following questions related to ([\ref=ConsSys]) are important in applications: determining the rate of convergence in ([\ref=ConsAsympt]) and relating it to the network topology and weight distribution [\cite=Boyd03] [\cite=Leonard10], finding an optimal weight distribution yielding ([\ref=ConsSys]) a desired property such as the fastest convergence (under certain constraints) [\cite=Boyd03] [\cite=Boyd06] or minimal effective resistance [\cite=Boyd08], or robustness to noise [\cite=Leonard10], to name a few. For studying these questions it is important to describe all weighted graphs that endow ([\ref=ConsAsympt]) with synchronous dynamics. For the discrete time counterpart of ([\ref=ConsEqn]), this question was answered in [\cite=Boyd03]. The following theorem describes all such graphs for the continuous time problem ([\ref=ConsEqn]).

Let x(t) be a solution of the initial value problem ([\ref=ConsSys]) with initial condition [formula]. Then ([\ref=ConsAsympt]) holds for any [formula] iff D∈D.

Proof.  By multiplying both sides of ([\ref=ConsSys]) by [formula] (cf. ([\ref=defineS])), we have

[formula]

The equilibrium of ([\ref=Eqnfory]) is asymptotically stable iff the symmetric part of [formula] is negative definite, i.e., when D∈D. [formula]

Therefore, dissipative coupling matrices are precisely those that enforce synchrony in ([\ref=ConsSys]). Remarkably, dissipative matrices admit an explicit characterization.

[\cite=medvedev10] [formula] iff

[formula]

for some [formula] with negative definite symmetric part and

[formula]

Theorem [\ref=QLthm] gives a convenient computational formula for [formula]:

[formula]

This formula can be used for studying the rate of convergence of solutions of ([\ref=ConsEqn]) to the homogeneous state for different network topologies. For some canonical network architectures, including nearest neighbor and all-to-all coupling schemes, the rate of convergence (i.e., the largest eigenvalue of [formula]) can be found analytically for other, such as networks with random connection weights, the rates can be computed numerically using ([\ref=compute]). We refer an interested reader to [\cite=medvedev10] [\cite=medvedev10b], where these examples are discussed in detail.

Theorem [\ref=thm.full] extends the argument used in Theorem [\ref=consensus] to networks whose local systems have multidimensional phase space and nontrivial dynamics. In the analysis of the general case, in addition to the global network architecture (i.e. D∈D), the coupling organization on the level of the local systems becomes important. For instance, it matters what local variables and in what manner are engaged in the coupling. This information is contained in matrix [formula] (cf. ([\ref=separable])). The analysis of the separable coupling schemes highlights the importance of the full versus partial coupling distinction. In the latter case, synchronization depends on the combination of the properties of the local dynamics and the coupling operator (cf. Assumption [\ref=assumptions]). The example in the following subsection is chosen to provide a reader with an intuition for the mechanisms of synchronization in fully and partially coupled systems.

The compartmental model

The following systems of differential equations is a nondimensional model of the dopamine neuron (cf. [\cite=MC04]):

[formula]

Here, v(i) and u(i) approximate membrane potential and calcium concentration in Compartment i of an axon or a dendrite of a neuron. Equations ([\ref=da1]) and ([\ref=da2]) describe the dynamics in each compartment using Hodgkin-Huxley formalism (see [\cite=DA] for more background on compartmental models). The nonlinear functions g1(v) and g2(u) and the values of the parameters appearing on the right hand sides of ([\ref=da1]) and ([\ref=da2]) are given in the appendix to this paper. Equations ([\ref=da1]) and ([\ref=da2]) reflect the contribution of the principal ionic currents (calcium and calcium dependent potassium currents) to the dynamics of v(i) and u(i). In addition, the coupling terms

[formula]

model the electrical current and calcium diffusion between the adjacent compartments. The off-diagonal entry [formula] of matrix [formula] corresponds to the conductance between Compartments i and j. The structure of the coupling matrix [formula], i.e., the pattern in which nonzero entries appear in [formula], reflects the geometry of the neuron. In the simplest case of a uniform linear cable with no-flux boundary conditions (see Fig. [\ref=f.1]a), the coupling matrix [formula] (cf. ([\ref=Lambda0])). [formula] may have a more interesting structure, e.g., in the models of dendrites with more complex spatial geometry (see Fig. [\ref=f.1]b). As follows from ([\ref=da1]) and ([\ref=da2]) the coupling is separable with

[formula]

where δ1 = g- 1δ. If δ > 0 the coupling is full. If calcium diffusion is ignored (δ = 0) the coupling becomes partial. Below, we discuss the assumptions of Theorem [\ref=thm.full] in relation to the model at hand.

We start with the conditions on the coupling matrix [formula]. To be specific, we assume the nearest-neighbor coupling (see Fig. [\ref=f.1]a), i.e., [formula]. Then

[formula]

Clearly, [formula] because [formula] is (symmetric) negative definite. The conditions on [formula] for both full and partial coupling cases obviously hold. As is typical for conductance-based models of neurons, away from the singular limit ε  →  0, the analytical estimates on the eigenvalues of the variational equations such as in Assumptions [\ref=localcycle] and [\ref=assumptions], may be difficult to derive. We verify ([\ref=unistable]) and ([\ref=intlambda]) numerically, after we briefly review basic numerics for ([\ref=da1]) and ([\ref=da2]). Fig. [\ref=f.2]a shows the limit cycle of a single uncoupled oscillator in ([\ref=da1]) and ([\ref=da2]). The corresponding time series are shown in Fig. [\ref=f.2]b. In Fig. [\ref=f.2]c, we plot μ1(θ), the largest eigenvalue of [formula]. In this example [formula] is scalar. Note that over one cycle of oscillations μ1(θ) takes both positive and negative values. However, since the integral over one cycle of oscillations [formula] is negative (Fig. [\ref=f.2]c), the limit cycle is exponentially stable (cf. Lemma [\ref=expstab]). Therefore, for the fully coupled variant of ([\ref=da1]) and ([\ref=da2]) all conditions of Theorem [\ref=thm.full] hold. Note that in the full coupling case, synchronization is determined solely by the properties of the coupling operator. The only information about the local system which we use is the exponential stability of the limit cycle. Conditions for synchronization in partially coupled systems use the information about the local dynamics (through matrix [formula] (cf. ([\ref=B1s]))) and about the coupling operator (through [formula]). The numerical verification of ([\ref=intlambda]) for partially coupled variant of ([\ref=da1]) and ([\ref=da2]) is given in Fig. [\ref=f.2]d.

The proof of Theorem [\ref=thm.full]

The proof proceeds as follows. In §[\ref=coordinates], we construct a suitable system of coordinates near the periodic orbit of the coupled system. In §[\ref=linearized], we analyze the linear part of the variational equation. In §[\ref=endgame] we extend the stability analysis to the full system.

The local coordinates for the coupled system near the limit cycle

The moving coordinates for the coupled system are obtained by combining the coordinates ([\ref=localvar]) for local systems

[formula]

where [formula]. By following the steps of the proof of Lemma [\ref=lem.polar], for each local system we have

[formula]

where CT stands for the coupling terms

[formula]

In the moving coordinate frame, the coupling operator becomes nonlinear. We linearize it using the Taylor's formula. It will be convenient to have Taylor's coefficients appearing in the expansions for local systems evaluated at a certain common value. For this purpose, we use the average phase defined by:

[formula]

The existence of [formula] with the properties specified in ([\ref=vartheta]) follows from the following considerations. First, because [formula], there is nonzero [formula]. ξ can be chosen to satisfy the second condition in ([\ref=vartheta]) provided [formula]. Supose the contrary. Then,

[formula]

Note that ξ and η are linearly independent. Since [formula],

[formula]

Therefore, either the geometric multiplicity of the zero eigenvalue of [formula] is greater or equal to 2, or the size of the block corresponding zero eigenvalue in the Jordan normal form of [formula] is greater or equal to 2. Either statement contradicts the assumption that [formula] (see Lemma 2.5 [\cite=medvedev10b]).

Similarly, we define

[formula]

From the definitions of ϑ and ϱ, we have

[formula]

where [formula], [formula], and [formula] is defined in ([\ref=defineS]).

Using ([\ref=deviation]), we represent

[formula]

By plugging ([\ref=taylor1]) and ([\ref=taylor2]) into ([\ref=CT]) and using ([\ref=deviation]), we have

[formula]

Using ([\ref=CT1]), we rewrite ([\ref=thetai]) and ([\ref=rhoi])

[formula]

where

[formula]

The averaged system is obtained by multiplying equations for θ(i) and ρ(i) by [formula] and adding them up:

[formula]

Using ϑ as an independent variable, we rewrite ([\ref=thetai-1]) and ([\ref=rhoi-1]) as follows

[formula]

or in matrix form for [formula]

[formula]

where [formula]. By multiplying both sides ([\ref=thetarhomatrix]) by [formula] and recalling [formula] and [formula], we obtain the system for [formula]:

[formula]

The final coordinate transformation in this series is used to make the matrix multiplied by [formula] in ([\ref=phir]) symmetric:

[formula]

Note,

[formula]

Using ([\ref=psiz]), ([\ref=psidot]), and ([\ref=zdot]) from ([\ref=phir]) we obtain

[formula]

where [formula], Q1(y,ϱ) = O(|ϱ|2,g|y|2,g|y||ϱ|) and

[formula]

and [formula]. We complement ([\ref=psiz]) by the equation for ϱ

[formula]

The linearized system

In this subsection, we study the linearization of ([\ref=finalform])

[formula]

In the following lemma, we prove that y = 0 is exponentially stable solution of ([\ref=systB]).

Let Φ(t) denote a fundamental matrix solution of ([\ref=systB]) and Φ(t,s): = Φ(t)Φ- 1(s). Then

[formula]

for certain positive constants C2 and λ.

The proof of Lemma [\ref=exp] follows from Lemmas [\ref=matrixM] and [\ref=perturbedeigenvalues], which we prove first.

The spectrum of symmetric matrix

[formula]

coincides with that of [formula]. In particular, for every [formula], [formula] is a positive semidefinite matrix whose rank is equal to [formula].

Proof.  This follows from

[formula]

where [formula] is an orthogonal matrix. [formula]

Let λ1(t) denote the largest eigenvalue of

[formula]

Then there exists g0 > 0 and λ̄ > 0 such that

[formula]

Proof.  Matrix [formula] is negative semidefinite for all t  ≥  0, because the eigenvalues of [formula] are negative and those of [formula] are nonnegative. Denote the (constant) eigenvalues of Bs0 by λ(0)k:

[formula]

For small δ > 0, the eigenvalues of Bs0  +  δBs1 perturb smoothly [\cite=gelfand]

[formula]

We consider the full rank coupling case first. If [formula] is full rank then so is [formula]. Denote

[formula]

Choose δ0 > 0 such that

[formula]

Then for g > g0: = δ- 10, the eigenvalues of Bs = gBs0 + Bs1 are negative and are bounded from zero by - gλ̃. This shows ([\ref=stableB]) for the full coupling case.

It remains to analyze the case [formula], i.e.,

[formula]

By ([\ref=pertEV]),

[formula]

For small δ > 0, we have

[formula]

Thus, to show ([\ref=stableB]) we need to verify

[formula]

For this, we review the construction of the correction terms λ(1)k(t) (cf. Appendix [\cite=gelfand]). Choose an orthonormal basis for ker Bs0(t) [formula]. Then λ(1)k(t) are the eigenvalues of G = (gij), gij = (Bs1(t)ηi(t),ηj(t)). Below we show that Assumption [\ref=assumptions] guarantees ([\ref=intmax]). To this end, we construct a basis for ker Bs1(t). Recall that [formula] stands for the orthonormal basis of [formula] and [formula]. We choose

[formula]

where [formula] [formula] and δji denotes the Kronecker delta. Vectors in ([\ref=chooseeta]) form an orthonormal basis of ker Bs1(t). Further,

[formula]

Thus, [formula] where [formula] is defined in ([\ref=matrixG]). The eigenvalues of G are those of [formula] taken with multiplicity (N - 1). Clearly, Assumption [\ref=assumptions] is equivalent to ([\ref=intmax]). This concludes the proof of the lemma. [formula]

Proof.  (Lemma [\ref=exp]) The statement of Lemma [\ref=exp] follows from Lemma [\ref=perturbedeigenvalues] and Lemma [\ref=expstab]. [formula]

The endgame

To complete the proof of stability of the synchronous solution, we study the initial value problem for ([\ref=finalform]) and ([\ref=dvarrho])

[formula]

where by abusing notation we denote x: = (y,ϱ)T and the independent variable by t. Matrices [formula] and B(t) are defined in ([\ref=Atheta]) and ([\ref=finalform]). The nonlinear terms are collected in Q(x): = (Q1(x),Q2(x)) = O(g|x|2). We suppress the dependence of Q on g, because once it is chosen sufficiently large g will be considered fixed. Let X(t) denote a principal matrix solution of the homogeneous system

[formula]

By Lemmas [\ref=expstab] and [\ref=exp], for X(t,s) = X(t)X- 1(s) we have

[formula]

for some C3 > 0 and 0 < κ  <   min {μ,λ} (cf. ([\ref=estfund]) and ([\ref=expbnd])).

Let g  ≥  g0 be fixed. Then for any sufficiently small ε > 0 (possibly depending on g) and any initial data

[formula]

solution of the initial value problem ([\ref=ivp]) and ([\ref=ic]) satisfies

[formula]

As in the famous theorem of Lyapunov on stability by the linear approximation, the statement of Lemma [\ref=success] follows from the stability of the linear part of ([\ref=ivp]) captured by ([\ref=kappa]). The proof of the lemma relies on a weaker statement, which we prove first.

Under the assumptions of Lemma [\ref=success], we have

[formula]

Proof.  Fix δ such that

[formula]

Because

[formula]

for sufficiently small ε > 0 we have

[formula]

Consider a functional sequence defined by

[formula]

We use induction to show that

[formula]

The induction hypothesis is verified, using ([\ref=kappa]), ([\ref=smallic]), ([\ref=choosedelta]), and ([\ref=LipQ]):

[formula]

Similarly, one shows that ([\ref=supxn]) for n = k implies ([\ref=supxn]) for n = k + 1. Thus, ([\ref=supxn]) holds for all natural n.

We complete the proof by showing that xn(t) uniformly converges the solution of ([\ref=ivp]) and ([\ref=ic]). To this end, we show

[formula]

Indeed, by subtracting ([\ref=next]) with n = k from ([\ref=next]) with n = k + 1 and using ([\ref=kappa]), ([\ref=LipQa]), and ([\ref=supxn]), we have

[formula]

Next, consider

[formula]

By ([\ref=geometric]), ([\ref=series]) is majorized by the geometric series [formula]. Therefore, {xn(t)} converges uniformly to x(t), the unique solution of ([\ref=ivp]), ([\ref=ic]). By ([\ref=supxn]), the latter (as the limit of {xn(t)}) is bounded by 2ε. This completes the proof. [formula]

Proof.  (Lemma [\ref=success]) We continue to use the notation introduced in the proof of Lemma [\ref=bounded]. In particular, positive ε and δ are as chosen above. By the variation of constants formula, we express the solution of ([\ref=ivp]) and ([\ref=ic]) as

[formula]

Using ([\ref=kappa]), ([\ref=smallic]), and ([\ref=LipQ]) from ([\ref=constants]) we have

[formula]

Rewrite ([\ref=preGronwall]) for y(t): = |x(t)| exp {κt}:

[formula]

By Gronwall's inequality,

[formula]

and, by recalling the definition of y(t) and ([\ref=choosedelta]), we finally derive

[formula]

[formula]

Acknowledgments. The author thanks Kresimir Josic for reading the manuscript and providing helpful comments. This work was done during sabbatical leave at Program of Applied and Computational Mathematics (PACM) at Princeton University. The author thanks PACM for hospitality.

Appendix. Parameter values for ([\ref=da1]) and ([\ref=da2])

The equations for the local systems in the neural network ([\ref=da1]) and ([\ref=da2]) are adopted from a nondimensional model of a dopamine neuron [\cite=MC04] (see also [\cite=medvedev10]). For biophysical background and details of nondimesionalization, we refer an interested reader to [\cite=MC04]. Terms on the right hand side of the voltage equation ([\ref=da1]) model ionic currents: a calcium current, a calcium dependent potassium current, and a small leak current. The equation for calcium concentration ([\ref=da2]) takes into account calcium current and calcium efflux due to calcium pump. The ionic conductances are sigmoid functions of the voltage and calcium concentration

[formula]

Constants 1,2,3 and E1,2,3 stand for maximal conductances and reversal potentials of the corresponding ionic currents; a1,2,3 are constants used in the descriptions of activation of calcium and calcium dependent potassium currents; ω and ε are certain constants that come up in the process of nondimesionalization of the conductance based model of a dopamine neuron (see [\cite=MC04] for details). The values of parameters used in the simulations shown in Figure [\ref=f.2] are summarized in the following table.

Table