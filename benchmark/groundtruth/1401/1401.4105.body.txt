Lemma Definition Proposition Corollary Example Algorithm

Remark

Learning [formula]-based analysis and synthesis sparsity priors using bi-level optimization

Introduction

Maximum a Posteriori (MAP) inference under the Bayesian framework is a popular method for solving various inverse problems in image processing. The MAP estimator is equivalent to an energy minimization problem, which consists of a data fidelity term and a signal prior term (also known as regularization term). Roughly speaking, the priors fall into two main prior types. One is the analysis-based prior and the other is the synthesis-based one.

Notation: In this paper our model presents a global prior over the entire image, in contrast to the common patch-based one. In order to distinguish between a patch and an image, we use the notation [formula] to indicate a patch (patch size: [formula], m is odd), and [formula] to indicate an image (image size:M  ×  N, with m  ≪  M,m  ≪  N). We refer [formula] and [formula] with m  ≤  n to the patch-based synthesis dictionary and analysis operator respectively. Furthermore, when the analysis operator A is applied to the entire image u, we use the common sliding-window fashion to compute the coefficients Ax for all MN patches in the 2-D image form of u. This result is equivalent to a multiplication of a sparse matrix [formula] and u, i.e., Au. We can group A to n separable sparse matrices [formula], where [formula] is associated with the ith row of A (Ai). If we consider Ai as a 2-D filter ([formula]), we have: Aiu is equivalent to the result of convolving image u with filter Ai. Finally, we use A that is expanded from the patch-based analysis operator A, to denote the global analysis operator associated with an entire image.

Patch based analysis and synthesis model: Under the framework of MAP, the patch-based analysis model is given as the following minimization problem

[formula]

where A is called analysis operator. The form of the penalty function φ depends on the prior utilized. For sparse representation, it can be [formula] or log (1 + |x|). The second type of prior is so-called synthesis prior. Basically, in the synthesis-based sparse representation model, a signal x is called sparse over a given dictionary D, when it can be approximated as a linear combination of a few atoms from dictionary D. This is formulated as following minimization problem using the MAP estimator. When we concentrate on the sparse prior, normally the penalty function φ is chose as [formula].

[formula]

Learning patch based analysis and synthesis prior: In order to pursue better performance, an intuitive possibility is to make a better choice for the analysis operator A and dictionary D based on training. Indeed, there exist several typical and successful training algorithms for over-complete dictionary learning: (i) the K-SVD algorithm [\cite=KSVDdenoising2006] [\cite=KSVD2006] (ii) On-line dictionary learning algorithm [\cite=MairalBPS09] (ii) efficient sparse coding algorithms [\cite=Lee2006]. However, compared to the extensive study for the training of the synthesis dictionary, the analysis operator learning problem has received relatively much less attention in the past decade, although the analysis model is the counterpart to the celebrated synthesis sparse model. But fortunately, it has been gaining more and more attention these two years. Consequently, there appear different algorithms for analysis operator learning [\cite=OphirSequentialLearning] [\cite=RubinsteinKSVD] [\cite=YahoobiAnalysisLearning] [\cite=YahoobiNoiseAware] [\cite=YahoobiConstrainedLearning] [\cite=HaweAnalysisLearning] [\cite=FadiliAnalysisLearning]. Among existing analysis operator learning algorithms, the learning approach proposed by Peyré and Fadili is very appealing since they consider this problem from a novel point of view. They interpret the action of analysis operator as convolution with some finite impulse response filters and they formulate the analysis operator learning task as a bi-level optimization problem [\cite=bileveloverview] which is solved using a gradient descent algorithm.

Contributions: Based on the investigation of existing dictionary and analysis operator learning algorithms, we find that (1) all the training approaches are based on patch priors; (2) the study of the later is immature since so far only few prior work has been tested with natural images [\cite=YahoobiNoiseAware] [\cite=YahoobiConstrainedLearning] [\cite=HaweAnalysisLearning]; and (3) most analysis operator learning algorithms have to impose some non-convex constraints on the operator A; this therefore makes the corresponding optimization problems relatively complex and difficult to solve. Thus three questions arise: (1) can we formulate the image-based model using the patch priors? (2) is it possible to formulate the analysis operator learning problem in a relatively easy way? (3) can we compare two types of priors under an unified framework? We give answers to these questions in this paper.

Analysis operator and dictionary learning via bi-level optimization

From patch-based model to image-based one: In this paper, we concentrate on convex [formula] sparse representation. In the case of analysis model, following the filter-based MRF model for image restoration, it is straightforward to extend the patch-based analysis model to the image-based one, which is given as:

[formula]

where A is the global analysis operator constructed from the local patch-based analysis operator A, u and f are images (M  ×  N). However, if we want to extend the patch-based synthesis model to the image-based one, we find it not as easy as the analysis case. Considering the common strategy that averages over-lapping patches, we can make explicit use of this strategy of patch-averaging to reconstruct the recovered image, then we arrive at our image-based synthesis model

[formula]

where the size of image f is M  ×  N, the patch size is [formula], matrix Rij is an m  ×  Np(Np  =  M  ×  N) matrix that extracts the (i,j) patch from the image, and αij is a n  ×  1 vector. We explicitly average all the over-lapping patches by a factor m, because [formula] (the number of patches is equal to the number of pixels using symmetrical boundary condition). Note that in our formulation, αij is not independent any more, in contrast to their independence in [\cite=KSVDdenoising2006]. If we stack all the αij and Rij to a huge column vector α and a huge matrix R respectively, and construct a huge diagonal-block matrix [formula] by using dictionary D, [\eqref=synthesis] can be rewritten as

[formula]

where [formula]. Now we can see the image-based model has the unified form with the patch-based one, which has a nice MAP interpretation. However, this formulation involves too many unknown variables (n  ×  Np), compared to Np unknown variables for the analysis model. This is a big drawback for our training scheme. we expect to formulate it by Np variables. Indeed, we succeed by considering its dual problem. We introduce a auxiliary variable u  =  Dα into the [formula] norm, and use v to denote the Lagrange multiplier, by using definition of the convex conjugate function to the [formula] norm [\cite=convexBoyd], we arrive at

[formula]

where the function δ(DTv) denotes the indicator function of the interval

[formula]

Learning experiments and application results for image denoising

We conducted our training experiments using the training images from the BSDS300[\cite=amfm_pami2011] image segmentation database. We used the whole 200 training images, and randomly sampled one 64  ×  64 patch from each training image, giving us a total of 200 training samples. We then generated the noisy versions by adding Gaussian noise with standard deviation σ  =  15. In our experiments, we learned an analysis operator [formula] and synthesis dictionary [formula] from the given training samples. In order to guarantee the property of mean-zero, each atom in A or D is expressed as the linear combination of the DCT-7 basis excluding the first filter with uniform entries.

After we learned an meaningful operator A and dictionary D, we applied them to the image denoising problem based on the same 68 test images used in [\cite=RothB09]. Tab. [\ref=tab:summary] presents the comparison of the average denoising results achieved by our [formula]-based analysis and synthesis model with (i) one state-of-the-art denoising method BM3D [\cite=BM3D] (ii) the K-SVD approach [\cite=KSVDdenoising2006] and (iii) the total variation (TV)-based ROF denoising model [\cite=pdpock]. We would like to point out that the TV based approach is the most commonly used [formula]-based analysis operator; the K-SVD approach is a synthesis sparse representation model based on [formula] optimization; BM3D is one current state-of-the-art denoising approach which is an image based, not generic prior based method, and is a specialized denoising algorithm. Fig. [\ref=scatterplots] presents a detailed comparison between our [formula]-based analysis model and our [formula]-based synthesis model along with three considered denoising methods over 68 test images for σ  =  25. A point above the line means better performance than our [formula]-based analysis model. (Due to space limits, we can not present this figure in a large scale. Please refer to the digital version for better visibility.)

Conclusions and future work

From Tab. [\ref=tab:summary] and Fig. [\ref=scatterplots] we can draw the following conclusions: (i) the [formula]-based analysis model is significantly superior to the [formula]-based synthesis model which is coherent with the findings in the work [\cite=EladAnalysisVSSynthesis]. We believe the essential reason lies in the ineffectual way the [formula]-based synthesis model characterizes the natural images, since it tries to model the noise signal, not the natural image itself as aforementioned. This inferiority also appeared in the training. (ii) our analysis model is comparable with the [formula]-based synthesis model K-SVD, as can be seen in Fig. [\ref=scatterplots]. Compared with specialized methods for image denoising task such as BM3D, our [formula]-based analysis model still can not compete. However, its denoising performance is always significantly better than the TV based approach.

It is well known that the probability density function (PDF) of the response of zero mean linear filters on natural images has heavily tailed distribution [\cite=Huang1999_Statistics]. Therefore, our future work will concentrate on non-convex penalty function such as [formula] or log (1 + |z|). According to our preliminary experience about the analysis model using log (1 + |z|) as penalty function, it clearly outperforms the [formula]-based synthesis model K-SVD, and has already been on par with BM3D. (We will present this result in our future work.) However, for the case of non-convex, since the Fenchel's duality we used in this paper is not available any more, how to handle the synthesis model becomes a problem. This will be the subject of our future work.