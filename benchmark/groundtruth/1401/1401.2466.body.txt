Quantifying the effects of local many-qubit errors and non-local two-qubit errors on the surface code

Many different approaches to achieving reliable quantum computation are under investigation [\cite=Naya08] [\cite=Bone12] [\cite=Gott13] [\cite=Bomb13] [\cite=Brav13b]. The current most practical known approach, the Kitaev surface code [\cite=Brav98] [\cite=Denn99], calls for a 2-D array of qubits with nearest neighbor interactions and a universal set of quantum gates with error rates below an approximate threshold of 1% [\cite=Raus07] [\cite=Raus07d] [\cite=Fowl12f]. Superconducting qubits with error rates at the surface code threshold now exist [\cite=Bare13].

There is extensive prior work showing the existence of a threshold error rate when arbitrary quantum error correction codes are subjected to a wide variety of noise models, including algebraically decaying two-body correlated noise [\cite=Ahar06], Gaussian non-Markovian noise [\cite=Ng09], and arbitrarily many-body correlated noise [\cite=Pres13]. In this work we focus on the simulated performance of the surface code below threshold.

To date, when the surface code has been simulated, quantum gates have only had the potential to introduce errors on the qubits they manipulated directly. In reality, manipulating any given qubit may disturb the state of a large number of surrounding qubits. Not all types of disturbance are particularly dangerous. Small random or systematic rotations of surrounding qubits lead only to independent random errors. Only correlated many-qubit errors deserve specific attention. This distinction is discussed in detail in Section [\ref=corr]. In this work we present a detailed study of precisely how well the surface code can handle this class of correlated errors.

Another important class of errors that has not received attention to date is those that would arise in an array of qubits interacting directly with one another via a polynomially decaying interaction such as the Coulomb or magnetic dipole interaction, or via a device coupling to many qubits. Pairs of qubits initially antiparallel can both flip without changing the energy of the total system. Two-qubit errors can therefore appear on widely separated qubits. We also present a detailed study of this class of correlated errors.

The discussion is organized as follows. In Section [\ref=corr], the meaning of independent and correlated errors is discussed in detail. In Section [\ref=sc], the surface code is briefly reviewed, our method of modeling local many-qubit errors is described, and simulation results of this case are presented. In Section [\ref=2q], our method of modeling non-local two-qubit errors is described, and simulation results presented. Section [\ref=conc] concludes.

Independent and correlated errors

Before presenting a study of correlated errors, it is worth discussing exactly what is and what is not a correlated error. For illustrative purposes, we center the discussion around a hypothetical quantum computer consisting of a 2-D array of mobile spins on a cooled substrate. A global magnetic field Bz sets the energy difference between |0〉 and |1〉. Local solenoids above and below the default location of each spin provide localized AC and DC fields to drive arbitrary single-qubit rotations. Pairs of spins are moved into close proximity to raise the strength of the magnetic dipole interaction and implement two-qubit entangling gates. For simplicity, we also imagine the solenoids can be used, when desired, as sensitive magnetic field detectors for qubit readout. See Fig. [\ref=arch]. This example maps well to architectures based on superconducting qubits [\cite=Bare13a], spin qubits [\cite=Holl06], and quantum dots [\cite=Loss98], and has features common to architectures based on ion traps [\cite=Kiel02], optical lattices [\cite=Bren99], and many others.

We now consider various error sources, and whether they are, or are not, correlated error sources. Firstly, we consider small fluctuations in the global magnetic field Bz, which will lead to small undesired systematic Z rotations on all qubits. At first glance, this may seem like the ultimate correlated error. However, provided the fluctuations are small and error detection is frequent, each individual small angle Z rotation will just look like a small probability of a Z error on each qubit. When performing error detection, most of the time no errors will be detected, as unwanted phase rotations will be removed by observation the majority of the time. This is a special case of the quantum Zeno effect [\cite=Misr77]. Any detected errors will appear random and independent. A global fluctuating field leads to a correlated probability of error p on all qubits, but the errors themselves will not be correlated, and the probability of errors from this noise source on any given pair of qubits will be p2. Note that it is critical that the fluctuations are small and error detection frequent -- for example if a global π rotation accumulates this will indeed lead to a global correlated error.

Secondly, consider crosstalk when driving a single-qubit gate. Under the assumption of widely separated spins and small solenoids, each solenoid will look like a magnetic dipole and the field seen by other spins will decay cubically with separation. The driving field will therefore induce cubically decaying small-angle rotations in all spins in the computer. For the same reason that small fluctuations in the global field Bz do not lead to correlated errors, this polynomially decaying crosstalk will also not lead to correlated errors. Provided the total error seen by any given qubit as a result of the sum of all crosstalk from all other actively manipulated qubits remains small, errors seen by the quantum error detection machinery will remain independent and sufficiently rare to be correctable.

Thirdly, consider the possibility that our hypothetical quantum computer is unshielded and located near an infrequent but energetic radiation source. Consider a hypothetical energetic particle that locally strongly heats the substrate on impact, but otherwise causes no physical degradation of the system. Imagine that the heating thermally randomizes spins in some neighborhood of the impact, with the neighborhood size proportional to the energy of the impact, and the probability distribution of increasingly energetic impacts decaying exponentially. Suppose furthermore that the cooling power per unit area of the substrate is sufficiently high to remove the excess heat in a small constant amount of time. This hypothetical scenario would lead to spatially correlated large-area errors with larger areas exponentially suppressed. Noise of this generic form shall be considered in Section [\ref=sc]. This section also considers the possibility of polynomial suppression of larger area errors.

Fourthly, consider direct magnetic dipole spin-spin interactions. A pair of antiparallel spins can spontaneously flip with no increase or decrease in energy of the system. The probability of this occurring is proportional to the interaction strength, which decays cubically. Pairwise noise of this form is two-body correlated noise. Note that each pairwise noise event requires the exchange of a virtual photon, so multiple pairwise noise events are random and uncorrelated. We shall consider noise of this generic form in Section [\ref=2q].

Finally, imagine that spins are sufficiently separated to make the direct dipole-dipole interaction negligible, however there are elements in the physical construction that behave like inductive loops around each column of spins. These could be control lines or long-range qubit-qubit coupling elements. Now any pair of initially antiparallel spins in a given column can flip. Such a noise source would not be suppressed with increasing qubit separation. This form of noise shall also be considered in Section [\ref=2q].

Undoubtedly other forms of noise could be considered, however we feel that the four correlated error classes listed above, namely 1) large-area exponentially decaying, 2) large-area polynomially decaying, 3) arbitrary qubit pairs polynomially decaying, 4) qubit pairs in columns non-decaying, cover the vast majority of basic behaviors likely to be found in physical devices. We would be happy to extend our work to cover other error classes of interest to the community, and welcome suggestions.

Surface code performance with local many-qubit errors

For our purposes, a distance d surface code is simply a (2d - 1)  ×  (2d - 1) 2-D array of qubits capable of protecting a single qubit of data by periodically executing a particular quantum circuit designed to detect errors [\cite=Fowl12f]. If we assume that each quantum gate in the periodic circuit has an error rate p, then given a distance d surface code we can use simulations to calculate the probability of a logical error per round of error detection pL, namely the probability pL that we fail to protect the single qubit of data distributed across the lattice of qubits. Fig. [\ref=logx_ft_c] shows pL as a function of p and d using asymptotically optimal error suppression techniques [\cite=Fowl13g]. This is our baseline performance. Introducing large-area errors will degrade this performance.

Consider Fig. [\ref=didj], which defines two quantities Δi, Δj that have meaning during the application of a quantum gate and will enable us to define our error models. We shall consider two particularly severe models of many-qubit errors, each with a single tunable parameter n determining its strength. Unlike in Section [\ref=corr] where large-area errors were motivated by a particle impact example, we shall associate such errors with the every application of every quantum gate.

When applying a gate with error rate p, a single random number x is generated. If x < p, the qubits involved in the gate will suffer random equally likely Pauli errors (with no chance of an identity error). Every other qubit in the surface code will suffer random equally likely errors I, X, Y, Z if at the location of the qubit x < p / nΔi + Δj (exponential model) or x < 0.1p / rn, where [formula] (polynomial model).

The motivation behind the exponential model's use of a non-Euclidean metric is qubits with a negligible direct qubit-qubit interaction that instead must be coupled via physical devices that are themselves non-interacting. In this scenario, qubits are physically well separated. The hypothetical energetic particle discussed in Section [\ref=corr] should be imagined as significantly raising the temperature or photon count of a specific component, and each successive device should provide additional isolation leading to Manhattan distance exponential suppression of the unwanted effects. The polynomial model is motivated by qubits that are closely spaced with thermal errors radiating through the substrate. All gates, including initialization, Hadamard, CNOT, measurement, and identity, are assumed to have a non-zero probability of suffering from such large-area errors during their implementation.

Figure [\ref=exp] shows the performance of the surface code with exponential model large-area errors and n = 2, 10, 100, and 1000. It can be seen that performance is still measurably degraded even for n = 1000, however strong exponential suppression of logical error at fixed p can still be achieved even for n = 10. To be quantitative, at an operating error rate of p = 10- 3, in the absence of large-area errors (Fig. [\ref=logx_ft_c]), a distance d = 7 surface code achieves a logical error rate per round of error detection of p = 2.0  ×  10- 6. For n = 1000, this is degraded to p = 2.4  ×  10- 6. This level of degradation would have negligible practical impact, with very slightly larger code distances required to compensate. Even for n = 10, where the logical error rate is degraded to p = 6.7  ×  10- 5, the degradation can be fully compensated by using a larger d = 11 code, leading to an approximate factor of (11 / 7)2  ~  2.5 additional qubits, independent of the size of the quantum computation protected in this manner. A factor of 2.5 overhead is significant but not excessively onerous, and we therefore claim that even quite moderate exponential suppression of large-area errors is tolerable in a practical manner when using the surface code.

A striking difference between Fig. [\ref=logx_ft_c] (no large-area errors) and Fig. [\ref=exp] (large-area errors) is the linear suppression of logical error in the latter for low values of p at a fixed code distance d as p is reduced further. This is due to the fact that any single error has the potential to cause a logical error, and at low values of p multiple temporally nearby gate errors become unlikely and the dominant logical error process becomes single large-area errors. Note that at fixed low p, logical error suppression is still exponential with increasing d.

We now consider polynomial suppression of large-area errors (Fig. [\ref=poly]). If large-area errors are only quadratically suppressed, adding an additional ring of qubits at distance r from any given qubit adds an O(1 / r) amount of error to that qubit, hence larger lattices of qubits will always be more error-prone and no threshold error rate will exist. For any rate of suppression greater than quadratic, arbitrarily reliable quantum computation can be achieved in principle, as the total error seen by any given qubit in an infinite lattice of qubits is bounded by a multiple of p.

At a moderately high error rate such as p = 10- 3 and modest code distances, the dominant logical error contribution is from multiple temporally local errors. Such logical errors are exponentially suppressed with increasing code distance. To be explicit, for n = 4, the polynomial of best fit through the data at p = 10- 3 is order 8 in d, and for n = 5 the best fit polynomial is order 16, clearly demonstrating that, in the high p low d regime, logical errors from single large-area physical errors are not dominant. At very large code distances, the quadratic growth of the number of gates per round of error detection and the exponential suppression of logical errors from multiple temporally local gate errors is expected to lead to weak O(1 / dn - 2) suppression of logical error due to single very large area errors, however this regime is outside what we can currently reach with simulations.

Based only on currently accessible parameter ranges, at p = 10- 3 the polynomial n = 4 and n = 5 overhead to achieve a given logical error rate is similar to the exponential n = 10 overhead. If the computation being protected by the surface code is not too large, it therefore may well be the case that the desired logical error rate can be reached without excessive overhead with only polynomial suppression of large-area errors at the physical level. Formally, however, it should be noted that the resources required to achieve computation with logical error ε would grow polynomially with 1 / ε for sufficiently small ε, which is not efficient in the computer science sense.

Surface code performance with non-local two-qubit errors

Only two-body interactions are observed in nature between fundamental particles, meaning the large-area multi-qubit errors considered in the previous Section could only arise from uncontrolled engineered multi-qubit interactions within a quantum computer or other exotic effects such as the radiation heating model described in Section [\ref=corr]. Unwanted two-body interactions, such as uncompensated Coulomb or magnetic dipole interaction, give rise to qualitatively and quantitatively different behavior. In this Section, we shall focus on long-range effects, and will therefore not consider interactions that decay exponentially quickly. As we shall see, even weakly polynomially decaying long-range interactions are quite tolerable, further justifying not considering exponentially decaying two-body interactions.

Any interaction between qubits is a potential source of unwanted evolution and hence error. When simulating the surface code using an array of qubits with polynomially decaying two-body interactions, if the characteristic gate error rate is p, at the beginning of each round of error detection each pair of qubits shall be modeled as suffering two-qubit depolarizing noise with probability Ap / rn. We shall focus on the most severe n = 2 case, and two values A = 1 and A = 0.1. The performance of the surface code with these two different levels of additional noise is shown in Fig. [\ref=poly2q].

It should be stressed that, as with quadratically suppressed large-area errors, any qubit in an infinite 2-D lattice of qubits will suffer unbounded error and the surface code will fail. However, it can be seen that for the finite-size qubit arrays considered in simulations, robust suppression of logical error can be achieved even for the most severe A = 1 case. The effect of a lack of a threshold error rate can be observed at p = 2  ×  10- 3 where the d = 25 logical error rate is higher than that for d = 11. Nevertheless, at error rates p < 10- 3, the observed logical error rate suppression trend with increasing code distance suggests that extremely low logical error rates can be achieved before using larger code distances starts to hurt. Note that for A = 1 and p  ≤  5  ×  10- 4 the observed logical error rates are less than or equal to those observed for n = 10 exponential large-area errors, meaning the overhead will be less than the factor of 2.5 calculated in the previous Section, for moderate values of d.

By making the computer quasi 2-D, namely a finite width 1-D strip, the physical error seen at any given qubit would only grow logarithmically with increasing strip length, very likely permitting a usefully large number of logical qubits with usefully low logical error rates to be achieved. Other techniques such as building an array with carefully arranged walls capable of shielding the problematic interaction, or coupling widely separated finite arrays with other types of quantum communication are also possible. In short, even severe long-range two-qubit quantum errors that are only suppressed quadratically with increasing qubit separation can be handled with practical overhead.

The final class of error we shall consider are those arising from large-scale coupling elements that interact with many qubits, specifically entire columns of the surface code in the situation we shall model. Our basic motivating system is a chain of spins in a global magnetic field and shared inductive loop. Any pair of antiparallel spins can spontaneously flip, so we shall model this as a probability Ap of error for every qubit pair in each column. Note that there is no suppression of this error with increasing qubit separation. Since any given qubit in a column has an increasing number of potential partners to flip with as the size of the surface code grows, there will again be no formal threshold error rate. We again focus on A = 1 and A = 0.1. Data is shown in Fig. [\ref=const2q].

For A = 1 (Fig. [\ref=const2q]a), it can be seen that at p = 10- 3 the lowest possible logical error rate is achieved with a distance 15 code. Fig. [\ref=poly2q] and Fig. [\ref=const2q] are qualitatively very similar as in both an array of qubits with quadratically suppressed interactions between all pairs of qubits and an array of qubits with columns coupled by single devices introducing errors with no suppression with increasing distance, the total error seen by any given qubit grows linearly with code distance. When A = 0.1 (Fig. [\ref=const2q]b), at p = 10- 3 it can be seen that very low logical error rates can be achieved with modest code distances. Again, despite the lack of a threshold error rate, it can be seen that this class of errors is tolerable with low overhead in practice.

Conclusion

We have shown that moderate exponential suppression of large-area errors is sufficient to observe strong exponential suppression of logical error with increasing code distance d. A factor of 10 suppression of each successively larger area class of physical errors leads to just a factor of 2.5 additional qubits to achieve the same logical error observed without large-area errors when gates have characteristic error p = 10- 3. Overhead is negligible (<  10%) for moderately large algorithm sizes and a factor of suppression of 103. Since 5+ body errors are expected to be exceedingly rare in most physical setups, it is reasonable to expect that this higher level of error suppression is experimentally achievable and that large-area errors can therefore mostly be ignored when analyzing the surface code.

A second class of errors, namely long-range two-qubit errors, has been shown to be remarkably tolerable, with even lower overhead than the exponentially suppressed large-area errors for [formula]. This is surprising as such noise, from a formal point of view, results in no threshold error rate, meaning arbitrarily reliable quantum computation cannot be achieved at any finite error rate. Nevertheless, sufficiently low logical error rates for practical purposes can be achieved with modest code distances.

In all cases where a threshold error rate exists, it remains well above 10- 3 and in most cases does not stray far from the baseline threshold error rate of approximately 0.5%. This is in line with expectations as the correlated errors introduced in the simulations are typically at least an order of magnitude less likely than the baseline gate errors, meaning they have low impact around the threshold error rate. The only exception to this is n = 2 exponentially suppressed large-area errors, where weight 5 errors, for example, are only half as likely as single-qubit errors, resulting in a degradation of the threshold error rate to just above 10- 3.

Collectively, these results imply that large-area and long-range errors pose no fundamental barriers to practical large-scale quantum computation, as both classes of error, from a practical point of view, can be well handled by the surface code. Experimentally, the implication is that, in a large device, one should focus on the gate error rate observed when the maximum possible number of qubits in the array are being actively manipulated in parallel. The parallel error rate is the figure of merit required to determine whether a physical device can be used to achieve low logical error rates.

Acknowledgements

We thank Daniel Gottesman for suggesting this project, and Rami Barends, Julian Kelly, Daniel Sank, Evan Jeffrey, Ted White, and John Preskill for helpful discussions. This research was funded by the US Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the US Army Research Office grant No. W911NF-10-1-0334. Supported in part by the Australian Research Council Centre of Excellence for Quantum Computation and Communication Technology (CE110001027) and the U.S. Army Research Office (W911NF-13-1-0024). All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of IARPA, the ODNI, or the US Government.