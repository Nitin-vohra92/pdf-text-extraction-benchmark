Counting people from above: Airborne video based crowd analysis

Introduction

The recognition of critical situations in crowded scenes is very important to prevent escalations and human casualties. On large scale events, like music festivals or sport events, important parameters for estimating the riskiness of a situation are, as follows, the number of persons, the density of individuals per square meter, the general motion direction of groups of people and motion patterns (like dangerous forward and backwards motions in front of a stage or an entrance). These parameters can be used to estimate the human pressure which indicates potential locations of violent crowd dynamics [\cite=helbing2007dynamics]. Despite the huge number of security forces and crowd control efforts, hundreds of lives are lost in crowd disasters each year (like at Roskilde Festival in 2000, or in Mina/Makkah during the Hajj in 2006, or in Duisburg at Love Parade in 2010). In the future, the presented framework will provide sufficiently robust cues to prevent such disastrous incidences.

In this paper we introduce a setup based on HD video data which can either be captured from a tower-mounted camera or from an airborne vehicle (airplane, helicopter, UAV). The resulting video, capturing parts of the crowded scene, is analyzed with computer vision techniques which extract the target parameters (count, density, motion). To be able to pipe such information in a crowd simulation framework the per-pixel information has to be geo-referenced into a world-coordinate system. This enables to measure in physical units, number of persons per square meter and motion in meters per second. A crucial parameter to detect critical situations in humans crowds is the human pressure P, defined by [formula] where [formula] is the spatial location, t the time, œÅ the estimated density and V the motion [\cite=helbing2007dynamics], which can be estimated employing the proposed framework. Such information can then be used to alert security staff who then triggers appropriate actions, like opening or closing a gate or restricting the access of following people. Our contribution. The main difference in our approach to the related work is to apply higher order features for density estimation and provide an accurate performance analysis in a geo-referenced framework, such as, using an object detector tailored for person detection, learning the density estimate from image features w.r.t. a given ground truth (can be seen as an automatic feature selection) and rectifying all information from 2D image geometry to 3D world coordinates. In addition, the proposed framework is general and could be combined with any existing visual features, with any object category and with any object detection method. For example, it could be applied - appropriate features presumed - to count trees or cars in airborne videos.

State of the art

Some principles for crowd monitoring and person counting have been published. For example, [\cite=chan2008privacy] count people in an outdoor scenario based on a fixed mounted static video camera using a motion segmentation followed by a feature extraction that serves as input for a Gaussian regression model. The main drawback w.r.t. our application is the prior motion segmentation. Such a system can only identify moving people, therefore all standing people are not counted. In addition, other moving objects like cars or pets will also appear in the motion segmentation. Authors of [\cite=butenuth2011integrating] detect individual people and crowd outlines from airborne nadir looking images. While isolated persons are detected using a custom tailored object detector, regions containing crowds are recognized when many local features (features from accelerated segment test (FAST)) jointly occur. The work does not contain an accuracy analysis and lacks a concept of how to map potential crowd regions to estimated person counts. It also seem problematic to define regions of crowds by low-level features, as in an arbitrary scenario also other objects than people will give a high FAST response (like textured vegetated areas). The work of [\cite=sirmacek2011automatic] also deals with airborne nadir looking images. This very interesting approach is similar to our methodology in terms that it extracts local features (in this case again FAST) and uses them to estimate the crowd density. The authors also include a feature selection step to reject local features which potentially are not corresponding to persons. The density itself is extracted using a kernel density estimate based on the feature occurrence. The number of individuals is spatially aggregated also using the FAST responses.

In the following we discuss related work in particular for object counting, density estimation, motion estimation and geo-referencing. Object Counting and Density Estimation. There are three main methodologies: (1) Counting by detection: The idea is to detect each individual object instance in the image and count their number (actually this is how human count). However, in computer vision object detection is far from being solved [\cite=everingham2009the] and the detection is a harder problem than counting alone. Huge problems arise when objects are overlapping and occlude each other. (2) Counting by regression: Those methods try to find a mapping from various image features to the number of objects using supervised machine learning methods. However, those methods do not use the location of the objects in the image instead they just find the regression to a single number, the number of objects. Therefore, huge training datasets are necessary to achieve useful results [\cite=kong2006a]. (3) Counting by density estimation: The main concept is to estimate an object density function whose integral over any image region gives the count of objects within this region [\cite=lempitsky2010learning]. For learning the proposed methods employ the ground truth location of objects and the learning can be posed as a convex linear or quadratic program. An additional benefit of the method is that after learning the density function can be estimated by simple multiplication of the individual features with learned weights and is therefore very efficient. Motion Estimation. Estimating small motions from adjacent video frames is considered to be solved, or to state it differently, the accuracy of state-of-the-art algorithms are sufficient for our needs. The so-called optical flow can be extracted by total variation methods in image geometry, [\cite=zach2007a]. Geo-Referencing. Geo-referencing, also called ortho-rectification, is a standard method in photogrammetry and in remote sensing ( [\cite=kraus2007photogrammetry]) which projects the image onto the earth's surface in a given map projection. To be able to handle the distortions due to the topography a digital surface model (DSM) is used (global digital surface models like SRTM or ASTER GDEM are freely available). If the terrain is rather flat the DSM can be replaced by the knowledge of the mean terrain height. For areas containing many obstacles like stages, bridges, etc. a laser scanner model will deliver most accurate results.

Methods

Workflow

The proposed approach is sketched in  [\ref=figure:WF_density] and in  [\ref=figure:density_image_geo]. The main idea is to extract image features which are related to the human density by machine learning techniques. We employ discretized features where the learning provides a weight for each feature number. Thus, after learning the density function can be calculated by simple multiplications. In addition, the density estimate is a real density function, meaning that the integral over the density yields the object count (therefore, the integral over a subregion holds the number of objects in this particular region). The motion between video frames is extracted using a variational method. All gathered information is then geo-referenced and can therefore be visualized and processed in any geographic information system.  [\ref=figure:density_image_geo] shows a video frame superimposed with the estimated density and motion and the same information geo-referenced and overlayed in Google Earth.

Object Counting and Density Estimation

For object counting and density estimation we employ the method by [\cite=lempitsky2010learning]. This method takes dense discretized feature maps extracted from the input images and learns the density estimate via a regression to a ground truth density. Thus, each pixel has to be described by a feature vector of the following form [formula] which is 1 at the dimension of the corresponding discretized feature and otherwise 0. Since we want to detect persons we apply the object detector of [\cite=felzenszwalb2010object] with the learned model for persons of the VOC 2009 challenge [\cite=everingham2009the]. This detector yields confidence values which have to be discretized. As we know from experience and previous tests that very small and very high confidences are useless for object counting, we set the minimal value to - 4.0 and the maximal to - 0.6 for all tests. High confidences usually only occur on isolated non-occluded persons, not in crowds. If we would not saturate the confidences, the density estimation would put too much emphasize on such objects. These bounds are used to scale the confidences to [formula]. Now, each of the possible 256 values define a feature vector, as discusses above, which is 1 at the position of the confidence value. Therefore, it yields 256 individual features (  [\ref=figure:WF_density]). In addition, we extract dense scale-invariant feature transform (SIFT) descriptors [\cite=lowe2004distinctive] using the implementation in [\cite=vedaldi08vlfeat] for each pixel. To be able to discretize this information we take 256 SIFT prototypes [\cite=lempitsky2010learning] and the closest prototype for each descriptor defines the quantized SIFT number. Therefore, for each pixel we get a discretized SIFT value in [formula]. These additional 256 features are employed to test if simpler cues than object detector confidences could yield useful results. For evaluation we train the density estimation framework for each feature class individually and for both, which is done by stacking the features.

The training itself minimizes the regularized Maximum Excess over SubArrays (MESA) distance where we use the L1 and the Tikhonov regularization [\cite=tikhonov1977solutions] to solve the linear or quadratic equation system ( min x||Ax  -  b|| or min x||Ax  -  b||  +  ||(x'Œìx)  /  2|| with ||x  ‚â•  0|| and Tikhonov matrix Œì being the identity matrix in our case). All details of this methodology are given in [\cite=lempitsky2010learning]. The result is a weight for each of the discretized features and the resulting density is calculated by multiplying the according weight with the extracted feature value. Thus, for each pixel the density function is given and the sum over all pixels represents the number of objects in the image, our person count.

Therefore, in the testing phase the discretized features are extracted for each image and multiplied by the learned weight vector directly resulting in the density estimation per pixel and corresponding person count. It should be noted that this approach introduces virtually no overhead over feature extraction [\cite=lempitsky2010learning]. In case of very efficient feature extraction methods, like decision tree and forests [\cite=sharp2008implementing] or cascades of boosted weak classifiers [\cite=viola01rapid], the whole density estimation would also run in real time.

Motion Estimation

The motion is estimated based on the optical flow in image geometry [\cite=zach2007a] where we used the implementation at. To get a more robust estimate the flow is not gathered from two adjacent video frames but from frames with a temporal distance of 10 frames. In addition a given number of those flows are temporally averaged to ensure smooth motion vectors.

Geo-Referencing

To keep it simple we define a common map frame for each of our test sites in WGS84 UTM 33 North projection (EPSG 32633) since our sites are located in western Austria, Europe. Then for each image and for each column/line coordinate the according 3D world coordinate is calculated which are used to rectify the density and motion information. Density. For geo-referencing the density we project each density pixel into the common frame. If a pixel gets hit more than once the values are summed up. This ensures that the sum of the density, the human count, stays the same in image and world coordinates. Since it happens that some pixels are hit more often than their neighbors due to aliasing, the whole geo-referenced density is smoothed using a Gaussian kernel. Motion. In image geometry we cannot differentiate between object motion and camera motion. Therefore, we transform the reference 2D image coordinate and the according search 2D images coordinate ( gathered via optical flow) into 3D world coordinates. These two world coordinates define the real object motion vector independent of the camera movement. Since the temporal difference of the two input video frames is know, the speed of the motion can be calculate in meters per second.

Results

Test Data

For evaluation of the presented concept videos from two different scenarios were acquired in HD quality. The first one, referred as Lakeside, originates from a music festival in Styria, Austria (  [\ref=figure:density_image_geo]). The video camera was mounted on a tower (approximately 30 meters above ground). The camera was therefore more or less static with small jiggling due to wind. To geo-reference the scene only one image was manually rectified and defines the geometry for all other images. The second scenario, called Donauinsel, originates from a huge open air festival in Vienna, Austria (  [\ref=figure:density_image_geo_DI]). Here the video camera was mounted on an airplane. For geo-referencing, the meta-data (GPS/IMU) supplied by the camera system was taken for each frame. Since every frame has a different exterior parameters, it was necessary to geo-reference every frame independently.  [\ref=table:test_data] lists the details of the video setups and parameters. We also manually labeled many frames to get the ground truth person counts in training and later in the testing phase (overall over 23500 persons were annotated with a mean height of 90 pixels,  [\ref=table:test_labels]). It is important to note that the scenes for learning are similar however different than the testing scenes. Since the Lakeside scenario contains a much larger data set, most of the experimental results are focused on this set. The Donauinsel scenario contains insufficient images for sustainable training and testing. In addition, the density estimate is evaluated in detail since the motion estimation can be solved by state-of-the-art algorithms.

Density Estimation

Learning. The accuracy of the learning process is listed in  [\ref=table:accuracy]. It can be seen that the used object detector has a better impact on the density estimation than the dense SIFT descriptors in 7 of 8 cases (the one exception stems for L1 based regularization, which is in general unstable). Using both features increases the accuracy. It is also interesting that the two regularizations yield similar results, even though the learned weights are very different. Overall, the L1 regularization tends towards a zero-solution, setting many weights to zero, while the Tikhonov regularization populates the weights a lot smoother (this is a property of the Tikhonov regularization, as it improves the condition of the problem and enables a more stable numerical solution). This aspect seems not important for the learning set, however it changes the performance in the testing phase. If we have a slight motion blur in one of the images, the according L1 weights drop to zero, while the Tikhonov weights do not. For the Donauinsel scenario the Tikhonov based regularization yields a lower accuracy than L1 in case of dense SIFTs. We assume that the low number of learning samples and the unfavorable mapping of discretized SIFT values to the real occurrence of persons (the stage rack contains many vertical structures, the same features of a person) yield a bad condition of the equation system and therefore the solution tends to a local minimum instead of the global one. While learning based in L1 regularization picks a few SIFT keys and a few object detection scores (only 10), the Tikhonov based learning takes more SIFTs and a logical weight distribution of the object detector (in total 453). Where logical means that the learned weights are dependent on the object detector confidences. Testing. The accuracy of the density estimation is given in  [\ref=table:accuracy]. Like in the training phase the Tikhonov regularization yields slightly higher accuracies than the L1 one. On average the mean person counting error is 4% of the Lakeside and 9% for the Donauinsel data set.  [\ref=figure:counts] shows the estimated person count of Lakeside with superimposed manually measured ground truth. Both resulting curves are similar however the Tikhonov regularization creates a smoother result. Experimentally we can prove this assumption by taking a look at the temporal smoothness of the estimated person count. The standard deviation of per frame differences of the estimated count is 4.4 for L1 regularization and 3.8 for Tikhonov regularization (for Lakeside and when using both feature sets). Obviously, a lower number represents a more realistic setting, as the number of persons in two adjacent frames should not vary much. When taking a close look to  [\ref=figure:counts] a rather huge error is visible towards the end of the sequence (image number 6500 to 6700). The reason for this issue are strong winds causing camera shaking and therefore a motion blur in the images. Consequently, the extracted features are different to the learned weights resulting in a lower human density estimate.

Conclusion

In this work we presented a method for people counting and crowd monitoring from airborne imagery. The estimated parameters from a given video stream were human count and human density and motion for each pixel. This information was geo-referenced into a world coordinate system. Overall, the estimated human counts were highly accurate with resulting 4% and 9% count error for the two presented scenarios, which could be reached by employing a custom tailored object detector instead of simple images features amongst other implementation details. The proposed framework is therefore higly important for security applications. Outlook. Currently, the framework is optimized for oblique views and thus it will not yield reasonable accuracies when employing nadir images. We envision to train the system on several viewing conditions, where the object detector should also be custom tailored (like a detector for head and shoulders for oblique views and a blob-like detector for nadir views). The viewing condition itself can be derived from the airplane's geo-sensors. When extracting the human densities the system is able to choose from the learned models according to the viewing parameters. Of course it would also be of interest to test different features and detectors on the accuracy and various regularizations for minimizing the MESA distance in the machine learning approach.