A Generalized Online Mirror Descent with Applications to Classification and Regression

Introduction

Online learning provides a scalable and flexible approach to the solution of a wide range of prediction problems, including classification, regression, and ranking. Popular online strategies for classification and regression include first-order gradient-based methods, such as the standard Perceptron algorithm and its many variants (e.g., p-norm Perceptron [\citep=Gentile03] and Passive-Aggressive [\citep=CrammerDKSSS06]), the Winnow algorithm of [\cite=Lit88], the Widrow-Hoff rule, the Exponentiated Gradient algorithm of [\cite=KW97], and many others. A more sophisticated adaptation to the data sequence is achieved by second-order methods, which use the inverse of the empirical feature correlation matrix (or, more generally, the inverse of the Hessian) as an dynamic conditioner for the gradient step. These methods include the Vovk-Azoury-Warmuth algorithm [\citep=Vov01] [\citep=AW01] (see also [\citep=Forster]), the second-order Perceptron [\citep=Cesa-BianchiCG05], the CW/AROW algorithms [\citep=CrammerDP08a] [\citep=CrammerDP08] [\citep=CrammerKD09] [\citep=Crammer:2012:CLC:2343676.2343704], the adaptive gradient algorithms proposed by [\cite=DuchiHS10] and [\cite=McMahanS10], and the Online Newton Step algorithm of [\cite=HazanAK07] for exp-concave losses.

Recently, online convex optimization has been proposed as a common unifying framework for designing and analyzing online algorithms. In particular, online mirror descent (OMD) is a generalized online gradient descent algorithm in which the gradient step is mediated by a strongly convex regularization function. By appropriately choosing the regularizer, most first-order online learning algorithms are recovered as special cases of OMD. Moreover, performance guarantees can be derived simply by instantiating the general OMD bounds to the specific regularizer being used. Note that OMD is the online version of the Mirror Descent algorithm for standard convex optimization (over a fixed convex function) by [\cite=nemirovskyproblem] --see also [\citep=beck2003mirror] and [\citep=Cesa-BianchiL06]. However, before this connection was made explicit, specific instances of OMD had been discovered and analyzed by [\cite=WJ97] and [\cite=KW01]. These works also pioneered the use of Bregman divergences (ways of measuring distances in linear spaces through convex functions) in the analysis of online algorithms, an approach later extended by [\cite=GentileW98] to non-differentiable loss functions.

A series of recent works [\citep=Shalev-ShwartzS07] [\citep=Shalev-Shwartz07] [\citep=shalev2009mind] investigates a different approach to online analysis based on primal-dual optimization methods. The work of [\cite=kakade2012regularization] showed indeed that many instances of OMD can be easily analyzed using only a few basic convex duality properties --see the recent survey by [\cite=Shalev12] for a lucid description of these advances. A related algorithm is Follow the Regularized Leader (FTRL), introduced in [\citep=AbernethyHR08] [\citep=AbernethyHR12], where at each step the prediction is computed as the minimizer of a regularization term plus the sum of losses on all past rounds. When losses are linear, FTRL and OMD are easily seen to be equivalent [\citep=AbernethyHR08] [\citep=RakhlinT08] [\citep=hazan2011].

In this paper, we extend the theoretical framework of [\cite=kakade2012regularization] by allowing OMD to use time-varying regularizers with updates that do not necessarily use the sub-gradient of the loss function. Time-varying regularizers have been recently proved to be the key to obtain improved guarantees in unconstrained settings [\citep=StreeterM12] [\citep=Orabona13] [\citep=McMahanO14], and updates that do not use the sub-gradient of the loss are known to lead to second-order algorithms. Indeed, we show that the Vovk-Azoury-Warmuth algorithm, the second-order Perceptron, and the AROW algorithm are recovered as special cases of our generalized OMD. Our unified analysis is simple, and in certain cases achieves slightly improved theoretical guarantees. Our generalized OMD also captures the efficient variants of second-order classification algorithms that only use the diagonal elements of the feature correlation matrix, a result which was not within reach of previous techniques. Moreover, we also show that a proper choice of the time-varying regularizer allows to cope with the composite setting of [\citep=DuchiS09] [\citep=Xiao10] [\citep=DuchiSST10] without using ad-hoc proof techniques.

Our framework improves on previous results even in the case of first-order algorithms. For example, although aggressive algorithms for binary classification often exhibit a better empirical performance than their conservative counterparts, a theoretical explanation of this behavior remained elusive until now. Using our refined analysis, we are able to prove the first bound for Passive-Aggressive (PA-I) that is never worse (and sometimes better) than the Perceptron bound.

Time-varying regularizers can also be used to perform other types of adaptation to the sequence of observed data. We give a concrete example by introducing new adaptive regularizers corresponding to weighted variants of the standard p-norm regularizer. The resulting updates, and their associated regret bounds, enjoy the property of being invariant with respect to arbitrary rescalings of individual features. Moreover, if the best linear predictor for the loss sequence is sparse, then our analysis delivers a better bound than that of OMD with 1-norm regularization, which is the standard regularizer for the sparse target assumption.

Online convex optimization

Let [formula] be any finite-dimensional linear space equipped with inner product [formula]. For example, [formula] where [formula] is the vector dot product, or [formula], the space of m  ×  n real matrices with inner product [formula].

In the online convex optimization protocol, an algorithm sequentially chooses elements from a convex set [formula], each time incurring a certain loss. At each step [formula] the algorithm chooses [formula] and then observes a convex loss function [formula]. The value [formula] is the loss of the learner at step t, and the goal is to control the regret,

[formula]

for all [formula] and for any sequence of convex loss functions [formula]. Important application domains for this protocol are online linear regression and classification. In these settings there is a fixed and known loss function [formula] and a fixed but unknown sequence [formula] of examples [formula]. At each step [formula] the learner observes [formula] and picks [formula]. The loss suffered at step t is then defined as [formula]. For example, in regression tasks we might use the square loss [formula]. In binary classification, where yt∈{ - 1, + 1}, a popular loss function is the hinge loss [formula], where +  =   max {0,a}. This loss is a convex upper bound on the mistake indicator function [formula], which is the true quantity of interest.

Note that Mirror Descent, and its online version OMD, can be also defined and analyzed in spaces much more general than those considered here --see, e.g., [\citep=sridharan2010convex] [\citep=srebro2011universality].

Further notation and definitions

We now introduce the basic notions of convex analysis that are used in the paper --see, e.g., [\cite=BauschkeC11]. We consider functions [formula] that are closed and convex. This is equivalent to saying that their epigraph [formula] is a convex and closed subset of [formula]. The (effective) domain of f, defined by [formula], is a convex set whenever f is convex. We can always choose any [formula] as the domain of f by letting [formula] for [formula].

Given a closed and convex function f with domain [formula], its Fenchel conjugate [formula] is defined by [formula]. Note that the domain of f* is always [formula]. Moreover, one can prove that f*  * = f.

A generic norm of a vector [formula] is denoted by [formula]. Its dual [formula] is the norm defined by [formula]. The Fenchel-Young inequality states that [formula] for all [formula].

A vector [formula] is a subgradient of a convex function f at [formula] if [formula] for any [formula] in the domain of f. The differential set of f at [formula], denoted by [formula], is the set of all the subgradients of f at [formula]. If f is also differentiable at [formula], then [formula] contains a single vector, denoted by [formula], which is the gradient of f at [formula]. A consequence of the Fenchel-Young inequality is the following: for all [formula] we have that [formula].

A function f is β-strongly convex with respect to a norm [formula] if for any [formula] in its domain, and any [formula],

[formula]

The Fenchel conjugate f* of a β-strongly convex function f is everywhere differentiable and [formula]-strongly smooth. This means that, for all [formula],

[formula]

See also the paper of [\cite=kakade2012regularization] and references therein. A further property of strongly convex functions [formula] is the following: for all [formula],

[formula]

This implies the useful identity

[formula]

Strong convexity and strong smoothness are key properties in the design of online learning algorithms.

Online Mirror Descent

We now introduce our main algorithmic tool: a generalization of the OMD algorithm in which the regularizer may change over time. The standard OMD algorithm for online convex optimization --see, e.g., [\citep=Shalev12]-- sets [formula], where f* is a strongly convex regularizer and [formula] is updated using subgradient descent: [formula] for η  >  0 and [formula]. For instance, if [formula], then f*  =  f and OMD specializes to standard online subgradient descent.

We generalize OMD in two ways: first, we allow f to change over time; second, we do not necessarily use the subgradient of the loss to update [formula], but rather use an input sequence of generic elements [formula]. The resulting algorithm is summarized in Alg. [\ref=alg:online].

Note the following remarkable property: while [formula] follows an arbitrary trajectory in [formula], as determined by the input sequence [formula], because of ([\ref=eq:proj-prop]) the property [formula] holds uniformly over t.

OMD with specific time-varying regularizers was implicitly analyzed by [\cite=Vov01] and independently by [\cite=AW01], who introduced the proof ideas currently used here. Another early example of OMD with time-varying regularizers is due to [\cite=BartlettHR07]. A more explicit analysis is contained in the work of [\cite=SridharanT10]. The following lemma is a generalization of two corollaries of [\citep=kakade2012regularization] and of [\citep=DuchiHS10].

Assume OMD is run with functions [formula] defined on a common convex domain [formula] and such that each ft is βt-strongly convex with respect to the norm [formula]. Let [formula] be the dual norm of [formula], for [formula]. Then, for any [formula], where we set [formula]. Moreover, for all t  ≥  1, we have

[formula]

Let [formula]. Then Since the functions f*t are [formula]-strongly smooth with respect to [formula], and recalling that [formula],

[formula]

where we used the definition of [formula] in the last step. On the other hand, the Fenchel-Young inequality implies Combining the upper and lower bound on Δt and summing over t we get

[formula]

We now prove the second statement. Recalling again the definition of [formula], we have that ([\ref=eq:useful]) implies [formula]. On the other hand, the Fenchel-Young inequality implies that [formula]. Combining the two we get [formula], as desired.

We are now ready to prove regret bounds for OMD applied to three different classes of time-varying regularizers.

Let S a convex set, [formula] be a convex function, and let [formula] be a sequence of convex functions [formula] such that [formula] for all [formula] and all [formula]. Fix η  >  0 and assume ft  =  gt  +  ηtF are βt-strongly convex w.r.t. [formula]. For each [formula] let [formula] be the dual norm of [formula]. If OMD is run on the input sequence [formula] for some [formula], then

[formula]

for all [formula].

Moreover, if [formula] where [formula] is β-strongly convex w.r.t. [formula], then

[formula]

for all [formula].

Finally, if ft  =  t  F, where F is β-strongly convex w.r.t. [formula], then

[formula]

for all [formula].

By convexity, [formula]. Using Lemma [\ref=lemma:general] we have,

[formula]

where we used the fact that the terms [formula] are nonpositive as per our assumption. Reordering terms we obtain ([\ref=eq:composite_bound]). In order to obtain ([\ref=eq:tuned-bound] it is sufficient to note that, by definition of strong convexity, [formula] is [formula]-strongly convex because g is β-strongly convex, hence ft is [formula]-strongly convex too. The elementary inequality [formula] concludes the proof of ([\ref=eq:tuned-bound]). Finally, bound ([\ref=eq:log-bound]) is proven by observing that ft  =  t  F is βt-strongly convex because F is β-strongly convex. The elementary inequality [formula] concludes the proof.

Note that the regret bounds obtained in Corollary [\ref=cor:convex_loss] are for the composite setting, where the algorithm minimizes the sum [formula] of two functions, where the first one is typically a loss and the other is a regularization term. Here, the only hypothesis on F is convexity, hence F can be a nondifferentiable function, like [formula] for inducing sparse solutions. While the composite setting is considered more difficult than the standard one, and requires specific ad-hoc algorithms [\citep=DuchiS09] [\citep=Xiao10] [\citep=DuchiSST10], here we show that this setting can be efficiently solved using OMD with a specific choice of the time-varying regularizer. Thus, we recover the results about minimization of strongly convex and composite loss functions, and adaptive learning rates, in a simple unified framework. Note that [\eqref=eq:residue], which was missing in previous analyses of OMD, is the key result to obtain this level of generality.

A special case of OMD is the Regularized Dual Averaging framework of [\cite=Xiao10], where the prediction at each step is defined by

[formula]

for some [formula], [formula]. Using ([\ref=eq:proj-prop]), it is easy to see that this update is equivalent to [formula], where [formula]. The framework of [\cite=Xiao10] has been extended by [\cite=DuchiSST10] to allow the strongly convex part of the regularizer to increase over time. A bound similar to ([\ref=eq:composite_bound]) has been also recently presented by [\cite=DuchiSST10]. There, a more immediate trade-off between the current gradient and the Bregman divergence from the new solution to the previous one is used to update at each time step. However, in both cases their analysis is not flexible enough to include algorithms whose update does not use the sub-gradient of the loss function. Examples of such algorithms are the Vovk-Azoury-Warmuth algorithm of the next section and the online binary classification algorithms of Section [\ref=sec-class].

Online regression with square loss

In this section we apply Lemma [\ref=lemma:general] to recover known regret bounds for online regression and adaptive filtering with the square loss. For simplicity, we set [formula] and let the inner product [formula] be the standard dot product [formula]. We also set [formula] where [formula] is some arbitrary sequence of examples [formula].

First, note that it is possible to specialize OMD to the Vovk-Azoury-Warmuth algorithm for online regression. Remember that, at each time step t, the Vovk-Azoury-Warmuth algorithm predicts with

[formula]

Now, by letting A0 = a  Id, [formula] for t  ≥  1, and [formula], we obtain the OMD update [formula], where [formula] and [formula]. Note that [formula] is not equal to the negative gradient of the square loss. In fact, the special structure of the square loss allows us to move some of the terms inside the regularizer, and use proxies for the gradients of the losses. The regret bound of this algorithm --see, e.g., [\cite=Cesa-BianchiL06]-- is recovered from Lemma [\ref=lemma:general] by noting that ft is 1-strongly convex with respect to the norm [formula]. Hence, the regret ([\ref=eq:regret]) is controlled as follows

[formula]

since [formula], and where Y  =   max t|yt|.

A related setting is that of adaptive filtering --see, e.g., [\citep=KivinenWH06] and references therein. In this setting the output signals yt are given by [formula], where [formula] is unknown and νt is some arbitrary noise process. The goal is to recover the uncorrupted output [formula]. This can be achieved by minimizing a suitable notion of regret, namely the adaptive filtering regret w.r.t. the square loss,

[formula]

[\cite=KivinenWH06] introduced the p-norm LMS algorithm, addressing adaptive filtering problems in a purely online nonostochastic setting. We now show that OMD can be easily applied to online adaptive filtering. First, note that

[formula]

where we set [formula]. Now, pick any function f which is 1-strongly convex with respect to some norm [formula] and let [formula], where [formula]. Lemma [\ref=lemma:general] then immediately implies that

[formula]

where we used the X2t-strong convexity of ft and the fact the ft  ≥  ft - 1. Combining ([\ref=eq:filtering1]) with ([\ref=eq:filtering2]) and simplifying, we obtain the following adaptive filtering bound

[formula]

This is a direct generalization to arbitrary regularizers f of the bound by [\cite=KivinenWH06]. However, their algorithm requires the prior knowledge of the maximum norm of [formula] to set a critical parameter. Instead, our algorithm, through the use of an increasing regularizer, has the ability to adapt to the maximum norm of [formula], without using any prior knowledge.

Scale-invariant algorithms

In this section we show the full power of our framework by introducing two new scale-invariant algorithms for online linear regression with an arbitrary convex and Lipschitz loss function.

Recall the online linear regression setting: given a convex loss [formula] and a fixed but unknown sequence [formula] of examples, at each step [formula] the online algorithm observes [formula] and picks [formula]. The associated loss is [formula].

Let [formula] be any fixed predictor with small total loss [formula]. Because of linearity, an arbitrary rescaling of any individual feature, xt,i  →  c  xt,i for [formula] (while [formula] are kept fixed) can be offset by a corresponding rescaling of ui without affecting the total loss. We might ask a similar scale-invariance for the online predictor. In other words, we would like the online algorithm to be independent of the units in which each data coordinate is expressed.

We now introduce two new time-varying regularizers that achieve this goal. As in the previous section, let [formula] and let the inner product [formula] be the standard dot product [formula].

The new regularizers are based on the following generalization of the squared q-norm. Given [formula] and q∈(1,2] define the weighted q-norm of [formula] by

[formula]

Define the corresponding regularization function by

[formula]

This function has the following properties (proof in the Appendix).

The Fenchel conjugate of f is

[formula]

Moreover, the function [formula] is 1-strictly convex with respect to the norm

[formula]

whose dual norm is defined by

[formula]

Given an arbitrary sequence [formula] of examples, we assume OMD is run with [formula] where, as usual, [formula]. We also assume the loss [formula] is such that [formula] is L-Lipschitz for each [formula] and [formula] is convex for all t. In the rest of this section, the following notation is used: [formula], and

[formula]

The time-varying regularizers we consider are defined as follows,

[formula]

As we show next, regularizers of type ([\ref=eq:f1]) give regret bounds that exhibit a logarithmic dependency on the maximum number of non-zero components observed. Instead, regularizers of type ([\ref=eq:f2]) give bounds that depend on [formula], and used a different learning rate for each coordinate. Roughly speaking, the first regularizer provides scale-invariance with a logarithmic dependency on the dimension d obtained through the p-norm regularization [\citep=Gentile03]. The second regularizer, instead, corresponds to a scale invariant version of AdaGrad [\citep=DuchiHS10].

In order to spell out the OMD update, we compute the derivative of the Fenchel dual of the regularization functions. Using the fact that if [formula], then [formula], for regularizers of type ([\ref=eq:f1]) we have

[formula]

For regularizers of type ([\ref=eq:f2]) we have

[formula]

These updates are such that [formula] is independent of arbitrary rescalings of individual features. To see this, recall that [formula] and

[formula]

where the partial derivative is at most L by Lipschitzness of [formula]. Hence, the ratios [formula] and [formula] are invariant with respect to arbitrary rescalings of the j-th feature. So, in both [\eqref=eq:update1] and [\eqref=eq:update2], wt,j scales as 1 / bt,j, and we have that [formula] is invariant to the rescaling of individual coordinates.

The formula in the right-hand side of [\eqref=eq:update2] also shows that, similar to the updates studied in [\citep=McMahanS10] [\citep=DuchiHS10], the second type of regularizer induces a different learning rate for each component of [formula].

We now prove the following regret bounds.

If OMD is run using regularizers of type ([\ref=eq:f1]), then for any [formula]

[formula]

If OMD is run using regularizers of type ([\ref=eq:f2]), then for any [formula]

[formula]

For the first algorithm, note that m2  /  ptt  =  e, and setting [formula], we have qt(1 - pt) =  - pt. Further note that [formula], where ft - 1  ≤  ft because qt is decreasing, bt,i is increasing, and βt is also increasing. Hence, using the convexity of [formula] and Lemma [\ref=lemma:general], we may write

[formula]

For the second term in [\eqref=eq:newbound1], using the fact that [formula] and the L-Lipschitzness, we have

[formula]

where the second inequality uses the elementary inequality

[formula]

(see, e.g., [\cite=AuerCG02]). Hence we have

[formula]

Finally, note that

[formula]

The proof of the second bound is similar. First note that [formula], where ft - 1  ≤  ft is easily verified by inspection of ([\ref=eq:f2]). Using the convexity of [formula] and Lemma [\ref=lemma:general] we then obtain

[formula]

For the second term, we have

[formula]

where the last inequality uses ([\ref=eq:auer]). The proof is finished by noting that

[formula]

Note that both bounds are invariant with respect to arbitrary scaling of individual coordinates of the data points [formula] in the following sense: if the i-th feature is rescaled xt,i  →  c  xt,i for all t, then a corresponding rescaling ui  →  ui / c, leaves the bounds unchanged.

This invariance property is not exhibited by standard OMD run with non-adaptive regularizers, whose regret bounds are of the form [formula]. In particular, by an appropriate tuning of η the regret in Corollary [\ref=cor:convex_loss] for the regularizer type ([\ref=eq:f1]) is bounded by a quantity of the order of

[formula]

When the good [formula] are sparse, implying that the norms [formula] are small, this is always better than running standard OMD with a non-weighted q-norm regularizer. For q  →  1 (the best choice for the sparse [formula] case), this gives bounds of the form

[formula]

Indeed, for regularizer ([\ref=eq:f1]), we have

[formula]

Similar regularization functions are studied by [\cite=GOB11] although in a different context.

Recently, a framework for studying scale-invariance in online algorithms has been proposed by [\cite=RML13]. In the variant of their setting closest to our model, the sequence of instances [formula] is such that there exists an unknown diagonal matrix S for which [formula] for all t. The algorithm they propose is a form of projected gradient descent with a diagonal update (see Subsection [\ref=sec:diagonal] for an explanation of diagonal updates), where adaptivity is achieved by means of a variable learning rate rather than a variable regularizer. Their algorithm achieves a regret bound of the form

[formula]

for any [formula] such that [formula], where C is a parameter used by the algorithm. The quantity Δi is of the form

[formula]

where ti is the first time step where the i-th feature has a nonzero value.

Clearly enough, introducing the parameter C in our setting might allow a dynamical tuning of η which we could not afford in our analysis. However, a rough comparison can be made by considering the intermediate bound ([\ref=eq:tocompare]) for the regularizer of type ([\ref=eq:f2]). Tuning [formula] leads to the regret bound

[formula]

for any [formula] such that [formula]. This last bound now bears some resemblance to ([\ref=eq:lang]), although further study is clearly necessary to bring out the connections between these scale-invariant updates.

I added to the journal a new algorithm (Corollary 8) using the weighted p-norm derived by Koby. Basically it shows how it is very easy in our framework to consider extremely complex regularizer that are chancing over time. I used a weighted p-norm regularizer that adapts to - the time T - the number of non-zero elements observed in the samples - the maximum norms of the single features.

I think this example clearly shows the generality and the power of the approach.

KL

Let [formula] be a vector with non-zero elements and A > 0, Let [formula] be a vector in the set,

[formula]

We define the weighted Entropy of [formula] to be,

[formula]

The dual function of [formula] is the function,

[formula]

We show that [formula] is the conjugate of [formula]. We set wi the dual variables to be the gradient of [\eqref=weighted_log_sum_exp],

[formula]

Clearly [formula] belongs to the set [\eqref=weighted_simplex]. Substituting into the equation [formula] we obtain,

[formula]

where the first equality follows the contraint [formula] and the second follows log (ab) =  log (a) +  log (b).

The function [formula] is (1 / A)-strictly convex with respect to the weighted [formula] norm,

[formula]

We use Lemma 7 of [\cite=Nesterov:2009] stating that [formula]. The (i,j)th element of the Hessian of [formula] is given by [formula] and thus it suffices to show that

[formula]

for all vector [formula] and for all [formula] that satisfy [\eqref=weighted_simplex].

Similar to [\cite=Nesterov:2009] we use Cauchy-Schwartz inequality and get,

[formula]

The dual-norm of the weighted [formula] norm [formula] is given by

[formula]

By definition we have,

[formula]

Using Hölder inequality we have,

[formula]

Equality can be satisfied by setting wi  =  bi for [formula] and wj = 0 otherwise.

We now apply Lemma [\ref=lemma:general] set [formula], where b2t,i = b2t - 1,i  +  At|xt,i|2 and b1,i = a. The functions ft are At-strongly convex w.r.t. the norms [formula]. The dual functions of [formula] are [formula], while the dual norms are [formula].

We assume that [formula] is coordinate-wise monotonic. The regret is,

[formula]

Note, we assume that the comparison vector [formula] satisfy, [formula]

If At  =  const then the set above shrinks to zero. If [formula] then I think that At is exponential in t, and thus the log-term becomes linear. We want something in between.

Binary classification: aggressive and diagonal updates

In this section we show the first mistake bounds for Passive-Aggressive [\citep=CrammerDKSSS06] that improve on the standard Perceptron mistake bound, and also prove the first known bound for AROW with diagonal updates. Moreover, we recover --with some minor improvement-- the known bounds for the second-order Perceptron [\citep=Cesa-BianchiCG05] and non-diagonalized AROW [\citep=CrammerKD09].

We start by introducing binary classification as a special case of online convex optimization. Let [formula] be any finite-dimensional inner product space. Given a fixed but unknown sequence [formula] of examples [formula], let [formula] be the hinge loss [formula]. It is easy to verify that the hinge loss satisfies the following condition:

[formula]

Note that when [formula] the subgradient notation is redundant, as [formula] is the singleton [formula]. In this secton, we apply the OMD algorithm to online binary classification by setting [formula] if [formula], and [formula] otherwise.

We prove bounds on the number of steps t in which the algorithm made a prediction mistake, defined by the condition [formula] or, equivalently, by [formula]. In the following, when the number T of prediction steps is understood from the context, we denote by M the subset of steps t such that [formula] and by M its cardinality. Similarly, we denote by U the set of margin error steps; that is, steps t where [formula] and [formula]. Also, we use U to denote the cardinality of U. Following a standard terminology, we call conservative or passive an algorithm that updates its classifier only on mistake steps, and aggressive an algorithm that updates its classifier both on mistake and margin-error steps.

First-order algorithms

We start by showing how our framework allows us to generalize and improve previous analyses for binary classification algorithms that use first-order aggressive updates. Let

[formula]

be the cumulative hinge loss of [formula] with respect to some sequence of examples. The next result provides a general mistake bound for first-order algorithms.

Assume OMD is run with ft = f, where f has domain [formula], is β-strongly convex with respect to the norm [formula], and satisfies [formula] for all [formula] and all [formula]. Further assume the input sequence is [formula] for some 0  ≤  ηt  ≤  1 such that ηt  =  1 whenever [formula]. Then, for all T  ≥  1,

[formula]

where M  =  |M|, [formula] and

[formula]

Fix any [formula]. Using the second bound of Lemma [\ref=cor:bound_hinge_loss] in the Appendix, with the assumption ηt  =  1 when t∈M, we get

[formula]

where we have used the fact that Xt  ≤  XT for all [formula]. Solving for M we get

[formula]

with [formula], and

[formula]

We further upper bound the right-hand side of ([\ref=eq:gen_aggr_bound2]) using the elementary inequality [formula] for all a > 0 and b  ≥   - a. This gives

[formula]

Applying the inequality [formula] and rearranging gives the desired bound.

The p-norm Perceptron of [\cite=Gentile03] is obtained by running OMD in conservative mode with [formula] for 1  <  p  ≤  2. In this case we have [formula], [formula] where [formula], and β  =  p - 1 because [formula] is (p - 1)-strongly convex with respect to [formula] for 1  <  p  ≤  2, see [\cite=Shalev-Shwartz07]. Hence Corollary [\ref=cor:gen-aggr] delivers the mistake bound of [\cite=Gentile03].

However, the term D in the bound of Corollary [\ref=cor:gen-aggr] can also be negative. We can minimize it, subject to 0  ≤  ηt  ≤  1, by setting

[formula]

This tuning of ηt is quite similar to that of the Passive-Aggressive algorithm (type I) of [\cite=CrammerDKSSS06]. In fact for [formula] we would have

[formula]

while the update rule for PA-I is

[formula]

The mistake bound of Corollary [\ref=cor:gen-aggr] is however better than the aggressive bounds for PA-I of [\cite=CrammerDKSSS06] and [\cite=Shalev-Shwartz07]. Indeed, while the PA-I bounds are generally worse than the Perceptron mistake bound

[formula]

as discussed by [\cite=CrammerDKSSS06], our bound is better as soon as D  <  0. Hence, it can be viewed as the first theoretical evidence in support of aggressive updates. It also improves over previous attempts to justify aggressive updates in [\citep=OrabonaKC09] [\citep=JieOFCC10].

Second-order algorithms

We now move on to the analysis of second-order algorithms for binary classification. Here we use [formula] and let the inner product [formula] be the standard dot product [formula].

Second-order algorithms for binary classification are online variants of Ridge regression. Recall that the Ridge regression linear predictor is defined by

[formula]

The closed-form expression for [formula], involving the design matrix [formula] and the label vector [formula], is given by [formula]. The second-order Perceptron (see below) uses this weight [formula], but St and [formula] only contain the examples [formula] such that [formula]. Namely, those previous examples on which a mistake occurred. In this sense, it is an online variant of the Ridge regression algorithm.

In practice, second-order algorithms may perform better than their first-order counterparts, such as the algorithms in the Perceptron family. There are two basic second-order algorithms: the second-order Perceptron of [\cite=Cesa-BianchiCG05] and the AROW algorithm of [\cite=CrammerKD09]. We show that both of them are instances of OMD and recover their mistake bounds as special cases of our analysis.

Let [formula] and [formula], where A0 = I and [formula] with r > 0. Each dual function f*t is given by [formula]. The functions ft are 1-strongly convex with respect to the norm [formula] with dual norm [formula].

The conservative version of OMD run with ft chosen as above and r = 1 corresponds to the second-order Perceptron. The aggressive version corresponds instead to AROW with a minor difference. Let [formula]. Then for AROW we have [formula] whereas for OMD it holds that [formula], where we used the Woodbury identity and set [formula]. Note that the sign of [formula] is the same for both algorithms, but OMD updates when [formula] while AROW updates when ytmt  ≤  1. Typically, for t large the value of χt is small and the two algorithms behave similarly.

In order to derive a mistake bound for OMD run with [formula], first observe that using the Woodbury identity we have

[formula]

Hence, using the second bound of Lemma [\ref=cor:bound_hinge_loss] in the Appendix, and setting ηt = 1, we obtain

[formula]

for all [formula].

This bound improves slightly over the known bound for AROW in the last sum in the square root. In fact in AROW we have the term U, while here we have

[formula]

where the first inequality holds because t∈M implies ytmt  ≤  0, which in turn implies mt(2ryt  -  mt)  ≤  0. In the conservative case, when [formula], the bound specializes to the standard second-order Perceptron bound.

Diagonal updates

Computing f*t in AROW and the second-order Perceptron requires inverting At, which can be done from A- 1t - 1 in time quadratic in d. A much better scaling, linear in d, can be obtained when the algorithm use a diagonalized version of At. We now use Corollary [\ref=cor:bound_hinge_loss] to prove a mistake bound for the diagonal version of the second-order Perceptron. Denote [formula] be the diagonal matrix that agrees with At on the diagonal, where At is defined as before, and let [formula]. Setting ηt = 1, using the second bound of Lemma [\ref=cor:bound_hinge_loss], and using also Lemma [\ref=lemma:log_diagonal], we have

[formula]

This allows us to theoretically analyze the cases where this algorithm could be advantageous. For example, features of textual data are typically binary, and it is often the case that most of the features are zero most of the time. On the other hand, these "rare" features are usually the most informative ones --see, e.g., the discussion of [\cite=CrammerDP08a] [\cite=Crammer:2012:CLC:2343676.2343704].

Figure [\ref=fig:nlp] shows the number of times each feature (word) appears in two sentiment datasets vs. the word rank. Clearly, there are a few very frequent words and many rare words. These exact properties originally motivated the CW and AROW algorithms, and now our analysis provides a theoretical justification. Concretely, the above considerations support the assumption that the optimal hyperplane [formula] satisfies

[formula]

where I is the set of informative and rare features, and s is the maximum number of times these features appear in the sequence. Running the diagonal version of the second order Pereptron so that [formula], and assuming that

[formula]

the last term in the mistake bound ([\ref=eq:diag-bound]) can be re-written as

[formula]

where we calculated the maximum of the sum, given the constraint

[formula]

We can now use Corollary [\ref=corollary:log2] in the Appendix to obtain

[formula]

Hence, when the hypothesis ([\ref=eq:nlp]) is verified, the number of mistakes of the diagonal version of AROW depends on [formula] rather than on [formula].

Diagonal updates for online convex optimization were also proposed and analyzed by [\citep=McMahanS10] [\citep=DuchiHS10] [\citep=RML13]. When instantiated to the binary classification setting studied in this section, their analysis delivers regret bounds which are not comparable to ours.

Following [\citep=Cesa-BianchiCG05], we can find the conditions on s ensuring a mistake bound better than the Perceptron's bound ([\ref=eq:percbound]). Using the stronger "implicit" version [formula] of the Perceptron bound, a sufficient condition is

[formula]

We now set r to be to value minimizing the left-hand side of this inequality. This optimal setting is

[formula]

where we assume [formula]. We then use [\cite=Cesa-BianchiCG05] to prove that this setting of r is a sufficient condition to have ([\ref=eq:diag_vs_perc]) verified.

A New Adaptive Second Order Algorithm

We now introduce a new algorithm with an update rule that interpolates from adaptive-second-order-information to fixed-second-order-information. We start from the first bound in Corollary [\ref=cor:bound_hinge_loss]. We set [formula], where [formula], and A0 = I. This is similar to the regularization used in AROW and SOP, but here we have rt > 0 changing over time. Again, denote [formula], and set ηt = 1. With this choices, we obtain the bound

[formula]

that holds for any λ > 0 and any choice of rt > 0. We would like to choose rt at each step to minimize the bound, in particular to have a small value of the sum [formula]. Altough we do not know the values of [formula] and λ, still we can have a good trade-off setting [formula] when [formula] and rt =  +   ∞   otherwise. Here b is a parameter. With this choice we have that [formula], and [formula], when [formula]. Hence we have

[formula]

where in the last inequality we used an extension of Lemma 4 in [\cite=CrammerKD09] to varying values of rt. Tuning λ we have

[formula]

This algorithm interpolates between a second order algorithm with adaptive second order information, like AROW, and one with a fixed second order information. Even the bound is in between these two worlds. In particular the matrix At is updated only if [formula], preventing its eigenvalues from growing too much, as in AROW/SOP. We thus call this algorithm NAROW, since its is a new adaptive algorithm, which narrows the range of possible eigenvalues of the matrix At. We illustrate empirically its properties in the next section.

Conclusions

We proposed a framework for online convex optimization combining online mirror descent with time-varying regularizers. This allowed us to view second-order algorithms (such as the Vovk-Azoury-Warmuth algorithm, the second-order Perceptron, and the AROW algorithm) and algorithms for composite losses as special cases of mirror descent. Our analysis also captures second-order variants that only employ the diagonal elements of the second order information matrix, a result which was not within reach of the previous techniques.

Within our framework, we also derived and analyzed new regularizers based on an adaptive weighted version of the p-norm Perceptron. These regularizers generate instances of OMD that are both efficient to implement and invariant to rescaling of individual coordinates in the data. In the case of sparse targets, the corresponding instances of OMD achieve performance bounds better than that of OMD with 1-norm regularization.

We also improved previous bounds for existing first-order algorithms. For example, we were able to formally explain the phenomenon according to which aggressive algorithms often exhibit better empirical performance than their conservative counterparts. Specifically, our refined analysis provides a bound for Passive-Aggressive (PA-I) that is never worse (and sometimes better) than the Perceptron bound.

One interesting direction to pursue is the derivation and analysis of algorithms based on time-varying versions of the entropic regularizers used by the EG and Winnow algorithms. A remarkable recent result along these lines is the work of [\cite=SteinhardtL14], in which a time-varying entropic regularizers is used to obtain an improved version of EG for the prediction with experts setting (a special case of online convex optimization).

More in general, it would be useful to devise a more systematic approach to the design of adaptive regularizers enjoying a given set of desired properties, such as invariance to rescalings. This should help in obtaining more examples of adaptation mechanisms that are not based on second-order information.

Acknowledgements

The second author gratefully acknowledges partial support by an Israeli Science Foundation grant ISF-1567/10. The third author acknowledges partial support by MIUR (project ARS TechnoMedia, PRIN 2010-2011, grant no. 2010N5K7EB 003).

Technical lemmas

Define [formula], where A is an invertible matrix. First note that the Fenchel conjugate of [formula] is [formula] --see for example [\citep=BauschkeC11]. Hence, the Fenchel conjugate of f is obtained by setting: [formula], [formula], and by using the known Fenchel conjugate of g.

In order to show the second part, using [\citep=Shalev-Shwartz07] we may prove strong convexity of h w.r.t. a norm [formula] by showing that

[formula]

Moreover, [\citep=Shalev-Shwartz07] proves that, for any [formula], we have

[formula]

Putting together [\eqref=eq:lemmweightpnorm1] and [\eqref=eq:lemmweightpnorm2] and the same setting of A and g used above, we have that the strong convexity of f.

We now prove that the dual norm of [formula] is [formula]. By definition of dual norm,

[formula]

where 1 / q + 1 / p = 1. Writing the last norm explicitly and observing that p = q / (q - 1),

[formula]

which concludes the proof.

Assume OMD is run with functions [formula] defined on [formula] and such that each ft is βt-strongly convex with respect to the norm [formula] and [formula] for all [formula] and all [formula]. For each [formula] let [formula] be the dual norm of [formula]. Assume further the input sequence is [formula] for some ηt  >  0, where [formula], [formula] implies [formula], and [formula] satisfies ([\ref=eq:hinge_condition]). Then, for all T  ≥  1,

[formula]

for any [formula] and any λ > 0, where

[formula]

In particular, choosing the optimal λ, we obtain

[formula]

We apply Lemma [\ref=lemma:general] with [formula] and using [formula] for any λ  >  0,

[formula]

Since [formula] implies [formula], and using ([\ref=eq:hinge_condition]),

[formula]

Dividing by λ and rearranging gives the first bound. The second bound is obtained by choosing the λ that makes equal the last two terms in the right-hand side of ([\ref=eq:pre-mb]).

For all [formula] let [formula] where A0 = I and [formula] for some r  >  0. Then

[formula]

Consider the sequence at  ≥  0 and define [formula] with a0 > 0. The concavity of the logarithm implies [formula] for all a,b > 0. Hence we have

[formula]

Using the above and the definition of Dt, we obtain

[formula]

We conclude the appendix by proving the results required to solve the implicit logarithmic equations of Section [\ref=sec:diagonal]. We use the following result --see [\citep=OrabonaCG12].

Let a,x > 0 be such that x  ≤  a ln x. Then for all n  >  1

[formula]

This allows to prove the following easy corollaries.

For all a,b,c,d,x > 0 such that x  ≤  a ln (bx + c) + d, we have

[formula]

For all a,b,c,d,x > 0 such that

[formula]

we have

[formula]

Assumption ([\ref=eq:log2_1]) implies

[formula]

From Corollary [\ref=corollary:log1] we have that if f,g,h,i,y > 0 satisfy y  ≤  f ln (gx + h) + i, then

[formula]

where we have used the elementary inequality [formula] for all y  ≥  0. Applying the above to ([\ref=eq:log2_2]) we obtain

[formula]

which implies

[formula]

Note that we have repeatedly used the elementary inequality [formula]. Choosing n = 2 and applying ([\ref=eq:log2_3]) to ([\ref=eq:log2_1]) we get

[formula]

concluding the proof.