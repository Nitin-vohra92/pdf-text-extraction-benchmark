=4

Principal Nested Spheres for Time Warped Functional Data Analysis

Title

Keywords: Functional data variability, Time warping, Principal Nested Spheres

Introduction

A common variability in functional data is that prominent features in the functions vary in position from one sample to another, such as the timing variation of the adolescent growth spurt in human growth curves. This positional, or phase, variation is called the . Another important component of variability in functional data is the amplitude variation, or the , such as the height difference among the individuals.

There is a large literature on statistical analysis of functions, such as Kneip and Gasser (1992) [\cite=KT], Locantore et al (1999) [\cite=LM]. A general overview of functional data analysis is provided by Ramsay and Silverman (2002 [\cite=ramsay2002applied] and 2005 [\cite=FDA]). Plenty of useful tools and methods are available, such as the Functional Principal Component Analysis (FPCA), with many important applications in a wide variety of scientific fields. One open problem in functional data analysis is that, in those traditional approaches, the functional data are analyzed under the [formula] metric, which tends to strongly focus on the vertical variation. The horizontal variation cannot be easily understood in these vertical analyses. Section [\ref=SEC:FPCA] gives examples illustrating the shortcoming of conventional FPCA.

The main purpose of this paper is to find an improved method for the , i.e. the analysis of the horizontal variation. Considering the special spherical structure of the horizontal variation (see Section [\ref=SEC:hSRVF] for details), we propose to use an approach involving Principal Nested Spheres (PNS) introduced by Jung et al (2010) [\cite=pns]. Comparison with several other popular approaches, such as the FPCA, suggests improved efficiency of PNS for horizontal analysis. A toy example (the left panel of Figure [\ref=FIG:align]) is used to illustrate the advantages of the PNS approach.

We find Object Oriented Data Analysis, introduced by Wang and Marron (2007) [\cite=ooda], very helpful in studying horizontal variation. The data objects are understood as the of the analysis. In this study, they are functions. Several different types of data objects (or functions) are considered in this paper for horizontal analysis. These different choices of data objects and the corresponding horizontal analyses are discussed in Section [\ref=SEC:ha].

Function Alignment Based on the Fisher Rao Metric

A useful approach to horizontal analysis is through the idea of . Some pioneering work in this area includes Ramsay and Li (1998) [\cite=RL], Gervini and Gasser (2004) [\cite=GG], Liu and Mueller (2004) [\cite=FR_liu], Kneip and Ramsay (2008) [\cite=KR], Tong and Mueller (2008) [\cite=TM]. The basic idea is to first separate the vertical and the horizontal variation through , or curve registration. In particular, consider a collection of functions fi(t), i = 1,2,...,n, having both vertical and horizontal variation, such as the bimodal functions shown in Figure [\ref=FIG:align] (left). If these functions are well aligned by warping the domain properly, then the horizontal and the vertical variation can be separately captured by the domain warping functions γi(t) and the resulting aligned functions fi(γi(t)), respectively. For this toy example, such a set of warping functions and the corresponding aligned functions are shown in the middle and right panels respectively (details about finding those warping functions are discussed later). Then, the horizontal analysis can be done by studying those warping functions.

A crucial step in the function alignment is to find appropriate domain warping functions. Consider two functions f1 and f2. Most of the past approaches involve solving [formula] to align f2 to f1, where [formula] is the standard [formula] metric, i.e. [formula]. However, this criterion is problematic, since the objective function is not symmetric in the sense that aligning f1 to f2 leads to a different optimal minimum. To illustrate this point, Figure [\ref=FIG:alignIssue] shows a simple example of aligning two step functions. It is seen that aligning f2 to f1 (middle) and aligning f1 to f2 (right) are different under the [formula] metric. The difference between the horizontally hatched blue area in Panel (2, 2) and the vertically hatched pink area in Panel (2, 3) indicates that the two corresponding objective functions [formula] and [formula] are not equal. This is because the [formula] metric is not invariant under re-parameterization, or domain warping. In particular, [formula]. A more appropriate metric is the Fisher Rao metric. See Srivastava et al (2011) [\cite=FR] for definition and relevant theory. This metric is derived from a Riemannian metric first introduced by C. R. Rao (1945) [\cite=FRdist]. A nice property of the Fisher Rao metric is that it is warping-invariant. In fact, Cencov (1982) [\cite=FRdist4] proved that it is the only metric that has this property. Thus, we propose to use the Fisher Rao metric to align functions for the purpose of the horizontal analysis.

Calculations based on the Fisher Rao metric are difficult. In practice, a convenient square-root velocity function (SRVF) representation, i.e. transforming the function f(t) to [formula], simplifies the Fisher Rao framework. Under the SRVF representation, the Fisher Rao metric becomes the standard [formula] metric, and thus, standard statistical tools for the [formula] space, such as mean, covariance and principal components, can be used. As an example of function alignment based on the Fisher Rao metric, the warping functions (middle) for the toy data (left) in Figure [\ref=FIG:align] are found by an automatic and unsupervised approach based on this metric, proposed by Srivastava et al (2011) [\cite=FR].

PNS for Spherical Structure of Horizontal SRVFs

One major benefit of the SRVF representation is that it transforms the manifold of the warping functions to a Hilbert sphere. In particular, suppose the domain of the functions fi, i = 1,2,...,n, to be

[formula]

). Let γi be a warping function for fi, i.e. γi∈Γ  =  {γ:   →  [0,1]| γ(0) = 0, γ(1)  =  1 and γ is a diffeomorphism}, where a refers to a bijective differentiable function whose inverse is also differentiable. Then the SRVF ψi of the warping function γi, referred to later as the , can be written as [formula]. Noting that [formula], these horizontal SRVFs naturally lie on the surface of a Hilbert unit sphere.

Due to the spherical structure of the horizontal SRVFs, we propose to use the PNS method for horizontal analysis. Introduced by Jung et al (2010) [\cite=pns], PNS is an extension of PCA for curved manifolds, especially for high dimensional spheres. This method finds a sequence of subspheres that best approximates the data using a backward approach, which starts with the high dimensional sphere and finds the best fitting subsphere of one dimension lower at each step. See Marron et al (2010) [\cite=BackwardPCA] for more discussion of backward PCA. It has been shown in a number of cases that PNS can provide more effective analysis of manifold data than many other analogous approaches. See Pizer et al (2011) [\cite=PGAskeletal] for such an example in the study of 3D shapes.

For comparison purposes, another popular approach for data lying in curved manifolds, Principal Geodesic Analysis (PGA) proposed by Fletcher et al (2004) [\cite=PGA], is also investigated in this paper. Unlike PNS, it is a forward approach, starting with the (also called fréchet or geodesic mean). In the following horizontal analysis, the Karcher mean refers to the representer defined in Definition 3 of Srivastava et al (2011) [\cite=FR]. PGA approximates the spherical surface by a tangent hyperplane centered at the Karcher mean. By performing PCA on this tangent plane, PGA finds the principal geodesics (i.e. great spheres) passing through the mean that best fit the data.

In contrast to PGA, the PNS method finds the best fitting subsphere regardless of whether it is a great sphere or not. When the major variance is non-geodesic, PNS tends to find the best-fitting small spheres instead of only great spheres. Thus, when the data variability on the sphere is big enough, the PNS can give a much more effective decomposition of this variability than PGA. On the other hand, if the data variability is small, the PNS method does not improve much over the PGA method. This is because in this case the data do not have much curvature and can be approximated by a tangent plane well enough. In the following discussion, we focus on examples with big horizontal variation.

Horizontal Analyses

This section compares different horizontal analyses with the toy example in Figure [\ref=FIG:align] (left), where the functions have big horizontal variation and the PNS method gives very useful improvement over PGA.

Figure [\ref=FIG:toyExample] (left) visualizes the pure horizontal shifts of the peaks for this toy example, which is done via warping the Karcher mean function (red curve in the right panel) by the Fisher Rao warping functions in Figure [\ref=FIG:align] (middle). These functions will be referred to later as the , denoted by hi, i = 1,2,...,n.

Three different types of data objects for horizontal analysis are studied in this section: the horizontally shifted functions hi, the warping functions γi and the horizontal SRVFs ψi. Oriented by the choice of data objects, four different horizontal analyses have been performed on the toy data: (1) FPCA of the horizontally shifted functions hi; (2) FPCA of the warping functions γi; (3) PGA of the horizontal SRVFs ψi; (4) PNS of the horizontal SRVFs ψi. The first two approaches, using the conventional FPCA, are discussed in Section [\ref=SEC:FPCA]. The latter two manifold approaches, motivated by the spherical structure of the horizontal SRVFs, are discussed in Section [\ref=SEC:SRVF]. Section [\ref=SEC:conclusion] summarizes the comparison of these four approaches.

Conventional FPCA

An intuitive way to understand the horizontal variation of the toy data is to analyze either the horizontally shifted functions hi or the warping functions γi. As the FPCA is one of the most widely used statistical tools for functional data analysis, this section discusses FPCA of the two different types of functions respectively. It is seen that the conventional FPCA is rarely a good option for horizontal analysis.

FPCA of Horizontally Shifted Functions

FPCA involves centering data with the cross-sectional mean based on the [formula] metric. However, in the case of big horizontal variability, this cross-sectional mean may hardly be useful in capturing the underlying shape of the functions. In the toy example, the unimodal cross-sectional mean (the blue dashed line in the right panel of Figure [\ref=FIG:toyExample]) is a poor representative of the bimodal functions hi, while the bimodal Karcher mean (the red solid line) based on the Fisher Rao metric is much more satisfying. Thus, the resulting PC projections from the FPCA of the horizontally shifted functions hi are difficult to interpret. See panels in the first column of Figure [\ref=FIG:proj] for the first two PC projections. Therefore, the FPCA of the hi is not an appropriate approach for horizontal analysis.

FPCA of Warping Functions

One challenge of performing the FPCA on the domain warping functions is how to interpret each component. Better interpretation comes from transforming the decomposition of the warping functions into the original function space, i.e. warping the Karcher mean function by the PC projections. The second column of Figure [\ref=FIG:proj] shows the first two transformed PC projections for the toy example. These two components provide a much more useful summary of the apparent horizontal variation in the raw data than the previous ones from the FPCA of the hi (first column). The first component reflects the horizontal shifts of the peaks, while the second one is about the horizontal distance between the two peaks.

However, this approach has a serious weakness. That is, the PC projection of a warping function is not necessarily bijective, and thus, not a warping function. In other words, the conventional FPCA leaves the space Γ of warping functions. To illustrate this, Figure [\ref=FIG:warpIssue] shows the FPCA of a set of simple two-dimensional warping functions γi (left panel), each of which is determined by two values, γi(1 / 3) and γi(2 / 3). It is seen in the right two panels that some of the PC1 projections (cyan) have a decreasing part, i.e. γi(1 / 3) > γi(2 / 3). Warping the Karcher mean function with these non-warping PC projections is problematic. Computationally, this causes the wiggly right end of the yellow and the red functions in Panel (1, 2) of Figure [\ref=FIG:proj].

Analyses on SRVF Manifold

The following analyses avoid the problem shown in Section [\ref=SEC:FPCAWarp], by appropriately using the spherical structure of the horizontal SRVFs ψi. The idea is to first decompose the variability of the spherical SRVFs and then transform the projections of the SRVF components back to the warping function space Γ using the formula [formula], where ξ is a point on the SRVF sphere. It can be easily checked that γξ∈Γ. Finally, the decomposition of the horizontal variation of the original functions can be obtained via warping the Karcher mean function with the transformed SRVF projections. The following discussion shows how manifold approaches, especially the PNS approach, can work better for the horizontal analysis than the conventional FPCA.

PGA of Horizontal SRVFs

The third column in Figure [\ref=FIG:proj] shows the first two components of the horizontal variation in the toy data, based on the PGA of the horizontal SRVFs. Compared with the previous FPCA results (the first two columns), this approach gives a much better decomposition of the horizontal variation. The first component captures most of the horizontal shifts of the peaks. The second one is similar to the PC2 of the warping functions in Panel (2, 2), but has a visually smaller horizontal variation. This shows more of the underlying signal in the data has been moved to PG1, i.e. better .

PNS of Horizontal SRVFs

The first two components of the horizontal variation in the toy data based on the PNS of the horizontal SRVFs are shown in the fourth column in Figure [\ref=FIG:proj]. Results from this decomposition give more signal compression than those from the previous analyses. The first component simultaneously captures both the mode of peak location and the mode of distance between peaks. The two components previously needed have been reduced to one. Among the four panels in the first row, these PNS1 projections explain the horizontal variation of the original bimodal functions best, as they are almost identical to the raw horizontal warps of the Karcher mean, shown in the left panel of Figure [\ref=FIG:toyExample]. Very little variability is left for the second PNS component to explain. This suggests that the horizontal variability is almost one dimensional in some sense, which is consistent with the fact that the warping function γi in this toy example can be summarized by a single parameter ai. In particular, these were generated as [formula], for ai∈[ -  5,5].

For further insight of this type, Figure [\ref=FIG:scores] shows the score scatter plot of the first two PNS components (third panel), which has a similar pattern to that of the scatter plot of the second and the third PG scores (second panel). This is because most of the horizontal SRVF variation is along some small circle, which is captured by the PNS1. However, PGA needs two principal geodesics (the first two; see the first panel for the corresponding score scatter plot) to capture the curvature of this small circle. In other words, the first two PNS components are able to explain the data variability captured by the first three components in the PGA. Thus, the PNS approach gives better signal compression by using more flexible components.

The right panel in Figure [\ref=FIG:scores] visualizes the proportion of variance explained by the first three components in each of the four analyses. It is seen that the PNS of the horizontal SRVFs (red, with the highest first point) and the FPCA of the warping functions (cyan, with the second highest first point) are the top two most efficient approaches, in the sense of explaining a higher proportion of data variability using a lower number of components. However, considering the difficulty in interpreting the FPCA decomposition (see Section [\ref=SEC:FPCAWarp]), PNS is the best approach for the horizontal analysis.

Conclusions

This paper aimed at finding an appropriate method for horizontal analysis of functional data, where the horizontal variation is separated from the vertical variation using a domain-warping method based on the Fisher Rao metric. Four different approaches have been discussed, including two conventional FPCA approaches (FPCA of the horizontally shifted functions of the Karcher mean and FPCA of the warping functions) and two manifold approaches based on the spherical structure of the horizontal SRVFs (PNS and PGA). A toy example of big horizontal variation is used to compare these approaches. The manifold approaches are generally better than the FPCA approaches, and the PNS works the best in terms of both the signal compression and the interpretability of the results. As a conclusion, we propose to use the PNS approach for horizontal analysis, especially when the horizontal variability is large.