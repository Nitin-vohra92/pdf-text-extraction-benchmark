Remark

Unsupervised model-free representation learning

Introduction

In many learning and control problems one has to deal with the situation where the input data is high-dimensional and abundant, but the feedback for the learning algorithm is scarce or absent. In such situations, finding the right representation of the data can be the key to solving the problem. The focus of this work is on problems in which all or a large significant part of the relevant information is in the time-series dependence of the process. This is the case in many applications, starting with speech or hand-written text recognition, and, more generally, including control and learning problems in which the input is a stream of sensor data of an agent interacting with its environment.

A more formal exposition of the problem follows. First, assume that we are given a stationary sequence [formula] where Xi belong to a large (continuous, high-dimensional) space X. For the moment, assume that the problem is non-interactive (the control part is introduced later). We are looking for a compact representation [formula] where f(Xi) belong to a small (for example, finite) space Y.

Let us first consider the following " ideal" situation. There exists a function f:X  →  Y such that each random variable Xi is independent of the rest of the sample [formula] given f(Xi) (for each [formula]). That is, all the time-series dependence is in the sequence [formula], and, given this sequence, the original sequence [formula] can be considered as noise, in the sense that Xi are conditionally independent. In this case we say that [formula] are conditionally independent given [formula]. We can show that in this "ideal" situation the function f maximizes the following information criterion

[formula]

where h(f(X0)) is the Shannon entropy of the first element and h∞ is the entropy rate of the (stationary) time series [formula]. This means that for any other function g:X  →  Y we have I∞(f)  ≥  I∞(g), with equality if and only [formula] are also conditionally independent given [formula].

This allows us to pass to the non-ideal situation, in which there is no function f that satisfies the conditional independence criterion. Given a set of functions mapping X to Y the function that preserves the most of the time-series dependence can be defined as the one that maximizes ([\ref=eq:tsi]). Such a function f can be said to preserve the most of time-series dependence of the original time series [formula] (as opposed to the ideal case, in which such a function f preserves all of the time-series dependence).

For a given function f, the quantity ([\ref=eq:tsi]) can be estimated empirically. Moreover, we can show that under certain conditions it is possible to estimate ([\ref=eq:tsi]) uniformly over a set F of functions f:X  →  Y. Importantly, the estimation can be carried out without estimating the distribution of the original time series [formula].

Of particular interest (especially to control problems) is the case where the time series [formula] form a Markov process. In this case, in the "ideal" situation (when [formula] are conditionally independent given [formula]) one can show that the process [formula] is also Markov, and I∞(f) = I1(f): = h(f(X0)) - h(f(X1)|f(X0)). In general, we show that in the Markov case to select a function that maximizes I∞(f) it is enough to maximize I1(f).

Next, assume that at each time step i we are allowed to take an action Ai, and the next observation Xi + 1 depends not only on [formula] but also on the actions [formula]. Thus, we are considering the control problem, and the time series [formula] do not have to be stationary any more. In this situation, the time-series information I∞(f) becomes dependent on the policy of the learner (that is, on the way the actions are chosen). However, we can show that in the Markov case, under some mild connectivity conditions, to select the function f that maximizes I∞(f), it is enough to consider just one policy that takes all actions with non-zero probability. This means that one can find the representation function f while executing a random policy, without any feedback from the environment (i.e., without rewards). One can then use this representation to solve the target control problem more easily.

Prior work. Learning representations, feature learning, model learning, as well as model and feature selection, are different variants and different names of the same general problem: making the data more amenable to learning. From the vast literature available on these problems we only mention a few that are somehow related to the approach in this work. First, note that in our "ideal" (conditional independence) case, if we further assume that (Xi) form a Markov chain, then we get a special case of Hidden Markov models (HMM) [\cite=Rabiner:86], with (unobserved) f(Xi) being hidden states. Indeed, as it was mentioned, in this case f(Xi) form a Markov chain (Section [\ref=s:mark]), and thus can be considered hidden states; the dependence between f(Xi) and Xi is deterministic, as opposed to randomized in HMM, so we get a special case. Thus, the general case (non-ideal situation, Xi are not necessarily Markov) can be considered a generalization of HMMs. A related approach to finding representations in HMMs is that of [\cite=Hutter:10] (see also [\cite=Hutter:09c]). The setting of [\cite=Hutter:10] can be related to our setting in Sections [\ref=s:mark], [\ref=s:cont]. Specifically, [\cite=Hutter:10] considers environments generated by HMMs, where the hidden states are deterministic functions of the observed variables. The approach of [\cite=Hutter:10] is then to maximize a penalized likelihood function, where the penalty is for larger state spaces. Consistency results are obtained for the case of finite or countably infinite sets of maps (representation functions), which are given by so-called finite-state machines of bounded memory, one of which is the true environment.

From a different perspective, if Xi are independent and identically distributed and, instead of the time-series dependence (which is absent in this case), we want to preserve as much as possible of the information about another sequence of variables (labels) [formula], then one can arrive at the information bottleneck method [\cite=Tishby:99]. The information bottleneck method can, in turn, be seen as a generalization of the rate-distortion theory of Shannon [\cite=Shannon:59]. Applied to dynamical systems, the information bottleneck method can be formulated [\cite=Creutzig:09] as follows: minimize [formula], where β is a parameter. A related idea is that of causal states [\cite=Shalizi:01]: two histories belong to the same causal state iff they give the same conditional distribution over futures. What distinguishes the approach of this work from those described, is that we never have to consider the probability distribution of the input time series Xi directly -- only through the distribution of the representations f(Xi). Thus, modelling or estimating Xi is not required; this is particularly important for empirical estimates.

For the control problem, to relate the proposed approach to others, first observe that in the case of an MDP, in the "ideal" scenario, that is, in the case when there exists a function f:X  →  Y such that [formula] are conditionally independent given [formula], then for any states x,x'∈X for which f(x) = f(x') all the transition probabilities are the same. In other words, states x,x'∈X for which f(x) = f(x') are equivalent in a very strong sense, and the function f can be viewed as state aggregation. Generalizations of this equivalence and aggregation (in the presence of rewards or costs) are studied in the bisimulation and homomorphism literature [\cite=Givan:03] [\cite=Ferns:06] [\cite=Taylor:09] [\cite=Ravindran:03]. The main difference of our approach (besides the absence of rewards) is in the treatment of approximate (non-ideal) cases and in the way we propose to find the representation (aggregation) functions. In bisimulation this is approached via a metric on the state space defined using a distance between the transition (and reward) probability distributions, which then has to be estimated [\cite=Ferns:06] [\cite=Taylor:09]. In our approach, all that has to be estimated concerns the representations f(X), rather than the observations (states) X themselves.

In the context of supervised reinforcement learning (that is, in the presence of rewards), a related problem is that of finding a (concise) representation of the input space such that the resulting process on representations is Markovian [\cite=Maillard:11] [\cite=Maillard:13].

It should also be noted that the conditional independence property has been previously studied in a different context (classification) in [\cite=Ryabko:06condiid]. The latter work shows that if the objects [formula] are conditionally independent given the labels [formula] then, effectively, one can use classification methods developed to work in the case of i.i.d. object-label pairs. Combined with the results of this work this means that in the ideal (conditional independence) case one can decompose a learning problem into i.i.d. classification and learning the time-series dependence. It is also worth noting that the quantity ([\ref=eq:tsi]) has been studied in a different context: [\cite=BRyabko:06a] uses it to construct a statistical test for the hypothesis that a time series consists of independent and identically distributed variables. Furthermore, one can show (see below) that for stationary time series I∞(f) equals to the following mutual information [formula]; this characteristic of time series has been extensively studied [\cite=Gray:90].

Organization. The rest of the paper is organized as follows. Section [\ref=s:pr] introduces some notation and definitions. Section [\ref=s:main] introduces the model and gives the main results concerning representation functions for stationary time series. Section [\ref=s:mark] considers the special case of (stationary) Markov chains; Section [\ref=s:uni] presents results on uniform empirical approximation of time-series information. Finally, Section [\ref=s:cont] extends the model and results to the control problem. Some longer proofs are deferred to Section [\ref=s:proof].

Preliminaries

Let [formula] and [formula] be measurable spaces. X is assumed to be large (e.g., a high-dimensional Euclidean space) and Y small. For simplicity of exposition, we assume that Y is finite; however, the results can be extended to infinite (and continuous) spaces Y as well.

Time-series (or process) distributions are probability measures on the space [formula] of one-way infinite sequences (where [formula] is the Borel sigma-algebra of [formula]). We use the abbreviation X0..k for [formula]. A distribution ρ is stationary if ρ(X0..k∈A) = ρ(Xn + 1..n + k∈A) for all A∈FXk, [formula] (with FXk being the sigma-algebra of Xk).

A stationary distribution on [formula] can be uniquely extended to a distribution on [formula] (that is, to a time series [formula]); we will assume such an extension whenever necessary.

For a random variable Z denote h(Z) its entropy. Define h(f) as the entropy of f(X0)

[formula]

and hk(f) the k-order entropy of [formula]

[formula]

For stationary time series [formula] the entropy rate is defined as

[formula]

When we speak about conditional distributions the equality of distributions should be understood in the "almost sure" sense.

Time-series information for stationary distributions

This section describes the main results concerning representation functions for stationary time series. We first introduce the "ideal" situation in which [formula] are conditionally independent given [formula] for some function f:X  →  Y, and define time-series information. We then show that under this condition the function f maximizes time-series information.

We say that [formula] are conditionally independent given [formula], if for all n,k, and all [formula] Xn is independent of [formula] given f(Xn):

[formula]

The time-series information of a series [formula] is defined as

[formula]

We can also define k-order time-series information as follows

[formula]

The following lemma helps to understand the nature of the quantities I∞(f) and Ik(f).

If the time series [formula] is stationary then

[formula]

Denote Yi: = f(Xi). We have

[formula]

where the first equality follows from the stationarity of [formula] and for the last see, e.g., [\cite=Gray:90]

The following is the main result concerning representations of stationary time series. Its proof is given in section [\ref=s:proof].

Let [formula] be a stationary time series, and let f:X  →  Y be such that are conditionally independent given [formula]. Then for any g:X  →  Y we have I∞(f)  ≥  I∞(g), with equality if and only if [formula] are conditionally independent given [formula].

Thus, given a set F of representation functions f:X  →  Y, the function that is "closest" to satisfying the conditional independence property [\ref=d:ciid] can be defined as the one that maximizes ([\ref=eq:ti]). If the set F is finite and the time series [formula] is stationary, then it is possible to find the function that maximizes ([\ref=eq:ti]) given a large enough sample of the time series, without knowing anything about its distribution. Indeed, it suffices to have a consistent estimator for h0(f) and a consistent estimator for the entropy rate h∞(f). The former can be estimated using empirical plug-in estimates, and the latter using, for example, data compressors, see, for example, [\cite=BRyabko:06a] [\cite=BRyabko:05a].

The situation is more difficult if the space of representation functions is infinite (possibly uncountable); moreover, we would like to introduce learner's actions into the process, potentially making the the time series [formula] non-stationary.

These scenarios are considered in the following sections. For the control problem, a special role is played by Markov environments; we first look at the simplifications gained by making this assumption in the stationary case.

Time-series information for Markov chains

If the [formula] form a stationary (k-order) Markov process then the situation simplifies considerably. First of all, if [formula] are conditionally independent given [formula] then [formula] also form a stationary (k-order) Markov chain. Moreover, to find the function that maximizes the time-series information ([\ref=eq:tsi]) it is enough to find the function that maximizes a simpler quantity [formula], as the following theorem shows. In the theorem and henceforth, for the sake of simplicity of notation, we only consider the case k = 1; the general case is analogous.

Suppose that Xi form a stationary Markov process and [formula] are conditionally independent given [formula]. Then

[formula] also form a stationary Markov chain.

In this case I∞(f) is the mutual information between f(X0) and f(X1):

[formula]

and for any g:X  →  Y we have I1(f)  ≥  I1(g) with equality if and only if [formula] are conditionally independent given [formula].

We use the notation Yi: = f(Xi). For the first statement, observe that

[formula]

where we have used successively conditional independence, the Markov property for [formula] and again conditional independence.

For the second statement, first note that h∞ = h1 for Markov chains, implying ([\ref=eq:imar]). Next, for any g:X  →  Y the process g(Xi) is stationary, which implies h∞(g(X))  ≤  h1(g(X)). Thus, using Theorem [\ref=t:m], we obtain

Uniform approximation

Given an infinite (possibly uncountable) set F of functions f:X  →  Y, we want to find a function that maximizes I∞(f). Here we first consider the problem of approximating Ik(f), and then based on it proceed with the problem of approximating I∞(f).

Since we do not know Ik(f), we can select a function that maximizes the empirical estimate Îk(f). The question arises, under what conditions is this procedure consistent? The requirements we impose to obtain consistency of this procedure are of the following two types: first, the set F should be sufficiently small, and, second, the time series [formula] should be such that uniform (over F) convergence guarantees can be established. Here the first condition is formalized in terms of VC dimension, and the second in terms of mixing times. We show that, under these conditions, the empirical estimator is indeed consistent and learning-theory-style finite-sample performance guarantees can be established.

For a function f:X  →  Y and a sample [formula] define the following estimators. [formula] and analogously for [formula] and multivariate entropies.

For a process distribution ρ define the mixing coefficients

[formula]

where σ(..) denotes the sigma-algebra of the random variables in brackets.

When [formula] the process ρ is called absolutely regular; this condition is much stronger than ergodicity, but is much weaker than the i.i.d. assumption.

The general tool that we use to obtain performance guarantees in this section is the following bound that can be obtained from the results of [\cite=Karandikar:02]. Let F be a set of VC dimension d and let ρ be a stationary distribution. Then

[formula]

where tn is integer in 1..n and ln = n / tn . The parameters tn should be set according to the values of β in order to optimize the bound.

Furthermore, assume geometric β-mixing distributions, that is, β(ρ,t)  ≤  γt for some γ < 1. Letting [formula] the bound ([\ref=eq:mixtl]) becomes

[formula]

Geometric β-mixing properties can be demonstrated for large classes of (k-order) (PO)MDPs [\cite=hernandez:03], and for many other distributions.

Let the time series [formula] be generated by a stationary distribution ρ whose β-mixing coefficients satisfy β(ρ,t)  ≤  γt for some γ < 1. Let F be a set of functions f:X  →  Y such that for each y∈Y the VC dimension of the set [formula] is not greater than d. Then

[formula]

where h- 1 stands for the inverse of the binary entropy (and is of order h- 1(ε)  ~  ε  /   log (1 / ε)).

The proof is deferred to Section [\ref=s:proof].

We proceed to construct an estimator of I∞(g) which is uniformly consistent over a set F of functions g, provided the time series satisfies mixing conditions. To this end, denote δk(n) the right-hand side of ([\ref=eq:thmix]). Observe that for each fixed [formula], δk(n) decreases exponentially fast with n. Therefore, it is possible to find a non-decreasing sequence [formula] such that δkn(n) decreases exponentially fast with n, while kn  →    ∞  . Define

[formula]

Furthermore, observe that, for any stationary time series we have, by definition, h∞(g) =  lim k  →    ∞hk(g). For uniform approximation of I∞ we need this convergence to hold uniformly over the set F. This is akin to the mixing conditions, but, in general, does not follow from them. Thus, we strengthen the mixing conditions by requiring that the following holds

[formula]

The following statement is easy to show from Theorem [\ref=th:mix], the definition ([\ref=eq:hinf]) of Î∞ and ([\ref=eq:fuck]).

Under the conditions of Theorem [\ref=th:mix], if ([\ref=eq:fuck]) holds true then

[formula]

The active case: MDPs

In this section we introduce learner's actions into the protocol. The setting is a sequential interaction between the learner and the environment. Given are a space of observations X and of a space actions A, where A is assumed finite. At each time step [formula] the environment provides an observation Xi, the learner takes an action Ai, then the next observation Xi + 1 is provided, and so on. Each next observation Xi + 1 is generated according to some (unknown) probability distribution [formula]. Actions are generated by a probability distribution π that is called a policy; in general, it has the form [formula].

Note that we do not introduce costs or rewards into consideration. Thus, we are dealing with an unsupervised version of the problem; the goal is just to find a concise representation that preserves the dynamics of the problem.

For a policy π, an environment P and a measurable function f we say that [formula] are conditionally independent given [formula] under the policy π if

[formula]

for all [formula], and all [formula] such that ij  ≠  n, j = 1..k, where Pπ refers to the joint distribution of Xi and Ai generated according to P and π.

The focus in this section is on time-homogeneous Markov environments, that is, on Markov Decision Processes (MDPs). Thus, we assume that Xi + 1 only depends on Xi and Ai, that is, P can be identified with a function from X  ×  A to the space P(X) of probability distributions on X In this case observations Xi are called states.

A policy is called stationary if each action only depends on the current state; that is, [formula] where for each x∈X π(A|x) is a distribution over A.

Call an MDP admissible if any stationary policy π has a (unique up to sets of measure 0) stationary distribution Pπ over states. The notation [formula], etc. refers to the stationary distribution of the policy π.

For MDPs we introduce the following policy-independent definition of conditional independence.

For an admissible MDP and a measurable function f:X  →  Y we say that [formula] are conditionally independent given [formula] if, for every stationary policy π, [formula] are conditionally independent given f(Xi) under policy π.

Call a stationary policy π stochastic if π(a|x)  ≥  α > 0 for every x∈X and every a∈A.

Call an admissible MDP (weakly) connected if there exists a stationary policy π such that (equivalently: for every stochastic policy π) for any other stationary policy π' we have Pπ  ≫  Pπ' (that is, for any measurable S  ⊂  X  ×  A Pπ'(S) > 0 implies Pπ(S) > 0).

For discrete MDPs this definition coincides with the usual definition of weak connectedness (for any pair of states s1,s2 there is a policy that gets from s1 to s2 in a finite number of steps with non-zero probability).

Fix an admissible weakly connected MDP and a stationary stochastic policy π. Then [formula] are conditionally independent given [formula] if and only if [formula] are conditionally independent given [formula] under π.

We only have to prove the "if" part (the other part is obvious). Let π0 be any stationary policy. Introduce the notation Yi: = f(Xi) and

[formula]

We have to establish ([\ref=eq:ciida]) for Pπ0; note that since the process is Markov we can take k = 2, i1 = 1, i2 =  - 1 in ([\ref=eq:ciida]) w.l.o.g.; thus, we need to demonstrate

[formula]

Since the policy π is stochastic, the measure Pπ dominates Pπ0. Therefore, the following probability-one statements are non-vacuous: for all [formula], where the first equality follows from ([\ref=eq:ciida]), and the second follows from the fact that conditionally on the actions the distributions Pπ and Pπ0 coincide. Moreover, Thus, [formula] are conditionally independent given [formula] under π0; since π0 was chosen arbitrary, this concludes the proof.

Fix an admissible MDP and a stationary stochastic policy π. Assume that for some f:X  →  Y [formula] are conditionally independent given [formula]. If [formula] then [formula] are conditionally independent given [formula].

The statement follows from Theorems [\ref=t:m] and [\ref=th:mdp].

Consider the following scenario. A real-life control problem is given, in which an (average, discounted) cost has to be optimized. In addition, a simulator for this problem is available; running the simulator does not incur any costs, but also does not provide any information about the costs -- it only simulates the dynamics of the problem. Given such a simulator, and a set F of representation functions, one can first execute a random policy to find the best representation function f as the one that maximizes Î1(f). Under the conditions given in Section [\ref=s:uni], the resulting estimator is consistent. One can then use the representation function found to learn the optimal policy in the real problem (with costs).

The problem of solving (efficiently) both problems together -- learning the representation and the finding the optimal policy in a control problem -- is left for future work.

Longer proofs

First note that from the definition ([\ref=d:ciid]) of conditional independence and using the chain rule for entropies, it is easy to show that for any [formula] and for any (measurable) function f':X  →  Y we have

[formula]

so that

[formula]

Consider the following entropies and information (with straightforward definitions): h0(f,g), hk(f,g), Ik(f,g) and I∞(f,g). We will first show that

[formula]

The latter equality follows from the former and the definition of h∞. To prove the former we will consider the case k = 1; the general case is analogous. Introduce the short-hand notation Yi: = f(Xi),Zi: = g(Xi), [formula]. First note that

[formula]

Moreover,

[formula]

where the first equality is by definition, the second is the chain rule for entropy and the third follows from ([\ref=eq:magic2]) and conditional independence of Xi given f(Xi). Thus, from ([\ref=eq:h0fg]), ([\ref=eq:h1fg]) and the definition of I1(f) we get

[formula]

finishing the proof of ([\ref=eq:ik]).

To prove the theorem it remains to show that, if [formula] are not conditionally independent given [formula] then I∞(f) > I∞(g). For that it is enough to show that

[formula]

from some k on. Assume that [formula] are not conditionally independent given [formula], so that

[formula]

for some k,n and [formula]. By stationarity, we obtain from ([\ref=eq:ne2]) that there exists [formula] such that

[formula]

We will show that ([\ref=eq:go2]) holds for all k for which ([\ref=eq:ne22]) holds. Clearly, if ([\ref=eq:ne22]) holds for [formula] then it also holds for all k' > k. Thus, it is enough to consider the case k = 1; the general case is analogous. With this simplification in mind, and using our Y and Z notation, note that ([\ref=eq:ne22]) implies that

[formula]

for otherwise we would get P(X0|Y0,Z0,X1,X- 1)  ≠  P(X0|Y0,Z0), contradicting conditional independence of X given f(X). Finally, from ([\ref=eq:ne222]) and ([\ref=eq:magic]) we get so that

[formula]

We will show that ([\ref=eq:h3]) implies that at least one of the following two inequalities holds

[formula]

[formula]

Indeed, if both ([\ref=eq:hne1]) and ([\ref=eq:hne2]) are false then from ([\ref=eq:magic]) and ([\ref=eq:magic2]) we obtain

[formula]

and

[formula]

Thus, using decomposition for conditional entropy and ([\ref=eq:hne11]) we derive

[formula]

Continuing in the same way but using ([\ref=eq:hne22]) we obtain

[formula]

contradicting ([\ref=eq:h3]). Thus, either ([\ref=eq:hne1]) or ([\ref=eq:hne2]) holds true; consider the former inequality -- the latter one is analogous. We have

[formula]

where we have used the definition of Ik, ([\ref=eq:hne1]), ([\ref=eq:magic2]), the definition of mutual information and the symmetry thereof. This demonstrates ([\ref=eq:go2]) and concludes the proof.

Introduce the shorthand notation

[formula]

Define the total variation distance between pg and its empirical estimate p̂g as [formula]. Observe that, from the definition of mixing, if a process ρ generating [formula] is mixing with coefficients β(ρ,m) then the process made of tuples [formula] is mixing with coefficients β(ρ,m - k). Next, for the VC dimensions, observe that if a set

[formula]

has VC dimension d (for every y∈Y) then the set

[formula]

has VC dimension bounded by 7kd (for all [formula]); see [\cite=Vaart:09], which also gives a more precise bound. Thus, from [\eqref=eq:mix] we obtain

[formula]

We will use the bound from [\cite=Zhang:07] that relates the difference between mutual information to the total variation between the corresponding distributions of two pairs of random variables:

[formula]

where h stands for the binary entropy. Thus,

[formula]

where in the last inequality we used [\eqref=eq:wt] and the fact that h is monotone increasing on

[formula]

Acknowledgments

This work was supported FP7/2007-2013 under grant agreements 270327 (CompLACS) and 216886 (PASCAL2), by the French National Research Agency (project Lampada ANR-09-EMER-007) and the Nord-Pas-de-Calais Regional Council and FEDER through CPER 2007-2013.