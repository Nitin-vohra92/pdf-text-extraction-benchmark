On quantum vs. classical probability

Institut für Theoretische Physik Johann Wolfgang Goethe-Universität Max-von-Laue-Str. 1, 60438 Frankfurt am Main, Germany

Introduction

Physical theories which are inherently probabilistic are notoriously difficult to grasp. Determinism seems so deeply ingrained in our thinking that already classical concepts such as, say, the notion of entropy, maximum-entropy priors or the second law of thermodynamics, albeit in principle well understood [\cite=jaynes:papers] [\cite=balian:book1] [\cite=balian:book2] [\cite=rau:physrep], trigger controversy up to this day [\cite=lebowitz:boltzmann] [\cite=barnumetal:letters] -- let alone the conceptual basis of quantum theory [\cite=wheeler:100years] [\cite=bell:book] [\cite=aharonov:book]. The particular difficulties with physical theories that are probabilistic are often aggravated by conceptual issues surrounding probability theory itself, where the profound antagonism between orthodox and Bayesian schools has long clouded a clear view on the subject [\cite=jaynes:where].

Not surprisingly, then, the desire to understand the mathematical framework of quantum theory in terms of simple, more easily comprehensible physical principles is almost as old as quantum theory itself [\cite=zeilinger:foundational]. Attempts at such a principles-based reconstruction of quantum theory are legion. They have been motivated by a desire to elucidate the conceptual basis and interpretation of quantum theory; by the search for (or exclusion of) possible modifications to quantum theory; or by the search for alternative mathematical formulations of quantum theory that might be more conducive to, say, a merger with general relativity into a quantum theory of gravity. The following are a few examples without any claim to completeness.

Quantum logic is arguably the oldest of the reconstructive approaches. First put forward by von Neumann in his famous 1932 monograph [\cite=neumann:book] and in greater detail together with Birkhoff [\cite=birkhoff:logic], it was subsequently developed and promulgated most prominently by members of the Geneva School [\cite=jauch:book] [\cite=piron:book]. Its basic idea is to consider a lattice of propositions that is complete, orthocomplemented, weakly modular and atomic. Such a lattice constitutes a generalisation of classical logic. The definition of the lattice entails the existence of Boolean "and" and "or" operations which, however, no longer need to obey classical rules such as distributivity. According to Piron's theorem [\cite=piron:axiomatique] all propositions within such a quantum logic can be identified with subspaces of a Hilbert space over some skew field.

Almost at the same time the algebraic approach originated with the work of Jordan, von Neumann and Wigner [\cite=jordan:algebraic]. It subsequently evolved into the modern theory of C*-algebras [\cite=gelfand:embedding] [\cite=segal:postulates] [\cite=thirring:book3] which, thanks to its mathematical rigor and prowess, has found successful application in quantum field theory [\cite=haag:algebraic] [\cite=haag:book]. Rather than a lattice of propositions it takes as its fundamental mathematical object the abstract algebra of operators whose Hermitian elements are identified with physical observables.

A broad class of operational approaches focuses not primarily on the structure of propositions or observables but on primitive laboratory operations such as preparations, reversible transformations and measurements. Thereby particular emphasis is placed on the probability functionals, or states, and their convex geometry; whence this sometimes goes under the name convex set or convex cone approach. From the convex geometry of the set of states and with the help of additional auxiliary assumptions the full apparatus of quantum theory can be deduced. Major efforts in this direction have been associated with Mackey [\cite=mackey:book], the Marburg School [\cite=ludwig:book] [\cite=hellwig:operations] [\cite=kraus:book], Varadarajan [\cite=varadarajan:probability] [\cite=varadarajan:book], Gudder [\cite=gudder:convex] [\cite=gudder:book] and others [\cite=davies:operational] [\cite=edwards:operational] [\cite=araki:statespace] [\cite=dariano:whatisspecial].

Branching off this broad current is the somewhat more recent test spaces approach, promoted mainly by members of the Amherst School [\cite=foulis:operational1] [\cite=randall:operational2] [\cite=foulis:whatare] [\cite=foulis:filters] [\cite=foulis:supports] [\cite=wilce:orthoalgebras]. It is based on a generalised notion of sample space and emphasises the status of quantum theory as one of many possible generalised probability theories. Indeed, quantum theory can be regarded as but a variant of classical probability theory: Like the latter it deals with hypotheses (represented mathematically by subspaces of Hilbert space) and their probabilities (expectation values of the associated projection operators), with many important theorems of classical probability theory carrying over to the quantum case. One example is the quantum analog of the de Finetti representation for exchangeable sequences [\cite=caves:definettistates], which in turn is just a special case of a more general result for test spaces [\cite=barrett:definettitest].

Still within the wider context of operational approaches Hardy [\cite=hardy:fiveaxioms] recently proposed to derive quantum theory from a simple set of "five reasonable axioms": (i) Probabilities are defined as limits of relative frequencies. (ii) Probability distributions are specified by the minimal number of degrees of freedom that is compatible with the other axioms. (iii) Systems of the same information-carrying capacity exhibit the same structure, regardless of whether they are isolated or the result of constraining some bigger system. (iv) Upon combining constituents into a composite system, dimension and number of degrees of freedom are multiplicative. (v) Between any two pure states there exists a continuous reversible transformation.

The advent of quantum computation [\cite=steane:computing] [\cite=nielsen:book] [\cite=mermin:book] has led to revived interest in, and cross-fertilisation with, foundational issues [\cite=keyl:fundamentals] [\cite=fuchs:qmasinfo1] [\cite=barnum:illumination]. For instance, there have been efforts to distill those computational features that are genuinely quantum mechanical rather than generic to a wider class of non-classical generalised probability theories [\cite=barrett:infoprocessing] [\cite=barnum:cloning] [\cite=barnum:nobroadcast] [\cite=barnum:teleportation]. With quantum theory opening the way to novel forms of computation, some researchers have taken the next logical step to regard quantum theory as nothing but a framework for (highly efficient) information processing. Reconstructive approaches in this spirit include axiomatisations on the basis of category theory [\cite=coecke:categorical] [\cite=harding:categorical] that take very much a computer science perspective, or attempts at characterising quantum theory in terms of purely information-theoretic constraints [\cite=clifton:constraints].

Finally, quantum Bayesianism is the quantum version of the homonymous program in classical probability theory [\cite=schack:bayesrule] [\cite=caves:quantumasbayes] [\cite=srednicki:probabilities] [\cite=timpson:bayesianism]. In the Bayesian view probability theory constitutes an extension of logic [\cite=bernardo:book] [\cite=jaynes:book]; it is but a consistent framework for plausible reasoning in the face of uncertainty. Probabilities, and hence states, embody some agent's knowledge about, rather than an objective property of, a physical system; they represent degrees of belief rather than limits of relative frequencies; and they can be legitimately assigned not just to ensembles but also to individual systems. Bayesian probability emphasises (and makes explicit) the role of prior knowledge as a key input on a par with measurement data. Probability assigments are, however, not entirely at an agent's discretion: They must satisfy a number of consistency requirements which ensure that different ways of using the same information lead to the same conclusions, irrespective of the particular analysis path chosen. As shown for the classical case by Cox [\cite=cox:probability] these consistency requirements manifest themselves mathematically in the sum and Bayes rules. In the quantum case these are supplemented by additional coherence conditions [\cite=fuchs:quantumness]. There is also an alternative approach that applies Cox-style consistency requirements to amplitudes rather than probabilities [\cite=caticha:amplitudes] [\cite=caticha:insufficient].

While each of the above approaches captures important aspects of quantum theory, most of them still fall short of singling out quantum theory uniquely or are based on assumptions that call themselves for a more compelling motivation. For instance, quantum logic does not specify the skew field and cannot exclude the possibility of real [\cite=stueckelberg:real] or quaternionic [\cite=finkelstein:quaternionic] Hilbert spaces. Moreover, Piron's theorem holds only for Hilbert space dimension greater than three. The algebraic approach, though very powerful mathematically, adds little to the conventional formulation of quantum theory in terms of physical understanding. The convex geometry of the set of states is too weak a constraint to rule out, e.g., generalisations of quantum theory that are nonlinear [\cite=mielnik:generalized]; additional assumptions are needed. For these additional assumptions there are various proposals which, however, have not yet settled on a universally agreed, easily comprehensible set of axioms. For example, Hardy's five axioms, though intuitively appealing, resort to a questionable "simplicity" argument and make strong implicit assumptions about the existence of a tensor product for composite systems. Information-theoretic constraints can specify the mathematical apparatus of quantum theory only if combined with further assumptions about its algebraic structure. As for test spaces, it is not obvious how to single out quantum theory from the multitude of possible generalised probability theories; there are indications, not yet proven, that such would require additional assumptions regarding the symmetry, topology and composition properties of quantum theory [\cite=wilce:compactness] [\cite=wilce:symtop] [\cite=wilce:topological] [\cite=wilce:qpl]. And finally, the Bayesian approach --when applied to probabilities-- must invoke additional coherence conditions for, e.g., symmetric informationally complete POVM that are of a simple mathematical form but whose full physical meaning is yet to be clarified [\cite=appleby:sic]; and when applied to amplitudes (in the form of Cox-style consistency requirements) the question arises why one should start from complex amplitudes in the first place.

In this paper I will not adhere to any specific of the above approaches; nor do I wish to propose my own reconstructive scheme. Rather, my more modest goal is to take a step back and have a fresh look at the commonalities and differences between quantum and classical probability. The purpose of such an analysis is threefold: to highlight in a comprehensive fashion the (surprisingly many) features that quantum and classical probability have in common; to distill the (few) properties that truly distinguish them; and in light of the above, to explore the room there is, if any, for further probabilistic theories other than classical or quantum theory. In doing so I will strive to keep abstract mathematics to a minimum and instead emphasise as much as possible the physical meaning of the various properties. A large portion of my paper will be dedicated to the first of the three objectives, as the full range of commonalities of classical and quantum probability is not often exposed -- in contrast to their fundamental differences, which ever since the Einstein-Bohr debate [\cite=epr:complete] [\cite=bohr:complete] have been scrutinised extensively. The key (and novel) technical result, on the other hand, will pertain to the second objective: I will show that the single distinguishing property of quantum theory is the juxtaposition of finite information-carrying capacity and smoothness, where the concept of smoothness will be carefully defined and motivated. The mathematical derivation of this result will involve close inspection of the symmetry group, with successive constraints leading unequivocally to the unitary group of transformations in complex Hilbert space. As for the final objective, I will provide arguments why there is likely no further probabilistic theory that satisfies basic physical desiderata.

The outline of this paper is as follows. In Sections [\ref=propositions] through [\ref=sec:reductionism] I will identify six areas where classical and quantum theory have important features in common: the logic of propositions, symmetry, probabilities, composition of systems, state preparation and reductionism. While the two subsections on the logic of propositions and on probabilities are fairly standard and largely inspired by a mix of the existing quantum logic, convex cone and test spaces approaches, the remaining parts of Section [\ref=commonalities] are dedicated to areas that are less often emphasised but in my opinion equally important for the full picture. Next, in Sections [\ref=classical] and [\ref=quantum] I will single out joint decidability and smoothness as the distinguishing properties of classical and quantum probability, respectively. Smoothness in particular will turn out to be a crucial concept, and I will dwell on its definition and physical meaning. In Section [\ref=further] I will explore whether the commonalities identified in Section [\ref=commonalities] may constitute an umbrella over not just classical and quantum probability but also other probabilistic theories. The answer is in principle yes, but the candidate theories will likely have no physical significance. Finally, in Section [\ref=discussion] I will briefly venture outside the umbrella and consider more remote alternatives that share with classical probability less structure than does quantum theory; but I will quickly dismiss these on physical grounds. This leads to the conjecture that quantum theory is in fact the only consistent alternative to classical probability. I will conclude with a brief summary.

Commonalities of classical and quantum probability

Propositions

Classical and quantum theory share a common logical structure as summarised in Table [\ref=translation]. This common structure is weaker than classical Aristotelian logic in that it lacks the Boolean "and" ([formula]) and "or" ([formula]) operations, reflecting the fact that in the quantum case hypotheses need not be jointly decidable. In this weaker structure any proposition a can be endowed with a substructure [formula] isomorphic to an orthomodular poset [\cite=birkhoff:book], with orthocomplementation being defined relative to the maximal element a.

An alternative mathematical description uses instead the partial operation [formula] corresponding classically to the union of disjoint sets, and in the quantum case to the sum of mutually orthogonal subspaces. This partial sum is symmetric and associative,

[formula]

[formula]

and has the absurd hypothesis as its unique neutral element

[formula]

By definition a partial sum exists if and only if the summands are mutually contradictory; or conversely

[formula]

Logical implication of hypotheses can be defined via

[formula]

where in turn [formula] is the complement of x relative to a defined via

[formula]

The uniqueness of the relative complement renders [formula] isomorphic to an orthoalgebra [\cite=foulis:whatare] [\cite=foulis:filters] [\cite=wilce:orthoalgebras] for any choice of maximal element a.

Whenever a (classical or quantum) hypothesis a is decomposed into mutually exclusive, collectively exhaustive (MECE) refinements, and each of these refinements into further MECE refinements, and so on in a tree-like fashion until this iterative process comes to a halt because hypotheses cannot be refined any further then regardless of the precise path chosen to arrive at such a maximal decomposition the total number of outermost branches equals the granularity

[formula]

It vanishes if and only if the proposition is absurd; is equal to one if and only if the proposition is most accurate; and adds up under partial summation,

[formula]

Granularity is closely related to information-carrying capacity: Having ascertained the truth of hypothesis a, the amount of additional information that can be extracted by way of further, more refined measurements is bounded from above --in both the classical and the quantum case-- by log d(a). For simplicity of argument I shall assume in the remainder of this paper that the granularity of all propositions is finite.

Associated with each hypothesis a are collections of ordered decompositions

[formula]

labelled by the granularity vector [formula]. These granularities must be non-zero and sum to d(a). For different granularity vectors the associated collections are mutually disjoint; while their union, over all [formula], contains all possible ordered decompositions of a. Two special cases are the set of all most accurate refinements of a,

[formula]

and the collection of unordered maximal decompositions

[formula]

which is isomorphic to its ordered counterpart modulo permutation of the branches. The pair (Xa,Aa) constitutes a test space or "manual", which in the test spaces approach is taken as the basic structure on which a probability theory is erected [\cite=wilce:orthoalgebras].

In sum, the common logical structure underlying both classical and quantum probability can be described in either of three equivalent ways: as a collection of orthomodular posets, of orthoalgebras, or of test spaces. All these mathematical structures can be associated to arbitrary maximal elements a. In the following I shall limit myself to those cases where the granularity of this maximal element is finite.

Symmetry

In both the classical and the quantum case the proposition system can be characterised entirely by its symmetry. Let G be the group of all automorphisms that preserve the partial sum,

[formula]

and hence the logical structure exhibited in Table [\ref=translation]; and Ga its subgroup that leaves in addition the hypothesis a and all its implications invariant,

[formula]

This subgroup also preserves all collections [formula]. Moreover in both the classical and the quantum case it acts on these collections transitively, rendering them homogeneous spaces

[formula]

Collections that pertain to different hypotheses and different granularity vectors are isomorphic whenever the granularity vectors agree up to a permutation,

[formula]

and can thus be grouped into equivalence classes M({ki}) labelled by the unordered set of granularities only. Likewise the subgroups pertaining to different hypotheses fall into equivalence classes that have granularity as their sole parameter. In effect all structure depends on granularity only, not on any specifics of the proposition system under consideration. The isomorphism ([\ref=transitivity]) then carries over to an isomorphism of equivalence classes

[formula]

including as special cases

[formula]

and

[formula]

The pair (X,A) becomes a symmetric G-test space [\cite=wilce:symtop].

In the classical case G(d) is the symmetric (or permutation) group Sd; whereas in the quantum case it is the unitary group U(d). Whether or not other groups might be physically meaningful is a key issue addressed in this paper.

Probabilities

A state ρ assigns to hypotheses a probability between zero and one,

[formula]

Two states are identical if and only if they yield the same probabilities,

[formula]

so specifying a state is tantamount to providing a complete list of all probabilities. Classically any probability is given by a sum of elementary probabilities [formula] over the appropriate subset of sample space, whereas in the quantum case it is given by the trace (ρPx) (Gleason's theorem [\cite=gleason:measures] [\cite=peres:book]). In both cases probabilities obey the sum rule

[formula]

as well as the product rule

[formula]

where the latter is weaker than the classical Bayes rule. Symmetry transformations of states are defined via the invariance requirement

[formula]

When probabilities are conditional, the order of the conditions may be relevant. The notation is such that conditions are to be read from right to left: i.e., [formula] denotes the probability of x given that one started from the prior ρ, then ascertained y1, subsequently y2, and so on. If one of the conditions is a most accurate proposition then --by the very definition of the term "most accurate"-- it supersedes all previous conditions. In particular, it supersedes any prior,

[formula]

so that in the posterior probabilities the most accurate proposition effectively plays the role of the new state. Such a state is termed "pure".

Finally, in both the classical and the quantum case the set of states exhibits one further important property: If {(x|ρ)} and {(x|σ)} are two lists of probabilities corresponding to states ρ and σ, respectively, then there exist states yielding any mixture {α(x|ρ) + β(x|σ)} with α,β∈[0,1] and α  +  β  ≤  1 (not necessarily equal to one as distributions need not be normalised). States thus form a convex cone. In the homonymous approach to quantum reconstruction this geometrical property is taken as the starting point from which, with the help of additional auxiliary assumptions, the full apparatus of quantum theory is deduced. However, this will not be the approach adopted here.

Composition

Whenever two hypotheses pertain to different physical systems A and B they are jointly decidable and can hence be concatenated via the Boolean "and" operation

[formula]

Here the notation "×  " rather than "[formula]" emphasises the fact that this Boolean "and" is not defined in general but only for propositions relating to disparate systems. Given this restriction, distributivity holds:

[formula]

And in both the classical and the quantum case joint probabilities satisfy the product rule

[formula]

If eA and eB are most accurate hypotheses about A and B, respectively, then eA  ×  eB constitutes a most accurate hypothesis about the composite A  ×  B. This implies for the granularity the product rule

[formula]

Moreover, all propositions of the product form eA  ×  eB are contained in the set of most accurate propositions about the composite system A  ×  B:

[formula]

In the classical case this is in fact an equality because all most accurate propositions about a composite system have the product form. Consequently, the classical cardinality #  X(d) = d satisfies the product rule #  X(dAdB) =   #  X(dA)  ·    #  X(dB). In contrast, in the quantum case there exist pure states (and hence most accurate propositions) that are not separable, rendering X(dAdB) strictly larger than the product X(dA)  ×  X(dB). Indeed, this possibility of entanglement is reflected in the manifold dimension dim X(d) = 2(d - 1) which for d  ≥  2 obeys the strict inequality dim X(dAdB) >  dim X(dA) +  dim X(dB).

In a similar vein arbitrary concerted action of reversible operations on different constituents yields an allowed reversible operation on the composite. In mathematical terms, if G is a finite group then the Cartesian product of independent subsets of G(dA) and G(dB) must be isomorphic to an independent subset of G(dAdB); or if G is continuous, the Cartesian product of Lie generators of G(dA) and G(dB) must be isomorphic to a subset of the Lie generators of G(dAdB). This entails the constraint

[formula]

where in the finite case μ' denotes the size of the largest independent subset. Indeed, for the classical symmetric group Sd it is μ'(Sd) = d - 1 [\cite=whiston:indepsets] [\cite=cameron:indepsets] and hence the upper inequality is satisfied; while in the quantum case the lower inequality is satisfied (and even saturated) by the unitary group U(d) with dim U(d) = d2.

Preparation

All knowledge about a physical system, embodied in its state, results from a series of experiments or "preparation procedures". Let a denote the proposition that the system under consideration exists at all, and σ its state prior to a given procedure. Each preparation procedure is then an arbitrary combination of (i) controlled reversible operations; (ii) measurements of MECE refinements {xi} of a, [formula]; and (iii) keeping or discarding (= setting a to "false") the system, with respective probabilities that may depend both on the prior and on the outcome of the measurement. As an example consider a photon (so the hypothesis a: "the photon exists" is true) whose polarisation is first (i) rotated, then (ii) measured along some axis with possible outcomes {x1,x2}, [formula], and (iii) allowed to pass if and only if the outcome is x1, and else discarded (hence a set to "false"). These three steps may or may not take place inside a black box hiding the measurement outcome from the observer, and there may be a finite probability of error. The net effect on probabilities is then of the general form

[formula]

g being the reversible operation and {λi} the respective probabilities (modulo normalisation) with which the system is kept, given the rotated prior and measurement outcome xi. The updated probabilities correspond to some posterior ρ. In the special case where the prior is pure (σ = e) and only a single outcome x is selected, this posterior remains pure (ρ = f):

[formula]

The procedure described above can be iterated, with the posterior ρ serving as the new prior for the next iteration, until preparation is completed. In both the classical and the quantum case all states can be prepared in this way.

An arbitrary probability distribution on the substructure of a, i.e., the list of all probabilities {(x|ρ)|x  ⊆  a}, is completely specified by a finite number of real parameters. (One of these parameters is (a|ρ) which need not be normalised to one.) Like the substructure itself the number of parameters depends on the granularity d of a only and shall be denoted by S(d). Since every distribution results from preparation procedures as described above, an alternative way to specify it is by (i) the set {xi}∈M({ki}), [formula], that was last subjected to measurement, requiring dim M({ki}) parameters; and (ii) associated with each possible outcome xi, the posterior λi(g(σ))  ·  (y|xi,g(σ)) on the substructure of xi, requiring S(ki) parameters. This way of characterising the distribution requires the same total number of parameters, so

[formula]

Indeed, this condition is satisfied in both the classical and the quantum case. Classically the set M({ki}) is discrete, whence dim M({ki}) = 0 and S(d) = d; whereas in the quantum case M({ki}) is a continuous manifold of dimension [formula], and S(d) = d2.

Reductionism

Let aA,aB be hypotheses that pertain to two distinct physical systems A and B. Probability distributions on their substructures are specified by S(dA) and S(dB) real parameters, respectively. Therefore any distribution on one of the substructures can be characterised completely by the probabilities of just some finite set of --not necessarily mutually exclusive-- hypotheses {bAi  ⊆  aA}S(dA)i = 1 or {bBj  ⊆  aB}S(dB)j = 1, respectively.

Suppose that one knows the S(dA)  ·  S(dB) probabilities of the combined hypotheses {bAi  ×  bBj}. Then these probabilities suffice to specify not only the two single-constituent distributions but also all constituent-constituent correlations. This can be seen as follows. First, without loss of generality the hypotheses {b} (for either system) can be chosen such that some subset of these, {bi}i∈I, [formula] constitutes a MECE decomposition of a, [formula]. Then distributivity ([\ref=distributivity]) and the sum rule ([\ref=sumrule]) give all probabilities

[formula]

Next, application of the product rule ([\ref=productrule]) yields all

[formula]

which, as the {bAi} are informationally complete, implies in fact knowledge of (xA|aA  ×  bBj,ρ) for any xA  ⊆  aA. By yet another application of the product rule ([\ref=productrule]) one obtains any

[formula]

as well as, via distributivity and sum rule, (xA  ×  aB|ρ). These in turn yield

[formula]

and thus, {bBj} being informationally complete, any (yB|xA  ×  aB,ρ). Finally, applying the product rule ([\ref=productrule]) one last time gives

[formula]

for any xA  ⊆  aA,yB  ⊆  aB, and hence indeed the complete statistics of both systems including their correlations.

In a reductionist theory the properties of a composite system are determined entirely by those of its constituents and their statistical correlations. Beyond these there are no genuinely "holistic" properties of the composite system. So knowing the probabilities of xA  ×  yB for any xA  ⊆  aA,yB  ⊆  aB is tantamount to knowing the global state of the composite system. The S(dA)  ·  S(dB) parameters that suffice to specify the former, therefore, also suffice to specify the latter:

[formula]

This inequality need not necessarily be saturated, as the effective number of degrees of freedom of the composite system might be reduced by constraints on the correlations that are allowed between subsystems. Classical probability and quantum theory are both reductionist without such constraints, and hence not only satisfy but also saturate the above inequality.

Reductionism is a key prerequisite for the ability to subject probabilistic models to experimental tests, and hence ultimately for the success of the scientific method. In the modern Bayesian view probabilities have a priori nothing to do with measurable relative frequencies, a distinction which is particularly apparent in those cases where probabilities pertain to isolated events that cannot be repeated or --as is the case in quantum theory-- to individual systems that cannot be subjected to measurements without disturbance [\cite=hartle:individual]. Yet whenever probabilities pertain to multi-partite sequences that are exchangeable (or more Bayesian: whose exchangeability is agreed upon by a group of agents) then it is possible to collect frequency data, and this data drives agents via Bayes rule towards a unique posterior distribution regardless of their initial, invariably subjective beliefs. This possibility to reach a consensus through the collection of data is a fundamental assumption underlying any empirical science; in particular, it is implicit whenever one speaks of "the" state of a system as being the result of some well-defined preparation procedure [\cite=peres:book] [\cite=caves:subjective]. But such convergence to a consensus is not self-evident: It rests on the existence of a de Finetti representation for exchangeable sequences, which in turn is guaranteed only in probabilistic theories that are reductionist [\cite=caves:definettistates].

Additional requirements needed to derive specific cases

Classical case: Joint decidability

In addition to the shared features discussed in Section [\ref=commonalities] classical probability theory makes one more basic assumption: that all hypotheses be jointly decidable. Two hypotheses a and b are jointly decidable (denoted a  ↔  b) if and only if they have a joint decomposition [formula],

[formula]

Under this assumption one can define the Boolean operations "and" ([formula]), "or" ([formula]) for arbitrary propositions via

[formula]

which satisfy the classical distributivity properties

[formula]

The product rule ([\ref=productrule]) then implies the classical Bayes rule

[formula]

on which, together with the sum rule ([\ref=sumrule]), the whole edifice of classical probability theory can be erected [\cite=sivia:book] [\cite=jaynes:book].

Quantum case: Smoothness

The distinguishing feature of quantum theory is its peculiar smoothness. Despite the limitation that accessible information be finite, quantum theory deals only with continua: The set of hypotheses about a system, the symmetry group and all probability distributions are continuous. In mathematical terms, while the information-carrying capacity and hence the granularity d are constrained to be finite the set of most accurate hypotheses X(d) forms a continuous manifold (for d  ≥  2) of dimension dim X(d) > 0, the simplest example being for qubits (d = 2) the surface of the Bloch sphere; this manifold is compact [\cite=fivel:interference]. Correspondingly, the symmetry group G(d) is a compact Lie group. And the continuity of probability distributions manifests itself in the property that given any a and e0∈Xa,

[formula]

where Ba(e0;δ) is an open ball (in the group-induced topology) in Xa around e0; i.e., probabilities which are initially equal to one do not suddenly jump to a lower value upon an infinitesimal transformation. I shall show that this continuity requirement alone, in combination with the commonalities discussed in Section [\ref=commonalities], constrains G(d) to be the unitary group U(d) and hence singles out the quantum case.

Before embarking on a proof of this assertion I shall elaborate briefly on the physical meaning of the continuity requirement. Continuity ensures that probability assignments are robust under small preparation inaccuracies, in a sense which I shall discuss further below. Moreover, continuity turns out to be linked to the quantum Zeno or "watched pot" effect. To see this, I first note that being a compact Lie group, G(d) is endowed with a positive definite invariant metric [\cite=barut:book]. In this metric let G(d;δ) denote an open ball of radius δ around the identity. Continuity then implies that, given any e∈X(d),

[formula]

Define δ(ε) as the radius which saturates the continuity condition,

[formula]

Due to symmetry this radius depends on ε only, not on the specific hypothesis e or granularity d. Then for N replicas

[formula]

where the approximation holds whenever ε is sufficiently small. By composition rule ([\ref=compositeprobability]) the N-fold product of probabilities can also be written as

[formula]

with shorthand [formula] and some N-partite transformation g taken from the tensor product of open balls [formula]. This tensor product is in turn contained in some open ball in the symmetry group G(dN) of the composite system. The minimal radius of the latter follows from the law of Pythagoras for the group metric; it equals [formula]. The infimum ([\ref=1star]) thus translates into

[formula]

yielding for sufficiently small ε the scaling property

[formula]

This scaling implies

[formula]

which can be interpreted as follows: Given a sequence of N independent and --to within some finite accuracy-- identically prepared systems, the composite proposition e×  N is confirmed with asymptotic probability O(1), i.e., larger than an arbitrary threshold 1 - ε (0 < ε < 1) that does not depend on N, if and only if the constituents were prepared in the pure state e to within an accuracy [formula]. In short, continuity ensures that the probabilistic model tolerates finite preparation inaccuracies of the order [formula].

A second, related consequence of the scaling property ([\ref=2star]) is

[formula]

for arbitrary finite d and δ, a result which is tantamount to the quantum Zeno effect [\cite=misra:zeno]: When the transformation of a pure state e over a finite distance δ on the group manifold is cut into N steps of equal length, each followed by a measurement of the original proposition e, then as N  →    ∞   the net effect is that the system remains trapped in its original state.

I now prove my original assertion that the continuity requirement leads uniquely to quantum theory in complex Hilbert space. In order to understand the implications of the continuity requirement for the group G(d) I return to its original formulation ([\ref=continuity]). By preparation rule ([\ref=preparationrule]), for each e in the open ball Ba(e0;δ) there exists a unique most accurate hypothesis f such that (f|x,e) = 1; or equivalently,

[formula]

The product rule ([\ref=productrule]) then implies that also

[formula]

so [formula] and hence

[formula]

This allows that the overarching hypothesis a can be decomposed in three different ways:

[formula]

Given a and x, and hence [formula], one can now specify e in two equivalent ways: (i) directly as e∈Ba(e0;δ); or (ii) as e∈Xy, where in turn y must be specified via f∈Xx. In both cases the total number of parameters needed must be the same:

[formula]

where d = d(a) and l = d(x). By induction with initial condition dim X(1) =  dim X(0) = 0,

[formula]

The isomorphism ([\ref=isomorphism2]) together with initial condition dim G(0) = 0 then constrain the Lie group dimension to be of the quadratic form

[formula]

and for the number of parameters S(d) the dimensional relation ([\ref=parameterrule]) together with the initial conditions S(0) = 0 and S(1) = 1 yield the similar form

[formula]

The constraint ([\ref=groupcomposition]) on group composition (continuous case) on the one hand, and reductionism ([\ref=reductionism]) on the other, allow in fact only a single non-zero value for dim X(2), namely dim X(2) = 2, and only the two values 0 and 1 for dim G(1). These parameter values correspond to the compact Lie groups [formula] and U(d), respectively. Moreover, convexity of the set of states presupposes that X(d) be the hull of a convex set. Yet for d = 2 the group [formula] yields X(2)  ~  S1  ×  S1 isomorphic to a torus, which is not simply connected and hence not the hull of a convex set; similar problems arise with [formula] in higher dimensions. Therefore, when combined with the properties discussed in Section [\ref=commonalities], the continuity requirement leaves indeed only the unitary group U(d) as a permitted symmetry and so leads unambiguously to quantum theory in complex Hilbert space.

Further cases?

If neither joint decidability nor smoothness are required then it is conceivable that the commonalities discussed in Section [\ref=commonalities] allow for additional cases. However, I shall argue that the set of possible alternatives to classical or quantum probability is severely limited and likely unphysical.

The isomorphism ([\ref=isomorphism]), composition constraint ([\ref=groupcomposition]), preparation rule ([\ref=parameterrule]) and reductionism ([\ref=reductionism]), as well as the initial conditions dim G(0) = 0, S(0) = 0 and S(1) = 1 constrain the number of degrees of freedom to obey a power law

[formula]

and the dimension of the group manifold to be of the form

[formula]

with some positive integer μ > 0 and dim G(1)∈{0,1}. The case μ = 1, dim G(1) = 0 encompasses the classical case G(d) = Sd but might in principle also accomodate finite groups other than the symmetric group; such alternative finite groups might correspond to "constrained" versions of classical probability theory. The case μ = 1, dim G(1) = 1 yields the same classical sample space but allows for a phase to be attached to each most accurate hypothesis, with associated symmetry group [formula]; this "semi-classical" case may be worth exploring further. The case μ = 2 corresponds to the quantum case discussed in Section [\ref=quantum] with symmetry group G(d) = U(d), possibly modulo some finite factor group.

As genuine non-classical alternatives, therefore, there remain only probabilistic theories of higher order μ  ≥  3, a possibility already pointed out by Hardy [\cite=hardy:fiveaxioms]. However, the central result of Section [\ref=quantum] implies that such a higher-order theory would necessarily exhibit some form of discontinuity. Moreover, it appears difficult to imagine the physical meaning of an associated Lie group such as, say, U(d2) that would be needed to yield the required dimensionalities. There is of course the possibility that the symmetry group is exceptional or even non-topological; yet the physical meaning of such an exotic scenario would be even more elusive. In sum, as long as the features discussed in Section [\ref=commonalities] are required to hold, there appears to be no reasonable alternative to classical or quantum probability.

Discussion

The commonalities of classical and quantum probability extend farther than one might at first expect. As discussed in Section [\ref=commonalities], classical and quantum theory share a substantial number of features regarding the logic of propositions, symmetry, probabilities, composition of systems, state preparation and reductionism. Only at a late stage do their ways part, with the requirement of joint decidability leading to classical theory, whereas demanding smoothness leads to quantum theory. The essence of quantum theory, therefore, is the juxtaposition of finite information-carrying capacity and smoothness: the fact that even though information-carrying capacity is finite, quantum theory describes systems that are continuous (i.e., subject to a continuum of hypotheses); or conversely, that even for continuous systems the information-carrying capacity remains finite. The step from classical to quantum probability thus amounts to --given finite information-carrying capacity-- forgoing joint decidability in favor of smoothness; or given smoothness, relinquishing joint decidability in favor of a finite information-carrying capacity.

While from the above perspective the step from classical to quantum probability may now seem rather small, it entails the following well known, far-reaching consequences:

Indeterminism. Quantum theory exhibits an irreducible probabilism in the sense that in every state, even if pure, there are always hypotheses whose probabilities are neither 0 nor 1. Mathematically, this manifests itself in non-commutativity and uncertainty relations.

Non-separability. The whole is more than the sum of its parts; it may be in a pure state that is not a product of constituent states. In contrast to classical separability, the whole can in general not be dissected into parts without a resulting loss of information. Mathematically, this manifests itself in the possibility of entanglement.

Observer-dependency. Measurement implies disturbance. The image of reality that emerges through acts of measurement reflects as much the history of intervention as it reflects the external world; there is no preexisting reality that is merely revealed, rather than influenced, by the act of measurement. Mathematically, this is encapsulated in the Bell and Kochen-Specker theorems [\cite=mermin:bell].

Besides quantum theory there appear to be no other meaningful alternatives to classical probability theory. I argued in Section [\ref=further] that if taken as constraints, the commonalities identified in Section [\ref=commonalities] leave little room for cases other than classical or quantum probability. Of course, those constraints might be relaxed: At least formally there may exist non-classical theories that share less structure with classical probability than does quantum theory, and that hence may lack some of the features discussed in Section [\ref=commonalities]. One example is quantum theory in real Hilbert space [\cite=stueckelberg:real]. There the symmetry group is the orthogonal group (d), and states are specified by S(d) = d(d + 1) / 2 parameters. Real quantum theory shares with classical probability the first five of the six commonalities discussed in Section [\ref=commonalities]. But it is not reductionist: Already for a system composed of two real qubits it is S(4) > S(2)  ·  S(2) in violation of Eq. ([\ref=reductionism]); there may be "holistic" properties of a two-qubit system that cannot be understood in terms of the constituent qubits and their correlations alone. As a consequence there is in general no de Finetti representation for exchangeable sequences, which deprives real quantum theory of a crucial link between probabilities and measurable frequencies [\cite=caves:definettistates].

Another non-classical alternative could be quantum theory in quaternionic Hilbert space [\cite=finkelstein:quaternionic]. Its symmetry group is the symplectic group (d) with group dimension d(2d + 1). States are specified by S(d) = d(2d - 1) parameters, yielding for two quaternionic qubits the strict inequality S(4) < S(2)  ·  S(2). This is compatible with reductionism, yet shows that there are constraints on the allowed correlations between subsystems. All exchangeable sequences have a de Finetti representation but the converse is no longer true: Not all distributions of the de Finetti form represent allowed states [\cite=caves:definettistates]. The main drawback of quaternionic quantum theory, however, is the fact that it no longer allows the arbitrary composition of reversible operations. Already for a system composed of two quaternionic qubits it is dim G(4) <  dim G(2)  ·   dim G(2) in violation of the composition constraint ([\ref=groupcomposition]).

The above examples strongly suggest that there is in fact no consistent alternative to quantum theory as we know it: Any non-classical probability theory other than quantum theory in complex Hilbert space likely violates basic physical desiderata. Future work will be aimed at further corroborating this conjecture. If this effort is successful then the commonalities exhibited in Section [\ref=commonalities] combined with the smoothness requirement of Section [\ref=quantum] may provide the basis for a novel reconstruction scheme for quantum theory.

Beyond the further development of reconstruction schemes it is my hope that the results of my investigation may also inform future efforts in other directions: e.g., the formulation of overarching frameworks for probability theory that encompass both classical and quantum probability as special cases; the generic definition of notions such as entropy or information without reference to any specific classical, quantum or other representation; or a deeper conceptual understanding of the essential features of quantum theory that make the genuine difference from classical probability.

Acknowledgments

I would like to thank Chris Fuchs, Lucien Hardy, Robert Spekkens and Cozmin Ududec for their hospitality and stimulating discussions during a visit to Perimeter Institute. I also thank Alex Wilce and Matt Leifer for helpful feedback on an earlier version of this paper.