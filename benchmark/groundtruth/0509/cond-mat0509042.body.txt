Fluctuation-dissipation relations for complex networks

Introduction

Recently, the statistical properties of real networks (including biological, social and technological systems) have attracted a large amount of attention among physicists (see e.g. [\cite=Handbook] [\cite=Dorogbook] [\cite=BArev]). It has been realized that despite functional diversity, most of real web-like systems share important structural features e.g. small average path length, high clustering and scale-free degree distribution. A number of network models have been proposed to embody the fundamental characteristics. The models can be roughly divided into two classes: static (homogeneous, equilibrium) and evolving (causal, nonequilibrium). The second class of causal networks encompasses, in particular, the famous BA model [\cite=BAScience], whereas configuration model [\cite=Molloy1995] [\cite=NewPRE2001] [\cite=Fron2005a] and the large group of networks with hidden variables [\cite=SodPRE2002] [\cite=BogPRE2003] [\cite=Fron2005b] belong to the first class of static networks. Although very intuitive, the mentioned representatives of static random networks are not properly defined from the point of view of the equilibrium statistical mechanics. Below we briefly outline what the mentioned lack of seemliness means with reference to random networks.

To start with, let us concentrate on the phrase random network. What does it mean that a network is random? One possible answer is that there is a large amount of randomness in the process of network construction. It treats to all the examples of homogeneous and causal networks quoted in the previous paragraph. The answer however suffers a few disadvantages among which the most striking is the issue of quantification of the randomness. Another answer to the asked question could be that random network is a member of a statistical ensemble of networks and the probability of the occurrence of a given network in random sampling is proportional to its statistical weight. Without a doubt, the last treatment directly follows principles of the equilibrium statistical mechanics.

At the moment, a simple example could be the configuration model. In this model, the total number of nodes is fixed to N and degrees of all nodes [formula] create a specific degree sequence {ki}. Until now, nothing has been said about connections between nodes. As a rule, random graphs with a given degree sequence are constructed in the following way: i. first, attach to each node i a number ki of stubs (ends of edges); ii. next, choose pairs of these stubs uniformly at random and join them together to make complete edges. For sure, such a procedure represents a large randomness justifying the phrase random networks. On the other hand, however, the second among the mentioned possible meanings of the phrase, treating the resulting networks as members of the ensemble of graphs with the desired degree sequence, seems to be more familiar to physicists.

As a truth, the concepts of statistical mechanics (including statistical ensembles, partition functions, averages over ensemble and so forth) has already been applied to analysis of complex networks. Although, the majority among the recently submitted articles still define network models through construction procedures, there has also been published several interesting papers on the genuine statistical mechanics of random networks (cf. [\cite=BurdaPRE2001] [\cite=BergPRL2002] [\cite=Bauer2002] [\cite=BurdaPRE2003] [\cite=DorogNuc2003] [\cite=PallaPRE2004] [\cite=Farkas2004] [\cite=ParkPRE2004] [\cite=Bogacz2005] [\cite=Bialas2005]). The general idea is similar to all the above mentioned papers. Statistical ensemble of networks is defined by specifying a set of networks G which one wants to study (e.g. simple graphs, digraphs, weighted graphs) and a rule that associates probability distribution P(G) with these networks G∈G. The differences between the quoted approaches consist in different weight assignment strategies.

In this contribution we extend the information-theoretic approach to random networks that was very recently proposed by Park and Newman [\cite=ParkPRE2004] (see also [\cite=Bauer2002]). Information theory [\cite=Jayens1] [\cite=bookCover] provides a criterion for setting up probability distributions over a given ensemble on the basis of partial knowledge and leads to a type of statistical inference. It is the least biased estimate possible on the given information i.e. it is maximally non-committal with regard to missing information. Since the procedure consists in entropy maximization under constraints imposed by the physical conditions of the ensemble, it is also known as maximum-entropy estimate [\cite=bookKapur].

In this paper, following Park and Newman [\cite=ParkPRE2004], we use Shannon entropy in order to establish probability distribution over analyzed networks [\cite=footnote1]. Park and Newman have presented a few exact solutions (in the sense of weighted averages over ensembles) of specific network models including undirected networks with a given degree sequence and networks incorporating arbitrary but independent edge probabilities. Here, we analyze these exactly solved models from the point of view fluctuations over ensembles. We discuss several fluctuation-dissipation relations for the mentioned ensembles. We also show that the quoted maximum-entropy models are equivalent to random networks with hidden variables [\cite=BogPRE2003].

General definitions

In this section we review the fundamentals of maximum-entropy random networks due to Park and Newman [\cite=ParkPRE2004].

In order, to correctly define statistical ensemble of networks one has to start with specifying a set of graphs G which one wants to study. In the following, we restrict ourselves to labelled simple graphs [\cite=footnote2] with a fixed number of nodes N. Let us remind that a simple graph has at most one link between any pair of vertices and it does not contain self-loops connecting vertices to themselves. Note also that there exists one-to-one correspondence (isomorphism) between simple graphs and symmetric matrices of size N with elements σij equal either 0 or 1.

Once the set G of possible networks has been established, in the next step one has to decide what kind of constraints should be imposed on the ensemble. The choice may be, for example, encouraged by properties of real networks like high clustering, significant modularity or scale-free degree distribution. In fact, due to the mentioned isomorphism between graphs and matrices only such ensembles can be exactly solved which constraints are simply expressed in terms of adjacency matrix.

Now, suppose that one would like to establish probability distribution over G in such a way that the expected values (i.e. averages over the ensemble) of several observables {xi(G)}, [formula] were respectively equal to {〈xi〉}. Due to maximum entropy principle the best choice for probability distribution P(G) is the one that maximizes the Shannon entropy

[formula]

subject to the constraints

[formula]

for [formula], plus the normalization condition

[formula]

The Langrangian for the above problem is given by the below expression

[formula]

where the multipliers α and θi are to be determined by ([\ref=war1]) and ([\ref=norm]).

Differentiating L with respect to P(G) and then equating the result to zero one obtains the desired probability distribution over the ensemble of graphs with given properties ([\ref=war1])

[formula]

where H(G) is the network Hamiltonian

[formula]

and Z represents the partition function (normalization constant)

[formula]

Finally, in order to complete the section devoted to general considerations it is useful to define the free energy of the ensemble

[formula]

The last quantity is of wide use in the rest of the paper.

Now, let us examine the introduced formalism with a few examples. In the next section, we will analyze fluctuations over the below presented ensembles.

Microcanonical ensemble of random networks

At the beginning, let us study the equivalent of the microcanonical ensemble for maximum-entropy random networks. Maximizing Shannon entropy ([\ref=entropy]) subject to only normalization condition ([\ref=norm]), i.e. omitting other constraints ([\ref=war1]), one obtains the uniform distribution over all simple graphs of size N

[formula]

where [formula] represents the total number of the considered networks i.e. the total number of 0 - 1 symmetric matrices of size N. The uniform distribution ([\ref=micPG]) means that each graph in the ensemble have the same weight regardless of its properties.

Since all graphs in the ensemble are equiprobable one can simply argue that the probability of a graph having m links is given by

[formula]

and respectively

[formula]

Similarly, the probability of an arbitrary node to have k nearest neighbors equals [formula], and the average connectivity is 〈k〉 = (N - 1) / 2.

In fact, the considered microcanonical ensemble of random networks is equivalent to the ensemble of classical random graphs with the connection probability p = 1 / 2. Ensembles of classical random graphs with an arbitrary linkage probability will be considered in the next subsection.

Classical random graphs

Now, let us consider an ensemble of networks with an expected number of links 〈m〉 (as stressed before the ensemble is equivalent to random graphs introduced by Erdös-Rényi). The Hamiltonian ([\ref=H0]) for this ensemble is given by

[formula]

where θ represents a field or an inverse temperature whose value is fixed and depends only on 〈m〉. Park and Newman [\cite=ParkPRE2004] have shown that the partition function ([\ref=Z0]) for the ensemble is equal to

[formula]

and respectively the free energy ([\ref=F0]) can be written as

[formula]

Having probability distribution ([\ref=PG0]) over the ensemble one can, for example, find the relation between the average number of links 〈m〉 and θ

[formula]

Now, since θ is fixed one can reexpress the last formula in terms of the linkage probability p that is known from the theory of classical random graphs

[formula]

where

[formula]

Finally, let us point out that in the limit of very small fields θ  →  0 (high temperatures) the ensemble of random networks with an expected number of links is equivalent to the microcanonical ensemble of random networks ([\ref=micPG]) introduced in the previous subsection.

Networks with a given degree sequence

At the moment, suppose that one would like to deal with random networks with an expected degree sequence

[formula]

In this case, the network Hamiltonian is given by

[formula]

where the multipliers θi represent a kind of potential assigned to each node and they only depend on the expected degrees 〈ki〉 (see Eqs. ([\ref=Pkki]) and ([\ref=Pkki1])). The partition function for the considered ensemble can be written as [\cite=ParkPRE2004]

[formula]

and the free energy is

[formula]

Performing weighted averages over the ensemble one can easily prove the below identities: the first one describing the connection probability between two nodes i and j

[formula]

and the second identity for the average connectivity of a node characterized by the local field θi

[formula]

The both expressions show the reverse relation between the two parameters (i.e. 〈ki〉 and θi) characterizing each node. The relation consist in the statement: small degrees correspond to large multipliers and vice versa, nodes with a large number of connections possess small multipliers.

Park and Newman [\cite=ParkPRE2004] have pointed out that instead of studying networks with an expected degree sequence ([\ref=Pk_seq]) one can deal with networks characterized by an expected degree distribution P(〈ki〉) [\cite=footnote3]. The authors have argued that one can produce any degree distribution by a judicious choice of the distribution of multipliers ρ(θi). In fact, due to ([\ref=Pkki]), ρ(θi) resulting in the desired P(〈ki〉) can be determined from the below expression

[formula]

where 〈ki〉(θi) is given by ([\ref=Pkki]). There are, however, a few subtleties related to the transition between the sequence [formula] and P(〈ki〉). First, the Eq. ([\ref=PkRt]) defines ρ(θi) as an implicit function which, except for a very few cases, can not be explicitly calculated. Second, performing such a transition one has to keep in mind that our phase space consists of labelled graphs in which every node [formula] has assigned its own multiplier θi (i.e. is distinguishable). Using ρ(θi) makes an impression that the nodes lose their identities. In such a case, there exists a threat of widening the original phase space.

To proceed further, let us consider sparse networks. In the case, connection probabilities ([\ref=Pkpij]) factorize

[formula]

where

[formula]

As shown in [\cite=BogPRE2003] [\cite=Fron2005b] such ensembles are equivalent to uncorrelated networks. The relation ([\ref=Pkki1]) between expected degrees and its multipliers makes the ensembles very simple for both Monte Carlo simulations and analytical treatment. In particular, the distribution of multipliers ([\ref=PkRt]) corresponding to P(〈ki〉) is simply

[formula]

where 〈ki〉 is given by ([\ref=Pkki1]).

There exist, however, some side effects of the approximation. First, since the connection probability pij  ≤  1, ([\ref=Pkpij1]), thus the assumption of sparse networks is only valid for networks with non-negative Lagrange multipliers (i.e. θi,θj  ≥  0). The restriction causes failure of the approach in the case of scale-free networks P(k)  ~  k-  γ with 2 < γ < 3. To existence of hubs kmax  ~  N1 / (γ - 1) [\cite=footnote4], i.e. nodes with negative multipliers (see comment after Eq. ([\ref=Pkki])), spontaneously develop degree correlations [\cite=MaslovPhysA2004] [\cite=ParkPRE2003]. It was argued [\cite=BurdaPRE2003] [\cite=BogEPJ2004] [\cite=CatanPRE2005] that one can omit the correlations by applying the so-called structural cutoff i.e. forcing the largest degree to scale as [formula]. At the moment, let us stress that the structural cutoff in uncorrelated networks naturally emerges form the Eq. ([\ref=Pkki1]) when θi  →  0.

Networks with two-point correlations

In order to study random networks with two-point correlations one may consider a class of Hamiltonians ([\ref=H0]) constructed on the basis of an expected linkage probability

[formula]

In the last expression σij(G) is an element of the adjacency matrix representing the graph G and Θij characterizes field coupled to the hypothetical link {i,j}. The partition function and the free energy for the ensemble are given by

[formula]

Comparing ([\ref=PkZ]) and ([\ref=corZF]) one can see that the previous ensemble of networks with an expected degree sequence is a special case (for Θij  =  θi  +  θj) of networks with arbitrary two-point correlations. Similarly to ([\ref=Pkpij]) one can also find that

[formula]

Fluctuations and responses

In classical thermodynamics, fields interacting with a system have conjugate variables which represent the response of the system to perturbation of the corresponding field. For example, the response of a gas to a change in volume is a change in pressure. The pressure p is the conjugate variable to the volume V. Similarly, the magnetization M of a magnet changes in response to the applied field B. The mentioned relations are produced by terms in the Hamiltonian of the form YX, where Y is a field and X is the conjugate variable to which it couples. Note, that the above also holds for maximum-entropy random networks being the subject of the paper, where (see Eq. ([\ref=H0]))

[formula]

Taking advantage of ([\ref=PG0])-([\ref=F0]), expectation values 〈xi〉 of observables xi can be calculated as first derivatives of the free energy with the appropriate field θi (cf. ([\ref=mer]),([\ref=Pkki]),([\ref=corpij]))

[formula]

Similarly, second derivatives of the free energy F give the mean square fluctuations of the variables

[formula]

Now, inserting ([\ref=sxi0]) into ([\ref=varxi0]) one obtains a very important result

[formula]

that is known as the fluctuation-dissipation theorem (FDT). The theorem states that fluctuations in an observable xi are proportional to the susceptibility χ(θ)i of the observable to its conjugate field θi. Let us remind that the susceptibility χ(θ)i measures the strength of the response of xi to changes in θi. In reality, due to practical purposes, it is often simpler to analyze the susceptibility χ(φ)i to other field φi that directly depends on θi ([\ref=fdt0])

[formula]

where ∂φi  /  ∂θi is the transitional derivative.

Probably the best known example of the theorem ([\ref=fdt0]) is the one arising from fluctuations of energy in the canonical ensemble

[formula]

where CV  =  ∂〈E〉  /  ∂T is the heat capacity (or thermal susceptibility), whereas kT2  =  ∂β  /  ∂T is the respective transitional derivative. Another example relates fluctuations in the magnetization to the magnetic susceptibility

[formula]

The fluctuation-dissipation theorems ([\ref=fdt0])-([\ref=fdtM]) are interesting for a number of reasons. First, they join both microscopic description (left-hand side) and macroscopic, properties (right-hand side) of the considered systems. Second, they relate the actual state (fluctuations) of the systems to their future behavior (response). Third, due to FDT phase transitions certified by singularities in susceptibilities can also be reported by large scale fluctuations.

Extending the idea of the susceptibility one can consider what happens with a variable xi when one changes the value of a field θj. To study the problem one can define a generalized susceptibility χ(θ)ij which is a measure of the response of 〈xi〉 to the variation of the field θj

[formula]

Again, the susceptibility χ(θ)ij is a second derivative of the free energy

[formula]

The issue of generalized susceptibilities is of special interest in lattice systems ([\ref=H00]), where the variables xi for [formula] may represent to the same observable but measured in different spatial points. Then, susceptibility χ(θ)ij is just the two-point correlation function between sites i and j.

In the following, we will concentrate on fluctuations over a few selected ensembles of random networks.

Classical random graphs

At the beginning, let us consider the ensemble of classical random graphs. By definition, the average number of links 〈m〉 is fixed in the ensemble. As stressed at the beginning of the section, there exist, however, fluctuations around the average. In fact, the probability of a graph G with m links is given by

[formula]

The variance of the above distribution calculated from ([\ref=fdt0]) is very similar to ([\ref=fdtE])

[formula]

where [formula] is the link capacity in classical random graphs. Note that for a given network size N the link capacity does not depend on the linkage probability Cm(p) = const (classical ideal gases reveal the analogous behavior CV(T) = const).

Networks with a given degree sequence

Now, let us continue with random networks characterized by an expected degree sequence ([\ref=Pk_seq]).

Taken advantage of ([\ref=Pkpij]) and ([\ref=Pkki]), fluctuation-dissipation relations ([\ref=fdt0]) for the ensemble may be written in the following form

[formula]

At the moment, before delving into the discussion of the last expression, let us note that the susceptibility χ(θ)i is also given by the below formula

[formula]

where Cij  =  ∂〈ki〉  /  ∂pij represents the link capacity. Here, since Cij = 1 the fluctuations in nodes degrees result only form the transitional derivative ∂pij  /  ∂θi.

The importance of the two above identities lies in the fact that from the fluctuations over degrees of nodes characterized by the same local field θi, one can deduce on the future behavior of the nodes in the face of possible changes in θi. Large (small) fluctuations correspond to high (low) local susceptibility χ(θ)i.

Now, let us note, that in the case of small degrees (see also the assumption of sparse networks ([\ref=Pkki1])), the fluctuation-dissipation relation ([\ref=Pk_fdt1]) gets a simplified form

[formula]

indicating the Poissonian fluctuations. Since however, in the case of sparse homogeneous networks one can omit the last term in ([\ref=Pk_fdt1]), in sparse scale-free networks with the characteristic exponent 2 < γ < 3, the mentioned term is dominated by hubs and the nodes susceptibilities χ(θ)i are much smaller than their expected degrees 〈ki〉. The total network susceptibility decreases making the system resistant against random changes in the landscape of multipliers and simultaneously susceptible to behavior of supernodes [\cite=HavPRL2000] [\cite=HavPRL2001].

In order to establish the better understanding of the statement included in the last paragraph, let us consider a trivial network consisting of N nodes with expected degrees 〈k〉 = 1 and one supernode with the tunable desired degree [formula] (in the sequel, the parameters denoted by the star apply to the supenode). Adjusting the degree of the supernode makes possible to smoothly pass between regular graphs (for 〈k*〉 = 1) and star networks (for 〈k*〉 = N) (see Fig. [\ref=fig1]). The transition enables the understanding of what the reduced network susceptibility consists in.

First, let us find the Lagrange multipliers θ and θ* corresponding to the nodes of the considered ensemble. Taking advantage of ([\ref=Pkki]) one can show, that the parameters fulfill the set of equations Solving the above equations for θ and θ* one gets (see Fig. [\ref=fig2]a) Next, inserting the multipliers into ([\ref=Pkpij]) and then taking advantage of ([\ref=Pk_fdt1]) one obtains the susceptibilities of expected degrees due to changes in the nodes intensive parameters. The relative susceptibilities are respectively given by

[formula]

for the bulk of nodes, and

[formula]

for the supernode.

The behaviour of susceptibilities ([\ref=Pk_fdtex1]) and ([\ref=Pk_fdtex2]) is depicted at the Fig. [\ref=fig2]b. One can see that the susceptibilities decrease to 0 when the expected degree of the hub 〈k*〉 approaches N. In the region of the vanishing susceptibilities, the small changes in the fields θ and θ* poorly affect the topological features (i.e. 〈k*〉) of the considered networks (c.f. Fig. [\ref=fig2]a). The last statement supports our previous claim of the resilience of such networks against random external perturbations. The large 〈k*〉 makes that the supernode accumulates most of the links present in the system and causes that there exist relatively small number of network realizations which both: i. fulfill physical constraints of the ensemble and ii. possess significant statistical weights. For example, only one such a realization exists in the limiting case of the star network with k* = N.

Equivalence of maximum entropy networks and networks with hidden variables

In the section, we continue the thread of Poissonian fluctuations ([\ref=Pk_fdt2]).

Random networks with hidden variables are simply defined through the construction procedure that consists of only two steps:

first, prepare N nodes and assign them hidden variables independently drawn from the probability distribution R(h),

next, each pair of nodes {i,j} link with a probability rij.

One can show, that the uncorrelated networks with hidden variables arise from the factorized connection probability

[formula]

whereas networks with two-point correlations require more sophisticated expressions for rij.

Even comparing the above short review to our previous results on sparse networks with an expected degree sequence (cf. Eqs. ([\ref=Pkpij1]) and ([\ref=rij])), allows one to deduce on the equivalence of the two approaches. In the course of the section, we will argue that the claimed equivalence also holds for networks with two-point correlations. We will prove it by recovering the so-called Poissonian propagators characterizing both correlated and uncorrelated sparse networks with hidden variables [\cite=BogPRE2003].

Networks with a given degree sequence

At the moment, it is clear that although the expected degree of the node i is 〈ki〉, but due to ensemble fluctuations its actual degree ki changes from network to network. Our aim is to find the so-called propagator P(ki  /  θi) i.e. the degree distribution of the node given that it is characterized by the multiplier θi.

At the beginning, let us reformulate the probability of a graph G in the ensemble

[formula]

where H(G) and Z are respectively the graph Hamiltonian ([\ref=PkH]) and the partition function ([\ref=PkZ]). Taking advantage of the connection probability pij ([\ref=Pkpij]), P(G) can be written in a similar form as in the case of classical random graphs ([\ref=er1PG])

[formula]

where

[formula]

whereas σij are elements of the adjacency matrix describing G and they are equal to either 1 or 0 depending on whether i and j are connected or not.

In the following, without the loss of generality, we will concentrate on the node i = 1. Having P(G) one can calculate the probability P({σ1j}) of the node to have a given linkage profile {σ1j} (e.g. {0,0,0,1,0,1,,0})

[formula]

where the first sum runs over all networks G* with the fixed sequence {σ1j} (i.e. the fixed neighborhood of the node i = 1). Now, in order to obtain P(k1  /  θ1) one has to sum the probabilities ([\ref=Pk2PG]) over different sequences {σ1j}* representing the same degree [formula]

[formula]

Up to this point, the derivation of P(k1  /  θ1) has been exact. Now, before proceeding with approximations let us test the formula ([\ref=prop0]) against the simplest ensemble i.e. networks with an expected homogenous degree sequence. In the ensemble, all nodes have the same desired degree and also [formula]. It is easy to check that the degree distribution of an arbitrary node ([\ref=prop0]) is given by

[formula]

where the binomial factor in the front of the expression arises from the fact that there exist [formula] different connection profiles corresponding to degree k and p = (e2θ + 1)- 1 ([\ref=Pkpij]) (please do not confuse it with ([\ref=per]), where θ has a different meaning !). One should not be surprised with the last result. If it is not obvious, let us stress that the ensemble of networks with an expected homogeneous degree sequence is in fact equivalent to the ensemble of classical random graphs. To become familiar with the statement compare the formulas ([\ref=Zer]) and ([\ref=PkZ]).

Now, in order to recover the claim of equivalence between the considered maximum-entropy models and random networks with hidden variables one has to apply the mean field approximation to the expression ([\ref=prop0])

[formula]

where [formula] ([\ref=Pkki]). The assumption of sparse networks enables further simplification of the distribution

[formula]

recovering the result previously derived by Söderberg [\cite=SodPRE2002] for uncorrelated networks with hidden variables. The Poissonian propagator ([\ref=prop2]) indicates the mentioned equivalence of the considered maximum-entropy networks and the well-known class of uncorrelated random networks with hidden variables (Lagrange multipliers characterizing nodes correspond to hidden attributes).

As pointed out in [\cite=BogPRE2003], the result is indeed very strong since it also holds for random networks with two-point degree correlations (see Eq. ([\ref=prop3]) and also Eq. (23) in [\cite=BogPRE2003]). The key point to notice with reference to the last expression is that in the region of small degrees, due to the Poissonian fluctuations (c.f. ([\ref=Pk_fdt2]) and ([\ref=prop2])), the real degree distributions observed in single realizations of the considered networks strongly differ from the desired degree distribution P(〈ki〉). On the other hand, in the limit of large degrees the real distribution and the expected one converge. The interplay between the two distributions has been carefully studied in our previous paper [\cite=Fron2005b].

Networks with two-point correlations

One can show that probability of a graph G in the ensemble ([\ref=corH]) can be transformed into the same form ([\ref=Pk1PG]) as the one for random networks with an expected degree sequence, where the linkage probability is given by ([\ref=corpij]). Performing the same calculations as in the case of ensembles analyzed in the previous subsection, one can prove that in the limit of sparse networks the degree distribution of a specific node is Poissonian ([\ref=prop2])

[formula]

where [formula] and p1i represents the connection probability given by ([\ref=corpij]). Again, the last formula supports the claimed equivalence between the analyzed maximum-entropy networks with two-point correlations and the class of correlated networks with hidden variables [\cite=BogPRE2003].

Conclusions

In this paper we have extended the information-theoretic approach to random networks that was very recently proposed by Park and Newman [\cite=ParkPRE2004]. We have concentrated on fluctuations over ensembles of undirected networks with non-interacting edges, including random networks with a given degree sequence and networks characterized by two-point correlations. We have studied a few fluctuation-dissipation relations characterizing susceptibilities of different networks to changes in the external fields. In the case of networks with a given degree sequence, we have argued that the scale-free topologies of real networks may arise as a result of self-organization of real systems into sparse structures with the low susceptibility to random external perturbations. Finally, we have also shown that maximum-entropy networks are equivalent to random networks with hidden variables.

Acknowledgments

A.F. acknowledges financial support from the Foundation for Polish Science (FNP 2005). A.F. and J.A.H. were partially supported by the State Committee for Scientific Research in Poland (Grant No. 1P03B04727). The work has also been supported by the European Commission Project CREEN FP6-2003-NEST-Path-012864.