Information-Theoretic Comparison of Quantum Many-Body Systems

Onicescu [\cite=Onicescu] introduced the concept of information energy E as a finer measure of dispersion distributions than that of Shannon's information entropy [\cite=Shannon1] [\cite=Shannon2]. So far, only the mathematical aspects of this concept have been developed, while the physical aspects have been neglected [\cite=Agop].

The information energy for a single statistical variable x with the normalized density ρ(x) is defined by

[formula]

For a Gaussian distribution of mean value μ, standard deviation σ and normalized density

[formula]

relation ([\ref=eq:eq1]) gives

[formula]

Thus

[formula]

Therefore, the greater the information energy E, the narrower the Gaussian distribution. E does not have the dimension of energy, but it has been connected with Planck's constant appearing in Heisenberg's uncertainty relation [\cite=Agop] [\cite=Ioannidou].

For a 3-dimensional spherically symmetric density distribution ρ(r) the obvious generalization of ([\ref=eq:eq1]) is

[formula]

and

[formula]

in position- and momentum-space respectively, where n(k) is the density distribution in momentum-space.

Er has the dimension of inverse volume, while Ek of volume. Thus the product ErEk is dimensionless and is a measure of the concentration (or the information content) of the density distribution of a quantum system. As seen from ([\ref=eq:eq3]) E increases as σ decreases (or the concentration increases) and Shannon's information entropy (or uncertainty) S decreases. Clearly, Shannon's information S and information energy E are reciprocal. In order to be able to compare them, we define the quantity

[formula]

as a measure of the information content of a quantum system in both position and momentum spaces.

In place of Shannon information, Brukner and Zeilinger [\cite=Brukner] propose the quantity

[formula]

from which they derive their notion of information content of a discrete probability distribution [formula]. The quantity [formula] is one of the class of measures of the concentration of a probability distribution given by Uffink [\cite=Uffink] [\cite=Maassen]. For a continous 3-dimensional density distribution ρ(r), relation ([\ref=eq:eq7]) is extended as (N = 1)

[formula]

and

[formula]

in position- and momentum space respectively, (r) is the equivalent uniform distribution defined according to the relation

[formula]

where ρ0=constant and RU=[formula] are fixed by the relation

[formula]

where

[formula]

and

[formula]

while

[formula]

and

[formula]

ñ(k) is the equivalent uniform distribution in momentum-space, defined in a similar way. Thus we define a measure of information content by the relation

[formula]

which gives ([\ref=eq:eq6]) putting (r) = ñ(k) = 0.

We calculate SE and SI as functions of the number of particles N for three quantum many-body systems, where ρ(r) and η(k) are calculated numerically:

Nuclei, using the Skyrme III parametrization of the nuclear field [\cite=Dover]. Here N is the number of nucleons in nuclei.

Atomic clusters, employing a Woods-Saxon potential parametrized by Ekardt [\cite=Ekardt]. Here N is the number of valence electrons.

A correlated bosonic system (atoms in a trap) [\cite=Moustakidis] [\cite=Fabrocini]. Here N is the number of atoms in the trap.

In Fig.1 we plot SE as a function of N for nuclei and clusters and in Fig.2 SI(N) for the same systems. In Fig.3 we plot SE(N) and in Fig.4 SI(N) for a correlated bosonic system. It is seen that SE depends linearly on N for both nuclei and atomic clusters. Also SI shows a similar trend (a power of N) for nuclei and clusters. However the dependence SE(N) and SI(N) is different for correlated bosons compared with nuclei and clusters.

Our fitted expressions are:

[formula]

[formula]

We can compare with the universal relation S(N) = a + b ln N (a,b are constants depending on the system) obtained recently [\cite=Massen2] for Shannon's information entropy for fermionic systems (atoms, nuclei and atomic clusters) and correlated bosonic systems [\cite=Moustakidis] (atoms in a trap). It was seen [\cite=Massen2] that S(N) shows the same dependence on N for all the systems considered i.e. nuclei, clusters, atoms and correlated bosons.

It is conjectured that nuclei and atomic clusters are equivalent from an information-theoretic point of view in the following sense: under any definition of information content (e.g. Shannon, Onicescu or Uffink), the dependence of information shows a similar trend (linear on ln N for Shannon, linear on N for Onicescu and a power of N for Uffink). However, the similarity breaks down for bosons. This indicates that SE and SI distinguish between fermions and correlated bosons i.e. they are finer measures of infomation than Shannon's S. Our results may contribute to the recent debate between Brukner-Zeilinger and Timpson for a possible inadequacy of the Shannon information [\cite=Brukner] [\cite=Timpson]