Fitting a Sum of Exponentials to Numerical Data

Introduction

From time to time the need arises to fit a sum of exponentials to numerical data. That means to approximate a given data- set consisting of pairs of real numbers (xj,yj) by the following expression:

[formula]

where [formula], [formula] and ai, bi are unknown real numbers which have to be chosen so that the fit becomes optimal.

If the bi were known, the task usually would be a well posed linear problem, but if the bi are unknown too, it turns out to be ill conditioned. The hopelessness of efforts dealing with this kind of problem has been described drastically by F.S. Acton [\cite=ac] in a chapter entitled "What not to compute".

At first sight, fitting equation [\ref=expsum] to a given data- set inevitably seems to be a nonlinear problem. However it has been noted [\cite=squire], [\cite=diamessis] that equation [\ref=expsum] may be expressed as a linear combination of powers of x and successive integrals of y(x), reducing the problem to a multilinear fitting procedure. This method is based on the fact that y(x) can be shown to satisfy an ordinary linear differential equation of N-th order with constant coefficients. The roots of the characteristic polynomial of this equation give the bi and the ai are identified as solutions to linear equations involving the bi and the derivatives of y(x) at x = 0. However, derivatives of experimental data- sets enhance the errors in the data, therefore it is desirable to eliminate them. Actually, [\cite=diamessis] shows already the way to do this, but only for the case N = 2 and a0  =  0, the general case not really being obvious. In the present paper, this method of linearizing the fitting procedure is revived and without referring to differential equations and without using derivatives, the general case is derived. Additionally, a method for estimating the errors in the exponential factors is presented. Of course, the problem remains ill posed, but linear fitting offers computational advantages over nonlinear approximation and also supplies estimates of the errors in the computed coefficients, which may be used to predict the errors in the exponential coefficients.

Results

The functions used to construct a linear approximation problem are powers of x and successive integrals of y(x). Before the announced relation can be asserted, some definitions are required.

Definition: The k-th integral of y(x) is defined recursively:

[formula]

Definition: βNij,αNij. Given the set [formula] of N exponential factors bk in equation [\ref=expsum] we consider the products of i different elements of B. Each of these products corresponds to a combination of i elements out of B, the number of these products therefore is

[formula]

as is proved in combinatorics. We assume that the products are ordered in some way. βNij then is the j-th of these products. Additionally, we define βN01 = 1.

αNij is the sum of all ak in equation [\ref=expsum] excluding those whose index is equal to that of one of the b's in βNij. By definition, [formula]. Obviously, each αNij contains at least a0. Example: N=3

With these definitions, the central statement of this article now may be asserted:

[formula]

Assuming the validity of equation [\ref=thatsit] the task now consists in approximating the data- set {(xj,yj)} by a linear combination of the 2N functions [formula] [formula] plus a constant. By standard linear approximation techniques the coefficients [formula] [formula] and the intercept d0 may be determined together with their errors [formula] [formula] and Δd0. It follows that

[formula]

Given the ci in [\ref=Vieta] Vieta's root theorem asserts that the bi are the N roots of the polynomial

[formula]

As soon as the bi are known, the expressions [\ref=a1] and [\ref=a2] represent a system of N+1 linear equations for the N+1 coefficients ai.

If the Δci are small, the relation between the errors may be approximated by the linear terms of the Taylor- series for P(x).

[formula]

As the bk are roots of P, ΔP should be zero and therefore, inserting bk for x, we get:

[formula]

Treating the ci and bk as probability variables with standard deviations sci and sbk, the standard deviation and therefore the estimated error of bk is given by

[formula]

As usual, Cov(ci,cj) means the covariance between ci and cj.

It remains to show that equation [\ref=thatsit] is valid. For this purpose it is useful to state some properties of the coefficients β.

For any l with 1  ≤  l  <  N and any i  ≤  N the sum of all βNlm may be divided into the sum of all βNlm containing bi and those not containing bi:

[formula]

With β( - i)(N - 1)lj we denote the products not containing bi that is, which are chosen from the set [formula] containing N-1 elements and not containing bi. For l  ≤  0 we define β( - i)(N - 1)lm = 1.

An important special case of [\ref=indgen] results if i = N. Then β( - N)(N - 1)lm = β(N - 1)lm and the following expression results:

[formula]

For a proof of equation [\ref=thatsit] consider the following system of equations:

[formula]

The validity of [\ref=defEq] is easily seen by performing the integrals in equation [\ref=defI] analytically.

Now consider the following linear transformations defined recursively on the set of equations [\ref=defEq]:

[formula]

For this kind of transformation a rather general relationship holds:

[formula]

Proof: Induction for h. For h = 1, proposition [\ref=lemTransf] just repeats the definition of I(1)k. Now assume that [\ref=lemTransf] holds for I(h)k. Then

where [\ref=ind1] has been used in order to obtain the last line. Obviously, this result may be converted into whereby the proof of [\ref=lemTransf] is completed.

Consider now I(N)0. By [\ref=lemTransf]

[formula]

Inserting [\ref=defEq] this expands into

[formula]

The motive for applying transform [\ref=defTransf] to I0 was to get rid of the exponential terms. The following proposition asserts that equation [\ref=basis] is actually free of exponential terms:

[formula]

Proof:

For h = 1 the assertion is trivial. Now assume that [\ref=lemMinOne] is valid for h. Then the following calculations prove the truth for h+1 and therefore for all h:

Using equation [\ref=lemMinOne] for and collecting all terms the last expression evaluates to -1.

To complete the proof of equation [\ref=thatsit] some more transformations on formula [\ref=basis] are required:

[formula]

where and

Using [\ref=indgen], for l  >  1 Sil transforms into Substituting in the second part of this sum q for p+1 this expression transforms into Therefore Sil is the negative sum of all βNlm which contain bi. Consequently, in expression [\ref=intermed] only those ai are not cancelled for which βNlm does not contain bi. For that, [\ref=intermed] may be written as which proves equation [\ref=thatsit]. Example: Consider this sum of two exponentials and a constant: The function is evaluated in in the interval 0  ≤  x  ≤  6 at Np equally spaced points. The discrete function values are multiplied by one plus a gaussian distributed random variable so that the relative error has the standard deviation σ. I1 and I2 are calculated using the trapezoidal method. For different settings of Np and σ the coefficients c1 and c2, d0 (the intercept), d1 and d2 and the corresponding errors of c1 and c2 as well as the covariance between these two factors are determined by the commercial statistics program STATISTICA and are listed in table 1. The parameters b1, b2, a0, a1, a2 and the errors Δb1 and Δb2 are calculated from these coefficients as described above and are listed in table 2.

The results show that for small errors in the coefficients the estimated variance of b1 and b2 is also small and the estimate is realistic. The first case was computed without artificial noise, in this case the accuracy seems to be determined mainly by the statistics program. Adding noise deteriorates the accuracy of the results rapidly. While a relative error of 0.0001 (case 2) still leads to a reasonable result, the tenfold relative error (case 3) already means that the calculated uncertainty of b2 is about 7%. A one- percent inaccuracy in the data (case 4) gives a result even with the first digit uncertain. As case 5 where the number of data- points is raised to 2001 shows, increasing the size of the data- set may at least partially compensate for noise.