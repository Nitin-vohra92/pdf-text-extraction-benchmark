Two non-regular extensions of large deviation bound

Keywords: Non-regular family, Large deviation, Relative Rényi entropy, Point estimation, Interval estimation

Introduction

As is known, Fisher information gives the bound of the accuracy of estimation. However, this fact holds only for a regular distribution family. Hence, if the distribution family does not satisfy the regurality condition, we have to treat the information quantity as alternative of Fisher information. In this paper, we consider this problem only for the location shift family on the real line [formula] such that the support depends on the true parameter. So far, this problem has been mainly studied concerning the mean square error. This paper treats this problem with the large deviation criterion, which was introduced by Bahadur [\cite=Ba60] [\cite=Ba67] [\cite=Ba71]. Owing this method, we can discuss the difference between the interval estimation and the point estimation. For this analysis, we introduce two type extensions of Bahadur's large deviation bound. One is the quantity α1(θ), which can be regarded as the limit of the accuracy of the interval estimation. The other is the quantity α2(θ), which can be regarded as the limit of the accuracy of the point estimation. We also show that these two quantities coincide in a regular distribution family, but they do not coincide in a non-regular distribution family.

In order to evaluate the actuary of a sequence of estimators [formula] for a probability distribution family {pθ|θ∈Θ} from the large deviation viewpoint, Bahadur focused on the error probability pnθ{|Tn  -  θ|    >  ε}, which goes to 0 exponentially. That is, the rate of the quantity -   log pnθ{|Tn  -  θ|    >  ε} is in order n. Hence, he discussed the following exponential decreasing rate of the error probability with a fixed error bar [\cite=Ba60] [\cite=Ba67] [\cite=Ba71]:

[formula]

For this purpose, he treated Kullback-Leibler's relative entropy (divergence) [formula]. Using the relation between its limit and Fisher information, he characterized the slope [formula] of the exponential rate by the Fisher information. Since this discussion is a fundamental for this paper, it is summarized in Section [\ref=s2].

However, this method cannot be applied to the family in which the support depends on the true parameter, because Kullback-Leibler's relative entropy diverges in this case. On the other hand, it is not impossible to define Fisher information even if Kullback-Leibler's relative entropy diverges. In this paper, we focus on the the limit of the relative Rényi entropies [formula], which is deeply treated in a non-regular location shift family by Hayashi [\cite=Haya-2]. In Section [\ref=s3], we define two quantities α1(θ) and α2(θ), and give their upper bounds [formula] and [formula]. Both of α1(θ) and α2(θ) are defined as the optimal slope of the exponential rate [formula] concerning ε at the limit ε  →  0, but their definitions are slightly different. Also we derive a necessary and sufficient condition for the coincidence of these two upper bounds. Since these upper bounds are proved by a very general treatment in Section [\ref=s4], they are valid for general non-regular families. In Section [\ref=s5], we focus on several estimators useful for location shift families. We calculate their exponential rates [formula] and their slopes [formula] concerning ε at the limit ε  →  0.

Bahadur theory

In this section, we begin by summarizing the results reported by Bahadur [\cite=Ba60] [\cite=Ba67] [\cite=Ba71], who discussed the decreasing rate of the tail probability in the estimation for a distribution family. Given n-i.i.d. data [formula], the exponential rate [formula] of the estimator [formula] is written as

[formula]

where the exponential rates of half-side error probabilities are given by

[formula]

When an estimator [formula] satisfies the weak consistency

[formula]

using the monotonicity of KL-divergence, we can prove the inequality

[formula]

Note that if, and only if, the family is exponential, there exists an estimator attaining the equality ([\ref=Baha4]) at [formula]. Therefore, for a general family, it is difficult to optimize the exponential rate [formula].

Instead of the exponential rate [formula], We usually consider the slope of the exponential rate:

[formula]

In this case, when the Fisher information Jθ satisfies the condition

[formula]

the inequality

[formula]

holds. Moreover, as was proven by Fu [\cite=Fu73], if the family satisfies the concavity of the logarithmic derivative lθ(ω) for θ and some other conditions, the MLE [formula] attains the equality of ([\ref=Baha1]). These facts are summarized in the two equations:

[formula]

As is mentioned later, these equations imply an interesting relation between the point estimation and the interval estimation.

Upper bounds

In this paper, the relative Rényi entropies [formula] substitute for the KL divergence. Note that the order of [formula] is not necessarily ε2 at the limit ε  →  0. However, its order is independent of the parameter s, as is guaranteed by the inequalities

[formula]

which are proven in Lemma [\ref=L38] of the Appendix. In several cases, the order of the exponential rate [formula] coincides with the order of the relative Rényi entropies [formula]. In the following, we use a strictly monotonically decreasing function g(x) such that [formula] and g(0) = 0.

Following equations ([\ref=Baha2]) and ([\ref=Baha3]), we define two extensions of Bahadur's bound (slope) α(θ) as

[formula]

where

[formula]

In the interval estimation, we consider only the error probability concerning the fixed interval

[formula]

for all ε. The performance of an estimator [formula] is characterized as the limit [formula]. Then, the bound of the performance of the point estimation is given by [formula]. In the regular family, equation ([\ref=Baha2]) can be regarded as the bound of the point estimation, while equation ([\ref=Baha3]) can be regarded as the limit of the bound of the interval estimation because [formula] corresponds to the bound of the interval estimation with width 2ε of the confidence interval. Therefore, we can conclude that there is no difference between the point estimation and the limit of the interval estimation in the estimation in the regular family. In the following, we consider whether there exists a difference between them.

Note that we take infimum inf θ  -  ε  ≤  θ'  ≤  θ  +  ε into account in ([\ref=8-a]), unlike ([\ref=2.5]). As was pointed out by Ibragimov and Has'minskii [\cite=IH], when KL-divergence is infinite, there exists a consistent super efficient estimator [formula] such that [formula] and [formula] is infinite at one point θ. Therefore, we need to take the infimum inf θ  -  ε  ≤  θ'  ≤  θ  +  ε into account. In this situation, we do not need to limit estimators to weakly consistent ones. As is proven in the next section, we can obtain the following theorems.

When the convergence [formula] is uniform for 0    <  s    <  1, the inequality

[formula]

holds, where κ and Isg,θ are defined by

[formula]

Lemma [\ref=ap1] proven in Appendix [\ref=A], guarantees the existence of such a real number κ.

Note that the function s  →  Isg is concave and continuous because the function [formula] is concave and continuous. Therefore, when Isg,θ = I1 - sg,θ, we have

[formula]

If the convergence [formula] is uniform for s∈(0,1) and θ∈K for any compact set [formula], the inequality

[formula]

holds.

In our proofs of these theorems, Chernoff's formula and Hoeffding's formula in simple hypothesis testing play important roles.

As was proven by Akahira [\cite=Ak], under some regularity conditions for a distribution family, the equation

[formula]

holds. When we choose g(x) = x2, we have [formula], and the relations

[formula]

hold. In particular, if the distribution family satisfies the concavity of the logarithmic derivative lθ(ω) for θ and some other conditions, the bound [formula] is attained by the MLE. Thus, the relations [formula] hold.

As a relation between two bounds [formula] and [formula], we can prove the following theorem.

The inequality

[formula]

holds, and ([\ref=14]) holds as an equality if and only if the equations

[formula]

hold. When κ  ≤  1, ([\ref=14]) holds as an equality if and only if equation ([\ref=16.3]) holds.

When Isg,θ is differentiable, condition ([\ref=16.3]) is equivalent to [formula].

Proofs of upper bounds

In our proofs of Theorems [\ref=thm1] and [\ref=thm2], Chernoff's formula and Hoeffding's formula in simple hypothesis testing are essential, and are summarized as follows. Let the probability p on Ω be the null hypothesis and q be the alternative hypothesis. When we discuss a hypothesis testing problem concerning n-i.i.d. data, we call a sequence [formula] a test, where An is an acceptance region, which is a subset of Ωn. The first error probability e1(An) and the second error probability e2(An) are defined as

[formula]

and their exponents are given by

[formula]

Chernoff [\cite=Cher] evaluated the exponent of the sum of the two errors as

[formula]

which is essential for our proof of Theorem [\ref=thm1]. This bound is achieved by both of the likelihood tests [formula] and [formula].

Hoeffding proved another formula for simple hypothesis testing [\cite=Hoef]:

[formula]

This formula is essential for our proof of Theorem [\ref=thm2].

Applying equation ([\ref=11]) to the two hypotheses pθ  -  ε and pθ  +  ε, we obtain

[formula]

Taking the limit ε  →  0, we have

[formula]

In the regular case, this method was used by Sievers [\cite=Sie].

Hoeffding's formula ([\ref=Hoe]) yields

[formula]

The uniformity of ([\ref=uni]) guarantees that

[formula]

From ([\ref=1-a]), we have

[formula]

We define the set C1 and α0 as

[formula]

Note that the function s  ↦  Isg,θ is concave. We define the convex function g(x) and another set C2 as

[formula]

Since inequality ([\ref=cut3]) guarantees

[formula]

the inequality

[formula]

holds. Relations ([\ref=28-1]), ([\ref=28-2]), and ([\ref=28-3]) guarantee the relation

[formula]

Applying Lemma [\ref=concave], we have

[formula]

In the following, we divide our situation into three cases κ    >  1,κ = 1,1    >  κ    >  0. When κ    >  1, relation ([\ref=28-4]) guarantees that

[formula]

Therefore,

[formula]

which implies ([\ref=9-a]).

When κ = 1, similarly, we can easily prove

[formula]

Finally, we consider the case where κ    <  1. Since function g is concave on (0,α0), there exists η0∈(0,1) such that

[formula]

Since C1 and Cc2 are convex, there exists a real number s0∈(0,1) such that

[formula]

In general, for any s∈(0,1), using ([\ref=4-19]), we obtain

[formula]

which lead to ([\ref=9-a]).

It is trivial in the case of κ = 1. When κ   > 1, it is also trivial because

[formula]

Next, we consider the case κ    <  1. The inequality

[formula]

follows from the two inequalities

[formula]

We thus obtain ([\ref=14]). In the following, we prove that the equality of ([\ref=14]) implies ([\ref=16.3]) and ([\ref=16.3]) implies ([\ref=15]) and the equality of ([\ref=14]) in the case where κ    <  1. If we assume that the equality of ([\ref=19]) holds, the equalities of ([\ref=17]) and ([\ref=18.9]) hold at the same s. The equality of ([\ref=18.9]) holds if and only if [formula]. Therefore,

[formula]

which is equivalent to ([\ref=16.3]). If we assume that ([\ref=16.3]) holds, inequality ([\ref=18.9]) guarantees that

[formula]

Substituting [formula] into s at the left hand side (LHS) in the definition of [formula], we obtain

[formula]

Thus, equation ([\ref=15]) holds. Combining ([\ref=16.3]) and ([\ref=15]), we obtain the equality of ([\ref=14]).

Exponential rates of useful estimators and their slopes

In the following, we discuss the exponential rate [formula] of a useful estimator [formula] for a location shift family [formula], where f is a probability density function (pdf) on [formula]. In particular, we focus on the case where the support of f is (a,b). Further, we assume that the pdf f is C1 continuous and satisfies that

[formula]

where κ1,κ2    >  0, as for the beta distributions [formula].

When its support is a half line (0,  ∞  ) as for the gamma distribution and Weibull distribution, our situation results in the above case where A2 = 0 if f is C3 continuous and [formula]. Also, when κ1    >  κ2, our situation results in the above case where A2 = 0.

In the first step, we will treat estimators useful for point estimation. After this discussion, we will discuss estimators useful for interval estimation.

Exponential rates of estimators useful for point estimation

When the support of f is (a,b), the two estimators [formula] and [formula] are useful for point estimation. These performances are characterized as follows.

The estimators [formula] and [formula] satisfy

[formula]

Since

[formula]

we obtain ([\ref=3-8-14]) and ([\ref=3-8-15]).

In order to strike a balance between two exponential rates [formula] and [formula], we define the convex combination (CC) estimator [formula] with the ratio λ:1 - λ of the two estimators [formula] and [formula], where 0    <  λ    <  1. These are characterized as follows.

The convex combination (CC) estimator [formula] satisfies that

[formula]

Define [formula]. Since the estimators [formula], and [formula] are covariant for location shift, we may discuss only the case that θ = 0. From the relation n    >  θ    >  θ  -  ε, we obtain the second equation of ([\ref=3-8-14]). Its joint probability density function [formula] is given by

[formula]

By defining

[formula]

The equation ([\ref=4-5-1]) yields

[formula]

From the continuity of [formula] and [formula], the equations

[formula]

hold. This implies the first equation of ([\ref=3-8-14]) and ([\ref=34.1]). In addition, we can similarly show the same fact for ([\ref=34.2]).

Next, we focus on the maximum likelihood estimator [formula].

When the function x  ↦   log f(x) is concave, MLE [formula] satisfies that

[formula]

Also, we have the following evaluations in the same assumption.

[formula]

This lemma is essentially proved as a one step of the proof of the main theorem in the paper [\cite=Fu73]. However, he proved the main theorem with a more general assumption and a different notation. Hence, it is not easy to find the relationship between the notation of the present paper and that of Fu's paper. Further, since his proof of this part is too short, it seems that a non-expert of large deviation theory cannot follow his proof. Therefore, for the reader's convenience, a proof of this lemma is given as follows.

Equations ([\ref=34]) and ([\ref=36]) are trivial. We prove ([\ref=33]). From the assumption that for any [formula], the function [formula] is concave on [formula], the function [formula] is monotonically decreasing, where [formula] on [formula]. When [formula] belongs to the support of fθ', the condition [formula] is equivalent to the condition

[formula]

Denoting the conditional probability f{A|x∈B} under the condition x∈B, we can evaluate

[formula]

where the probability density function fθ,ε is defined on the support (a + ε,b) by

[formula]

Chernoff's theorem (Theorem 3.1 in Bahadur [\cite=Ba71]) guarantees that

[formula]

Combining ([\ref=31]) and ([\ref=32]), we obtain ([\ref=33]). Similarly, we can prove ([\ref=35]).

Next, in order to show ([\ref=4-5-3]), we choose a real number δ  >  0. When

[formula]

the condition

[formula]

holds. Hence, substituting θ  -  ε and ε into θ' and δ, respectively, we have

[formula]

As is mentioned at ([\ref=11]) in Section [\ref=s4], the exponential rate of the RHS is equal to [formula]. Thus, we obtain ([\ref=4-5-3]). Similarly, we can prove ([\ref=4-5-4]).

When f(x) is monotonically decreasing, the MLE θML,n equals the estimator n.

For any data [formula], if [formula], [formula]. Conversely, if [formula], the obtained [formula]. Thus, n is the MLE.

Slopes of exponential rates of estimators useful for point estimation

We discuss the slopes of exponential rates of estimators discussed in the above. First, we focus on the estimators [formula] and [formula]. From Lemma [\ref=l34.3], the estimators [formula] and [formula] satisfy that

[formula]

Next, we proceed to the convex combination estimator [formula]. When κ1  =  κ2  =  κ, Lemma [\ref=l34.3-2] yields the equations

[formula]

Since [formula], the relations

[formula]

hold.

When the function x  ↦   log f(x) is concave, the relations ([\ref=4-5-3]) and ([\ref=4-5-4]) yield that

[formula]

Further, when f(x) is monotonically decreasing, Lemma [\ref=4-5-7] and ([\ref=4-5-6]) imply that

[formula]

Exponential rates of estimators for interval estimations

In order to improve the exponential rate for a fixed width 2ε, we focus on the likelihood ratio estimator, which is discussed by Huber, Sievers, and Fu from the large deviation viewpoint concerning the regular family[\cite=Hu] [\cite=Sie] [\cite=Fu85]. Assume that the function dε(x): = f(x  +  ε) / f(x  -  ε) is monotonically decreasing w.r.t. x when both f(x  +  ε) and f(x  -  ε) are not zero. Then, we can define the likelihood ratio estimator [formula], which depends on the constant ε    >  0, as shown by

[formula]

where the monotonically decreasing function k(z) is defined by

[formula]

Note that when log f(x) is concave, the above condition is satisfied. This definition is well defined although the monotonically decreasing function k(z) is not continuous.

If the support of f is (a,b), we need to modify the definition as follows. In this case, we modify the estimator θLR,ε,n by using the two estimators [formula] and n. When [formula], the estimated value is defined by ([\ref=725.2]) in the interval [formula]. When [formula], we define [formula]. Moreover, when the support of f is a half line (0,  ∞  ), the estimated value is defined by ([\ref=725.2]) in the half line (n  -  ε,  ∞  ).

Then, the exponential rate is characterized as follows. (A regular version of this lemma was discussed by Huber [\cite=Hu], Sievers [\cite=Sie], and Fu [\cite=Fu85].)

When log f(x) is concave, the equation

[formula]

holds, where fθ(x): = f(x - θ).

Therefore, the equality of inequality ([\ref=7-a]) holds in this case.

Note that [formula] and [formula] because of the shift-invariance. From the concavity, the condition θLR,ε,n  ≤  θ is equivalent to the condition sup {z|k(z)    <  0}  ≤  θ, which implies that k(θ)  ≥  0. Thus, we have

[formula]

Conversely, the condition

[formula]

implies that k(θ)    >  0. Thus, we have sup {z|k(z)    <  0}  ≤  θ, which is equivalent to the condition θLR,ε,n  ≤  θ. Therefore, we have the relations

[formula]

Similarly, we can prove

[formula]

Applying ([\ref=11]), we obtain

[formula]

which implies equation ([\ref=725.3]).

Next, in order to improve the estimator [formula] for a fixed value ε, we define the estimator [formula]. This estimator satisfies the following lemma.

The estimator [formula] satisfies

[formula]

Further, when f(x) is monotonically decreasing, the exponential rate of the estimator [formula] has another form

[formula]

From the construction of [formula], the relations ([\ref=3-8-1-a]) and ([\ref=3-8-1-b]) follow from ([\ref=3-8-14]). Then, we proceed to the proof of ([\ref=3-8-1]). Since n    >  θ, we have ε,n    >  θ  -  ε, which implies ([\ref=3-8-1]). If ε,n  ≥  θ  +  ε, we have n  ≥  θ  +  2ε. Thus, f(xi - (θ + 2ε))  ≥  f(xi  -  θ) for any [formula]. Therefore,

[formula]

Conversely, if [formula], we have n  ≥  θ  +  2ε. Thus,

[formula]

Since the likelihood test [formula] achieves the optimal rate ([\ref=11]), we have

[formula]

Slopes of exponential rates of estimators useful for interval estimation

Next, we proceed to the slopes of exponential rates. Concerning the estimator [formula], from ([\ref=3-8-1-a]) and ([\ref=3-8-1-b]), we obtain the following characterization:

[formula]

Further, when f(x) is monotonically decreasing, the equation ([\ref=3-8-1]) implies the equation

[formula]

When the function log f(x) is concave, Lemma [\ref=thm7.2] yields the equation

[formula]

Thus, the following theorem holds.

When the function log f(x) is concave or monotonically decreasing, the relation

[formula]

holds.

Therefore, if the condition of the above theorem holds and condition ([\ref=16.3]) in Theorem [\ref=thm3] does not hold, i.e., Isg,θ is not symmetric, then the first criterion is different from the second one, i.e.,

[formula]

Conclusion

We have discussed large deviation theories under a more general setting than existing studies. For this purpose, we introduced two criteria for the bound of estimation accuracy from the large deviation viewpoint. One criterion α1(θ) corresponds to the interval estimation with taking the limit that the width of error bar goes to 0. The other α2(θ) corresponds to the point estimation. The upper bounds of them are given by the limit of the relative Rényi entropy. These characterizations have been derived by the method of simple hypothesis testing. We have also calculated the slope of the exponential decreasing rates of several estimators. As a result, we have succeeded in deriving upper bounds [formula] and [formula] of α1(θ) and α2(θ) and a necessary and sufficient condition for the coincidence of these two upper bounds [formula] and [formula]. In the next step, we have treated several estimators as candidates to attain the optimal bounds α1(θ) and α2(θ) in a local shift family. That is, we derived lower bounds of α1(θ) and α2(θ). Further, we derived a sufficient condition for the gap between two criteria α1(θ) and α2(θ).

Unfortunately, we cannot calculate two criteria α1(θ) and α2(θ) in the concrete examples. For this purpose, we need to calculate the limit of the relative Rényi entropy, which was discussed in another paper [\cite=Haya-2]. In the next paper, we will treat this calculation based on the obtained result and the result by Hayashi [\cite=Haya-2].

Historically, Nagaoka initiated a discussion of two kinds of large deviation bounds in a quantum setting [\cite=Naga2] [\cite=Naga3], and Hayashi discussed these in more depth [\cite=Haya]. The two kinds of large deviation bounds do not necessarily coincide in a quantum setting. In quantum setting, this difference corresponds to the non-uniqueness of quantum extension of Fisher information. In the quantum case, α1(θ) corresponds to Kubo-Mori-Bogoljubov (KMB) inner product and α2(θ) does to Symmetric logarithmic derivative (SLD) inner product. This research is strongly motivated by this quantum study. In the quantum case, the family of estimators attaining the bound [formula] depends on the true parameter. However, in some of non-regular location families, such a family of estimators does not depends on the true parameter. This is different point between our setting and quantum setting. Gaining an understanding of these differences from a unified viewpoint remains a goal for the future.

Acknowledgment

The present study was supported in part by Laboratory for Mathematical Neuroscience, Brain Science Institute, RIKEN and MEXT through a Grant-in-Aid for Scientific Research on Priority Area "Deepening and Expansion of Statistical Mechanical Informatics (DEX-SMI)," No. 18079014. The author is grateful for Professor Shun-ichi Amari to helpful discussions on this topics.

Concave function

When a concave function f  ≥  0 is defined in (0,1),

[formula]

Substituting s into t we have

[formula]

Taking the infimum inf x   > 0, we obtain

[formula]

Next, we proceed to the opposite inequality. From the concavity of f, we can define the upper derivative [formula] and the lower derivative [formula] as

[formula]

Since the concavity guarantees that

[formula]

we obtain

[formula]

Therefore,

[formula]

The proof is now complete.

Other lemmas

When g is strictly monotonically decreasing and continuous, g(0) = 0, and the limit [formula] exists, there exists κ    >  0 such that

[formula]

Let h(x) be the RHS of ([\ref=82]). Since

[formula]

h(xy) = h(x)h(y). Thus, there exists κ    >  0 satisfying ([\ref=82]).

For [formula], the inequalities

[formula]

hold.

Since [formula], the Hölder inequality guarantees that

[formula]

Thus, we obtain

[formula]

Similarly, since [formula], we can apply the Hölder inequality as

[formula]

which implies that

[formula]