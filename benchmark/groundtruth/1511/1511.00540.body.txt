Spiking Analog VLSI Neuron Assemblies as Constraint Satisfaction Problem Solvers

Introduction

Constraint satisfaction problems (CSPs) include some of the most prominent problems in science and engineering. A CSP is defined by a set of variables and a set of conditions (constraints) on those variables that need to be satisfied simultaneously. Solutions to a given CSP typically form a vanishingly small subset of an exponentially large search space, rendering this type of problem NP-hard in the general case. Many CSP solvers therefore involve problem-specific heuristics to avoid searching the entire space. Alternatively, one can consider a massively parallel system of simple computing elements that tackle different parts of the problem simultaneously, and discover a global solution through communicating local search mechanisms. Here, we explore event-based neural hardware as a substrate of computation, however, there are various other approaches, such as quantum annealing [\cite=Farhi2001], special cellular automata hardware [\cite=Fujita2010], or oscillator networks [\cite=Mostafa2015]. The idea to use recurrent neural networks for finding solutions to computationally hard problems goes back to Hinton [\cite=Hinton1984] and Hopfield [\cite=Hopfield1985]. While the deterministic Hopfield network fails in the general case because of local minima it can get caught in, the probabilistic Boltzmann machine proposed by Hinton and Sejnowski [\cite=Hinton1984] overcomes this limitation by sampling its states from a probability distribution rather than evolving along a deterministic trajectory.

It has been shown recently, how such stochastic samplers can be implemented in networks of spiking neurons [\cite=Buesing2011] [\cite=Maass2014]. The resulting neural sampling framework has been applied to constraint satisfaction problems by Jonke et al. [\cite=Jonke2014], demonstrating advantages of the spiking network approach over standard Gibbs sampling in certain cases. The main technological advantage of using such neural samplers in practical applications lies in the ability to implement spiking neural networks efficiently in neuromorphic hardware. However, the models proposed in [\cite=Buesing2011] [\cite=Jonke2014] are difficult to directly transfer onto spiking VLSI neurons, because they require individual neurons to emit spikes probabilistically, sampling from a probability distribution of a specific form. This is not easily implementable in an electronic circuit without explicit sources of noise, or random number generators. Here we show, for the first time, how a similar sampling mechanism can be implemented in a system of integrate and fire neurons, implemented using analog VLSI circuits, to find solutions to CSPs. To avoid the additional cost of implementing dedicated noise sources on chip, we propose a mechanism that makes use of small amounts of (thermal) noise that is available in any analog electronic system to achieve the desired stochastic network dynamics. We demonstrate these principles using a standalone system, based on a reconfigurable neuromorphic processor [\cite=Qiao2015] which integrates a configurable network of adaptive exponential integrate and fire neurons and synapse circuits with biophysically realistic dynamics [\cite=Chicca2014]. Once programmed for a given CSP, the system will output streams of candidate solutions. Our experimental results demonstrate that these samples predominantly represent configurations with no or few violated constraints.

Solving CSPs with Spiking Neural Networks

Solving an NP-complete decision problem stochastically through sampling means transforming it into an NP-hard optimization problem, and solving it using some kind of annealing mechanism rather than sophisticated algorithmics. Thereby, a cost function is formulated such that the solutions to a given problem are transformed into optima. It is not clear whether the corresponding optimization problem is easier to solve in general, however, typically the conversion can be done in polynomial time, and in some cases the optimization problem can be parallelized more easily or more efficiently than the decision problem. Many types of problems can be transformed to simple graph structures with little effort [\cite=Lucas2014], and thus almost naturally map onto a network of nodes that interact through positive and negative links. In contrast to most conventional algorithmic solvers, the optimization-based approach does not depend on problem-specific heuristics, and can thus be regarded as more general.

As a first step, we outline how arbitrary discrete CSPs can be mapped onto a network of neurons. We consider constraint satisfaction problems defined by a set of n discrete variables [formula] on finite domains and a set of constraints, each linking several of those variables, e.g. [formula] etc. Without loss of generality, any such problem can easily be expressed in terms of binary variables by using a one-hot scheme, i.e. by representing each discrete variable xi as a vector of binary variables (xi,k)k, where exactly one is active at any time, xi = a  ⇔  xi,a = 1, ~ xi,k  ≠  a = 0. Furthermore, a CSP can be written in conjunctive normal form, i.e. of the form [formula], where the lij are literals (binary variables or their negations).

In a network of spiking neurons, following the models introduced in [\cite=Buesing2011] [\cite=Nessler2013], the state of each variable is represented by the spiking activity of multiple cells. There is one cell per value the variable can assume, and a cell is called active at time t if it has emitted a spike within a certain time window

[formula]

Stochastic Dynamics in an Analog VLSI Neural Network

In the following, we introduce a simple neuron model that can be used to describe our analog VLSI implementation of neural sampling. The stochastic spiking neurons used in previous work [\cite=Buesing2011] [\cite=Nessler2013] [\cite=Jonke2014] can be approximated by integrate-and-fire neurons, which are injected a large amount of noise, as proposed by [\cite=Merolla2010] [\cite=Neftci2013] [\cite=Petrovici2013]. This approach, however, requires an independent noise source for every cell, and therefore cannot be easily implemented directly in hardware. Instead, we propose a mechanism that is based on conventional deterministic neuron models, and becomes stochastic through slight jitter in the duration of temporally extended pulses in analog VLSI, thus generating stochastic network dynamics. Such small (thermal) fluctuations are inherent to any analog electronic system, and thus can be exploited in analog hardware implementations of the proposed model.

To illustrate our model, we assume simple leaky integrate-and-fire neurons that produce an output spike and remain in refractory period for a duration [formula] when their membrane potential crosses a threshold Θ. A spike in one cell triggers an excitatory or inhibitory postsynaptic potential (PSP) at synapses connecting it to other cells. In the simplest case, which is also at the core of previous models [\cite=Buesing2011] [\cite=Nessler2013], this can be thought of as a rectangular signal of duration [formula] or [formula], respectively. We make the assumption that the magnitudes of these signals are large enough to either trigger a spike in the target cell almost instantaneously (for excitatory inputs), or silence the cell completely (for inhibitory inputs), such that additional excitatory inputs have no effect. Note that the refractory period can be thought of as a strong inhibitory input of a cell to itself. The stochasticity in our system is then introduced by small amounts of noise in the duration of those PSPs and refractory periods. As a consequence, we can regard [formula], [formula], and [formula] as mean values, and in practice the durations are jittered around those values, as shown in .

As an example, assume two neurons that are coupled through inhibitory connections and are driven by a constant external current, and assume further that [formula]. In this circuit, whichever cell became active first would keep inhibiting the other cell. This is due to the short refractory period, which lets the active neuron spike again before the IPSP it provides to the other cell ends. This network would end up in a local optimum, and would never explore any other possible state where the second cell is active. If, however, the refractory period [formula] and the inhibitory pulse width [formula] are of similar size, small amounts of noise in the analog system, leading to jitter in the duration of both pulses, will sometimes cause the inhibitory PSP to be longer than the refractory period, and vice versa. Such a system could indeed explore all possible states. This mechanism can be regarded as a kind of noise amplification or, alternatively the system can be regarded as being close to a critical point, where vanishingly small fluctuations can lead to dramatically different behavior. Intuitively, longer refractory periods cause more explorative behavior, whereas short refractory periods let the network settle down in local energy minima. Note that all time constants in the system are defined relative to each other, and the relation to real time is of little relevance. Thus, extending the refractory period is equivalent to shortening the PSPs, i.e. weakening the links between nodes. In that sense, the refractory period can be regarded as a kind of temperature, that could be used in an annealing schedule to steer the dynamics of the network.

The assumptions made to construct our model are fulfilled in intrinsically noisy analog neural hardware which can, with this method, be configured such that very small fluctuations in the electronic signals can lead to large deviations in the network dynamics. The jitter in the refractory period or PSP durations is thereby introduced by thermal fluctuations in the analog signals representing those variables. For our experimental setup, we used the programmable neuromorphic device described by [\cite=Qiao2015], comprising 256 integrate and fire neurons and 128k programmable synapses, to implement a network that evolves in real time and produces a stream of output events that can be interpreted as states of the system. The network described in was programmed into the hardware by setting the respective inhibitory connections and run by injecting small amounts of direct current into each cell. The mean values of the tunable time constants [formula] and [formula] were set to approximately 100 ms for all cells, whereby, due to fabrication-induced deviations, the values are not exactly the same across neurons. The sampling rate at which the network states were evaluated was set to 10 Hz, such that the network activity was binned over 100 ms for each sample.

Experimental Results

shows representative spiking activity of the Sudoku solver network implemented and running on an analog VLSI chip. The system occasionally converges to states solving the problem (0 constraints violated), however, it is also able to escape from those local optima and explore other possible states. The "temperature" or "exploration rate", can be controlled by tuning the neuron parameters, i.e. the IPSP duration or the length of the refractory period. As expected from our considerations above, the system can be constrained to lower energy regions by lowering the temperature parameter, i.e. decreasing the refractory period. shows the distribution of generated samples, ordered by the number of constraints violated, i.e. the measure of "energy" that we intend to minimize. As expected, we observe fundamentally different behavior for the case where the refractory period is larger than the IPSP, compared to the case where it is smaller than the IPSP. This leads to a phase transition-like phenomenon when this threshold is crossed. For longer refractory periods the distribution can be well fitted by an exponential function, indicating a strong concentration around the low energy states. If the refractory period is shorter than the IPSP, however, the distribution is even more concentrated around zero, and can be approximated by a double exponential function. As shown in , the same effect can be observed in the distribution of energy jumps, i.e. the difference in the number of constraints violated when the system moves from one state to the next. Good fits are again obtained by exponential and a double exponential functions, respectively.

Conclusion

We present the first analog VLSI implementation of a CSP solver based on neural sampling with spiking neurons. Our contribution is a simple neuron model that achieves stochasticity without the external noise sources required by previous approaches, but instead exploits small variations and noise in the duration of temporally extended signals. The empirical results obtained from an analog neuromorphic processor demonstrate the function and performance of the proposed mechanism. While the hardware system used in our experiments is deliberately slowed down to operate at timescales similar to real neurons, our approach could, without restrictions, be used with much faster hardware to solve computationally hard problems quickly and efficiently. We can empirically relate the duration of the refractory period, or alternatively the duration of PSPs, to a temperature parameter, such that those time constants could be varied in an annealing schedule to control the temperature. We regard our work as a proof-of-concept, and further research is required to optimize performance as well as analyzing theoretical properties.

Acknowledgment

We thank Wolfgang Maass and our colleagues at the Institute of Neuroinformatics for fruitful discussions. The research was supported by the Swiss National Science Foundation Grant 200021_146608 and by the EU ERC Grant “neuroP” (257219).