=1

From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge

Introduction

"Imagine, for example, a computer that could look at an arbitrary scene, anything from a sunset at a fishing village to Grand Central Station at rush hour and produce a verbal description. This is a problem of overwhelming difficulty, relying as it does to finding solutions to both vision and language and then integrating them. I suspect that scene analysis will be one of the last cognitive tasks to be performed well by computers". This fifteen year old quote, attributed to A. Rosenfeld [\cite=stork1998hal], one of the founders of the field of Computer Vision, pointed to the fundamental problem of generating semantics of visual scenes. Since then, researchers have attempted a few approaches that mostly centered on asking "what" and "where" questions about the scene in view. In this methodology, scenes are recognized by detecting the inside objects [\cite=lowe1999object] [\cite=dalal2005histograms] [\cite=Krizhevsky2012], objects are recognized by detecting their parts or attributes [\cite=felzenszwalb2008discriminatively] [\cite=lampert2009learning] [\cite=farhadi2009describing] [\cite=yu2010attribute] [\cite=teo2015gestaltist] [\cite=teo2013embedding] [\cite=yu2011active] and activities are recognized by detecting the motions, objects and contexts involved in the activities [\cite=laptev2005space] [\cite=messing2009activity] [\cite=wang2011action] [\cite=gupta2007objects] [\cite=conf/eccv/OgaleKA06] [\cite=yang2014cognitive].

Recently, researchers have advanced the viewpoint that if we are able to develop a semantic understanding of a visual scene, then we should be able to produce natural language descriptions of such semantics. This has given rise to a new area in the field that integrates vision, knowledge and natural language. Knowledge becomes especially important, as without background knowledge, it has become increasingly hard to obtain a desirable level of accuracy in this problem. And as such knowledge can be often mined from text, the problem now stands at the intersection between Computer Vision and Natural Language Processing. Mining such knowledge, storing it in a form that retains the semantics, and reasoning using this knowledge to develop a better understanding of scenes are the fundamental issues that are addressed in this paper.

Current developments [\cite=mao2014explain] [\cite=kiros2014unifying] [\cite=donahue2014long] [\cite=karpathy2014deep] [\cite=vinyals2014show] [\cite=chen2014learning] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success. It is indeed an exciting achievement. However, current state-of-the-art image captioning systems still have a few drawbacks such as: 1) a brute-force image-to-text mapping makes it inconvenient to conduct Logical Reasoning beyond just doing inferences from annotated data; 2) due to the lack of intermediate semantic representations, they are all language-dependent; and 3) most importantly, when the system produces wrong results, it is almost impossible to trace back the system and analyze the failure case (See Figure [\ref=fig:examples_stanford]).

Let us consider how humans accomplish this task. Human perception is active, selective and exploratory. We continuously shift our gaze to different locations in the scene. After recognizing objects, we fixate again at a new location, and so on. We interpret visual input by using our knowledge of activities, events and objects. When we analyze a visual scene, visual processes continuously interact with our high-level knowledge, some of which is represented in the form of language. In some sense, perception and language are engaged in an interaction, as they exchange information that leads to meaning and understanding. Thus, our problem requires at least two modules for its solution: (a) a vision module and (b) a reasoning module that are interacting with each other. In this paper we propose to model the early stages of this process. The available datasets make it impossible to perform experiments that will consider vision as an active process, although this is the ultimate goal. Thus, our question becomes: if the vision module produces a number of (probabilistic) detections, how much the reasoning module can infer about the scene if it possesses common sense abilities? It turns out that the reasoning module can infer a great deal.

Motivated by such intuitions, we present here an effort to integrate deep learning based vision and state-of-the-art concept modeling from commonsense knowledge obtained from text. We use a deep learning-based perception system to obtain the objects, scenes and constituents with probabilistic weights from an input image. To predict how the objects interact in the scene, we build a common-sense knowledge base from image annotations along with a Bayesian Network capturing dependencies among commonly occurring objects and "Abstract Visual Concepts" (defined later). These two precomputed resources help us infer the following: 1) the correct set of correlated objects based on the high-confidence objects detected; 2) the most probable events that these objects participate in; 3) the role that the objects play in this event; and 4) given the events, objects and constituents, the "Concept" that emerges from such information. Based on these inferences, we output a Scene Description Graph (SDG) that depicts how these different entities and events interact. In Figure [\ref=fig:example_knowledge_structure], we show a possible SDG for an example image. SDG is essentially a directed labeled graph among entities and events that enables an array of possibilities to do further analysis beyond visual appearance, such as Event-Entity based analysis, question answering about the scene and flexible caption generation. The fundamental contribution of this work is a novel algorithm that uses automatically constructed Knowledge Base to create an SDG from an image, which facilitates further reasoning and caption generation. SDGs have advantages over ground-truth sentences because : 1) they can be easily processed by machines/AI systems in comparison to sentences; 2) the output can be rich in information-content; 3) they are not bounded by specific templates, that are often used by researchers to convert labels into sentences and 4) the SDG also can be used to generate sentence descriptions. We also create a Knowledge Base which captures the knowledge about the commonly-occurring Concepts, events and entities. The knowledge base can be used to provide answers to the following queries: 1) the event or set of events that connect two entities; 2) the role an entity plays in an event and 3) a subset of all possible concepts involving the entities and connecting events. Lastly, further inferences about the scenes such as "Will the player holding the ball be able to tackle the blocker and under what conditions" can also be attempted by feeding the SDG output as predicates to Reasoning modules along with additional background knowledge.

Related Works

Our work is influenced by various lines of work where researchers have proposed approaches to extract meaningful information from images and videos. As [\cite=karpathy2014deep] suggests, such works can be categorized into 1) dense image annotations, 2) generating textual descriptions, 3) grounding natural language in images and 4) neural networks in visual and language domains.

According to the above categorization, we share our roots with the works of generating textual descriptions. This includes the works that retrieves and ranks sentences from training sets given an image such as [\cite=hodosh2013framing], [\cite=FarhadiPictureStory],[\cite=im2textOrdonez], [\cite=DBLP:journals/tacl/SocherKLMN14]. [\cite=DBLP:conf/emnlp/ElliottK13], [\cite=Kulkarni11babytalk:], [\cite=Kuznetsova:2012:CGN:2390524.2390575], [\cite=Yang:2011:CSG:2145432.2145484], [\cite=journals/pieee/YaoYLLZ10] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.

Several works have shown promising efforts to acquire and apply commonsense in different aspects of Scene Analysis. [\cite=conf/cvpr/ZitnickP13] uses abstraction to discover semantically similar images. [\cite=DBLP:conf/cvpr/DivvalaFG14] proposes to learn all variations pertaining to all concepts and [\cite=Santofimia12CommonSenseVision] uses common-sense to learn actions.

Recently, [\cite=johnsoncvpr2015] introduced scene graphs to describe scenes and [\cite=schuster-EtAl:2015:VL] creates scene graphs from descriptions. However, we automatically construct the graph from an image, and we believe, due to the event-entity-attribute based representation and meaningful edge-labels (borrowed from KM-ontology[\cite=Clark2004]) , SDGs are more equipped to facilitate symbolic-level reasoning.

State-of-the-art Visual Detections

The recent development of deep neural networks based approaches revolutionized visual recognition research. Different from the traditional hand-crafted features, a multi-layer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [\cite=Bengio2012], which has shown superior performance on standard scene recognition [\cite=zhou2014places], object recognition [\cite=girshick14CVPR] and image captioning [\cite=karpathy2014deep] benchmarks.

Image Dataset: In this paper, we use three image data sets, which are popularly referred to as Flickr 8k, Flickr 30k and Coco datasets [\cite=hodosh2013framing]. These three datasets have 8,092, 31,783 and more than 160K images respectively. All the images from these datasets are accompanied with 5 hand-annotated sentences that describe the image. For all datasets, we used the train-test splits from [\cite=karpathy2014deep] and the 4000 testing images (1000 each from Flickr 8k and 30k; 2000 from MS-COCO validation set; denoted as I) serve as the testing bed for our reasoning experiments.

Deep Object Recognition: We use the trained bottom-up region proposals and convolutional neural networks(CNN) object detection method from [\cite=girshick14CVPR]. It considers 200 common everyday object classes (denoted as N) and trained on ILSVRC 2013 dataset. We apply the method on the testing images(I) and then convert the object detection scores to Pr(n|I).

Deep Scene Recognition: We use the trained CNN scene classification method from [\cite=zhou2014places]. The classification model is trained on 205 scene categories (denoted as S) and each of the category has more than 5000 training samples. We apply the method on the testing images and then convert the scene classification scores to Pr(s|I).

Constituent Annotation Collection and Deep Constituent Recognition: Images from the wild cannot always be categorized into a limited number of Scene categories. However, scene constituents describing properties or actions of objects, attributes of scenes occur frequently across images and can be utilized to describe the image. In this work, we further augment the Flickr 8K image dataset with human annotation of constituents using Amazon Mechanical Turks. We specifically ask the human labeler to annotate not only objects, but what objects are doing or properties of objects. We allow the labelers to use free-form text for describing constituents to reduce annotation effort. To obtain a standardized set of constituents from the annotations, we perform stop-words removal, parts-of-speech processing to retain nouns, adjectives and verbs. We replace the nouns with their superclasses such as man, boy, father by person, and then, we rank the resulting phrases according to their frequencies. Some of the top phrases are grass, dog run, dog play, kid play, person wear short etc. For the rest of our processing, we post-process the annotations for each training image and consider them if they are among the 1000 top constituents (denoted as C). Recent empirical results from a diverse range of visual recognition tasks indicate that the generic descriptors extracted from the CNN are very powerful [\cite=donahue2014decaf] [\cite=razavian2014cnn]. In this work, we use a pre-trained CNN from [\cite=Krizhevsky2012]. For each image in I, we use this pre-trained model to extract a 4096 dimensional feature vector using [\cite=donahue2014decaf] [\cite=Krizhevsky2012]. We then trained a multi-label SVM to do constituents recognition using the deep features. The trained model is applied on all the testing images and we convert the classification scores to Pr(c|I).

The set of Pr(n|I),Pr(s|I),Pr(c|I) makes up the initial visual perception output.

Constructing SDGs from Noisy Visual Detections

Next, we explain the reasoning framework to construct SDGs from noisy labels with the aid of knowledge from text. To provide a better understanding of this complex system, we provide a diagram of the architecture explaining the reasoning process for an example image in Figure [\ref=fig:reasoning].

As shown, for each image, the above perception system produces object, scene and constituent detection tuples. Each detection is provided with a confidence score. For objects, scores are provided for each bounding box. Top five scene labels and top ten constituent detections are considered for the reasoning framework. Most of these detections are quite noisy. We develop an elaborate reasoning framework to construct SDGs from such noisy detections, with the help of pre-processed background knowledge.

Pre-processing Phase Data Accumulation

In this phase, we collect Ontological information about object classes in Object Meta-data table (OT) and Scene classes in Scene Metadata (SM). We also store scene detection tuples (ST) and human annotation of images (Ad) for all training images. We create a Knowledge Base Kb, a Bayesian Network Bn and a Scenes to Abstract Visual Concepts (AVC) Mapping Table (SM).

Scene Detection tuples (ST): We use the perception system of the previous section to create scene detection tuples ({(si,Pr(si|Itr))|i∈{1,..,5}}) of set of training images (Itr). These are used to learn the Bayes Net Bn.

Image Annotations (Ad): We collect all textual descriptions of the training images provided with Image Datasets and use them for building the knowledge base Kb and Bayes Net Bn. However, both can be built using any repository of sentences that describe day-to-day concepts.

Object Meta-data (OT): For each of the 200 object classes, we collected all synonyms, hyponyms and hypernyms. The list is prepared using Wordnet API. This is dataset-independent and only needs to be augmented when the set of object classifiers expands.

Scenes-to-AVCs Mapping Table (SM): For each scene in S, we added ontological information involving a set of abstract concepts and a set of synonyms. To obtain the synonyms, we again used WordNet API. We hand-annotated all the AVCs for each scene and learnt a prior belief for each AVC in scene from human annotations. For example, for the scene airport_terminal, we add {waiting room, big glass view, people} as the list of AVCs and terminal as the synonym; and learn the priors [formula] respectively for AVCs.

In the following sub-sections, we first introduce the Reasoning Framework briefly, followed by a description of the construction of the Knowledge Base Kb and the Bayesian Network Bn. Lastly, we describe our reasoning framework in detail.

Reasoning Framework

Equipped with the background knowledge stored in the form of (Kb,Bn,SM,OT), we process the objects, scene and constituent detections for an image to construct an appropriate SDG in the following way: i) we populate synonyms, hypernyms, hyponyms of objects and synonyms, AVCs (with priors) of scenes; ii) (Scene Constituents:) we extract entities and events from each constituent. Such as, the constituent person wear short results in an event wear with two edges: one labeled agent joining the entity person and another labeled recipient joining the entity short; iii) (Abstract Visual Concepts:) we choose the AVCs iteratively that maximizes the conditional probability given high confidence objects; iv) (Objects:) for low-scoring objects, we choose the sibling (in the hyponym-hypernym hierarchy) which maximizes the conditional probability given high-confidence objects and AVCs; v) (Events:) we search the Kb to find the most compatible events that connect pairs of high-confidence objects. We add the events obtained from Constituents to this set of compatible events; vi) (Concepts :) given the above events and AVCs, we search the Kb for Concepts that best suits the events and AVCs, and we also construct an SDG based on just high-confidence objects, events and AVCs.

Knowledge Base (Kb)

In Figure [\ref=fig:kparser1], we describe how we construct Kb from a set of Image Annotations (Ad) using the Stanford Parser and K-Parser [\cite=DBLP:conf/ijcai/SharmaVAB15]. For each sentence, we first parse using the Stanford Parser to get a dependency graph. The K-parser then maps these dependency labels using a set of rules to a set of meaningful labels from KM-Ontology[\cite=Clark2004] and the resulting graph is further augmented using ontological and semantic information from different sources (more details on ). We then generalize each of these graphs i.e. replace entities by their superclasses. Then we merge them based on overlapping entities and events, and create a single graph (Kb).

Kb is defined as the tuple (G,C). G = (V,E) denoting set of vertices V, set of edges E. Each vertex and edge has a label. Each vertex can be of three types: events, entities and traits. Events correspond to verbs, entities correspond to superclasses of nouns that directly interact with events and traits represent all other nouns. Edge labels in the Kb are exactly the same as in the K-parser (Figure [\ref=fig:kparser1]). C is a set of concepts which corresponds to generalized K-parser graphs of sentences and is essentially a sub-graph of G.

From Flickr8k annotations, we process nearly 16000 sentences provided for the images (2 per each image) to build the Kb. A visualization of a part of the Kb is given in Figure [\ref=fig:kparser1](b). After parsing the annotated sentences in Flickr8k, Kb consists of 1102 events, 2500 entities and 1869 traits. The total number of edges and distinct concepts in the graph are 25271 and 14325.

Conditional Probability Estimation

In this sub-section, we describe the type of conditional probabilities we estimate and the Bayesian Network we learn to estimate such probabilities. We use conditional probability calculations in two of the steps of our approach: inferring the most probable collection of Abstract Visual Concepts and rectifying low-scoring erroneous objects.

For inferring the most probable collection of AVCs, we first make a list (Cfreq) of all the frequent AVCs (with frequency >  2 in our experiments) from all scenes detected for a test image. Then we follow Algorithm [\ref=algo:inferConcepts] to get the set of inferred concepts Cinf from the set of high-scoring (score >  αh) entities Oimg and the set of scenes Simg detected for image img∈I. We iterate till the entropy keeps decreasing.

Next, we attempt to rectify the low-scoring entities based on high-scoring entities (Oimg) and the above Cinf. For each low-scoring entity, we get all its siblings i.e. we get all the children of its hypernyms. For example, if bathing cap is assigned a low score, the assigned superclass is headwear and its children are headband, hat etc. We calculate the following omax  =   arg   max o∈siblingsP(o|Cinf,Oimg) and then add omax to the high-scoring entities list (Oimg).

As the above paragraphs suggest, we need to estimate the conditional probabilities: P(s|Cinf,Oimg) and P(o|Cinf,Oimg). To estimate the conditional probabilities, we learn a Bayesian Network Bn using Ad and ST.

Learning the Bayesian Network Bn

To capture the knowledge of naturally co-occurring entities and Abstract Visual Concepts, we learn a Bayesian Network that represents the dependencies among them. We create the training data D which is a set of tuples T = (ti)i,i∈1,..,N where N is the total number of entities and AVCs. Each term ti is binary and denotes 1 if the ith entity (or AVC) occurs in the tuple. Then, we use the Tabu Search (tabu) algorithm to learn the structure and then we populate the Conditional Probability Tables using the R-bnlearn package [\cite=bnlearn]. A subgraph of the learnt Bayesian Network is shown in the Figure [\ref=fig:bn].

To create the training data D, we process each training image (in Itr), and we automatically detect entities and AVCs and then output the tuple T. To detect entities, we parse the image annotations (Ad) and extract entities from it. Some of the AVCs such as people and people wear shorts are detected using rule-based techniques. However, for scenes such as airport-terminal, it is unlikely that AVCs such as waiting room can be found in human descriptions of an image; as we tend to describe only the entities and their interactions. Keeping this idea in mind, we ran the scene classifier system from the previous Section [\ref=visualPrediction], and we consider all the AVCs of the scene with the highest score (Pr(s|Itr)), from the Scene-to-AVC lookup table (SM).

Ranking and Inferring Final Concepts

Given the most relevant set of Abstract Visual Concepts (Cinf) and entities (Oimg), we find Concepts that the image describes. To do this, we use the Kb to search first for events that these entities (i.e. objects) participate in and then we use these events and entities together to search for Concepts in the set of concepts C in Kb.

We rely on two assumptions about the Knowledge Base: I) Kb reflects a more-or-less complete view of the relevant world knowledge and hence we can find the most suitable events from it. This assumption is valid if the images come from the same domain; such as in our examples, we have used the Flickr8k dataset and the domain corresponds to pictures of humans and dogs in natural setting; and II) Kb contains all concepts possible with the given events and entities. This is a strict assumption, which might not be true even if we parse the whole Web. To alleviate the problem, we give two final outputs: i) an SDG involving the entities, AVCs and events and ii) another SDG of the top Concept that is obtained from C in Kb.

Search Connecting Events: The motivation behind building a Knowledge Base was to logically explain why certain co-occurring events are suitable for the combination of entities. For example, consider the entities person and swimming trunks. Note, swimming trunks corresponds to the vertex trunk in Kb. We get events such as sniff, climb, wear etc., i.e., some corresponding to tree-trunk and others to swimming-trunks. To logically find suitable events, we find all connecting events from G in Kb and then filter spurious events based on ontological and background knowledge from OT and C in Kb.

For a pair of entity in Oimg, we traverse the path from one entity to another in the graph G and consider event-nodes on the path. As shown in Figure [\ref=fig:kparser1](b), two entities can be connected by an event. However, in some cases, they could be connected by a chain of events and entities. We employ a greedy breadth-first search over the graph G for such pairs. We denote the set of entities that are related to each other by some event, by Oev.

For filtering spurious events, we introduce the notion of Edge-Compatible Events. An event is edge-compatible with respect to two entities if they are connected to the event using edges with compatible labels. As these labels are well-defined relations between entities and events from KM-Ontology, the label-compatibility is easy to observe. For example, (agent,recipient) is a compatible pair and only an animate entity can be an agent. Based on the rules, the event wear is edge-compatible with respect to entities person and trunk.

Even after this, we still obtain events like climb etc. To filter such events, we consult the table OT and the set of concepts C. We know that the entity swimming trunks belongs to the superclass clothing, and hence we retain only those events that are connected to an entity trunk which is of the same superclass, in some concept in C.

SDG Construction: After obtaining a set of suitable events (such as wear), we construct an SDG using the following set of rules: i) add has(scene, component, s) for all AVC s in Cinf; ii) add has(event, location, scene) for the top detected events; iii) add all compatible edges related to the events such as has(wear,agent,person) and has(wear,recipient,trunk); and iv) for all entities oim in [formula], do the following: if it is an animate entity, add has(oim, location, scene); Otherwise, find the shortest path from oI to the top detected event in the Kb and add the edges on the path to the SDG.

Search Concepts: Given the events and entities (Oev), we search the set of Concepts C in Kb. Recall, in the Kb, a Concept is a generalized K-parser graph of a sentence. We consider a Concept as candidate if all edges from a detected Edge-Compatible Event are present in it.

Next, we weight each candidate Concept using the remaining entities in [formula] and AVCs; i.e., increase a counter if an entity or AVC occurs in the graph. We also calculate a joint confidence-score for each Concept based on the Pr(n|I),Pr(s|I),Pr(c|I) values of the object, scene and constituents present in the Concept. Based on the counters and the joint confidence-score, we rank the Concepts.

Template Based Sentence Generation: We generate textual descriptions from the SDG using the SimpleNLG[\cite=Gatt:2009:SRE:1610195.1610208] package. For example, for the edges has(wear,agent,person) and has(wear,recipient,shorts), we will generate the sentence "a person is wearing shorts". Based on the edge-labels (labels from KM-ontology) we populate the verb, subject, object and adjectives (including quantitative) of sentences using simple rules. It should be noted that these K-Parser labels are a direct mapping from the set of Stanford Dependencies, and theoretically we can populate all the parts-of-speeches of a sentence from the SDG. Herein lies the effectiveness of producing an SDG from an image.

Experiments and Results

The Knowledge-Structure representing a scene should be rich in information-content and should carry enough semantics to describe the image. We adopted three sets of experiments. First, we detect the accuracy with which our system can detect events and entities present in the image. We perform a qualitative evaluation ("relevance" and "thoroughness") of the textual descriptions generated from SDGs with the sentences generated by [\cite=karpathy2014deep] using the Amazon Mechanical Turkers (AMT). And lastly, to evaluate the image-sentence alignment quality, we design an Image Retrieval task and report our results on Image Search based on generated annotations. To conclude, we provide a few example images and their SDGs.

For comparison purposes, we use the implementation from [\cite=karpathy2014deep] to generate a textual caption S for each testing image. The method is based on a combination of CNN over image regions, bidirectional recurrent neural networks over sentences, and a structured multimodal embedding. We denote the set of captions as SNN.

Training Phase: Our model can be represented by the tuple (Kb,Bn,ST,Ad,OT,SM). Among these, [formula] are collected and stored once, and re-used for all datasets. For our experiments, we re-use the same Bayesian Network Bn learnt from Flickr8k data for all the datasets. Though, we build the Kb each time from the annotated sentences, this can be easily avoided by using the same Kb for all the datasets. In essence, for the reasoning part, we donot require any training at all for new datasets.

Entity and Event Detection Accuracy: For this experiment, we extracted entities and events (gold-standard) from constituent annotations for the 1000 test images of Flickr8k. We manually checked them to remove noise. To provide a baseline, we also extracted entities and events from SNN automatically using K-parser. Subsequently, we compared the gold-standard with entity-event set from [\cite=karpathy2014deep] and the SDG output from our system for each image. The statistics of our evaluation is given in Table [\ref=table:stats_table].

AMT Evaluation of Generated Sentences: Since sentence generation to describe a scene is innately a creative process, a good metric is to ask humans to evaluate these sentences. The evaluation metrics: Relevance and Thoroughness, are therefore proposed as empirical measures of how much the description conveys the image content (relevance) and how much of the image content is conveyed by the description (thoroughness). We engaged the services of AMT to judge the generated descriptions based on a discrete scale ranging from 1-5 (low relevance/thoroughness to high relevance/thoroughness). The average of the scores and their deviation are summarized in Table [\ref=tab:sengen_res1] for Flickr8k, Flickr30k test images and MS-COCO validation images. For comparison, we asked the AMTs to also judge one random gold-standard description and the output from [\cite=karpathy2014deep], a state-of-the-art image captioning system.

In our experiments, we found that Kb from Flickr8k annotations can be used for Flickr30k without much effect on accuracy. However, for MS-COCO datasets, Kb from Flickr8k annotations falls short of producing a desired accuracy as the COCO data is much more varied.

Image-Sentence Alignment Evaluation: Similar to the experiments in [\cite=karpathy2014deep] [\cite=johnsoncvpr2015], we also evaluate the image-sentence alignment quality using ranking experiments. We withhold the set of testing images and use the generated sentences as queries.

We process the textual query and construct Gquery = (Vq,Eq) using the same procedure by which we construct Kb. For each image, we take the SDG Gimg = (Vimg,Eimg) and calculate similarity between the SDG and the query using the following formula:

Similarity between two vertices are calculated based on their word-meaning similarity and neighbor similarity. Here wnsim(.,.) is WordNet-Lin Similarity [\cite=lin1998information] between two words and Jaccard(.,.) is the standard Jaccard coefficient similarity. Based on the above similarity measure, we give the image retrieval results compared with few of the state-of-the-art results in Table [\ref=retrieval].

One of the primary contributions of our work is the Knowledge-Structure representation that bridges the gap between semantic information in text and images. From the results of this experiment, the benefit of having such an intermediate representation is easy to observe.

Example Images and SDGs: As examples, we pick a few images which produces objects and scene recognitions with comparably good confidence scores. The images and their corresponding SDGs are provided in Figure [\ref=fig:graph_rich]. As we can observe, the information produced by these SDGs are easily processed by machines. We can answer questions such as how entities interact in an event, which possible events are in the scene and how entities interact in a scene. We should also mention that the concept-level modeling provided by SDGs is what separates this work from other recent approaches [\cite=johnsoncvpr2015]. Furthermore, comparing these structures with the K-Parser output in Figure [\ref=fig:kparser1], we can see how the sentences and images can seamlessly converge to such space of graphical representations. This could have huge repercussions in search in Image and Textual space and storing knowledge from images and text together in a unified Knowledge Base.

Conclusion

This paper introduced a reasoning module to generate textual descriptions from images by first constructing a new intermediate semantic representation, namely the Scene Description Graph (SDG), which is later used to generate sentences. The reasoning module uses an automatically constructed Knowledge Base created from text, to capture "commonsense" knowledge. Having built the Knowledge Base, we proposed a method of obtaining such SDGs from noisy labels using our prediction system. The SDG is a representation of the scene in view that integrates direct visual knowledge (objects and their locations in the scene) with background commonsense knowledge. In addition, the SDGs have a structure similar to semantic representations of sentences, thus facilitating the interaction between Vision and Natural Language. The notion of the SDG has great potential. Here we used the SDG for the automatic creation of sentences describing the scene; but, equipped with background knowledge, it also allows reasoning and question/answering about the scene .

To demonstrate the effectiveness of the sentences and constructed SDGs, we performed a number of experiments. Our AMT evaluations on popular datasets show that our sentences performs comparatively well with respect to the state-of-the-art in measures of relevance and thoroughness. A Gold-Standard based evaluation shows that our output SDGs can detect events and entities with comparable accuracy as a state-of-the-art system. And lastly, our Image Retrieval experiment shows that the Image-Sentence alignment quality is comparable with state-of-the-art results.