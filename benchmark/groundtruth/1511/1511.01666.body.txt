Comparing Writing Styles using Word Embedding and Dynamic Time Warping

Introduction

Writing style of a novel is based on many factors including the personal style of the author and the type of the novel itself. For example, thriller novels by the same author tend to be similar in the way the story develops. Apart from the specific factual details, literary works tend to have a definite flow of emotions that define the overall subjective feel of the work as a whole.

This flow can be quantified and compared by analyzing the text using natural language processing techniques. This study uses word embedding models to generate time series for novels and then compare the resulting series using dynamic time warping to find similarities. Considering time series analysis rather than a pure statistical one can capture the flow of the works and thus the similarities generated will provide a metric for comparing the novels based on the progression.

The following section explains the word embedding models and dynamic time warping. Section [\ref=approach] describes the approach followed. Results on few classic literary works are described in Section [\ref=res]. Conclusions are drawn in Section [\ref=conc].

Background

Word Embedding

Word embedding refers to techniques which allow representing words to vectors of real numbers in a continuous space. The vector representation can be learned using many techniques and relies on using the information based on the context, a word is present in.

We use a model proposed by [\citet=Mikolov2013a] [\citet=Mikolov2013b] that uses Continuous Bag-of-Words and Skip-Gram model to learn these continuous representations. This approach uses the following two models.

Continuous Bag-of-Words model

This model (figure [\ref=fig:cbow]) samples a context of size defined by c from text around a word and tries to learn a projection to vector space that correctly predicts the middle word. Assuming words represented as one hot vectors w(i), where i denotes the position of the word in text, the model takes [formula] and predicts the middle word w(t).

Skip-Gram model

The skip-gram model (figure [\ref=fig:skip]) does the opposite of previous model. It tries to predict the context, given the middle word. Using the previous notation, this model takes w(t) as input and outputs [formula].

The projection layers are of desired vector dimension. The weights from one hot encoding of words to this layer provide, after training, the final transformation matrix for continuous word representation.

Once trained, words in the vector space are arranged according to semantic connections. This allows the vectors to have properties as shown below.

W is the function from vocabulary to the vector space.

Dynamic Time Warping

Dynamic Time Warping (DTW) is a technique to measure similarity between two time series. DTW handles the difference in speed and time between signals and has been used in applications like speech recognition and signature matching where the difference of signal speed should not affect the final result. It is based on optimal matching and the algorithm outputs a value corresponding to the separation of the two time series. One considerable advantage of DTW is that the two series need not to be of same length, which is the case here, as two different books will have different word count and consequently different length.

The objective of this algorithm is to calculate a distance measure for a given pair of temporal series, that can represent the similarity / dissimilarity between those two series. This is done by determining a path W which minimizes the cumulative euclidean distance between elements of the two series. Let the two temporal series, which need to be compared, are

First, all the euclidean distances are calculated between each possible set of elements from the two series, which results in total of m  ×  n values. Let the matrix depicting these distances be

where the element di,j represents the euclidean distance between ai and bj. Now, using dynamic programming, the optimal path W is determined from point (1,1) to (m,n) along which the cumulative sum of the euclidean distances (i.e. the sum of di,j) is minimum. This path is continuous, which means that the indices of two consecutive elements of W do not differ by more than one in either series. The path is determined using the following recursive function:

where γ(i,j) represents the cumulative sum up to elements ai, and bj. Figure [\ref=fig:dtw] shows an example using two sinusoidal series. The optimal path and the series are shown.

Approach

Novels to Signal

Our approach for converting a novel to signal uses sentence vectors by taking mean of word vectors in a sentence. A word embedding model is trained using 1000 free e-books gathered from Project Gutenberg. The model is trained using Word2Vec.

Training is done using feature (word vector) size of 100 and context window size of 10 words. Once trained, a novel with Nw words results in a 100 dimensional time series of length Nw.

To reduce the dimension of data word vectors in a sentence are averaged, resulting in a time series matrix of shape Ns  ×  100, where Ns is the number of sentences in the novel. To reduce the row dimensions, we find cosine similarity of each row with few anchor points in the word space. This provides a measure of the movement of the signal in the whole vocabulary space.

For selecting appropriate anchor points to better cover the space in its entirety, we perform k-means clustering on the vocabulary of the embedding model and find 4 cluster centers. These 4 points act as the anchors, resulting in a final matrix of size Ns  ×  4 for each book.

A plot of distances from these 4 points for The Sign of the Four by Arthur Conan Doyle is shown in figure [\ref=fig:anchor]. Due to the fickle nature of text a large amount of noise is present. After smoothing with a gaussian filter (window size = 200, σ = 60), the essential rising and falling trend of curve is preserved. Filtered lines are shown with deeper color in the plot.

Comparing Signals

Once generated, the signals are compared using FastDTW [\citep=Salvador2004] which implements a faster version of DTW algorithm. Vanilla DTW requires O(n2) computation, while FastDTW computes in linear time.

We use 24 classic novels collected from Project Gutenberg (listed in table [\ref=table:books]) and generate distance matrix for the items.

Results

A scatter plot of the books using multi dimensional scaling is shown in figure [\ref=fig:res]. Although the similarity measure itself is a useful metric for comparison, the scatter plot also clusters the authors according to the writing styles, affirming the hypothesis that a comparison of flow in the time series of word vectors can have subjective projections.

Conclusions and Future Works

This study provides a way to explore literary works as signals in word embedding space. The clusters of authors formed as the result of analysis provides encouraging support for this method as a high level text analysis technique.

A more rigorous study can be done by building on present method to identify the principle components involved in shaping the overall picture of a book while at the same time, being agnostic of factual details. The text representation can be made better using paragraph (or sentence) vectors instead of word vectors. Anchor points can be improved based on the effects and the number of points. A frequency based analysis of signals can provide a better insight.

References