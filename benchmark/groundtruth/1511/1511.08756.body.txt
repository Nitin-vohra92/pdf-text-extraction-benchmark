="2D

Reverse Engineering Intel DRAM Addressing and Exploitation - Work in Progress -

Introduction

The DRAM is the main physical memory used by the CPU. As such it is shared among threads and processes as well as CPU cores and even multiple CPUs. However, the side-channel attacks exploiting properties of DRAM have not been examined to date. As CPU caches are easier to exploit, research has focused on CPU cache attacks. Such attacks comprise covert channels and side-channel attacks even in virtualized environments, effectively eliminating isolation between virtual machines. Cache attacks exploit timing differences that originate from the limited amount of cache memory. An attacker can distinguish between cache hits and cache misses based on the timing. Modern CPUs split the last-level cache into slices. The physical address is used to select the cache slice. Until recently this mapping has been unknown and thus efficient last-level cache attacks were much harder. However, based on this reverse engineering [\cite=Maurice2015RAID] [\cite=Inci2015iacr898] [\cite=Yarom2015iacr905], powerful cross-VM attacks on cloud servers are possible. To prevent cache attacks in cloud scenarios, virtual machines can be scheduled to different physical CPUs. Thus the two virtual machines do not share a cache and cache attacks are not possible.

However, multiple CPUs can still share resources, first of all the DRAM. The DRAM is organized in multiple channels consisting of usually up to 2 ranks, each consisting of multiple banks. Each bank consists of rows of columns that contain the actual data words. Each bank has a small directly-mapped cache called "row buffer". Subsequent accesses targeting the same row can be performed on the row buffer. If a different row needs to be accessed a row buffer switch occurs. Then the DRAM actually writes the row buffer back into the DRAM cells and reads a new row into the row buffer.

The row buffer can be exploited in two ways. First, by performing row buffer switches in a high frequency the actual DRAM cells are accessed in a high frequency as well. If the frequency of the accesses is high enough it can trigger bit flips in DRAM cells. This effect is known as the Rowhammer bug. The second way to exploit the row buffer is to derive information from timing. If the latency is low, no row conflict and thus no row buffer switch occurred. However, if the latency is high, a row conflict occurred and the row buffer was switched before the data has been retrieved from the DRAM cells and returned to the CPU.

Seaborn [\cite=Seaborn2015DRAMmap] manually reverse engineered the complex addressing function on a Sandy Bridge processor for one specific DRAM configuration by educated guessing. Seaborn uses the reversed DRAM addressing function to reduce the search space in a Rowhammer attack. However, the DRAM addressing functions on more recent architectures are more complex and thus Seaborn's approach does not work anymore. In this paper, we demonstrate how to retrieve the correct addressing functions by performing physical probing of the memory bus. Thereby, we reverse the exact channel, rank and bank functions on a variety of systems listed in Table [\ref=tbl:machines]. Subsequently, we show how to automate reverse engineering of the DRAM complex addressing in order to make these attacks more practical even in virtual machines in the cloud. We develop a fully automatic approach to resolve the complex addressing of all DRAM mapping functions. Our technique relies on timing differences caused by row conflicts. As a result, we obtain sets of addresses that cause row conflicts. Based on these sets we efficiently find minimal functions generating these sets.

In summary, this paper presents the following main contributions:

We show that physical probing can be used to reverse engineer the mapping of physical addresses to DRAM channels, ranks and banks.

We provide a method to automatically reverse DRAM addressing functions on any platform in software.

We provide reverse-engineered functions for various architectures and DRAM configurations.

We demonstrate a memory-based covert channel three orders of magnitude faster than previous memory-based covert channels, using the reverse-engineered functions.

We reduce the search space in a Rowhammer attack significantly.

The remainder of the paper is organized as follows. In Section [\ref=sec:background], we provide background information on DRAM, the Rowhammer attack, and covert and side channels on shared hardware. In Section [\ref=sec:reveng], we describe our two approaches to reverse engineer the DRAM complex addressing. The first approach uses physical probes on the DRAM bus. The second approach is only based on software, using timing differences. We verify the correctness of the functions by cross validation and subsequently building attacks. In Section [\ref=sec:covert], we build a high speed covert channel using the complex DRAM addressing functions. In Section [\ref=sec:rowhammer], we evaluate the impact on the rowhammer attack. We discuss countermeasures against our attack in Section [\ref=sec:countermeasures]. Finally, we discuss future work in Section [\ref=sec:futurework] and conclude in Section [\ref=sec:conclusions].

Background and related work

In this section, we give an introduction to DRAM and the Rowhammer bug. Furthermore, we describe related work in the field of covert channels, based on data caches and memory.

DRAM

Modern DRAM is organized in channels, banks and ranks. A system can have a one or more channels, that are physical links between the DRAM memory and the memory controller. Channels can be accessed in parallel thus distributing the memory traffic over multiple channels, increasing the throughput and reducing the latency in many cases. A channel comprises multiple Dual Inline Memory Modules (DIMMs), that are the physical memory modules attached to the mainboard. Each DIMM has typically one or two ranks, that often correspond to the sides of the physical module. Each rank is composed of banks, typically 8 on DDR3 DRAM and 16 on DDR4 DRAM. Accesses to different banks are served concurrently. Banks are represented as a collection of rows, typically 214 to 217. Each bank has a row buffer that serves as a directly-mapped cache for the row. The actual DRAM cells are only accessed when reading them into the row buffer on request or writing them back when another row is requested.

DRAM is volatile memory that discharges over time. Each row needs to be refreshed in a certain interval, called the refresh interval to avoid data loss. Refreshing a row opens the row and closes it again, i.e., it reads and restores the charge of the cells. The DDR3 DRAM specification defines a 64ms time window [\cite=Kim2014ISCA] in which all rows must have been refreshed. The selection of channel, rank, bank and row is done by a subset of physical address bits. AMD documents the addressing function used by its processors, but Intel does not. This function can vary between different systems and system configurations. The mapping for one Intel Sandy Bridge machine in one configuration has been reverse engineered by Seaborn [\cite=Seaborn2015DRAMmap] and we verified that it is identical on our Sandy Bridge machines in the same configuration. However, with a different number of DIMMs we found the function input bits to be shifted.

As channel, rank and bank form a hierarchy, two addresses can only be physically adjacent if they are in the same channel, DIMM, rank and bank. In this case we just use the term same bank. Within the same bank, two addresses can be in the same row and thus can be accessed without a rowbuffer switch or in a different row and thus can only be accessed with a rowbuffer switch.

The Rowhammer bug

The increase of DRAM density has led to physically smaller cells, thus capable of storing smaller charges. As a result, the cells have a lower noise margin, and cells can interact electrically with each other although they should be isolated. The so called Rowhammer bug consists in the corruption of data, not in rows that are directly accessed, but rather in rows nearby the one accessed.

DRAM and CPU manufacturers have known the Rowhammer bug since at least 2012, date of the filing of several patent applications by Intel [\cite=bains2014row1] [\cite=bains2014row2]. In fact, hammering a DRAM chip is one of the quality assurance tests applied to modules [\cite=al2005dram]. As refreshing DRAM cells consumes time, DRAM manufacturers optimize the refresh rate to the lowest rate that still has a probability of virtually zero for bit flips to occur accidentally.

The Rowhammer bug has only been studied recently in academic research [\cite=Kim2014ISCA] [\cite=Huang2012] [\cite=Park2014]. In particular, Kim et al.[\cite=Kim2014ISCA] studied the vulnerability of off-the-shelf DRAM modules to bit flips, on Intel and AMD CPUs. They built a program that induces bit flips by software using the clflush instruction. The clflush instruction flushes data from all cache levels, forcing the CPU to serve the next memory access from the DRAM instead of cache. Their proof-of-concept implementation frequently accesses and flushes two different memory locations in a loop, causing bit flips in a third memory location. Gruss et al.[\cite=Gruss2015arxivRowhammer] later showed that it is possible to trigger bit flips without the clflush instruction, and demonstrated the attack from JavaScript.

Seaborn [\cite=Seaborn2015row] implemented two attacks that exploit the Rowhammer bug, showing the severity of faulting single bits for security. The first exploit is a kernel privilege escalation on a Linux system, caused by a bit flip in a page table entry. The second one is an escape of Native Client sandbox caused by a bit flip in an instruction sequence for indirect jumps.

Covert channels due to hardware sharing

In a covert channel, the sender and the receiver are cooperating to exchange information, in a context where they are not allowed to. They can use shared hardware as a medium to leak information.

Data cache  Covert channels using the data cache exploit the fact that cache hits are faster than cache misses, and all rely on cache eviction. Three different techniques have been presented, and can be used either to build covert channels or side channels, namely Prime+Probe [\cite=Percival2005] [\cite=Maurice2015DIMVA] [\cite=Liu2015SP], Flush+Reload [\cite=Yarom2014Usenix] and Flush+Flush [\cite=Gruss2015arxivFF]. Flush+Reload and Flush+Flush both use shared memory such as shared libraries or memory deduplication. Gruss et al.[\cite=Gruss2015arxivFF] implemented these three covert channels with the same protocol to normalize the results. The covert channel using Prime+Probe obtains 536Kbps, the one using Flush+Reload 2.3Mbps and the one using Flush+Flush 3.8Mbps. The most recent cache attacks target the last-level cache that is shared across cores of the same CPU. The last-level cache is divided in slices, and a physical address is mapped to a slice by an undocumented function that has been since reverse-engineered [\cite=Maurice2015RAID] [\cite=Inci2015iacr898] [\cite=Yarom2015iacr905]. These attacks are thus cross-cores, but limit the sender and the receiver to be on the same CPU.

Memory and memory bus  Xiao et al.[\cite=Xiao2013] used memory deduplication, that causes identical pages to be merged to a single read-only page. A write to this page triggers a copy, causing a higher latency compared to a regular write access. The authors built a covert channel that can attain up to 90bps, and 40bps on a system under memory pressure. Wu et al.[\cite=Wu2014] proposed a bus-contention based covert channel, that uses atomic memory operations that lock the shared memory bus. The authors obtained a raw bandwidth of 38kbps between two virtual machines, and about 746bps with a protocol that handles error correction. They further evaluated the covert channel on EC2 virtual machines, obtaining more than 100bps when the sender and receiver are throttled.

Reverse engineering Intel's DRAM mapping

In this section, we discuss the assumptions we make for the analysis. We then present two approaches to reverse engineer the mapping, first using physical probing, and then with a software-only method using timing differences. Finally, we present the outcome of this analysis. Throughout this section, we denote with a a physical memory address. With ai, we denote the i-th bit of said address.

Assumption of linearity

The DRAM addressing functions are reverse engineered in two phases. First, a measuring phase and second, a subsequent solving phase. In this work we assume that the addressing functions are linear, i.e., they are XORs of selected physical-address bits.

This assumption is based on the fact that Intel used such functions for DRAM addressing in previous architectures. For instance, Seaborn [\cite=Seaborn2015DRAMmap] reports that on his Sandy Bridge setup the bank address is computed by XORing the bits a14..a16 with the lower bits of the row number (a18..a20). (cf. Figure [\ref=fig:mapping_sandy]). This is done in order to minimize the number of row conflicts during runtime. Additionally, Intel uses linear functions for CPU-cache addressing. Maurice et al.[\cite=Maurice2015RAID] showed that the complex addressing function, which is used to select cache slices, is an XOR of many physical-address bits.

As it turns out, the assumption of linearity holds on all our tested configurations. However, there are setups in which it might be violated. For instance, systems with three equally sized modules per channel will probably use different mappings. Note that we did not test such systems and leave a reverse engineering to future work.

Reverse engineering using using physical probing

Our first approach to reverse engineer the DRAM mapping is to physically probe the memory bus and to directly read the control signals. This probing can be done in a non-invasive manner, i.e., it does not require to physically alter the system in any way. The DIMM slot exposes the metal contacts in the slits on the side. By simply inserting a slim piece of metal into these slits physical contact can be established. We then used a high-bandwidth oscilloscope to measure the voltage at the pin and deduced its logic value.

We contacted all pins of interest, namely the bank-address bits BA0, BA1, BA2, and the chip select CS, on all inserted memory modules. We then recorded the logic levels (HI, LOW, Z) on these pins for many randomly selected physical addresses. We then used the previously stated assumption of linearity. Thus, we created an overdefined system of linear equations in the address bits, and then solved this system using standard linear algebra. This solution is then the sought-after addressing function.

Reverse engineering the DRAM addressing functions using physical probes has significant drawbacks. First, expensive measurement equipment, e.g., a relatively high-performance oscilloscope, is needed. And second, it requires physical access to the internals of the tested machine. However, it has the big advantage that the address mapping can be reconstructed for each signal individually and exactly. Thus, we can determine the exact individual functions for the bus pins. Furthermore, every platform only needs to be measured once in order to learn the addressing functions. Thus, an attacker does not need physical access to the attacked system if the measurements are performed on a system with the same CPU and DRAM configuration. If an attacker has no such system it is still possible to fall back to a timing attack as we next show.

Reverse engineering using timing differences

Our second approach to reverse engineer the DRAM mapping is to use timing differences. This approach uses the fact that row conflicts lead to higher access times. We use these differences to find sets of addresses that map to the same bank but to a different row. Subsequently, we use these sets to automatically determine the addressing functions. We verify the correctness of the functions based on the results of our physical probing.

Timing attack  In the timing attack we try to find row conflicts between addresses in a large array mapped into the attackers' address space. As shown in Figure [\ref=fig:histograms], accesss times are significantly higher for some address pairs. We verified that these address pairs cause a row conflicts using the physical address and the previously reverse-engineered DRAM addressing functions. Based on this timing difference an attacker can distinguish row conflict pairs from other address pairs. To successfully reverse engineer the address functions in this timing attack, it is essential to map more than half of the physical memory to ensure that we have addresses for every combination of channel, DIMM, rank and bank. We then create a pool of addresses by randomly selecting addresses from the mapped memory.

From this address pool, we select one random address called the "base address". The remaining address pool is then checked for row conflicts with this address by performing a Rowhammer-like timing attack. If the timing of the access loop is high, the addresses map to the same bank, rank, DIMM and channel. All such addresses are added to a set having the same rank, bank, DIMM and channel for all addresses. Using this method, we try to identify as many sets as possible. While our method finds duplicate sets and might not even find all sets, it finds enough sets to successfully determine the addressing functions.

Function reconstruction  The second phase of the software based approach is to reconstruct the addressing functions with the help of the identified sets.

As already stated in the previous section, we assume linearity of the functions. In order to reconstruct the addressing functions from timing we perform a brute-force search of linear functions in one arbitrarily chosen set. For this, we generate all linear functions that use exactly n bits as coefficients. We apply these functions to all addresses in the set. If the result is the same for all addresses, we apply this potential function to all other sets. Since we spread our addresses randomly across the memory, we expect every address bit to be roughly uniformely distributed over the sets. With this assumption, we can already eliminate many potential functions whose output deviation varies considerably from a uniform distribution. Repeating these steps for linear functions of all sizes n = 1..N, we obtain a list of possible complex addressing functions. In our tests we limited N to 8 in order to speed up computation. However, the approach also works for higher values for N.

To further narrow down the choice of addressing functions, we remove all functions that are linear combinations of other functions. These functions are redundant and thus not relevant to find the correct addressing functions.

Depending on the random address selection, we now have a complete set of correct addressing functions. To increase the certainty, we can repeat the whole process several times and compute the intersection of the potential addressing functions.

Compared to the probing approach, this purely software-based method has significant advantages. It does not require any additional measurement equipment and can be executed remotely. However, it cannot assign the identified functions to the individual address bits.

Results

In this section, we give the results of our reverse engineering. That is, we present the found mappings which we received by using both described methods. We found that the basic scheme is always as follows. The three lower bits of the physical address are used as byte index into a 64-bit (8-byte) memory word. Thus, they are never transmitted on the memory bus. Then, the next bits are used for column selection, one bit in between is used for channel addressing. This is then followed by bits responsible for bank, rank, and DIMM addressing. The remaining upper bits are used for row selection.

The detailed mapping, however, differs for different setups. To give a quick overview of the main differences, we show the mapping of one selected memory configuration in Figure [\ref=fig:mappings_examples]. Here we chose a configuration with two equally sized DIMMs in dual-channel configuration, as it is found in many off-the-shelf PCs. All our setups use dual-rank DIMMs and use 10 bits for column addressing. Figure [\ref=fig:mapping_sandy] shows the mapping on the Sandy Bridge platform, as reported by Seaborn [\cite=Seaborn2015DRAMmap]. Here, only a6 is used to select the memory channel, a17 is used for rank selection. The bank-address bits are computed by XORing bits a14..a16 with the lower bits of the row index (a18..a20).

The channel selection function changed on later platforms, such as Haswell. As shown in Figure [\ref=fig:mapping_haswell], the channel-selection bit is now computed by XORing seven bits of the physical address. Further analysis showed that bit a7 is used exclusively, i.e., it is not used as part of the row- or column address. Additionally, rank selection is now similar to bank addressing and also uses XORs.

Due to the doubling of the available banks on DDR4, from previously 8 on DDR3 to now 16, the addressing function necessarily changed again. Figure [\ref=fig:mapping_skylake] shows that a7 is now used for bank addressing and does not a play a role in the channel selection any more. Measurements on these platforms were done using the software method, thus assigning the found functions to signals is not possible. We guessed this assignment based on the results of the Haswell platform.

Table [\ref=tab:mapping] shows a comprehensive overview of all our analyzed platforms and memory configurations. As all found functions are linear, we simply list the index of the physical address bits that are XORed together. In the case of Haswell, one can clearly see that the general patterns are always the same. The index of the used bits is shifted to accommodate the different setting. For instance, in single-channel configurations a7 is not needed for channel selection, which is why bank addressing starts with a13 instead of a14.

A high speed covert channel

In this section, we present a first application to the physical address mapping. That is, we show how with the help of the address functions the row buffer can be used to create a covert channel between processes. Indeed, the row buffer behaves like a directly mapped cache for the DRAM row. Thus, it can be attacked like a CPU cache. The processes in this covert channel only need to share the same memory bus. The channel can be used to communicate cross-VM and does not depend on resources of the executing CPU. Thus it potentially allows cross-CPU attacks on multi processor systems. The latter is not possible with currently published cache-based covert channels. Before diving into the details of the setup, we first describe the basic idea.

Basic concept

We use the timing differences stemming from row conflicts to build a covert channel by using the following approach. The receiver process repeatedly accesses one selected physical address in the RAM and measures the access time. If the sender process now repeatedly accesses another address in the same bank but different row, then this triggers row conflicts which leads to higher average access times in the receiver process. By switching the activity of the sender process on and off, bits can be transmitted through discriminating the mean access time in the receiver process. We assign a logic value of 0 to low access times (the sender is inactive) and a value of 1 to high access times (the sender process hammers).

Each (channel, DIMM, rank, bank) tuple can be used as a separate transmission channel. On our testing machine, with a dual-channel setup and 1 dual-rank DIMM per channel, we can thus use up to 32 different channels. However, the noise increases, and there is also a strict limit in the usable parallelism, thus a much lower number leads to higher performances. Note that transmission channels are unidirectional, but the direction can be chosen for each one independently. Thus, two-way communication is also possible.

Implementation details

In order to evaluate the performance of our newly proposed covert channel, we created a proof-of-concept implementation. Note that here we restrict ourselves to unidirectional communication, i.e., there is one dedicated sender and one dedicated receiver. We now describe the inner workings of our implementation.

The sender and receiver processes of the covert channel are started independently and possibly in different virtual machines on the same host. Both processes request a large array using mmap or malloc. As observed previous work [\cite=Gruss2015esorics] [\cite=Gruss2015arxivRowhammer], system libraries and the operating system perform optimizations to reduce the memory footprint of the system and enhance the performance by using 2 MB pages for arrays that are significantly larger than 2 MB. However, the start of the array is not necessarily on a 2 MB border and all pages before this border are allocated using 4 KB pages. Thus, it is necessary to skip this memory region until the 2 MB border. This can be done by choosing the next virtual address having the 21 lowest bits equal to zero. Within a 2 MB page the 21 lowest bits of the virtual address and the physical address are identical. Depending on the hardware setup all bits used for addressing banks, ranks and channels can be part of the lowest 21 bits. While this is not necessarily the case on other setups, an attacker can perform our timing attack described in Section [\ref=sec:timingreveng] to retrieve the missing information in any case. Thus, the attacker can even perform the covert channel attack without any privileges.

In our covert channel implementation we measure the access time using rdtsc. The memory accesses are performed using volatile pointers and memory is flushed from the cache using clflush. To ensure that cache invalidation is finished before the following access time measurements in the receiver process, it is necessary to use an mfence after clflush. In the sender process no mfence is necessary, as it does not perform any timing measurements.

The synchronization between sender and receiver process is implemented using the system clock. If a synchronized system clock is not available to the two processes in two different virtual machines, then it is still possible to use a bidirectional covert channel and use a self-synchronizing protocol.

In order to characterize the channel, we continuously profile the average access time. During profiling, the sender ceases activity while the receiver process measures average DRAM access times without an increased number of row conflicts. This average is later used as a threshold to distinguish between sending a 0 and a 1.

In between profiling, the actual transmission takes place. During transmission, the sender keeps sending data blocks by causing row conflicts in transmission channels sending a logic 1. The receiver measures each bit and computes the average access time. This mean is used for classifying bits. If it is above a certain threshold (which depends on the profiling data), then we received a logic 1. Each block is sent and measured for a fixed period of time. Decreasing this period increases the raw bitrate, but it also increases the error rate, as shown in Figure [\ref=fig:bitrate].

In order to achieve optimal usage of the memory bus we employ multiple threads for both the sender and receiver processes. Thus, memory accesses are performed in parallel increasing the performance of the covert channel.

Performance

We evaluated the performance of our covert-channel implementation on a system featuring an Intel i7-4790 (Haswell). It was equipped with 2 Kingston DDR3 KVR16N11/8 dual-rank 8GB DIMMs in dual-channel configuration (cf. Figure [\ref=fig:mapping_haswell]). For evaluation in ideal conditions we fixed the CPU clock to 3.6 GHz. Thus noise due to speed-stepping is avoided. Furthermore, the system was mostly idle during the tests, i.e., , there were no other tasks causing significant load on the system. The DRAM clock was set to its default of 800 MHz (DDR3-1600).

We transmit 6 bits per block (use 6 out of 32 (channel, DIMM, rank, bank) tuples) in the covert channel and instantiate 2 threads in both the sender and the receiver process. Every thread is fixed to one of the four CPU cores. The average access time profiling is performed every microsecond for [formula]s. We then performed tests with measurement time frames of different lengths to find the optimal length for the measurement time. For this purpose we measure the raw channel capacity and the bit error probability. While the raw channel capacity increases almost linearly to the reduction of the measurement time, the bit error rate increases significantly if the measurement time is too short.

In order to find the best transmission rate, we use the channel capacity as metric. When using the binary symmetric channel model, this metric is computed by multiplying the raw bitrate with 1 - H(e), with e the bit error probability and H(e)  =   - e  ·   log 2(e)  -  (1 - e)  ·   log 2(1 - e) the binary entropy function. Figure [\ref=fig:bitrate] shows how the error rate develops depending on the raw bitrate. We can see that up to a raw bitrate of 1.6  Mb/s the error rate stays below 1%. Here the effective channel capacity peaks at 1.5Mb/s. Beyond that peak, the increasing error probability causes a decrease in the effective channel capacity despite having higher raw bitrates.

Note that even higher transmission rates are likely to be possible. For instance, more sophisticated signal processing (instead of mean-based discrimination) might further reduce the required measurement time per transmitted block. Also, a higher level of parallelism on other platforms might increase the performance significantly. Furthermore, the parameters can still be tuned to achieve optimal performance.

Comparison with state of the art

We compare the bitrate of our DRAM covert channel with Gruss et al.[\cite=Gruss2015arxivFF] normalized implementation of three different cache covert channels. For an error rate that is less than 1%, the covert channel using Prime+Probe obtains 536Kbps, the one using Flush+Reload 2.3Mbps and the one using Flush+Flush 3.8Mbps. With a capacity of 1.5Mbps, our covert channel is a bit slower than Flush+Flush covert channel, but of the same order of magnitude of current cache attacks. However, as the cache is not a resource that is shared between two CPUs, these cache attacks cannot be performed across CPUs, contrary to attacks performed on memory.

The covert channel of Xiao et al.[\cite=Xiao2013] using memory deduplication achieves up to 90bps. However, due to security concerns, memory deduplication has been disabled by some cloud providers. The covert channel of Wu et al.[\cite=Wu2014] using the memory bus achieves 746bps with error correction. Our covert channel is therefore three orders of magnitude higher than state-of-the-art memory-based covert channels.

Impact on Rowhammer

There are mainly two different variants of Rowhammer. The so called single-sided hammering chooses a set of unrelated addresses and tries to access them in a high frequency. While this approach is likely to succeed if the number of addresses that are hammered at once is small enough and by chance two addresses that map to the same channel, rank and bank but to different rows are contained in the set of addresses. The probability that 2 random addresses fulfil such criteria is [formula] with C the number of channels, R the number of ranks, and B the number of banks. On a quad-channel system with dual rank DDR4 DIMMs this is [formula].

The more efficient way to perform the Rowhammer attack is double-sided hammering. However, existing implementations also make additional assumptions. First, the memory is scanned in blocks of 256 KB [\cite=DullienSeaborn2015github]. This approach works on dual channel systems with a single DIMM per channel. However, if a system has a different number of channels or the number of DIMMs is not identical to the number of channels, this approach does not work and will not find bitflips at all. Second, the program now chooses 3 adjacent such blocks. The outer blocks are used for hammering while the middle one is checked for bitflips. Existing implementations hammer all combinations of 4 KB-aligned addresses from the outer blocks. However, it would suffice to hammer each row pair only once.

By using the DRAM addressing function we can reduce the number of addresses to hammer per row pair to a single address pair in any case. Table [\ref=tab:reduction_rowhammer] shows the complexity of the search for same bank different row pairs using previously published methods for different architectures and configurations. Using the DRAM addressing functions the search complexity is always 1 as we do not have to search for same bank different row pairs, but instead simply compute them based on the physical address.

Countermeasures

It is much harder to defend against this kind of side-channel attack than for instance cache attacks. Making the corresponding operations constant time would introduce inaccaptable performance degredation. However, as long as the timing difference exists and can be measured, the side channel cannot be closed. One countermeasure that has been proposed for various cache attacks is restricting clflush. An attacker would have to use eviction instead. Due to the additional cache misses caused when performing eviction through memory accesses more row conflicts will occur. Thus, it might become impractical to perform the DRAM row buffer side-channel attack as described in this paper due to the noise from the additional row conflicts. Restricting the rdtsc instruction would help against our timing-based attack. However, it would also introduce a performance penalty for benign applications.

Avoiding 2 MB pages would not prevent our attack. An attacker can still perform the timing attack we described on virtual addresses and use the identified sets for the covert channel.

Future work

This paper is a work in progress and in this section we want to give an outlook what we are currently working on to complement this work. While our attack exploits no CPU resource we are still in the process of evaluating the attack in a cross-CPU scenario. Furthermore, we are evaluating its performance on ARM CPUs in order to perform more efficient Rowhammer attacks on mobile devices. The physical approach is not possible here anymore without special equipment. Thus, we have to rely on the software method.

We also work on side-channel attacks similar to Evict+Reload attacks. Here a victim shares a DRAM row with the attacker. This is easily possible as a DRAM row is 8 KB in size and the regular page size on x86 systems is 4 KB. In such a scenario an attacker measures the access time for the shared row and then accesses another row to evict the row buffer. A lower access time on the shared row reveals that the victim opened the same row. Another attack we are working is a Prime+Probe attack where we try to keep a DRAM row opened and measure when it is closed.

Conclusions

In this paper we described a method to reverse engineer the undocumented DRAM addressing functions based on physically probing the DRAM bus or performing a timing attack in software. Based on the reversed functions an attacker can target specific rows, banks, ranks and channels. Thus is enables implementing cross-CPU side channel attacks and it reduces the complexity of the Rowhammer attack. We verified our method on a wide range of architectures including Sandy Bridge, Ivy Bridge, Haswell and Skylake micro-architectures. The functions we derived can be used on these platforms to perform cross-CPU attacks. We demonstrate the power of our attack by implementing a covert channel that achieves a transmission rate of 1.5 Mb/s. Our work shows that complex addressing functions are not only used in CPU caches but also in memory controllers.

Acknowledgments

We would like to thank Anders Fogh for discussions and coordination with his concurrent work.