>m >m >m

Fixed Point Quantization of Deep Convolutional Networks

Introduction

Recent advances in the development of deep convolution networks (DCNs) have led to significant progress in solving non-trivial machine learning problems involving image recognition [\citep=Krizhevsky_2012] and speech recognition [\citep=Deng_2013]. Over the last two years several advances in the design of DCNs [\citep=Zeiler_2014] [\citep=Simoyan_2014] [\citep=Szegedy_2014] [\citep=Chatfield_2014] [\citep=He_2014] [\citep=Ioffe_2015] have not only led to a further boost in achieved accuracy on image recognition tasks but also have played crucial role as a feature generator for other machine learning tasks such as object detection [\citep=Krizhevsky_2012] and localization [\citep=Sermanet_2013], semantic segmentation [\citep=Girshick_2014] and image retrieval [\citep=Krizhevsky_2012] [\citep=Razavian_2014]. These advances have come with an added cost of computational complexity, resulting from DCN designs involving any combinations of: increasing the number of layers in the DCN [\citep=Szegedy_2014] [\citep=Simoyan_2014] [\citep=Chatfield_2014], increasing the number of filters per convolution layer [\citep=Zeiler_2014], decreasing stride per convolution layer [\citep=Sermanet_2013] [\citep=Simoyan_2014] and hybrid architectures that combine various DCN layers [\citep=Szegedy_2014] [\citep=He_2014] [\citep=Ioffe_2015].

While increasing computational complexity has afforded improvements in the state-of-the-art performance, the added burden of training and testing times make these networks impractical for real world applications that involve real time processing and for deployment on mobile devices with limited power budget. One approach to alleviate this burden is to increase the computational power of the hardware used to deploy these networks. An alternative approach that may be cost efficient for large scale deployment is to implement DCNs in fixed point, which may offer advantages in reducing memory bandwidth, lowering power consumption and computation time as well as the storage requirements for the DCNs.

This paper investigates methods to achieve fixed point implementation of DCN. The paper is organized as follows: In Section [\ref=sec:related_work], we present a literature survey of the related work. In Section [\ref=sec:fixed_point_conversion], we develop quantizer design for fixed point DCNs. In Section [\ref=sec:bitwidth_optimization] we formulate an optimization problem to identify optimal fixed point bit-width allocation per layer of DCNs to maximize the achieved reduction in complexity relative to the loss in the classification accuracy of the DCN model. Results from our experiments with one of the state-of-the-art floating point DCNs for CIFAR-10 benchmark are reported in Section [\ref=sec:experiments] followed by conclusions in the last section.

Related work

Fixed width implementation of DCN have been explored in earlier works [\citep=courbariaux2014low] [\citep=gupta2015deep]. These works primarily focused on training DCNs using low precision fixed-point arithmetic. More recently, [\citep=Zhouhanlin_2014] showed that deep neural networks can be effectively trained using only binary weights, which in some cases can even improve classification accuracy relative to the floating point baseline. The works of [\citet=Kuyeon_2014] [\citet=Sajid_2014] more closely resemble our work. In [\citep=Kuyeon_2014], the authors propose a floating point to fixed point conversion algorithm for fully-connected networks. The authors used an exhaustive search strategy to identify optimal fixed point bit-width for the entire network. In a follow up paper [\citep=Sajid_2014], the authors applied their proposed algorithm to DCN models where they analyzed the quantization sensitivity of the network for each layer and then manually decide the quantizaition bit-wdiths. Other works that are somewhat closely related are [\citep=vanhoucke2011improving] [\citep=han2015deep] [\citep=gong2014compressing]. [\cite=vanhoucke2011improving] quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. [\cite=han2015deep] and [\cite=gong2014compressing] on the other hand applied codebook based on scalar and vector quantization methods respectively, in order to reduce the model size.

In the spirit of [\citep=Sajid_2014], we also focus on optimizing DCN models that are pre-trained with floating point precision. However, as opposed to exhaustive search method adopted by [\cite=Sajid_2014], our objective is to convert the pre-trained DCN model into a fixed-point model using an optimization strategy based on signal-to-quantization noise ratio (SQNR). In doing so, we aim to improve upon the inference speed of the network and reduce storage requirements. There has been some work in literature related to our stated objective. Our work is also different from [\citep=vanhoucke2011improving] [\citep=han2015deep] [\citep=gong2014compressing] in the following sense:

We use different bitwidths across layers, and propose a method for bitwidth allocation.

We use quantizer step sizes derived from optimal quantizer design theory.

Floating point to fixed point conversion

In this section, we will propose an algorithm to convert a floating point DCN to fixed point. For a given layer of DCN the goal of conversion is to represent the input activations, the output activations, and the parameters of that layer in fixed point. This can be seen as a process of quantization.

Optimal uniform quantizer

There are three inter-dependent parameters to determine for the fixed point representation of a floating point DCN: bit-width, step-size (resolution), and dynamic range. These are related as follows:

[formula]

Given a fixed bit-width, the trade-off is between having large enough range to reduce the chance of overflow and small enough resolution to reduce the quantization error. The problem of striking the best trade-off between overflow error and quantization error has been extensively studied in the literature. Table 1 below shows the step sizes of the optimal symmetric uniform quantizer for Gaussian, Laplacian and Gamma distributions. The quantizers are optimal in the sense of minimizing the signal-to-quantization-noise ratio (SQNR).

For example, suppose the input is Gaussian distributed with zero mean and unit variance. If we need a uniform quantizer with bit-width of 1 (i.e. 2 levels), the best approach is to place the quantized values at -0.798 and 0.798. In other words, the step size is 1.596. If we need a quantizer with bit-width of 2 (i.e. 4 levels), the best approach is to place the quantized values at -1.494, -0.498, 0.498, and 1.494. In other words, the step size is 0.996.

In practice, however, even though a symmetric quantizer is optimal for a symmetric input distribution, it is often desirable to have 0 as one of the quantized values because of the potential savings in model storage and computational complexity. This means that for a quantizer with 2 levels, the quantized values would be 0 and 1.596 (or -1.596 and 0). While this is clearly suboptimal when the number of quantized levels is small, it performs increasingly closer to a symmetric quantizer when the bit-width, β >  > 1.

Given an optimal uniform quantizer with ideal input, the resulting SQNR as a function of the bit-width is shown in Figure [\ref=fig:quant_eff]. It can be observed that the quantization efficiency decreases as the Kurtosis of the input distribution increases.

Another take-away from this figure is that there is an approximately linear relationship between the bit-width and resulting SQNR:

[formula]

where, γdB = 10 log 10(γ), is the SQNR in dB, κ is the quantization efficiency, and β is the bit-width. Note that the slopes of the lines in Figure [\ref=fig:quant_eff] depict the optimal quantization efficiency for ideal distributions. The quantization efficiency for uniform distribution is the well-known value of 6dB/bit [\citep=Shi08], while the quantization efficiency for Gaussian distribution is about 5dB/bit [\citep=You10] . Actual quantization efficiency for non-ideal inputs can be significantly lower. Our experiments show that the SQNR resulting from uniform quantization of the actual weights and activations in the DCN is between 2 to 4dB/bit.

Empirical distributions in DCN

Our simulations have revealed some interesting statistics for DCNs. Figure [\ref=fig:weights_pdf] and Figure [\ref=fig:acts_pdf] depict the empirical distributions of weights and activations, respectively for the CIFAR-10 benchmark DCN (see Section [\ref=sec:experiments]).

Note that the activations plotted here are before applying the activation functions. Taking the ReLU neurons for example, the activations at the output of the neuron would not have any negative values. It is seen that in most cases, both the weights and activations are roughly Gaussian distributed. The activations of earlier layers have longer tails, making them resemble the Laplacian distributions. In our fixed point design, Gaussian distribution is typically assumed. Since the weight and activation distributions of some layers are less Gaussian-like, it is also of interest to experiment with step-sizes for other distributions (see Table [\ref=tab:step_size]), which is beyond the scope of present work.

Model conversion

Any floating point DCN model can be converted to fixed point by following these steps:

Run a forward pass in floating point using a sufficiently large set of typical inputs and record the activations

Collect the statistics of weights, biases and activations for each layer

Determine the number formats of the weights, biases and activations for each layer

Note that determining the number format for a fixed point quantity is equivalent to determining the resolution, which in turn means identifying the number of fractional bits it requires to represent the number. The following equations can be used to compute the number of fractional bits:

Determine the effective standard deviation of the quantity being quantized: σ̂

Calculate step size according to Table [\ref=tab:step_size]: [formula]

Compute the number of fractional bits: [formula]

In these equations,

σ̂ is the effective standard deviation of the quantity being quantized, an indication of the width of the distribution we want to quantize. For example, if the quantized quantities follow an ideal zero mean Gaussian distribution, then σ̂  =  σ, where σ is the true standard deviation of quantized values. If the actual distribution has longer tails than Gaussian, which is often the case as shown in Figure [\ref=fig:weights_pdf] and [\ref=fig:acts_pdf], then σ̂  >  σ. In our experiments in Section [\ref=sec:experiments], we set σ̂ = 3σ.

Stepsize(β) is the optimal step size corresponding to quantization bit-width of β, as listed in Table [\ref=tab:step_size].

s is the computed step size for the quantized distribution.

n is the number of fractional bits in the fixed point representation. Equivalently, 2- n is the resolution of the fixed point representation and a quantized version of s. Note that ⌈  ·  ⌉ is one choice of a rounding function and is not unique.

Bit-width optimization across a deep network

In the absence of model fine-tuning, converting a floating point deep network into a fixed point deep network is essentially a processing of introducing quantization noise into the neural network. It is well understood in fields like audio processing or digital communications that as the quantization noise increases, the system performance degrades. The effect of quantization can be accurately captured in a single quantity, the SQNR.

In deep learning, there is not a well-formulated relationship between SQNR and classification accuracy, but it is reasonable to assume that in general higher quantization noise level leads to worse classification performance. Therefore, as shown in Figure [\ref=fig:sqnr_accuracy], to simplify our study we will make an approximating conjecture that the SQNR of the output activations used for classification is a surrogate measure for the final accuracy.

While the relationship between SQNR and classification accuracy is not exactly monotonic, as our experiments in Section [\ref=sec:results_quantizer] indicate, it is a useful approximation. This approximation is also important because it is usually difficult to directly predict the classification accuracy. SQNR, on the other hand, can be approximated theoretically and analyzed layer by layer, as can be seen in the next section.

Impact of quantization on SQNR

In this section, we will derive the relationship between the quantization of the weight, bias and activation values respectively, and the resulting output SQNR.

Quantization of individual values

Quantization of individual values in a DCN, whether it is an activation or weight value, readily follows the quantizer discussion in Section [\ref=sec:quantizer]. For instance, for weight value w, the quantized version w̃ can written as:

[formula]

where nw is the quantization noise. As illustrated in Figure [\ref=fig:quant_eff], if w is approximately follows a Gaussian, Laplacian or Gamma distribution, the SQNR, γw, as a result of the quantization process can be written as:

[formula]

where κ is the quantization efficiency and β is the quantizer bit-width.

Quantization of both activations and weights

Consider the case where weight w is multiplied by activation a, where both w and a are quantized with quantization noise nw and na, respectively. The product can be approximated, for small nw and na, as follows:

[formula]

The last equality holds if |na| <  < |a| and |nw| <  < |w|. A very important observation is that the SQNR of the product, w  ·  a, as a result of quantization, satisfies

[formula]

This is characteristic of a linear system. The defining benefit of this realization is that introducing quantization noise to weights and activations independently is equivalent to adding the total noise after the product operation in a normalized system. This property will be used in later analysis.

Forward pass through one layer

In a DCN with multiple layers, computation of the ith activation in layer l + 1 of the DCN can be expressed as follows:

[formula]

where (l) represents the lth layer, N represents number of additions, wi,j represents the weight and bi represents the bias.

Ignoring the bias term for the time being, since a(i + 1)i is simply a sum of terms like w(l + 1)i,ja(l)j, which when quantized all have the same SQNR γw(l + 1)  ·  a(l). Assuming the product terms w(l + 1)i,ja(l)j are independent, it follows that the value of a(i + 1)i, before further quantization, has inverse SQNR that equals

[formula]

After a(l + 1)i is quantized to the assigned bit-width, the resulting inverse SQNR then becomes [formula]. We are not considering the biases in this analysis because, assuming that the biases are quantized at the same bit-width as the weights, the SQNR is dominated by the product term w(l + 1)i,ja(l)j. Note that Equation [\ref=eq:gamma_a] matches rather well with experiments even though the independence assumption of w(l + 1)i,ja(l)j does not always hold.

Forward pass through the entire network

Equation [\ref=eq:gamma_a] can be generalized to all the layers in a DCN (although we have found empirically that the approximation applies better for convolutional layers than fully-connected layers). Consider layer L in a deep network as shown in Figure [\ref=fig:dcn_layers] in Appendix [\ref=sec:layer_description], where all the activations and weights are quantized. Extending Equation [\ref=eq:gamma_a], we obtain the SQNR (γoutput) at the output of layer L as:

[formula]

In other word, the SQNR at the output of a layer in DCN is the Harmonic Mean of the SQNRs of all preceeding quantization steps. This simple relationship reveals some very interesting insights:

All the quantization steps contribute equally to the overall SQNR of the output, regardless if it's the quantization of weights, activations, or input, and irrespective of where it happens (at the top or bottom of the network).

Since the output SQNR is the harmonic mean. The network performance will be dominated by the worst quantization step. For example, if the activations of a particular layer has a much smaller bit-width than other layers, it will be the bottleneck of network performance, because based on Equation [\ref=eq:gamma_output], γoutput  ≤  γa(l) for all l.

Depth makes quantization more challenging, but not exceedingly so. The rest being the same, doubling the depth of a DCN will half the output SQNR (3dB loss). But this loss can be readily recovered by adding 1 bit to the bit-width of all weights and activations, assuming the quantization efficiency is more than 3dB/bit. However, this theoretical prediction will need to be empirically verified in future works.

Effects of other network components

Batch normalization: Batch normalization [\citep=Ioffe_2015] improves the speed of training a deep network by normalizing layer inputs. After the network is trained, the batch normalization layer is a fixed linear transformation and can be absorbed into the neighboring convolutional layer or fully-connected layer. Therefore, the quantization effect due to batch normalization does not need to be explicitly modeled.

ReLU: In Equation [\ref=eq:a_i], for simplicity we omitted the activation function applied to a(l)j. When the activation function is ReLU and the quantization noise is small, all the positive values at the input to the activation function will have the same SQNR at the output, and the negative values become zero (effectively reducing the number of additions, N). In other words,

[formula]

where g(  ·  ) is the ReLU function and M  ≤  N is the number of a(l)j's that are positive.

In this case, the ReLU function has little impact on the SQNR of a(l + 1)i. ReLU only starts to affect SQNR calculation when the perturbation caused by quantization is sufficiently large to alter the sign of a(l)j. Therefore, our analysis may become increasingly inaccurate as the bit-width becomes too small (quantization noise too large).

Non-ReLU activations: Other nonlinear activation functions such as tanh, sigmoid, pReLU functions are much harder to model and analyze. However, in Section [\ref=sec:cross_layer_opt] we will see that applying the analysis in this section to a network with pReLU activation functions still yields useful enhancements.

Cross-layer bit-width optimization

From Equation [\ref=eq:gamma_output], it is seen that trade-offs can be made between quantizers of different layers to produce the same γoutput. That is to say, we can choose to use smaller bit-widths for some layers by increasing bit-widths for other layers. For example, this may be desirable because of the following reasons:

Some layers may require large number of computations. Reducing the bit-widths for these layers would reduce the overall network computation load.

Some layers may contain large number of network parameters (weights). Reducing the weight bit-widths for these layers would reduce the overall model size.

Interestingly, such objectives can be formulated as an optimization problem and solved exactly. Suppose our goal is to reduce model size while maintaining a minimum SQNR at the DCN output. We can use ρi as the scaling factor at quantization step i, which in this case represents the number of parameters being quantized in the quantization step. The problem can be written as:

[formula]

where 10 log γi is the SQNR in dB domain, and (10 log γi) / κ is the bit-width in the ith quantization step according to Equation [\ref=eq:kappa_beta]. γ min is the minimum output SQNR required to achieve a certain level of accuracy. The summation of γi's follows from Equation [\ref=eq:gamma_output] that the output SQNR is the harmonic mean of the SQNR of intermediate quantization steps.

Substituting by [formula] and removing the constant scalars from the objective function, the problem can be reformulated as:

[formula]

where the constant [formula]. This is a classic constrained optimization problem with a well-known solution: [formula]. Or equivalently,

[formula]

Recognizing that 10 log γi  =  κβi based on Equation [\ref=eq:kappa_beta], the solution can be rewritten as:

[formula]

In other words, the difference between the optimal bit-widths of two quantization steps is inversely proportional to the difference of ρ's in dB, scaled by quantization efficiency.

[formula]

This is a surprisingly simple and insightful relationship. For example, assuming κ = 3dB/bit, the bit-widths βi and βj would differ by 1 bit if ρj is 3dB (or 2x) larger than ρi. In other words, for model size reduction, layers with more parameters should use relatively lower bit-width, because it leads to better model compression under the overall SQNR constraint.

Experiments

In this section we study the effect of reduced bit-width for both weights and activations versus traditional 32-bit single-precision floating point approach. In particular, we will implement the fixed point quantization algorithm described in Section [\ref=sec:fixed_point_conversion] and investigate the effectiveness of the bit-width optimization algorithm in Section [\ref=sec:bitwidth_optimization]. In addition, using the quantized fixed point network as the starting point, we will attempt to further fine-tune the fixed point network within the restricted alphabets of weight and activation values.

Note that although we have not experimented with 16-bit floating point, it is reasonable to expect 16-bit floating point to produce comparable results as 16-bit fixed point or 32-bit floating point for the CIFAR-10 network we are studying.

Fixed point quantization algorithm

We evaluate our proposed floating-point to fixed-point conversion algorithm on the CIFAR-10 benchmark. The description of the DCN we have designed can be found in Appendix [\ref=sec:dcn_description].

Table [\ref=tab:results_no_ft] lists the classification error rate (in %) for both floating point and 4-bit, 8-bit, and 16-bit fixed point weight and activation bit-width combinations for the CIFAR-10 network.

It is seen that, using the proposed quantization algorithm with no fine-tuning, the quantized classifiers work well down to (8b, 8b) combination (8-bit weights and 8-bit activations) and generally degrade as bit-width decreases, but (16b, 16b) and (float, 16b) fixed point models are even better than the floating point baseline. We will discuss the results with fine-tuning in Section [\ref=sec:fine_tuning_results].

Bit-width optimization across layers

In Section [\ref=sec:results_quantizer], we focused on scenarios where the same bit-width is shared by weights (or activations) across all layers. In this section, we will look into how to further optimize the classification performance via the cross-layer bit-width optimization algorithm prescribed in Section [\ref=sec:opt_bitwidth_derivation].

In Table [\ref=tab:model_details], we compute the number of parameters in each layer of our CIFAR-10 network. Consider the objective of minimizing the overall model size. Our derivation in Section [\ref=sec:opt_bitwidth_derivation] shows that, provided that the quantization efficiency κ = 3dB/bit, the optimal bit-width of layer conv0 and conv1 would differ by 10 log (0.295 / 0.007) / κ  =  5bits. Similarly, assuming the bit-width for layer conv0 is β0, the subsequent convolutional layers should have bit-width values as indicated in Table [\ref=tab:opt_bitwidth].

In our experiment in this section, we will ignore the fully-connected layer and assume a fixed bit-width of 16 for both weights and activations. This is because fully-connected layers have different SQNR characteristics and need to be optimized separately. Although the fully-connected layers can often be quantized more aggressively than convolutional layers, in this experiment, since the number of parameters of fc0 is very small, we will set the bit-width to a large value to eliminate the impact of quantizing fc0 from the analysis, knowing that the large bit-width of fc0 has very little impact on the overall model size. We will also set the activation bit-widths of all the layers to a large value of 16 because they contribute little to the model size.

Table [\ref=tab:bitwidth_scenarios] lists a number of bit-width allocation schemes where the weights of different layers either share the same bit-width or are optimized according to Table [\ref=tab:opt_bitwidth]. The last column shows the classification error rate for each scenario.

Figure [\ref=fig:equal_vs_opt] displays the model size vs. error rate in a scatter plot, we can clearly see the advantage of cross-layer bit-width optimization. When the model size is large (bit-width is high), the error rate saturates at around 6.9%. When the model size reduces below approximately 25Mbits, the error rate starts to increase quickly as the model size decreases. In this region, cross-layer bit-width optimization offers > 20% reduction in model size for the same performance.

In Appendix [\ref=sec:alexnet], we adopt the same optimization framework for an AlexNet-like DCN [\citep=Krizhevsky_2012] that is trained on Imagenet-1000 dataset. Our results demonstrate that the SQNR-based optimization is in general applicable for different networks that are also trained on larger datasets.

Validation for SQNR prediction

In this section we will evaluate the accuracy of the SQNR calculations presented in Section [\ref=sec:quantization_sqnr]. In particular, we will focus on the optimized networks #10, #11, #12, and #13 in Table [\ref=tab:bitwidth_scenarios] and compare the measured SQNR per layer to the theoretical SQNR predictions according to Equation [\ref=eq:gamma_w] and [\ref=eq:gamma_a]. As before, we assume the the quantization efficiency κ = 3dB/bit.

Table [\ref=tab:sqnr_compare] contains the comparison between the theoretical SQNR and the measured SQNR (in dB) for layers conv1 to conv5 for each of the optimized networks. We observe that while the two SQNR values do not match numerically, they follow similar decreasing trend as the activations propagate deeper into the network. It should be noted that our theoretical SQNR predictions are based purely on the weight and activation bit-widths of each layer as well as the quantization efficiency κ. The theory does not rely on any information related to the network parameters or the data it is tested on.

Model fine-tuning

Table [\ref=tab:results_no_ft] also contains the classification error rate (in %) for the CIFAR-10 network after fine-tuning the model for 30 epochs. It is shown that even the (4b, 4b) bit-width combination works well (8.30% error rate) when the network is fine-tuned after quantization. In addition, the (float, 8b) setting generates an error rate of 6.78%, which is the new state-of-the-art result even though the activations are only 8-bit fixed point values. This may be attributed to the regularization effect of added noise, which reduces overfitting [\citep=Zhouhanlin_2014] [\citep=Luo_2014].

Conclusions and future work

We showed that quantizing all the layers in a neural network with the same uniform bit-width value is not the optimal strategy. We formulated an SQNR based optimization problem with a closed-form solution for bit-width allocation across layers. We showed that the proposed method achieves lower classification error on CIFAR10 dataset compared to the strategy of allocating uniform bit-width for all layers. We also performed fine tuning experiments and demonstrate that even with a non-optimal bit-width allocation strategy, classification accuracy of DCN can be boosted with fine-tuning. It will be interesting to determine the effect of fine-tuning on a fixed-point implementation of DCN that follows the optimal strategy. Furthermore, It will be interesting to study the relation between SQNR and classification performance in a closer detail. Specifically, the relation between the depth of network and quantization is interesting. The SQNR formulation suggests using an additional bit on doubling the depth of the neural network. We plan to investigate these questions in future works.

We would like to acknowledge fruitful discussions and valuable feedback from our colleagues at Qualcomm Research: David Julian, Anthony Sarah, Daniel Fontijne, Somdeb Majumdar, Aniket Vartak, Blythe Towal, and Mark Staskauskas.

Appendix

Description of layers in a DCN

Figure [\ref=fig:dcn_layers] provides a graphical illustration of the layered structure of a typical DCN.

Description of our DCN for CIFAR-10 dataset

The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The classes include airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. Of the 60,000 images, 50,000 are training images and 10,000 are test images.

Our floating point model produces the state-of-the-art 6.98% error rate with pReLU non-linearity [\citep=Talathi_2015] after training of 550 epochs. The DCN architecture is summarized in Table [\ref=tab:DCN_Architecture].

Bit-width optimizing for ImageNet classification

In Section [\ref=sec:cross_layer_opt], we performed cross-layer bit-width optimization for our CIFAR-10 network with the objective of minimizing the overall model size. In this section, we carry out a similar exercise for an AlexNet-like DCN that is trained on Imagenet-1000. The DCN architecture is described in Table [\ref=tab:model_details_alexnet].

For setting the bit-width of convolution layers of this DCN, we follow the steps in Section [\ref=sec:cross_layer_opt] with the assumption that the bit-width for layer conv1 is β1. The resulting bit-width allocation for all conv layers is summarized in Table [\ref=tab:opt_bitwidth_alexnet].

For fully-connected layers, given that it dominates in terms of mode parameters, we take the following steps in bit-width optimization.

First keep the network as floating point and quantize the weights of fully-connected layers only. Reduce bit-width of fully-connected layers until the classification accuracy starts to degrade. We found that the minimum bit-width for the fully-connected layers before performance degradation occurs is 6.

Then vary the bit-widths of the convolutional layers assumed either equal bit-width across layers or through the optimized relationship as summarized in Table [\ref=tab:opt_bitwidth_alexnet].

Table [\ref=tab:bitwidth_scenarios_alexnet] lists a number of bit-width allocation schemes we have tried out. The last two columns contain the top-5 and top-1 classification error rate for each scenario. The model size column contains the model size of convolutional layers only because we are only optimizing across the convolutional layers and the model size of the fully-connected layers is fixed.

Figure [\ref=fig:equal_vs_opt_alexnet] depicts the convolutional layer model size vs. top-5 error rate tradeoff for both optimized bit-width and equal bit-width scenarios. Similar to our CIFAR-10 network, there is a clear benefit of cross-layer bit-width optimization in terms of model size reduction. In some regions the saving can be upto 1Mbits.

However, contrary to our CIFAR-10 network where the convolutional layers make up most of the model size, in AlexNet-like DCN the fully-connected layers dominate in terms of number of parameters. With bit-width of 6, the overall size of fully-connected layers is more than 100Mbits. This means that the saving of 1Mbits brings in less than 1% of overall model size reduction. This is in clear contrast to the 20% model size reduction we reported for our CIFAR-10 network. In general, the proposed cross-layer layer bit-width optimization algorithm is most effective when the network size is dominated by convolutional layers, and is less effective otherwise.