An Automatic Machine Translation Evaluation Metric Based on Dependency Parsing Model

Introduction

Automatic machine translation (MT) evaluation not only evaluates the performance of MT systems, but also accelerates the development of MT systems [\cite=och2003minimum]. According to the type of the employed information, the automatic MT evaluation metrics can be classified into three categories, lexicon-based metrics, syntax-based metrics and semantic-based metrics.

Most of the syntax-based evaluation metrics obtain the similarity between reference and hypothesis by comparing the sub-structures between the trees of reference and hypothesis, such as HWCM [\cite=Liu05syntacticfeatures] and the LFG-based metric [\cite=Owczarzak:2007:DAE:1626281.1626292]. HWCM uses the headword chains extracted from the dependency tree, while the LFG-based metric uses the Lexical-Functional Grammar dependency tree. Some syntax-based metrics calculate the similarity between the sub-structure of the reference tree and the string of hypothesis, such as BLEUÂTRE [\cite=mdobrink:BLEUATRE] and RED [\cite=yu-EtAl:2014:Coling3]. The sub-structures in these metrics are defined by human and can't express all the information in the trees because of the limited length of sub-structures. In addition, the overlapped parts between these sub-structures are computed repeatedly.

To avoid the above defects, we propose a new metric from the view of dependency tree generation. We don't need to define sub-structures by human for the new metric. We train a dependency parsing model by the reference dependency tree. By this model, we can obtain the dependency tree of the hypothesis and the corresponding probability which is also the score of the dependency parsing model. The syntactic similarity between the hypothesis and the reference can be judged by this score. In order to obtain the lexicon similarity, we also introduce the unigram F-score to the new metric. The experiment results show that the new metric gets the-state-of-art performance on system level evaluation, and gets the comparable correlation with METEOR on sentence level evaluation.

The remainder of this paper is organized as follows. Section 2 describes the maximum-entropy-based dependency parsing model. Section 3 presents the new MT evaluation metric based on dependency parsing model. Section 4 gives the experiment results. Conclusions and future work are discussed in Section 5.

Maximum-entropy-based Dependency Parsing Model

Shift-reduce algorithm is used in the dependency parsing model. In the shift-reduce algorithm, the input sentence is scanned from left to right. In each step, one of the following two actions is selected, shift the current word into the stack or reduce the two (or more than two) items on the top of the stack to one item.

Generally, the reduce action includes two sub-actions reduceL and reduceR. reduceL means that the left item is considered as the head after reducing, and reduceR means that the right item is considered as the head after reducing. Formally, the transition state in the shift-reduce parser can be represented as a tuple < S,Q,A > . S is a stack. Q is a sequence of unprocessed words. A is the already-built set of dependency arcs, which is part of the dependency tree in the current state. In each step, one of the following three actions is selected.

shift: shift the head word in the queue Q into the Stack S.

reduceL: merge the top two items (st and st - 1) in S into st, t >  = 2. st is considered as the head, and the left arc (st, st - 1) is added to the set A.

reduceR: merge the top two items (st and st - 1) in S into st - 1, t >  = 2. st - 1is considered as the head, and the right arc (st - 1, st) is added to the set A.

In the traditional shift-reduce decoder algorithm, the next action can be predicted by Formula [\eqref=pre-action], when the state of the dependency parser is s. In Formula [\eqref=pre-action], action = {shift,reduceL,reduceR}. scoreact(T,s) is the score of action T when the current state is s.

[formula]

We use the method of classification to decide which action should be chosen in the transition sequence. We combine the action and the corresponding context as a training example, which describes which action should be chosen in a certain context. The context can be represented as a series of features. The feature templates used in this paper are the same as those used in Huang et al. .

We use the maximum entropy as the classification method to train the examples and get modelME. When calculating the score of a transition action, we use Formula [\eqref=3-line].

[formula]

fi(T',s) is the ith feature when the current state is s and the transition action is T'. λi is the weight of the ith feature. In shift-reduce algorithm, there are three kinds of actions in each transition action. The probability that the scores of all the three actions are zero is very low, because the feature templates include POS (Part-of-Speech) of the current word and POS of the two words before the current word. If modelME chooses two kinds of actions, the score of the third action is zero. To avoid the zero score, we use the normalization method in Formula [\eqref=3-regular]. Pact(T',s) is the normalized probability of the chosen action T' when the current state is s. z is the constant for normalization. set(s) in Formula [\eqref=3-z] is the set of all possible actions when the current state is s.

[formula]

[formula]

Beam search algorithm [\cite=zhang2008tale] is used in shift-reduce decoder algorithm. For a sentence x, we can get many dependency trees and use gen(x) to represent the set of the dependency trees. Then the best one can be obtained by Formula [\eqref=formula-parser]. actset(y) represents the set of all the actions when generating dependency tree y.

[formula]

modelME is trained with the data which contain the information in the process of dependency parsing and is used to parse a sentence. So we name the trained model modelME as dependency parsing model. The score of the dependency parsing model is defined in Formula [\eqref=formula-dep].

[formula]

Dependency-parsing-model-based MT Evaluation Metric

Training of Dependency Parsing Model

We should get the reference dependency tree first for training dependency parsing model. The reference dependency tree can be generated by the open-source tools or labeled by human. We use the Stanford tools to generate reference dependency tree. After obtaining the reference dependency tree, we can use it to train the dependency parsing model. The reference dependency tree is used as training corpus to extract features, according to the feature templates defined in Huang et al. . A training example is achieved by combining the features and the action in shift-reduce algorithm. The format of the training example is shown in Table [\ref=feat-exam]. We train the extracted examples using the maximum entropy and get a dependency parsing model. According to the method introduced in Section 2, we parse the hypothesis using this dependency parsing model. We can get a Score(hyp) of the dependency parsing model for hypothesis hyp as in Formula [\eqref=formula-dep].

We train a dependency parsing model for each sentence separately. That is to say, the reference dependency tree of sentence i is only used to train the dependency parsing model for the hypothesis of sentence i. We also tried other methods, such as using all the reference dependency trees to train the model for each hypothesis, or adding a background corpus together with the reference dependency tree to train the model for each hypothesis. For the above two methods, we give a higher weight to the dependency tree of sentence i when training the model for hypothesis i. However, for these two methods, the performance is worse than only using the reference dependency tree of sentence i when training the model for hypothesis i.

The dependency parsing model is trained by maximum entropy model, which can ensure smoothness when satisfying all of the conditions. In the case of data sparse, all the features of all the actions in a state may be zero, according to Formula [\eqref=3-regular]. For this state, the probabilities of all the actions are equal. Sometimes none of the words in hypothesis appears in reference, but the POS of some words may appear in the reference. The dependency parsing model can differentiate this case, because POS is used in the feature templates. Table [\ref=3-POS] gives a reference, two hypotheses and the corresponding POS sequences of the three sentences. We can see that, none of the words in hyp1 or hyp2 appears in the reference, but the POS of some words appear in the reference. According to the dependency parsing model defined in Formula [\eqref=formula-dep], we can get Score(hyp1) =  - 4.46 and Score(hyp2) =  - 5.87. From these two scores, we can conclude that hyp1 is better than hyp2, which is the truth.

Normalization of the Dependency Parsing Model Score

A transition sequence is obtained in the process of generating the dependency tree according to the shift-reduce algorithm. Each word in the sentence should be pushed into the stack once, and each word is popped from the stack once for reduction except the root node. Therefore, there are n steps of shift actions and n - 1 steps of reduce actions, 2n - 1 actions in all, which means that the length of the transition sequence is 2n - 1. n is the length of the sentence. The score of the dependency parsing model is the sum of the logarithms of the transition actions' probabilities, as in Formula [\eqref=formula-dep]. Because the value is negative after the logarithm, it will cause penalty for long sentences. Some sentences can achieve high scores because of a shorter length and not because of higher quality. Therefore, we need to normalize the score of the dependency parsing model, as in Formula [\eqref=prode-norm]. hyp is a hypothesis. n is the length of hyp. Score(hyp) is defined in Formula [\eqref=formula-dep]. The normalized score of the Dependency Parsing Model is named as DPM which is a value between 0 and 1.

[formula]

Lexical Similarity

Dependency parsing model mainly evaluates the syntax structure similarity between the reference and the hypothesis. Besides the syntax structure, another important factor is the lexical similarity. Therefore, unigram F-score is used to represent the lexical similarity in our metric.

F-score can be calculated by Formula [\eqref=fscore]. α is a decimal between 0 and 1, which can balance the effects of precision and recall. P means precision and R means recall.

[formula]

Many automatic evaluation metrics can only find the exact match between the reference and the hypothesis, and the information provided by the limited number of references is not sufficient. Some evaluation metrics, such as TERp [\cite=snover2009fluency] and METOER [\cite=banerjee-lavie:2005:MTSumm] [\cite=lavie2009meteor] [\cite=denkowski:lavie:meteor-wmt:2014], introduce extra resources to expand the reference information. We also introduce some extra resources when calculating F-score, such as stem [\cite=porter2001snowball], synonym and paraphrase. First, we obtain the alignment with Meteor Aligner [\cite=denkowski:lavie:meteor-wmt:2011] in which exact, stem, synonym and paraphrase are all considered. Then we can find the matched words using the alignment, and every matched word corresponds to a match module type (exact, stem, synonym or paraphrase). Different match module types have different match weights, which can be represented as wexact, wstem, wsynonym and wparaphrase.

The words within a sentence can be classified into content words and function words. The effects of the two kinds of words are different and they should not have the same matching score, so we introduce a parameter wf to distinguish them.

After introducing extra resources, the precision P and recall R can be calculated by Formula [\eqref=precision] and Formula [\eqref=recall] respectively.

[formula]

[formula]

In Formula [\eqref=precision], i is the ith word in the matched unigrams, 0 < i  ≤  n, and n is the number of the matched unigrams. mi is the weight of the match module which the ith matched word belongs to. wf is the weight of function words. numf(h) is the number of function words in the hypothesis, and numc(h) is the number of content words in the hypothesis. fh(i) represents whether the ith matched unigram in hypothesis is function word. ch(i) represent whether the ith matched unigram in hypothesis is content word.

In Formula [\eqref=recall], i, mi and wf have the same meanings as those in Formula [\eqref=precision]. numf(r) and numc(r) are the number of function words and content words respectively in reference. fr(i) represents whether the ith matched word in reference is function word. cr(i) represent whether the ith matched unigram in reference is content word.

Final Score of DPMF

After obtaining the score of dependency parsing model and lexical similarity, we can calculate the final score of the new metric. Because we use both the Dependency Parsing Model and F-score, we name the score as DPMF. As in Formula [\eqref=prode-last], DPMF can evaluate the similarities both on syntax and on lexicon.

[formula]

The system level score is the average score of all the sentences. There are some parameters when calculating F-score. The meaning of each parameter is listed in Table [\ref=para-meaning].

Experiment

To verify the effectiveness of DPM and DPMF, we carry out experiments on both the system level and the sentence level.

Data

The data used in the experiment are WMT 2012, WMT 2013 and WMT 2014. The language pairs are Czech-to-English, German-to-English, Spanish-to-English, French-to-English, Russian-to-English and Hindi-to-English. The number of translation systems for each language pair are shown in Table [\ref=data-into].

All the parameters of DPMF are also included in METEOR and METEOR has tuned these parameters for better performance. So we use the same parameter values as METEOR as empirical value in DPMF and don't need to tune the parameters again. The parameter values used in the experiment are listed in Table [\ref=value-into].

System Level Correlation

To evaluate the correlation with human judges, Spearman's rank correlation coefficient ρ is used for system level. ρ is calculated using Formula [\eqref=rho].

[formula]

di is the difference between the human rank and metric’s rank for system i. n is the number of systems.

In the experiment, we give the correlations of DPM and DPMF respectively. For comparison, the baseline metrics are the widely-used metrics, BLEU, TER and METEOR. In addition, we also give the correlations of the metrics with the best performance on average according to the published results of WMT 2012, WMT 2013 and WMT 2014. For WMT 2012 and WMT 2013, the metrics with the best performance on average are SEMPOS [\cite=machavcek2011approximating] and METEOR respectively. For WMT 2014, the top-four metrics are DISCOTK-PARTY-TUNED [\cite=joty-EtAl:2014:W14-33], LAYERED [\cite=gautam-bhattacharyya:2014:W14-33], DISCOTK-PARTY [\cite=joty-EtAl:2014:W14-33] and UPC-STOUT [\cite=gonzalez-barroncedeno-marquez:2014:W14-33]. They are all hybrid metrics which include many kinds of other metrics. For fairness, we also give the result of the metric with the best performance on average in the single metrics, VERTA-W [\cite=comelles-atserias:2014:W14-33].

System level correlations are shown in Table [\ref=3-rst-sys]. According to Table [\ref=3-rst-sys], DPM can get higher correlations than BLEU and TER on the three data sets. DPM also gets higher correlations than METEOR on WMT 2012 and WMT 2014. The experiment results show that DPM can effectively evaluate the hypothesis. In order to evaluate the lexical information, we also introduce the F-score to DPM and add some extra linguistic resources to F-score to more accurately evaluate the similarity between the hypothesis and the reference on lexicon. After adding F-score, the performance of DPMF is greatly improved over DPM on the three data sets. So it is effective to add F-score to DPM to evaluate the lexical information. On WMT 2012, WMT 2013 and WMT 2014, DPMF gets higher correlations than METEOR. Compared with the best metric SEMPOS in WMT 2012, DPMF achieves higher correlations on the three language pairs cs-en, es-en and fr-en, and gets 1.3 points improvement over SEMPOS on average. Compared with the best metric METEOR in WMT 2013, DPMF achieves higher correlations on all the language pairs except an equal correlation on fr-en. On average, DPMF obtains 2.3 points improvement over METEOR. Compared with the best single metric VERTA-W in WMT 2014, the correlation improvement of DPMF is 1.4 points. DPMF also outperforms the hybrid metrics LAYERED and DISCOTK-PARTY, but there is still some work to do to catch up with the best hybrid metric for DPMF.

Sentence Level Correlation

To evaluate the performance of DPM and DPMF further, we also carry out the experiments on sentence level. On sentence level, Kendall's τ correlation coefficient is used. τ is calculated using the following equation. num_con_pairs is the number of concordant pairs and num_dis_pairs is the number of disconcordant pairs.

In the experiments, we give the results of DPM and DPMF respectively. For comparison, the baseline metrics are the widely-used metrics, BLEU and METEOR. In addition, we also give the correlations of the metric with the best performance on average according to the published results of WMT 2012, WMT 2013 and WMT 2014. The metrics with the best performance on average are spede07_pP on WMT 2012, SIMPBLEU-RECALL on WMT 2013 and DISCOTK-PARTY-TUNED on WMT 2014 respectively. Because DISCOTK-PARTY-TUNED is a hybrid metric, we also give the result of the single metric with the best performance on average, BEER [\cite=stanojevic-simaan:2014:W14-33].

Sentence level correlations are shown in Table [\ref=3-rst-sent]. From Table [\ref=3-rst-sent], we can see that the performance of DPM is not good and a little lower than BLEU. The reason is that DPM mainly considers the syntactic structure information. After introducing lexical information (F-score), DPMF achieves a significant improvement over DPM and BLEU. DPMF outperforms METEOR on WMT 2012 and is comparable with METEOR on WMT 2013 and WMT 2014. The above results show that DPMF can give an effective evaluation for the hypothesis on sentence level. Compared with the best metric spede07_pP on WMT 2012, DPMF can achieve a comparable correlation.

Conclusion and Future Work

In this paper, we propose a novel dependency-parsing-model-based automatic MT evaluation metric DPMF. DPMF evaluates the syntactic similarity through the score of hypothesis dependency parsing model and evaluates the lexical similarity by unigram F-score. The syntactic similarity method is designed from the view of dependency tree generation, which is totally different from the method of comparing the sub-structures and avoids the defects of defining sub-structures by human. The experiment results show the effectiveness of DPMF on both system level evaluation and sentence level evaluation. DPMF gets the-state-of-art performance on system level on WMT 2012, WMT 2013 and WMT 2014. On sentence level, the performance of DPMF is comparable with METEOR on all of the three data sets.

In future, we will continue our work in two directions. When generating the hypothesis dependency tree, the model is trained only using a limited number of reference sentences (only one reference for WMT corpus), so one direction is that we will enrich the references. The other direction is that we will apply DPMF to the tuning process of statistical machine translation to improve the translation quality.