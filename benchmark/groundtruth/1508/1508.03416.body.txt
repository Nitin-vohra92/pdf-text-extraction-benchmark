Semiparametrically Efficient Estimation of Constrained Euclidean Parameters

Introduction

Let [formula] be i.i.d. copies of X taking values in the measurable space (X,A) in a semiparametric model with Euclidean parameter θ∈Θ where Θ is an open subset of [formula] We denote this semiparametric model by

[formula]

Typically, the nuisance parameter space G is a subset of a Banach or Hilbert space. This space may also be finite dimensional, thus resulting in a parametric model.

We assume an asymptotically efficient estimator [formula] is given of the parameter of interest θ, which under regularity conditions means that

[formula]

holds. Here [formula] is the efficient influence function at Pθ,G for estimation of θ within P and

[formula]

is the corresponding efficient score function at Pθ,G for estimation of θ within P.

The topic of this paper is asymptotically efficient estimation when it is known that θ lies on a general surface, or equivalently, when it is known that θ is determined by a lower dimensional parameter via a continuously differentiable function, which we denote by

[formula]

Here [formula] with d < k is known, N is open, the Jacobian

[formula]

of f is assumed to be of full rank on N, and ν is the unknown d-dimensional parameter to be estimated. Thus, we focus on the (semi)parametric model

[formula]

The first main result of this paper is that a semiparametrically efficient estimator of ν, the parameter of interest, has to be asymptotically linear with efficient score function for estimation of ν equal to

[formula]

Such a semiparametrically efficient estimator of the parameter of interest can be defined in terms of f(  ·  ) and the efficient estimator n of θ; see equation ([\ref=efficientestimatornu]) in Section [\ref=EEPI]. This is our second main result. How ([\ref=scorenu]) is related to the chain rule for differentiation will be explained in Section [\ref=CRSF], which proves this chain rule for score functions. The semiparametric lower bound for estimators of ν is obtained via the Hájek-LeCam Convolution Theorem for regular parametric models and without projection techniques in Section [\ref=CTMR]. In Section [\ref=EEPI] efficient estimators within Q of ν and θ are constructed, as well as efficient estimators of θ under linear restrictions on θ. The generality of our approach facilitates the analysis of numerous statistical models. We discuss some of such parametric and semiparametric models and related literature in Section [\ref=E]. One of the proofs will be given in Appendix [\ref=TP].

The topic of this paper should not be confused with estimation of the parameter θ when it is known to lie in a subset of the original parameter space described by linear inequalities. A comprehensive treatment of such estimation problems may be found in [\cite=vanEeden06]. Our model Q with its constrained Euclidean parameters also differs from the constraint defined models as studied by Bickel et al. (1993, 1998) (henceforth called BKRW), which are defined by restrictions on the distributions in P.

The Chain Rule for Score Functions

The basic building block for the asymptotic theory of semiparametric models as presented in e.g. [\cite=BKRW93] is the concept of regular parametric model. Let [formula] with [formula] open be a parametric model with all Pθ dominated by a σ-finite measure μ on [formula] Denote the density of Pθ with respect to μ by p(θ) = p(  ·  ;θ,PΘ) and the L2(μ)-norm by [formula] If for each θ0∈Θ there exists a k-dimensional column vector [formula] of elements of L2(Pθ0), the so-called score function, such that the Fréchet differentiability

[formula]

holds and the k  ×  k Fisher information matrix

[formula]

is nonsingular, and, moreover, the map [formula] from Θ to Lk2(μ) is continuous, then PΘ is called a regular parametric model. Often the score function may be determined by computing the logarithmic derivative of the density with respect to θ; cf. Proposition 2.1.1 of [\cite=BKRW93]. We will call P from ([\ref=model]) a regular semiparametric model if for all G∈G

[formula]

is a regular parametric model.

Fix θ0∈Θ and G0∈G, and write Pθ0,G0 = P0. Let ψ  :  Θ  →  G with ψ(θ0) = G0 be such that

[formula]

is a regular parametric submodel of P with score function [formula] at θ0 and Fisher information matrix I(θ0,Pψ), say. Let the density of Pθ,ψ(θ) with respect to μ be denoted by q(θ). Since Pψ is a regular parametric model the score function [formula] for θ at θ0 within Pψ satisfies (cf. ([\ref=Frechet]))

[formula]

Considering now the (semi)parametric submodel Q from([\ref=submodel]) we fix ν0 and write f(ν0) = θ0 and f(ν)  =  θ. Within Q the Fréchet differentiability ([\ref=Frechetefficient]) yields

[formula]

and hence

[formula]

in view of the differentiability of f(  ·  ). Since [formula] is continuous, this means that

[formula]

is a regular parametric submodel of Q with score function

[formula]

for ν at P0 and Fisher information matrix

[formula]

We have proved

Let P as in ([\ref=model]) be a regular semiparametric model and let Q as in ([\ref=submodel]) be a regular semiparametric submodel with f(  ·  ) and [formula] defined as in and below ([\ref=flat]) and ([\ref=Jacobian]). If there exists a regular parametric submodel Pψ of P with score function [formula] for θ at θ0 = f(ν0), then there exists a regular parametric submodel Qψ of Q with score function [formula] for ν at ν0 satisfying ([\ref=chainscore]).

This Proposition is also valid for parametric models, as may be seen by choosing G finite dimensional or even degenerate. The basic version of the chain rule for score functions is for such a parametric model PΘ. We have chosen the more elaborate formulation of Proposition [\ref=chaindifferentiability] since we are going to apply the chain rule for such parametric submodels Pψ of semiparametric models P.

Convolution Theorem and Main Result

An estimator [formula] of θ within the regular semiparametric model P is called (locally) regular at P0 = Pθ0,G0 if it is (locally) regular at P0 within Pψ for all regular parametric submodels Pψ of P containing PΘ,G0. According to the Hájek-LeCam Convolution Theorem for regular parametric models (see e.g. Section 2.3 of [\cite=BKRW93]) this implies that such a regular estimator [formula] of θ within P has a limit distribution under P0 that is the convolution of a normal distribution with mean 0 and covariance matrix I- 1(θ0,Pψ) and another distribution, for any regular parametric submodel Pψ containing P0. If there exists ψ  =  ψ0 such that this last distribution is degenerate at 0, we call [formula] (locally) efficient at P0 and Pψ0 a least favorable parametric submodel for estimation of θ within P at P0. Then the Hájek-LeCam Convolution Theorem also implies that [formula] is asymptotically linear in the efficient influence function [formula] satisfying

[formula]

which means

[formula]

The argument above can be extended to the more general situation that there exists a least favorable sequence of parametric submodels indexed by [formula] such that the corresponding score functions [formula] for θ at θ0 within model Pψj converge in Lk2(P0) to [formula] say. A regular estimator [formula] of θ within P is called efficient then, if it is asymptotically linear as in ([\ref=efficientestimatorunderpsi0]) with efficient influence function [formula] satisfying

[formula]

Indeed, by the Convolution Theorem for regular parametric models the convergence

[formula]

holds with the k-vectors Rj and Zj independent and Zj normal with mean 0 and covariance matrix I- 1(θ0,Pψj). Taking limits as j  →    ∞   we see by tightness arguments and by the convergence of [formula] to [formula] in Lk2(P0), that also

[formula]

holds with RP and ZP independent. If RP is degenerate at 0, then [formula] is locally asymptotically efficient at P0 within P and the sequence of regular parametric submodels Pψj is least favorable indeed.

Now, let us assume such a least favorable sequence and efficient estimator [formula] exist at P0 = Pθ0,G0 with θ0 = f(ν0) and f(  ·  ) from ([\ref=flat]) and ([\ref=Jacobian]) continuously differentiable. By the chain rule for score functions from Proposition [\ref=chaindifferentiability] the score function [formula] for ν at ν0 within Qψj satisfies

[formula]

and hence the corresponding influence function [formula] satisfies

[formula]

Let [formula] be a locally regular estimator of ν at P0 within the regular semiparametric model Q. By the convergence of [formula] to [formula] in Lk2(P0), the influence functions from ([\ref=influencefunctionQpsij]) converge in Ld2(P0) to

[formula]

and the argument leading to ([\ref=convolution2]) yields the convergence

[formula]

with RQ and ZQ independent. Note that ZQ has a normal distribution with mean 0 and covariance matrix

[formula]

Under an additional condition on f(  ·  ) we shall construct an estimator [formula] of ν based on [formula] for which RQ is degenerate. This construction of [formula] will be given in the next section together with a proof of its efficiency, and this will complete the proof of our main result formulated as follows.

Let P from ([\ref=model]) be a regular semiparametric model with P0 = Pθ0,G0∈P,θ0 = f(ν0), and f(  ·  ) from ([\ref=flat]) and ([\ref=Jacobian]) continuously differentiable. Furthermore, let f(  ·  ) have an inverse on f(N) that is differentiable with a bounded Jacobian. If there exists a least favorable sequence of regular parametric submodels Pψj and an asymptotically efficient estimator [formula] of θ satisfying ([\ref=convolution2]) with RP = 0 a.s., then there exists a least favorable sequence of regular parametric submodels Qψj of the restricted model Q from ([\ref=submodel]) and an asymptotically efficient estimator [formula] of ν satisfying ([\ref=convolutionnu]) with RQ = 0 a.s. and attaining the asymptotic information bound ([\ref=informationboundnu]).

Note that the convolution result ([\ref=convolutionnu]) and ([\ref=influencefunctionQ]) also holds if the convergent sequence of regular parametric submodels Pψj is not least favorable, and that it implies by the central limit theorem that the limit distribution of [formula] is the convolution of a normal distribution with mean 0 and covariance matrix

[formula]

and the distribution of RQ.

Efficient Estimator of the Parameter of Interest

There are many ways of constructing efficient estimators in (semi)parametric models. One of the common approaches is upgrading a [formula]-consistent estimator as in Sections 2.5 and 7.8 of [\cite=BKRW93]. A somewhat different upgrading approach is used in the following construction.

Consider the situation of Theorem [\ref=mainresult]. If the symmetric positive definite k  ×  k-matrix În is a consistent estimator of I(θ,G,P) within P and [formula] is a [formula]-consistent estimator of ν within Q, then

[formula]

is efficient, i.e., it satisfies ([\ref=convolutionnu]) with RQ = 0 a.s.

Proof The continuity of [formula] and the consistency of [formula] and În imply that

[formula]

converges in probability under P0 to

[formula]

This means that [formula] consistently estimates K0. In view of ([\ref=efficientestimatornu]), ([\ref=influencefunctionQ]), ([\ref=efficientinfluencefunctiongeneral]), and ([\ref=convolution2]) with RP = 0 we obtain

[formula]

By the consistency of [formula] the second term at the right hand side of ([\ref=efficiencyargument]) converges to 0 in probability under P0 in view of the central limit theorem. Because [formula] holds and [formula] equals the d  ×  d identity matrix, the first part of the right hand side of ([\ref=efficiencyargument]) also converges to 0 in probability under P0. [formula]

To complete the proof of Theorem [\ref=mainresult] with the help of Theorem [\ref=onestep] we will construct a [formula]-consistent estimator [formula] of ν and subsequently a consistent estimator În of I(θ,G,P). Let [formula] be a Euclidean norm on [formula] We choose [formula] in such a way that

[formula]

holds. Of course, if the infimum is attained, we choose [formula] as the minimizer. By the triangle inequality and the [formula]-consistency of [formula] we obtain

[formula]

The assumption from Theorem [\ref=mainresult] that f(  ·  ) has an inverse on f(N) that is differentiable with a bounded Jacobian, suffices to conclude that ([\ref=triangle2]) guarantees [formula]-consistency of [formula]

In constructing a consistent estimator of the Fisher information matrix based on the given efficient estimator [formula] we split the sample in blocks as follows. Let [formula] and (mn) be sequences of integers such that [formula] and [formula] hold as n  →    ∞  . For [formula] let [formula] be the efficient estimator of θ based on the observations [formula] and [formula] be the efficient estimator of θ based on the remaining observations [formula] Consider the "empirical" characteristic function

[formula]

which we rewrite as

[formula]

In view of mn / (n  -  kn)  →  0 and ([\ref=convolution2]) with RP = 0 a.s. we see that the first factor at the right hand side of ([\ref=echfrewritten]) converges to 1 as n  →    ∞  . The efficiency of [formula] in ([\ref=convolution2]) with RP = 0 a.s. also implies

[formula]

as n  →    ∞  , with ZP normally distributed with mean 0 and covariance matrix I- 1(θ0,G0,P). Some computation shows

[formula]

It follows by Chebyshev's inequality that φ̃n(t) and hence [formula] converges under P0 = Pθ0,G0 to the characteristic function of ZP at t,

[formula]

For every [formula] we obtain

[formula]

Choosing k(k + 1) / 2 appropriate values of t we may obtain from ([\ref=convergencelogphi]) an estimator of I- 1(θ0,G0,P) and hence of I(θ0,G0,P). Indeed, with t equal to the unit vectors ui we obtain estimators of the diagonal elements of I- 1(θ0,G0,P) and an estimator of its (i,j) element is obtained via

[formula]

When needed, the resulting estimator of I(θ0,G0,P) can be made positive definite by changing appropriate components of it by an asymptotically negligible amount, while the symmetry is maintained.

Under a mild uniform integrability condition it has been shown by [\cite=Klaassen87], that existence of an efficient estimator [formula] of θ in P implies the existence of a consistent and [formula]-unbiased estimator of the efficient influence function [formula] Basing this estimator on one half of the sample and taking the average of this estimated efficient influence function at the observations from the other half of the sample, we could have constructed another estimator of the efficient Fisher information. However, this estimator would have been more involved, and, moreover, it needs this extra uniformity condition.

With the help of Theorem [\ref=onestep], the estimator [formula] of ν from ([\ref=triangle]), and the construction via ([\ref=convergencelogphi]) of an estimator În of the efficient Fisher information we have completed our construction of an efficient estimator [formula] as in ([\ref=efficientestimatornu]) of ν. This estimator can be turned into an efficient estimator of θ  =  f(ν) within the model Q from ([\ref=submodel]) by

[formula]

with efficient influence function

[formula]

and asymptotic information bound

[formula]

Indeed, according to [\cite=BKRW93] Section 2.3, n is efficient for estimation of θ under the additional information θ = f(ν).

If f(  ·  ) is a linear function, i.e., θ  =  Lν  +  α holds with the k  ×  d-matrix L of maximum rank d, then

[formula]

attains the infimum at the right hand side of ([\ref=triangle]). So, the estimator [\eqref=efficientestimatornu] becomes

[formula]

with efficient influence function ([\ref=influencefunctionQ]) and asymptotic information bound ([\ref=informationboundnu]) with [formula] and the estimator from ([\ref=efficientestimatorfnu])

[formula]

Note that [formula] is the projection of [formula] on the flat [formula] under the inner product determined by În (cf. Appendix [\ref=TP]) and that the covariance matrix of its limit distribution equals the asymptotic information bound

[formula]

Another way to describe this submodel Q with θ = Lν  +  α is by linear restrictions

[formula]

where RTα  =  β holds and the k  ×  d-matrix L and the k  ×  (k - d)-matrix R are matching such that the columns of L are orthogonal to those of R and the k  ×  k-matrix (L  R) is of rank k. Note that the open subset N of [formula] determines the open subset Θ of [formula] and vice versa. See [\cite=Cobb28], [\cite=Stone54], [\cite=Nyquist91], and [\cite=Kim95] for some examples of estimation under linear restrictions.

In terms of the restrictions described by R and β the efficient estimator [formula] of θ from([\ref=efficientestimatorfnuforlinear]) within the submodel Q can be rewritten as

[formula]

with asymptotic information bound

[formula]

as will be proved in Appendix [\ref=TP].

Examples

In this section we present five examples, which illustrate our construction of (semi)parametrically efficient estimators. We shall discuss location-scale, Gaussian copula, and semiparametric regression models, and parametric models under linear restrictions.

Additional Proofs

In this appendix proofs will be presented of ([\ref=efficientestimatorfnurestrictions]) and ([\ref=informationboundrestriction]).

Since În has been chosen to be symmetric and positive definite, [formula] is an inner product on [formula] Define the k  ×  k-matrices Πn,L and Πn,R by

[formula]

With the above inner product these matrices are projection matrices on the linear subspaces spanned by the columns of L and Î- 1nR, respectively. Indeed, [formula] and [formula] hold. The linear subspaces spanned by the columns of L and Î- 1nR have dimensions d and k - d, respectively, since the matrices (L,R) and În are nonsingular. Moreover, these linear subspaces are orthogonal in view of LTÎnÎ- 1nR  =  LTR = 0. This implies

[formula]

Combining ([\ref=projectionmatrices]), ([\ref=orthogonalprojections]), and ([\ref=efficientestimatorfnuforlinear]) we obtain ([\ref=efficientestimatorfnurestrictions]) and, by the consistency of În, ([\ref=informationboundrestriction]).

Acknowledgements

We would like to thank Raymond Veldhuis for inspiring us to study problems with structured correlation matrices, which triggered the reported research, and Constance van Eeden for references.

Supplementary Material

Computations needed for ([\ref=covarianceinfluencefunctions]) and ([\ref=inversematrix]) are collected as supplementary material.

Supplementary Material For "Semiparametrically Efficient Estimation of Constrained Euclidean Parameters"

In this supplement we present the computational details for [\eqref=covarianceinfluencefunctions] and [\eqref=inversematrix] presented in Example [\ref=Gaussiancopula]. Since our computations will be based on fourth moments of multivariate normal random variables, we consider The following fourth moments of Z can be obtained by straightforward computations:

E(Z4a) = 3

E(Z3aZb) = 3ρab

E(Z2aZ2b) = 1 + 2ρ2ab

E(Z2aZbZc) = ρbc + 2ρabρac

E(ZaZbZcZd) = ρabρcd  +  ρacρbd  +  ρadρbc.

For every [formula] let Mij be the element in the i-th row and j-th column of the efficient lower bound I- 1(θ,G,P). Because of θi  =  ρab,  θj  =  ρcd for some a,b,c, and d, we have We have three cases:

[formula]

[formula]

[formula] (without lost of generality assume d = a)

[formula]

[formula]

[formula]

Finally, substitution of the correlation structures in Subexample [\ref=exchangeable] and Subexample [\ref=circular] give [\eqref=covarianceinfluencefunctions] and [\eqref=inversematrix], respectively.