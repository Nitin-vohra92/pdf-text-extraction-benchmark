Introduction

Cache plays an important role in bridging the gap between the speed of processor and main memory. Many cache architectures have been proposed in history. Hash is an important technique to improve cache performance, such as hash-rehash cache. As the succeeding generation to Nehalem, one of Sandy Bridge processor's new features is that LLC is divided into several slices which are connected by a ring bus, as shown in Figure [\ref=fig:sixslices]. And the location of a given data block on LLC is decided by an undocumented hash function.

This article proposes a novel method using HMTT [\cite=bao2008hmtt] to crack the hash function and further verifies the correctness of the cracking result based on the phenomenon that when the number of accessed data blocks that are mapped to the same cache set exceeds the associativity of cache set, average access latency increases sharply. Compared to the statement that page coloring doesn't work on caches that are indexed using hashing [\cite=sanchez2012scalable], this articles ported User Level Cache Control, which is a software runtime library used to improve cache performance by implementing cache partition based on page coloring, and proves hat it is possible to implement cache partition using page coloring on Intel Sandy Bridge processors.

This articles has the following contributions:

This article is organized as follows: Section 2 defines the problem. Section 3 describes the procedure to crack cache hash function. Section 4 presents the observations based on which this article comes up with the assumption about the implementation of the hash function. Section 5 describes the assumptions about the implementations of the hash function. Section 6 presents the details of the cracking scheme. Section 7 presents the cracking results. Section 8 describes the details to verify the correctness of the cracking result. Section 9 describes the results of performance of cache partition implemented based on page coloring. Section 10 summarizes the contributions.

Problem

All the physical memory is divided into data blocks, and the size of data block is the same with the size of cache line.

A mapping function exists between data blocks and cache sets. For an unknown processor P, let Ccache be the capacity of the cache, let Cmemory be the capacity of the memory installed, let Ccacheline be the size of cache line, let associativity be the associativity of the cache, and the total number of data block is

[formula]

the number of cache sets is

[formula]

On Sandy Bridge processor, LLC is also set-associative. What's different is that one hash function exists to distribute all data blocks across cache slices. Let location is the location on cache where a given pa is stored; as LLC is divided into slices, location consists of two parts, slice_id and set_index. slice_id is the LLC slice the data block is mapped to, and set_index is the index of the set the physical address is mapped to.

Using these notations, this article defines the hash function on Sandy Bridge processor as follows: given a physical address pa, slice_id and set_index, this article defines two functions [formula] and [formula] to describe the relationship

[formula]

[formula]

Procedure

Figure [\ref=fig:TestProcedure] presents the procedure to crack the cache hash function on Intel Sandy Bridge processor. Based on some observations in prior work and some experiments, this article first presents a hypothesis. Then this article proves the hypothesis to be correct.

This article proposes a logical model to describe the problem clearly. As presented in [\cite=Multi-core-cache_rajeev], physical cache has complex organization. The method discussed in this article isn't able to distinguish between the following situations as presented in figure [\ref=fig:PhysicalMappingScheme]. In order to describe the problem clearly, this article proposes a logical model of cache organization, as presented in figure [\ref=fig:LogicalModel]. In this model, each slice is divided into different cache sets, the number of cache sets on each cache slice can be decided based on the capacity of each cache slice, the size of cache line and associativity of cache set. Each cache slice consists of the same number of cache sets. One exact cache set is selected by specifying slice_id and set_index. Besides, without specifying slice_id , the cache sets with the same set_index on each cache slice will be selected.

Observation

Main memory and LLC access latency

When the data accessed can't be held in cache, cache miss will cause average access latency to increase.

Substring of physical address serves as set index

As described in section methodology, the test program is able to access data blocks in specified cache sets. This article first accesses the physical memory with different stride, and the size of physical memory and the stride is recorded. Besides, when the number of accessed data blocks exceeds the number of the selected cache sets, serious conflict will cause average access latency to increase sharply.

As presented in figure [\ref=fig:SetIndexBits], average access latency increases sharply at one point. This point represents the configuration of the test, including array size and stride. With each configuration, the number of data blocks accessed is decided by array size and stride. As shown in figure [\ref=tab:SetIndexBitsSingularity], when the stride is larger than 32KB, the number of cache lines LLC can hold equals to 120. Chances are that some bits in physical address serves as set index during cache access. When the stride is large enough, the set index will remain unchanged for the data blocks accessed. Considering the fact that the associativity of the Sandy Bridge processor in this test is 20, and has 6  Ã—  20  =  120 cache sets.

The hash function meets some properties

As presented in [\cite=seznec1993case], the set-associative organization cache should meet the following properties to provide better performance.

Assumption

Let nslice be the total number of slices and nset be the number of sets on each slice, [formula] be the binary representation of the physical address. As presented in figure [\ref=fig:SplitPhysicalAddress], this article splits the binary representation of an address pa into bit substrings [formula], A0 is a c bit string: the displacement in the line. A1 is a n_slice bit string and A2 is the string of the most significant bits. Based on the work presented in [\cite=sandybridgehash], this article makes the following assumptions:

the value of A0 is used as block address;

the value of A1 is used as set index on a specified cache slice;

A2 is used to decide the slice_id of a given pa;

Methodology

This section discusses the following three questions:

Platform

Table [\ref=table:ServerConfiguration] presents the parameters of the processors used in this article.

Classifying criteria

Evicting relationship exists between data block s that are mapped to the same cache set. When the number of accessed data blocks exceeds the associativity of cache set, cache conflict occur, these data block will begin to evict each other, as presented in figure [\ref=fig:New09CacheReplacementHmttTrace]. This article uses this evicting relationship to classify data block into different groups.

Getting the evicting relationship between data block

Accessing data in desired cache set

In order to get the evicting relationship, the testing program should be able to fill data into specified cache set. We boot the operating system with 2GB memory. In this way, the OS can only use the lower 2GB memory. This article further implement a driver to map the other physical memory ranging from 2GB to 3GB into kernel space. In this way, the physical address accessed can be calculated by subtracting based address from the linear address of the operating system.

Test sequence generation

As presented in table [\ref=table:3bitscombination], the physical address is generated by bit combination. In this way, this article verifies the effect of every bit on the result of hash function.

Array initialization

The testing program firstly allocates an array. Then the testing program initializes the test array in a data-dependent manner, which means that the data stored at the current read address is the address of the next read command. As shown in Figure [\ref=fig:cyclicaccess], the main operation of the test program is to read data and use the data as the address of the following read operation.

Ensure the correctness of evicting relationship

As depicted in code snippet [\ref=snippetwithmakedirty], idle loop is inserted between adjacent memory accesses to make sure that only two physical addresses with eviction relationship exists between each other are temporally adjacent.

Collecting memory reference trace

HMTT is a hybrid hardware/software memory trace monitoring system. This tool collects all the memory reference trace, which memory controller issues to memory modules. Although the tool can be programmed to collect various information [\cite=chen2014cmd], the information needed in this article includes physical address, time interval between different consecutive physical addresses and read/write bit of the physical address, as presented in table [\ref=table:SampleMemoryReferenceTrace]. The whole process is presented in figure [\ref=fig:New09CacheReplacementHmttTrace]. When the cache set is full and testing program reads another data block into cache, one of the data blocks already read into cache needed to be evicted to make room for the newly read data block. The two operations are collected by HMTT and save as two memory reference trace. The trace consists of information about the address of the operation and whether the operation is read or write.

Classify all the physical address into different groups

As presented in figure [\ref=fig:ConnectedGraphNodeEdge], if evicting relationship exists between physical address A and B, and also exists between physical address B and C, then it is concluded that evicting relationship also exists between physical address B and C. This article further classifies all the physical addresses into different groups based on connected subgraph related method.

Results

This article presents the mapping relationship in the form of mapping table. It's true with both Sandy Bridge 4 core and 6 core processor that bit string A1 in physical address selects cache set directly. For both processor, there are nset cache sets per cache slice. The number of data blocks that are mapped to cache set with the same set index is

[formula]

Let Cmemory be the installed memory, let Ccapacity the size of data block, and let nblock be total number of data blocks, due to the fact that, as presented in figure [\ref=fig:SplitInto4Simplication], substring A1 in physical address is used to select cache set, for those data blocks those share the same set index, they will be mapped to cache sets on all cache slice sharing the same cache set index, the total number of cache slice is nslice. For those blocks sharing the same set index, this article uses a mapping table to describe the relationship.

4 core processor

On Sandy Bridge 4 core processor, the installed memory is 16GB, so the total data block is

[formula]

The number of cache sets on each cache slice is 2048, the number of data block that share the same cache set index is

[formula]

These data blocks will be mapped to these selected cache sets. As presented in table [\ref=tab:4CoreHashTable], as all the blocks share the same set index, this article only presents the A2 string of each block address here. Each set index corresponds to a mapping table. This article finds that on Intel 4 core processor, the mapping tables corresponds to different set index are the same.

The installed physical memory is 16GB, so we have the mapping table of 34 bit width physical address. Because bit string A1 in physical address selects cache set directly, there are 2048 cache sets on each cache slice. As a result, there should have been 2048 mapping table to describe the relationship. However, this article finds that the mapping table is the same for all set index.

Reduction of Sandy Bridge processor mapping table. The mapping table can be reduced to a simple formula. As presented in figure [\ref=fig:DecideBlockLocation], two intermediate value [\ref=fig:TwoIntermediateValue], bit_a0, bit_a1, these four value is related to four different cache slices. The value of bit16bit15 is used to select one set from from four cache sets selected by set index of the data block.

6 core processor

On Sandy Bridge 6 core processor, the installed memory is 64GB, so the total data block is

[formula]

Because there are 2048 cache sets on each slice, the number of data block sharing the same cache set index is

[formula]

However, on 6 core processor, cache set with different cache set index might have different mapping table. There are 2048 cache set on each set index. As a result, there exist 2048 mapping tables corresponding to 2048 set indexes. After further analysis, this article has tested every set index with 1GB physical memory (30 bits physical address). As presented in table [\ref=tab:32MappingTable], the result shows that the mapping table of some set indexes are the same. Set index 0, 2, 65, 67 share the same mapping table. And There are 32 different mapping table. Set index ranging from 0 to 2047 fall into 32 mapping tables.

Higher physical address bits also affect the result of hash function. On Sandy Bridge 6 core processor, this article has verified each set index with 1GB physical memory. Let [formula] be the binary representation of physical address, with 1GB physical memory tested, this article gets the result of bits [formula]. When it comes to the higher address bits [formula], this article verifies with 64GB(36 bits physical address) physical memory. The result shows the higher bits also affects the result of hash function. However, the data blocks sharing the same set index can still be split into 6 groups. This means that substring A2 is used to select the slice_id of a given pa.

Verification of the correctness of the cracking result of Sandy Bridge cache hash function

Object of correctness verification

This article proposes a method to verify the correctness of the cracking result. The cracked function will map different data blocks to different cache sets. This article verifies the correctness of the cracking result by checking that the data blocks that are indicated by the cracking result to be in one cache set are truly in one cache set.

As presented in figure [\ref=fig:SingleThreadPolluteNopollute], average access latency increases sharply when the number of data blocks accessed exceeds the associativity of LLC. This is relatively obvious. This article finds that performing another write operation after the data block is read into cache will affect cache replacement policy. As presented in figure [\ref=fig:SingleThreadPolluteNopollute], polluting the cache line means that adding another write operation, and do not pollute the cache line means perform only dependent read operation. It can be seen from the figure that performing another read operation causes the average access latency to increase slowly compared to the read operation only configuration. One possible explanation is as shown figure [\ref=fig:New08CacheReplacementPolicy], when a new cache line arrives, if the newly accessed data block is inserted into the MRU position, when the number of data blocks accessed exceeds the associativity of LLC, even if only exceeds by one, the average access latency will increase sharply. The method is able to check every cache set.

Verify the correctness of the cracking result using two threads. In this verifying scenario, use two threads to perform data dependent access. The average access latency is recorded for each configuration. In the first configuration, the data blocks accessed by thread 1 and the data blocks accessed by thread 2 are mapped to different cache sets, the results is presented in figure [\ref=fig:TwoThreadsDifferentSets]. In the second configuration, the data blocks accessed by thread 1 and the data blocks accessed by thread 2 are mapped to different cache sets, the results is presented in figure [\ref=fig:TwoThreadsSameSets]. For both configurations, the number of data blocks accessed by thread 2 vary from 1 to 40. And the number of data block accessed by thread 1 varies from 1 to 4. When the number of data blocks accessed by thread 1 exceeds the associativity, average latency will increase sharply. When thread 1 performs data dependent access on 1 data block, in the first configuration, as the data blocks accessed by two threads are mapped to the same cache set and thread 1 accessed 1 data block, average access latency of thread 2 increases sharply when the number of accessed data blocks of thread 2 is 18; On the contrary, when the data blocks accessed by two threads are mapped to different cache sets, average access latency of thread 2 increases sharply when the number of accessed data blocks of thread 2 is 18. This article gets similar results when the number of data blocks access by thread1 varies from 1 to 4.

Write operation has effect on cache replacement policy. In the following test, the program performs data dependent access, two configuration of the program is as follows:

Let Laverage be the average access latency, Lmemory be the access latency of memory, LLLC be the access latency of LLC, N be the associativity of LLC, n be the number of data blocks that are accessed sequentially by the testing program, then the relationship between average access latency and the number of data blocks accessed can be describes as:

[formula]

Memory reference trace collected with HMTT offers a different prospective on cache replacement policy. This article further analyzes the trace: cut a segment of the whole trace, and count how many times each data block is accessed in this segment of trace (or this period of execution). As presented in figure [\ref=fig:pollute_nopollute], without polluting the cache line, for simplicity, label each data block accessed with an index, the access number of each data block is uniform. However, when perform an extra write operation to pollute the cache line, the access number of different data blocks becomes non-uniform, which means that some of the data blocks are held in cache longer than the other part of data blocks. This fact reveals the fact that write operation affects the cache replacement policy.

ulcc

User Level Cache Control (ULCC) [\cite=ding2011ulcc] is a software package to implement cache partition using page coloring. It improves performance of multi-threaded program by enforcing a user demand cache capacity allocation. By modifying the macro that extracts page color from physical address, this article ported ULCC to Intel Sandy processor.

MergeSort

MergeSort is implemented in multiple threads. During the execution of program, the intermediate result of every block is highly reused. ULCC improves the performance by allocating different cache capacity for data of different reuse degree.

The performance of the program with and without ULCC support is depicted in figure [\ref=fig:ULCCSortBlockTime]. When choosing size of sorting block properly, the execution time is reduced by 20%. The result is using one thread to finish merge sort. So the performance gain is only from preventing cache pollution of data in one thread.

MatMul

The MatMul program multiplies two double precision matrices A and B, and produces the product matrix C. To achieve necessary data reuse in LLC, the matrix multiplication is carried out block by block. For the block a on the ith block row and jth block column of matrix A, it is multiplied with all the blocks on the jth block row of matrix B, and the results are accumulated into the blocks on the ith block row of matrix C. So the data in matrix A is of high reuse degree and before the program finishes the computation with block a, it is desirable that the data in a can be kept in the cache. However, without a dedicated space for block a, the data in it may be repeatedly evicted from the cache before its next use every time the program switches blocks in matrix B and matrix C, even with a rather small block size. To reduce the chance that the data in each block of matrix A is evicted from the last level cache prematurely, the size of sorting block should has a most suitable size.

The performance of the program with and without ULCC support is depicted in figure [\ref=fig:MMBlockTime]. When choose data block element properly to make sure that the frequently used data can be held in cache, this article achieves the same performance improvement as presented in [\cite=ding2011ulcc]. This further proves the correctness of the cracking results.

General test

This article proposed a method to crack hash function without the support of HMTT. The core of idea is that when the number of accessed addresses exceeds the associativity of cache set, the average access latency will increase sharply.

The main procedures of the cracking method without support of HMTT is as follows:

The problem with this method is that it takes too long to finish the test.

Conclusions

On Intel Sandy Bridge processor, last level cache (LLC) is divided into cache slices and all physical addresses are distributed across all cache slices using a hash function. With this undocumented hash function existing, it is impossible to implement cache partition based on page coloring.

This article cracks the hash function on two types of Intel Sandy Bridge processors. It's true on both 4 core and 6 core processors that bit substring of physical address is used to select cache sets. What is different is that: on Intel Sandy 4 core processor, the mapping relationship for different cache set indexes is the same. And on Intel Sandy Bridge 4 core processor, the cracked hash function is reduced to a simple formula. On the contrary, on Intel Sandy Bridge 6 core processor, different cache set indexes have different mapping relationship. The article has not reduced the hash function to a simple formula. Instead, the hash function is presented in the form of mapping tables.

This article proves that it's possible to implement cache partition based on page coloring. On 4 core processor, based on the cracking result, it's easy to implement cache partition based on page coloring. On 6 core processor, without reducing the hash function to a simple formula, cache partition can at least be implemented based on set index, as bit substring of physical address is used to select cache sets.