A Potential Reduction Algorithm for Two-person Zero-sum Mean Payoff Stochastic Games

This research was partially supported by DIMACS, Center for Discrete Mathematics and Theoretical Computer Science, Rutgers University, and by the Scientific Grant-in-Aid from Ministry of Education, Science, Sports and Culture of Japan. Part of this research was done at the Mathematisches Forschungsinstitut Oberwolfach during stays within the Research in Pairs Program. The first author also acknowledges the partial support of NSF Grant IIS-1161476.

An extended abstract of this paper was published in the proceedings of the Combinatorial Optimization and Applications, 2014 [\cite=BEGM14].

Khaled Elbassioni

Vladimir Gurvich Kazuhisa Makino

keywords: undiscounted stochastic games, limiting average payoff, mean payoff, local reward, potential transformation, computational game theory

Introduction

Basic Concepts and Notation

Stochastic games were introduced in 1953 by Shapley [\cite=Sha53] for the discounted case, and extended to the undiscounted case by Gillette [\cite=Gil57]. Each such game [formula], [formula] is played by two players on a finite set V of vertices (states, or positions); Kv and Lv for v∈V are finite sets of actions (pure strategies) of the two players; [formula] is the transition probability from state v to state u if players chose actions k∈Kv and [formula] at state v∈V; and [formula] is the reward player 1 (the maximizer) receives from player 2 (the minimizer), correpsonding to this transition. We assume that the game is non-stopping, that is, [formula] for all v∈V and [formula]. To simplify later expressions, let us denote by Pvu∈[0,1]Kv  ×  Lv the transition matrix, the elements of which are the probabilities [formula], and associate in Γ a local expected reward matrix Av to every v∈V defined by

[formula]

In the game Γ, players first agree on an initial vertex v0∈V to start. Then, in a general step j = 0,1,..., when the game arrives to state vj = v∈V, they choose mixed strategies [formula] and βv∈Δ(Lv), player 1 receives the amount of bj  =  αvAvβv from player 2, and the game moves to the next state u chosen according to the transition probabilities pvuα,β ~  =  ~ αvPvuβv.

The undiscounted limiting average (effective) payoff is the Cesaro average

[formula]

where the expectation is taken over all random choices made (according to mixed strategies and transition probabilities) up to step j of the play. The purpose of player 1 is to maximize gv0(Γ), while player 2 would like to minimize it.

In 1981, Mertens and Neymann in their seminal paper [\cite=MN81] proved that every stochastic game has a value from any initial position in terms of history dependent strategies. An example (the so-called Big Match) showing that the same does not hold when restricted to stationary strategies was given in 1957 in Gillette's paper [\cite=Gil57]; see also [\cite=BF68].

In this paper we shall restrict ourselves (and the players) to the so-called stationary strategies, that is, the mixed strategy chosen in a position v∈V can depend only on v but not on the preceding positions or moves before reaching v (i.e., not on the history of the play). We will denote by K(Γ) and L(Γ) the sets of stationary strategies of  and , respectively, that is,

Vrieze (1980) showed that if a stochastic game Γ has a value gv0(Γ) = m, which is a constant, independent of the initial state v0∈V, then it has a value in ε-optimal stationary strategies for any ε > 0. We call such games ergodic and extend their definition as follows.

For ε > 0, a stochastic game Γ is said to be ε-ergodic if the game values from any two initial positions differ by at most ε, that is, |gv(Γ) - gu(Γ)|  ≤  ε, for all u,v∈V. A 0-ergodic game will be simply called ergodic.

Our main result in this paper is an algorithm that decides, for any given stochastic game Γ and ε > 0, whether or not Γ is ε-ergodic, and provides a witness for its ε-ergodicity/non-ergodicity. As a corollary, we get a constructive proof of the above mentioned theorem of Vrieze [\cite=V80]. A notion central to our algorithm is the concept of a potential transformation introduced in the following section.

Potential transformations

In 1958 Gallai [\cite=Gal58] suggested the following simple transformation. Let [formula] be a mapping that assigns to each state v∈V a real number xv called the potential of v. For every transition (v,u) and pair of actions k∈Kv and [formula] let us transform the payoff [formula] as follows:

[formula]

Then the one step expected payoff amount changes to [formula], where vj∈V is the (random) position reached at step j of the play. However, as the sum of these expectations telescopes, the limiting average payoff remains the same for all finite potentials:

[formula]

Thus, the transformed game remains equivalent with the original one.

Using potential transformations we may be able to obtain a proof for ergodicity/non-ergodicity. This is made more precise in the following section.

mv is the value of the matrix game Av at state v.

Local and Global Values and Concepts of Ergodicity

Let us consider an arbitrary potential [formula], and define the local value mv(x) at position v∈V as the value of the |Kv|  ×  |Lv| local reward matrix game Av(x) with entries

[formula]

that is,

[formula]

To a pair of stationary strategies α = (αv|v∈V)∈K(Γ) and β = (βv|v∈V)∈L(Γ) we associate a Markov chain Mα,β(Γ) on states in V, defined by the transition probabilities pvuα,β ~  =  ~ αvPvuβv. Then, this Markov chain has unique limiting probability distributions (qvuα,β| u∈V), where qvuα,β is the probability of staying in state u∈V when the initial vertex is v∈V. With this notation, The limiting average payoff ([\ref=mean]) starting from vertex v∈V can be computed as

[formula]

The game is said to be to be solvable in uniformly optimal stationary strategies, if there exist stationary strategies ∈K(Γ) and ∈L(Γ), such that for all initial states v∈V

[formula]

This common quantity, if exists, is the value of the game with initial position v∈V, and will be simply denoted by gv = gv(Γ).

Main Result

Given an undiscounted zero-sum stochastic game, we try to reduce the range of its local values by a potential transformation [formula]. If they are equalized by some potential x, that is, mv(x)  =  m is a constant for all v∈V, we say that the game is brought to its ergodic canonical form [\cite=DGAA13]. In this case, one can show that the values gv exist and are equal to m for all initial positions v∈V, and furthermore, locally optimal strategies are globally optimal [\cite=DGAA13]. Thus, the game is solved in uniformly optimal strategies. However, typically we are not that lucky.

To state our main theorem, we need more notation.

W > 0 is smallest integer s.t. either [formula] or [formula]

R is the smallest real s.t.

[formula]

N =  max v∈V{ max {|Kv|,|Lv|}}.

n = |V|

η  =   max { log 2R, log 2W} (maximum "bit length")

For every stochastic game and ε > 0 we can find in [formula] time either a potential vector [formula] proving that the game is (24ε)-ergodic, or stationary strategies for the players proving that it is not ε-ergodic.

The proof of Theorem [\ref=t-main] will be given in Section [\ref=sg]. One major hurdle that we face is that the range of potentials can grow doubly exponentially as iterations proceed, leading to much worse bounds than those stated in the theorem. To deal with this issue, we use quantifier elimination techniques [\cite=BPR96] [\cite=GV88] [\cite=R92] to reduce the range of potentials after each iteration; see the discussion preceding Lemma [\ref=t1].

Related Work

The above definition of ergodicity follows Moulin's concept of the ergodic extension of a matrix game [\cite=Mou76] (which is a very special example of a stochastic game with perfect information). Let us note that slightly different terminology is used in the Markov chain theory; see, for example, [\cite=KS63].

The following four algorithms for undiscounted stochastic games are based on stronger "ergodicity type" conditions: the strategy iteration algorithm by Hoffman and Karp [\cite=HK66] requires that for any pair of stationary strategies of the two players the obtained Markov chain has to be irreducible; two value iteration algorithms by Federgruen are based on similar but slightly weaker requirements; see [\cite=F80] for the definitions and more details; the recent algorithm of Chatterjee and Ibsen-Jensen [\cite=CI14] assumes a weaker requirement than the strong ergodicity required by Hoffman and Karp [\cite=HK66]: they call a stochastic game almost surely ergodic if for any pair of (not necessarily stationary) strategies of the two players, and any starting position, some strongly ergodic class (in the sense of [\cite=HK66]) is reached with probability 1.

While these restrictions apply to the structure of the game, our ergodicity definition only restricts the value. Moreover, the results in [\cite=HK66] and [\cite=CI14] apply to a game that already satisfies the ergodicity assumption, which seems to be hard to check. Our algorithm, on the other hand, always produces an answer, regardless whether the game is ergodic or not.

Interestingly, potentials appear in [\cite=F80] implicitly, as the differences of local values of positions, as well as in [\cite=HK66], as the dual variables to linear programs corresponding to the controlled Markov processes, which appear when a player optimizes his strategy against a given strategy of the opponent. Yet, the potential transformation is not considered explicitly in these papers.

We prove Theorem [\ref=t-main] by an algorithm that extends the approach recently obtained for ergodic stochastic games with perfect information [\cite=IPCO-2010] and extended to the general (not necessarily ergodic) case in [\cite=BEGM-ICALP13]. This approach is also somewhat similar to the first of two value iteration algorithms suggested by Federgruen in [\cite=F80], though our approach has some distinct characteristics: It is assumed in [\cite=F80] that the values gv exist and are equal for all v; in particular, this assumption implies the ε-ergodicity for every ε  >  0. For our approach we do not need such an assumption. We can verify ε-ergodicity for an arbitrary given ε  >  0, or provide a proof for non-ergodicity (with a small gap) in a finite time. Moreover, while the approach of [\cite=F80] was only shown to converge, we provide a bound in terms of the input parameters for the number of steps.

Several other algorithms for solving undiscounted zero-sum stochastic games in stationary strategies are surveyed by Raghavan and Filar; see Sections 4 (B) and 5 in [\cite=RF91]. The only algorithmic results that we are aware of that provide bounds on the running time for approximating the value of general (undiscounted) stochastic games are those given in [\cite=CMH08] [\cite=HKLMT11]: in [\cite=CMH08], the authors provide an algorithm that approximates, within any factor of ε > 0, the value of any stochastic game (in history dependent strategies) in time [formula]. In [\cite=HKLMT11], the authors give algorithms for discounted and recursive stochastic games that run in time [formula], and claim also that similar bounds can be obtained for general stochastic games, by reducing them to the discounted version using a discount factor of δ  =  εηNO(n2) (and this bound on δ is almost tight [\cite=M11]). These results are based on quantifier elimination techniques and yield very complicated history-dependent strategies. For almost sure ergodic games, a variant of the algorithm of Hoffman and Karp [\cite=HK66] was given in [\cite=CI14]; this algorithm finds ε-optimal stationary strategies in time (roughly) [formula]. This result is not comparable to ours, since the class of games they deal with are somewhat different (although both generalize the class of strongly ergodic games of [\cite=HK66]). Furthermore, the algorithm in Theorem [\ref=t-main] exhibits the additional feature that it either provides a solution in stationary strategies in the ergodic case, if one exists, or produces a pair of stationary strategies that witness the non-ergodicity.

Pumping Algorithm

We begin by describing our procedure on an abstract level. Then we specialize it to stochastic games in Section [\ref=sg].

Given a subset S  ⊆  V, let us denote by eS∈{0,1}V the characteristic vector of S.

Let us further assume that mv(x) for v∈V are functions depending on potentials [formula] (where n = |V|) and satisfying the following properties for all subsets S  ⊆  V and reals δ  ≥  0:

mv(x - δeS) is a monotone decreasing function of δ if v∈S;

mv(x - δeS) is a monotone increasing function of δ if [formula];

|mv(x) - mv(x - δeS)|  ≤  δ for all v∈V.

We show in this section that under the above conditions we can change iteratively the potentials to some [formula] such that either all values mv(x'), v∈V, are very close to one another or we can find a decomposition of the states V into disjoint subsets proving that such convergence of the values is not possible.

Our main procedure is described in Algorithm [\ref=algo1] below. Given the current vector of potentials xτ at iteration τ, the procedure partitions the set of vertices into four sets according to the local value mv(x). If either the first (top) set Tτ or forth (bottom) set Bτ is empty, the procedure terminates; otherwise, the potentials of all the vertices in the first and second sets are reduced by the same amount δ, and the computation proceeds to the next iteration.

We can show next that properties (i), (ii) and (iii) above guarantee some simple properties for the above procedure.

We have Tτ + 1  ⊆  Tτ, Bτ + 1  ⊆  Bτ and Mτ + 1  ⊇  Mτ for all iterations [formula]

Proof   Indeed, by (i) and (iii) we can conclude that mv(xτ)  ≥  m-  +  δ holds for all v∈Pτ. Analogously, by (ii) and (iii) mv(xτ) < m- + 3δ follows for all [formula]. [formula]

Either [formula] or [formula] for some finite τ, or there are nonempty disjoint subsets I,F  ⊆  S, I  ⊇  Tτ, F  ⊇  Bτ, and a threshold τ0, such that for every real Δ  ≥  0 there exists a finite index τ(Δ)  ≥  τ0 such that

mv(xτ)  ≥  m- + 2δ for all v∈I and mv(xτ) < m- + 2δ for all v∈F, and for all τ  ≥  τ0;

xuτ - xvτ  ≥  Δ for all v∈I and [formula], and for all τ  ≥  τ(Δ);

xvτ - xuτ  ≥  Δ for all v∈F and [formula], and for all τ  ≥  τ(Δ).

Proof   By Lemma [\ref=l1] sets Tτ and Bτ can change only monotonically, and hence only at most |S| times. Thus, if Pump(x,S) does not stop in a finite number of iterations, then after a finite number of iterations the sets Tτ and Bτ will never change and all positions in Tτ remain always pumped (that is, have their potentials reduced), while all positions in Bτ will be never pumped again.

Assuming now that the pumping algorithm Pump(x,S) does not terminate, let us define the subset I  ⊆  S as the set of all those positions which are always pumped with the exception of a finite number of iterations. Analogously, let F be the subset of all those positions that are never pumped with the exception of a finite number of iterations. Since I and F are finite sets, there must exist a finite τ0 such that for all τ  ≥  τ0 we have I  ⊆  Pτ and [formula], implying (a). Note that any vertex in Tτ is always pumped by (iii) and hence Tτ  ⊆  I for any τ  ≥  τ0; similarly, Bτ  ⊆  F for any τ  ≥  τ0.

Let us next observe that all positions not in [formula] are both pumped and not pumped infinitely many times. Thus, since δ is a fixed constant, for every Δ there must exist an iteration τ(Δ)  ≥  τ0 such that all positions not in I are not pumped by at least Δ  /  δ many more times than those in I, and all positions not in F are pumped by at least Δ  /  δ many more times than those in F, implying (b) and (c). [formula]

Let us next describe the use of Pump(x,S) for repeatedly shrinking the range of the mv values, or to produce some evidence that this is not possible. A simplest version is the following:

Note that by our above analysis, RepeatedPumping either returns a potential transformation for which all mv, v∈V values are within an ε-band, or returns the sets I and F as in Lemma [\ref=l2] with arbitrary large potential differences from the other positions. In the next section we use a modification of these procedures for stochastic games, and show that those large potential differences can be used to prove that the game is not ε-ergodic.

Application of Pumping for Stochastic Games

We show in this section how to use RepeatedPumping to find potential transformations verifying ε-ergodicity, or proving that the game is not ε-ergodic, thus establishing a proof of Theorem [\ref=t-main]. Towards this end, we shall give some necessary and sufficient conditions for ε-non-ergodicity, and consider a modified version of the pumping algorithm described in the previous section which will provide a constructive proof for the above theorem.

Let us first observe that the local value function of stochastic games satisfies the properties required to run the pumping algorithm described in the previous section.

For every subset S  ⊆  V and δ  ≥  0 and for all v∈V we have

[formula]

Furthermore, the value functions mv(x) for v∈V satisfy properties (i), (ii) and (iii) stated in Section [\ref=pump].

Proof   According to [\eqref=e99] we must have for all δ  ≥  0 that Av(x)  ≥  Av(x - δeS) for all v∈S and Av(x)  ≤  Av(x - δeS) for all [formula] proving properties (i) and (ii) (Indeed, [formula] for v∈S and [formula] for [formula], where Ev is the |Kv|  ×  |Lv|-matrix of all ones. Since the operator [formula] is monotone increasing in B, inequalities ([\ref=e999]) follow). Property (iii) follows directly from ([\ref=e999]). [formula]

The above lemma implies that procedures Pump and RepeatedPumping could, in principle, be used to find a potential transformation yielding an ε-ergodic solution. It does not offer, however, a way to discover ε-non-ergodicity. Towards this end, we need to find some sufficient and algorithmically achievable conditions for ε-non-ergodicity.

Let us first analyze (0-)non-ergodicity of stochastic games (in stationary strategies).

A stochastic game is non-ergodic if and only if it is ε-non-ergodic for some positive ε.

Proof   A stochastic game is non-ergodic by definition if there exists a threshold σ, positions v,u∈V, and stationary strategies α and β for the players, such that no matter what other strategy β' player 2 chooses the Markov chain resulting by fixing (α,β') has a value >  σ when using initial position v0 = v (guaranteeing for player 1 more than σ from v), and the Markov chain obtained by fixing (α',β) has a value <  σ when using initial position v0 = u (guaranteeing for player 2 less than σ from u). Since strategies α' and β' are chosen from a compact space, the above implies that there are σ' > σ  >  σ'' such that α guarantees for player 1 at least σ' from the initial position v, and β guarantees for player 2 at most σ'' from initial position u. Hence the game is ε-non-ergodic for any ε ~  <  ~ σ' - σ''.[formula]

A stochastic game Γ is ε-non-ergodic if there exist disjoint non-empty subsets of the positions I,F  ⊆  V, reals a,b with b - a  ≥  ε, stationary strategies αv, v∈I, for player 1, and βu, u∈F, for player 2, and a vector of potentials [formula], such that

[formula] for all v∈I, [formula], k∈Kv and [formula],

[formula] for all u∈F, [formula], [formula] and k∈Ku, and

for all v∈I and u∈F:

[formula]

Proof   Let us note that (N1) and (N3) imply that for all strategies β'∈L(Γ) of player 2, the pair of strategies (,β'), where v: = αv for v∈I and v∈Δ(Kv) is chosen arbitrarily for [formula], results in a Markov chain in which subset I induces one or more absorbing sets (that is, pvuβ' = 0), and in which all positions have values at least b. Analogously, (N2) and (N3) imply that F will always induce an absorbing set with values less than a, if we fix any pair of strategies (α',), where α' is any strategy in K(Γ), v: = βv for v∈F and v∈Δ(Lv) is chosen arbitrarily, for [formula]. Hence choosing any positions v∈I and u∈F and strategies [formula] and [formula] provides a witness for the ε-nonergodicity of Γ. (Here, we use the well-known fact [\cite=MO70] that, to each player's stationary strategy, there is a best response of the opponent which is also stationary.) [formula]

Let us introduce a notation for denoting upper bounds on the entries of the matrices, more precisely on the part of these entries which do not depend on negative potential differences. Specifically, define

[formula]

where, as before, m+(x): =  max vmv(x),  m-(x): =  min vmv(x). Define further

[formula]

Note that

[formula]

which implies

[formula]

With this notation we can state a more constructive version of Lemma [\ref=l02].

A stochastic game Γ satisfying ([\ref=e77]) is ε-non-ergodic if there exist disjoint non-empty subsets I,F  ⊆  V, a vector of potentials [formula], and reals a',b'∈[0,m + (x)] with b' - a'  ≥  3ε, [formula], [formula],such that

mv(x)  ≥  b' for all v∈I, and mu(x) < a' for all u∈F;

xu - xv  ≥  |Lv|WRv(x)2  /  ε for all [formula], and v∈I;

xu - xv  ≥  |Kv|WRv(x)2  /  ε for all u∈F, and [formula].

Proof   We first show that (N4)-(N5) imply the existence of strategies αv, for v∈I, satisfying (N1) and (N3). We shall then observe that a similar argument can be applied to (N4) and (N6) to show the existence of strategies βu, for u∈F, such that those satisfy (N2) and (N3). Consequently, our claim will follow by Lemma [\ref=l02].

Let us now fix a position v∈I and denote respectively by v and v the optimal strategies of players with respect to the payoff matrix Av(x). Denote further by [formula] the uniform strategy for player 2, and set [formula].

Let us then note that we have

[formula]

since at least one of the entries of (N5) has at least [formula] as a coefficient in rows which are not in qv.

Note that b' > 0 implies by ([\ref=e11]) that Rv(x) > 0. Thus by the optimality of [formula] and by the above inequalities we have

[formula]

implying that [formula] Since by (N4) we have 0 < a', inequalities ε < a' + 3ε  ≤  b' < mv(x)  ≤  Rv(x) follow, and hence [formula] must hold, implying that the set qv is not empty. Let us then denote by [formula] the truncated strategy defined by

[formula]

With this we have for any [formula]

[formula]

Let us then define [formula] and repeat the same for all v∈I. Then, these strategies satisfy (N1) and (N3) with b = b' - ε.

Let us next note that by adding a constant to a matrix game it changes its value with exactly the same constant. Furthermore, multiplying all entries by - 1 and transposing it, changes its value by a factor of - 1, interchanges the roles of row and column players, but leaves otherwise optimal strategies still optimal. Thus, we can repeat the above arguments for the matrices Bu(x) = m+(x)Eu - Au(x)T, where E is the |Lu|  ×  |Ku|-matrix of all ones, and obtain the same way strategies βu, u∈F satisfying (N2) and (N3) with a = a' + ε. This completes the proof of the lemma.[formula]

To create a finite algorithm to find sets I and F and potentials satisfying (N4)-(N6) we need to do some modifications in our procedures.

First, we allow a more flexible partitioning of the m-range by allowing the m-range boundaries to be passed as parameters and replacing line [\ref=(P1)] in procedure Pump by

Set δ: = (m+ - m-) / 4.

Next, Let us replace in procedure Pump, line [\ref=(P4)] by the following lines, where ε > 0 is a prespecified parameter, and call the new procedure with these modifications ModifiedPump(ε,x,S,m-,m+):

Otherwise set Pτ: = {v∈S|mv(xτ)  ≥  m- + 2δ} and compute

[formula]

where [formula] and [formula] are defined by [\eqref=e55].

Create an auxiliary directed graph G = (V,E) on vertex set V such that (v,u)∈E iff

[formula]

Find subsets Iτ and Fτ of V such that Tτ  ⊆  Iτ  ⊆  Pτ, [formula], and no arcs are leaving these sets in G (this can be done by a finding the strong components of G, or by the method described int he proof of Theorem [\ref=t-main]).

if such sets are found STOP and output these sets, otherwise continue with step [\ref=(P5)].

Before starting to analyze this modified pumping algorithm, let us observe that we have for all iterations

[formula]

as long as m+ - m-  >  ε.

Procedure ModifiedPump(ε,x,S) terminates in a finite number of steps.

Proof   Let us observe that by Lemma [\ref=l2] procedure Pump would either terminate with [formula] for some finite τ  ≥  τ0, or there exist sets I = Iτ and F = Fτ satisfying conditions (b) and (c) of the lemma, for Δ = NWQ2  /  ε, where [formula], and [formula]. Thus, in the latter case, ModifiedPump will indeed find some sets Iτ and Fτ, and hence terminate for some finite τ. [formula]

Procedure ModifiedPump(ε,x,V) either shrinks the m-range by a factor of 3 / 4 or outputs potentials x = xτ and sets I = Iτ and F = Fτ which satisfy conditions (N4)-(N6) with a' < b'.

Proof   When the procedure terminates without shrinking the m-range, then it outputs sets I = Iτ and F = Fτ such that in the auxiliary graph G there are no arcs leaving these sets. Since I  ⊆  Pτ and [formula], condition (N4) holds with [formula]. Furthermore, the lack of leaving arcs in G implies that for all (v,u), v∈I and [formula] and also for all (u,v) with u∈F and [formula] we must have the reverse inequalities in (7b), implying that conditions (N5) and (N6) hold.[formula]

Let us observe that the bounds and strategies obtained by Lemmas [\ref=l4] and [\ref=l5] do not necessarily imply the ε-non-ergodicity of the game since those positions in Iτ and Fτ may not have enough separation in m-values (i.e. the condition b' - a'  ≥  3ε in Lemma [\ref=l03] is not satisfied). To fix this we need to make one more use of the pumping algorithm, as described in the ModifiedRepeatedPumping procedure below. After each range-shrinking in this algorithm, we use a routine called ReducePotential(Γ,x,m-,m+) which takes the current potential vector x and range

[formula]

. By choosing δ sufficiently smaller than the desired accuracy ε, we can ignore the effect of such approximation.

ModifiedRepeatedPumping(ε) terminates in a finite number [formula] of iterations, and either provides a potential transformation proving that the game is 24ε-ergodic, or outputs two nonempty subsets I and F and strategies αv, v∈I, for player 1 and βv, v∈F, for player 2 such that conditions (N4), (N5) and (N6) hold with b',a' satisfying the condition in Lemma [\ref=l03].

Proof   Let us note that if [formula] after the second ModifiedPump call, then the range of the m-values has shrunk by a factor of [formula] (at least), while if this happens in the first stage the m-range has shrunk by a factor of 3 / 4.

On the other hand if the m-range is not shrinking, and we have [formula] after the second call of ModifiedPump, then we would also have [formula] for all v∈I, while mu(xτ) < (m+ + m-) / 2 = a' for all u∈F, and hence (N4)-(N6) hold with these a' and b' values. Since the m-range has not shrunk, we must have m+ - m- > 24ε, and hence [formula] follows. (Note that, since in the second stage we pump only positions in Iτ, the potentials of these positions may go down, while those of the positions outside Iτ remain unchanged, and hence condition (N5) remains satisfied.)

Finally, if the m-range is not shrinking, and the second call returns a new set Iτ, then all m-values of this set are at least [formula], and with the same set F we can conclude again that conditions (N4)-(N6) hold. [formula]

To complete the proof of Theorem [\ref=t-main], we need to analyze the time complexity of the above procedure, in particular, bounding the number of pumping steps performed in ModifiedPump.

Let us note that as long as m+ - m- > 24ε we pump the upper half Pτ by exactly δ  ≥  6ε. Let Pτ(v) (resp., Nτ(v)) denote the number of iterations, among the first τ, in which position v was pumped, that is, v∈Pτ (resp., not pumped, that is, [formula]).

Let us next sort the positions v∈V such that we have

[formula]

and write Δj = xvj + 1τ - xvjτ for j = 1,2,...,n - 1. Note that Pτ(v1) = τ and Nτ(vn) = τ.

Let iτ be the largest index in [formula], such that viτ∈Pτ. Then, by [\eqref=e55] we have for [formula] that

[formula]

where the sum over the empty sum is zero by definition. Similarly, for [formula], we have

[formula]

From ([\ref=e90]) and ([\ref=e93]), it follows that

[formula]

Let [formula] be the smallest index i such that

[formula]

and let îτ be the largest index i  ≤  n - 1 such that

[formula]

From the definition of [formula], we know that

[formula]

Solving this recurrence, we get

[formula]

Similarly, the definition of îτ gives

[formula]

from which follows

[formula]

Note that if [formula] then ([\ref=e91]) implies that taking [formula] would satisfy condition (N5) and guarantee that Iτ  ⊆  Pτ.

Indeed, for all [formula] and [formula], we have

[formula]

Similarly, having îτ  ≥  iτ guarantees that taking [formula] would satisfy (N6) and [formula].

, since for all i  ≥  îτ + 1 and [formula], we have

[formula]

On the other hand, if [formula], then ([\ref=e95]) implies that viτ + 1 was always pumped except for at most [formula] iterations, that is, Nτ(viτ + 1)  ≤  κ(R). Also, since [formula], then at time τ, viτ + 1 is not pumped. Similarly, if îτ  <  iτ, then ([\ref=e98]) implies that viτ was never pumped except for at most κ(R) iterations, that is, Pτ(viτ)  ≤  κ(R), while it is pumped at time τ. Since we have at most n candidates for each of viτ and viτ + 1, it follows that after τ = 2nκ(R) + 1, neither of these events ([formula] and îτ  <  iτ) can happen, which by our earlier observations implies that the algorithm constructs the sets Iτ and Fτ. We can conclude that ModifiedPump(ε,x,V) must terminate in at most 2nκ(R) + 1 iterations, either producing m+ - m-  ≤  24ε or outputting the subsets Iτ and Fτ proving ε-non-ergodicity.

One can similarly bound the running time for the second call of ModifiedPump (line [\ref=(M6)]), and the running time for each iteration of ModifiedRepeatedPumping(ε) (but with R replaced by [formula]).

It remains now to bound the running time for the second call of ModifiedPump (line [\ref=(M6)]), and the running time for each iteration of ModifiedRepeatedPumping(ε). We can repeat essentially the same analysis as above, assuming that we modify the rewards with the potential vector obtained up to this point in time. Since, by the above argument, the maximum potential difference between any vertices before at the time τ, when we make the second call to ModifiedPump is at most δ(2nκ(R) + 1), it follows that the maximum absolute value of the transformed rewards at time τ is [formula] (note that the non-negativity of the rewards was only needed to bound m-  ≥  0 initially). It follows by the same argument as above that the second call ModifiedPump terminates in time [formula].

After shrinking the m-range, we apply potential reductions which guarantees that the bit length of each entry in potential vector is bounded by a polynomial in the original bit length η. It follows that the new transformed rewards will have absolute value bounded by [formula]. We repeat the same argument for the different phases of ModifiedRepeatedPumping(ε) to arrive at the running time claimed in Theorem [\ref=t-main].

This completes the proof of the theorem.[formula]