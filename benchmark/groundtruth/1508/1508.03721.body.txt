A Comparative Study on Regularization Strategies for Embedding-based Neural Networks

, 1  Lili Mou,* 1 Ge Li,[formula]  Yunchuan Chen,2 Yangyang Lu,1 Zhi Jin1 1Software Institute, Peking University, 100871, P. R. China {penghao.pku, doublepower.mou}{lige, luyy11, zhijin} 2University of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn

Introduction

Neural networks have exhibited considerable potential in various fields [\cite=imagenet] [\cite=speechrnn]. In early years on neural NLP research, neural networks were used in language modeling [\cite=LM] [\cite=hierarchical] [\cite=hierarchical2]; recently, they have been applied to various supervised tasks, such as named entity recognition [\cite=unified], sentiment analysis [\cite=RAE] [\cite=sentenceTBCNN], relation classification [\cite=re] [\cite=sdplstm], etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like ; then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters.

A curious question is whether we can regularize embedding-based NLP neural models to improve generalization. Although existing and newly proposed regularization methods might alleviate the problem, their inherent performance in neural NLP models is not clear: the use of embeddings is sparse; the behaviors may be different from those in other scenarios like image recognition. Further, selecting hyperparameters to pursue the best performance by validation is extremely time-consuming, as suggested in . Therefore, new studies are needed to provide a more complete picture regarding regularization for neural natural language processing. Specifically, we focus on the following research questions in this paper.

In this paper, we systematically and quantitatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding [\cite=reembed], and dropout [\cite=dropout]. We analyzed these regularization methods by two widely studied models and tasks. We also emphasized on incremental hyperparameter tuning and the combination of different regularization methods.

Our experiments provide some interesting results: (1) Regularizations do help generalization, but their effect depends largely on the datasets' size. (2) Penalizing [formula]-norm of embeddings helps optimization as well, improving training accuracy unexpectedly. (3) Incremental hyperparameter tuning achieves similar performance, indicating that regularizations mainly serve as a "local" effect. (4) Dropout performs slightly worse than [formula] penalty in our experiments; however, provided very small [formula] penalty, dropping out hidden units and penalizing [formula]-norm are generally complementary. (5) The newly proposed re-embedding words method is not effective in our experiments.

Tasks, Models, and Setup

Experiment I: Relation extraction. The dataset in this experiment comes from SemEval-2010 Task 8. The goal is to classify the relationship between two marked entities in each sentence. We refer interested readers to recent advances, e.g., , , and . To make our task and model general, however, we do not consider entity tagging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default |other|.

Regarding the neural model, we applied Collobert's convolutional neural network (CNN) [\cite=unified] with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, [formula] padded at the end of each sentence; a max pooling layer; a tanh  hidden layer; and a [formula] output layer.

Experiment II: Sentiment analysis. This is another testbed for neural NLP, aiming to predict the sentiment of a sentence. The dataset is the Stanford sentiment treebank [\cite=RAE]; target labels are |strongly/weakly| |positive/negative|, or |neutral|.

We used the recursive neural network (RNN), which is proposed in , and further developed in ; . RNNs make use of binarized constituency trees, and recursively encode children's information to their parent's; the root vector is finally used for sentiment classification.

Experimental Setup. To setup a fair comparison, we set all layers to be 50-dimensional in advance (rather than by validation). Such setting has been used in previous work like . Our embeddings are pretrained on the Wikipedia corpus using . The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to prevent parameter blowup (probably due to the recursive, and thus chaotic nature). Therefore, we applied power decay [\cite=learningrate] with power equal to - 1. For each strategy, we tried a large range of regularization coefficients, [formula], extensively from underfitting to no effect with granularity 10x. We ran the model 5 times with different initializations. We used mini-batch stochastic gradient descent; gradients are computed by standard backpropagation. For source code, please refer to our project website.

It needs to be noticed that, the goal of this paper is not to outperform or reproduce state-of-the-art results. Instead, we would like to have a fair comparison. The testbed of our work is two widely studied models and tasks, which were not chosen on purpose. During the experiments, we tried to make the comparison as fair as possible. Therefore, we think that the results of this work can be generalized to similar scenarios.

Regularization Strategies

In this section, we describe four regularization strategies used in our experiment.

Individual Regularization Behaviors

This section compares the behavior of each strategy. We first conducted both experiments without regularization, achieving accuracies of 54.02  ±  0.84%, 41.47  ±  2.85%, respectively. Then we plot in Figure [\ref=fIndividual] learning curves when each regularization strategy is applied individually. We report training and validation accuracies through out this paper. The main findings are as follows.

Penalizing [formula]-norm of weights helps generalization; the effect depends largely on the size of training set. Experiment I contains 7,000 training samples and the improvement is 6.98%; Experiment II contains more than 150k samples, and the improvement is only 2.07%. Such results are consistent with other machine learning models.

Penalizing [formula]-norm of embeddings unexpectedly helps optimization (improves training accuracy). One plausible explanation is that since embeddings are trained on a large corpus by unsupervised methods, they tend to settle down to large values and may not perfectly agree with the tasks of interest. [formula] penalty pushes the embeddings towards small values and thus helps optimization. Regarding validation accuracy, Experiment I is improved by 6.89%, whereas Experiment II has no significant difference.

Re-embedding words does not improve generalization. Particularly, in Experiment II, the ultimate accuracy is improved by 0.44, which is not large. Further, too much penalty hurts the models in both experiments. In the limit [formula] to infinity, re-embedding words is mathematically equivalent to using embeddings as surface features, that is, freezing embeddings. Such strategy is sometimes applied in the literature like , but is not favorable as suggested by the experiment.

Dropout helps generalization. Under the best settings, the eventual accuracy is improved by 3.12% and 1.76%, respectively. In our experiments, dropout alone is not as useful as [formula] penalty. However, other studies report that dropout is very effective [\cite=deepRNN]. Our results are not consistent; different dimensionality may contribute to this disagreement, but more experiments are needed to confirm the hypothesis.

Incremental Hyperparameter Tuning

The above experiments show that regularization generally helps prevent overfitting. To pursue the best performance, we need to try out different hyperparameters through validation. Unfortunately, training deep neural networks is time-consuming, preventing full grid search from being a practical technique. Things will get easier if we can incrementally tune hyperparameters, that is, to train the model without regularization first, and then add penalty.

In this section, we study whether [formula] penalty of weights and embeddings can be tuned incrementally. We exclude the dropout strategy because its does not make much sense to incrementally drop out hidden units. Besides, from this section, we only focus on Experiment I due to time and space limit.

Before continuing, we may envision several possibilities on how regularization works.

To verify the above conjectures, we design four settings: adding penalty (1) at the beginning, (2) before overfitting at epoch 2, (3) at peak performance (epoch 5), and (4) after overfitting (validation accuracy drops) at epoch 10.

Figure [\ref=fIncremental] plots the learning curves regarding penalizing weights and embeddings, respectively; baseline (without regularization) is also included.

For both weights and embeddings, all settings yield similar ultimate validation accuracies. This shows [formula] regularization mainly serves as a "local" effect--it changes the error surface, but parameters tend to settle down to a same catchment basin. We notice a recent report also shows local optima may not play an important role in training neural networks, if the effect of parameter symmetry is ruled out [\cite=localoptima].

We also observe that regularization helps generalization as soon as it is added (Figure [\ref=fIncremental]a), and that regularizing embeddings helps optimization also right after the penalty is applied (Figure [\ref=fIncremental]b).

Combination of Regularizations

We are further curious about the behaviors when different regularization methods are combined.

Table [\ref=tCombineWE] shows that combining [formula]-norm of weights and embeddings results in a further accuracy improvement of 3-4 percents from applying either single one of them. In a certain range of coefficients, weights and embeddings are complementary: given one hyperparameter, we can tune the other to achieve a result among highest ones.

Such compensation is also observed in penalizing [formula]-norm versus dropout (Table [\ref=tCombineDropout])--although the peak performance is obtained by pure [formula] regularization, applying dropout with small [formula] penalty also achieves a similar accuracy. The dropout rate is not very sensitive, provided it is small.

Discussion

In this paper, we systematically compared four regularization strategies for embedding-based neural networks in NLP. Based on the experimental results, we answer our research questions as follows. (1) Regularization methods (except re-embedding words) basically help generalization. Penalizing [formula]-norm of embeddings unexpectedly helps optimization as well. Regularization performance depends largely on the dataset's size. (2) [formula] penalty mainly acts as a local effect; hyperparameters can be tuned incrementally. (3) Combining [formula]-norm of weights and biases (dropout and [formula] penalty) further improves generalization; their coefficients are mostly complementary within a certain range. These empirical results of regularization strategies shed some light on tuning neural models for NLP.

Acknowledgments

This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant No. 61232015. We would also like to thank Hao Jia and Ran Jia.