Fast Asynchronous Parallel Stochastic Gradient Decent

Introduction

Assume we have a set of labeled instances [formula], where [formula] is the feature vector for instance i, p is the feature size and [formula] is the class label of [formula]. In machine learning, we often need to solve the following regularized empirical risk minimization problem:

[formula]

where [formula] is the parameter to learn, [formula] is the loss function defined on instance i, and often with a regularization term to avoid overfitting. For example, [formula] can be [formula] which is known as the logistic loss, or [formula] which is known as the hinge loss in support vector machine (SVM). The regularization term can be [formula], [formula], or some other forms.

Due to their efficiency and effectiveness, stochastic gradient descent (SGD) and its variants [\cite=DBLP:conf/nips/Xiao09] [\cite=DBLP:journals/jmlr/DuchiS09] [\cite=DBLP:conf/nips/RouxSB12] [\cite=DBLP:conf/icml/Mairal13] [\cite=DBLP:conf/nips/Johnson013] [\cite=DBLP:journals/jmlr/Shalev-Shwartz013] [\cite=DBLP:conf/nips/Nitanda14] have recently attracted much attention to solve machine learning problems like that in ([\ref=problem]). Many works have proved that SGD and its variants can outperform traditional batch learning algorithms such as gradient descent or Newton methods in real applications.

In many real-world problems, the number of instances n is typically very large. In this case, the traditional sequential SGD methods might not be efficient enough to find the optimal solution for ([\ref=problem]). On the other hand, clusters and multicore systems have become popular in recent years. Hence, to handle large-scale problems, researchers have recently proposed several distributed SGD methods for clusters and parallel SGD methods for multicore systems. Although distributed SGD methods for clusters like those in [\cite=DBLP:conf/nips/ZinkevichSL09] [\cite=DBLP:conf/nips/DuchiAW10] [\cite=DBLP:conf/nips/ZinkevichWSL10] are meaningful to handle very large-scale problems, there also exist a lot of problems which can be solved by a single machine with multiple cores. Furthermore, even in distributed settings with clusters, each machine (node) of the cluster typically have multiple cores. Hence, how to design effective parallel SGD methods for multicore systems has become a key issue to solve large-scale learning problems like that in ([\ref=problem]).

There have appeared some parallel SGD methods for multicore systems. The round-robin scheme proposed in [\cite=DBLP:conf/nips/ZinkevichSL09] tries to order the processors and then each processor update the variables in order. Hogwild! [\cite=recht2011hogwild] is a lock-free approach for parallel SGD. Experimental results in [\cite=recht2011hogwild] have shown that Hogwild! can outperform the round-robin scheme in [\cite=DBLP:conf/nips/ZinkevichSL09]. However, Hogwild! can only achieve a sub-linear convergence rate. Hence, Hogwild! is not efficient (fast) enough to achieve satisfactory performance.

In this paper, we propose a fast asynchronous parallel SGD method, called AsySVRG, by designing an asynchronous strategy to parallelize the recently proposed SGD variant called stochastic variance reduced gradient (SVRG) [\cite=DBLP:conf/nips/Johnson013]. The contributions of AsySVRG can be outlined as follows:

Two asynchronous schemes, consistent reading and inconsistent reading, are proposed to coordinate different threads. Theoretical analysis is provided to show that both schemes have linear convergence rate, which is faster than that of Hogwild!

The implementation of AsySVRG is simple.

Empirical results on real datasets show that AsySVRG can outperform Hogwild! in terms of computation cost.

Preliminary

We use [formula] to denote the objective function in ([\ref=problem]), which means [formula]. In this paper, we use [formula] to denote the L2-norm [formula] and [formula] to denote the optimal solution of the objective function.

Algorithm

Assume that we have p processors (threads) which can access a shared memory, and [formula] is stored in the shared memory. Furthermore, we assume each thread has access to a shared data structure for the vector [formula] and has access to choose any instance randomly to compute the gradient [formula]. We also assume consistent reading of [formula], which means that all the elements of [formula] in the shared memory have the same "age" (time clock).

Our AsySVRG algorithm is presented in Algorithm [\ref=alg:AsySVRG]. We can find that in the tth iteration, each thread completes the following operations:

All threads parallelly compute the full gradient [formula]. Assume the gradients computed by thread a are denoted by φa which is a subset of [formula]. We have [formula] if a  ≠  b, and [formula].

Run an inner-loop in which each iteration randomly chooses an instance indexed by im and computes the gradient [formula], where [formula], and compute the vector

[formula]

Then update the vector

[formula]

where η > 0 is a step size.

Here, m is the total number of updates on [formula] from all threads and k(m) is the [formula]-iteration at which the update [formula] was calculated. Since each thread can compute an update and change the [formula], k(m)  ≤  m obviously. At the same time, we should guarantee that the update is not too old. Hence, we need m  -  k(m)  ≤  τ, where τ is a positive integer, and usually called the bounded delay. If τ  =  0, the algorithm AsySVRG degenerates to the sequential (single-thread) version of SVRG.

Convergence Analysis

Our convergence analysis is based on the Option 2 in Algorithm [\ref=alg:AsySVRG]. Please note that we have p threads and let each thread calculate M times of update. Hence, the total times of updates on [formula] in the shared memory, which is denoted by [formula], must satisfy that 0 <   ≤  pM. And obviously, the larger the M is, the larger the [formula] will be.

Consistent Reading

Since [formula] is a vector with several elements, it is typically impossible to complete updating [formula] in an atomic operation. We have to give this step a lock for each thread. More specifically, it need a lock whenever a thread tries to read [formula] or update [formula] in the shared memory. This is called consistent reading scheme.

First, we give some notations as follows:

[formula]

It is easy to find that [formula] and the update of [formula] can be written as follows:

[formula]

One key to get the convergence rate is the estimation of the variance of [formula]. We use the technique in [\cite=liu2013asynchronous] and get the following result:

There exists a constant ρ  >  1 such that [formula].

[formula]

The fourth inequality uses Assumption [\ref=L-smooth] and r  >  0 is a constant. Summing ([\ref=var:p]) from i = 1 to n, and taking expectation about im, we have

[formula]

We use [formula] and choose r,η such that 0 < c < 1, then we can get [formula]. Please note that k(0)  =  0. Then, we obtain that

[formula]

where ρ satisfies [formula] and [formula].

According to Lemma [\ref=lemma1], ρ  >  1. If we want ρ to be small enough, we need a small step size η. This is reasonable because [formula] should be changed slowly if the gradient applied to update [formula] is relatively old.

With the Assumption [\ref=L-smooth] and [\ref=strong], choosing a small step size η and large [formula], we have the following result:

[formula]

where [formula] and 1  -  2(τ  +  1)ρ2τηL > 0.

Inconsistent Reading

The consistent reading scheme would cost much waiting time because we need a lock whenever a thread tries to read [formula] or update [formula]. In this subsection, we will introduce an inconsistent reading scheme, in which a thread does not need a lock when reading current [formula] in the memory. For the update step, the thread still need a lock. Please note that our inconsistent reading scheme is different from that in [\cite=hsieh2015passcode] which adopts the atomic update strategy. Since the update vector applied to [formula] is usually dense, the atomic update strategy used in [\cite=hsieh2015passcode] is not applicable for our case.

For convenience, we use [formula] to denote the vector set generated in the inner loop of our algorithm, and m to denote the vector that one thread gets from the shared memory and uses to update [formula]. Then, we have

[formula]

We also need the following assumption:

Since we do not use locks when a thread reads [formula] in the shared memory, some elements in [formula] which have not been read by one thread may be changed by other threads. Usually, [formula]. If we call the age of each element of [formula] to be m, the ages of elements of m may not be the same. We use a(m) to denote the smallest age of the elements of m. Of course, we expect that a(m) is not too small. Given a positive integer τ, we assume that m  -  a(m)  ≤  τ. With Assumption [\ref=assum], according to the definition of m and a(m), we have

[formula]

where [formula] is a set that belongs to [formula], [formula] is a diagonal matrix that only the kth diagonal position is 1, [formula], and other elements of [formula] are 0.

The ([\ref=def:hatw]) is right because with an update lock and Assumption [\ref=assum], at most one thread is updating [formula] at any time. If a thread begins to read [formula], only two cases would happen. One is that no threads are updating [formula], which leads [formula]. Another is that one thread is updating [formula], which leads to the result that the thread would get a new [formula] and may also get some old elements. Obviously, they enjoy the same age of [formula] if it reads at a good pace.

Then, we can get the following results:

[formula] or [formula].

[formula], and [formula], which means that [formula], Ip is an identity matrix.

Similar to ([\ref=def:pmi]) and ([\ref=def:qm]), we give the following definitions:

[formula]

We can find that m  =  i(m) and [formula].

We give a notation for any two integers m,n, i.e., [formula].

According to the proof in Lemma [\ref=lemma1], we can get the property that [formula],

[formula]

There exists a constant ρ > 1 and a corresponding suitable step size η that make:

[formula]

According to ([\ref=based_ineq]), we have

[formula]

According to ([\ref=def:hatw]), we have

[formula]

In the first inequality, a(m + 1) may be less than a(m), but it won't impact the result of the second inequality. Summing from i = 1 to n for ([\ref=var:hatp]), we can get

[formula]

Taking expectation to [formula] which are the random numbers selected by Algorithm [\ref=alg:AsySVRG], we obtain

[formula]

When ρ,r,η satisfy the following condition:

[formula]

we have

[formula]

For the relation between m and [formula], we have the following result

[formula]

where [formula].

In ([\ref=based_ineq]), if we take [formula] and [formula], we can obtain

[formula]

Summing from i = 1 to n, we obtain

[formula]

where the second inequality uses the fact that [formula].

Taking expectation on both sides, and using Lemma [\ref=estimate:hatq], we get

[formula]

which means that

[formula]

Similar to Theorem ([\ref=theorem1]), we have the following result about inconsistent reading:

With Assumption [\ref=L-smooth], [\ref=strong], [\ref=assum], a suitable step size η which satisfies the condition in Lemma [\ref=estimate:hatq], [\ref=estimate:hatvm], and a large [formula], we can get our convergence result for the inconsistent reading scheme:

[formula]

where [formula].

Remark: In our convergence analysis for both consistent reading and inconsistent reading schemes, there are a lot of parameters, such as r,η,ρ,τ,,μ,L. We can set [formula]. Since μ,L are determined by the objective function and ρ,τ are constants, we only need the step size η to be small enough and [formula] to be large enough. Then, all the conditions in these lemmas and theorems will be satisfied.

Experiments

We choose logistic regression with a L2-norm regularization term to evaluate our AsySVRG. Hence, the [formula] is defined as follows:

[formula]

We choose Hogwild! as baseline because Hogwild! has been proved to be the state-of-the-art parallel SGD methods for multicore systems [\cite=recht2011hogwild]. The experiments are conducted on a server with 12 Intel cores and 64G memory.

Dataset and Evaluation Metric

We choose three datasets for evaluation. They are rcv1, real-sim, and news20, which can be downloaded from the LibSVM website . Detailed information is shown in Table [\ref=data], where λ is the hyper-parameter in [formula].

We adopt the speedup and convergence rate for evaluation. The definition of speedup is as follows:

[formula]

We get a suboptimal solution by stopping the algorithms when the gap between training loss and the optimal solution [formula] is less than 10- 4.

We set M in Algorithm [\ref=alg:AsySVRG] to be [formula], where n is the number of training instances and p is number of threads. When p = 1, the setting about M is the same as that in SVRG [\cite=DBLP:conf/nips/Johnson013]. According to our theorems, the step size should be small. However, we can also get good performance with a relatively large step size in practice. For the Hogwild!, in each epoch, we run each thread [formula] iterations. We use a constant step size γ, and we set γ←0.9γ after every epoch. These settings are the same as those in the experiments in Hogwild![\cite=recht2011hogwild]. For each epoch, our algorithm will visit the whole dataset three times and the Hogwild! will visit the whole dataset only once. To make a fair comparison about the convergence rate, we study the change of objective value versus the number of effective passes. One effective pass of the dataset means the whole dataset is visited once.

Results

In practice, we find that our AsySVRG algorithm without any lock strategy, denoted by AsySVRG-unlock, can achieve the best performance. Table [\ref=exp1] shows the running time and speedup results of consistent reading, inconsistent reading, and unclock schemes for AsySVRG on dateset rcv1. Here, 77.15s denotes 77.15 seconds, 1.94x means the speedup is 1.94, i.e., it is 1.94 times faster than the sequential (one-thread) algorithm.

We find that the consistent reading scheme has the worst performance. Hence, in the following experiments, we only report the results of inconsistent reading scheme, denoted by AsySVRG-lock, and AsySVRG-unlock.

Table [\ref=time] compares the time cost between AsySVRG and Hogwild! to achieve a gap less than 10- 4 with 10 threads. We can find that our AsySVRG is much faster than Hogwild!, either with lock or without lock.

Figure [\ref=fig] shows the speedup and convergence rate on three datasets. Here, AsySVRG-lock-10 denotes AsySVRG with lock strategy on 10 threads. Similar nations are used for other settings of AsySVRG and Hogwild!. We can find that the speedup of AsySVRG and Hogwild! is comparable. Combined with the results in Table [\ref=time], we can find that Hogwild! is slower than AsySVRG with different numbers of threads. From Figure [\ref=fig], we can also find that the convergence rate of AsySVRG is much faster than that of Hogwild!.

Conclusion

In this paper, we have proposed a novel asynchronous parallel SGD method, called AsySVRG, for multicore systems. Both theoretical and empirical results show that AsySVRG can outperform other state-of-the-art methods.

Notations for Proof

For the proof of Theorem [\ref=theorem1].

According to ([\ref=update_rule]), we obtain that

[formula]

For the old gradient, we have

[formula]

Since [formula] is L-smooth, we have

[formula]

Substituting the above inequalities into ([\ref=eq]), we obtain

[formula]

Since

[formula]

Taking expectation and using Lemma [\ref=lemma1], we obtain

[formula]

The second inequality uses 2Lη  ≤  1. In the first equality, we use the fact that [formula]. The last inequality uses the inequality

[formula]

which has been proved in [\cite=DBLP:conf/nips/Johnson013]. Then summing up from m = 0 to [formula], and taking [formula] or randomly choosing a [formula] to be [formula], we can get

[formula]

Then, we have

[formula]

Of course, we need 1  -  2(τ  +  1)ρ2τηL > 0.

For the proof of Theorem [\ref=theorem:inconsistent].

According to ([\ref=update:inconsistent]), we have

[formula]

Similar to the analysis of ([\ref=old_gradient]) in Theorem [\ref=theorem1], we can get

[formula]

The second inequality is the same as the analysis in ([\ref=EBhatvm]). The third inequality uses Lemma [\ref=estimate:hatq]. The fourth inequality uses Lemma [\ref=estimate:hatvm].

For convenience, we use [formula] and sum the above inequality from m = 0 to  - 1, and take [formula]. Then, we obtain

[formula]

which means that

[formula]