Efficient Superimposition Recovering Algorithm

Assume we have the extracted gradients and transformations of each latent transparent layer, the reconstruction step is the final crucial part for reflection separation. We assume the variations of transmitted layers in each mixtures conform to a parametric transformation f(x,θ) (x is the pixel coordinates) with different parameters θi. Here we propose an Efficient Superimposition Recovering Algorithm (ESRA) to fast recover the high quality latent layers.

Efficient Superimposition Recovering Algorithm

With estimated transformation parameters θi, we align the transmitted layers by warping [formula] Ii with f- 1(x,θi). Then our mixing model is rewritten as:

[formula]

Here Lt is the latent transmitted layer, Lr( i ) is the reflected layer in ith (mixtures), ai1,ai2 is the mixing coefficients. With this new mixing model, the influence of parametric transformations f(x,θi) can be ignored in the intermediate recovering process. For simplicity, we use Ii( x ) to represent Ii(f- 1i( x )). L1(x) and Li + 1( x ) denote Lt( x ) and a2iLr( i )(f- 1i( x )), respectively. Let Ei( x ) stand for the extracted gradients from Li( x ). To recover high quality latent image layers, we propose to employ L1 penalty on the extracted gradients and nonnegative constraints on the layers' intensities along with the L2 loss of the mixing model. Thus our recovering objective function is written as:

[formula]

where [formula] is a large vector containing all pixel values in all latent layers. The first L1 term enforces the agreement between reconstructed layer gradients and extracted layer gradients, while the second L2 term tends to satisfy our mixing mode. Since the extracted gradients are nonzero at very few coordinates, the L1 norm term not only prefers layers with sparse gradients but also avoids over-smooth results. λ is a trade off coefficient.

To solve the nonsmooth convex optimization model ([\ref=eq:originalLoss]) efficiently, we denote

[formula]

Here g(lvec) is the [formula] penalty on the extracted gradients and f(lvec) corresponds to the L2 loss and nonnegative constraints. f(lvec) can be formulated in the following matrix form:

[formula]

where f(lvec) is continuously differentiable and [formula], of which Lipschitz constant [formula], and [formula] is the unit matrix. We note the objective function in ([\ref=eq:originalLoss]) is a composite function of a differential term f(lvec) and a non-differential term g(lvec). Denote

[formula]

which is the first order Taylor expansion of f(lvec) at lveck - 1, with the squared Euclidean distance between lvec and lveck - 1 as the regularization term. The traditional gradient descent algorithm obtains the solution at the k-th iteration (k  ≥  1) by [formula] with a proper step size Ls (greater than L(f)). Here we propose to employ the accelerated gradient descent [\cite=nemirovski2005efficient] [\cite=nesterov2004introductory] to solve the reconstruction problem, named Efficient Superimposition Recovering Algorithm (ESRA). Here we generate a solution at the k-th iteration (k  ≥  1) by computing the following proximal operator

[formula]

where Y1  =  lvec0 and [formula] for k  ≥  1. We note that Yk is a linear combination of lveck - 1 and lveck - 2. The combination coefficient plays an important role in the convergence of the algorithm. As suggested by [\cite=beck2009], we set t0 = 1 and [formula] for k  ≥  1. According to the theoretical analysis in [\cite=beck2009], this accelerated gradient descent method can get within O(1 / k2) of the optimal objective value after k steps. While solving problem ([\ref=eq:proximalOperator]) is still very challenging, we propose a Parallel Algorithm with Constrained Total Variation (PACTV) method to find the optimal solution, which is presented in the sequel.

PACTV via dual approach

Given problem ([\ref=eq:proximalOperator]), we observe it can be solved block separable in the following way. If we denote [formula] ([formula], we can split [formula] into m + 1 separable parts. Then by employing the definition of ([\ref=eq:GFX]), we transform ([\ref=eq:proximalOperator]) into the following form:

[formula]

As illustrated in ([\ref=eq:PLYkobj]), finding lveck is to solve following m   +   1 separable problems with constrained total variation in parallel:

[formula]

Here β  =  λ / Ls, and L,d,E represent Li,di,Ei, respectively. Similar with the image denoising problem [\cite=beck2009fast] [\cite=beck2009], we propose a dual approach to solve ([\ref=eq:FGPobjective]) and give some notation in order:

P is the set of matrix-pairs (p,q) where [formula] and [formula] that satisfy [formula]. And we assume [formula], for every [formula].

The linear operation [formula] is defined by the formula [formula]

The operator [formula] which is adjoint to L is given by [formula] where pi,j = Li   +   1,j - Li,j and qi,j = Li,j   +   1 - Li,j.

PC is the orthogonal projection operator on the convex closed set C  =  {L:0  ≤  L  ≤  1}.

Equipped with these notation, we derive a dual problem of ([\ref=eq:FGPobjective]), and give following proposition to state the relation between the primal and dual optimal solutions.

Let (p,q)∈P be the optimal solution of the problem

[formula]

where HC(L) = L - PC(L) for every [formula]. Then the optimal solution of ([\ref=eq:FGPobjective]) is given by L  =  PC(d - βL(p,q)).

First note the following relation holds true:

[formula]

Hence, we can give

[formula]

where,

[formula]

With this notation we have

[formula]

Thus the original problem ([\ref=eq:FGPobjective]) becomes

[formula]

Since the objective function is convex in L and concave in p,q, we can exchange the order of the minimum and maximum and get

[formula]

and which can be written as

[formula]

Thus the optimal solution of the inner minimization problem is

[formula]

And last, we plug the above expression for L back into ([\ref=eq:mmdualproblemformsimple]) and ignore the constant term, we obtain the dual problem is

[formula]

which is the same as ([\ref=eq:dualFGPpq]).

what's more, given ([\ref=eq:dualFGPpq]), we can easily have following lemma.

The objective funtion H of ([\ref=eq:dualFGPpq]) is continuously differentiable and its gradient is given by

[formula]

And let L(H) be the Lipschitz constant of [formula], then L(H)  ≤  8β2.

Consider the function [formula] defined by

[formula]

Then the dual function ([\ref=eq:dualFGPpq]) can be written as:

[formula]

Obviously, s(  ·  ) is continuously differentiable and its gradient is given by

[formula]

Therefore,

[formula]

Then for every two pairs of matrices (p1,q1),(p2,q2) where [formula] and [formula] for i = 1,2, we have

[formula]

here the above inequalities follow from the non-expensiveness property of the orthogonal projection operator and property of linear operators [formula]. And from [\cite=beck2009fast], we have [formula]. Therefore, implying that [formula] and hence L(H)  ≤  8β2.

With definition of H(p,q) and [formula], fast gradient projection (FGP) is applied on the dual problem ([\ref=eq:dualFGPpq]). And the complexity of each iteration in FGP is O(hw). Above all, our proposed Parallel Algorithm with Constrained Total Variation (PACTV) is using FGP to solve the m + 1 dual problems ([\ref=eq:dualFGPpq]) in parallel. Then we catenate the optimal L*i [formula] and resize them into vector form to achieve lveck.

Given above proposition and lemma, we can use the fast gradient projection (FGP) on dual problem ([\ref=eq:dualFGPpq]). Fast gradient projection (FGP) is outlined in Algorithm [\ref=alg:FGP]. Here PP(p,q) means projecting the matrix-pair (p,q) on the set P. And finally we achieve the optimal solution of ([\ref=eq:FGPobjective]). Then our recovering method ESRA is outlined in Algorithm [\ref=alg:ESRA].

In our implementations, we set the total iteration number of ESRA is 100 and FGP tolerance is 0.0001, and we also set Ls = 2L(f) to ensure a constant stepsize. The initial value of lvec is zero. The final recovered reflected layers of ([\ref=eq:originalLoss]) should be warped with fi and enhance the intensity by 2 to be visible. Our recovering method launches a general optimization framework and can be extended to solve other reconstruction problems in [\cite=GaiKun09CVPR] [\cite=gai2011blind].