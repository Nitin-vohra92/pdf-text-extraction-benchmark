true in true in

Empirical average-case relation between undersampling and sparsity in x-ray CT

Jakob S. Jørgensen

Department of Applied Mathematics and Computer Science, Technical University of Denmark

Richard Petersens Plads, Building 324, 2800 Kgs. Lyngby, Denmark,

Emil Y. Sidky

Department of Radiology, University of Chicago

5841 South Maryland Avenue, Chicago, IL 60637, USA,

Per Christian Hansen

Department of Applied Mathematics and Computer Science, Technical University of Denmark

Richard Petersens Plads, Building 324, 2800 Kgs. Lyngby, Denmark,

Xiaochuan Pan

Department of Radiology, University of Chicago

5841 South Maryland Avenue, Chicago, IL 60637, USA,

Introduction

In x-ray computed tomography (CT) an image of an object is reconstructed from projections obtained by measuring the attenuation of x-rays passed through the object. Motivated by reducing the exposure to radiation, there is a growing interest in low-dose CT, cf. [\cite=Yu:2009] and references therein. This is relevant many fields, for example, in medical imaging to reduce the risk of radiation-induced cancer, in biomedical imaging where high doses can damage the specimen under study, and in materials science and non-destructive testing to reduce scanning time.

Classical reconstruction methods are based on closed-form analytical or approximate inverses of the continuous forward operator; examples are the filtered back-projection method [\cite=natterer1986mathematics] and the Feldkamp-Davis-Kress method for cone-beam CT [\cite=FDK84]. Their main advantages are low memory demands and computational efficiency, which make them the current methods of choice in commercial CT scanners [\cite=PanIP:09]. However, they are known to have limitations on reduced data.

Alternatively, an algebraic formulation can be used, in which both object and data domains are discretized, yielding a large sparse system of linear equations. This approach can handle non-standard scanning geometries for which no analytical inverse is available. Furthermore, data reduction arising from low-dose imaging can sometimes be compensated for by exploiting prior information about the image, such as smoothness or as in our case sparsity, i.e., having a representation with few non-zero coefficients.

Developments in compressed sensing (CS) [\cite=candes2006robust] [\cite=Donoho2006] show potential for a reduction in data while maintaining or even improving reconstruction quality. This is made possible by exploiting image sparsity; loosely speaking, if the image is "sparse enough", it admits accurate reconstruction from undersampled data. We refer to such methods as sparsity-exploiting methods. Under specific conditions on the sampling procedure, e.g., incoherence, there exist guarantees of perfect recovery Different types of sparsity are relevant in CT. For blood vessels [\cite=li2002accurate] the image itself can be considered sparse, and reconstruction based on minimizing the 1-norm can be expected to work well. The human body consists of well-separated areas of relatively homogeneous tissue and many materials, such as metals, consist of non-overlapping uniform sub-components. In these cases the image gradient is approximately sparse, and reconstruction based on minimizing total variation (TV) of the image [\cite=rudin1992nonlinear] is often a good choice. Empirical studies using simulated as well as clinical data, using standard highly structured, i.e., non-random, sampling patterns, have demonstrated that sparsity-exploiting methods indeed allow for reconstruction from fewer projections [\cite=Bian:10] [\cite=han2011algorithm] [\cite=ritschl2011improved] [\cite=SidkyTV:06] [\cite=sidky2008image].

However, as will be explained in Section [\ref=sec:guarantees], CS guarantees of recovery from undersampled data fall far short of extending to the sampling done in CT. Therefore, in spite of the positive empirical results, we still lack a fundamental understanding of conditions -- especially in a quantitative sense -- under which such methods can be expected to perform well in CT: How sparse images can be reconstructed and from how few samples?

The present paper demonstrates empirically an average-case relation between image sparsity and the sufficient number of CT projections enabling image recovery. Inspired by work of Donoho and Tanner [\cite=DonohoTanner:2009] we use computer simulations to empirically study recoverability within well-defined classes of sparse images. Specifically, we are interested in the average number of CT projections sufficient for exact recovery of an image as function of the image sparsity. To simplify our analysis we focus on images with sparsity in the image domain and reconstruction through minimization of the image 1-norm subject to a data equality constraint, as motivated by CS. These studies set the stage for forthcoming studies of other regularizers, such as TV, as well as other types of sparsity. We believe that our findings shed light on the connection between sparsity and sufficient sampling in CT and that they suggest the existence of a yet unknown theoretical foundation for CS in CT.

The paper is organized as follows. Section [\ref=sec:sparsityexploiting] presents the reconstruction problem of interest, describes relevant previous work and results from CS and discusses the application to CT. Section [\ref=sec:experimentaldesign] describes all aspects of the experimental design, including the CT imaging model, generation of sparse images, and how to robustly solve the optimization problem of reconstruction. Section [\ref=sec:numericalresults] contains our results establishing quantitatively a relation between image sparsity and sufficient sampling for recovery. Section [\ref=sec:noise] presents results for the noisy case and is followed by a discussion in Section [\ref=sec:discussion].

Sparsity-exploiting reconstruction methods

This section describes our choice of sparsity-exploiting reconstruction method and explains how existing recovery guarantees from CS do not prove useful in the setting of CT.

Reconstruction based on 1-norm minimization

We consider the discrete inverse problem of recovering a signal [formula] from data [formula]. The imaging model, which is assumed to be linear and discrete-to-discrete [\cite=Barrett:FIS], relates the image and the data through a system matrix [formula],

[formula]

where the elements of [formula] are pixel values stacked into a vector. We focus in the present work on the "undersampled" and consistent case where M < N such that [\eqref=eq:eqsystem] has infinitely many solutions. To determine a unique solution we consider the problem

[formula]

which seeks to recover the most sparse solution, i.e., the one with fewest non-zero components. More generally, one could consider other forms of regularization based on prior knowledge or assumptions to restrict the set of solutions. A common type of regularization takes the form:

[formula]

where J(x) is the regularizer. The regularization parameter ε controls the level of regularization and in the limit ε  →  0 we obtain the equality-constrained problem.

The inequality-constrained problem [\eqref=eq:ineqgeneralform] is of more practical interest than [\eqref=eq:L1] because it allows for noisy and inconsistent measurements, but its solution depends in a complex way on the noise and inconsistencies in the data, as well as the choice of the parameter ε. Studies of the equality-constrained problem, on the other hand, provide an basic understanding of a given regularizer's reconstruction potential, independent of specific noise. Furthermore, many of the initial CS recovery guarantees deal with the equality-constrained formulation, and as such the equality-constrained problem in CT is the most natural place to first attempt to establish recovery results in CT. Therefore, we focus for the major part of the present work on the equality-constrained problem, however we conclude with a brief study of robustness with respect to additive Gaussian white noise.

Existing recovery guarantees do not apply to CT

We call a tuple ([formula], A) a problem instance and say that [formula] is recoverable if the L1 solution with [formula] is identical to [formula]. A caveat is that L1 does not necessarily have a unique solution. The solution set may consist of a single image or an entire hyperface or hyperedge on the ball in case this set is aligned with the affine space of feasible points {x  ~  |  ~  Ax  =  b}. For being recoverable we therefore require that [formula] be the unique solution.

CS establishes guarantees of recovery of sparse signals from a small number of measurements. An important concept is the restricted isometry property (RIP): A vector [formula] with k non-zero elements is called k-sparse. A matrix A is satisfies the RIP of order k if a constant [formula] exists such that

[formula]

for all k-sparse vectors x. If the RIP-constant [formula] is small enough, then recovery of sparse signals is possible; more precisely, if [formula] then the L1 solution [formula] for data [formula] recovers the original image [formula] [\cite=candes2006stable]. There also exist RIP-based guarantees for TV [\cite=NeedellWard:SIR:13]. Such results are sometimes referred to as uniform or strong recovery guarantees [\cite=FoucartRauhut:2013] since recovery of all vectors of a given sparsity is ensured.

Unfortunately, the RIP is impractical to use because computing the RIP-constant for a given matrix is NP-hard [\cite=Tillmann2014]. RIP-constants have been established only for few, very restricted class of matrices, e.g., with Gaussian i.i.d. entries [\cite=candes2006robust]. In [\cite=SidkyPC:10] the authors computed lower bounds close to 1 on the RIP-constant for very low sparsity values for x-ray CT system matrices, thereby excluding the possibility of RIP-based guarantees for tomography for other than extremely sparse signals.

Other CS guarantees rely on incoherence [\cite=CandesRomberg2007], the spark [\cite=Donoho2003spark] or the null-space property [\cite=Cohen2009] but also fail to give useful guarantees, either due to being extremely pessimistic or NP-hard to compute, see, e.g., [\cite=Dossal:2010]. In fact, it is possible to construct examples of very sparse vectors that cannot be recovered from CT measurements [\cite=PustelnikEUSIPCO2012], implying that we cannot hope for uniform recovery guarantees for CT.

Instead, recoverability can be studied in an average-case sense. This can be motivated by a desire to ensure recovery of "typical" images of a given sparsity, but not all possible pathological images. Such results are sometimes referred to as non-uniform or weak recovery guarantees [\cite=FoucartRauhut:2013]. Donoho and Tanner [\cite=DonohoTanner:2009] derived weak recovery guarantees, i.e., established a relation between image sparsity and the critical average-case sampling level for recovery for example for Gaussian matrices. Their so-called phase diagram demonstrated agreement of the theoretical sampling level for a given sparsity with empirical recovery experiments by revealing a sharp phase transition between recovered and non-recovered images.

Weak recovery guarantees have been established for discrete tomography [\cite=PetraSchnoerr2014] using very restrictive sampling patterns (essentially only rays perfectly aligned with pixels) and for discrete-valued tomography [\cite=Gouillart2013]. These results, however, do not apply to the general case of x-ray CT, see [\cite=HermanKuba:1999] for background on discrete tomography. To our knowledge recovery guarantees for x-ray CT remain an open question.

Our contribution

In the present paper we empirically establish a quantitative relation between the number of measurements and the image sparsity sufficient for average-case recovery. We generate random images from specific classes of images motivated by tomographic applications and determine the average critical sampling level for recovery by L1 as function of image sparsity. Our empirical study is inspired by the Donoho-Tanner (DT) phase diagram, however we use a slightly modified diagram in which the quantities of our interest, namely sparsity and sampling, can be read directly off the axes.

With this approach we provide extensive empirical evidence of an average-case relation between sampling and sparsity occurring across different image classes, image sizes as well as showing robustness to noise.

Design of numerical experiments

In this section we describe our overall empirical study design after presenting the chosen imaging model, generation of sparse test images and numerical optimization for the reconstruction problem.

CT imaging geometry

We consider a typical 2D fan-beam geometry with [formula] circular source path and [formula] equi-angular projections, or views, onto a curved detector. We consider a square domain of [formula] pixels, and due to rotational symmetry we restrict the region-of-interest to be within a disk-shaped mask inside the square domain consisting of approximately [formula] pixels. The source-to-center distance is [formula], and the fan-angle [formula] is set to precisely illuminate the disk-shaped mask. The detector consists of [formula] bins, so the total number of measurements is [formula]. The M  ×  N system matrix A is computed by means of the MATLAB package AIR Tools [\cite=Hansen2012].

Sparse image classes

By an image class we mean a set of test images described by a set of specifications, such that we can generate random realizations from the class. We refer to such an image as an image instance from the class, and multiple image instances from the same class form an image ensemble.

For the spikes image class, given an image size N and a target sparsity k, we generate an image instance as follows: starting from the zero image, randomly select k pixel indices, and set each select pixel value to a random number from a uniform distribution over

[formula]

Robust solution of optimization problem

Given a numerically computed solution, the robustness of the decision regarding recovery depends on the accuracy of the solution. False conclusions may result from incorrect or inaccurate solutions. To robustly solve the optimization problem L1 we must therefore use a numerical method which gives a reliable indication of whether a correct solution, within a given accuracy, has been computed. Our choice is the package MOSEK [\cite=MOSEK], which uses a primal-dual interior-point method and issues warnings in the rare case that an accurate solution can not be computed. For all problem instances in our studies, MOSEK returned a certified accurate solution.

To solve L1 using MOSEK we recast it as the linear program

[formula]

where [formula] and the inequality constraints imply non-negativity of w.

Simulations

Using the presented imaging model, method for generating sparse test images, and robust optimization algorithm we carry out simulation studies of recoverability within an image class. A single recovery simulation consists of the following steps, assuming image sparsity k and number of projections [formula]:

Generate k-sparse test image [formula],

generate system matrix A using [formula] projections,

compute perfect data [formula],

solve L1 numerically to obtain [formula], and

test for recovery numerically using

[formula]

where the threshold τ is chosen based on the chosen accuracy of the optimization algorithm; empirically we found τ  =  10- 4 to be well-suited in our set-up.

We wish to study how the number of projections sufficient for L1 recovery depends on the image sparsity. In order to make comparisons across image size we introduce the following normalized measures of sparsity and sampling. For a given problem instance, we define the relative sparsity as

[formula]

In our studies we will generate test images of a desired relative sparsity κ and set the sparsity as [formula]. We let the sufficient projection number [formula] denote the smallest number of projections that causes A to have full column rank. At [formula] the original image is the unique feasible point and hence minimizer no matter what objective is minimized and we therefore use [formula] as a reference point of full sampling. For a given problem instance, the number of projections for L1 recovery [formula] denotes the smallest number of projections for which recovery is observed for all [formula]. We define the

[formula]

For the test problems considered here, existence of an L1 solution is guaranteed by the way we generate data. As mentioned in Section [\ref=sec:guarantees], uniqueness is not guaranteed and for a given problem instance it cannot be known in advance whether the solution is unique. The computed solution depends on the optimization algorithm, and therefore our conclusions of recoverability by L1 are, in principle, subject to our use of MOSEK. We do not specifically check for uniqueness; however, in the event of infinitely many solutions, it is unlikely that any optimization algorithm will select precisely the original image, so we believe that our observations of recoverability based on solving L1 correspond to existence of a unique solution.

Simulation results

In this section we present our numerical results. We first establish that L1 indeed can recover the original image from fewer than [formula] projections. We then systematically study how [formula] depends on the image sparsity, image size, image class and finally the robustness to noise.

Recovery from undersampled data

To verify that L1 is capable of recovering an image from undersampled CT measurements in our set-up, we use a spikes image [formula] with [formula], leading to N  =  3228 pixels in the disk-shaped mask. The relative sparsity is set to κ  =  0.20, which yields 646 non-zeros. We consider reconstruction from data corresponding to [formula] projections; the smallest and largest system matrices are of sizes [formula] and [formula], respectively. At [formula], the matrix is [formula] and [formula], at [formula] it is [formula] and has rank 3185, while at [formula], the matrix is [formula] and full-rank; hence [formula]. Selected L1 reconstructed images [formula]are shown in Figure [\ref=fig:somereconstructions] along with the error images [formula] to better visualize the abrupt drop in error when the image is recovered. L1 recovery occurs already at [formula], where A has size [formula] and rank 1524, i.e., a substantial undersampling relative to the full-sampling reference point of [formula].

To see how relative sparsity affects recovery, we repeat the experiment for κ  =  0.4, 0.6 and 0.8. Figure [\ref=fig:reconstruction_errors] shows the relative 2-norm error from [\eqref=eq:teststrongrecov] for the L1 reconstructions as a function of [formula]. In all cases an abrupt drop in error to a numerically accurate reconstruction is observed and the relative sampling for L1 recovery [formula] changes from [formula] at κ  =  0.2 to 16, 20 and 24. This indicates a very simple relation between sparsity and number of projections for L1 recovery.

Recovery phase diagram

In general, we can not expect all image instances of the same relative sparsity to have the same [formula]. We can study the variation within an image class by determining the number of projections for L1 recovery of an ensemble of images of different sparsity. At each of the relative sparsity values κ = 0.025, 0.05, [formula] we create 100 image instances of the spikes class. For each instance we compute the L1 reconstruction from [formula] projections. Based on the relative 2-norm error in [\eqref=eq:teststrongrecov] we assess whether the original is recovered. Figure [\ref=fig:averagecaserecovery] (left) shows the percentage of recovered instances as function of relative sparsity κ and relative sampling μ. Each square corresponds to the κ value at the left edge of the square and the μ value at the bottom edge. We refer to this plot as a phase diagram. The phase diagram shows two distinct regions: the lower right black one, in which no image instances were recovered, and the upper left white one, in which all images were recovered.

An important observation is the sharp phase transition from non-recovery to recovery, meaning that the number of projections for L1 recovery is essentially constant for same-sparsity images of the spikes class. A sharp phase transition is often seen in CS [\cite=DonohoTanner:2009], but to the best of our knowledge has not been reported for CT-matrices before, and we therefore believe the sharp transition to be a novel observation. The sharpness of the transition is perhaps better appreciated in Figure [\ref=fig:averagecaserecovery] (right), which shows the average [formula] over all instances at each κ and its 99% confidence interval estimated using the empirical standard deviation, illustrated by errorbars. The confidence intervals are very narrow, in fact, in several cases of width zero, due to zero variation of [formula], which agrees with the visual observation of a sharp transition.

The relative sampling for recovery [formula] increases monotonically with the relative sparsity κ. As κ  →  0, also [formula], showing that extremely sparse images can be recovered from very few projections, i.e., highly undersampled data. As κ  →  1, [formula], confirming that L1 does not admit undersampling for non-sparse signals. Furthermore, the phase diagram makes these observations quantitative. Assume, for example, that we are given an image of relative sparsity κ  =  0.1, how many projections would suffice for recovery? The phase diagram shows that at κ  =  0.1 the average [formula], corresponding to [formula] projections. Or conversely, the maximal relative sparsity that, on average, allows recovery from 8 projections is κ  =  0.1.

We note that the phase diagram introduced by Donoho and Tanner [\cite=DonohoTanner:2009] is slightly different from the one presented here. The DT diagram is parametrized by the sparsity fraction ρ  =  k  /  M, i.e. normalized by the number of measurements, not pixels, and undersampling fraction δ  =  M / N. We create a DT phase diagram for our set-up by assessing recovery for 100 instances at [formula], 4, [formula], 32 and [formula], see Figure [\ref=fig:dt_diagram]. The DT phase diagram confirms the observations from Figure [\ref=fig:averagecaserecovery], in particular the sharp phase transition.

We find that phase diagrams such as the one in Figure [\ref=fig:averagecaserecovery] are more intuitive to interpret because the quantities of interest, namely sparsity and sampling, can be read more directly of the axes. Furthermore, normalizing the sparsity by the number of samples as in the Donoho-Tanner phase diagrams leads to two minor issues: First, where in Figure [\ref=fig:averagecaserecovery] each column is based on 100 specific instances of a certain sparsity, each square in the Donoho-Tanner diagram contains 100 new instances, and hence sufficient sampling for specific instances is not addressed. Second, having M  ≥  N is common in CT, for example if using a large number of projections compared to the number of pixels, but as k cannot be larger than N, the upper right square cannot be realized, so they are marked by crosses. In summary, we consider phase diagrams of the former type more convenient in the setting of CT, and for the remainder of the paper we will only show this type of phase diagram.

Dependence on image size

To study how recoverability depends on image size, we construct the phase diagram for [formula], see Figure [\ref=fig:RSS_imagesize]. For [formula] we have [formula], so by taking [formula], 8, , 64 we obtain approximately the same relative sampling values as for [formula].

Overall, for [formula] we see the same monotone increase in [formula] with increasing κ as we did for [formula]. The only differences are a generally sharper transition, i.e., narrower confidence intervals, as well as slightly better recovery at the extreme κ-values. We conclude that, with appropriate normalization, the observed relation between the average number of projections for L1 recovery and the image sparsity does not depends on the image size. Moreover, [formula] is sufficiently large to give representative results that can be extrapolated to predict the sparsity-sampling relation for larger images.

Dependence on image class

Next, we study how the image class affects recoverability. Figure [\ref=fig:rss_ppower] shows phase diagrams for the 1-power and 2-power classes for [formula]. Comparing with spikes in Figure [\ref=fig:averagecaserecovery] we observe similar overall trends but also some differences, which are more clearly seen in the plot of the average [formula] for all classes in Figure [\ref=fig:rss_ppower] (right). For 1-power the transition from non-recovery to recovery occurs at almost exactly the same (κ,μ)-values, except around κ  =  0.2 where [formula] is slightly lower, and is almost as sharp as for the spikes class. For 2-power the transition is more gradual, and occurs at lower μ-values for the mid and upper range of κ.

Based on these results we conclude that image classes with increasing structure on average admit recovery from a smaller number of projections but the in-class recovery variability is larger. Thus, while recoverability is clearly tied to sparsity, the spatial correlation of the non-zero pixel locations also plays a role.

Robustness to noise

In the previous section we have empirically established a relation between the number of projections for L1 recovery and image sparsity in the noise-free setting. A natural question is whether and how the results generalize in the case of noisy data. Noise and inconsistencies in CT data are complex subjects arising from many different sources including scatter and preprocessing steps applied to the raw data before the reconstruction step. A comprehensive CT noise model is very application-specific and beyond our scope; rather we wish to investigate how recovery changes when subject to simple Gaussian white noise.

We consider the reconstruction problem

[formula]

which can be solved in MOSEK by introducing a quadratic constraint:

[formula]

We model each CT projection to have the same fixed x-ray exposure by letting the data in each projection bp, [formula] be perturbed by an additive zero-mean Gaussian noise vector ep of constant magnitude [formula], [formula]. Hence, the noisy data are [formula], where e is the concatenation of noise vectors for all projections. We use three noise levels, δ  =  10- 4, 10- 2, 100, corresponding to relative noise levels [formula] of 0.00016%, 0.016% and 1.6%. We reconstruct using L1[formula] with [formula] and show the relative reconstruction errors from [\eqref=eq:teststrongrecov] in Figure [\ref=fig:noisy_reconstruction_errors].

For δ  =  10- 4 and 10- 2 the abrupt error drop when the image is recovered is observed at the same number of projections as in the noise-free case. The limiting reconstruction error is now governed by the choice of δ and not by the numerical accuracy of the algorithm as in the noise-free case. For the high noise level of δ = 100 no abrupt error drop can be observed. However, the reconstruction error does continue to decay after the number of projections for L1 recovery seen at the lower noise levels and approach a limiting level consistent with the lower noise-level error curves.

In order to set up phase diagrams we must choose appropriate thresholds τ to match the limiting reconstruction error at each noise level. In the noise-free case we used τ = 10- 4 chosen to be roughly the midpoint between the initial and limiting errors of the order of 100 and 10- 8, respectively. Using the same strategy we obtain thresholds 10- 2.5, 10- 1.5, 10- 0.5 for increasing noise level δ. We determine the phase diagrams and for brevity and ease of comparison we only show the average sampling plots in Figure [\ref=fig:noisy_reconstruction_errors]. The low-noise phase transition is essentially unchanged from the noise-free case. With increasing noise level we see that the location of the transition is gradually shifted to higher μ values for the medium and large κ values. At the high noise level and the largest κ  =  0.9 we even see that there are instances that are not recovered (to the chosen threshold τ) at [formula].

We conclude that the sparsity-sampling relation revealed by the phase diagram in the noise-free case is robust to low levels of Gaussian noise. For medium and high levels of noise, the phase diagram shows that a sharp transition continues to hold (for the particular noise considered) but the location of the transition changes to require more data for accurate reconstruction.

Discussion

Our simulation studies for x-ray CT show that for several image classes with different sparsity structure it is possible to observe a sharp transition from non-recovery to recovery in the sense that, on average, same-sparsity images require essentially the same sampling for recovery. This is similar to what is observed in CS, but as explained in Section [\ref=sec:guarantees] no theory predicts that this should be the case for x-ray CT. Based on the empirical evidence we conjecture that an underlying theoretical explanation exists.

The use of a robust optimization algorithm limits the possible image size; with MOSEK, we found [formula] to be impractical. Faster algorithms applicable to larger problems exist, however in our experience MOSEK is extremely robust in delivering an accurate L1 solution, and the use of less robust optimization software may affect the decision of recoverability. Furthermore, we found the relation between relative sparsity and relative sampling to hold independently of image size, so we expect that larger images can be studied indirectly through extrapolation.

The present work considers an idealized CT system, by focusing on recovery with L1 rather than L1[formula], and as such the quantitative conclusions are only valid for the specific CT geometry and image classes. Nevertheless we believe that our results can provide some preliminary guidance on sampling in a realistic CT system, as Section [\ref=sec:noise] indicates robustness of the relation between sparsity and sampling.

Future work

The phase diagram allows for generalization to increasingly realistic set-ups. For example, more realistic image classes, sparsity in, e.g., gradient or wavelet domains, and other types of noise and inconsistencies can be considered by changing the optimization problem accordingly. We expect that such studies will lead to improved understanding of the role of sparsity in CT.

Our earlier studies of TV reconstructions [\cite=Joergensen_TMI:2013] indicate a relation between the sparsity level in the image gradient and sufficient sampling for accurate reconstruction, but due to the complexity of the test problems in that study we found it difficult to establish any quantitative relation. An investigation based on the phase diagram could provide more structured insight. For instance, we might learn that TV reconstructions of a class of "blocky" or piecewise constant images exhibit a well-defined recovery-curve similar to the ones in the present study.

With the present approach we always face the problem of possible non-unique solutions to L1, leading to phase diagrams that, in principle, depend on the particular choice of optimization algorithm. We expect that uniqueness of the L1 solution can be studied by numerically verifying a set of necessary and sufficient conditions [\cite=Grasmair2011]. We did not pursue that idea in the present work in order to focus on an empirical approach easily generalizable to other penalties, such as TV, for which similar unique conditions may not be available.

Finally, it would be interesting to study the in-class recovery variability, i.e., why the 2-power class transition from non-recovery to recovery is more gradual. Can differences be identified between instances that were recovered and ones that were not, e.g., in the spatial location of the non-zero pixels? In [\cite=PetraSchnoerr2014] it is found that the number of zero-measurements affects recoverability. In our case, the structure of 2-power leads to a higher and more variable number of zero-measurements, which might be connected with the larger in-class recovery variability.

Conclusion

We demonstrated empirically in extensive numerical studies a pronounced average-case relation between image sparsity and the number of CT projections sufficient for recovering an image through L1 minimization. The relation allows for quantitatively predicting the number of projections that, on average, suffices for L1-recovery of images from a specific class, or conversely, to determine the maximal sparsity that can be recovered for a certain number of projections.

The specific relation was found to depend on the image class with a smaller, but also more variable, number of projections sufficing for an image class of more structured images. Classes of less-structured images were found to exhibit a sharp phase transition from non-recovery to recovery. We further demonstrated empirically that the sparsity-sampling relation is independent of the image size and robust to small amounts of additive Gaussian noise.

With these initial results we have taken a step toward better quantitative understanding of undersampling potential of sparsity-exploiting methods in x-ray CT.

Acknowledgments

This work was supported in part by Advanced Grant 291405 "HD-Tomo" from the European Research Council and by grant 274-07-0065 "CSI: Computational Science in Imaging" from the Danish Research Council for Technology and Production Sciences. JSJ acknowledges support from The Danish Ministry of Science, Innovation and Higher Education's Elite Research Scholarship. This work was supported in part by NIH R01 grants CA158446, CA120540 and EB000225. The contents of this article are solely the responsibility of the authors and do not necessarily represent the official views of the National Institutes of Health.