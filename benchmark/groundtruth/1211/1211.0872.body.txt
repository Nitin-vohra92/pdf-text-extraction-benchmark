Lemma Definition Problem Proposition Corollary Remark Example Question

Phase Retrieval: Stability and Recovery Guarantees

Shahar Mendelson

Introduction

Recently there has been growing interest in recovering an input vector [formula] from quadratic measurements

[formula]

where wi is noise, and ai are a set of known vectors. Since only the magnitude of [formula] is measured, and not the phase (or the sign, in the real case), this problem is referred to as phase retrieval. Phase retrieval problems arise in many areas of optics, where the detector can only measure the magnitude of the received optical wave. Several important applications of phase retrieval include X-ray crystallography, transmission electron microscopy and coherent diffractive imaging [\cite=Q10] [\cite=H01p] [\cite=H93] [\cite=W63].

Many methods have been developed for phase recovery [\cite=H01p] which often rely on prior information about the signal, such as positivity or support constraints. One of the most popular techniques is based on alternating projections, where the current signal estimate is transformed back and forth between the object and the Fourier domains. The prior information and observations are used in each domain in order to form the next estimate. Two of the main approaches of this type are Gerchberg-Saxton [\cite=GS72] and Fienup [\cite=F82]. In general, these methods are not guaranteed to converge, and often require careful parameter selection and sufficient prior information.

To circumvent the difficulties associated with alternating projections, more recently, phase retrieval problems have been treated using semidefinite relaxation, and low-rank matrix recovery ideas [\cite=CESV12] [\cite=SESE11]. In [\cite=CESV12] several masks where used in the measurement process in order to ensure the ability to retrieve the phase. Another approach to generate robust solutions is to assume that the input signal x is sparse, namely, that it contains only a few non-zeros values in an appropriate basis expansion. Sparsity has long been exploited in signal processing, applied mathematics, statistics and computer science for tasks such as compression, denoising, model selection, image processing and more. Despite the great interest in exploiting sparsity in various applications, most of the work to date has focused on recovering sparse or low rank data from linear measurements [\cite=EK12] [\cite=D06] [\cite=CRT06a]. Recently, the basic sparse recovery problem has been generalized to the case in which the measurements are quadratic [\cite=SESE11], or given by a more general nonlinear transform of the unknown input [\cite=BE12]. The first paper to consider sparse phase retrieval was [\cite=SESE11], based on semidefinite relaxation combined with a row-sparsity constraint on the resulting matrix. An iterative thresholding algorithm was then proposed that approximates the solution. Similar approaches were later used in [\cite=JOH12] [\cite=OYDS12]. An alternative algorithm was recently designed in [\cite=BE12] [\cite=SBE12] using a greedy search method which is far more efficient than the semidefinite relaxation, and often yields more accurate solutions.

Despite the vast interest in phase retrieval, there has been little theoretical work on the fundamental limits of this problem. One important question in this context is how many measurements are needed in order to ensure robust recovery of the input x, regardless of the specific recovery method used. Several recent works treat this problem. Most of the papers discuss the case in which x is a general input, namely, there is no sparsity (or other) constraint on x. The first result of this kind was obtained in [\cite=BCE06], where it is shown that with probability one N = 4n - 2 randomized equations are sufficient for recovery using a brute force (intractable) method, when there is no noise. However, it is not clear whether a stable recovery method exists with this number of measurements. In [\cite=CL12] [\cite=CSV12] the authors consider the case in which ai are real or complex vectors that are either uniform on the sphere of radius [formula], or iid zero-mean Gaussian vectors with unit variance. Under these assumptions they show that on the order of n measurements are needed in order to recover a generic x using a semidefinite relaxation approach. In the presence of noise, it is shown in [\cite=CL12] that one can find an estimate x̂ satisfying

[formula]

for some φ, where C0 is a constant and w is the noise vector that is assumed to be bounded so that [formula] is finite.

The paper [\cite=LV12] treats the case in which the input x is k-sparse and ai are iid zero-mean normal vectors. When there is no noise, they show that in the real case N  ≥  4k - 1 measurements are needed for uniqueness and in the complex case, N  ≥  8k - 2 measurements are required. They further prove that if N is on the order of k2 log n then the solution can be obtained using a sparse semidefinite relaxation approach as in [\cite=SESE11] [\cite=JOH12].

Here we treat the real case and random measurements, using reasonable ensembles. Our methods may be extended to the complex case (again, using real, random measurements), but since the core of the problem is the real case, we will restrict our analysis to it. For this setting, we develop conditions leading to stable uniqueness, namely, conditions that ensure a unique solution can be found in a stable way. We do not restrict ourselves to a certain class of inputs, but rather allow for general input sets T which include, as special cases, [formula] and the class of sparse vectors. It turns out that to ensure stable uniqueness for a given set of signals, a very natural notion of complexity of the set determines the number of data points required. For example, we show that for k-sparse vectors, O(k log (n / k)) measurements are needed for stability. This result is better by a factor of k than the estimate from [\cite=LV12] that guarantees recovery (without noise) using a semidefinite approach. When x can be any vector in [formula], we show that O(n) measurements suffice, which is also the bound derived in [\cite=CL12] for recovery using semidefinite relaxation.

It turns out that the natural complexity parameter for this problem is the same as the one used for analyzing stability in linear measurements, as we will discuss in Section [\ref=sec:linear]. Thus, in a rather general sense, the number of measurements required for stable recovery in the quadratic setting we treat here is of the same order of magnitude as the one needed to ensure stability under linear sampling. In that sense there is no substantial price to be paid for not knowing the phase of the measurements, and for very general choices of input sets T.

The second main result of this article deals with the noisy phase retrieval problem. More specifically, we consider recovering an input x in a set T from noisy measurements of the form ([\ref=eq:meas]). A straightforward approach is to seek the value of x that minimizes the empirical risk (or a least-squares approach). Since this leads to a nonconvex problem, finding its global solution is in general not possible. Nonetheless, we show that if one can find a value x̂ for which the empirical risk is bounded by a given, computable constant (which depends on the set T), then [formula] is bounded above by an expression that once again depends on the complexity parameter of the set, and which converges to 0 faster than N- 1 / 2 + δ for any δ > 0. Here x0 is the true (unknown) input. The complexity parameter that determines the rate in the noisy setting is essentially the same as in the stability analysis. Moreover, the resulting sample complexity is of the same order of magnitude as in the linear case in the examples of sets T we consider. An exact formulation of both main results is presented in the next section.

An important practical conclusion from this analysis is that although the squared-error in the case of nonlinear measurements as in ([\ref=eq:meas]) cannot be minimized directly, it is sufficient to find a point for which the error is bounded by a known constant. Thus, one may use any desired recovery algorithm and check whether the solution x̂ satisfies the bound. For this purpose, methods such as those developed in [\cite=SBE12] are advantageous since they allow for arbitrary initial points. As different initializations lead to different choices of x̂, the algorithm can be used several times until an appropriate value of x̂ is found. Our theoretical results ensure that such an x̂ is sufficiently close to x0 or to - x0 if enough measurements are used. In particular, for k-sparse vectors one can guarantee stable recovery from O(k log (n / k) log k) noisy measurements, and when x can be any vector in [formula], O(n log n) noisy measurements suffice.

The reminder of the article is organized as follows. The problem and the main results are formulated in Section [\ref=sec:main]. Stability results in the noise-free setting are developed in Section [\ref=sec:noisef], while the noisy setting is treated in Section [\ref=sec:noise]. In Section [\ref=sec:linear] the relation between the results in the quadratic case and those in the linear setting are discussed.

Throughout the article we use the following notation. The statistical expectation is denoted by [formula], and if the probability space is a product space [formula], [formula] and [formula] are the conditional expectations. If X is a random variable, then [formula]. The relation a  ~  b means that a is equal to b up to absolute multiplicative constants, i.e., that there are c and C, independent of a,b or any other parameters of the problem, for which ca  ≤  b  ≤  Ca. The inequality [formula] means that a  ≤  Cb for some constant C. We use [formula] to denote the fact that the constant C depends only on L and γ.

Problem Formulation and Main Results

Suppose one is given measurements yi as in ([\ref=eq:meas]). Let s be a vector in [formula], set φ(s) to be the length-N vector with elements |si|2 and put [formula]. With this notation, ([\ref=eq:meas]) can be written as

[formula]

Our goal is to study conditions under which stable recovery is possible irrespective of the specific recovery method used, and to develop guarantees that ensure that empirical minimization or approximate empirical minimization (namely, least-squares recovery) lead to an estimate x̂ that is close to x in a squared-error sense.

Assumptions on x and a

We assume throughout that x lies in a subset T of [formula], which can be arbitrary. It is natural to expect that the number of measurements needed for stable recovery or for noisy recovery depend on the set T, though the way in which it depends on T is not obvious. Here, we prove that this number is a function of a natural complexity parameter of T, in an estimate that is sharp in the stable recovery problem, and sharp up to logarithmic factors in the noisy recovery problem.

The assumption on the measurement vectors ai is that they are independent, and distributed according to a probability measure μ on [formula] that is isotropic and L-subgaussian [\cite=LAMA] [\cite=VW] [\cite=BK00]:

Let μ be a probability measure on [formula] and let a be distributed according to μ. The measure μ is isotropic if for every [formula], [formula]. It is L-subgaussian if for every [formula] and every u  ≥  1, [formula].

Among the examples of isotropic, L-subgaussian measures on [formula] for a constant L that is independent of the dimension n are the standard Gaussian measure, the uniform measure on { - 1,1}n and the volume measure on the "correct" multiple of the unit ball of [formula] for 2  ≤  p  ≤    ∞   (that is, the volume measure on cnn1 / pBnp, where cn  ~  1). Also, if X is a mean-zero, variance 1 random variable that satisfies Pr(|X|  ≥  Lu)  ≤  2 exp ( - u2 / 2), then a vector of iid copies of X is isotropic and cL subgaussian, for a suitable absolute constant c.

More generally, we have the following:

If F is a class of functions on a probability space (Ω,μ), then it is L-subgaussian if for every [formula] and every t  ≥  1

[formula]

where X is distributed according to μ.

It is standard to verify if μ is an L-subgaussian measure on [formula] then every class of linear functionals on [formula] is L-subgaussian (see, e.g. [\cite=LAMA]).

Our goal is to study when the mapping φ(Ax) is both invertible and stable first, when w = 0 (the noise-free case) and second, in the presence of noise.

Stability Results

We begin in Section [\ref=sec:noisef] by treating the noise-free setting. Since one is given only the absolute values of Ax, it is impossible to distinguish x and - x. Therefore, uniqueness will always be up to the sign of x. If φ(Ax) is an invertible stable mapping, it is natural to expect that for any s and t for which s  ≠  t and s  ≠   - t, φ(As) is far enough from φ(At) in some sense; here we consider the [formula] sense.

The mapping φ(Ax) is stable with a constant C in a set T if for every s,t∈T,

[formula]

Note that stability in a set is a much stronger property than invertibility. Indeed, for the latter it suffices that if [formula] then [formula], but without any quantitative estimate on the difference.

The [formula] norm, used on the left-hand side, is the natural way of measuring distances for the quadratic function φ, if one wishes to compare the results with the linear case, in which the [formula] distance is used (see Section [\ref=sec:linear] for more information). Using the [formula] distance also has a technical advantage, as it simplifies the analysis considerably. Measure distances based on other [formula] norms lead to processes that are much harder to control, since higher powers emphasize the "unbounded" or "peaky" parts of a random variable, and make concentration around the mean much harder.

To formulate our stability result, let us define the main complexity parameter required. For [formula], define

[formula]

Let (gi)ni = 1 be independent Gaussian random variables, that have mean zero and variance 1. Set

[formula]

and put

[formula]

Throughout this section, we will refer to ρT,N as the complexity measure of T.

The main result in the noise free case is the following:

For every L  ≥  1 there exist constants c1,c2 and c3 that depend only on L for which the following holds. Let μ be an isotropic, L-subgaussian measure. Then, for u  ≥  c1, with probability at least 1 - 2 exp ( - c2u2 min {N,E2}), for every s,t∈T,

[formula]

where for every [formula],

[formula]

To put Theorem [\ref=thm:main-random] in the right perspective, one has to obtain lower bounds on κ(s - t,s + t) and upper bounds on ρT,N. Since the latter depends on the number of measurements N, its behavior provides insight into the number of measurements that are needed for stability.

The value of κ(v,w) may be bounded using several methods, as we will explain in Section [\ref=sec:kappa]. One natural example in which inf v,w∈Sn - 1κ(v,w) is bounded from below (where Sn - 1 is the unit Euclidean sphere in [formula]), is when a satisfies a small-ball assumption, namely, that for every [formula] and every ε > 0,

[formula]

It turns out that if [\eqref=eq:small-ball] holds, then inf v,w∈Sn - 1κ(v,w)  ≥  c1, where c1 only depends on the constant c in [\eqref=eq:small-ball]. This assumption is satisfied for a large family of measures, such as the Gaussian measure on [formula] (see Section [\ref=sec:kappa] for more details).

As for ρT,N, we will show, for example, that if T is the set of k-sparse vectors in [formula], then [formula]. Hence, under [\eqref=eq:small-ball], since [formula], it suffices to select N large enough to ensure that c2u3ρT,N  ≤  c1 / 2 to obtain a stability result. This leads to the following estimate:

For every L,c > 0 there exist absolute constants c1, c2,c3 and c4 for which the following holds. Let T be the set of k-sparse vectors in [formula], set μ to be an isotropic, L-subgaussian measure, and assume that a is distributed according to μ. If a satisfies [\eqref=eq:small-ball] with constant c, then for u > c1 and N  ≥  c2u3k log (en / k), with probability at least 1 - 2 exp ( - c3u2k log (en / k)), for every s,t∈T

[formula]

In particular, the result is true for a random Gaussian matrix A, where c1,c2,c3 and c4 are absolute constants.

Interestingly, it can be shown that in the case of linear measurements, stable recovery is guaranteed as long as N  ~  k log (en / k). Thus, the number of measurements needed for stable recovery in the nonlinear and linear settings is the same up to multiplicative constants - at least for ensembles that have a well behaved inf v,w∈Sn - 1κ(v,w). As mentioned in the introduction, this observation is not a coincidence and will be explained in more detail in Section [\ref=sec:linear].

In Section [\ref=sec:examples] we study other choices of T, and the number of measurements needed in order to guarantee stability.

Noisy Recovery Results

Section [\ref=sec:noise] is devoted to the case in which the measurements are contaminated with iid noise. The goal is to find a point x̂ for which [formula] is small, using the data (ai,yi)Ni = 1 and the fact that y is generated according to ([\ref=eq:meas]) for some x0∈T.

A natural approach is to recover x0 from y by minimizing the empirical risk:

[formula]

for some p > 1. The objective in ([\ref=eq:erm]) is not convex, and therefore it is not clear how to find the value x0 minimizing ([\ref=eq:erm]). Fortunately, we prove that in order to find an estimate x̂ close to x0 one does not need to strictly minimize ([\ref=eq:erm]). Instead, it is sufficient to find a point x̂ for which the empirical risk [formula] is small enough.

Given a set [formula], let

[formula]

be the Gaussian complexity of T, where g1,...,gn are iid standard Gaussian variables, and put

[formula]

From the geometric point of view, [formula] measures the best correlation (or width) of T in a random direction generated by the random vector G = (g1,...,gn). This parameter appears in many different areas of mathematics, and is also essential in the study of compressed sensing problems (see, for example [\cite=LAMA]). We refer the reader to the books [\cite=LT] [\cite=Tal] [\cite=Pis] [\cite=MS] for more information on this parameter and for methods of computing it. For example, it is well known that [formula] can be bounded from above and below (with a possible [formula] gap between the upper and lower bounds) using of the [formula] covering numbers of T.

Suppose that for a given 1 < p  ≤  2, and u  ≥  1 (which will later on govern our probability estimates), one produces x̂ satisfying

[formula]

Here [formula], 1  ≤  α  ≤  2 is a measure of the decay properties of the noise, and will be defined formally in ([\ref=sec:noised]), QT,N is a complexity measure similar to ρT,N (defined formally in ([\ref=eq:qtn])), and [formula]. Our main result shows that, with high probability, such a point x̂ is close to either x0 or to - x0. To find an appropriate x̂, it is possible, for example, to use the greedy method of [\cite=SBE12] with different starting points and stop once a solution that satisfies the bound is found.

For every κ > 0 and every L  ≥  1 there exists constants c1,c2,c3 and c4 that depend only on L and κ, for which the following holds. Let a be distributed according to an isotropic, L-subgaussian measure, and assume that κT  ≥  κ where κT  =   inf s,t∈Tκ(s,t). Assume further that [formula]. For every integer N set

[formula]

and

[formula]

Let x̂ be chosen to satisfy [\eqref=eq:hax-tt]. Then, for u  ≥  c2, with probability at least 1 - 2 exp ( - c3u1 / 3),

[formula]

Theorem [\ref=thm:noisy-maint] shows that stable recovery is possible if [formula]. In particular, for k-sparse vectors, stable recovery is possible from O(k log (n / k) log k) noisy measurements (this estimate is off only by a log k factor from the optimal estimate in the linear case), and when x0 can be any vector in [formula], O(n log n) noisy measurements suffice.

Technical Tool

The main technical tool needed in the proof of both main results is a general estimate on properties of empirical processes indexed by {fh:f∈F, h∈H}. Although the result is true in a far more general situation than needed here, for the sake of simplicity we will present it only in the cases required. We refer the reader to [\cite=Men-orc] for the more general statement and precise results.

In the cases considered here, F and H are classes of linear functionals or of absolute values of linear functionals on [formula], which is endowed with an isotropic, L-subgaussian probability measure μ. For the stability result [formula] and [formula], while in the noisy case, [formula] and [formula]. In both scenarios, the two indexing sets are denoted by [formula].

[\cite=Men-orc] For every L  ≥  1 there are constants c1,c2,c3 and c4 that depend only on L and for which the following hold. Let [formula] of cardinality at least 2 and set F and H to be the corresponding classes as above, respectively. Assume without loss of generality that [formula]. Then, for every u  ≥  c1, with probability at least

[formula]

[formula]

In particular, for every q  ≥  2,

[formula]

Stability Results

In this section we present the proof of Theorem [\ref=thm:main-random], followed by estimates on the values of κ(v,w) and ρT,N appearing in the theorem.

Proof of Theorem [\ref=thm:main-random]

Observe that

[formula]

Therefore, to establish the desired stability result, it suffices to show that

[formula]

i.e., that

[formula]

where

[formula]

Since [formula], if κ(s - t,s + t) is very small, then a random selection of ai is unlikely to lead to [\eqref=eq:stable-alt]. Therefore, a reasonable pre-requisite for a stability result is that [formula] is bounded away from zero. Indeed, with this assumption, one may obtain a stability result.

For every L  ≥  1 there exist constants c1,c2 and c3 that depend only on L for which the following holds. Let μ be an isotropic, L-subgaussian measure on [formula] and set a to be a random vector distributed according to μ. Then, for u  ≥  c1, with probability at least 1 - 2 exp ( - c2u2 min {N,E2}), for every s,t∈T,

[formula]

Observe that

[formula]

By Theorem [\ref=thm:main-emp-est] for [formula] and [formula], it follows that if N  ≥  c1E2 and u  ≥  c2 then with probability at least 1 - 2 exp ( - c3u2E2),

[formula]

The claim now follows immediately from the definition of zs,t and of κ(s - t,s + t).

Computing κ and ρT,N

Bounding ρT,N

It is well known that if [formula] then [formula] (and therefore, ρT,N as well) is determined by the Euclidean metric structure of T. This is the outcome of the celebrated majorizing measures/generic chaining theory (see the books [\cite=LT] [\cite=Dud-book] [\cite=Tal] for a detailed exposition on this topic). In the examples we present here, the following estimate, which is, in general, suboptimal, suffices.

Let (T,d) be a compact metric space. For every ε > 0, let N(T,d,ε) be the smallest number of open balls of radius ε needed to cover T. The numbers N(T,d,ε) are called the ε-covering numbers of T relative to the metric d.

Given [formula], set [formula], i.e., the covering numbers relative to the Euclidean metric.

There exist absolute constants c and C for which the following holds. If [formula] then

[formula]

The upper bound is due to Dudley [\cite=Dud67] and the lower to Sudakov [\cite=Sud]. The proof of both bounds may be found, for example, in [\cite=LT] [\cite=Pis] [\cite=Dud-book].

It is straightforward to verify that the gap between the upper and lower bounds in Proposition [\ref=prop:rho-cover] is at most [formula], and in all the examples we study below, the resulting estimate is sharp.

Bounding κ

Here, we present two simple methods for bounding inf v,w∈Sn - 1κ(v,w) from below. These methods are not the only possibilities by which one may obtain such a bound; rather, they serve as an indication that the assumption on κ is less restrictive than may appear at first glance.

Recall the small ball assumption: for every [formula] and every ε > 0, [formula].

If a satisfies the small ball assumption with constant c then

[formula]

where κ depends only on c.

Proof.   Consider ε for which cε  ≤  1 / 4. Then for every v∈Sn - 1, there is an event of measure at least 3 / 4 on which [formula]. Hence, for two fixed vectors v,w∈Sn - 1,

[formula]

and thus [formula].

This type of small ball property is true in many case. The simplest example is the standard gaussian measure on [formula]. Indeed, if a = (g1,...,gn) then [formula] is distributed as [formula] and the small ball property follows immediately by applying the L∞ estimate on the density of g.

A more general example is based on the notion of log-concavity. A measure μ on [formula] is called log-concave if for every nonempty, Borel measurable sets [formula], and any 0  ≤  λ  ≤  1, μ(λA  +  (1 - λ)B)  ≥  μλ(A)μ1 - λ(B), where λA + (1 - λ)B  =  {λa  +  (1 - λ)b:a∈A,b∈B}. It is well known that μ is a log-concave measure if and only if it has a density of the form exp (φ) for a concave function [formula].

The following lemma is standard (see e.g. [\cite=Gia] [\cite=Bob] [\cite=MP]).

There exists an absolute constant c for which the following holds. Let a be distributed according to an isotropic, symmetric, log-concave measure. Then for every θ∈Sn - 1, [formula] is distributed according to an isotropic, symmetric, log-concave measure on [formula]. Also, if fθ is the density of [formula] then [formula].

The desired small-ball estimate clearly follows from the lemma, since

[formula]

Among the family of log-concave measures are volume measures on convex symmetric bodies (i.e. measures that have a constant density on the body and zero outside the body). Moreover, it can be shown (see, e.g. [\cite=Gia]), that for every convex body [formula] there is an invertible linear operator G for which the volume measure on GK is also isotropic.

Another example of log-concave measures on [formula] are product measures of log-concave measures on [formula]. If X is a real valued, symmetric, log-concave random variable (i.e. with a log-concave density) with variance one, and X1,...,Xn are iid copies of X, then a = (X1,...,Xn) is an isotropic log-concave measure on [formula]. Standard examples for log-concave measures on [formula] are those with a density proportional to exp ( - cp|t|p) for p  ≥  1. And, of course, any measure with density proportional to exp (φ) for a convex function φ is log-concave.

The second method, which we only outline, is based on the Paley-Zygmund argument.

[\cite=GdlP] Let Z be a random variable, set 0 < p < q and put [formula]. Then, for every 0  ≤  λ  ≤  1,

[formula]

and in particular, [formula], where c1 depends only on p,q and cp,q.

We will use the lemma for p = 2 and q > 2. Assume that (Xi)ni = 1 are iid copies of a symmetric, variance 1 random variable and set a = (X1,...,Xn). If v,w∈Sn - 1, then a straightforward computation shows that

[formula]

Using the fact that [formula], [\eqref=eq:moment-computation-1] reduces to

[formula]

Consider two cases. First, if [formula], and since [formula], then by [\eqref=eq:moment-computation-2],

[formula]

On the other hand, if the reverse inequality holds, then using

[formula]

and applying [\eqref=eq:moment-computation-1],

[formula]

Let X be a symmetric, variance 1 random variable, with a finite L2q moment for some q > 2. If a = (X1,...,Xn) then

[formula]

where c depends on q and on [formula].

Observe that the two assumptions we make are not very restrictive, since we assume throughout that a is isotropic and L-subgaussian. Hence, if a = (X1,...,Xn), then [formula] for every q  ≥  2 (see Section [\ref=sec:prelim]). Also note that for any random variable X, [formula], so that the square-root is well defined.

Proof.   Assume that X∈L2q for some q > 2. Observe that if v∈Sn - 1 then for every 2  ≤  r  ≤  2q, [formula]. Indeed, by a Rosenthal type inequality (see, e.g. [\cite=GdlP], Section 1.5),

[formula]

Since [formula] and [formula], the claim follows.

Therefore, [formula], and thus,

[formula]

Let [formula]. Using the notation of Lemma [\ref=Lemma:Paley-Zygmund], [formula]. Hence, for every v,w∈Sn - 1, [formula], where c depends only on q and on [formula], as claimed.

Observe that if [formula], it is possible that [formula], even for v,w of the specific form one would like to control - namely, [formula] and [formula] for [formula]. Indeed, let X be a symmetric, { - 1,1}-valued random variable (and in particular, it is L-subgaussian as well). Let (ei)ni = 1 be the standard basis in [formula] and set s = e1, t = e2. It is straightforward to verify that in this case, [formula] with probability 1, and therefore, the assumption on [formula] can not be relaxed if one is interested in a uniform bound.

Examples

Let us turn to a few special cases of Theorem [\ref=thm:main-random]. To that end, explicit expressions for ρT,N are required for the sets of interest.

Entire Space [formula]

If [formula] then T+ = T- = Sn - 1, where Sn - 1 is the Euclidean unit sphere in [formula]. Therefore,

[formula]

implying that

[formula]

For every L  ≥  1 there are constants c1, c2 and c3 that depend only on L and for which the following holds. If inf v,w∈Sn - 1κ(v,w)  ≥  κ, u  ≥  c1 and N  ≥  c2u3n / κ2, then with probability at least 1 - 2 exp ( - c3un), for every [formula],

[formula]

The corollary follows from the fact that with this choice of N, cu3ρT,N is proportional to κ / 2.

When κ is given by a constant, independent of the dimension n, Corollary [\ref=cor:examples-entire-space] implies that it is sufficient to choose N  ~  n to ensure stable recovery with high probability.

Sparse Vectors

Let T = Sk, the set of k-sparse vectors in [formula], put [formula] and observe that T+,T-  ⊂  U2k. Therefore,

[formula]

where (v*i)ni = 1 is a monotone rearrangement of (|vi|)ni = 1. It is standard to check (see, e.g., [\cite=GLMP]) that there is an absolute constant c such that for every 1  ≤  k  ≤  n / 4,

[formula]

Therefore,

[formula]

For every L  ≥  1 there are constants c1, c2 and c3 that depend only on L and for which the following holds. If inf v,w∈Ukκ(v,w)  ≥  κ, u  ≥  c1 and N  ≥  c2u3k log (en / k) / κ2, then with probability at least 1 - 2 exp ( - c3uk log (en / k)), for every s,t∈Sk,

[formula]

When κ is an absolute constant, Corollary [\ref=cor:examples-entire-space] implies that it is sufficient to choose N  ~  k log (en / k) to ensure stable recovery with high probability.

Finite Set

Assume that T is a finite set. Then T+,T-  ⊂  Sn - 1 are of cardinality at most |T|2. A straightforward application of the union bound to each random variable [formula] shows that if [formula] is a finite set, then

[formula]

Therefore, [formula], implying that

[formula]

For every L  ≥  1 there are constants c1, c2 and c3 that depend only on L and for which the following holds. If inf v,w∈T+κ(v,w)  ≥  κ, u  ≥  c1 and N  ≥  c2u3 log |T| / κ2, then with probability at least 1 - 2 exp ( - c3u log |T|), for every s,t∈T,

[formula]

In this case, with constant κ, N  ~   log |T| measurements ensure stable recovery with high probability.

Block Sparse Vectors

We next treat the case in which T = Sdk consists of block sparse vectors of size d [\cite=EKB10] [\cite=EM09a]. Let [formula], [formula] be a decomposition of {1,...,n} to disjoint blocks of cardinality d. Set Wk to be the set of vectors in the unit sphere, supported on at most k blocks. Then T+,T-  ⊂  W2k, and it remains to estimate

[formula]

There exist absolute constants c1 and c2 for which the following holds. For every 0 < ε < 1 / 2,

[formula]

Therefore,

[formula]

Proof.   Let IJ  =  {i∈Ij, j∈J} and observe that

[formula]

where for every I  ⊂  {1,...,n}, SI is the Euclidean sphere on the coordinates I. Clearly, there are at most [formula] such subsets J. Using a standard volumetric estimate (see, e.g., [\cite=Pis] [\cite=LAMA]), for every fixed set J and every ε < 1 / 2, one needs at most (5 / ε)d|J| = (5 / ε)dk Euclidean balls of radius ε to cover SIJ. Therefore, for every 0 < ε < 1 / 2,

[formula]

as claimed.

The second part of the claim is an immediate consequence of Proposition [\ref=prop:rho-cover] and the fact that N(T,ε) is a decreasing function of ε.

For every L  ≥  1 there are constants c1, c2 and c3 that depend only on L and for which the following holds. If inf v,w∈Wkκ(v,w)  ≥  κ, u  ≥  c1 and N  ≥  c2u3(k log (en / (dk))  +  dk) / κ2, then with probability at least 1 - 2 exp ( - c3u(k log (en / (dk))  +  dk)), for every s,t∈Sdk,

[formula]

When κ is constant we conclude that N  ~  k( log (en / (kd))  +  d) measurements are needed for stability. This result is consistent with that of [\cite=EM09a] which shows that the same value N ensures that a random Gaussian matrix satisfies the block restricted isometry constant.

Noisy Measurements

Next, consider the phase retrieval problem in the presence of noise. The goal is to find an estimate x̂ of the true signal x0 that is close to x0 (or - x0) in a squared error sense.

Suppose that

[formula]

for some x0∈T. Let a be an isotropic, L-subgaussian random vector and assume that the noise w is independent of a, symmetric, and of reasonable decay properties, which will be specified in Assumption [\ref=asp:noise] below.

Given (ai,yi)Ni = 1, combined with the information that the noisy data yi is generated by a point x0∈T via ([\ref=eq:measn]), is it possible to produce an estimate x̂∈T for which [formula] is small?

Note that the error is measured by the product [formula], since it is impossible to distinguish between x0 and - x0.

The answer to this question is affirmative, as shown in Theorem [\ref=thm:noisy-main].

Preliminaries: ψα Random Variables

Throughout our analysis we assume that the noise w decays properly. In order to quantify this decay we rely on the notion of ψα random variables, which are defined below (see [\cite=LT] [\cite=VW] [\cite=LAMA] as general references for properties of ψα random variables).

Let X be a random variable. For 1  ≤  α  ≤  2 let

[formula]

and denote by Lψα the set of random variables for which [formula].

The ψα norm can be characterized using information on the tail of X. Indeed, there exists an absolute constant c, for which, if t  ≥  1, then [formula]. The reverse direction is also true, that is, if Pr(|X|  ≥  t)  ≤  2 exp ( - tα / Aα), then [formula] for an absolute constant c1.

It is well known that [formula] is a norm on Lψα, and that

[formula]

In other words,

[formula]

In the language of the previous section, X is L-subgaussian if and only if [formula]. Since the ψα norms have a natural hierarchy, it follows that if X is L-subgaussian then

[formula]

Therefore, if X is L-subgaussian and mean-zero then [formula], where σX is the standard deviation of X.

A straightforward application of the tail behavior of a ψα random variable implies that if X1,...,XN are independent copies of X and t  ≥  1, then

[formula]

hence,

[formula]

From the definition of the ψα norm it is evident that if α  =  β / q then

[formula]

and in particular, X∈Lψβ for β > 1 if and only if |X|β∈Lψ1.

Although there are versions of the following theorem (and of Definition [\ref=def-psi-alpha]) for any 0  <  α, for the sake of simplicity, we shall restrict ourselves to the case α = 1, which is the setting needed in the proofs below.

There exists an absolute constant c1 for which the following holds. If X∈Lψ1 and X1,...,XN are independent copies of X, then for every t > 0,

[formula]

Combining Theorem [\ref=thm:concentration-psi-1] and [\eqref=eq:psi-beta] leads to the following corollary:

Let p > 1 and assume that w is a random variable for which |w|p∈Lψ1 (or w∈Lψp). Then, with probability at least 1 - 2 exp ( - ct),

[formula]

The corollary follows immediately from Theorem [\ref=thm:concentration-psi-1] by taking [formula] for 0 < t < N, and since [formula].

We will also be interested in decay properties of the random variable [formula] for a set [formula]. If μ is an isotropic, L-subgaussian measure on [formula], one has the following (see, e.g. [\cite=Men-GAFA]).

For every L > 1 there exist constants c1, c2,c3 and c4 that depend only on L and for which the following holds. If u  ≥  c1, then with probability at least 1 - 2 exp ( - c2u log N),

[formula]

where [formula] and d(T) are defined by [\eqref=eq:lt] and [\eqref=eq:dt]. In particular,

[formula]

The Recovery Algorithm

The assumptions we make throughout this section are as follows:

Recall that the goal is to find an estimate x̂ of x0 that is close to x0 or to - x0. Given the measurements (yi)Ni = 1, a reasonable approach is to seek a value of x that minimizes the empirical risk function:

[formula]

for some p. Here we will consider values of p in the regime 1 < p  ≤  2; the exact choice of p will become clear later on. Note that for every x∈T,

[formula]

Since the empirical average [formula] is not a convex function in x, it is impossible in general to find a value of x that minimizes it. Luckily, for our purposes, one does not need an exact minimizer. Instead, in order to bound the estimation error, it is sufficient to find a value of x for which the empirical risk is bounded above, as incorporated in the Definition [\ref=def:hax-t] below. To this end, one may use any algorithm for phase minimization and check whether the resulting solution satisfies the bound. Particularly useful in this context are techniques that depend on the initial starting point; such methods can be started from several different points, and in that way, if a particular solution does not satisfy the bound then the algorithm may be used again, but from a different starting point. Eventually, with high probability, a point satisfying the bound will be obtained. One algorithm of this form is the GESPAR method developed in [\cite=SBE12].

Let 1 < p  ≤  2 be given, and choose a value of u  ≥  1. Given the data (ai,yi)Ni = 1, x̂∈T is called a good estimate if it satisfies that

[formula]

where

[formula]

and [formula] are defined by [\eqref=eq:lt],[\eqref=eq:dt].

To motivate the choice of x̂ in Definition [\ref=def:hax-t], observe that QT,N,W captures the "statistical complexity" of the problem - namely, the sum of the "gaussian complexity" of T, QT,N, and the influence of the noise, [formula]. The parameter u tunes the probability estimate, for the moment is of secondary importance. The exact choice of p and u will be specified in Theorem [\ref=thm:noisy-main].

This approach is based on a modified empirical risk minimization - modified in two ways. First, instead of minimizing the loss functional [formula], the search is for an empirical feasible point; some x∈T for which

[formula]

Observe that the value on the left hand side of [\eqref=eq:hax-x-emp] is the empirical excess risk PNLx where

[formula]

is the excess loss functional. The definition implies that the empirical excess risk at x̂ is of the same order of magnitude as the "statistical error" and thus

[formula]

One of the key components of the proof is to show that if PNLx̂ is small, then so is the conditional expectation, [formula]. The second key component is that if [formula] is small, then so is [formula].

Unfortunately, it is impossible to estimate the empirical excess risk since one does not have access to the sampled noise w1,...,wN, and therefore, nor to [formula] - which is the reason for the second modification. By Assumption [\ref=asp:noise], w∈Lψ2 and consequently |w|p∈Lψ1. From Corollary [\ref=cor:ewp], if u  ≤  N, then with probability at least 1 - 2 exp ( - c1u2),

[formula]

Therefore, if x̂ satisfies [\eqref=eq:hax-t], then it also satisfies [\eqref=eq:hax-x-emp], meaning that its empirical excess risk is bounded above by the desired quantity. This leads to the following proposition.

There exists an absolute constant c1 for which the following holds. Let x̂ be a point that satisfies [\eqref=eq:hax-t] and let w∈Lψ2. If 0  ≤  u  ≤  N, then with probability at least 1 - 2 exp ( - c1u2), PNLx̂  ≤  uQT,N,W.

To see that there is always a point x̂ that satisfies [\eqref=eq:hax-t], observe that for x0 and 0  <  u  ≤  N, with probability at least 1 - 2 exp ( - cu2) (see Corollary [\ref=cor:ewp]),

[formula]

Moreover, unless T is very small and W is very large, QT,N is the dominant term in QT,N,W. For example, consider the case in which w is a centered Gaussian with variance σ and T is the set of k-sparse vectors on the unit sphere. Then, [formula], while [formula] which clearly is larger than [formula], as long as k is large relative to σ.

We are now ready to state our main result. To this end recall the definition of κ(s,t) given by [\eqref=eq:kappa], and let κT  =   inf s,t∈Tκ(s,t).

For every κ > 0 and every L  ≥  1 there exists constants c1,c2,c3 and c4 that depend only on L and κ, for which the following holds. Let a be distributed according to an isotropic, L-subgaussian measure, and assume that κT  ≥  κ. Assume further that [formula]. For every integer N set

[formula]

and

[formula]

Let x̂ be chosen to satisfy [\eqref=eq:hax-t]. Then, for u  ≥  c2, with probability at least 1 - 2 exp ( - c3u1 / 3),

[formula]

where QT,N,W is defined by ([\ref=eq:qtn]).

Note that [formula] implies that [formula] for any p  ≤  2.

Since QT,N,W decays as [formula] while βN grows as log N, it is always possible to choose N large enough so that the error given in the theorem is made sufficiently small. As an example, consider the case of k-sparse vectors on the sphere. Hence, d(T) = 1, and recall from Section [\ref=sec:examples] that [formula]. If w is L-subgaussian then [formula] where σ is the noise standard deviation, and

[formula]

If k log (en / k)  ≥  (σ + 1) log N (which is the reasonable range, as one expects N  ~  k up to logarithmic factors), then βN  ~  k log (en / k), and by Theorem [\ref=thm:noisy-main],

[formula]

where c is a constant.

To proceed, and as will be noted in Section [\ref=sec:linear], in the case of linear measurements, with high probability, To compare the "quadratic" estimate with the linear one, note that if N  ≤  (k log (en / k))γ for γ  ≥  c1 and some constant c1  ≥  1, then recalling that for every x, x1 /  log x  ≤  e, it is evident that

[formula]

where C is an absolute constant. Therefore, with this choice of N,

[formula]

and up to logarithmic factors scales as the estimate in the linear case.

Clearly, it suffices to take [formula] to ensure that [formula], which is off only by a log k factor from the optimal estimate in the linear case.

For every L  ≥  1 and κ > 0 there exist constants c1, c2,c3 that depend only on L and κ and for which the following holds. Let T be the set of k-sparse vectors on the sphere, set a to be distributed according to an isotropic, L-subgaussian measure and assume that κT  ≥  κ. If the noise w is L-subgaussian, N  ≤  (k log (en / k))γ for γ  ≥  c1  ≥  1 and u > c2, then with probability at least 1 - 2 exp ( - c3u1 / 3),

[formula]

In particular, if [formula] then [formula] with probability at least 1 - δ.

Proof of Theorem [\ref=thm:noisy-main]

The proof of the theorem requires several preliminary facts about empirical and Bernoulli processes. We refer the reader to [\cite=LT] [\cite=KW] for more details on these processes.

Throughout this section, (Ω,μ) is a probability space and (Xi)Ni = 1 are iid, distributed according to μ. Let ε1,...,εN be independent, symmetric, { - 1,1}-valued random variables, that are independent of X1,...,XN.

The first result we require is the contraction inequality for Bernoulli processes.

[\cite=LT] Let [formula] be convex and increasing and let [formula] satisfy that φ(0) = 0 and [formula], where [formula] is the Lipschitz constant of φi. Then, for any bounded [formula],

[formula]

The following symmetrization argument allows one to bound an empirical process using the Bernoulli process indexed by the random set {(h(Xi))Ni = 1:h∈H}.

[\cite=VW] Let [formula] be convex and increasing and let H be a class of functions. Then

[formula]

We will use Theorem [\ref=thm:contraction-Bernoulli] and Theorem [\ref=thm:symmetrization] with F(x) = |x|q for q  ≥  2.

The final result we require is the Kahane-Khintchine inequality [\cite=LT], on the moments of Bernoulli processes.

There exists an absolute constant c for which the following holds. If [formula] and q  ≥  2 then

[formula]

The first step in the proof of Theorem [\ref=thm:noisy-main] is to obtain an oracle inequality for [formula] that holds for any x∈T (see Lemma [\ref=lemma:noisy-oracle-inequality] below). The oracle inequality is used for x = x̂, and noting that for a good x̂, PNLx̂ is bounded above by uQT,N,W leads to an upper bound of the form [formula]. The second part of the proof consists of establishing a lower bound on [formula] which is a function of [formula].

For every L  ≥  1 there exist constants c1,c2 and c3 that depend only on L for which the following holds. If p is chosen as in Theorem [\ref=thm:noisy-main], then for u  ≥  c1, with probability at least 1 - 2 exp ( - c2u1 / 3), for every x∈T,

[formula]

The choice made above, of p = 1 + 1 /  log βN is the key point in the proof, and it is there to balance two issues. On one hand, if p is larger than 1, then the empirical process [formula] becomes much harder to control. On the other, if p = 1, then the loss is not strictly convex, and it is impossible to lower bound [formula] using a function of [formula]. This choice of p is sufficiently close to 1 to enable control of the empirical process (the main point is [\eqref=eq:D-control]), while it is far enough from 1 to give enough convexity to enable the lower bound.

Proof.   Fix q  ≥  2. By the symmetrization theorem (Theorem [\ref=thm:symmetrization]) and the independence of a and W,

[formula]

Let

[formula]

and observe that for every realization of (wi)Ni = 1, the functions y  →  |y - wi|p - |wi|p vanish at 0 and are Lipschitz on

[formula]

B=sup| < a,x-x > < a,x+x >- < a,x-x > < a,x+x > |,

[formula]

sup|P L - | ≤ c D B,

[formula]

B ≤ cq(d(T) (T)/ + (T)/N).

[formula]

max sup |< a,x-x > < a,x+x >| ≤ max sup |< a,x >|.

[formula]

max sup |< a,x-x > < a,x+x >| (T) + d(T) log N.

[formula]

( D) (p-1)qD,

[formula]

D ((p-1)q) D ((p-1)q) β ≤ cq.

[formula]

sup |P L - | ≤ c u(d(T) + ),

[formula]

≤ P L + cu(d(T) + ),

[formula]

Q = (d(T) + )+.

[formula]

P L ≤ uQ

[formula]

≤ P L + cu Q.

[formula]

(c+(p-1)d)-c ≥ (p-1)d.

[formula]

≥ (p-1) |h(a)| = (p-1) |< a,x-x > < a,x+x >|.

[formula]

( |< a,t > < a,s > |) ≥ |< a,t > < a,s > | ≥ κ t s ≥ κ t s.

[formula]

≥ κ (p-1) x-x x+x.

[formula]

Examples

Let us present some of the examples seen in Section [\ref=sec:examples], in the noisy setting. Other examples may be obtained with similar ease.

In order to apply the results of Theorem [\ref=thm:noisy-main], one has to determine [formula] and [formula].

In all the examples below we will assume that T  ⊂  Sn - 1 and so d(T) = 1. Since w is symmetric and L-subgaussian, then [formula], where σ is the noise variance. Also, since 1  <  p  ≤  2, [formula].

Entire Space [formula]

If T = Sn - 1 then [formula], implying that

[formula]

for the regime of N we are interested in, and

[formula]

In addition,

[formula]

Suppose that n  ≥  (σ + 1) log N. Then, by Theorem [\ref=thm:noisy-main],

[formula]

where c is an absolute constant. If N  ≤  nγ for γ  ≥  c1  ≥  1, then

[formula]

for a suitable absolute constant C. Therefore, with this choice of N,

[formula]

and it suffices to take [formula] to ensure that [formula] with probability at least 1 - δ.

For every L  ≥  1 and κ > 0 there exist constants c1, c2,c3 that depend only on L and κ and for which the following holds. If μ, a and w are as above, T = Sn - 1 and N  ≤  nγ for γ  ≥  c1  ≥  1, then for u > c2 with probability at least 1 - 2 exp ( - c3u1 / 3),

[formula]

Sparse Vectors

We already treated the case of sparse vectors in Corollary [\ref=cor-noisy-sparse]. The block-sparse setting can be treated in a similar manner, leading to the following corollary.

For every L  ≥  1 and κ > 0 there exist constants c1, c2,c3 that depend only on L and κ and for which the following holds. If a and w are as above, T is the set of k-block sparse vectors of length d on the sphere and N  ≤  (k log (en / dk) + dk)γ for γ  ≥  c1  ≥  1, then for u > c2 with probability at least 1 - 2 exp ( - c3u1 / 3),

[formula]

In particular, if [formula] then [formula] with probability at least 1 - δ.

Connection with Results on Linear Estimation

It should come as no surprise that the methods used here are very similar in nature to the analogous "linear questions". Both stability and noisy recovery are well understood in the linear case, and in a sharp way, as we will explain below.

First, consider the question of stability. Suppose that the measurements are given by y = Ax for some N  ×  n matrix A. In the linear setting, a natural notion of stability in a set [formula] is that for all s,t∈T,

[formula]

where C is a constant.

Note that here the [formula] norm is used in the left hand side, rather than the [formula] norm. An [formula] stability result is superior to an [formula] estimate, simply because the [formula] norm is smaller. And, It is natural to compare an [formula] stability result in the linear case to the [formula] stability result for quadratic measurements we established.

Stability in a set T for a random ensemble depends on the way in which a typical operator acts on the set

[formula]

Indeed, because a is distributed according to an isotropic measure, for every z∈Sn - 1, [formula]. Thus, stability on T is equivalent to an estimate on

[formula]

which is strictly smaller than 1.

With this in mind, the stability constant in [formula] is a lower bound on the smallest singular value of a typical operator from the given random ensemble.

The study of the process [\eqref=eq:quad-proc], both for T- = Sn - 1 and for an arbitrary subset of the sphere has been extensive in recent years. A good starting point for the interested reader would be [\cite=KM] [\cite=MPT] for subgaussian ensembles, [\cite=ALPT] [\cite=Men-GAFA] for log-concave ensembles, and [\cite=SV] [\cite=MenPao1] [\cite=MenPao2] for ensembles with heavy tails (though this does not begin to cover the extensive literature on the topic).

In the context of this paper, subgaussian ensembles, the best estimate on [\eqref=eq:quad-proc] follows from Theorem [\ref=thm:main-emp-est], applied to the class [formula]. Moreover, in [\cite=Men-orc] it was shown that under very mild assumptions on the set T-, the estimate is sharp.

For every L  ≥  1 there exist constants c1,c2 and c3 that depend only on L for which the following holds. If [formula] and a is distributed according to an isotropic, L-subgaussian measure, then for u  ≥  c1, with probability at least [formula], for every s,t∈T,

[formula]

provided that [formula].

Proof.   Since

[formula]

for [formula], then setting [formula], it suffices to bound [formula] from below. Since a is isotropic, [formula]. Applying Theorem [\ref=thm:main-emp-est] for [formula] and recalling that T-  ⊂  Sn - 1, it follows that with probability at least [formula],

[formula]

On that event, for every [formula],

[formula]

as claimed.

Observe that the same complexity parameter appears in the linear case as in the "quadratic" stability result - the gaussian complexity of a "projection" of T - T onto the sphere (and, of course, the T + T component does not appear). In all the examples we presented in this note, T + T and T - T have essentially (or exactly) the same complexity, and thus the stability estimates in the linear case coincide with quadratic bounds, as will be the case for any [formula] with a similar property. Thus, in these cases, there is no harm in requiring stability over quadratic measurements rather than with respect to linear ones.

The noisy recovery problem in the linear case is much simpler, since the resulting empirical process is well behaved even if one uses the squared loss functional. The advantage in considering the squared loss functional is that one has the benefit of the required convexity "for free". With this objective, noisy recovery becomes a linear regression problem in [formula], indexed by T. This is a well studied topic in statistics. We refer the reader to [\cite=Kolt] for relatively recent results related to this question.

The best results to-date on linear regression that take into account the complexity of the indexing set T can be found in [\cite=Men-orc]. One may show that these estimates are sharp under very mild assumptions on T, and it turns out that these assumptions are satisfied in the examples that were presented here. Since our bounds in the "quadratic" case are of the same order of magnitude as in the easier, linear case, and since these bounds are optimal in the linear case, it is reasonable to expect that they are optimal in the quadratic scenario as well. Unfortunately, the methods required to prove this optimality are rather involved, and we will not explore this issue here. Rather we refer the reader to [\cite=Men-orc], in which the linear case is explored.