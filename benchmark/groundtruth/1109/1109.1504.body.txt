A New Method for Lower Bounds on the Running Time of Evolutionary Algorithms

A preliminary version with parts of the results has been presented at a conference [\cite=Sudholt2010a]. The results therein were limited to mutation rate 1 / n.

Introduction

Evolutionary algorithms (EAs) and other randomized search heuristics have been successfully applied to countless difficult practical problems. One important reason for their popularity and their success is that they can be applied to a broad range of problems. They are usually easy to implement and they typically produce reasonable results in short time, with little effort.

However, getting the best possible results requires much greater effort. When aiming for maximum efficiency, one has to think carefully about what search algorithm to use, how to make design choices, and how to tune the parameters of the algorithm. In the search for the best strategy, researchers and practitioners alike are faced with a range of fundamental questions:

How effective is search algorithm A on problem/problem class P?

What is the best parameter setting for A on P?

Is search algorithm B faster than A on P?

What is the best search algorithm for P?

Finding answers to these questions is now more pressing then ever. The field of evolutionary computation has grown immensely in the last decades and it has led to the development of countless variants of search algorithms, with new bio-inspired optimization paradigms emerging every year. This can prove to be a burden as practitioners are faced with an overwhelming variety of search algorithms.

Running time analysis has emerged as an important and very active area in evolutionary computation. The goal is to formally analyze the random or expected time until an evolutionary algorithm has found a satisfactory solution for a given problem. By assessing how the expected running time grows with the problem dimension, we can gain valuable insights into their scalability. These insights apply to arbitrary, not too small problem dimensions--even to very large dimensions that are beyond the capabilities of today's hardware.

It also yields a solid foundation for the comparison of different EAs or different heuristic paradigms. This includes the question in how far design choices affect performance such as the choice of representations, operators, and parameters. In some cases running time analyses allow to draw conclusions about optimal parameter settings. Some of the above questions can be answered. Last but not least, theoretical analyses lead to insight into the working principles of EAs and to a better understanding of their behavior.

Running time analyses have been performed for classes of pseudo-Boolean functions such as unimodal functions [\cite=Droste2002], linear functions [\cite=Droste2002] [\cite=He2004] [\cite=Jagerskupper2011] [\cite=Doerr2010] [\cite=Witt2011a], functions with plateaus [\cite=Jansen2002a], monotone polynomials [\cite=Wegener2005c], and monotone functions [\cite=Doerr2010c]. The same approach has been used for the analysis of problems from combinatorial optimization, see the survey by Oliveto, He, and Yao [\cite=Oliveto2007] or the recent text book by Neumann and Witt [\cite=BookNeuWit]. Also many other metaheuristics have been studied such as memetic algorithms [\cite=Sudholt2009] [\cite=Sudholt2010] [\cite=Sudholt2010b], estimation-of-distribution algorithms [\cite=Droste2006a] [\cite=Chen2010], ant colony optimization [\cite=Gutjahr2008a] [\cite=Neumann2009b] [\cite=Neumann2009] [\cite=Sudholt2011a], particle swarm optimization [\cite=Sudholtsubmitteda] [\cite=Witt2009], and artificial immune systems [\cite=Zarges2008] [\cite=Jansen2011]. A good summary of recent developments is given in the edited book by Auger and Doerr [\cite=Auger2011].

However, running time analysis comes with several drawbacks. In many cases running time analyses are very challenging. Search heuristics represent complex dynamic systems that are often hard to handle analytically. Hence, studies have often been limited to very specific settings. Comparisons between different search algorithms--or variants of the same algorithm--have often been performed on contrived artificial functions that were designed specifically to enable an analysis.

Furthermore, many analyses are restricted to a single, very specific algorithm such as the with mutation probability 1 / n. This helps to keep the analyses simple, but it also means that conclusions are limited to this particular algorithm. Another shortcoming is that, when considering polynomial expected running times, often only upper bounds on the expected running time are shown. Upper bounds are more appealing than lower bounds as they show that a particular search algorithm is effective on a particular problem. Lower bounds are typically harder to prove and often more imprecise, compared to upper bounds. For example, for the function OneMax an upper bound with the exact leading constant (i. e., the constant factor preceding the fastest-growing term) is known from the 1990s (see Rudolph [\cite=Rudolph1997a]). But a matching lower bound with the same leading constant was only proved recently, in 2010, by Doerr, Fouz, and Witt [\cite=Doerr2010a].

When only upper bounds are available it is hard to make comparisons between different algorithms. Even when an upper bound for search algorithm A is much lower than an upper bound for B, we cannot conclude with rigor that A is more efficient than B. It could be that the analysis for A is more precise than that for B, but in fact B is more efficient than A. One has to take care not to draw wrong conclusions when interpreting running time bounds. Only if we have a lower bound for B that is larger than the upper bound for A we can say with certainty that A is more effective than B. This stresses the importance of lower bounds, and that of having precise running time bounds.

Many researchers have tried to develop methods for proving lower bounds. Drift analysis has emerged as one powerful tool [\cite=Oliveto2011] [\cite=He2004] [\cite=Doerr2010b] [\cite=Lehre2010a] [\cite=Doerr2011c]. However, it is not always easy to apply. We present a new method for proving lower bounds on the running time of stochastic search algorithms (see Section [\ref=sec:method]). It follows the idea of fitness-based partitions or fitness levels, a well-known tool for proving upper running time bounds. The idea is to partition the search space into a sequence of sets called fitness levels. These sets have to be traversed in order to find a global optimum. Lower bounds can be derived if we have upper bounds on the probability of reaching a better fitness level and additional information about the transition probabilities between fitness levels.

The method is illustrated with applications to well-studied test problems. The function [formula] counts the number of ones in the bit string. The optimum is the all-ones bit string. Assessing the performance of a search algorithm on OneMax equals the question how effective the algorithm is at hill climbing--and at finding a particular target point if best possible hints are given. The function LeadingOnes, shortly [formula], is another popular test function that counts the number of leading ones in the bit string. All bits have to be optimized sequentially. This gives an example of a unimodal function that is more difficult than OneMax. It also resembles worst-case inputs for shortest path problems [\cite=Sudholt2011a]. Long k-paths [\cite=Horn1994] [\cite=Rudolph1997] [\cite=Droste2002] [\cite=Sudholt2009] represent even more difficult unimodal functions where EAs typically climb up a path. As the path can have exponential length and shortcuts are unlikely, this a very challenging problem. For details we refer to Section [\ref=sec:long-k-paths].

The example applications show that the new method is applicable to a wide range of problems and to a very broad class of evolutionary algorithms. We introduce the term mutation-based EAs for a class of EAs that first generate initial search points uniformly at random, and afterwards only use common bit-flip mutation operators for variation. This class contains all common EAs that do not use crossover, e. g., all (μ  +  λ) EAs, all (μ,λ) EAs as well as parallel variants such as island models. Basically, the class contains all EAs regardless of the selection operators and population models (see Section [\ref=sec:preliminaries]).

The resulting lower bounds apply to all mutation-based EAs. They are not only tight in an asymptotic sense. They contain best possible leading constants when compared to upper bounds for the best EAs in this class, up to lower-order terms. The bounds also show how the expected running time depends on the mutation rate. This highlights the impact of this parameter on performance and it allows for conclusions on the optimal mutation rate. Along the way, we also present a refinement of the fitness-level method for proving upper bounds in Section [\ref=sec:refined-upper-bound].

The results allow to make conclusions about optimal EAs, where optimality is regarded as minimizing the expected number of function evaluations. A summary of the results derived from applying the new method is as follows. denotes a variant of the initialized with a best out of μ individuals generated uniformly at random.

For LO we get a lower bound for all mutation-based EAs, see Section [\ref=sec:LO]. This bound equals a refined upper bound for the . For all μ we get an exact formula for the expected running time of the , including the . Together with the independent work by Böttcher, Doerr, and Neumann [\cite=Boettcher2010], this is the first time that an exact formula for an expected running time of an EA can be determined. Following [\cite=Boettcher2010], the optimal mutation rate can be computed as p  ≈  1.59 / n. The optimal mutation-based EA turns out to be the for some value μ  >  1.

For OneMax we also get a lower bound for all mutation-based EAs, see Section [\ref=sec:onemax]. For all reasonable mutation rates the lower bound matches an upper bound for the using the same mutation rate, up to terms of smaller order. The optimal mutation rate turns out to be p = 1 / n (see also Witt [\cite=Witt2011a]). The optimal mutation-based EA is again the for a proper μ  >  1.

The above lower bound on OneMax generalizes to the very large class of functions that have a unique optimum, see Section [\ref=sec:unique]. This is based on the structural insight that for all mutation-based EAs finding a single target point for any problem is never easier than optimizing OneMax.

For long k-paths we get upper and lower bounds that match up to smaller order terms, for all reasonable mutation rates, when considering the starting on the first point on the path, see Section [\ref=sec:long-k-paths]. Like for OneMax, p = 1 / n is the optimal mutation rate.

In addition to these remarkably powerful results, the method is easy to describe and it has a simple, direct proof. As such, it is well suited for teaching purposes and it shows that precise lower bounds can be obtained without using drift analysis.

Previous and Related Work

There is a long history of results on pseudo-Boolean optimization. We review results on lower bounds and also describe work that preceded, relates to, or has followed from this work [\cite=Sudholt2010a].

Already Droste, Jansen, and Wegener [\cite=Droste2002] presented a lower bound of Ω(n log n) for the on every n-bit pseudo-Boolean function with unique global optimum. The constant factor preceding the n log n-term is 1 / 2  ·  (1 - e- 1 / 2)  ≈  0.196. Wegener [\cite=Wegener2002] mentions a lower bound (1 - ε)  ·  n ln n - cn where ε  >  0 is an arbitrarily small constant and the constant c  >  0 depends on ε. Doerr, Fouz, and Witt [\cite=Doerr2010a] presented a lower bound (1  -  o(1))en ln n for the on OneMax. The last result was extended by Doerr, Johannsen, and Winzen [\cite=Doerr2010]. They proved that the same bound holds for the on every function with a unique global optimum.

Later on, Doerr, Fouz, and Witt [\cite=Doerr2011c] were inspired by the lower bound en ln n  -  2 log  log n  -  16n for mutation-based EAs with p = 1 / n on OneMax in the preliminary work [\cite=Sudholt2010a]. Their goal was to remove the 2 log  log n-term in order to arrive at an even more precise bound for . They managed to get a lower bound of en ln n  -  O(n) and along the way they introduced two new techniques to the analysis of randomized search heuristics: lower bounds with variable drift and probability-generating functions.

Witt [\cite=Witt2011a] followed up on this work [\cite=Sudholt2010a] and presented lower bounds for the class of mutation-based EAs on linear functions. He proved that the mutation rate p = 1 / n is an optimal choice for the on linear functions. He also generalized a structural result from [\cite=Sudholt2010a] in the following sense. The original statement is that the expected optimization time of any mutation-based EA with mutation probability 1 / n on any function with unique global optimum is at least as large as the expected optimization time of the with mutation probability 1 / n on OneMax. Witt generalized this towards arbitrary mutation probabilities 0  <  p  ≤  1 / 2 and stochastic dominance. We will discuss and apply this result in Section [\ref=sec:unique].

The LeadingOnes function has been equally popular. Droste, Jansen, and Wegener [\cite=Droste2002] showed that the running time of the on LO is at least c1n2 with probability 1 - 2-  Ω(n), for some constant c1  >  0. Böttcher, Doerr, and Neumann presented an exact formula for the expected running time of the on LO at the same conference [\cite=Boettcher2010]. While the preliminary version of this work [\cite=Sudholt2010a] considered mutation rates of p = 1 / n only, the authors considered general mutation rates p. Their results were limited to the as opposed to all mutation-based EAs. They showed that the optimal fixed mutation rate for LO is not p = 1 / n, but a slightly higher value of p  ≈  1.59 / n. In addition, they presented a simple adaptive scheme for choosing the mutation rate and showed that this leads to even smaller numbers of function evaluations.

These findings show that the often recommended choice p = 1 / n is not always optimal. Another reason why the choice of the mutation probability is far from settled is that even on a seemingly easy class of functions a constant factor in the mutation rate can change a polynomial expected running time into an exponential one [\cite=Doerr2010c].

Black-box complexity of search algorithms as introduced by Droste, Jansen, and Wegener [\cite=Droste2006] is another method for proving lower bounds. These bounds hold for all algorithms in a black-box setting where only the class of functions to be optimized is known, but the precise instance is hidden from the algorithm. Their results imply that every black-box algorithm needs at least Ω(n /   log n) function evaluations to optimize OneMax and LO (or, to be more precise, straightforward generalizations to function classes). Recently Lehre and Witt [\cite=Lehre2010] presented a more restricted black-box model. If only unary operators are used (that is, operators taking a single search point as input, such as mutation) and all operators are unbiased with respect to bit values and bit positions, every black-box algorithm needs Ω(n log n) function evaluations for every function with a unique global optimum. The constant factor hidden in the Ω is not specified; it is known to be at most 1. This line of research has been extended subsequently to more general conditions for unbiasedness [\cite=Rowe2011], higher-arity operators [\cite=Doerr2011a] and more restricted black-box models [\cite=Doerr2011b].

Investigating conditions for the optimality of search algorithms, Borisovsky and Eremeev [\cite=Borisovsky2008] introduced the concept of dominance for the performance comparison of evolutionary algorithms. For sorting problems and the function OneMax they give sufficient conditions on when the is faster than evolutionary algorithms with other reproduction operators.

Recently, drift analysis has received a lot of attention [\cite=Oliveto2011] [\cite=He2004] [\cite=Doerr2010b] [\cite=Lehre2010a] [\cite=Doerr2011c]. Assume a non-negative potential function such that the optimum is reached only if the potential is 0. If the expected decrease ("drift") of the potential in one generation is bounded from below, an upper bound on the expected optimization time follows. Conversely, an upper bound on the drift implies lower bounds on the expected optimization time. If there is a drift pointing away from the optimum on a part of the potential's domain then exponential lower bounds can be shown [\cite=Lehre2010a] [\cite=Oliveto2011].

Preliminaries

The presentation in this work is for maximization problems, but it can be easily adapted for minimization. For the usage of asymptotic notation we refer to text books such as Cormen, Leiserson, Rivest, and Stein [\cite=Cormen2001].

Mutation-Based Evolutionary Algorithms

The technique for proving lower bounds will be applied to a very general class of evolutionary algorithms. It contains all EAs that generate [formula] individuals uniformly at random and afterwards only use standard mutations to generate offspring (see Algorithm [\ref=alg:construct]).

Mutation is done by flipping each bit independently with some given mutation probability 0  <  p  ≤  1 / 2. The most extreme value p = 1 / 2 corresponds to choosing an offspring uniformly at random, i. e., random search. We do not consider mutation rates p  >  1 / 2 as this choice would favor offspring far away from the parent, thus contradicting the purpose of mutation.

The optimization time is given by the time index t that counts the number of function evaluations. It is defined as the time index t when a global optimum is found first. In a more general sense, we can also regard the (expected) hitting time of a set of desirable search points. For some lower bounds and for small values of μ, we pessimistically disregard the effort for creating the μ search points.

The parent selection mechanism is very general as any mechanism based on the time index t and fitness values of previous search points may be used. Any mechanism for managing a population fits in this framework. This includes parent populations and offspring populations with arbitrary selection strategies and even parallel evolutionary algorithms with spatial structures and migration such as the island model [\cite=Lassig2011].

The is a well-known special case with population size μ  =  1. It maintains a single individual x and in every iteration it creates x' by mutating x and replacing x by x' if f(x')  ≥  f(x). We denote by a generalization of the that is initialized with a best individual out of μ individuals which are generated uniformly at random.

Before introducing the new lower-bound method we elaborate on the range of sensible values for the mutation rate p. The expected number of flipping bits equals pn. This is 1 for the standard choice p = 1 / n. If p  ≪  1 / n then the expected number of flipping bits is close to 0. The expected time until mutation creates any offspring that is different from its parent is then at most 1 / (pn) as pn is an upper bound on the probability that any bit flips. This means that p must be at least an inverse polynomial to allow for polynomial expected running times (unless the initialization finds a global optimum with high probability).

If the problem only contains a single optimum that has to be hit, p cannot be too large. If p  ≤  1 / 2 then the best probability for hitting the optimum from a non-optimal parent is obtained when the parent has Hamming distance 1 to the optimum. Then the probability is p(1 - p)n - 1  ≤  (1 - p)n  ≤  e- pn and the expected waiting time until this happens is epn. We summarize these findings in the following theorem, showing that unreasonable parameter settings lead to unreasonable running times. Note that the optimum is not found during initialization with population size μ with probability at least 1 - μ  ·  2- n.

Let f be a function with a unique global optimum. The expected optimization time of every mutation-based EA on f with mutation probability 0  <  p  ≤  1 / 2 is at least (1 - μ  ·  2- n)  ·  1 / (pn) and at least (1 - μ  ·  2- n)  ·  epn.

In particular, for every μ the expected optimization time is superpolynomial if p  ≤  n-  ω(1) or p  =  ω( log n) / n and exponential (i. e. 2nε for some constant ε  >  0) if p  ≤  2- nΩ(1) or p  =  nΩ(1) - 1.

The result can be extended towards functions with multiple global optima, but the above result suffices for our purposes.

The Fitness-Level Method for Proving Upper Bounds

We review the fitness-level method, also known as the method of f-based partitions [\cite=Wegener2002]. It yields upper bounds for EAs whose best fitness value in the population never decreases. We call these algorithms elitist EAs.

The idea is as follows. We partition the search space into sets that are strictly ordered with respect to the fitness of the contained individuals. Every search point in a higher fitness-level set has a strictly higher fitness than any search point in a lower fitness-level set. We say that an elitist algorithm is on a particular level if the best search point created to far is in the respective fitness-level set. Due to the elitism, the algorithm can only increase its current fitness level. If we have a lower bound on the probability of increasing the current level, the reciprocal is an upper bound on the expected time until a particular fitness level is left. As each level is left for good, the sum of all these times--starting from the initial level--yields an upper bound on the expected optimization time.

For two sets A,B  ⊆  {0,1}n and fitness function f let A  <  fB if f(a)  <  f(b) for all a∈A and all b∈B. Consider a partition of the search space into non-empty sets [formula] such that [formula] and Am only contains global optima. For a mutation-based EA [formula] we say that [formula] is in Ai or on level i if the best individual created so far is in Ai. Consider some elitist EA [formula] and let si be a lower bound on the probability of creating a new offspring in [formula], provided [formula] is in Ai. Then the expected optimization time of [formula] on f (without the cost of initialization) is bounded by

[formula]

The second bound results from pessimistically assuming that the algorithm is always initialized in A1.

Let us illustrate the method with two examples for the with mutation probability p = 1 / n. We define the canonical partition as the partition in which Ai contains exactly all search points with fitness i. For LO the method applied to the canonical partition yields an upper bound of [formula] since the probability of finding an improvement is lower bounded by the probability of flipping the first bit with value 0. This probability is at least 1 / n  ·  (1 - 1 / n)n - 1  ≥  1 / (en). For OneMax we get an upper bound of [formula] for the since on level i there are n - i 1-bit mutations that flip a 0-bit to 1 and hence improve the fitness.

In order to make an effort towards a unified theory of search heuristics, we also present the following extension. After finding an improvement, stochastic search algorithms often need some time to adapt their underlying probabilistic models. For instance, the algorithm (μ+1) EA investigated by Witt [\cite=Witt2006] needs some time until the population contains "enough" individuals on a new fitness level, so that an improvement can be found with a good probability. The ant colony optimization algorithms investigated in Gutjahr and Sebastiani [\cite=Gutjahr2008a] as well as in Neumann, Sudholt, and Witt [\cite=Neumann2009] need some time to adapt their pheromones towards a new best solution. A similar argument holds for velocities in a binary particle swarm optimization algorithm investigated by Sudholt and Witt [\cite=Sudholt2008c].

In all these studies, it is pessimistically disregarded that an improvement might be found while waiting for the algorithm to adapt. Fix a notion of adaptation and let Ti be the (random) time until an algorithm has adapted, after a new fitness level i has been found. Redefining pi to the worst-case probability of finding an improvement in one iteration after adaptation, the expected optimization time can be bounded by

[formula]

In addition, Lehre [\cite=Lehre2011] recently presented an extension towards non-elitist populations, with applications to comma strategies and various selection operators. Roughly speaking, he proves that if

the probability of generating an offspring on a worse fitness level is not too large,

selection has a strong enough tendency to pick high-fitness individuals, and

the population is large enough

then an upper bound similar to the one in Theorem [\ref=the:fitness-levels] applies. The running time bound is asymptotic, not revealing a precise constant factor, though. But his work shows that the method is applicable in a much more general context.

Lower Bounds with Fitness Levels

We now show that fitness-level arguments can also be applied to show tight lower bounds on the running time. Researchers have attempted to make this step earlier. The best lower bounds with fitness-level arguments known so far were presented by Wegener in [\cite=Wegener2002].

Let [formula] be a fitness-level partition for some fitness function f. Let ui be an upper bound on the probability of an EA [formula] creating a new offspring in [formula], provided [formula] is in Ai (where "[formula] is in Ai" is defined as in Theorem [\ref=the:fitness-levels]). Then the expected optimization time of [formula] on f is at least

[formula]

The resulting lower bounds are very weak since we only look at the time it takes to leave the initial fitness level and then pessimistically assume that the optimum is found.

For instance, for the with mutation probability p = 1 / n on OneMax Lemma [\ref=lem:crude-lower-bound-method] yields the lower bound

[formula]

as the initialization is very likely to create a search point with around n / 2 1-bits. For the on LO we get the lower bound

[formula]

which is again very crude; the real expected running time is of order Θ(n2).

Much better lower bounds can be achieved by making an additional assumption about the transition probabilities between fitness levels. The idea is as follows. If we know that a search algorithm typically does not skip too many fitness levels, it is likely that many fitness levels need to be traversed. This yields a lower bound that is proportional to the upper bound from Theorem [\ref=the:fitness-levels].

In the following theorem γi,j can be regarded as the conditional probability of jumping from level i to level j, given that the algorithm leaves level i.

Consider an algorithm  and a partition of the search space into non-empty sets [formula]. For a mutation-based EA [formula] we again say that [formula] is in Ai or on level i if the best individual created so far is in Ai. Let the probability of traversing from level i to level j in one step be at most ui  ·  γi,j and [formula]. Assume that for all j  >  i and some 0  ≤  χ  ≤  1 it holds

[formula]

Then the expected hitting time of Am is at least

[formula]

The variable χ was coined viscosity by Jon Rowe [\cite=RowePersonal]. Similar to the viscosity of a liquid, it resembles the viscosity of the fitness-level partition on a scale between 0 and 1. A low viscosity means that we can have situations where a search algorithm skips many fitness levels and only few levels are actually encountered. A high viscosity means that a search algorithm typically encounters many fitness levels as large jumps to higher fitness levels are unlikely.

For χ  >  0 the reciprocal, 1 / χ, is an upper bound on the expected number of fitness levels gained during an improvement. To see this, note that condition [\eqref=eq:gamma-condition] implies [formula] for all j  >  i. This implies [formula]. Using [formula] if X takes only non-negative integer values, the expected progress in terms of fitness levels, assuming current level i, is at most

[formula]

This yields 1 / χ as an upper bound that is independent from the current level.

In a case of extreme viscosity, i. e., χ = 1 condition [\eqref=eq:gamma-condition] can only hold if γi,i + 1  =  1 and γi,k  =  0 for all 1  ≤  i  ≤  m - 1 and all 2  ≤  k  ≤  m - i. This means that the algorithm deterministically reaches the next fitness level when an improvement is made. It passes through all fitness levels between the initial one and the optimal one. These are the strongest possible conditions on the transition probabilities.

Contrarily, if χ  =  0 we have no viscosity at all. Condition [\eqref=eq:gamma-condition] is trivially satisfied for all choices of the γ-variables. This is the weakest possible setting and it leaves open the possibility that the optimum is reached by a direct jump, when the current fitness level is left. In fact, the resulting bound [\eqref=eq:complex-lower-bound] equals the one from Lemma [\ref=lem:crude-lower-bound-method].

The most interesting settings are those where the viscosity is between 0 and 1. For instance, if χ  =  1 / 2 then condition [\eqref=eq:gamma-condition] is roughly equivalent to the γ-variables decreasing exponentially with base 2: γi,i + k  ≤  2- k. Larger viscosities require a steeper decay, while smaller viscosities allow for a less steep decay. For selected fitness levels on OneMax it turns out that the transition probabilities decay rapidly, allowing to choose χ as high as 1 - o(1). This means that only a vanishing fraction of fitness levels is skipped--in expectation--and it leads to a very tight lower bound.

Before we get to the proof of Theorem [\ref=the:lower-bound-method], we state the following conclusions about how tight the upper and lower bounds with fitness levels can be.

Let [formula], and γi,j for 1  ≤  i,j  ≤  m be defined as in Theorems [\ref=the:fitness-levels] and [\ref=the:lower-bound-method]. Let all conditions in these theorems hold.

If si  =  ui for all i then the lower bound [\eqref=eq:simple-lower-bound] matches the upper bound [\eqref=eq:upper-bound-with-fitness-levels] up to a factor of χ.

If χ  >  0 is a constant and there is a constant c  ≥  1 such that ui  ≤  c  ·  ui for 1  ≤  i  ≤  m - 1 then [\eqref=eq:simple-lower-bound] and [\eqref=eq:upper-bound-with-fitness-levels] are asymptotically equal.

If χ  =  1 - o(1) and ui  ≤  (1 + o(1))  ·  si for 1  ≤  i  ≤  m - 1 then [\eqref=eq:simple-lower-bound] and [\eqref=eq:upper-bound-with-fitness-levels] are equal up to lower-order terms.

A fitness-level partition that obeys the second case was called (asymptotically) tight f-based partition in [\cite=Lassig2011].

We proceed by proving Theorem [\ref=the:lower-bound-method]. Afterwards, we give advice on how to apply it, including example applications in the following sections.

The second bound immediately follows from the first one since 0  ≤  χ  ≤  1. Let Ei be the minimum expected remaining optimization time, where the minimum is taken for all possible histories [formula] of previous search points with [formula]. By definition [formula] as the conditions on the histories are subsequently relaxed. By the law of total expectation the unconditional expected optimization time is at least [formula], hence we only need to bound Ei.

After one step, for each i  <  k the algorithm is in Ek with probability at most uiγi,k and it remains in i with probability [formula]. This establishes the recurrence

[formula]

Subtracting (1 - ui)Ei on both sides and dividing by ui yields

[formula]

Assume for an induction that for all k  >  i it holds [formula]. Then we get

[formula]

Note that

[formula]

since on the left-hand side every term 1 / uj appears for all summands [formula] in the outer sum, each summand weighted by γi,kχ. Together, we get

[formula]

One crucial asset of the theorem is that in order to apply it, we do not need to know the transition probabilities exactly. It suffices to state upper bounds on the transition probabilities. More precisely, we require that ui  ·  γi,j is an upper bound on the probability of jumping from level i to level j. We have the freedom to choose ui and γi,j as long as the γ-variables sum up to 1 and they fulfil [\eqref=eq:gamma-condition].

In cases where the transition probabilities are not known precisely or where it is not possible or feasible to derive an analytical expression, we can use different γ-variables as substitutes. Note that the condition on ui  ·  γi,j upper bounding the real transition probability is easier to fulfil if ui is large. So, we can choose ui as large as necessary in order to prove the conditions of Theorem [\ref=the:lower-bound-method]. The price for choosing a large ui is that the resulting lower bound becomes smaller as the ui's grow.

A similar observation holds for the choice of χ. As remarked before, the higher the viscosity χ, the stronger the conditions on the transition probabilities are. The lower χ, the easier it is to establish condition [\eqref=eq:gamma-condition], and the smaller the lower bound becomes.

The method is hence very versatile and flexible as we are free to choose χ and the u-, γ-variables such that all conditions hold. The upcoming example applications give advice as to how these values can be chosen.

Note that the theorem does not require the sets Ai to form fitness levels: we do not assume [formula]. The conditions on the γ-variables indirectly imply that sets with small index are "worse" than sets with higher index. Also note that Theorem [\ref=the:lower-bound-method] bounds the expected hitting time of set Am. This includes the expected optimization time as special case in which Am contains exactly all global optima. Alternatively, Am can contain other desirable solutions such as those with a certain minimum fitness, all local optima, all feasible solutions, etc.

Refined Upper Bounds with Fitness Levels

It has become clear that information about the transition probabilities is essential for proving meaningful lower bounds. This knowledge can also help to obtain refined upper bounds. The following result is very similar to Theorem [\ref=the:lower-bound-method], with some inequalities reversed. Also the proof ideas are very similar to the ones in Theorem [\ref=the:lower-bound-method]. In contrast to the lower bound, we need to add the condition (1 - χ)sj  ≤  sj + 1 for all 1  ≤  j  ≤  m - 2, which states that the success probabilities must not be imbalanced.

Consider a partition of the search space into non-empty sets [formula] such that only Am contains global optima. For an elitist mutation-based EA [formula] we again say that [formula] is in Ai or on level i if the best individual created so far is in Ai. Let the probability of traversing from level i to level j in one step be at least si  ·  γi,j and [formula]. Assume that for all j  >  i and some 0  <  χ  ≤  1 it holds

[formula]

Further assume (1 - χ)sj  ≤  sj + 1 for all 1  ≤  j  ≤  m - 2. Then the expected hitting time of Am is at most

[formula]

For maximum viscosity, i. e., χ  =  1, the condition (1 - χ)sj  ≤  sj + 1 as well as condition [\eqref=eq:gamma-condition-upper-bounds] are always true. We then get the classical fitness-level method from Theorem [\ref=the:fitness-levels]. The refined upper bound method from Theorem [\ref=the:refined-upper-bound] is hence more general than the classical method from Theorem [\ref=the:fitness-levels]. Lower viscosities lead to better upper bounds. For instance, a constant viscosity between 0 and 1 typically reduces the upper bound by a constant, compared to Theorem [\ref=the:fitness-levels]. Unlike for lower bounds, a viscosity of χ  =  0 is impossible. Similar to the lower bound, we have that [formula] is now an upper bound for the expected number of gained fitness levels in an improvement from level i.

Let Ei be the worst-case expected remaining optimization time, given that the algorithm is in Ai. The worst case is over all histories that contain at least one search point in Ai. By the law of total expectation the unconditional expected optimization time is at most [formula], hence we only need to bound Ei.

Assume for an induction that for all i  <  k  ≤  m - 1 it holds

[formula]

bk denoting an upper bound for Ek. The assumption holds trivially for i = m - 1.

We now claim that Ei  ≤  bi. Note that the bounds are non-increasing: [formula]. The reason is that for all j  >  i we have

[formula]

as (1 - χ)sj  ≤  sj + 1 by assumption. Now, if Ei  ≤  bi + 1 then also Ei  ≤  bi and the claim follows. We therefore assume Ei  >  bi + 1 in the following, which implies Ei  >  bj for all j  >  i. Intuitively, this means that, when relying on Ei and the upper bounds [formula], leaving Ai towards any Aj, j  >  i, is always better than staying in Ai. We are being pessimistic if we overestimate the probability of staying in Ai.

This justifies the following recurrence. After one step the algorithm is in Ak with probability at least siγi,k and then the expected remaining optimization time is bounded by bk. The algorithm remains in Ai with probability [formula] and then the remaining time is again bounded by Ei. This gives

[formula]

and rearranging yields

[formula]

Then we get

[formula]

An Exact Formula for LeadingOnes

Our first application of the lower-bound method is for LO as here the γ-values can be estimated in a very natural and precise way.

Let Xμ be a random variable that describes the maximum LO-value among μ individuals created independently and uniformly at random. For every n  ≥  2 the expected optimization time of every mutation-based EA on LO using mutation probability 0  <  p  ≤  1 / 2 is at least

[formula]

the last inequality holding for p  ≥  n-  Ω(1) and p  ≤   ln  ln n  ·  1 / n.

Consider the canonical partition and assume that the algorithm is on level i  <  n. This implies that in the best individual created so far the first i + 1 bits are predetermined. In addition, in all individuals created so far the bits at positions [formula] have not contributed to the fitness yet. These bits have been initialized uniformly at random and they have been subjected to random mutations. It is easy to see that this again results in uniform random bits. More precisely, the probability that a specific bit j with j  ≥  i + 2 in a specific individual has a specific bit value 0 or 1 is exactly 1 / 2 (see the proof of Theorem 17 in Droste, Jansen, and Wegener [\cite=Droste2002]).

Consider an individual x that has been selected as parent among the created individuals. Let [formula]. We bound the probability of creating an offspring with k leading ones for some i + 1  ≤  k  ≤  n. One necessary condition is that the first j leading ones do not flip, which happens with probability (1 - p)j. The bit at position j + 1 is 0, hence it must be flipped. All bits at positions [formula] must obtain the value 1 in the offspring. This probability is determined by the number of ones among these bits. But clearly (1 - p)i - j is a lower bound on this probability since this reflects the best-case scenario that all these bits are 1 in the parent. (Since p  ≤  1 / 2 the probability of flipping a bit is not larger than the probability of not flipping it.) The last necessary condition is to create exactly k - 1 - i ones among at positions [formula]. By the preceding arguments on the "randomness" of these bits, the probability of creating exactly k - 1 - i ones is 2- k + i: = γi,k if k  <  n and 2- k + i + 1: = γi,k if k = n. Putting everything together, we have that [formula] is an upper bound on the probability of jumping to level k.

Checking the condition on the γ-values, [formula] and for all i  <  j  ≤  n condition [\eqref=eq:gamma-condition] holds with equality since

[formula]

Setting χ  =  1 / 2, the preconditions for Theorem [\ref=the:lower-bound-method] are fulfilled. Using ui: = p(1 - p)i, this proves the bound

[formula]

and hence [\eqref=eq:LO-general-lower].

For the second bound, observe that the bracketed term in [\eqref=eq:LO-general-lower] can be simplified as

[formula]

The third bound [\eqref=eq:LO-precise-lower] follows by simple calculations and the following case distinctions. Note that due to the asymptotic term - O( log n) we only need to prove the bound for large n, i. e., for n  ≥  n0 where we can fix [formula].

Observe that the bound [\eqref=eq:LO-general-lower] is never larger than μ̄: = 1 / (2p(1 - p)n - 1), even for the special case Xμ  =  0. If μ  ≥  μ̄ then the probability that the optimum is not found during the first μ̄ individuals created during initialization is at most μ̄  ·  2- n  ≤  1 / n for n large enough. This proves the claimed lower bound.

If μ  ≤  μ̄ then [formula]. Pessimistically assuming that Xμ  =   log (μ̄ / p) in case Xμ  ≤   log (μ̄ / p) and estimating the conditional expected optimization time by 0 in case Xμ  >   log (μ̄ / p) results in the following bound.

[formula]

We use (1 - p)- n + 1  ≤  (1 - p)- n  ≤  epn  ≤  eln  ln n  =   ln n to estimate the term - p(1 - p)- n + 1. For the same reason log (μ̄ / p)  ≤   log (1 / (2p2)  ·   ln n)  =  O( log n), recalling p  ≥  n-  Ω(1). Assuming that n is large enough to make p  ·   log (μ̄ / p)  ≤   ln  ln n  ·  O(( log n) / n)  ≤  1 / 2,

[formula]

Together, we get a lower bound of

[formula]

Note that a term - O( log n) / p is, in general, necessary since with, say, μ = n an EA will start with an average of Θ( log n) leading ones in the best search point. As the with mutation probability Θ(1 / n) needs expected time Θ(n log n) to collect Θ( log n) leading ones, the needs roughly Θ(n log n)  -  n less generations than the .

For the uiγi,j is the exact probability of jumping from fitness level i to level j  >  i. Also recall that all conditions [\eqref=eq:gamma-condition] on the γi,j-values hold with equality. Therefore, defining si: = ui and using γi,j and χ as in Theorem [\ref=the:lower-LO], we get an upper bound for the using Theorem [\ref=the:refined-upper-bound]. It is easy to see that (1 - χ)si  ≤  si + 1 for all 0  ≤  i  ≤  n - 2 as 1 / 2  ·  p(1 - p)i  ≤  p(1 - p)i + 1 is equivalent to 1 / 2  ≤  1 - p. The resulting upper bound equals the lower bounds [\eqref=eq:LO-general-lower] and [\eqref=eq:LO-general-lower-refined] from Theorem [\ref=the:lower-LO].

As the upper bound holds for the but the lower bound holds for all mutation-based EAs, this proves that among all mutation-based EAs the is an optimal algorithm for the function LO.

The term [\eqref=eq:LO-general-lower] describes the exact expected optimization time of the with mutation probability 0  <  p  ≤  1 / 2 on LO. Among all mutation-based EAs with mutation probability 0  ≤  p  ≤  1 / 2, the , for an appropriate choice of μ, minimizes the expected number of function evaluations.

For μ = 1 we get the following.

The expected optimization time of the with mutation probability 0  <  p  ≤  1 / 2 on LO is exactly

[formula]

The second bound follows from a simple but tedious calculation. It is omitted here. For p = 1 / n we get that the expected running time of the is

[formula]

The factor preceding n2 converges to (e - 1) / 2 from below. Note that we have reproduced one of the main results from Böttcher, Doerr, and Neumann [\cite=Boettcher2010] for general mutation probabilities. The latter authors derived the same formula and used it to compute the optimal mutation probability. They found that p  ≈  1.59 / n is the optimal fixed mutation probability in that it minimizes the expected number of function evaluations. Our lower-bound method allows for the same conclusions to be drawn. Even stronger, while Böttcher et al. [\cite=Boettcher2010] only consider the , we can make the following statement for the broad class of mutation-based EAs.

Among all mutation-based EAs the expected number of fitness evaluations on LO is minimized by the with mutation probability p = 1.59 / n and 1  <  μ  =  O(n log n).

As shown by Böttcher, Doerr, and Neumann [\cite=Boettcher2010], the expected optimization time can be further decreased by allowing adaptive schemes for choosing the mutation probability. Theorems [\ref=the:LO-upper-bound] and [\ref=the:optimal-algorithm-for-LO] only apply to fixed mutation rates. This is not due to a limitation of the lower-bound method. The method is applicable to their adaptive algorithm as well. We refrain from going into detail as this would overlap to a large extend with results already published in [\cite=Boettcher2010].

A Lower Bound for OneMax

We turn to the function OneMax instead. This function is the easiest function with a unique global optimum and it has been studied in the context of many search heuristics [\cite=Droste2002] [\cite=Lassig2010a] [\cite=Gutjahr2008a] [\cite=Neumann2010a] [\cite=Droste2006a] [\cite=Sudholtsubmitteda]. In this section we now derive a lower bound for the expected running time of all mutation-based EAs on OneMax. This lower bound will be very close to a simple upper bound for the . Using the fitness-level method for upper bounds, the expected running time of the with mutation probability p can easily be bounded as follows.

Let H(n) denote the n-th harmonic number. For any initial search point, the expected running time of the with mutation probability p, 0  <  p  <  1, is bounded from above by

[formula]

Define the canonical fitness levels [formula] for 0  ≤  i  ≤  n. The increase the current fitness level i  <  n if only a single 0-bit flips and no 1-bit flips. This probability is at least

[formula]

resulting in the upper bound

[formula]

The second bound follows from H(n)  ≤  ( ln n)  +  1.

We remark that Witt [\cite=Witt2011a] recently presented a similar, but more complicated upper bound. It applies to all linear functions and also allows for tail bounds.

The main result in this section is the following lower bound.

The expected optimization time of every mutation-based EA using mutation probability p on OneMax with n  ≥  2 bits is at least

[formula]

if 2- n / 3  ≤  p  ≤  1 / n and at least

[formula]

if [formula].

For the default mutation probability p = 1 / n, we get the following using the common estimation 1 / n  ·  (1 - 1 / n)n  ≤  1 / (en).

The expected optimization time of every mutation-based EA using the default mutation probability p  =  1 / n on OneMax is at least

[formula]

Note that for mutation probabilities p  =  α / n for some polylogarithmic term [formula] (defined as O( log kn), k  >  0 an arbitrary constant), the term ln (1 / (p2n)) in the second bound of Theorem [\ref=the:lower-onemax] simplifies to ln (n / α2)  =   ln n  -  2 ln (α)  =   ln n  -  o( ln n). Hence, for mutation probabilities up to [formula], Theorem [\ref=the:lower-onemax] gives lower bounds that match the simple upper bound from Theorem [\ref=the:upper-onemax] up to lower-order terms.

An immediate conclusion from this result is that for the mentioned mutation probabilities the expected running time of the is dominated by the term [formula]. (Recall that for all mutation probabilities not covered by Theorem [\ref=the:lower-onemax] the expected running time is exponential by Theorem [\ref=the:pointless-mutation-rates].) As p(1 - p)n - 1 is maximized by the choice p: = 1 / n, the expected running time is minimized for this value, assuming that n is large enough. This establishes p = 1 / n as the optimal mutation rate for the on OneMax.

This finding has recently been derived independently by Witt [\cite=Witt2011a]. His result holds for all linear functions. The proof uses sophisticated drift analysis techniques. In this light it is surprising that the same statement (for OneMax) can be derived by simple fitness level arguments. This further demonstrates the strength of the new lower bound method.

In order to show Theorem [\ref=the:lower-onemax], we first show the following upper bounds on transition probabilities by mutation on OneMax. The lemma may be of independent interest.

Let pi,i + k denote the probability that mutating a search point with i 1-bits using mutation probability p results in an offspring with i + k 1-bits. For every [formula] we have

[formula]

If, additionally, [formula] and i  ≥  2n / 3 then for every 0  ≤  i'  ≤  i

[formula]

The last statement means that, under the stated conditions, starting from a search point with a smaller number i'  <  i of 1-bits does not give a better guarantee on the probability of jumping to level i + k. This statement always holds for mutation probability p = 1 / n, even without the mentioned conditions. However, for larger mutation probabilities this is non-trivial. There are examples where, under conditions different to the ones in Lemma [\ref=lem:p-i-to-i+k], pi',i + k  >  pi,i + k for i'  ≤  i.

An offspring with i + k 1-bits is created if and only if there is an integer [formula] such that j 1-bits flip and k + j 0-bits flip. Using (k + j)!  ≥  k!(j + 1)! for all [formula], [formula],

[formula]

The second bound for i'  =  i follows from

[formula]

For i'  <  i let d: = i  -  i'. Note that i  ≥  2n / 3 implies

[formula]

hence [formula]. Along with the first statement, we have

[formula]

Now we proceed with the proof of the lower bound.

Assume that n  ≥  91 as otherwise both bounds are negative and the claim is trivial. If [formula] then the probability that the first μ̄ search points generated during initialization find the optimum is at most μ̄  ·  2- n  ≪  1 / 2, which establishes the lower bound [formula] and proves both bounds. In the following we assume μ  ≤  μ̄ and neglect the cost of initialization.

Let [formula]. Consider the following partition [formula]. Define [formula] for [formula] and let [formula] contain all remaining search points. With probability at least [formula] for n  ≥  91 the initial population only contains individuals on the first fitness level.

For j  >  i let pi,j be the probability of the event that mutating an individual with i ones results in an offspring that contains j ones. If [formula] then

[formula]

From Lemma [\ref=lem:p-i-to-i+k] we know that then for every [formula] and every i'  ≤  i

[formula]

Without loss of generality, we can assume i': = i in the following, i. e., that the algorithm always selects a best individual from the population as parent. For [formula] define

[formula]

where the prime indicates that these will not be the final variables used in the application of Theorem [\ref=the:lower-bound-method]. Observe that

[formula]

Since Theorem [\ref=the:lower-onemax] requires the γi,j-variables to sum up to 1, we consider the following normalized variables: [formula] and [formula]. As uiγi,j  =  ui'γi,j'  ≥  pi,j, the conditions on the transition probabilities are fulfilled. The condition [formula] is equivalent to [formula]. Also note that

[formula]

the second inequality following from [formula] if p  ≤  1 / n and [formula] if p  ≥  1 / n. Noting that [formula], we get

[formula]

Hence, choosing [formula] we obtain

[formula]

as required. Now that all conditions are verified, we proceed by estimating the variables ui. Bounding the sum of the γi,j'-values as before,

[formula]

Using 1 + x  ≤  1 / (1 - x) for x  <  1 and [\eqref=eq:bound-i(n-1)p^2] we get

[formula]

Applying Theorem [\ref=the:lower-bound-method] and recalling that the algorithm is initialized on the first fitness level with probability at least [formula] yields the lower bound

[formula]

Since [formula] for any [formula], the bound is at least

[formula]

Note ln ( log n)  =   ln (( ln n) /  ln 2)  =   ln  ln n  -   ln  ln 2  <   ln  ln n  +  0.37. For p  ≤  1 / n and n  ≥  91 the lower bound simplifies to

[formula]

For [formula], using again n  ≥  91, we get

[formula]

The above lower bound holds for a very broad class of evolutionary algorithms. This indicates what performance can be achieved by EAs using the most common mutation operator, and what the optimal mutation rate is. It is interesting to note that the lower bound does not apply to all known search heuristics, though. Some search heuristics can perform better, including local mutation operators flipping only a single bit [\cite=Droste1998], quasirandom evolutionary algorithms [\cite=Doerr2011c], biased mutation operators [\cite=Jansen2010], and genetic algorithms with a fitness-invariant shuffling operator [\cite=Koetzing2011a].

A Lower Bound for all Functions with Unique Optimum

Intuitively, OneMax is the easiest function with a unique global optimum. The function gives the best possible hints to reach the optimum. This can be regarded as the task of finding a single target point in the search space. A lower bound for the time until this target is found also applies to a much broader class of functions.

We therefore consider the class of functions with a unique global optimum. This class contains all linear functions, all monotone functions (as defined in [\cite=Doerr2010c]), and all unimodal functions (when unimodality is defined as having a single local optimum). It is even much broader as it also contains many multimodal problems, needle functions, trap functions, and many more functions.

We first consider the lower bound for mutation probability 1 / n from Corollary [\ref=cor:lower-onemax]. Using arguments by Doerr, Johannsen, and Winzen [\cite=Doerr2010], we show that this lower bound transfers to all functions with a unique global optimum. This yields a more precise result than the asymptotic bound Ω(n log n) from unbiased black-box complexity by Lehre and Witt [\cite=Lehre2010].

In [\cite=Doerr2010] the authors proved that the expected optimization time of the with mutation probability 1 / n on OneMax is not larger than the expected optimization time of the on any other function with unique global optimum. Their proof extends to arbitrary mutation-based EAs with mutation probability 1 / n in a straightforward way.

The expected number of function evaluations for every mutation-based EA [formula] with mutation probability 1 / n on every function f with n  ≥  2 bits and a unique global optimum is at least en ln n  -  en ln  ln n  -  3en.

For some a∈{0,1}n denote by fa the function [formula] where [formula] denote the bit-wise exclusive or. Observe that this transformation does not change the behavior of a mutation-based EA in any way, i. e., all mutation-based EAs have the same runtime distribution on fa as on f. Hence, we do not lose generality if we transform the function f in such a way that 1n is the global optimum.

Let [formula] denote the expected optimization time of [formula] on f and assume that the algorithm has already created search points [formula]. Let [formula] be the minimum expected remaining optimization time for [formula] given that [formula] has only created individuals on the first i fitness levels so far, formally [formula] with [formula] the canonical partition for OneMax.

Observe that by definition, since the conditions on [formula] are subsequently restricted,

[formula]

Further define a more specific and slightly modified quantity for the : let [formula] be defined like [formula], but with the additional condition that the history [formula] contains at least one search point in Ai. Since we have only added a constraint, [formula].

Following Doerr, Johannsen, and Winzen [\cite=Doerr2010], we now prove inductively that for all i it holds [formula]. Clearly [formula]. Assume [formula] for all j  >  i. Let x' be the next offspring constructed by [formula]. If the best OneMax-value seen so far is at most i and [formula] then the expected remaining optimization time is at best [formula] (or larger). If the new offspring has a smaller number of ones, the remaining expected optimization time is still bounded below by [formula]. Thus, using the assumption of our induction,

[formula]

The best distribution for [formula] is obtained when a parent z with exactly i ones is selected. A formal proof of this claim is given in [\cite=Doerr2010]. (Note that the probability of selecting such a z might be 0, in which cases the real bound is even larger.) Let Z be the random number of ones when mutating z, then

[formula]

On one hand this is equivalent to

[formula]

On the other hand for the on OneMax we have

[formula]

which is equivalent to

[formula]

Taking [\eqref=eq:Tmuea] and [\eqref=eq:Tea] together yields [formula]. Moreover, [formula]. As [formula] and [formula] are initialized in the same way, they share the same distribution for the initial fitness level. We conclude [formula] and the bound follows from Corollary [\ref=cor:lower-onemax] applied to [formula].

Witt [\cite=Witt2011a] recently generalized the above proof towards arbitrary mutation probabilities and stochastic dominance. The latter is a stronger statement than a comparison of expectations. If the running time of an algorithm [formula] dominates that of B then this implies that the expected running time of [formula] is higher than that of B.

The generalization towards arbitrary mutation probabilities p  ≤  1 / 2 is non-trivial. In contrast to the above proof, it is not always the case that choosing the parent with the largest number of 1-bits yields the best progress. For this reason, we just cite his result here.

Consider a mutation-based EA [formula] with population size μ and mutation probability p  ≤  1 / 2 on any function with a unique global optimum. Then the optimization time of [formula] is stochastically at least as large as the optimization time of the [formula] on OneMax.

This immediately implies that the lower bound from [\ref=the:lower-onemax] transfers to every function with a unique global optimum.

The expected optimization time of every mutation-based EA using mutation probability p on every function with a unique optimum is at least

[formula]

if 2- n / 3  ≤  p  ≤  1 / n and at least

[formula]

if [formula].

As a side result, we have also shown that the is an optimal algorithm for OneMax. For every fixed value of μ the is never worse than any other algorithm initialized with μ uniform random individuals. It is interesting to note that, as for LO, the [formula], i. e., the with μ = 1, is generally not the best mutation-based algorithm for OneMax. In fact, for a proper choice of μ and reasonable p the [formula] has a strictly smaller expected optimization time.

Compare, for instance, the with the [formula] for p = 1 / n and μ  =  Θ( log n). For both we consider the time until the algorithms find a search point with at least [formula] 1-bits. It is known that the probability that initialization creates a search point with at least [formula] 1-bits is at least a constant. Hence, with high probability the [formula] will start with at least this value after initialization. (The running time in case this does not happen is negligible.)

Contrarily, if the starts with [formula] 1-bits then by simple drift arguments it needs at least time [formula] to reach a search point with fitness at least [formula]. The reason is that the expected progress is clearly bounded by the expected number of flipping bits, which is 1. It is not hard to see that the then needs [formula] generations in expectation to reach the threshold.

As both algorithms behave equally after having reached the threshold (modulo possible small differences for overshooting the threshold), the is faster than the by an additive term of [formula].

Note that μ cannot be too large, either. It is known that, with high probability, the number of 1-bits in a random search point is at most [formula]. If [formula] then the gets to this threshold faster than the [formula].

Among all mutation-based EAs the expected number of fitness evaluations on OneMax is minimized by the with mutation probability p = 1 / n and [formula].

This result contrasts the result by Borisovsky and Eremeev [\cite=Borisovsky2008] on the optimality of the on OneMax. The authors do not consider the impact of initialization. Strictly speaking, their concept of dominance does not generally hold when comparing an algorithm with the that is initialized in a different way.

As word of caution, we remark that it is clearly not worth optimizing for μ in practice as the differences in the expected running time only concern additive terms of small order.

An Exponential Lower Bound for Long k-Paths

Finally, we extend the proposed lower-bound method towards settings where many transition probabilities have to be considered. A common setting is that transition probabilities to the next few higher fitness levels can be estimated quite easily. But if there are many fitness levels, dealing with those to fitness levels that are "far away" can become tedious. Also, in some settings condition [\eqref=eq:gamma-condition] on the transition probabilities may be violated when transition probabilities become very small. If this only happens when the transition probabilities are very small anyway, we still expect the lower bound from Theorem [\ref=the:lower-bound-method] to hold, apart from small error terms.

This reasoning is made precise in the following theorem. For each fitness level we only consider the next d fitness levels, where [formula] can be chosen arbitrarily. The conditions involving transition probabilities only need to hold for these values. If d  ≪  m this means that we only have to consider a tiny fraction of all transition probabilities. We also introduce a variable α as a lower bound for the probability that a transition is only made to these d levels. The resulting bound equals the one from Theorem [\ref=the:lower-bound-method] apart from a factor αm - i. This factor can be regarded as (an upper bound on) the probability that the algorithm on every fitness level makes jumps up to at most d fitness levels.

Consider an algorithm  and a partition of the search space into non-empty sets [formula]. Choose [formula] and let the probability of traversing from level i to level i  <  j  ≤  i + d in one step be at most ui  ·  γi,j, where [formula].

Define α  =  α(d) such that [formula] for all 1  ≤  i  ≤  m - d - 1. Assume that for all i  <  j  ≤  i + d and some 0  ≤  χ  ≤  1 it holds

[formula]

Then the expected hitting time of Am is at least

[formula]

As the proof is very similar to the proof of Theorem [\ref=the:lower-bound-method], it is omitted. Alternatively, the statement can be proven by conditioning on the event that in each improvement of the current best fitness level the algorithm advances by at most d levels, and applying the law of total expectation.

A prime example for a setting where the new method is applicable is the class of long k-paths. These functions were introduced by Horn, Goldberg, and Deb [\cite=Horn1994], formally defined by Rudolph [\cite=Rudolph1997], and analyzed by Droste, Jansen, and Wegener [\cite=Droste2002]. We stick to a slightly cleaner formulation from [\cite=Sudholt2009]. A long k-path is a sequence of search points called path. Two neighbored points on the path differ in exactly one bit. Assigning increasing fitness values to the points on the path enables an EA to climb up the path. All search points outside the path have worse fitness and they give hints to reach the start of the path.

The parameter k indicates the distance between different parts of the path. For all points x on the path, the i-th successor has Hamming distance i to x, for 1  ≤  i  ≤  k. All further successors of x have Hamming distance at least k to x. This means that in order to take a shortcut on the path, an EA must flip at least k bits at the same time. If k is not too small, an EA typically climbs to the end of the path in small steps. For [formula] the probability of taking a shortcut is exponentially small, and the length of the path is still exponential. More precisely, the length of a long k-path on n bits is k  ·  2k / n - k [\cite=Droste2002] [\cite=Sudholt2009].

Long k-paths are a prime example for this extension because they give rise to a potentially exponential number of fitness values. For every point on the path, the Hamming distances to the next k successors on the path are well known. But for all further search points we only know that they have Hamming distance at least k. Putting d: = k, it is easy to apply the modified lower-bound method.

For simplicity, we assume that the is initialized with the first point on the long k-path. This not an essential restriction. It is very unlikely that the long k-path is reached beforehand as the "density" of points on the long k-path is extremely low, for reasonable values of k. By definition, each Hamming ball of radius k / 2 contains roughly nk / 2 / (k / 2)! search points, but at most k of these can be part of the long k-path. This means that it is extremely unlikely to find a point on the path by chance (except for the first k points), while being guided towards the start of the path.

Consider the with mutation probability p starting at the first point of the long k-path. Let m + 1  =  k  ·  2n / k  -  k be the number of search points on the long k-path, then the expected optimization time of the is at least

[formula]

In order to make sense of this lower bound, note that the term [formula] reflects the expected time to make m specific 1-bit flips. This would be the exact expected optimization time if the would never accept a mutation that flips more than one bit. It also represents an upper bound on the expected optimization time of the by a straightforward application of Theorem [\ref=the:fitness-levels]. The term [formula] is necessary to account for successful mutations that flip more than one bit. The last term [formula] roughly equals the probability that no improving mutation makes a progress by more than k on the path on all fitness levels.

For the common choice [formula] we get the following. The bound from Theorem [\ref=the:lower-bound-long-k-paths] is simplified by applying the inequality (1 - x)m  ≥  e- 2xm for 0  ≤  x  ≤  1 / 2 and m  ≥  1 to [formula].

Consider the with mutation probability 0  <  p  ≤  1 / 3 starting at the first point of the long k-path with [formula]. Then the expected optimization time of the is at least

[formula]

For every 0  <  p  =  o(1) the expectation is

[formula]

i. e., upper and lower bounds are tight up to lower-order terms. Furthermore, the choice p = 1 / n for the mutation probability minimizes the expected number of function evaluations of the in this setting if n is large enough.

The dominant term for p  =  1 / n is [formula]. The leading constant is by a factor of 2e larger than the leading constant in the previous best known lower bound [formula]. The latter can be derived from enhancing the proof of Theorem 23 in [\cite=Droste2002] with modern drift analysis techniques, and assuming that the starts on the first point of the path.

Consider the canonical fitness-level partition [formula], i. e., A0 contains the first point 0n on the path and Am contains the last point on the path. The transition probabilities are cut off after a jump length of d: = k, where k is the parameter of the long k-path. For all 0  ≤  i  ≤  m and 1  ≤  j  ≤  m - i define

[formula]

and

[formula]

Intuitively, by defining these values we pretend that the j-th successor on the path has Hamming distance j, for all j--not just for 1  ≤  j  ≤  k. It is obvious from the definition that [formula] for all 0  ≤  i  ≤  m. For i  <  j  ≤  i + d we have

[formula]

which is precisely the probability of mutation reaching the j-th successor of the current search point on the path.

Define

[formula]

then for i  <  j  ≤  i + d condition [\ref=eq:gamma-condition-capped] resolves to

[formula]

and this is equivalent to

[formula]

which is true since j  ≥  1. Now for all 0  ≤  i  ≤  m  -  d  -  1 we need to define α as a lower bound for

[formula]

The worst case is obtained for i  =  0 where we get

[formula]

Applying Theorem [\ref=the:lower-bound-method-capped] yields the lower bound

[formula]

Conclusions

We have presented a new method for proving lower bounds on the expected optimization time of randomized search heuristics. The method is based on an adaptation of the fitness-level method, with additional conditions on transition probabilities. It is intuitive, elegant, versatile, and easy to apply as one can freely choose values for χ, ui, and γi,j (1  ≤  i  <  j  ≤  m) subject to the required conditions. As a side result, it has also led to a refinement of the well-known upper-bound method with fitness levels.

The lower-bound method has been accompanied by several applications to a broad range of evolutionary algorithms. To this end, we have introduced the class of mutation-based evolutionary algorithms. It captures all EAs that only use mutation, regardless of parent selection or population models. We have derived very precise lower bounds for LO, OneMax, and all functions with a unique global optimum. These bounds apply to all mutation-based EAs. Such a generality was previously only known for black-box complexity results. A further application for the on long k-paths has shown that the method still yields tight lower bounds, even when considering only a tiny fraction of all transition probabilities.

All bounds are parametrized with the mutation probability p. The lower bounds for LO, OneMax, and long k-paths are tight, compared with upper bounds for the , up to smaller-order terms, for all reasonable mutation probabilities. This is a rare occasion of results that are both very general and very precise at the same time.

The results have also allowed to formally identify optimal mutation-based EAs for LO and OneMax, i. e., which algorithm minimizes the expected number of fitness evaluations. In both cases this is a variant of the that creates more than one search point uniformly at random during initialization. Furthermore, we have seen that p  ≈  1.59 / n is an optimal fixed mutation rate for LO (see [\cite=Boettcher2010]), p = 1 / n is optimal for OneMax (see [\cite=Witt2011a]) and p = 1 / n is optimal for the on long k-paths. These very strong conclusions further demonstrate the strength of the new lower-bound method.

Summarizing, we have made an important step forward towards understanding how EAs work, how to find optimal parameter settings, and which EAs are optimal for certain problems. Note that the method itself is not restricted to mutation-based EAs in binary spaces. It is ready to be applied to other search spaces and further stochastic search algorithms; either in its pure form or as a part of a more general analysis.

Acknowledgments

The author was partially supported by a postdoctoral fellowship from the German Academic Exchange Service while visiting the International Computer Science Institute in Berkeley, CA, USA as well as by EPSRC grant EP/D052785/1. The author thanks Jon Rowe for suggesting the term viscosity, Carsten Witt for insightful discussions about large mutation probabilities on OneMax, and Chao Qian for pointing out an error in Theorem [\ref=the:refined-upper-bound].