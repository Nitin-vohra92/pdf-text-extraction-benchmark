Performance of orthogonal matching pursuit for multiple measurement vectors

Introduction

Finding the sparse solution to an under-determined linear equation is one of the most basic problems in some fields of signal processing. It has also received significant attention since the introduction of compressive sensing (CS), which is widely investigated in the past decade [\cite=Romberg]. The basic mathematical formula is

[formula]

In the field of CS, [formula] denotes the measurement vector, [formula] with m < n is called the sensing matrix, and [formula] is the sparse signal to be recovered, which means most of its entries are zero. ([\ref=dj12]) is also termed single measurement vector (SMV) problem. Many algorithms, including orthogonal matching pursuit (OMP), are proposed to solve the SMV problem. The recovery performances of these algorithms are studied in several scenarios, depending on whether the noise on [formula] or [formula] exists. Many researches have been done on the recovery accuracy of OMP in those scenarios [\cite=Ding] [\cite=Denis] [\cite=Mark].

In this paper, the recovery performance of computing sparse solutions to multiple measurement vectors (MMV) is analyzed. The MMV problem was initially motivated by a neuromagnetic inverse problem which is involved in Magnetoencephalography [\cite=Rao]. In a recently proposed system named the modulated wideband converter (MWC) [\cite=Moshe], MMV recovery also plays a important role in detecting the locations of narrowband signals. The MMV problem is formulated as the following equations

[formula]

where [formula], [formula], and [formula]. The matrix [formula] is made up of L measurement vectors, and matrix [formula] is the sparse signal to be recovered. Here, a matrix [formula] is called jointly k-sparse or k-sparse if it contains no more than k nonzero rows.

Recovery algorithms to the MMV problem include convex relaxation and OMPMMV, which is an extension of OMP to the MMV [\cite=J] [\cite=Chen]. Other algorithms, including FOCal Underdetermined System Solver (FOCUSS) [\cite=Rao] and Joint [formula] Approximation Algorithm (JLZA) [\cite=Hyder], are also put forward. When [formula] and [formula] are unperturbed, the recovery process of OMPMMV can be written as:

[formula]

where R denotes the recovery process, k denotes the sparsity level, and [formula] is the approximation of the original sparse signal [formula]. (N0) process is an ideal one. It has been shown that under certain matrix-norm-based conditions, sparse signal can be exactly recovered [\cite=Chen], i.e. [formula].

More generally, the perturbed observation matrix [formula] and sensing matrix [formula] in the form of

[formula]

needs to be taken into account. In such scenario, the recovery process becomes

[formula]

Such consideration is necessary because perturbations on [formula] and [formula] always exist in practice. In many CS scenarios, such as when [formula] represents a system model [\cite=Moshe], [formula] denotes the system perturbation in realization. Also, [formula] denotes the measurement perturbation when quantization effects introduce considerable noise to [formula].

As far as we know, few researches have been done yet on the robustness of OMPMMV in (N2) process. In this paper, the corresponding theorem for SMV in [\cite=Ding] is extended to MMV under the general (N2) scenario. The main result shows that under certain conditions based on the Restricted Isometry Property (RIP), the locations of nonzero rows of [formula] can be exactly recovered, and an upper bound on the recovery error is given. In addition, it is demonstrated that such conditions guarantee exact recovery of [formula] in (N0) process.

Notations and Assumptions

Assume [formula] is a matrix, then its columns and rows are represented as [formula] and [formula], respectively. Notice that in this paper, upper-case letters are used to denote matrices. The support set [formula] denotes the indices of nonzero entries in vector [formula]. The support set of a matrix [formula] is defined as [formula]. The number of elements in [formula] and in [formula] are denoted as [formula] and [formula], respectively.

The symbols σmax(  ·  ), σmin(  ·  ), [formula], and [formula], denote the maximum, minimum nonzero singular values, spectral norm, and Frobenius norm of a matrix, respectively. Let [formula] denote the largest spectral norm taken over all k-column submatrices. The perturbations [formula] and [formula] can be quantified with the following relative bounds,

[formula]

In this paper, it is assumed that ε0,  ε, and εb are far less than 1.

Background

Orthogonal Matching Pursuit for MMV (OMPMMV)

The key idea of OMPMMV, which is similar to OMP, lies in the attempt to reconstruct the support set Λ of [formula] iteratively by starting with [formula]. The pseudo-code of OMPMMV is described in Table [\ref=ompmmvalgorithm]. In fact, the process of OMPMMV can be mathematically expressed as follows.

Suppose [formula]. Let [formula] denote the |Λ|  ×  1 vector containing the entries of [formula] indexed by Λ. Let [formula] denote the m  ×  |Λ| matrix obtained by selecting the columns of sensing matrix [formula] indexed by Λ. [formula] denotes the Moore-Penrose pseudoinverse of [formula]. Define [formula] and [formula] as the orthogonal projection operator onto the column space of [formula] and its orthogonal complement, respectively. Define [formula]. From the theory of linear algebra, orthogonal projection operator [formula] obeys [formula] and that the columns of [formula] indexed by Λ equal zero.

Now, suppose that OMPMMV performs at lth iteration, and Λl - 1 is the estimation of [formula] from the previous iteration. The following discussion demonstrates the generation of Λl.

In the update step of the previous iteration, which is actually solving a least squares problem, it can be derived that

[formula]

In the matching step, one has

[formula]

Then, in the identify step, [formula]

From ([\ref=dj214]), ([\ref=dj228]), and the fact that the columns of [formula] indexed by Λ equal zero, several useful conclusions can be derived.

[formula] Therefore [formula], |Λl - 1| = l - 1.

([\ref=dj228]) can be written as [formula], where [formula] and [formula].

It is easy to check that 1) still holds when [formula] and [formula] in the above analysis are contaminated ones, i.e.

[formula]

The Restricted Isometry Property (RIP)

For each integer [formula], the RIP for any matrix [formula] defines the restricted isometry constant (RIC) δk which is the smallest nonnegative number such that

[formula]

holds for any k-sparse vector [formula]. In other words, [formula] acts as an approximate isometry on the set of k-sparse vectors for a small δk.

Contributions

Theorem 1:   Suppose that the inputs [formula] and [formula] of OMPMMV algorithm are contaminated by noise as in ([\ref=dj413]). Define the relative perturbations ε0, ε, and εb as in ([\ref=dj216]). Let [formula]  and

[formula]

If [formula] satisfies the RIP of order k + 1 with isometry constant

[formula]

then for any k-sparse signal [formula], OMPMMV will recover the support set of [formula] exactly from [formula] and [formula] in k iterations, and the error between [formula] and the recovered signal [formula] can be bounded as

[formula]

The functions in ([\ref=dj17]) and ([\ref=dj100]) are defined as follows:

[formula]

[formula]

Remark 1: The above conclusion generalizes the result of Theorem 1 in [\cite=Ding]. If L = 1, then the observation matrix and the signal matrix are reduced to vectors, which becomes an SMV problem. Thus Theorem 1 here can be regarded as an extension from SMV to MMV framework. Notice that the requirements in the above Theorem 1 and the correspondent theorem in [\cite=Ding] share many similarities in form. The differences include the definition of εb, t0, and the change from [formula] norm of [formula] to Frobenius norm of [formula]. Besides, the estimation error of [formula] is derived in a relative form.

Remark 2: For OMPMMV in the perturbed scenario ([\ref=dj413]), it is unrealistic to achieve the exact recovery of [formula], because the least square problem is actually solved with perturbed data. However, one can still recover the support of [formula]. This is of great significance in many practical applications, e.g. the MWC [\cite=Moshe] mentioned before.

Remark 3: An MMV problem can be considered as how to achieve sparse representations for SMVs simultaneously. However, by looking at εb, we show that MMV has an advantage over a simple combination of SMVs. In ([\ref=dj216]), εb is defined as an upper bound on:

[formula]

From SMV view, [formula] cannot be too large for support recovery. Under the MMV scenario, however, a very large [formula] may be balanced by other [formula], and therefore the support recovery is not influenced.

Though a completely perturbed situation is considered in Theorem 1, it is helpful to consider three specific situations. The following three corollaries are derived from Theorem 1.

Corollary 1:   Suppose that [formula] satisfies the RIP of order k + 1 with isometry constant

[formula]

then OMPMMV will recover [formula] exactly from [formula] and [formula] in k iterations.

The proof of Corollary 1 is directly derived by setting [formula], and ε0  =  ε  =  εb = 0. Thus εh = 0, and ([\ref=dj17]) reduces to ([\ref=dj500]).

Because SMV is a special case of MMV when L = 1 holds, Corollary 1 generalizes the results of Davenport [\cite=Mark] and Liu [\cite=Entao]. It was shown in the SMV case that [formula] is sufficient for OMP to exactly recover any k-sparse signal [\cite=Mark] (Th.3.1). Later, Liu and Temlyakov relaxed the bound on the isometry constant to [formula] [\cite=Entao] (Th.5.2).

Corollary 2:   Suppose that [formula], [formula], t0, and εb meet the assumptions made in Theorem 1, and [formula], which means that only the observation matrix is perturbed. Define

[formula]

If [formula] satisfies the RIP of order k + 1 with isometry constant

[formula]

then OMPMMV will recover the support set of [formula] exactly from [formula] and [formula] in k iterations. The recovery error can be bounded as

[formula]

Corollary 3:   Suppose that [formula], [formula], t0, ε0, and ε meet the assumptions made in Theorem 1, and [formula], which means that only the sensing matrix is perturbed. Define

[formula]

If [formula] satisfies the RIP of order k + 1 with isometry constant

[formula]

then OMPMMV will recover the support set of [formula] exactly from [formula] and [formula] in k iterations. The recovery error can be bounded as

[formula]

The above two corollaries suggest that the relative recovery error scales almost linearly with the noise level εb or ε when k is fixed.

Proofs

Some Lemmas

Before proceeding to the proof of Theorem 1, some helpful lemmas are provided first.

Lemma 1:   Suppose vector [formula], then [formula], i.e. for arbitrary [formula],

[formula]

Lemma 2 ([\cite=Mark], Lemma 3.1 ):   Let [formula] be given, and suppose that matrix [formula] satisfies the RIP of order [formula] with isometry constant δ. Then

[formula]

Lemma 3 ([\cite=Mark], Lemma 3.2 ):   Suppose that [formula] satisfies the RIP of order k with isometry constant δ, and let [formula]. If |Λ| < k, then

[formula]

holds for all [formula] such that [formula] and [formula].

Lemma 4:   Let [formula] and suppose [formula] with [formula]. Define [formula]. Then if [formula] satisfies the RIP of order [formula] with isometry constant δ, it holds that

[formula]

for all j∉Λ.

According to the definition of [formula], it can be implied that

[formula]

where [formula] denotes the j-th vector from the cardinal basis. Lemma 3 implies that the restriction of [formula] to the columns indexed by Λc satisfies the RIP of order [formula] with isometry constant δ / (1 - δ).

Consider the scenario of j∉Λ. Because for all [formula], [formula] and [formula], it can be concluded from Lemma 2 that

[formula]

Combining ([\ref=dj102]) with ([\ref=dj1101]), it can be easily derived that

[formula]

Lemma 5:   Suppose that [formula] meet the assumptions specified in Lemma 4, C is a constant, and [formula] is the matrix in the matching step of OMPMMV which satisfies [formula]. If

[formula]

it is guaranteed that [formula] [formula].

According to Lemma 4, for all j∉Λ, [formula], thus

[formula]

Then for indices j∉ [formula], we will have [formula] (Recall that [formula] for j∈Λ). If ([\ref=dj104]) is satisfied, then there exists some [formula] with [formula]. From ([\ref=dj103]) and the triangle inequality, it can be concluded that for this index j, [formula].

Lemma 6:   Suppose that [formula] holds in the l-th ([formula]) iteration during OMPMMV running, and [formula] satisfies the RIP of order k + 1 with isometry constant

[formula]

Here, C and [formula] are two constants. Then OMPMMV will recover the support set of [formula] exactly from [formula] and [formula] in k iterations.

The proof works by induction. To begin with, consider the first iteration where [formula]. Because [formula], one has

[formula]

and this implies that

[formula]

Notice that [formula] is no less than [formula]. Therefore ([\ref=dj104]) can be satisfied. Consequently, according to Lemma 5, it is proved that

[formula]

Furthermore, consider the general induction step. Suppose that OMPMMV is at lth iteration and all previous iterations succeed, which means that Λl - 1 is a subset of [formula]. Recall that the intersection of [formula] and Λl - 1 is empty, |Λl - 1| equals l - 1, and [formula] is not greater than k - l + 1. Assume that [formula] satisfies the RIP of order [formula] with isometry constant δl - 1. Because

[formula]

and from the fact that t0 is not greater than [formula], one has

[formula]

which implies that

[formula]

Notice that

[formula]

Thus ([\ref=dj104]) can be satisfied. Therefore, according to Lemma 5, it can be concluded that

[formula]

and thus

[formula]

which completes the proof of induction.

Proof of Theorem 1

Similar to the scenario of OMP, here we need to give the upper bound on [formula] (for all [formula]), and then replace the C in Lemma 6 with this bound. [formula] can be written as [formula] where

[formula]

From the proof of (38) in [\cite=Ding], there is

[formula]

Combining this and Lemma 1, one gets

[formula]

Notice that

[formula]

Applying ([\ref=n2]) to ([\ref=n1]), one gets the upper bound on [formula]:

[formula]

At the end of the proof, ([\ref=dj100]) is proved as follows. According to ([\ref=dj601]) and ([\ref=dj413]), there is

[formula]

Because [formula] is exactly recovered, one has

[formula]

Thus

[formula]

Notice that

[formula]

it can be concluded that

[formula]

It has been mentioned in [\cite=Mark] that the theoretical upper bound of δk + 1 is [formula] for exact recovery of support set, if δk + 1 is used as a sufficient condition for recovery of [formula]. Thus, [formula]. Applying this to ([\ref=dj101]), one finally gets ([\ref=dj100]).

Conclusion

In this paper, robustness of OMPMMV under general perturbations, which are in the form of [formula] and [formula], is studied. Though exact recovery of the sparse solutions [formula] from [formula] and [formula] is no longer realistic, Theorem 1 shows that exact recovery of the support set can be guaranteed under suitable conditions, which is important in many practical applications. Furthermore, the recovery error is bounded. This completely perturbed framework extends some prior work in [\cite=Mark] [\cite=Entao] [\cite=Ding] from SMV to MMV.