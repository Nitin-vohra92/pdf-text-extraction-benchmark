Significant Subgraph Mining with Multiple Testing Correction

Introduction

A graph is one of the most general data types to represent structured objects, and massive amounts of structured data are now available as graphs across a wide range of domains, such as chemical compounds in PubChem [\cite=Bolton08], biological pathways in KEGG [\cite=Kanehisa00], protein structures in PDB [\cite=Berman00], and social networks on the web. Analyzing such databases, that is, graph mining, has evolved into an important branch of data mining and knowledge discovery. Graph databases often include two or more distinct classes of graphs and, in many application domains, the ultimate purpose is to discover significant subgraphs that are statistically significantly enriched in one particular class of graphs. In drug discovery, for instance, chemists try to identify a key substructure of chemical compounds which is significantly associated with a particular activity, e.g., anticancer activity [\cite=Takigawa13]. In a similar fashion, biologists seek substructures of proteins that are required for particular docking events [\cite=Weil09].

Finding such significant subgraphs is an open problem, as the large number of candidate subgraphs causes both a computational and a statistical problem: the computational problem is that it is often extremely expensive to check all subgraphs for enrichment, given that their number scales exponentially in the number of nodes of the largest graph in the database. The statistical problem is the multiple hypothesis testing problem caused by the fact that a huge number--often billions--of subgraphs are being tested for significant enrichment, each of which represents a hypothesis. If one ignores this multiple testing problem, one may find an enormous number of false positives, subgraphs that are deemed to be significant by mistake. In particular in the natural sciences, where significant subgraphs typically undergo further experimental investigation, a large number of false positives leads to a severe waste of time and resources. Thus multiple testing correction, calibration of the significance level in each test, is needed to control the total error rate of false positives.

Our goal in this paper is to overcome these two problems: we present efficient strategies to detect significantly enriched subgraphs while correcting for multiple testing.

A common approach to multiple testing correction is Bonferroni correction [\cite=Bonferroni36]. It tends to be highly conservative, that is, it will miss many significant observations if the number of tests performed is massive, as in graph mining or pattern mining in general. Tarone [\cite=Tarone90] proposed an improved, less conservative Bonferroni correction on categorical data. Key to this strategy is that on categorical data, only a subset of tests, called testable hypotheses, can reach significance, thereby hypotheses that are not testable can be safely removed without affecting the probability of reporting false positives. Terada et al. [\cite=Terada13] recently made it possible to enumerate testable hypotheses using a frequent itemset mining algorithm and successfully applied Tarone's insight for discovering significant combinations of transcription factors in gene regulatory network analysis.

A relevant question for graph mining is whether Tarone's strategy of only correcting for testable hypotheses can be successfully transferred to significant subgraph mining as well. This is not a trivial question as the search space in graph mining is often exponentially larger than that in itemset mining due to combinations of vertices and edges. In this paper, we give a positive answer to this question by (1) extending the approach by Terada et al. [\cite=Terada13] to solve the important open problem of significant subgraph mining with multiple testing correction via frequent subgraph mining  [\cite=Borgelt02] [\cite=Inokuchi00] [\cite=Nijssen04] [\cite=Yan02], (2) proposing efficient search strategies for detecting testable subgraphs, one of which is empirically orders of magnitude faster than their method, and (3) further improving over naïve Bonferroni correction by considering the dependence between subgraph occurrences [\cite=Moskvina08] [\cite=Nyholt04].

This paper is organized as follows: we present our approach to significant subgraph mining in Section [\ref=sec:method]. First we provide the necessary statistical concepts and problem statements (Sections [\ref=subsec:significant], [\ref=subsec:multhyptest], and [\ref=subsec:testable]), then we propose search algorithms for significant subgraph detection in Section [\ref=subsec:enumeration], followed by introducing the improved multiple testing correction via the effective number of tests in Section [\ref=subsec:effective]. We discuss related work in Section [\ref=sec:related] and evaluate our algorithms on real-world datasets in Section [\ref=sec:experiment]. Finally, we summarize our contributions in Section [\ref=sec:conclusion].

Method

Let G be a graph, which is mathematically defined as an ordered pair of vertices V(G) and edges E(G)  ⊆  V(G)  ×  V(G). A graph H is a subgraph of G, denoted by [formula], if its vertex set V(H) is a subset of V(G) and its edge set E(H) is a subset of E(G) and is restricted to its vertices, i.e., V(H)  ⊆  V(G) and [formula]. Our notation is summarized in Table [\ref=Table:notation].

In the following we assume that our datasets of graphs comprises two classes of graphs, but our results also transfer to more than two classes when considering one-versus-rest classification, that is enrichment of a subgraph in one class versus all others.

Statistically significant subgraphs

Suppose we are given two collections of graphs G and G', where the numbers of graphs in these sets are |G|  =  n and |G'|  =  n' with n  ≤  n' without loss of generality. For each subgraph [formula] with [formula], we formulate a null hypothesis that the occurrence of the subgraph H is independent from the class membership of G. Our task is to find for which subgraphs H the data provide enough evidence to reject the null hypothesis and to deem H as a significant subgraph associated with the class membership.

From given data, we measure the statistical association between two binary random variables: the indicator vector of the class membership and the occurrence/absence of the subgraph H within each graph G in the database.

Let x and x' be the frequencies of H in G and G', respectively. That is, [formula] and [formula], as represented in the following 2  ×  2 contingency table.

The strength of the association between binary random variables is quantified as a p-value, defined as the probability of observing an association at least as strong as the one present in the data under the assumption that the null hypothesis of independence holds true. To compute the p-value, Fisher's exact test is commonly used. It relies on the fact that, when the margins x + x', n, and n + n' are fixed, the probability q(x) of obtaining these counts x and x' is given by the hypergeometric distribution:

[formula]

Formally, define [formula] and [formula] as the left-tail and the right-tail of the hypergeometric distribution, respectively. That is, given an observed count x, [formula] is the probability of observing a smaller count and [formula] the probability of observing a larger one:

[formula]

They are used as one-tailed p-values, and a two-tailed p-value [formula] is defined as [formula] [\cite=Bland00].

We say that a subgraph H is statistically significant if its p-value is smaller than a predetermined significance level α. Note that, by construction, α equals to the Type I error probability; the probability of falsely deeming a subgraph significant.

Multiple hypothesis testing

In our setup, one must test all subgraphs in a database. The procedure described above guarantees that, for a single subgraph, the probability of being a false positive is upper bounded by α. However, when many hypotheses are tested in parallel, the probability that at least one subgraph is a false positive, called the Family-Wise Error Rate (FWER), approaches one. This is the well-known multiple hypothesis testing problem.

To deal with this issue, one needs to correct the significance level α in each test to guarantee that [formula]. The most common method is the Bonferroni correction [\cite=Bonferroni36], which simply divides α by the number m of tests. The resulting FWER can be readily shown to be smaller than α. The number of tests m is called the Bonferroni factor, which in our case is the same as the number of subgraphs. Despite its popularity, the Bonferroni correction is known to be too conservative in many cases, that is, the statistical power, the probability to detect truly significant subgraphs, becomes too small. The problem is even more extreme in our application: as m is the huge number of subgraphs tested, the Bonferroni corrected significance level α  /  m is so small that hardly any subgraph can ever reach the significance level.

Testable subgraphs

Tarone [\cite=Tarone90] showed that when testing the association of discrete random variables, as in our setup, one can improve the Bonferroni correction. The key idea is that the discreteness of the problem implies the existence of a minimum achievable p-value for each subgraph H. Let [formula] be the frequency of H in the whole set of graphs [formula], and assume that f(H)  ≤  n.

If the marginals f(H), n, and n' are fixed, the minimum p-value, denoted by [formula], is achieved for the most biased case when x  =  0 or x = f(H). Since [formula] and [formula] are minimized at x  =   max {0,f(H)  -  n'} and x  =   min {f(H),n}, their minimum values are q(0) and q(f(H)), respectively. From n  ≤  n', q(f(H))  ≤  q(0) holds. Thus we have

[formula]

for a one-tailed test, and this value is doubled for a two-tailed test. If f(H)  >  n and hence f(H)  =  x  +  x'  >  (n  +  n')  /  2, we follow the definition in [\cite=Terada13], that is, we simply define [formula]. Then ψ is always monotonically decreasing, which is required for our algorithms.

If the minimum p-value [formula] is larger than the significance threshold, the subgraph H can never be significant regardless of the class membership of the graphs in which it occurs. Tarone's insight is that such untestable subgraphs do not increase the FWER, and hence we can exclude them from candidate subgraphs and reduce the Bonferroni factor. Formally, let [formula] be the set of all subgraphs in the database and define for each natural number k

[formula]

as the number of subgraphs whose minimum achievable p-value is smaller than α  /  k. Let [formula] satisfy

[formula]

that is, [formula] is the rounded root of m(k)  -  k. Since m(k) monotonically decreases as k increases, we have m(k)  -  k  >  0 for all [formula] and m(k)  -  k  ≤  0 for all [formula]. Then we can see that [formula] even if we reduce the Bonferroni factor from |H| to [formula], since we have As a result, we have the set of testable subgraphs τ(H), which is given by

[formula]

and our task of detecting all significant subgraphs is achieved by finding the root [formula] and enumerating the set τ(H) of testable subgraphs.

Terada et al. [\cite=Terada13] used Tarone's method in the context of itemset mining for discovering gene regulatory motifs, where efficient enumeration of testable itemsets was achieved by applying a frequent itemset mining algorithm. Next we show how to apply Tarone's method to significant subgraph mining.

Enumeration of testable subgraphs

To use Tarone's results for our purpose, the challenge is now to efficiently compute all testable hypotheses, that is all testable subgraphs. Here we show how to use frequent subgraph mining to enumerate all testable subgraphs. Frequent subgraph mining algorithms find all subgraphs whose frequencies are higher than the user specified threshold σ (or its ratio θ  =  σ  /  (n  +  n')). Since the minimum p-value ψ is a monotonically decreasing function (the proof is provided in [\cite=Terada13]), we have [formula] for every frequent subgraph H.

The set of testable subgraphs τ(H) coincides with the set of frequent subgraphs for the threshold [formula] such that

Proof. We have for [formula],

In the following, we present four variants to efficiently find this (rounded) root frequency [formula] and enumerate all testable subgraphs. Note that every single method gives exactly the same root frequency and testable subgraphs, resulting in the same significant subgraphs. Our search procedures can be combined with any of the many algorithms for frequent subgraph mining (an FSM algorithm for short), e.g., with AGM [\cite=Inokuchi00], gSpan [\cite=Yan02], Mofa [\cite=Borgelt02], or Gaston [\cite=Nijssen04], as long as they report actual frequencies of detected frequent subgraphs.

An important property of our search algorithms is that they require a significance level α as an input but do not require the frequency threshold to be prespecified, which is attractive as it is often difficult to find an appropriate frequency threshold for a particular problem in practice.

In parallel to our work, a sped up version of LAMP was published by Minato et al. [\cite=Minato14] for significant itemset mining, which also uses incremental search. Unlike our approach, in which pattern mining and incremental search can be combined in an arbitrary, modular fashion, they change the mining process itself to prune untestable hypotheses as early as possible.

[formula]

Effective number of tests

Many subgraphs are expected to be highly correlated with each other due to combinatorial constraints on graphs such as subgraph-supergraph relationships [\cite=Ugander13]. To exploit the dependence between subgraphs and further increase the power, we use the effective number of tests. In the idák correction [\cite=Sidak67], the significance level α' for each test is given as 1  -  (1  -  α)1  /  m for m independent tests. This means that if we have m tests and some of them are correlated, only [formula] tests, defined by

[formula]

are effective for controlling the FWER [\cite=Moskvina08], hence [formula] can be used as a reduced Bonferroni factor. This [formula] is called the effective number of tests and estimation methods, such as the Cheverud-Nyholt estimate [\cite=Nyholt04], have been proposed in particular in statistical genetics.

We directly estimate the significance level α' for each test by random permutations of class labels, which gives the null distribution of independent subgraphs. Although this method gives the optimal estimation of [formula] in theory, its drawback is the high computational cost O(mh) (m  =  |H| in our case), where h is the number of iterations. Here we overcome this drawback by considering only testable subgraphs. Since we can ignore untestable hypotheses (subgraphs) for controlling the FWER, we apply the above permutation-based estimation to only testable subgraphs. The complexity reduces to O(|τ(H)|h), which is expected to be much cheaper than O(|H|h) if we can eliminate many untestable subgraphs. We set the number of permutations to be 1,000 throughout the paper, which is recommended for α  =  0.05 [\cite=Churchill94] and commonly used [\cite=Moskvina08].

Related Work

The statistical significance of subgraph occurrence in networks has been investigated before, first in specific application domains, such as social networks [\cite=Wasserman94] and gene regulatory networks [\cite=Shen02], and the formulation was later extended to general graphs [\cite=Arora14] [\cite=He06] [\cite=Milo02] [\cite=Ranu09] [\cite=Yan08]. In all of these studies, however, the significance is defined using a random database, that is, the p-value of a subgraph is the probability of its frequency being larger than the user-specified threshold under a certain distribution of graphs (or labels on graphs) and, to the best of our knowledge, no study directly detects subgraphs that are significantly associated with class memberships of graphs. Moreover, our method overcomes the following three drawbacks of previous approaches: (1) their p-values depend on the frequency threshold, which is often difficult to determine in practice, while our method requires only the significance level α; (2) their p-value computation requires a distribution of graphs, which is not trivial to estimate, while our method does not need to consider such a distribution and can still calculate the exact p-values; (3) to the best of our knowledge, all previous studies did not consider the multiple testing problem, which leads to many false positives, while our method strictly controls the FWER.

Subgraph detection has also been intensively studied in graph classification, where subgraphs are used as features to describe graphs. This means that each graph G is represented as a feature vector in which each feature corresponds to another graph H and the value is one if [formula] and zero otherwise. The general objective is to find informative subgraphs for discrimination to improve the accuracy of the subsequent classification, which can also be viewed as a supervised feature selection problem. A number of methods have been proposed, for example, gBoost [\cite=Kudo05] and a Lasso-based method [\cite=Tsuda07]. Note that, however, in classification we do not need to control the FWER (false positives) as long as we can build a good classifier, while our ultimate goal in this paper is to detect key substructures for a better understanding of the target phenomenon and the FWER must be controlled to avoid false positives for further investigation in application domains.

Multiple (hypothesis) testing is a classical problem in statistics, with Bonferroni correction [\cite=Bonferroni36] being the most prominent correction technique. Since Bonferroni correction is known to be too conservative, other correction methods have been proposed, for instance, Holm's correction [\cite=Holm79]. However, these methods also require the exact number of tests (subgraphs) for correction, which is highly expensive to compute in graph mining. Another approach is to use random subsampling to estimate the correction factor [\cite=Dudoit03], but this also needs high computational cost if the number of tests is massive. Controlling the false discovery rate (FDR) [\cite=Benjamini95] is recently becoming popular as an alternative to the FWER, which leads to more power in multiple testing. However, it also requires the exact number of tests and hence is also extremely expensive to compute.

Experiments

We examined our methods on real-world graph data and compared them to the brute-force approach (BF for short) and two state-of-the-art approaches (LAMP and LEAP) in our framework. BF naïvely enumerates subgraphs occurring more than once to set the Bonferroni correction factor. Notice that, with respect to assessing the quality of results, that is, the number of significant subgraphs, BF can be our only comparison partner, since there exists no method for finding significant subgraphs while controlling the FWER by multiple testing correction. On the efficiency side, we compare BF and our four search strategies, in which two of them (decremental LAMP search and bisection LEAP search) are the state-of-the-art.

As an FSM algorithm, we employ Gaston [\cite=Nijssen04] since it is reported to be one of the fastest FSM algorithms [\cite=Worlein05]. We integrated our search strategies into Gaston, which are written in C++ and compiled with gcc 4.6.3. The significance level α was always set to 0.05 and a two-tailed test was used. We repeated 1,000 permutations to obtain the effective number of tests. We used Ubuntu version 12.04.3 with a single 2.6 GHz AMD Opteron CPU and 512 GB of memory. All experiments were performed in R 3.0.1.

The PTC (Predictive Toxicology Challenge) dataset contains data of 601 chemical compounds in total (including training and test sets), which is originally designed for a prediction challenge of carcinogenic effects. Graphs are classified according to their carcinogenicity assayed on rats and mice. We assume that graphs labeled as CE, SE, or P as positive, and those of NE or N as negative, the same setting as in [\cite=Kong10] [\cite=Zhao11ICDM]. The dataset is divided into four overlapping subsets according to their animal models: male rats (MR), female rats (FR), male mice (MM), and female mice (FM). We used only MR since the properties of other datasets are similar.

MUTAG [\cite=Debnath91] is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds, which are classified into two classes of mutagenically active or inactive on the bacterium Salmonella typhimurium.

ENZYMES is a dataset of protein tertiary structures used in [\cite=Borgwardt05bio], which consists of 600 enzymes, extracted from the BRENDA database [\cite=Schomburg04]. Each enzyme is classified into one of six Enzyme Commission top level enzyme classes (EC1 to EC6). We classified enzymes from EC1 to EC3 to one class, and from EC4 to EC6 to the other for our binary classification problem.

D&D is a dataset of 1178 protein structures created by Dobson and Doig [\cite=Dobson03], and they are classified into enzymes and non-enzymes. As we can see in Table 1, the size of each graph in this dataset is relatively large compared to the other datasets.

NCI (National Cancer Institute) datasets contain data of chemical compounds that are classified according to their anti-cancer activity [\cite=Wale08]. Datasets are numbered by their bioassay IDs. NCI1 is balanced subsets, which is often used in the literature [\cite=Li12] [\cite=Shervashidze11], and the others are the full sets retrieved from the official website.

The resulting correction factors are plotted in Figure [\ref=Figure:factor] and the numbers of significant subgraphs we detected and the empirical FWERs are shown in Figures [\ref=Figure:signum] and [\ref=Figure:FWER], respectively. There are some missing values in the plots, in particular results of the Bonferroni factor (red cross marks), due to a huge amount of computation time. These plots clearly show that, in all datasets, our correction factor is much smaller than the Bonferroni factor and the difference between them becomes larger as the maximum subgraph size increases. In particular in PTC(MR) and D&D, our factors (blue circles and green triangles) become stable in large maximum subgraph sizes while the Bonferroni factors increase exponentially. The reason might be that most of large subgraphs become untestable because they tend to have small frequencies in general. Moreover, we can confirm that in all datasets correction factors are further reduced using the effective number of tests. This is because many subgraphs are highly correlated with each other due to combinatorial constraints of graphs [\cite=Ugander13].

In terms of the number of significant subgraphs (Figure [\ref=Figure:signum]), we can find more subgraphs because of the reduced correction factor across our datasets. On several datasets the effect is dramatic, such as MUTAG, ENZYMES or D&D, where our methods find thousands of significant subgraphs missed by the standard Bonferroni correction. Examples are shown in Figure [\ref=Figure:example]. In PTC(MR), one cannot find any significant subgraphs by the Bonferroni correction when the maximum subgraph size is larger than 6, but one can detect 2 to 4 (testable) or 3 to 8 (effective) significant subgraphs using our factors. Moreover, the number of significant subgraphs in the Bonferroni factor rapidly decreases in the D&D dataset as the maximum subgraph size increases, while numbers are stable in our methods even if the maximum subgraph size is unlimited. Since it is often difficult to appropriately upper bound the subgraph size beforehand in practice, this is another advantage in practical applications. In NCI220 the number of significant subgraphs exhibits an interesting behavior, that is, significant subgraphs are detected only if the maximum subgraph size is 10 or 11 (testable) and from 10 to 16 (effective). The reason is that the size of these significant subgraphs is 10 or 11 and we cannot detect them if the maximum subgraph size is smaller than that. Furthermore, these subgraphs are no longer significant if the maximum subgraph size becomes larger due to the increase of the correction factor.

We can also confirm the higher statistical power from the empirical FWERs (Figure [\ref=Figure:FWER]). Note that the FWER should be α  =  0.05 in the best case, and the correction factor is too large if the FWER is smaller than α. By reducing the correction factor with the testability criterion and the effective number of tests, the FWERs get closer to α  =  0.05.

The results clearly show that all four searches using the testability criterion are faster than BF on average. This means that reducing the number of subgraph candidates using the testability of them contributes not only to the effectiveness in terms of finding significant subgraphs but also the efficiency of the whole process. Furthermore, our new incremental search is one to two orders of magnitude faster than the other state-of-the-art search strategies (decremental LAMP and bisection LEAP) and more than two orders of magnitude faster than the one-pass search and BF on average. In contrast, the decremental LAMP search is slow, with its speed being similar to the one-pass search on average, and it is often even slower than BF. The reason is that in practice the root frequency [formula] is relatively small (around 20, see Table [\ref=Table:root]) and hence the decremental search needs to repeat an FSM algorithm many times until reaching this frequency. This is also the reason for the efficiency of the incremental search as it can quickly find the root frequency. Although the bisection LEAP search is faster than the decremental and the one-pass search on average, it is slower than the incremental search. The reason is the same as in the discussion above, that is, the root frequency is usually small and it tends to repeat subgraph mining with high frequencies.

The running time for computing the effective number of tests is faster than the above-mentioned search of testable subgraphs in most cases. This means that the testability criterion also contributes to the efficiency of computing the effective number of tests and makes it feasible within a reasonable time.

Conclusion

In this paper, we have presented a solution for finding subgraphs that are statistically significantly enriched in one class of graphs but not another. The difficulty of the problem stems from the two facts that (1) one has to consider an enormous search space of candidate subgraphs and that (2) one has to correct the significance level for multiple testing to control the FWER, as one tests a large number of candidate subgraphs simultaneously. The first problem leads to enormous computational runtime problems, the second one to a loss in the statistical power to detect significant subgraphs.

We have shown that the problem can be exactly and efficiently solved by considering only testable subgraphs, which include all significant subgraphs and dramatically reduce the number of tests performed, thereby leading to a gain in statistical power. Moreover, we can further increase the power using the effective number of tests, which reduces the correction factor according to the dependence between subgraphs. We have presented several search strategies that use frequent subgraph mining algorithms to efficiently retrieve the set of testable subgraphs. Experimental results show that our method finds significant subgraphs with higher speed and higher statistical power than any state-of-the-art approach. This result promises to open the door to many interesting applications in chemoinformatics, structural biology and personalized medicine.

We also believe that our approach lays the foundation for follow-up studies in several important directions: developing and integrating other approaches which exploit the dependence between tests [\cite=Zhang08], considering other types of structured data such as strings, and summarizing the solution set of significant subgraphs, which sometimes grows extremely large.

Acknowledgments

This work was funded in part by a Grant-in-Aid for Scientific Research (Research Activity Start-up) 26880013 (MS), the SNSF Starting Grant "Significant Pattern Mining" (KMB), the Alfried Krupp von Bohlen und Halbach-Stiftung (KMB), and the Marie Curie Initial Training Network MLPM2012, Grant No. 316861. (FLL, KMB).