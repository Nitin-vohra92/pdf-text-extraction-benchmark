Pushbroom Stereo for High-Speed Navigation in Cluttered Environments

Introduction

Recently we have seen an explosion of exciting results on small, unmanned aerial vehicles (UAVs) such as incredible obstacle avoidance and trajectory tracking [\cite=Mellinger10], formation flight [\cite=Mellinger10c] [\cite=Kushleyev12], and cooperative interactions with the environment [\cite=Ritz12] [\cite=Lindsey12] [\cite=Brescianini13]. All these systems, however, rely on an external motion-capture apparatus that gives the vehicles almost perfect state information at high rates. As we move these tasks out of the lab and into the field, we need new techniques to provide this sensing information.

A major challenge in gathering sensing data necessary for flight is the limited payload, computation, and battery life of the vehicles. These small aircraft, weighing under 1-2kg, struggle to carry enough sensing payload for high-speed navigation in complex 3D environments. Lightweight cameras are a good solution, but require computationally efficient machine vision algorithms that can run within the limits of these vehicles. For these reasons, camera based systems have so far been limited to relatively slow, stable flight regimes [\cite=Engel12] [\cite=Sa13]. We aim to fly at speeds of 7-15m/s through cluttered environments like a forest, well outside the typical speed regime. The short wingspan required to fit through gaps in the clutter limits our payload capacity while we need fast framerates with short exposures to avoid motion blur and to have enough time to avoid oncoming obstacles.

To this end, we propose a novel method for stereo vision computation that is dramatically faster than the state of the art. Our method performs a subset of the processing traditionally required for stereo vision, but is able to recover obstacles in real time at 120 frames-per-second (fps) on a conventional CPU. Our system is lightweight and accurate enough to run in real time on our aircraft, allowing for true, self-contained obstacle detection.

Related work

Obstacle Detection on UAVs

Obstacle detection on small outdoor UAVs continues to be a challenging problem. Laser rangefinders usually only support 2D detections and are generally too heavy for flight, although some systems with large wingspans [\cite=Bry12] or limited flight time [\cite=Richter13] exist. Other active rangefinders such as the Microsoft Kinect and PrimeSense systems rely on less focused infrared light and do not work in bright outdoor environments. Here we detail some of the related vision and other lightweight sensors for this class of vehicles.

Optical Flow

Embedded optical flow techniques rely on hardware (such as commonly found in optical mice) to compute the inter-frame changes between images to extract depth information. These techniques have worked well on UAVs, demonstrating autonomous takeoff, landing [\cite=Beyeler09a] [\cite=Barber07] and obstacle avoidance [\cite=Zufferey08] [\cite=Beyeler09]. This technology has been successful for aircraft flight control and is now available commercially. Embedded optical flow, however, is limited in its resolution, providing only general guidence about obstacles. For more sophisticated flight, such as flying in a cluttered environment like a forest, we must look beyond optical flow techniques for solutions that provide greater resolution.

Monocular Vision

Monocular vision techniques are attractive because they rely on only a single, lightweight camera, are readily available, and easy to deploy. State of the art monocular depth estimation, however, is generally not fast and reliable enough for obstacle avoidance on fast-flying UAVs. Machine learning work has shown progress, such as Michels's radio controlled car [\cite=Michels05]. More modern work includes scale-estimation in hover and slow flight regimes [\cite=Sa13]. When integrating pressure and inertial measurement sensors such as Achtelik et al.'s implemention of PTAM (parallel tracking and mapping) [\cite=Klein07] with a barometric altimeter, stable flights in indoor and outdoor environments are possible [\cite=Achtelik11a]. With a full vison-aided inertial navigation system (VINS), Li et al. have shown remarkable tracking with commodity phone hardware, demonstrating tracking within 0.5-0.8% of distance traveled for significant distances [\cite=Li13] [\cite=Li13a].

Stereo Vision

While we have seen substantial improvements in monocular vision systems recently, they are not yet fast or accurate enough for high-speed obstacle avoidance on small UAVs. Stereo systems suffer from a similar speed issue, with most modern systems running at or below 30 Hz [\cite=Yang03] [\cite=Byrne06].

Honegger et al. recently demonstrated an FPGA (Field Programmable Gate Array) based system that can compute optical flow and depth from stereo on a 376x240 image pair at 127 fps or 752x480 at 60 fps [\cite=Honegger12] [\cite=Honegger14]. Their system is small and lightweight enough for use on a small UAV, but requires specialized hardware and has not yet been flight tested. By comparison, our approach performs less computation and can work easily on conventional hardware, but relies on the observation that it is sufficient for high-speed flight to compute a subset of the stereo matches.

Proposed Method

Block-Matching Stereo

A standard block-matching stereo system produces depth estimates by finding pixel-block matches between two images. Given a pixel block in the left image, for example, the system will search through the epipolar line to find the best match. The position of the match relative to its coordinate on the left image, or the disparity, allows the user to compute the 3D position of the object in that pixel block.

Pushbroom Stereo

One can think of a standard block-matching stereo vision system as a search through depth. As we search along the epipolar line for a pixel group that matches our candidate block, we are exploring the space of distance away from the cameras. For example, given a pixel block in a left image, we might start searching through the right image with a large disparity, corresponding to an object close to the cameras. As we decrease disparity (changing where in the right image we are searching), we examine pixel blocks that correspond to objects further and further away, until reaching zero disparity, where the stereo base distance is insignificant compared to the distance away and we can no longer determine the obstacle's location.

Given that framework, it is easy to see that if we limit our search through distance to a single value, d meters away, we can substantially speed up our processing, at the cost of neglecting obstacles at distances other than d. While this might seem limiting, our cameras are on a moving platform (in this case, an aircraft), so we can quickly recover the missing depth information by integrating our odometry and previous single-disparity results (Figure [\ref=movingDetection]). The main thing we sacrifice is the ability to take the best-matching block as our stereo match; instead we must threshold for a possible match.

We give this algorithm the name "pushbroom stereo" because we are "pushing" the detection region forward, sweeping up obstacles like a broom on a floor (and similar to pushbroom LIDAR systems [\cite=Napier13]). We note that this is distinct from a "pushbroom camera," which is a one-dimensional array of pixels arranged perpendicular to the camera's motion [\cite=Gupta97]. These cameras are often found on satellites and can be used for stereo vision [\cite=Hirschmuller05].

Odometry

Our system requires relatively accurate odometry over short time horizons. This requirement is not particularly onerous because we do not require long-term accuracy like many map-making algorithms. In our case, the odometry is only used until the aircraft catches up to its detection horizon, which on many platforms is 5-10 meters away. We demonstrate that on aircraft, a wind-corrected airspeed measurement (from a pitot tube) is sufficient. On a ground robot, we expect that wheel odometry would be adequate.

Implementation

Pushbroom Algorithm

Like other block-matching algorithms, we use sum of absolute differences (SAD) to detect pixel block similarity. In addition to detecting matching regions, we score blocks based on their abundance of edges. This allows us to disambiguate the situation where two pixel blocks might both be completely black, giving a good similarity score, but still not providing a valid stereo match. To generate an edge map, we use a Laplacian with an aperture size (ksize) of 3. We then take the summation of the 5x5 block in the edge map and reject any blocks below a threshold for edge-abundance.

After rejecting blocks for lack of edges, we score the remaining blocks based on SAD match divided by the summation of edge-values in the pixel block:

[formula]

where p(i) denotes a pixel value in the 5x5 block and L is the Laplacian. We then threshold on the score, S, to determine if there is a match.

We have deliberately chosen a design and parameters to cause sparse detections with few false positives. For obstacle avoidance, we do not need to see every point on an obstacle but a false positive might cause the aircraft to take unnecessary risks to avoid a phantom obstacle.

All two-camera stereo systems suffer from some ambiguities. With horizontal cameras, we cannot disambiguate scenes with horizontal self-similarity, such as buildings with grid-like windows or an uninterrupted horizon. These horizontal repeating patterns can fool stereo into thinking that it has found an obstacle when it has not.

While we cannot correct these blocks without more sophisticated processing or additional cameras, we can detect and eliminate them. To do this, we perform additional block-matching searches in the right image near our candidate obstacle. If we find that one block in the left image matches blocks in the right image at different disparities, we conclude that the pixel block exhibits local self-similarity and reject it. While this search may seem expensive, in practice the block-matching above reduces the search size so dramatically that we can run this filter online. Figure [\ref=horizontalInvariance] demonstrates this filter running on flight data.

Hardware Platform

We implemented the pushbroom stereo algorithm on a quad-core 1.7Ghz ARM, commercially available in the ODROID-U2 package, weighing under 50 grams. Our cameras' resolution and stereo baseline can support reliable detections out to approximately 5 meters, so we use 4.8 meters as our single-disparity distance. We detect over 5x5 pixel blocks, iterating through the left image with 8 parallel threads.

We use two Point Grey Firefly MV cameras, configured for 8-bit grayscale with 2x2 pixel binning, running at 376x240 at 120 frames per second. A second ODROID-U2, communicating over LCM [\cite=Huang10], runs our state-estimatator (a 12-state Kalman filter from [\cite=Bry12]) and connects to our low-level interface, a firmware-modified APM 2.5, which provides access to our servo motors, barometric altimeter, pitot tube airspeed sensor, and 3-axis accelerometer, gyroscope, and magnetometer suite.

This platform sits aboard a modified Team Black Sheep Caipirinha, a 34 inch (86cm) wingspan delta wing with a 2000kV brushless DC motor and 8-inch propeller (Figure [\ref=deltaPhoto]). All outdoor flights are conducted with the aircraft under control of a passively-spooling non-conductive 250 meter safety tether.

Results

Single-Disparity Stereo

To determine the cost of thresholding stereo points instead of using the best-matching block from a search through depth, we walked with our aircraft near obstacles and recorded the output of the onboard stereo system with the state-estimator disabled. We then, offline, used OpenCV's [\cite=Bradski00] block-matching stereo implementation (StereoBM) to compute a full depth map at each frame. We then removed any 3D point that did not correspond to a match within 0.5 meters of our single-disparity depth to produce a comparison metric for the two systems.

With these data, we detected false-positives by computing the Euclidean distance from each single-disparity stereo coordinate to the nearest point produced by the depth-cropped StereoBM (Figure [\ref=bmStereoSketch]). Single-disparity stereo points that are far away from any StereoBM points may be false-positives introduced by our more limited computation technique. StereoBM produces a large number of false negatives, so we do not perform a false-negative comparison on this dataset (see Section [\ref=sec:flightExperiments] below.)

Our ground dataset includes over 23,000 frames in four different locations with varying lighting conditions, types of obstacles, and obstacle density. Over the entire dataset, we find that single-disparity stereo produces points within 0.5 meters of StereoBM 60.9% and within 1.0 meters 71.2% of the time (Figure [\ref=singleDepthStereo]). For context, the aircraft's wingspan is 0.86 meters and it covers 0.5 meters in 0.03 to 0.07 seconds.

Flight Experiments

To test the full system with an integrated state-estimator, we flew our platform close to obstacles (Figure [\ref=inFlight]) on three different flights, recorded control inputs, sensor data, camera images, and on-board stereo processing results. Figures [\ref=stereoDetectionSeq] and [\ref=obstacleFlyby] show on-board stereo detections as the aircraft approaches an obstacle.

During each flight, we detected points on every obstacle in real time. Our state estimate was robust enough to provide online estimation of how the location of the obstacles evolved relative to the aircraft. While these flights were manually piloted, we are confident that the system could autonomously avoid the obstacles with these data. The integration of the planning and control system will be reported in future work.

To benchmark our system, we again used OpenCV's block-matching stereo as a coarse, offline, approximation of ground truth. At each frame, we ran full block-matching stereo, recorded all 3D points detected, and then hand-labeled regions in which there were obstacles to increase StereoBM's accuracy.

We compared those data to pushbroom stereo's 3D data in two ways. First, we performed the same false-positive detection as in Section [\ref=sec:singleDisparityResults], except we included all 3D points seen and remembered as we flew forward. Second, we searched for false-negatives, or obstacles pushbroom stereo missed, by computing the distance from each StereoBM coordinate to the nearest 3D coordinate seen and remembered by pushbroom stereo (Figure [\ref=falseNegTechnique]).

Figures [\ref=bmStereoComparisonFalsePos] and [\ref=falseNegData] show the results of the false-positive and false-negative benchmarks on all three flights respectively. Our system does not produce many false-positives, with 74.8% points falling within 0.5 meters and 92.6% falling less than one meter from OpenCV's StereoBM implementation. For comparison, a system producing random points at approximately the same frequency gives approximately 1.2% and 3.2% for 0.5 and 1.0 meters respectively.

As Figure [\ref=bmStereoComparisonFalseNeg] shows, pushbroom stereo detects most of the points on obstacles that StereoBM sees, missing by 1.0 meters or more 32.4% of the time. A random system misses approximately 86% of the time by the same metric. For context, the closest our skilled pilot ever flew to an obstacle was about two meters.

These metrics demonstrate that the pushbroom stereo system scarifies a limited amount of performance for a substantial reduction in computational cost, and thus a gain in speed. Finally, we note that all data in this paper used identical threshold, scoring, and camera calibration parameters.

Conclusion

Here we describe a system that performs stereo detection with a single disparity. A natural extension would be to search at multiple disparities, perhaps enabling tracking of obstacles through two or more depths. As computational power increases, we can increase the number of depths we search, continuously improving our detection.

We have demonstrated a novel stereo vision algorithm for high-framerate detections of obstacles. Our system is capable of quickly and accurately detecting obstacles at a single disparity and using a state-estimator to update the position of obstacles seen in the past, building a full, local, 3D map. It is capable of running at 120fps on a standard mobile-CPU and is lightweight and robust enough for flight experiments on small UAVs. This system will allow a new class of autonomous UAVs to fly in clutter with all perception and computation onboard.