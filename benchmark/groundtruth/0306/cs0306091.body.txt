plus 1ex minus 0ex =1mm

Corollary

height1pt Universal Sequential Decisions in Unknown Environments height1pt

Introduction: Every inductive inference problem can be brought into the following form: Given a string [formula], take a guess at its continuation xt. We will assume that the strings which have to be continued are drawn from a probability distribution μ. The maximal prior information a prediction algorithm can possess is the exact knowledge of μ, but often the true distribution is unknown. Instead, prediction is based on a guess ρ of μ. We expect that a predictor based on ρ performs well, if ρ is close to μ or converges to μ.

Universal probability distribution: Let M : =  {μ1,μ2,...} be a finite or countable set of candidate probability distributions on strings. We define a weighted average on M, We call ξ universal relative to M, as it multiplicatively dominates all distributions in M, i.e. ξ(x1:n)  ≥  wμi   ·   μi(x1:n) for all μi ∈ M. In the following, we assume that M is known and contains the true distribution from which x1x2... is sampled, i.e. μ ∈ M. The condition μ ∈ M is not a serious constraint if we include all computable probability distributions in M with high weights assigned to simple μi. Solomonoff-Levin's universal semi-measure is obtained if we include all enumerable semi-measures in M with weights wμi   ~   2- K(μi), where K(μi) is the length of the shortest program for μi [\cite=Hutter:01loss] [\cite=Hutter:04uaibook]. One can show that the conditional ξ and μ probabilities rapidly converge to each other:

[formula]

Since the conditional probabilities are the basis of the decision algorithms considered in this work, we expect a good prediction performance if we use ξ as a guess of μ.

Bayesian decisions: Let [formula] be the received loss when predicting yt ∈ Y, but xt ∈ X turns out to be the true tth symbol of the sequence. Let LnΛρ be the total expected loss for the first n symbols of the Bayes predictor Λρ which minimizes the ρ expected loss. For instance for X   =   Y   =   {0,1}, Λρ is a threshold strategy with yΛρt   =   0 / 1 for ρ(1|x< t)  ><  γ, where [formula]. Let Λ be any prediction scheme (deterministic or probabilistic) with no constraint at all, taking any action yΛt ∈ Y with total expected loss LnΛ. If μ is known, Λμ is obviously the best prediction scheme in the sense of achieving minimal expected loss LnΛμ   ≤   LnΛ for any Λ. For the predictor Λξ based on the universal distribution ξ, on can show [formula], i.e. Λξ has optimal asymptotics for LnΛμ   →    ∞   with rapid convergence of the quotient to 1. If L∞  Λμ is finite, then also L∞  Λξ [\cite=Hutter:01loss] [\cite=Hutter:04uaibook].

More active systems: Prediction means guessing the future, but not influencing it. One step in the direction to more active systems was to allow the Λ system to act and to receive a loss [formula] depending on the action yt and the outcome xt. The probability μ is still independent of the action, and the loss function [formula] has to be known in advance. This ensures that the greedy Λμ strategy is still optimal. The loss function can also be generalized to depend on the history x< t and on t.

Agents in known probabilistic environments: The full model of an acting agent influencing the environment has been developed in [\cite=Hutter:01aixi] [\cite=Hutter:04uaibook]. The probability of the next symbol (input, perception) xt depends in this case not only on the past sequence x< t but also on the past actions (outputs) y1:t, i.e. μ   =   μ(xt|x< ty1:t). We call probability distributions of this form chronological. The total μ expected loss is [formula], where we assumed a total number of n interaction cycles. Action yt(x< ty< t) and loss function [formula] may depend on the complete history, which allows planning and delayed loss assignment.

Sequential decision theory: The goal is to perform the actions which minimize the total μ expected loss:

[formula]

The minimization over yt is in chronological order to correctly incorporate the dependency of xt and yt on the history. Note that yt only depends on the known history x< ty< t, whereas minima and expectations are taken over the unknown xt:nyt:n variables. The policy ([\ref=ydotrec]) (called AIμ model) is optimal in the sense that no other policy leads to lower μ-expected loss.

Bellman equations: In the case that [formula] is independent of y< t and μ is independent of y1:n, policy ([\ref=ydotrec]) reduces to the greedy Bayes Λμ strategy. For (completely observable) Markov Decision Processes μ   =   μ(xt|xt - 1yt) ([\ref=ydotrec]) and ([\ref=voptdef]) can be written as recursive Bellman equations of sequential decision theory with state space X, action space Y, state transition matrix μ(xt|xt - 1yt), rewards [formula], etc. The general (non-MDP) case may also be (artificially) reduced to Bellman equations by identifying complete histories x< ty< t with states and μ(xt|x< ty1:t) with the state transition matrix. Due to the use of complete histories as state space, the AIμ model neither assumes stationarity, nor the Markov property, nor complete accessibility of the environment. But since every state occurs at most once in the lifetime of the system the explicit formulation ([\ref=ydotrec]) is more useful than a pseudo-recursive Bellman equation form. There is no principle problem in determining yk as long as μ is known and computable and X, Y and n are finite.

Reinforcement learning for unknown environment: Things dramatically change if μ is unknown. Reinforcement learning algorithms are commonly used in this case to learn the unknown μ (or directly a value function). They succeed if the state space is either small or has effectively been made small by generalization or function approximation techniques. In almost all approaches, the solutions are either ad hoc, or work in restricted domains only, or have serious problems with state space exploration versus exploitation, or have non-optimal learning rate. Below we propose the AIξ model as a universal and optimal solution to these problems.

Unknown loss function: Furthermore, the loss function [formula] may also be unknown, but there is an easy "solution" to this problem. The specification of the loss function can be absorbed in the probability distribution μ by increasing the input space X. Let [formula], where x't is the regular input, lt is interpreted as the loss, [formula] is replaced by lt in ([\ref=ydotrec]) and ([\ref=voptdef]), and μ is only non-zero if lt is consistent with the loss, i.e. [formula]. In this way all possible unknowns are absorbed in μ.

The universal AIξ model: Encouraged by the good performance of the universal sequence predictor Λξ, we propose a new model, where the probability distribution μ is learned indirectly by replacing it with a universal prior ξ. We define [formula] as a weighted sum over chronological probability distributions in M. Convergence ξ(xn|x< ny1:n)  →  μ(xn|x< ny1:n) can be proven analogously to ([\ref=xitomu]). Replacing μ by ξ in ([\ref=ydotrec]) the AIξ system outputs

[formula]