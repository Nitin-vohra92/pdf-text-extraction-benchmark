and and

estimator of and : [formula] = theoretical rate for experiment or bin j [formula] = rate measured in experiment or bin j [formula], where [formula] is the number of data points V = covariance matrix: experimental and theoretical uncertainties Probability to observe a minimum of X2 larger than the one actually observed, assuming for [formula] a χ2 distribution with [formula] degrees of freedom ([formula] is the number of fitted parameters). The standard 100β% CL regions in the tan 2ϑ-Δm2 plane are given by the condition β = Confidence Level (CL) ΔX2(β) = value of χ2 such that the cumulative χ2 distribution for 2 degrees of freedom is equal to β

[formula]

Super-Kamiokande rate/BP-SSM has decreased from 0.474  ±  0.020 [\cite=SK-sun-spectrum-98] to 0.451  ±  0.008 [\cite=SK-sun-01]. Best Fit in VO region! Due to decrease of SK rate/BP-SSM. Best Fit continues to be in LMA region! No SMA region at 99% CL! Due to flat spectrum. At 99% CL VO region almost vanish! Due to flat energy spectrum and Day-Night asymmetry (albeit small). Best Fit continues to be in SMA region! Poor Goodness of Fit [formula] sterile disfavored! Best Fit in LMA region! Due to flat energy spectrum and decrease of Super-Kamiokande rate/BP-SSM (the incompatibility of a flat energy spectrum with the Homestake rate is alleviated).

The theoretical rates [formula] depend linearly on the parameters Δm2 and tan 2 θ to be determined in the fit. The errors [formula] are multinormally distributed. The covariance matrix V does not depend on Δm2 and tan 2 θ.

In reality these three conditions are not satisfied: The theoretical rates [formula] do not depend at all linearly on the parameters Δm2, tan 2 θ. This is the reason why there are several allowed regions in the tan 2 θ-Δm2 plane and these regions do not have elliptic form. The errors [formula] are not multinormally distributed, because although the fluxes [formula] and the cross sections [formula] are assumed to be multinormally distributed, their products, that determine the theoretical rates through the relations

[formula]

are not multinormally distributed. The covariance matrix V depends on Δm2 and tan 2 θ.

is important!

is irrelevant: the multinormal approximation is very good, as shown by the following three figures.

Dotted lines: probability distribution function of experimental rates generated with Monte Carlo. Solid lines: probability distribution function of experimental rates assuming a multinormal distribution given by the Likelihood function

is not negligible, as shown by the following comparison of the Standard Regions with the regions obtained with the log-Likelihood method (see [\cite=Eadie-71])

Estimate best-fit values of Δm2, tan 2 θ through [formula]. Call the best-fit values [formula], [formula]. Assume that [formula], [formula] are reasonable surrogates of the true values [formula], [formula]. Using [formula], [formula], generate Ns synthetic random data sets with the standard gaussian distribution for the experimental and theoretical uncertainties. Apply the Least-Squares method to each synthetic data set, leading to an ensemble of simulated best-fit parameters (s), (s) with [formula], each one with his associated [formula]. Calculate GoF as the fraction of simulated [formula] in the ensemble that are larger than the one actually observed, [formula].

Definition: 100β% CL Allowed Regions belong to a set of allowed regions that cover the true value of the parameters with probability β.

Given the usual "100β% CL" allowed regions in the tan 2ϑ-Δm2 plane, calculate their Monte Carlo Confidence Level [formula] with a method similar to the one used for the Goodness of Fit.

Assume that [formula], [formula] are reasonable surrogates of the true values [formula], [formula].

Generate a large number of synthetic data sets.

Apply the standard procedure to each synthetic data set and obtain the corresponding "100β% CL" Standard Allowed Regions in the tan 2 θ-Δm2 plane.

Count the number of synthetic "100β% CL" Standard Allowed Regions that cover the assumed surrogate [formula], [formula] of the true values.

The ratio of this number and the total number of synthetically generated data set gives the Confidence Level [formula] of the "100β% CL" Standard Allowed Regions.

Frequentist Statistics allows to calculate allowed regions with correct coverage using Neyman's method.

But there is arbitrariness in the choice of 1) Estimator of the parameters 2) Method for the construction of acceptance regions

In [\cite=Garzelli-Giunti-sf-00] we have calculated "exact" confidence regions using as estimate of tan 2ϑ and Δm2 their value at [formula].

In [\cite=Creminelli-Signorelli-Strumia-01] it has been argued that [formula] may be an insufficient estimator, leading to a loss of information. Notice that if this is true, the standard χ2 method suffers from the same problem!

In order to prevent any loss of information, it is better to use the full data set as estimator of tan 2ϑ and Δm2, as done in [\cite=Creminelli-Signorelli-Strumia-01].

However, there is still the problem of choice of the method for the construction of acceptance intervals.

In [\cite=Creminelli-Signorelli-Strumia-01] it has been argued that the Unified Approach (UA) [\cite=Feldman-Cousins-98] is more appropriate than the smallest acceptance intervals method, also known as "Crow-Gardner" (CG).

Unfortunately, it is well known that when the UA differs from the smallest acceptance intervals method it gives unreliable confidence intervals (see [\cite=Giunti-Laveder-physical-00] [\cite=Giunti-Laveder-power-00])

Infamous example: The KARMEN 1998 limit on μ  →  e oscillations obtained with the Unified Approach was unreliably much more stringent than the sensitivity of the experiment [\cite=KARMEN-nu98]. The KARMEN 1999 limit is less stringent than the 1998 one. More data [formula] less information!

KARMEN 1998 exclusion limit and sensitivity [\cite=KARMEN-nu98].

KARMEN 1999 exclusion limit and sensitivity [\cite=KARMEN-Moriond-99]. Solid line: 1999 limit. Dash-Dotted line: 1999 sensitivity. Dashed line: 1998 limit.

Other infamous examples:

The 1999 limit on neutrinoless double-beta decay obtained in the Heidelberg-Moscow experiment [\cite=Heidelberg-Moscow-99] obtained with the Unified Approach was much more stringent than the sensitivity of the experiment. That is why now they do not use the Unified Approach any more [\cite=Heidelberg-Moscow-01]! (Got burned!)

The present upper limit on νμ  →  ντ neutrino oscillations obtained in the NOMAD experiment [\cite=NOMAD-00] is stronger than the one obtained in the CHORUS experiment [\cite=CHORUS-01] not because the NOMAD experiment has a better sensitivity than the CHORUS experiment (see discussion in [\cite=CHORUS-01]), but because the NOMAD collaboration uses the Unified Approach, which gives unphysically stringent upper bounds when the number of observed events is smaller than the expected background.

The UA and similar methods [\cite=Giunti-bo-99] [\cite=Ciampolillo-98] [\cite=Mandelkern-Schultz-99] are appropriate in order to get allowed regions even in the presence of an unlikely statistical fluctuation of the data, such that the data are very unlikely for any value of the parameters.

However, the physical reliability of such allowed regions is highly questionable.

If there is no statistical fluctuation of data, the UA and the CG methods are equivalent.

From the value of the GoF (see Table at pag. ) one can see that there is no unlikely statistical fluctuation in solar neutrino data in the case of νe  →  νμ,τ oscillations and in the case of the analysis of the rates in terms of νe  →  νs oscillations.

On the other hand, if the solar neutrino problem is due to νe  →  νs oscillations, there is an unlikely statistical fluctuation of the shape of the energy spectrum and the global analysis of solar ν data with the CG method is unreliable.

Therefore, the CG method (that is computationally much easier than the UA method) can be applied to the analisis of solar ν data in terms of νe  →  νμ,τ oscillations and to the analysis of the rates of solar neutrino experiments in terms of νe  →  νs oscillations.

Bayesian Theory allows to calculate the improvement of knowledge as a consequence of experimental measurements (see [\cite=D'Agostini-99]).

This is how our mind works and how science improves. Therefore, Bayesian Theory is the natural statistical tool for scientists.

Bayesian probability density function of tan 2ϑ and Δm2 after measurement of rates [formula]: p( tan 2 θ,Δm2) = prior probability density function

Prior knowledge on tan 2ϑ and Δm2: All values are allowed, but we know that solar ν data are sensitive to different orders of magnitude of tan 2ϑ and Δm2 through different mechanisms.