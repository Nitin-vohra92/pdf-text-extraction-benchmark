Efficient File Synchronization: a Distributed Source Coding Approach[formula]

Introduction

This material is based upon work supported by the US National Science Foundation (NSF) under grants 23287 and 30149 and by a gift from Qualcomm Inc.. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

In distributed file backup or file sharing systems, different source nodes may have different versions of the same file differing by a small number of edits including deletions and insertions. The edits usually appear in bursts, for example, a paragraph of text is deleted, or several consecutive frames of video are inserted. An important question is: how to efficiently send a file to a remote node that has a different version of it? Further, what is the fundamental limit of the number of bits that needs to be sent to achieve this goal?

In this paper, we study the problem of reconstructing a source sequence with the help of decoder side-information using a distributed source coding framework (see Figure [\ref=fig:system] for an illustration of the system). In this paper we focus on a simple case where the side-information is a deleted version of the source sequence. Consider a binary sequence of length n denoted by [formula]. Consider another binary sequence of length n called deletion pattern, denoted by [formula], which determines how Xn is to be deleted. The outcome of the deletion process, denoted by y(Xn,Dn), is derived from Xn by deleting the bits at those locations where the deletion pattern is 1. Here is an example:

[formula]

Note that the deletion pattern Dn tends to have bursts of consecutive 1's, which lead to bursty deletions. The original files Xn and the deleted files y(Xn,Dn) are available to the encoder and the decoder, respectively. The encoder sends a message to the decoder, so that the latter can reconstruct (synchronize) the original files Xn with an error probability that is vanishing when n goes to infinity. The objective of this work is to characterize the minimum rate of the message defined as the minimum number of bits per source bit.

The problem of synchronizing edited sequences has been studied by [\cite=Leven] [\cite=OrlitskyDeletion] under the assumptions (1) the decoder is not allowed to make any error, and (2) the number of edits is a constant that does not increase with the length of the sequence. Upper and lower bounds of the minimum number of communication bits were provided as functions of the number of edits and the length of the sequence. In [\cite=venkataramanan-interactive], an interactive, low-complexity and asymptotically optimal scheme was proposed. In comparison, in this paper, we consider on information theoretic formulation allowing a positive probability of error that vanishes as n increases. This assumption allows us to use additional techniques like random binning to improve the minimum rate. Unlike in assumption (2), we consider the case that a vanishing fraction of source bits, rather than a constant number of bits, is deleted, to get which makes the problem harder and more realistic.

In this paper, we characterize the minimum rate in terms of the limit of the conditional entropy of the source sequence given the side-information. We interpret the minimum rate as the amount of information in the deleted content and the locations of the deletions, subtracting the uncertainty of the locations given the source and side-information. We refer to the latter as "nature's secret". This is the information that the decoder will never find out even if it knows the source sequence and the side-information exactly; it represents the over-counting of information in the locations of the deletions. For example, if Xn = (0,0) and y(Xn,Dn) = (0), the decoder will never know and never needs to know whether the first bit or the second bit is deleted. Therefore the information about the precise location of the deleted bit is over-counted and should be subtracted. For small deletion rate and geometrically distributed burst length, the minimum rate is computed up to the precision of two leading terms.

If the deletion pattern Dn is independent and identically distributed (iid), Xn and y(Xn,Dn) are the input and output of a binary iid deletion channel (see [\cite=MitzSurvey] and references therein). In this case, the problem of characterizing the minimum rate to reconstruct iid uniform source sequences in the distributed source coding problem is closely related to the evaluation of the mutual information across the deletion channel with iid uniform input distribution. For small deletion probability, the second and third order terms of the channel capacity are achieved by iid uniform input distribution and are computed in [\cite=MontanariISIT10]. In this paper we consider the asymptotic expansion of the minimum rate for the general bursty deletion process where the deletions are correlated over time. In the special case of iid deletion process, the expansion in Theorem [\ref=thm:smallbeta] reduces to [\cite=MontanariISIT10]. Note that in the source coding problem, the constant term becomes zero, which means that the second and third order terms of the channel capacity correspond to the first and second order terms of the minimum rate. Therefore, although it is mathematically equivalent to evaluate the these terms for the source coding and channel coding problems, from the practical point of view, the evaluation is more important for the source coding problem than for the channel coding problem. See Remark [\ref=rem:Montanari] for detailed discussions.

When we generalize the iid deletion process to bursty deletion process, new techniques are introduced. The most interesting technique is the generalization of the usual concept of a "run". We view the sequence (1,0,1,0,1,0) as a run with respect to deletion bursts of length two, because deleting two consecutive bits from that sequence always results in the same outcome sequence (1,0,1,0).

The rest of this paper is organized as follows. In Section [\ref=sec:problem] we formally setup the problem and provide a preview of the main result. In Section [\ref=sec:general] we provide information theoretic expressions of the minimum rate for general parameters of the deletion pattern. In Section [\ref=sec:smallbeta] we focus on the asymptotics when the deletion rate is small and compute the two leading terms of the minimum rate. All the proofs are provided in the appendices.

Notation: With the exception of the symbols R,E,C, and J, random quantities are denoted in upper case and their specific instantiations in lower case. For [formula], Vji denotes the sequence [formula] and Vi denotes Vi1. The binary entropy function is denoted by h2(  ·  ). All logarithms are base 2. The notation {0,1}n denotes the n-fold Cartesian product of {0,1}, and {0,1}* denotes [formula].

Problem Formulation and Main Result

Problem formulation

The source sequence [formula] is iid Bernoulli(1 / 2). Let α,β∈(0,1). The deletion pattern [formula] is a two-state stationary Markov chain illustrated in Figure [\ref=fig:markov] with the initial distribution pD0  ~   Bernoulli(d), where d: = β / (α  +  β) and transition probabilities [formula] and [formula], for all [formula]. Note that the initial distribution pD0 is the stationary distribution of the Markov chain. The deleted sequence y(Xn,Dn)∈{0,1}* is a subsequence of Xn, which is derived from Xn by deleting all those Xi's with Di = 1 . The length of y(Xn,Dn), denoted by Ly, is a random variable taking values in [formula]. For i  <  Ly, Yi denotes the i-th bit in the y(Xn,Dn) sequence. A run of consecutive 1's in the deletion pattern is called a burst of deletion. Since β is the probability to initiate a burst of deletion, it is called the deletion rate.

The source sequence Xn is available to the encoder and the deleted sequence y(Xn,Dn) is available only to the decoder as side-information. The deletion patterns Dn is available to neither the encoder nor the decoder. The encoder encodes Xn and sends a message to the decoder so that the decoder can reproduce the source with high probability.

If β  =  1 - α = d, Dn becomes iid, and the relation between Xn and y(Xn,Dn) can be modeled as an iid deletion channel with deletion probability d. In this paper we consider the Markov deletion pattern to emphasize the bursty nature of the deletion process in the source coding problem.

The formal definitions of a code and an achievable rate are as follows.

A distributed source code for deletion side-information with parameters (n,|Mn|) is the tuple (fn,gn) consisting of an encoding function fn:{0,1}n  →  Mn and a decoding function gn:Mn  ×  {0,1}*  →  {0,1}n.

A real number R is called an achievable rate if, there exists a sequence of distributed source codes {(fn,gn)}n  ≥  1 for deletion side-information with parameters (n,|Mn|) satisfying [formula] and lim sup n  →    ∞(1 / n) log |Mn|  ≤  R.

The set of all achievable rates is necessarily closed and hence the minimum exists. The minimum achievable rate is denoted by Rmin. The focus of this paper is to characterize Rmin, especially for small β.

Main result

In Section [\ref=sec:general] we express Rmin using information theoretic quantities when the parameters α and β take arbitrary values. Unfortunately, we cannot provide an explicit expression of Rmin as a function of α and β. Hence we focus on asymptotic regimes in Section [\ref=sec:smallbeta] when β is small.

Since the main difference between the erasure process and the deletion process is that the locations of the erasures are explicit but those of the deletions are not, it is interesting to focus on a regime where the amount of information to describe the locations of the deletions should play a significant role in the minimum rate. When α is vanishing and the length of bursts of deletions is increasing, for each burst, the number of bits to describe the deleted content increases linearly with respect to the length of the burst, but the number of bits to describe the location and length of the burst increases logarithmly. Therefore the regime with a vanishing α is not interesting. On the contrary, when α is fixed, the length of a burst is of order Θ(1) and we have an interesting regime. In this case, we evaluate Rmin(α,β) as follows.

When α is fixed, for any ε > 0, we have

[formula]

where [formula].

The proof of Theorem [\ref=thm:smallbeta] based on Lemmas [\ref=lem:Rit] and [\ref=lem:breakdown], and is provided in Appendix [\ref=app:proofsmallbeta]. Detailed discussions about the proof techniques are given in Section [\ref=subsec:fixedalphasmallbeta].

The dominating term on the right side of ([\ref=eqn:smallbeta]) is -  β log β, and the second leading term is of order Θ(β). Since -   log β tends to infinity slowly as β decreases to zero, in practice these two terms are often in the same order of magnitude. Therefore we need to evaluate both of them.

In [\cite=MontanariISIT10], the authors evaluated the mutual information across the iid deletion channel with iid Bernoulli(1 / 2) input as

[formula]

which implies that

[formula]

This expression should be compared with ([\ref=eqn:smallbeta]) in the special case that the deletion process is iid, which requires β  =  1  -  α  =  d. Under this condition, ([\ref=eqn:smallbeta]) also has the same two leading terms - d log d  +  d( log 2e - C). Therefore in the special case of iid deletion process, ([\ref=eqn:smallbeta]) is consistent with the result in [\cite=MontanariISIT10].

Theorem [\ref=thm:smallbeta] implies that when the input distribution is iid Bernoulli(1 / 2), the mutual information across the bursty deletion channel is

[formula]

In [\cite=Dobrushin], Dobrushin showed that the channel capacity of the iid deletion channel is lim n  →    ∞(1 / n) max pXnI(Xn;y(Xn,Dn)). If this expression can be extended to the bursty deletion channel where the deletion pattern process is a Markov chain, then ([\ref=eqn:channel]) provides an asymptotic lower bound for the capacity of the bursty deletion channel for small values of β.

Information Theoretic Expression for General α and β

We can write the minimum achievable rate Rmin as the following information theoretic expression.

[formula]

The proof of Lemma [\ref=lem:Rit] is given in Appendix [\ref=app:proofthmRit]. The structure of the proof is as follows: (1) we show that the limit lim n  →    ∞(1 / n)H(Xn|y(Xn,Dn),D0,Dn + 1) exists, (2) using the information-spectrum method [\cite=Han], we have [formula]   ~          lim sup n  →    ∞(1 / n) log (1 / pXn|y(Xn,Dn)(Xn|y(Xn,Dn))), which is the conditional spectral sup-entropy, (3) we show that [formula]. The techniques we use in step (3) are similar to those Dobrushin used in [\cite=Dobrushin], where the capacity of the iid deletion channel is characterized by lim n  →    ∞(1 / n) max pXnI(Xn;y(Xn,Dn)).

In Lemma [\ref=lem:breakdown], the information theoretic expression of the minimum rate is written in another way, which has a more intuitive interpretation as explained in Remark [\ref=rem:breakdown].

[formula]

where E∞: =  lim n  →    ∞En, and En: = H(D1|D0,Xn,y(Xn,Dn),Dn + 1).

The proof of Lemma [\ref=lem:breakdown] is given in Appendix [\ref=app:proofbreakdown].

Lemma [\ref=lem:breakdown] expresses Rmin in terms of three parts, which can be intuitively interpreted as follows. The first term d is the fraction of deleted bits in Xn. It represents the amount of information per source bit in the deleted content, and thus the rate needed to send the deleted content. The second term is the entropy rate of the deletion pattern process, which is the rate needed to describe the locations of deletions. If the encoder knew the locations and sent them together with the deleted content, the decoder could reproduce Xn. However, this is excessive information. In fact, even if the decoder can correctly reproduce Xn, it can never know the exact deletion pattern. Therefore the uncertainty of the deletion pattern Dn, given Xn and y(Xn,Dn), is not required to be revealed in order to reproduce Xn.

The uncertainty in the deletion pattern, given the source sequence and side-information is the nature's secret, which is known only to an imaginary third party (nature) who generates the deletion pattern. Since nature's secret is not required to reproduce Xn, it should be subtracted from the message rate. Lemma [\ref=lem:breakdown] shows that nature's secret per source bit, which is the uncertainty in the whole deletion pattern Dn normalized by n, can be expressed as E∞, which is the uncertainty in only D1. An intuitive explanation is that, the uncertainty in each bit in Dn is approximately the same, therefore the uncertainty can be represented by the uncertainty in only D1.

Asymptotic behavior of Rmin for small values of β

In typical settings the number of edits is often much less than the file size. Since β is the probability to start a burst of deletions, the asymptotic behavior of Rmin for small β is of special interest.

Case 1: Few number of long bursts of deletion: α  ≪  1,β  ≪  1, and α  /  β is fixed

When α  ≪  1,β  ≪  1 and α  /  β is fixed, the number of bursts are much smaller than the length of the sequence, and each burst is so long that the overall fraction of deletion d = β / (α  +  β) is a constant.

On the right side of ([\ref=eqn:breakdown]), the first term d is a constant. For any ε > 0, the second term H(D1|D0) = dh2(α) + (1 - d)h2(β) = O(β1 - ε), and the third term E∞  ≤  H(D1|D0) = O(β1 - ε). According to Lemma [\ref=lem:breakdown], we have

[formula]

Intuitively speaking, if we have a small number of long bursts of deletion, the amount of information of the locations of deletions is orderwise less than the amount of information of the content of deletion. Therefore Rmin is dominated by the rate needed to deliver the deleted content.

A more interesting case is when all three terms of ([\ref=eqn:breakdown]) are comparable.

Case 2: Few number of short bursts of deletion: α is fixed and β  ≪  1

When α is fixed and β  ≪  1, the number of bursts is much smaller than the length of the sequence. Since the length of a burst is drawn from a geometric distribution with parameter α, the expected length is of order Θ(1). The overall proportion of deleted bits is d  =  β / (α  +  β)  =  β  /  α  +  Θ(β2). In this case, unlike in Case 1, the location information and "nature's secret" are comparable to the content information. Therefore we need to evaluate all three terms for this case. The three terms on the right side of ([\ref=eqn:breakdown]) are evaluated as follows. For any ε > 0, we have

[formula]

where [formula]. Combining ([\ref=eqn:smallbetad]) through ([\ref=eqn:smallbetasecret]) gives Theorem [\ref=thm:smallbeta].

The proofs of ([\ref=eqn:smallbetad]) and ([\ref=eqn:smallbetaentropyrate]) are trivial. The proof of ([\ref=eqn:smallbetasecret]) is highly nontrivial and is the essence of the proof of Theorem [\ref=thm:smallbeta]. The complete proof of ([\ref=eqn:smallbetasecret]) is given in Appendix [\ref=app:proofsmallbeta]. In this subsection we explain only the intuition of ([\ref=eqn:smallbetasecret]).

Let us first consider the case that the deletion is not bursty (α = 1), i.e., no consecutive bits are deleted. In order to evaluate nature's secret E∞ we need to estimate the uncertainty in D1 given Xn,y(Xn,Dn),D0 and Dn + 1. The uncertainty is significant if the first run of Xn is different from the first run of y(Xn,Dn). For example, if Xn = (0,0,0,1) and y(Xn,Dn) = (0,0,1), we know that one bit is deleted in the first run (first three bits) of Xn, but do not know which bit is deleted. The true identity of the deleted bit is nature's secret. Since there are three equally likely possible deletion patterns and only one leads to D1 = 1, the conditional entropy of D1 is h2(1 / 3). The length of the first run of Xn is L, a geometrically distributed random variable with parameter 1 / 2. If one bit is deleted in the first run, the conditional entropy is h2(1 / L). The probability that any bit in L bits is deleted is roughly Lβ, therefore the average uncertainty is [formula].

Let us now extend the discussion in the previous paragraph to the case of bursty deletions (α < 1). First, we need to generalize the usual definition of "run" to b-run.

For any b and [formula], a sequence [formula] is called a b-run of extent l if for all i,j satisfying [formula], xi = xj holds.

For example, (1,1,1,1,1) is a 1-run of extent 5, and 1-run is the usual definition of a run. The sequence (1,0,1,0,1) is a 2-run of extent 4. Note that there are l different ways to delete b consecutive bits in a sequence of length l + b - 1. A special property of a b-run of extent l is that, all the l ways of deletion result in the same outcome. For example, all four ways of deleting two consecutive bits in (1,0,1,0,1) lead to the same outcome (1,0,1). This observation is formally stated in the following fact.

Let xb + l - 1 be a b-run of extent l. Let [formula] denote the sequence of (i - 1) 0's followed by b 1's, then followed by (l - i) 0's. Then [formula] is the same for all [formula].

For any [formula], the first b-run of a sequence [formula] is the longest segment starting from x1 that is a b-run.

For example, the first 2-run of (0,1,0,1,1) is (0,1,0,1).

Now let us consider the uncertainty in D1 given Xn,y(Xn,Dn),D0 and Dn + 1 through an example. If we know that a burst of 2 bits is deleted in Xn  =  (0,1,0,1,1) to produce y(Xn,Dn) = (0,1,1), we know that the deletion occurs within the first 2-run, i.e., (0,1,0,1). Since there are three indistinguishable deletion patterns, (1,1,0,0,0), (0,1,1,0,0), and (0,0,1,1,0), among which only the first one satisfies D1 = 1, the conditional entropy of D1 is h2(1 / 3).

For any b, the extent of the first b-run, L, is a geometrically distributed random variable with parameter 1 / 2, as in the non-bursty case. This fact can be seen by sequentially generating [formula]. For arbitrary realization of Xb = xb, Xb always belongs to the first b-run. If the first b-run has been extended to the (i - 1)-th bit, it will be extended to the i-th bit if Xi = xi - b, which occurs with probability [formula]. Therefore the extent of the first b-run is a geometrically distributed variable. If one burst of b is deleted in the first b-run, the conditional entropy of D1 is h2(1 / L). Since given the length of burst b, the probability that any deletion pattern among all L possible deletion patterns occurs is roughly Lβ, the average uncertainty of [formula]. Note that the result is the same for all b. In other words, nature's secret is always C  ≈  1.29 bits per burst, regardless of the length of burst.

Since nature's secret is Cβ  +  O(β2 - ε) for any given value of the length of burst [formula], the fact that nature's secret averaged across different possible values of b is Cβ  +  O(β2 - ε), regardless of the distribution of the length of a burst of deletions. This implies that Theorem [\ref=thm:smallbeta] may generalize to more general deletion processes beyond the two-state Markov chains. In order to draw a rigorous statement, however, one has to revisit Lemmas [\ref=lem:Rit] and [\ref=lem:breakdown] and prove them for the general setup.

Concluding Remarks

We studied the distributed source coding problem of synchronizing source sequences based on bursty deletion side-information. We evaluated the two leading terms of the minimum achievable rate for small deletion rate. Directions for future work include considering insertions in addition to deletions, and evaluating the leading terms of the capacity of the bursty deletion channel.

Proof of Lemma [\ref=lem:Rit]

(1) We first show that Rn: = (1 / n)H(Xn|y(Xn,Dn),D0,Dn + 1) converges as n  →    ∞  , so that the limit in the statement of Lemma [\ref=lem:Rit] is well defined.

For all [formula], we have

[formula]

where step (a) holds because the tuple (y(Xm,Dm),y(Xnm + 1,Dnm + 1)) determines y(Xn,Dn), and step (b) holds because the Markov chains (y(Xnm + 1,Dnm + 1),Dn + 1) - Dm + 1 - (Xm,y(Xm,Dm),D0) and (y(Xm,Dm),D0)  -  Dm  -  (Xnm + 1,y(Xnm + 1,Dnm + 1),Dn + 1) hold. Therefore the sequence [formula] is superadditive. By Fekete's lemma[\cite=Feketeslemma], the limit lim n  →    ∞Rn exists.

(2) Using the information-spectral version of the Slepian-Wolf theorem [\cite=Han], we have [formula]   ~      lim sup n  →    ∞(1 / n) log (1 / pXn|y(Xn,Dn)(Xn|y(Xn,Dn))). In the rest of this appendix, for any random variables A,B, we abbreviate pA(A) and pA|B(A|B) to p(A) and p(A|B), respectively, to avoid cumbersome notations.

(3) Now we show that the sequence of random variables (1 / n) log (1 / p(Xn|y(Xn,Dn))) converges in probability to the limit lim n  →    ∞Rn.

We introduce a segmented deletion process as follows. Let k  ≥  3 be the length of a segment. Let g: = ⌊n / k⌋ be the number of complete segments and l: = n - gk be the length of the remainder. Consider the outcome of a segmented deletion process as follows: let [formula] be a vector with (3g + 1) components, where [formula], ZiL: = y(X(i - 1)k + 1,D(i - 1)k + 1), ZiM: = y(Xik - 1(i - 1)k + 2,Dik - 1(i - 1)k + 1), ZiR: = y(Xik,Dik), and Zremainder: = y(Xngk + 1,Dngk + 1). From z(Xn,Dn) we can find out how many source bits are deleted in each segment and the remainder, and whether the first and last bits of each segment are deleted. The sequence y(Xn,Dn) can be obtained by merging all the (3g + 1) components of z(Xn,Dn). Therefore the sequence z(Xn,Dn) contains more information than y(Xn,Dn). We will first fix k and let n go to infinity. Then we increase k to prove the final result.

The statement to be proved is based on the following three facts.

For any k  ≥  3, n and any δ > 0, there exists a function ε1(k) satisfying lim k  →    ∞ε1(k) = 0, so that

For any k and any δ > 0, there exists a function ε2(k) satisfying lim k  →    ∞ε2(k) = 0, so that as n  →    ∞  ,

[formula]

[formula]

Proof of Fact [\ref=fact:YtoZ]:

Since y(Xn,Dn) can be determined by z(Xn,Dn), there exists a function φn such that y(Xn,Dn)  =  φn(z(Xn,Dn)). For any realization of z(Xn,Dn) = z, we have [formula], which implies that [formula] always holds. Let LZ be the vector of (3g+1) components representing the lengths of all the components of z(Xn,Dn). Then we have

[formula]

By Markov's inequality,

[formula]

Using the same argument we also have

[formula]

Combining the last two inequalities completes the proof of Fact [\ref=fact:YtoZ].

Proof of Fact [\ref=fact:ZtoH]:

Let [formula]. Then

[formula]

where step (c) holds because given ZB, [formula] are conditionally independent, and step (d) holds because Dn is a Markov chain.

Since the expectation of the first term of ([\ref=eqn:lln]) is equal to (1 / n)H(ZB)  ≤  (2g + l) / n log 3, by Markov's inequality we have [formula].

Due to the law of large number, as n  →    ∞  , which implies g  →    ∞  , the second term of ([\ref=eqn:lln]) converges to (1 / k)H(y(Xk - 12,Dk - 12)|D1,Dk) in probability.

Therefore we have: for any k and n  →    ∞  , for some ε'2(k) which vanishes as k increases.

Using the same argument we also have

[formula]

Combining the last two inequalities completes the proof of Fact [\ref=fact:ZtoH]

Proof of Fact [\ref=fact:HtoRmin]: Fact [\ref=fact:HtoRmin] holds because (i) pXk - 12,Dk - 12  =  pXk - 2,Dk - 2 and (ii) (k - 2) / k  →  1 as k  →    ∞  .

Combining Facts [\ref=fact:YtoZ] and [\ref=fact:ZtoH], we have: for any fixed k and δ, as n  →    ∞  ,

[formula]

for some ε3(k) which vanishes as k increases. By choosing a large enough k, the right hand side of ([\ref=eqn:spectrum2]) can be made arbitrarily small. Combining ([\ref=eqn:spectrum2]) and Fact [\ref=fact:HtoRmin], the sequence of random variables (1 / n) log (1 / p(Xn|y(Xn,Dn))) is shown to be converging in probability to the limit lim n  →    ∞Rn.

Combining (1), (2) and (3) we have Rmin  =   lim n  →    ∞(1 / n)H(Xn|y(Xn,Dn),D0,Dn + 1).

Proof of Lemma [\ref=lem:breakdown]

We will first introduce a sequence [formula] and show that lim n  →    ∞Jn  =  Rmin.

For all [formula], let Jn: = d  +  (1 / n)H(y(Xn,Dn)|Xn,D0,Dn + 1). Then we have lim n  →    ∞Jn  =  Rmin.

We have

[formula]

Since

[formula]

we have [formula]. Since given Ly = l and given (D0,Dn + 1) the sequence y(Xn,Dn) is an iid Bernoulli(1 / 2) sequence, H(y(Xn,Dn)|Ly = l,D0,Dn + t) = l holds. Therefore [formula] and hence

[formula]

In conclusion,

[formula]

which completes the proof of Lemma [\ref=lem:Jn].

Now let us use Lemma [\ref=lem:Jn] to prove Lemma [\ref=lem:breakdown].

Expanding I(D1;y(Xn,Dn)|Xn,D0,Dn + 1) in two ways, we have

[formula]

The first term on the left side of ([\ref=eqn:fourterms]) is equal to H(D1|D0,Dn + 1). The second term on the left side of ([\ref=eqn:fourterms]) is denoted by En. The first term on the right side of ([\ref=eqn:fourterms]) is equal to n(Jn  -  d). The second term on the right side of ([\ref=eqn:fourterms]) is:

[formula]

where step (e) holds because X1 is independent of (Dn + 1,Xn2,y(Xn2,Dn2)). Therefore ([\ref=eqn:fourterms]) becomes

[formula]

Now let us take the limit as n  →    ∞   on both sides of ([\ref=eqn:newfourterms]). Because of mixing of the Markov chain {Di}i  ≥  0, the distribution pDn + 1|D0,D1(  ·  |d0,d1) converges to the stationary distribution regardless of the initial values (d0,d1) as n goes to infinity. Therefore lim n  →    ∞H(D1|D0,Dn + 1) = H(D1|D0). For the second term on the left side of ([\ref=eqn:newfourterms]), Lemma [\ref=lem:Enincreasing] guarantees the convergence of {En}n  ≥  1.

(1) The sequence {En}n  ≥  1 is nondecreasing. (2) lim n  →En exists.

(1) For all n  ≥  2, we have

[formula]

Therefore {En}n  ≥  1 is nondecreasing.

(2) Since for all n, En  ≥  1 holds and {En}n  ≥  1 is nondecreasing, E∞  =   lim n  →En exists.

By Lemma [\ref=lem:Enincreasing], the left side of ([\ref=eqn:newfourterms]) converges to H(D1|D0) - E∞ as n  →    ∞  . Since ([\ref=eqn:newfourterms]) holds, the right side also converges and the limit is [formula]. Since {Jn}n  ≥  1 is a converging sequence and the lim n  →    ∞n(Jn  -  Jn - 1) exists, lim n  →    ∞n(Jn  -  Jn - 1) = 0. Therefore in the limit as n  →    ∞  , ([\ref=eqn:newfourterms]) becomes

[formula]

which completes the proof of Lemma [\ref=lem:breakdown].

Proof of Theorem [\ref=thm:smallbeta]

When α is a fixed constant and β  ≪  1, it is easy to verify that the first two terms of ([\ref=eqn:breakdown]) are

[formula]

for any ε > 0. We will show that the third term of ([\ref=eqn:breakdown]) E∞  =  Cβ  +  O(β2 - ε).

Let us first define "typicality" of the deletion pattern. Since E∞ is the conditional entropy of D1, which is more relevant to the first a few bits of Dn0, the typicality of the Dn0 concerns about only the first a few bits.

Let k  =   max {6,6 / ( log (1 - α))}. For n  >   - k log β, the deletion pattern Dn0 is typical if the following two conditions hold.

There is at most one run of 1's in [formula].

There are no more than ( - k / 3 log β) 1's in [formula].

Lemma [\ref=lem:typicality] states that the deletion pattern is typical with high probability.

For any ε > 0, the probability that Dn0 is typical is at least 1 - O(β2 - ε).

Since any deletion pattern that has r runs of 1's in [formula] occurs with probability O(βr) and there are no more than ( - k log β)2r such patterns, [formula] contains r runs of 1's) = O(βr - ε) for any ε > 0. Hence condition 1) of Definition [\ref=def:typicality] holds with probability 1 - O(β2 - ε). Given that condition 1) holds, condition 2) is violated if there is a burst of deletion longer than ( - k / 3 log β), which occurs with the probability O((1 - α)- k / 3 log β) = O(β2). In conclusion, [formula] is typical ) = 1 - O(β2 - ε) for any ε > 0.

Let the indicator random variable T: = 1 if Dn0 is typical and T: = 0 otherwise. Lemma [\ref=lem:typicality] implies that [formula]. Lemma [\ref=lem:focusontypical] states that we can focus on the typical case T = 1 in order to evaluate E∞ to the precision of O(β2 - ε).

[formula]

For all n  >   - k log β, we have the following lower bound of En

[formula]

and the following upper bound

[formula]

Taking the limit as n  →    ∞   completes the proof.

For all n  >   - k log β, we have

[formula]

We will separately analyze the following two cases: (1) D0 = 1,T = 1 and (2) D0 = 0,T = 1.

Case (1): D0 = 1,T = 1. In this case we check whether X- k log β  =  Y- k log β. Let M1: = 1 if they match and M1: = 0 otherwise. Note that M1 is determined by Xn and y(Xn,Dn).

Case (1.1): D0 = 1,T = 1,M1 = 0. There exists at least one 1 in D- k log β1. Since D0 = 1 and there is at most one run of 1 in D- k log β0 in a typical deletion pattern, D1 = 1 must hold. Therefore H(D1|D0 = 1,T = 1,M1 = 0) = 0.

Case (1.2): D0 = 1,T = 1,M1 = 1. In this case, both D1 = 0 and D1 = 1 are possible. Given D0 = 1,T = 1, if D1 = 0, then for all [formula], Di = 0, which implies that X- k log β  =  Y- k log β. If D1 = 1, then for all [formula], Xi and Yi are independently generated fair bits, hence the event Xi = Yi occurs with probability 1 / 2. Since events {Xi = Yi}i are independent across i, [formula]. Since [formula] and [formula], by Bayes' rule, we have [formula]. Therefore [formula].

In conclusion, the contribution of Case (1) to E∞ is

[formula]

Case (2): D0 = 0,T = 1. In this case we will first check whether X- k / 3 log β  =  Y- k / 3 log β. Let M2: = 1 if they match and M2: = 0 otherwise.

Case (2.1): D0 = 0,T = 1,M2 = 1. By the same argument as in Case 1 for M1 = 1, we have [formula], and [formula].

Case (2.2): D0 = 0,T = 1,M2 = 0. We try to find a length-( - k / 3 log β) segment in Y- k log β that matches X- k log β- 2k / 3 log β + 1. Since (i) M2 = 0 implies that at least one bit in the first -  k  /  3 log β bits is deleted and (ii) a burst of deletion in a typical deletion pattern is no longer than -  k  /  3 log β, there must be no deletion in D- k log β- 2k / 3 log β + 1, which implies that there must be at least one segment in Y- k log β that matches X- k log β- 2k / 3 log β + 1. Define B: = 0 if there are two or more segments that match X- k log β- 2k / 3 log β; and for [formula], define B: = b if there is a unique segment Y- k log β - b- 2k / 3 log β + 1  -  b that matches X- k log β- 2k / 3 log β + 1 with an offset b.

Case (2.2.1): D0 = 0,T = 1,M2 = 0,B = 0. The condition B = 0 requires at least ( - k / 3 log β) independent bit-wise matches, each of which occurs with probability (1 / 2). Hence B = 0 occurs with probability at most (1 / 2)- k / 3 log β = O(β2). Therefore the contribution of Case (2.2.1) is H(D1|D0 = 0,T = 1,M2 = 0,B = 0)pD0,T,M2,B(0,1,0,0) = O(β2).

Case (2.2.2): [formula]. There must be a burst of deletion of length b taking place in D- 2k / 3 log β1 which causes the offset of b between X- k log β- 2k / 3 log β + 1 and the matching segment in y(Xn,Dn). Since the length of the burst is bounded by ( - k / 3 log β) in a typical deletion pattern, b  ≤  ( - k / 3 log β) must hold. Since we can find a correct correspondence between a segment of Xn to its outcome of deletion, the deletion process to the left of the segment is conditionally independent to the deletion process to the right. Therefore in order to evaluate the conditional entropy of D1 we need to focus on the process to the left of the segment only. Hence the contribution of this case to E∞ is: [formula], where n': =  -  2k  /  3 log β. Lemma [\ref=lem:cbeta] will show that the contribution of Case (2.2.2) is Cβ  +  O(β2 - ε). This is the only case that is responsible for the leading term Cβ in E∞.

As a summary, the contribution of all the cases (1.1), (1.2), (2.1), (2.2.1) to E∞ is of order O(β2 - ε). Lemma [\ref=lem:cbeta] will show that the contribution of Case (2.2.2) is Cβ  +  O(β2 - ε), which will complete the proof of Theorem [\ref=thm:smallbeta].

For n': =  -  2k  /  3 log β, we have [formula].

Using the abbreviation Y: = y(Xn',Dn'), we have

[formula]

where step (f) holds because of the following reason. Given (T,D0,M2,B) = (1,0,0,b), if D1 = 1, then [formula] and [formula] hold, which imply that Y  =  Xn'b + 1. Therefore the conditional entropy in ([\ref=eqn:entropyexpansion]) is nonzero only if y  =  xn'b + 1. Step (g) holds because given y  =  xn'b + 1, the probability that M2 = 0 is of order O(β2).

Define [formula] to be the length of the first b-run of xn (c.f. Definitions [\ref=def:brun] and [\ref=def:firstbrun]). In other words, for [formula], lb(xn): = l if (i) [formula], xi  =  xi - b and (ii) xb + l  ≠  xl. Let [formula] denote the sequence dn'1∈{0,1}n' satisfying that if [formula], then dj = 1, otherwise dj = 0. Due to Fact [\ref=fact:deletionbrun], if lb(xn')  =  l, then [formula] holds for all [formula], but does not hold for any i > l. Since given D0 = 0 and Dn + 1 = 0 all l deletion patterns [formula] occurs with the same probability α(1 - α)b - 1β(1 - β)n' - b, and only one of them, [formula], satisfies D1 = 1, we have H(D1|Xn' = xn',Y = xn'b + 1,l(Xn') = l,(T,D0,B) = (1,0,b)) = h2(1 / l).

For a sequence xn' satisfying lb(xn') = l, we have

[formula]

for any ε > 0.

Therefore we continue ([\ref=eqn:entropyexpansion2]) as

[formula]

where step (h) holds because k  =   max {6,6 / ( log (1 - α))} and n' =  - 2k / 3 log β, which guarantee that changing the limits of summations to infinity only leads to a change of order O(β2), and step (i) holds because [formula].