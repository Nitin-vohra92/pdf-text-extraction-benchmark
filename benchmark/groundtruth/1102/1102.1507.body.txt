Generalized Measures of Information Transfer

Introduction

Many scientific problems involve understanding the behavior of complex systems in terms of the interactions between their component parts. Using information theory, these interactions can be quantified as the information exchanged between components [\cite=Hlavackova2007] [\cite=*Lungarella2007]. Such an approach has the advantages that informational measures are sensitive to arbitrary nonlinear interactions between components and have units of measurement (bits) that are easily interpreted and compared across systems. In particular, the information-theoretic measure transfer entropy (TE) [\cite=Schreiber2000] [\cite=Kaiser2002] has become widely adopted as a standard measure of information transfer, with applications in neuroscience [\cite=Staniek2008] [\cite=*Honey2007] [\cite=*Gourevitch2007], cellular biology [\cite=Pahle2008], chaotic synchronization [\cite=Hung2008] [\cite=*Otsuka2002] [\cite=*Palus2001], and econophysics [\cite=Kwon2008] [\cite=*Marschinski2002] to name just a few.

Transfer entropy provides a directional measure of the influence that one random process, the source, has on another, the target. This influence is measured by the information that the source provides about the next state of the target when conditioned on the target's history. The idea behind TE is that conditioning on the target's history removes the information shared by the source and target due to common histories or inputs, thereby isolating the information that is actually transferred. However, conditioning does not simply remove shared information; it also adds in higher-order synergistic information, an idea that was formalized in the recently proposed partial information (PI) decomposition [\cite=Williams2010].

Here we apply this basic property of conditional information to generalize TE in two complementary ways. First, we decompose TE into two kinds of information transfer that differ regarding the influence of the target's state. We show that the resulting measures are formally related to the control-theoretic concepts of open-loop and closed-loop control, and quantify separately the state-independent and state-dependent influences of the source onto the target. Second, we apply a similar decomposition to the case of multiple sources and derive a novel multivariate generalization of TE. The resulting measures quantify separately the unique, redundant, and synergistic influences of multiple sources onto a target. Together these results provide a general framework for characterizing not only the magnitudes and directions but also the kinds of information exchange that occur between random processes.

We begin by introducing PI-decomposition for a system of three random variables, X, Y, and Z, for which we ask: How much total information do Y and Z provide about X? And, how do Y and Z contribute to the total information? The answer to the former is given by the mutual information (MI)

[formula]

where [formula] is the familiar Shannon entropy [\cite=Cover2006]. For the latter, we can identify three distinct possibilities, that is, three kinds of information that Y and Z may provide. First, Y may provide information that Z does not, or vice versa (unique information). For example, if Y is a copy of X and Z is a degenerate random variable, then the total information reduces to the unique information from Y. Second, Y and Z may provide the same or overlapping information (redundancy). For example, if Y and Z are both copies of X then they redundantly provide complete information. Third, the combination of Y and Z may provide information that is not available from either alone (synergy). The classic example for binary variables is the exclusive-OR function [formula], in which case Y and Z individually provide no information but together provide complete information. Thus, intuitively, I(X;Y,Z) decomposes into unique information from Y and Z, redundant information shared by Y and Z, and synergistic information contributed jointly by Y and Z.

PI-decomposition formalizes this idea, starting with a measure of redundancy. Letting [formula], redundancy is defined as

[formula]

where

[formula]

is the specific information that R provides about each state X = x. Thus, redundancy is defined as the minimum information that Y or Z provides about each state of X, averaged over all possible states. This definition captures the idea that redundancy is the information shared by Y and Z (the minimum that either provides) while taking into account that Y and Z may provide information about different states of X.

Using Imin and the inclusion-exclusion principle [\cite=Stanley1997], the total information I(X;Y,Z) can then be decomposed into partial information terms, given by the PI-function [formula]. The redundancy is given by [formula]. The unique information from Y is given by [formula], or the total information from Y minus the redundancy, and likewise for Z. Finally, the synergy is given by [formula], where Imax is defined the same as Imin except substituting max  for min . Together these terms yield the decomposition:

[formula]

An immediate consequence of Eqs. [\eqref=MIDecompEq1] and [\eqref=MIDecompEq2] is that the conditional MI I(X;Y|Z) decomposes into the unique information from Y plus the synergy from Y and Z:

[formula]

Thus, conditioning I(X;Y) on Z not only removes the redundancy from Y and Z, but also adds in their synergy. This observation makes intuitive sense if we think of I(X;Y|Z) as answering the question: How much information do we gain from learning Y when we already know Z? Clearly, this will include both the information that comes uniquely from Y plus the synergistic information that comes from Y and Z together.

Transfer entropy can be analyzed in a similar way, since it is simply an application of conditional MI to stochastic processes. Given processes X and Y, the TE from Y to X is defined as

[formula]

where X(k)t is the k-dimensional delay vector for X, and likewise for Y(l)t and Y (henceforth the superscripts are omitted for clarity). In other words, TY  →  X quantifies the information that previous values of Y provide about the next state of X when conditioned on X's own history. In terms of transition probabilities, TY  →  X can also be thought of as quantifying deviation from the generalized Markov property p(xt + 1|xt,yt)  =  p(xt + 1|xt), with TY  →  X  =  0 iff Y has no influence on the transitions of X.

Decomposing Transfer Entropy

Our first main result is that, by decomposing TY  →  X, we can distinguish two kinds of information transfer; that is, two distinct ways that Y can influence the transitions of X (FIG. [\ref=te-pi-decomposition]). Letting [formula] and combining Eqs. [\eqref=CondMIDecompEq] and [\eqref=TEEq], we have that

[formula]

where [formula] is the unique information that Yt provides about Xt + 1 and [formula] is the synergistic information from Xt and Yt. As we will show, [formula] corresponds to state-independent transfer entropy (SITE): it measures the portion of Yt's influence on Xt + 1 that does not depend on Xt. The complementary term [formula] is the state-dependent transfer entropy (SDTE): it measures the influence that Yt has on Xt + 1 only when combined with an appropriate state of Xt. To ground this interpretation, we next establish a formal connection between SITE and SDTE and the control-theoretic notions of open-loop and closed-loop control.

In control theory, one considers a process Xt--characterized by its initial state X and final state X'--and a controller C, with the two related by a distribution p(x'|x,c) [\cite=Touchette2000] [\cite=Touchette2004]. The aim is to specify a control policy, given by the distribution p(c|x), that moves the system to certain desired final states. In open-loop control, the controller C acts independently of the initial state X (I(X;C)  =  0), while closed-loop control is characterized by state-dependent actuation.

A fundamental property of a control system is its controllability, which is the extent to which it can be moved through its entire state space. In particular, a system has perfect controllability iff there is a control policy that moves the system deterministically from any x∈X to any x'∈X'. In [\cite=Touchette2004], it is shown that a natural information-theoretic measure of controllability is I(X';C|X)--the information transfer from the controller to the controlled process--which is maximal exactly in the case of perfect controllability. Thus, there is a close parallel between information transfer and controllability, where essentially the only difference is semantic: information transfer applies to arbitrary interactions between processes, while controllability is concerned specifically with using one process to influence another.

With this in mind, the following result connects SITE and SDTE with open-loop and closed-loop control (proof in Appendix [\ref=app:proofs]). Thus, decomposing I(X';C|X) as in Equation [\eqref=TEDecompEq], SITE from C to X' measures a system's open-loop controllability (maximal for perfect open-loop control), while SDTE measures the additional contribution from closed-loop control. More generally, this connection grounds the interpretation of SITE as the state-independent (open-loop) influence of one process on another, and likewise for SDTE and state-dependent (closed-loop) influence.

As a simple example to illustrate the two kinds of transfer, consider two binary state Markov processes X and Y, where Y is purely random and X is stochastically coupled to Y. Specifically, if xt  =  0, then xt + 1  =  yt, while if xt  =  1, the probability that xt + 1  =  yt is 1 - d and that xt + 1  =  1 - yt is d. A simple eigenvector calculation yields the stationary distribution p(x,y) = 1 / 4 for all x and y, and from this all informational quantities can be computed. When d = 0, Xt + 1 is simply set to Yt regardless of its own previous state, thus corresponding to pure SITE (FIG. [\ref=SDTEvsSITE]). With this parameter setting, the system is essentially equivalent to the discrete example considered in [\cite=Kaiser2002]. In contrast, when d = 1, yt = 0 causes X to remain in the same state and yt  =  1 causes X to switch states. Consequently, Y's influence on Xt + 1 depends entirely on Xt, corresponding to pure SDTE. In fact, if one imagines using Y to control X, then d = 1 corresponds to a 'controlled-NOT' gate, which is known to require closed-loop control [\cite=Touchette2000] [\cite=Touchette2004]. FIG. [\ref=SDTEvsSITE] shows how varying d produces a smooth transition between these two extremes.

Finally, we note that the distinction between SITE and SDTE also clarifies the relationship between TE and the time-delayed mutual information (TDMI) I(Xt + 1;Yt), which was the standard measure of information transfer prior to TE [\cite=Schreiber2000]. Transfer entropy was initially proposed as an alternative to TDMI because the latter fails to remove shared information due to common histories or inputs. From FIG. [\ref=te-pi-decomposition], it is clear that this shared information corresponds to [formula], the redundancy between Xt and Yt. However, FIG. [\ref=te-pi-decomposition] also reveals a second crucial difference between TDMI and TE, which is that TDMI fails to include SDTE. Thus, not only does TDMI incorrectly add in shared information, but it also leaves out a significant component of information transfer.

Multivariate Information Transfer

Our second main result is a novel multivariate generalization of TE, based on applying PI-decomposition to the information from multiple sources. Schreiber [\cite=Schreiber2000] originally proposed a generalization of TE based on 'conditioning out' other sources, an idea that has since been adopted and extended by others [\cite=Frenzel2007] [\cite=*Lizier2008]. However, it should be clear from the preceding discussion that such a generalization is problematic, since conditioning does not simply remove shared information. Our generalization addresses this deficiency by quantifying separately the unique, redundant, and synergistic transfer from multiple sources.

For simplicity, we consider only two sources Y and Z acting on a target X (the general case is discussed momentarily), in which case the total TE is given by TY,Z  →  X  =  I(Xt + 1;Yt,Zt|Xt). Applying PI-decomposition as before, we arrive at measures for the redundant transfer from Y and Z: T{Y}{Z}  →  X  =  Imin(Xt + 1;Yt,Zt|Xt); the unique transfer from Y (resp. Z): [formula]; and the synergistic transfer from Y and Z: T{Y,Z}  →  X  =  TY,Z  →  X  -  Imax(Xt + 1;Yt,Zt|Xt). Generally speaking, redundant transfer corresponds to situations where the apparent influence from multiple sources may in fact be due to any one (or several) of them, indicating that interventional methods are required to determine the true causal structure [\cite=Ay2008]. In contrast, unique transfer represents the portion of a source's influence that can only come from that source, or, if all possible sources are considered, that must come from that source. Finally, synergistic transfer indicates that several sources act together cooperatively to influence the target.

To illustrate, consider three binary state Markov processes X, Y, and Z. Y is purely random, and Z is stochastically coupled to Y such that zt  =  yt with probability (1 + c) / 2 and zt  =  1 - yt with probability (1 - c) / 2. This coupling can be thought of as an external signal driving Y and Z to synchronize: as c goes from 0 to 1, Y and Z transition from independence to complete synchronization. X in turn is coupled to both Y and Z such that, if zt  =  0, xt + 1  =  yt, while if zt  =  1, xt + 1  =  yt with probability (1 - d) and xt + 1  =  (1 - yt) with probability d. Thus, xt + 1  =  yt when d = 0, and [formula] when d = 1. At the extreme parameter settings, this system exhibits three different behaviors (FIG. [\ref=mvte_plot]). With (c = 0,d = 0), Y and Z are independent and X depends only on Y, so the only influence is unique transfer from Y to X. In contrast, with (c = 1,d = 0), X again depends only on Y but Y and Z are now synchronized, so there is only redundant transfer from Y and Z. Indeed, in this case it is impossible to determine from observation alone whether Y or Z (or both) is driving X. Finally, with (c = 1,d = 0), Y and Z are independent and [formula], corresponding to pure synergistic transfer.

As a final example, we extend the analysis of a multivariate physiological time series presented in [\cite=Schreiber2000] [\cite=Kaiser2002]. The data consists of simultaneous recordings of the breath rate (chest volume), heart rate, and blood oxygen concentration for a patient suffering from sleep apnea. Previous analysis compared TE and TDMI for both directions between the breath and heart signals. However, directly comparing TE and TDMI is problematic and potentially misleading, since both measures detect SITE but differ regarding SDTE and shared information (FIG. [\ref=te-pi-decomposition]). Indeed, with no additional information, it is impossible to determine even whether TE and TDMI are detecting the same or different aspects of an interaction.

To address this issue, we calculated SITE and SDTE between the breath and heart signals. Joint probability estimates were obtained by kernel estimation using a rectangular kernel with bandwidth r. Neighboring points closer than 20 time steps were excluded and points with fewer than 5 neighbors were ignored, following the suggestions of Schreiber and others [\cite=Schreiber2000] [\cite=Kaiser2002] [\cite=Pahle2008]. Our main finding is that SITE is consistent with zero in both directions between the breath and heart signals. Thus, for these signals, TE and TDMI in fact quantify entirely separate things: TDMI is due only to shared information from common histories or inputs, while TE detects state-dependent information exchange. This state dependence can be seen by plotting information transfer as a function of the target state, shown in FIG [\ref=physio_state_dep_plot] for [formula]. For pure SITE, this plot would be uniform across target states, while FIG. [\ref=physio_state_dep_plot] shows a clear bimodal distribution. These two modes correspond to downswings and upswings in chest volume, suggesting that heart rate has the largest influence on respiration when chest volume is low and, to a lesser extent, when it is high, but minimal influence when chest volume is near its mean (see also FIG. [\ref=supp_physio_sdte_plot] in Appendix [\ref=app:fig]).

We also analyzed the combined influence of heart rate and blood oxygen level on breath rate (FIG. [\ref=physio_mvte_plot]). The most significant component is consistently the unique information transfer from the heart rate, indicating that most of the TE discussed above is uniquely attributable to the heart signal. However, there is also considerable redundant and synergistic transfer, of roughly comparable magnitude, from heart rate and blood oxygen concentration. In contrast, there is essentially no unique information transfer from blood oxygen concentration, indicating that all of its apparent influence could also be due to heart rate.

We conclude by noting that the multivariate generalization described here extends naturally to any number of sources, simply by applying the general form of PI-decomposition [\cite=Williams2010]. The two extensions described in this Letter can also be applied in conjunction, allowing one to quantify, e.g., state-dependent synergistic transfer. Thus, together these methods provide a completely general framework for characterizing information exchange in complex systems.

Proof of Theorem 1

As defined in [\cite=Touchette2004], a system has perfect controllability iff for any initial state x and final state x' there exists a controller state c such that p(x'|x,c)  =  1. We first prove that an equivalent definition of perfect controllability is that a system can be moved deterministically to any final state from any distribution of initial states.

If a system is perfectly controllable, we know that for a given x' there exists at least one c for each x such that p(x'|x,c)  =  1. Thus, we can choose

[formula]

for each x∈X, which guarantees that p(x')  =  1 for any distribution p(x). As this is verified for any x', this proves the direct part of the theorem.

Conversely, note that if p(x')  =  1 for a given x' and any distribution p(x), it must be that

[formula]

for each x∈X, and thus for each x there must be at least one c for which p(x'|x,c)  =  1. As this holds for any x', the converse is proven.

In order for a system to be perfectly controllable via open-loop control, we also require that the controller acts independently of the initial state, leading to the following definition.

An alternative definition of perfect open-loop controllability is given by the following lemma.

If I(X;C) = 0, then p(x') can be written as

[formula]

If in addition we have that p(x') = 1 for a given x' and any distribution p(x), then there must exist a c for which p(x'|x,c) = 1 for all x, i.e., p(x'|c) = 1. This holds for any x', so the direct part of the theorem is proven.

Conversely, if for any x' there exists a c such that p(x'|c) = 1, then for a given x' we can choose

[formula]

with p(c) = p(c|x) for all c and x, ensuring that p(x') = 1 and I(X;C) = 0. As this holds for any x', the converse is proven.

In [\cite=Touchette2004], it is shown that an equivalent information-theoretic definition of perfect controllability is that there exists a distribution p(c|x) such that each final state is reachable from each initial state, i.e.,

[formula]

for all x and x' and that, for any distribution p(x), I(X';C|X) is maximal, i.e.,

[formula]

so that

[formula]

Consequently, I(X';C|X) is naturally interpreted as a system's degree of controllability, which is maximal iff the system is perfectly controllable.

By the same reasoning, Theorem 1 establishes that the SITE from C to X' is a natural measure of a system's open-loop controllability, maximal exactly in the case of perfect open-loop control. In order to prove Theorem 1, we will need the following basic property of SDTE.

[formula]

where and [formula] is the Kullback-Leibler divergence [\cite=Cover2006]. [formula] is nonnegative, so [formula] iff, for each x'∈X', I(X' = x';C|X)  =  0 or I(X' = x';X|C)  =  0. Furthermore, since [formula] iff q = r, [formula] iff, for each x'∈X',

[formula]

or, equivalently,

[formula]

Now we are in a position to prove Theorem 1.

We will show that a system has perfect open-loop controllability iff there exists a distribution p(c|x) such that Eqs. [\eqref=reachability] and [\eqref=determ-trans] are satisfied and there is no SDTE from C to X',

[formula]

If a system is open-loop controllable, then for each x' there exists a c such that p(x'|c)  =  1. Choosing

[formula]

over all x'∈X', with p(c) = p(c|x) for all c and x, ensures that H(X'|X,C)  ≤  H(X'|C)  =  0 and p(x'|x)  ≠  0. Also, the chosen distribution p(c|x) ensures that [formula] so that [formula] by the preceding lemma. This proves the direct part of the theorem.

For the converse, note that p(x'|x)  ≠  0 for a given x' and x means that there is at least one c for which p(x'|x,c)  ≠  0 and, since H(X'|X,C)  =  0, we can further conclude that p(x'|x,c)  =  1. If we also have that [formula], we know that, for each x'∈X',

[formula]

and thus that, for each x'∈X',

[formula]

But it cannot be the case that [formula], since that would violate the reachability condition (Eq. [\eqref=reachability]), so we conclude that for each x' there exists a c such that p(x'|c)  =  1. This proves the converse.

State-Dependent Influence on Breath Rate

As mentioned in the main text, we found that heart rate has an exclusively state-dependent influence on breath rate. This is shown most clearly by superimposing the information transfer values on the breath rate signal (FIG. [\ref=supp_physio_sdte_plot]).