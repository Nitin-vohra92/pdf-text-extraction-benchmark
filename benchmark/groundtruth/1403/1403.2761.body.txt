Maximum-Likelihood Approach to Topological Charge Fluctuations in Lattice Gauge Theory

Introduction

Lattice field theory is a powerful technique for the numerical study of Yang-Mills gauge theories. Recovery of continuum field-theory results requires extrapolations in the lattice spacing and volume, which are generally controlled and well-understood. One effect of working in a finite volume V is that the theory becomes dependent on the global topological charge Q [\cite=Brower:2003yx] [\cite=Aoki:2007ka]. Locality and cluster decomposition properties suggest that such effects vanish as V  →    ∞  , but they must be accounted for in the extrapolation.

In Euclidean Yang-Mills quantum field theory on a torus, the topological charge Q is quantized, dividing the configuration space into distinct topological sectors. These sectors are separated by an action barrier, so that the use of sampling algorithms which favor small changes in the action (such as the commonly used hybrid Monte Carlo algorithm) can lead to poor sampling of this distribution. Since the action barrier can grow with decreasing lattice spacing [\cite=Alles:1996vn] [\cite=DelDebbio:2004xh] [\cite=Luscher:2010iy] or increasing number of flavors Nf [\cite=Appelquist:2009ka] [\cite=Appelquist:2012nz], the cost of tunneling to different topological sectors can vary greatly depending on the details of the calculation.

The "freezing" of topological charge resulting from these algorithmic problems leads to extremely long autocorrelation times, so that the distribution of Q is poorly sampled. Correction of the resulting systematic effects on observables can be done [\cite=Brower:2003yx] [\cite=Aoki:2007ka], but these corrections require as inputs the cumulants of the topological charge distribution, particularly the variance [formula], where χt is the topological susceptibility. Using the standard estimator for variance requires many independent samples; autocorrelations can lead to relatively few independent measurements and a biased estimate with a large sampling error.

In this work, we suggest two ways to proceed when confronted with this problem. First, it is generally believed that the maximum likelihood (ML) method (see Sec. 36.1.2 of [\cite=Beringer:1900zz]) can produce reliable estimates of model parameters when there are relatively few independent samples of a distribution, provided the functional form of the underlying distribution is known analytically. Inspired by the work of Phil Nelson and collaborators [\cite=Beausang:2011], we present such a maximum likelihood approach (following an example by Franco [\cite=Franco:2003] in the context of financial time-series) to analyze the complete time-series information {Qn}. The analysis is done without blocking, since the effect of autocorrelations is built into the model. This method in principle allows the estimation of χt from even a handful of tunneling events.

Eventually, if one performs a calculation at sufficiently small (but finite) lattice spacing [\cite=Luscher:1981zq], global topological charge will never change in any finite number of Markov steps. If we choose a lattice volume V such that Vχt  ≫  1, we can consider the distribution of topological charge Qs computed only in subvolumes Vs  ≫  χ- 1t, which by locality and cluster decomposition should also be distributed asymptotically as the stationary distribution P(Q). In this scenario, we can employ either our ML method or a more standard blocked sample variance estimate to compute the susceptibility, depending on the number of independent samples . Empirically, we find that the calculation of χt based on a subvolume gives the most robust estimates of χt in the case that relatively few uncorrelated measurements of Q are available, although further study of this approach is needed.

The contents of this manuscript are as follows: in we discuss what is known about the distribution of topological charge in lattice simulations of Yang-Mills gauge theories. gives the definition of an Ornstein-Uhlenbeck (OU) process, which uniquely describes continuous Markov processes that remain Gaussian distributed. describes the maximum-likelihood estimation of χt based on the OU model. In , the implications of studying calculations with nearly-fixed global topological charge are discussed, and a modification of the maximum-likelihood estimate using lattice subvolumes is introduced. demonstrates the use of the proposed methods to extract χt on an example set of lattice configurations, and compares to other standard approaches. Finally, summarizes our results and discusses future applications and possible improvements.

Distribution of topological charge

Numerical lattice computations make use of a Markov process to sample the configuration space, generating a sequence of configurations [formula] with corresponding topological charges {Qn}. As the sample size n increases, the distribution P(Qn) converges to a stationary distribution P(Q).

What is known about the distribution P(Q)? With zero θ-parameter, all odd cumulants of the distribution must vanish by parity invariance. Furthermore, analysis of SU(Nc) gauge theories at large-Nc shows that the even cumulants scale as κ2n  ~  N2 - 2nc [\cite='tHooft:1973jz] [\cite=Witten:1978bc] [\cite=Witten:1980sp], suggesting that the distribution will be approximately Gaussian. Given the suppression of higher cumulants, it seems reasonable to express the distribution P(Q) in terms of its Edgeworth series [\cite=Blinnikov:1997jq], truncated to the first non-Gaussian term:

[formula]

where PG(x) is the Gaussian distribution with zero mean and unit variance and [formula] is a Hermite polynomial. The variance κ2 and 4th-order cumulant κ4 for this distribution are

[formula]

We identify the variance [formula], which defines the topological susceptibility χt. As ε  →  0, this distribution becomes purely Gaussian; several lattice studies have empirically found non-zero ε in SU(Nc) gauge theories [\cite=D'Elia:2003gr] [\cite=DelDebbio:2006df] [\cite=Durr:2006ky] [\cite=Giusti:2007tu] [\cite=Bonati:2013tt]. We note that the dependence of this non-Gaussianity on the presence of light fermions is unclear, and large-Nc arguments may be inapplicable for theories with many fermions Nf, unless Nf / Nc is held fixed as Nc  →    ∞  .

Ornstein-Uhlenbeck process

We wish to consider Markov processes that can reproduce the approximately-Gaussian topological charge distribution . In fact, up to linear transformations in the variables, there is a unique non-trivial example of a continuous Markov process in which the expected distribution at any point in the stochastic evolution is Gaussian: the Ornstein-Uhlenbeck (OU) process [\cite=Uhlenbeck:1930zz] [\cite=Doob:1942zz], which describes the Brownian motion of a massive particle in the presence of arbitrary linear friction. This process is described by the stochastic differential equation

[formula]

where η  >  0, σ  >  0 and W(t) is the stochastic Wiener process of Brownian motion. The standard solution leads to the following statistics:

[formula]

which converge to a Gaussian with mean x̄ and variance σ2  /  2η as t  →    ∞  , independent of the starting position x(0). We will discuss the accuracy of this model in the presence of small non-Gaussianities in below.

The detailed evolution of topological charge in a lattice gauge theory calculation is quite complex and dependent on unphysical details such as the choice of the update algorithm. Since it is a Markov process and since Q is distributed as a Gaussian asymptotically (up to corrections which we will discuss), we will model the evolution of topological charge as an OU process.

The friction parameter η is sensitive to algorithmic details and therefore not physically relevant, so it will be treated as a nuisance parameter. Although we will not investigate it in detail here, we note that the parameter η may be of interest in the comparison of different lattice update algorithms (with the underlying physical parameters held fixed). In particular, the autocorrelation R(τ) for the process x(t) from the standard solution is given by

[formula]

which for t  ≫  1 / η converges to e-  ητ. We can therefore identify 1 / η as the standard autocorrelation time for x. Our approach to be described below therefore gives an alternate way to estimate the autocorrelation time for an observable which is expected to always be approximately Gaussian distributed.

Maximum Likelihood Estimate

We assume that we have N + 1 computations of the topological charge Qi at steps ni in the Markov chain, where the ni need not be equally spaced. Due to parity invariance of Yang-Mills theory, all odd moments are identically zero, including the mean 〈Q〉  =  0. The second moment, or equivalently the variance, gives the topological susceptibility

[formula]

In the OU model, we identify the susceptibility in terms of the model parameters, Vχt  =  σ2  /  2η. The conditional probability of finding Qi at step ni given Qi - 1 was found at step ni - 1 is Gaussian with mean and variance given by Eq. ([\ref=eq:OUvarmean]) with appropriate asymptotic values:

[formula]

The log-likelihood function (dropping additive constants) given the time series is

[formula]

where for later convenience we have defined the sum

[formula]

The maximum likelihood (ML) estimates [formula] and t minimize the log-likelihood function L(η,Vχt). At the minimum,

[formula]

which leads to

[formula]

If we substitute S(η) / N for Vχt in the log-likelihood function we now need to solve the one-dimensional problem to find [formula] that minimizes

[formula]

where we have dropped further additive constants. Once [formula] is known then Vt is known as well.

Since the OU model assumes the underlying distribution is Gaussian, it is interesting to understand how well the OU-model ML estimates can reproduce the variance of the nearly Gaussian distribution in Eq. ([\ref=eq:EdgeworthQ]) for ε  ≈  0.2 [\cite=Giusti:2007tu]. As a simple test, we generated 100,000 samples of the distribution for σ2 = 2 and [formula] and used both the OU ML method and the standard sample variance to estimate 〈Q2〉. Both estimates agree well with the analytic value, as shown in . In addition, near-perfect agreement is seen between the OU model and the standard sample variance, for this test in which the underlying true distribution is near-Gaussian and well-sampled.

NEARLY FIXED TOPOLOGY

For lattice calculations in which the topological charge tunnels frequently, the distribution of Q will be well-sampled, and χt can be estimated simply from the empirical sample variance, or from a least-squares (LS) fit of a Gaussian to the Q distribution. The advantage of the ML method is that it should still yield robust estimates of χt even when the distribution is relatively poorly sampled. However, in extreme cases where the number of observed tunneling events is O(10) or less, the uncertainty in χt can become very large, as a lack of tunneling events can be explained by either large χt and small η, or vice-versa. Marginalizing over η leads to essentially a lower bound on χt.

Recently, it has been suggested that the use of Neumann boundary conditions along one of the directions of the lattice would eliminate the barrier to changing topology [\cite=Luscher:2011kk]. One can think rather informally of this scenario as topological charge being allowed to flow freely through the boundaries between the lattice and an infinite reservoir. This physical picture suggests an alternative approach to estimation of χt.

Consider a periodic lattice with volume V = L3  ×  T and T  ≫  L, and then select some contiguous interval of length Ts  ≪  T, such that [formula]. The total topological charge Qs contained within this subvolume Vs will be a continuous variable, since charge is no longer conserved; it can move freely into the complement of Vs, which we can think of as a reservoir.

The existence of a non-zero global topological charge Q on the full volume may bias the distribution of charge within the subvolume; in particular, if Q is fixed, then the mean charge contained within Vs will be equal to QVs  /  V. We therefore define a "subtracted" subvolume charge,

[formula]

where q(x) is the topological charge density at lattice site x. We then carry out the analysis exactly as before, but with the substitutions V  →  Vs and Q  →  Qs,.

It seems reasonable, although not proven, that χt computed this way is an acceptable estimator of topological susceptibility when using the methods suggested in [\cite=Brower:2003yx], given that V was periodic and translationally invariant and Vs was chosen at random. Thus, we can apply our same ML method to a time series in Qs to get an estimate of Vsχt. Even with nearly-fixed Q, it may be possible for Qs to fluctuate frequently enough to allow a reliable LS fit. In this case, we can check that ML and sample variance methods produce compatible results for Vsχt.

EXAMPLES

As a trial of this method, we take a few time series of topological charge on a set of three 163  ×  32 lattice ensembles with Nf  =  2+1 domain wall fermions, generated by the RBC and UKQCD collaborations [\cite=Allton:2007hx]. The relevant data and empirical distributions of Q are plotted in . For the analysis to follow we take a thermalization cut of 200 MD time units on all three ensembles. The topological charge is measured every 5 MD time units.

In Fig. [\ref=fig:rbc01-contour], for the lightest mass ml  =  0.01 ensemble we show the 2ΔL  =  1,4,9 contours appropriate for estimating the 1, 2, 3 σ confidence intervals on Vχt while marginalizing over the friction parameter η. The resulting 1-σ confidence interval on Vχt is found to be in good agreement with the standard sample-variance estimate.

The negative correlation between Vχt and η is expected, since they are inversely related through the asymptotic variance of the model distribution, Vχt  =  σ2  /  2η. For data sets with relatively few tunneling events, we expect the ellipsoid will become elongated and follow a hyperbolic curve due to this relation.

We would now like to test the proposed subvolume analysis of , in conjunction with both the sample variance and ML methods. The use of only a fixed subvolume from all configurations would reduce the available statistics, so we make use of a bootstrap procedure in order to improve our statistical precision. We draw Nb  =  1000 bootstrap replications from the distribution of pairs {Qi,Qi + 1} in the topological charge time series, allowing us to resample while preserving the information on transitions required by the ML method. We fix the subvolume size Ts  ≤  T, and then within each bootstrap replication choose a random starting position t∈[0,Nt  -  1] for the subvolume on each configuration in the timeseries; the choice is randomized for each bootstrap replication. This procedure imposes the expected translation invariance in the t-direction.

For the sample variance procedure, the data are blocked before drawing bootstrap samples, in order to deal with autocorrelation effects. Empirical tests on the data show stability of error estimates on χt for a block length of roughly ~  100 trajectories or 20 configurations. No blocking is used for the ML analysis, which includes autocorrelation effects in the model. For both the sample variance and ML subvolume analyses, the central value and error estimates correspond to the median and one-sigma quantiles of the bootstrap distribution, respectively.

In Fig. [\ref=fig:rbc-compare] and , we summarize our determination of Vχt for the three ensembles shown in Fig. [\ref=fig:topology] using the various methods described. The subvolume results here are for fixed Ts  =  8. As expected from a time series with many independent samples of P(Q), the maximum likelihood (ML) result agrees closely with the sample variance (SV) estimate of Vχt.

We further investigate the subvolume estimates, and in particular their dependence on the choice of subvolume size, by varying the temporal extent Ts and repeating the analysis. The results are shown for all three ensembles in . A strong variation is seen at small Ts, setting in approximately where Vsχt  ≈  1, which is where our assumptions about the simplicity of the distribution P(Q) are anticipated to break down. For large Ts the dependence on subvolume size is nearly flat, but with a small systematic trend evident, particularly on the ml  =  0.03 ensemble. We have no immediate physical explanation for the origin of this subleading effect, but plan to investigate further in a future work.

It is apparent that the ML method does not offer any significant advantage in the determination of Vχt over a simple calculation of the sample variance when the underlying distribution P(Q) is well-sampled, as is the case for the full time series on each of the RBC ensembles. However, we expect the ML technique to be a robust approach even when a small number of independent samples are available. Furthermore, even when the distribution is well-sampled, the ML method has the advantage of including autocorrelation effects automatically, into the friction parameter η, whereas the SV analysis requires an autocorrelation analysis and blocking to be carried out first.

We can test what might happen in a case with poor sampling by analyzing a restricted subset of the RBC time series. Fig. [\ref=fig:plot-ts-compare] shows the results of this test on the ml  =  0.01 ensemble, with the analysis considered on the restricted time series with MD time [formula]; as a reminder, Q is measured every 5 MD time units. For the SV analysis, we adjust the blocking when only a small number of configurations are available; specifically, we use a block length of τ  =  50 when less than 200 time units are available, and τ  =  25 for less than 100 time units available. With only a subset of the configurations, the full-volume methods show a clear bias with respect to the best estimate of Vχt from the full ensemble. On the other hand, both the ML and SV subvolume approaches converge rapidly to cover the asymptotic estimate, with the ML being particularly effective at small [formula] where a simple blocking analysis cannot adequately account for the known autocorrelation effects.

Discussion

We have introduced a new maximum-likelihood approach to estimation of the topological susceptibility χt in lattice calculations, based on maximum-likelihood analysis of the full time-series information. This approach can give an advantage over more traditional methods such as calculation of the sample variance of Q, particularly in the case that autocorrelation times are long and relatively few independent samples are available, due to the inclusion of autocorrelation effects within the ML model. The autocorrelation time of Q can also be estimated as a byproduct of the analysis.

In addition, we have explored the analysis of topological charge fluctuations on lattice subvolumes. This technique may be necessary in cases where the global topological charge goes through few or even no tunneling events within a lattice calculation. Even when Q fluctuates adequately, the subvolume method (in conjunction with the ML analysis) was found to give the most robust estimates of χt, with confidence intervals rapidly converging to cover the asymptotic estimates of this quantity even on small amounts of data. Stability of the estimate with respect to the subvolume size was observed empirically down to Vsχt  ≈  1, at which point our physical assumptions about the fluctuations should break down.

A modification of the ML approach to a more complex model than the OU process, which might be able to deal with non-Gaussian distributions and therefore extract higher moments by the maximum-likelihood approach, would be interesting to study in a future work. The modeling of higher-order systematic dependence on the subvolume size, as hinted at by our current analysis, also merits further study.

We thank the RBC-UKQCD collaboration for the use of their lattice configurations, first published in Ref. [\cite=Allton:2007hx], and we also thank Tom Blum and Philippe de Forcrand for useful discussions. This work was supported in part by the National Science Foundation under Grant No. PHYS-1066293 and the hospitality of the Aspen Center for Physics. M.L. was partially supported by SciDAC-3 and Argonne Leadership Computing Facility at Argonne National Laboratory under contract DE-AC02-06CH11357, and the Brookhaven National Laboratory Program Development under grant PD13-003. D.S. was supported by DOE Grant Nos. DE-SC0010005, DE-SC0008669 and DE-SC0009998. R. C. B., C. R., and E. W. were supported by DOE grant DE-SC0010025. In addition, R. C. B., C. R., M. C. and O. W. acknowledge the support of NSF grant OCI-0749300, and G. F. and G. V. were supported by NSF grant PHY11-00905. We thank LLNL for funding from LDRD13-ERD-023, and E. R., C. S., and P. V. acknowledge the support of the U. S. Department of Energy under Contract DE-AC52-07NA27344 (LLNL).