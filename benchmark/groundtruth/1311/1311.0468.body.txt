Proposition

Thompson Sampling for Online Learningwith Linear Experts

Setup

Consider the full-information linear generalization setting, similar to the one studied by Kalai and Vempala [\cite=KalaiV05]. We can select, at each time t  ≥  1, a decision dt from an action set [formula]. Following the t-th decision dt, we get to observe [formula] and receive a reward of [formula]. The goal is to maximize the total reward [formula].

As shorthand we will write St for the vector [formula], and xi for the i-th coordinate of a vector x. Throughout, In and [formula] denote the identity matrix and all-ones vector in dimension n respectively.

Consider the Thompson Sampling algorithm TSG(ε), with ε  >  0, and Gaussian prior and likelihood (Algorithm [\ref=alg:tsgauss]).

By standard results, upon observing iid standard normal samples [formula] distributed as N(μ,σ2), with nonrandom variance σ2 and prior μ  ~  N(μ0,σ20), the posterior distribution of the mean μ is again Gaussian with mean [formula] and variance [formula]. In our case, at time t  ≥  2,

[formula]

Thus, the TSG algorithm perturbs the aggregate 'state' St - 1 seen so far with Gaussian noise, and takes the best decision for this perturbed state. This is akin to the Follow-the-Perturbed-Leader (FPL) strategy developed by Kalai and Vempala [\cite=KalaiV05], and we apply their techniques to provide regret bounds for TSG that hold over all sequences [formula] in S. Our result involves the following parameters:

[formula]

As usual, for a sequence of states [formula], we define the regret RA(T) of a strategy A to be the difference between the reward earned by A on the sequence and the reward earned by the best fixed decision in hindsight:

[formula]

The expected regret of TSG(ε) satisfies

[formula]

where K2,n and K∞  ,n are positive constants that depend only on n.

Note: Setting [formula] implies an expected regret of [formula].

Let us introduce the notation [formula]. TSG chooses the decision M(St - 1 + pt) at time t, where [formula], [formula].

First, an application of Lemma 3.1 in [\cite=KalaiV05] gives that for any state sequence [formula], T  >  0 and vectors [formula],

[formula]

Next, observe that the expected reward is unchanged if for each t  >  1, [formula]. For such a noise sequence,

[formula]

TSG earns reward [formula] at each time t, and the best possible reward in hindsight over the entire time horizon [formula] is [formula], so in order to bound the regret of TSG using ([\ref=eqn:basicbd]), it remains to bound the expectation of the difference [formula]. Let [formula], and let dνa(  ·  ) be Gaussian measure on [formula] with mean 0 and variance a- 1In. Observe that

[formula]

Thus, we can write

[formula]

where [formula] for p  ≥  1. Combining the above with ([\ref=eqn:basicbd]) and ([\ref=eqn:noisebd]) and summing over [formula] gives

[formula]

completing the proof.