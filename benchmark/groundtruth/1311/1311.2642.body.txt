Volumetric Reconstruction Applied to Perceptual Studies of Size and Weight

Introduction

Motivation

We like to believe that our sensory systems provide us with precise and accurate information about objects within the environment, but our perception is often subject to systematic errors, or illusions (Figure [\ref=fig:intro]). These can also occur between sensory modalities, often with visual information influencing haptic (touch) estimates of properties such as size or weight. For example, a curious experience occurs when we lift two objects of equal weight but different size; systematically and repeatably, the smaller object feels heavier than the larger. This size-weight illusion (SWI) [\cite=Charpentier91] cannot be explained by simple motor force error (i.e., it is not simply due to the production of more lifting or grip force for the larger object) [\cite=Flanagan00] [\cite=Grandy06], and so carries important implications for the dynamics of sensory integration between vision and haptics. Likewise, altered visual appearance of an object (e.g. through stereoscopic goggles [\cite=Ernst02] or optical distortion with prisms [\cite=Rock64]) can significantly impact haptically-judged estimates of its size. Simply put, when an object looks bigger than it really is, it feels bigger, too - and any mismatch between vision and touch often goes completely unnoticed.

In order to establish a solid quantitative empirical assessment of these illusions, we have developed methodologies to examine the relationship between true size and perceived size. Previous investigations have uncovered evidence that the relationship between an object's true volume and its perceived volume often follows a power function with an average exponent of 0.704 (Ïƒ = 0.08) [\cite=Frayman81]. However, these prior investigations have predominantly used objects which are geometric, symmetrical, and convex - properties which alone cannot adequately capture the range of objects regularly encountered in everyday environments. Thus, to systematically and comprehensively explore the relationship between true volume and perceived volume so as to better understand this percept's contribution to visual-haptic integration and, consequently, perception in general, we have developed dedicated methods to capture an ecologically valid set of stimuli.

Our goal is to build a data base of digital models of our specimens, which would allow us to infer volume and any other desired geometric properties. A method to create such models should meet the following list of criteria:

It is mandatory that the sensing modality of choice be contactless: Cell phones and other hand-held consumer electronics, e.g., are among the classes of objects relevant to our psychological studies. They cannot simply be sunk in a fluid leveraging Archimedes' principle, neither can anything which is permeated by the fluid, as doing so would not provide the desired visible volume.

The underlying sensor should be inexpensive and easy to use for non-experts; e.g., researchers in psychology and neuroscience. This rules out dedicated lab equipment, e.g., for white-light interferometry and the like.

The resulting models should exhibit some topological structure: For volume computations, it must at least be possible to distinguish interior and exterior. Therefore, point clouds alone are insufficient in this regard.

Given the complexity of everyday objects - and that they frequently depart from the cubic, spherical, or cylindrical - we target an improvement of accuracy over back-of-the-envelope estimates.

A rich spectrum of object classes is covered in terms of admissible geometry and reflectance properties. Specular surfaces, e.g., would need to be coated with powder to make them amenable to laser scanning. But this would contravene the first criterion and thus eliminates laser scanning from the list of candidates.

In light of these requirements, we opt for triangulation based on structured-light encoding as the primary sensing modality. The principle behind this method has been known for decades but has seen a renaissance in computer vision ever since Primesense introduced a fully functional color-range ("RGBD") sensor unit in integrated-circuit design. This system-on-chip later became the core component of Microsoft's Kinect, which subsequently had an considerable impact in geometry reconstruction, tracking, occlusion detection, and action recognition, among other applications.

Contribution and overview

We develop a system for structured-light scanning of small- to medium-scale objects, which we dub Yet Another Scanner, or YAS. Naturally, the question arises why we would need yet another scanner when several systems and commercial products are already available, e.g., Kinect Fusion [\cite=Newcombe2011], ReconstructMe, Artec Studio, KScan3D, Scanect, Scenect, and Fablitec's 3d scanner; in particular, when most of these generate visually highly-pleasing results.

The main reason is that, albeit visually pleasing, the reconstructions provided by these methods are subject to biases that make them unsuitable for scientific investigation. The analysis, which is presented in Sect. [\ref=subsec:results], compares the performance of YAS with that of two competing state-of-the-art implementations. While the reconstruction algorithm described in Sects. [\ref=subsec:alignment] and [\ref=subsec:poisson] itself is not novel, we carefully justify all choices to be made in its design w.r.t. above-listed requirements. Additionally, we address the issue that an aligned series of range images suffers from incompleteness precisely where the ground plane supports the object. Sect. [\ref=subsec:volumeestimation] proposes a strategy to circumvent this problem in volume estimation which avoids complicated hole-filling algorithms. We believe that a tool which outperforms commercial software but is accessible for further scientific development may be of interest to the computer vision community as well. Hence, as the final contribution, we will distribute the source through the repository at .

System description

Data acquisition and calibration

In all our experiments studies, we used Microsoft's Kinect and Primesense's Carmine 1.09. Both devices are shipped with a factory calibration of depth and RGB camera intrinsics as well as the coordinate transformation between their local reference frames. Initial visual assessment (by the naked human eye) approves of the default calibration simply because the point clouds computed from the range image seem to be accurately colored by the values of the RGB image. Extensive tests, however, have shown that - in the spirit of our introductory remarks - such an evaluation is misleading, and significant metric improvements through manual re-calibration are possible. For this purpose, we rely on the toolbox accompanying the paper [\cite=DanielHerreraC2012] to estimate all aforementioned parameters plus a depth uncertainty pattern, which varies both spatially and with depth itself.

View alignment

A calibrated sensor immediately delivers physically plausible depth data. The integration of measurements from different vantage points into a common 3-d model can thus be seen as the core challenge here. A comprehensive overview of the state of the art in scan alignment is found in the recent survey [\cite=Tam2013]. Essentially, one can distinguish between two approaches: tracking and wide-baseline matching. The former is at the heart of Kinect Fusion [\cite=Newcombe2011] and the majority of commercially available software. Its main motivation stems from the fact that correspondence is easier to establish when two images haven been acquired closely in time - supposing, of course, that the motion the camera has undergone between each image acquisition and the next meets certain continuity constraints. We believe, however, that for the purpose of small-scale object reconstruction, the disadvantages of tracking predominate. First and foremost, there is the question of redundancy: How do we deal with the stream of depth data when operating an RGBD camera at frame rates up to [formula]? On the one hand, redundancy is desirable because single depth images may not cover the entire surface of the unknown object, e.g., due to occlusions or radiometric disturbances of the projected infrared pattern. On the other hand, integration of overcomplete range data into a common 3-d model puts high demands on the quality of alignment. Most feature trackers operate on a reduced motion model and are thus prone to drift. Such deviations in combination with the uncertainty in the raw depth data can lead to a stratification of points in regions appearing in more than a single image. This effect is illustrated in Fig. [\ref=fig:misalign1], which becomes more severe with higher numbers of processed images.

Kinect Fusion [\cite=Newcombe2011] deals with redundancy by instantaneously merging the depth stream into an implicit surface representation over a probabilistic voxel grid. The extra dimension, however, raises memory consumption - even in implementations utilizing truncation or an efficient data structure such as an octree. Also, without a-priori knowledge of the specimen's size, it is difficult to gauge the interplay between the spatial resolutions of 3-d grid and raw depth data, the latter being left exploited only sub-optimally. Last but not least, a system based on tracking is little user-friendly: It requires the operator to move the sensor as steadily as possible. Otherwise, temporal under-sampling or motion blur can lead to a total breakdown of the alignment process.

Here, we closely follow the wide-baseline matching procedure developed by the robotics community, notably in the work of Henry et al. [\cite=Henry2012]. It follows two quasi canonical steps: First, a set of local descriptors around interest points in each RGBD image is computed as well as a set of tentative matches between them. Such descriptors incorporate radiance and depth information either exclusively or in combination. Second, subsets of cardinality three are selected from all matches at random. Each subset admits a hypothesis about the rigid motion that transforms one of the point clouds into the other. The winning hypothesis is taken to be the one supporting the most matches, i.e., generating the highest number of inliers [\cite=Fischler1981]. We review these initial two steps in the following sections.

Sampling

Let us formally consider the case of two views l = 0,1, i.e., we look for two rigid motions [formula], [formula], with [formula] and [formula]. Without loss of generality, one can assume that g0 coincides with the world reference frame, i.e., [formula] and [formula], which leads to a simpler notation of the unknowns [formula] and [formula].

A number of interest points [formula] with high response is extracted from each of the two RGB images corresponding to g0,1 by means of the SIFT detector. The points are combined into a set of putative correspondences [formula] by thresholded forward-backward comparison of the distances between associated SIFT descriptors [\cite=Lowe1999]. The search for nearest neighbors can be sped up by a locality-sensitive hashing technique or similar. However, we found in all of our experiments that the time consumed by a brute-force search was within acceptable limits. Next, we repeatedly draw a sample of three matches from C and obtain a set of triples H  =  {(k1,k2,k3)  |  1  â‰¤  k1,k2,k3  â‰¤  n,k1  â‰   k2  â‰   k3}.

Consensus

Implicitly, each of the elements of H determines a hypothesis about the transformation we are looking for: Suppose we already know [formula], then the geometric least-squares error for some (k1,k2,k3)âˆˆH is given by

[formula]

Here, the six points [formula] equal the backprojections of the three matches [formula] forming the current hypothesis. Given the intrinsic camera parameters, they can be easily computed from the data delivered by the calibrated depth sensor. Conversely, given a triple (k1,k2,k3)âˆˆH, we can find a global minimizer g*1 of the convex function [\eqref=eq:lse] in the following way: Denote by 0 the mean of [formula] over k and define [formula]. The quantities 1 and jk1 are defined analogously. A minimizer in the entire general linear group of matrices [formula] is

[formula]

cf. [\cite=Williams2001]. One needs to make sure that the optimal g*1 involves a genuine rotation matrix by projecting [formula] onto [formula]. This is commonly achieved by Procrustes analysis, essentially consisting of a singular-value decomposition: Write [formula] as the product [formula] with two orthogonal factors [formula], then [formula]. Once [formula] is known, the optimal translation vector can be computed as [formula]. The transformation for the element of H attaining the highest consensus among all matches in C constitutes the solution to the global alignment problem [\cite=Fischler1981]. The result is refined based on the inlier correspondences with the iterative closest-point (ICP) method [\cite=Besl1992]. This also ensures a geometrically continuous alignment, which is not guaranteed because H was generated merely based on photometry.

Surface reconstruction

The common point cloud obtained after merging all aligned depth maps carries no information about the topological relationship between its elements, but as we will see shortly, such information plays a crucial part in volume estimation. There exists a wealth of algorithms for point-to-surface conversion, most of which depend on the signed or unsigned Euclidean distance field [formula] induced by the point cloud. Kinect Fusion, e.g., computes Ï† directly. Alternatively, when the point cloud is oriented, i.e., each point [formula] is endowed with an estimate of the normal vector [formula] the surface should have at that location, one can search for the function Ï† minimizing

[formula]

Here, with slight abuse of notation, [formula] refers to an (arbitrary) continuation of the normal field from the point set to some sufficiently large rectangular domain [formula]. Since the gradient of any scalar function is orthogonal to its level sets, this gives a family of integral surfaces

[formula]

of [formula]. A minimizer of [\eqref=eq:poisson] is found as the solution of the Euler-Lagrange equation

[formula]

under natural boundary conditions (here of Neumann type). Eq. [\eqref=eq:poisson] is the well-known Poisson equation and eponymous for the Poisson reconstruction algorithm proposed in [\cite=Kazhdan2006].

The motivation for increasing the order of differentiation as compared to the direct approach (i.e., that followed in Kinect Fusion) is twofold: First, Eq. [\eqref=eq:dirichlet] is a variant of the Dirichlet energy, which implies that small holes in the point cloud, i.e., areas where [formula], will automatically be in-painted harmonically. Second, for a solution to exist in the strong sense [formula], the normal field must be integrable or curl-free. Noise, which is very common in RGBD images, is responsible for most of the non-integrability in a measured normal field [formula]. In the variational setting [\eqref=eq:dirichlet], however, [formula] is implicitly replaced by the next-best gradient (its so-called Hodge projection, cf. [\cite=Cantarella2002]), which makes the approach very resilient to stochastic disturbances but at the same time destroys fine details. The smoothing effect can be mitigated by imposing Dirichlet conditions on [\eqref=eq:poisson] at a sparse set of salient points. This so-called screened Poisson reconstruction has recently been introduced in [\cite=Kazhdan2013].

The point cloud is easily oriented exploiting the known topological structure of the pixel lattice: Given a depth parametrization of the surface z(x,y) over the two orthogonal camera coordinate directions x and y, the normal can be written as [formula]. The partial derivatives of z w.r.t. camera and image coordinates [formula] respectively [formula] are related by the chain rule:

[formula]

Here, fu,fv are the focal lengths of the pinhole depth camera, and finite-differencing provides an approximation to the gradient of z(u,v). For the details of numerically solving [\eqref=eq:poisson] and selecting the constant C in [\eqref=eq:isosurface] appropriately, we refer the reader to the original paper [\cite=Kazhdan2006].

Volume estimation

Closed surfaces

Suppose for the moment that Î“ given by [\eqref=eq:isosurface] is compact and closed. The volume V of the domain [formula] it encompasses is defined as the integral of the characteristic function Ï‡Î© of Î©:

[formula]

Unfortunately, an evaluation of this integral is not very practical for two reasons. First, doing so would require a regular grid over Î©, which introduces undesirable artefacts where it interacts with a discrete version of Î“. Second, an expensive nearest-neighbor problem would need to be solved to determine whether a point is inside or outside of Î©. The following trick is based on the classic Gauss divergence theorem, cf. [\cite=Mirtich1996], which relates the flow of any continuously differentiable vector field [formula] through the boundary Î“  =  âˆ‚Î© of Î© with its source density or divergence in the interior:

[formula]

The left-hand side of this equation does not quite resemble the right-hand side of [\eqref=eq:volume], yet. However, this can be achieved by a clever choice of [formula], e.g., [formula], but note that several variants will work equally well and that [formula] is not unitary. We will return to this point later in Sect. [\ref=subsubsec:holefilling]. We have [formula] so that combining [\eqref=eq:volume] and [\eqref=eq:gauss] yields

[formula]

Let us look at the discrete case: Here, a level set Î“h of [\eqref=eq:isosurface] is extracted by the marching cubes in the form of a triangular mesh [\cite=Lorensen1987]. Such a piece-wise linear representation of the geometry provides a likewise locally-linear approximation of any function f whose values fi are known at the vertices [formula], [formula]. A Gauss-Legendre quadrature rule for f with linear precision defined over the triangle T is

[formula]

where A(T) equals the area of T, and I(T) enumerates its three corner vertices. Now substitute fi by the flow [formula]. The vertex normals [formula] are usually taken to be the normalized mean of the face normals in a one-ring neighborhood of [formula]. Altogether, we finally obtain the following approximation of Eq. [\eqref=eq:volintegral]

[formula]

Surfaces with boundary

As explained in Sect. [\ref=subsec:poisson], Poisson reconstruction accounts for most smaller holes in the aligned point clouds. The support of the object, i.e., its "bottom" or the area where it is in contact with the ground plane, however, remains usually unfilled. At the beginning of Sect. [\ref=subsubsec:volcont], we demanded that Î“ be compact and closed because only then the volume of Î© is well-defined. We can lift this assumption in parts simply by a coordinate transformation: Remember that we chose [formula] to point in the direction of the x-axis of the world coordinate system. Consequently, the flow through any of the planes [formula] or [formula] vanishes. As shown in Fig. [\ref=fig:vol], all we have to do is align the support of the model with one of these planes. Without loss of generality, we choose [formula]. In our scanning scenario, it is reasonable to assume that the specimens to be measured are spatially isolated enough that the depth images capture a significant portion of the ground plane surrounding the object, which can thus be detected fully automatically. To this end, we again invoke a RANSAC-type procedure, which samples triplets of points, calculates their common plane as a putative solution, and evaluates each such hypothetical plane by how many other points in the cloud it contains.

Experiments

Implementation

We created a C++ implementation of most of the reconstruction pipeline, including raw data acquisition, coarse and fine registration as well as detection of the ground plane. Ease of use is of premier priority in view of the interdisciplinary nature of this project. Therefore, the number of dependencies was kept as small as possible: The OpenCV library supplies us with all functionality for feature matching (Sect. [\ref=subsubsec:sampling]). Our implementation of the ICP method requires fast nearest-neighbor lookup which is based on the kd-tree data structure from the ANN library. We also created a graphical QT frontend which is showcased in the video included in the supplemental material. Poisson reconstruction (Sect. [\ref=subsec:poisson]) is currently done in Meshlab but will be integrated into our code in upcoming releases.

We compare our method to the Kinect Fusion algorithm and Scenect, which is one of the few commercial software packages without hindering functionality restraints in the trial version. To warrant a fair comparison, all participating systems should be operated with the same sensor and the same intrinsic calibration. This proved to be somewhat difficult: Both Scenect and YAS access devices through the OpenNI framework driver supporting all of Primesense's products and the Xtion by Asus among others. The Point Cloud Library provides an open-source version of the Kinect Fusion algorithm which could potentially function with the Carmine 1.09 as well, but our experiences with it were little encouraging. For this reason, we had to resort to Microsoft's own implementation Kinect Fusion Explorer, which works exclusively with proprietary hardware, the Kinect.

Results

The set of 20 specimens can be divided into two equally-sized groups: The first group contains objects of simple geometry like cubes and cylinders, whose basic dimensions can be measured manually with a tape measure or ruler, see Fig. [\ref=fig:cubes]. Free-forms of sizes ranging from just a few centimeters (fifth row of Fig. [\ref=fig:examplerec]) to the size of a human upper body (first row of Fig. [\ref=fig:examplerec]) make up the second group. The "ground truth" volumes for the first group are listed in the first column of Tab. [\ref=tab:volumes]. Needless to say these are afflicted by their own uncertainty, given that they were determined through measurements with a ruler. Our reconstructions of the cube data set are depicted in Fig. [\ref=fig:cubes]. We obtain the best average relative volume error [formula] of - 0.34 %. The performance of Scenect is comparable, which is somewhat surprising in view of Fig. [\ref=fig:scenect]. The meshes created by the Kinect Fusion explorer are of inferior topological quality: they contain a high number of non-manifold simplices. This, however, does not seem to affect the volume estimates negatively. Also it can be said that the sensitivity of volumes w.r.t. the ground plane parameters is relatively low.

As can be seen from the last two columns of Tab. [\ref=tab:volumes], the Kinect Fusion explorer systematically overestimates the ground truth volume by a significant margin. We conjecture that the issue is rooted in calibration. In fact, an important lesson learned during our experimental studies was that a good calibration can make a difference in error of an order of magnitude. Indeed, Scenect provides a calibration program, but Kinect Fusion Explorer does not. A visualization of Tab. [\ref=tab:volumes] is plotted in Fig. [\ref=fig:plots].

Results for the second group of objects are shown in Fig. [\ref=fig:examplerec]. Scenect performs worst among the three compared methods. It must be said, though, that Scenect does not offer mesh reconstruction feature, and the point clouds it exports are not oriented. Normals can be computed by singular value decomposition considering the nearest neighbors of a point, which is probably less accurate than the finite-differences approximations of [\eqref=eq:normal]. The lack of loop-closure whose effect can be better identified in the point cloud in Fig. [\ref=fig:misalign2] carries over to the triangular mesh in row of Fig. [\ref=fig:scenect]. Premature termination of the tracker is responsible for the poor reconstruction of the penguin in the last row of Fig. [\ref=fig:scenect].

Although it behaved unreliably during volume estimation, Kinect Fusion produced high quality models in real-time, which justifies the tremendous success it had since its inception. Still, the scale bias shows, and a lot of details appear to be missing - details which are present in our YAS reconstructions despite the fact that the Poisson algorithm is known to possess the characteristics of a low-pass filter (see the discussion in Sect. [\ref=subsec:poisson]). The precision setting in Kinect Fusion is quite rigid since the dimensions of the Cartesian grid have to be fixed before the reconstruction process even starts. We found that the maximal resolution of 512  Ã—  512  Ã—  512 voxels rendered the tracking unstable and/or led to incomplete meshes.

Discussion

We establish correspondence based on photometry, hence our approach fails in the absence of sufficiently exciting texture. This however does not necessarily need to be on the target object itself but can be found in the background, which may be "enriched" since after all, in our application, it is not of interest. The majority of competing approaches, including the two we compare against, can cope with the issue. The main reason is the trade-off between a sparse sampling of viewpoints and aforementioned need for sufficient texture: All previous method implicitly leverage on geometry in the motion estimation stage, which is only made possible by the small baseline between adjacent frames, i.e., through tracking, the disadvantages of which have been discussed in Sect. [\ref=subsec:alignment]. In fact, ICP measures the similarity between two points by their distance, hence endows each of them with a geometry descriptor, although a primitive one, too primitive to support wide-baseline matching. More suitable descriptors are available, but we are not yet considering them here, for already the process of salient point detection let alone the computation of informative geometry descriptors is extremely challenging on noisy, occlusion-ridden, and incomplete depth data such as from RGBD sensors.

The system has no real-time capabilities. We believe this is not necessary for our target application of small-scale object scanning (unlike e.g. navigation, map-building, or odometry). Considering the low resolution of RGB images, the matching process is a matter of seconds. This bottleneck could be removed by an approximate nearest-neighbors search. Our impression is that our system requires the same or even less overall acquisition time compared to e.g. Kinect Fusion, which requires slow and steady motion around the object (and sometimes even a complete reset).

Conclusion

We described a system for integrating a set of RGBD images of small-scale objects into a geometrically faithful 3-d reconstruction. The system is intended to support researchers in the field of cognitive neuroscience who will use it for acquiring ground truth data for their own experimental studies. To assess the suitability of our 3-d models and those obtained by comparable algorithms, we performed an in-depth analysis of theoretical and empirical kind. There are two main conclusions we would like to draw here: First, the quality of a set of calibration parameters or metric reconstruction can be deceptive. Only quantitative analysis enables veridical information. Second, while the uncertainties of hand-held structured-light scanners may be tremendous in the eyes of the optical metrologist, they help improving studies in the cognitive neurosciences, where manual measurement is still common practice.