Lemma Proof

A Blockwise Descent Algorithm for Group-penalized Multiresponse and Multinomial Regression

Introduction

Consider the usual linear regressions framework with y an n-vector of responses and X, an n by p matrix of covariates. Traditionally, problems involve n < p and it is standard to estimate a regression line with least squares. In many recent applications (genomics and advertising, among others) we have p  ≫  n, and standard regression fails. In these cases, one is often interested in a solution involving only few covariates. Toward this end, [\citet=tibs1996] proposed the Lasso: to find our regression coefficients by solving the regularized least squares problem

[formula]

This method regularizes β by trading off "goodness of fit" for a reduction in "wildness of coefficients" -- it also has the effect of giving a solution with few nonzero entries in β. This was generalized by [\citet=YL2007] to deal with grouped covariates; they propose to solve

[formula]

where the covariates are partitioned into disjoint groups and I(k) denotes the indices of the kth group of covariates (i.e., βI(k) indicates the sub-vectors of β corresponding to group k). This approach gives a solution with few non-zero groups.

Now, instead of usual linear regression, one might be interested in multiresponse regression -- instead of an n-vector, Y is an n  ×  M matrix, and β is a p  ×  M matrix. In some cases one might believe that our response variables are related, in particular that they have roughly the same set of important explanatory variables, a subset of all predictor variables measured (i.e., in each row of β either all of the elements are zero or all are non-zero.). A number of authors have similar suggestions for this problem ([\citealp=obozinski2007], [\citealp=argyriou2007], among others) which build on the group-lasso idea of [\citet=YL2007]; to use a group-penalty on rows of β:

[formula]

where βk  · refers to the kth row of β (likewise, we will, in the future, use β·  m to denote the mth column of β). For the remainder of this paper we will refer to Equation [\ref=eqn:mrlasso] as the "multiresponse lasso".

Multinomial regression (via a generalized linear model) is also initimately related to multiresponse regression. In particular, common methods for finding the MLE in non-penalized multinomial regression (eg Newton-Raphson) reduce the problem to that of solving a series of weighted multiresponse regression problems. In multinomial regression, β is again an n  ×  M matrix, where M is now the number of classes -- the (k,m) entry gives the contribution of variable xk to class m. More specifically, the probability of an observation with covariate vector x belonging to a class l is parametrized as

[formula]

As in linear regression, one might want to fit a penalized version of this model. Standard practice has been to fit

[formula]

where [formula] is the multinomial log-likelihood. While this does give sparsity in β it may give a very different set of non-zero coefficients in each class. We instead discuss using

[formula]

which we will refer to as the group-penalized multinomial lasso. Because each term in the penalty sum is non-differentiable only when all elements of the vector βk  · are 0, this formulation has the advantage of giving the same nonzero coefficients in each class. It was recently proposed by [\citet=vincent2012]. When the true underlying model has the same (or similar) non-zero coefficient structure across classes, this model can improve prediction accuracy. Furthermore, this approach can lead to more interpretable model estimates.

One downside of this approach is that minimization of the criterion in Equation [\ref=eq:groupedMultinom] requires new tools. The criterion is convex so for small to moderate sized problems interior point methods can be used -- however, in many applications there will be many features (p  >  10,000) and possibly a large number of classes and/or observations. For the usual lasso, there are coordinate descent and first order algorithms that can scale to much larger problem sizes. These algorithms must be adjusted for the grouped multinomial lasso problem.

In this paper we discuss an efficient block coordinate descent algorithm to fit the group-penalized multiresponse lasso, and group-penalized multinomial lasso. The algorithm in this paper is the multiresponse analog to [\citet=FHT2010]. In particular, we have incorporated this algorithm into glmnet [\citep=glmnet] a widely used R package for solving penalized regression problems.

Penalized multiresponse regression

We first consider our Gaussian objective, Equation [\ref=eqn:mrlasso]:

[formula]

This can be minimized by blockwise coordinate descent (one row of β at a time). Consider a single βk  · with fixed βj  · for all j  ≠  k. Our objective becomes

[formula]

where X·  k refers to the kth column of X, and [formula] is the partial residual. If we take a subderivative with respect βk  ·, then we get that k  · satisfies

[formula]

where S(k  ·) is its sub-differential

[formula]

From here, simple algebra gives us that

[formula]

where (a)+  =   max (0,a). By cyclically applying these updates we can minimize our objective. The convergence guarantees of this style of algorithm are discussed in [\citet=tseng2001]. Gaussian Algorithm Now we have the following very simple algorithm. We can include all the other bells and whistles from glmnet as well

Initialize β  =  β0, R  =  Y  -  Xβ0.

Iterate until convergence: for [formula]

Update R- k by

[formula]

Update βk  · by

[formula]

Update R by

[formula]

Note that in step (2b) if [formula], the update is simply [formula].

If we would like to include intercept terms in the regression, we need only mean center the columns of X and Y before carrying out the algorithm (this is equivalent to a partial minimization with respect to the intercept term).

Extension to multinomial regression

We now extend this idea to the multinomial setting. Suppose we have M different classes. In this setting, Y is an n  ×  M matrix of zeros and ones -- the ith row has a single one corresponding to the class of observation i. In a multinomial generalized linear model one assumes that the probability of observation i coming from class m has the form

[formula]

with

[formula]

This is the symmetric parametrization. From here we get the multinomial log-likelihood

[formula]

which we can rewrite as

[formula]

since [formula] for each i. Thus our final minimization problem is

[formula]

Uniqueness

Before proceeding, we should note that, because we choose to use the symmetric version of the multinomial log-likelihood, without our penalty (or with λ = 0) the solution to this objective is never unique. If we add or subtract a constant to an entire row of β, the unpenalized objective is unchanged. To see this, consider replacing our estimate for the kth row k  · by [formula] (for some scalar δ). Then we have

[formula]

Now, as the unpenalized loss is entirely determined by the estimated probabilities and the outcomes, this result tells us that the row means do not affect the unpenalized loss. This is not the case for our penalized problem -- here, the row means are all 0.

For a given X matrix, y vector, and λ  >  0, let β* denote the solution to the minimization of ([\ref=eq:penalizedMultinomial]). Let μ* be the vector of row-means of β*. We have that [formula]

Let [formula] be the "row-mean centered" version of β*. Plugging these in to the penalized log-likelihood in Equation [\ref=eq:penalizedMultinomial], we see that the difference between two penalized log-likelihoods is

[formula]

Thus [formula], so [formula].

Multinomial optimization

This optimization problem is nastier than its Gaussian counterpart, as the coordinate updates no longer have a closed form solution. However, as in glmnet we can use an approximate Newton scheme and optimize our multinomial objective by repeatedly approximating with a quadratic and minimizing the corresponding gaussian problem.

We begin with [formula], some inital guess of β. From this estimate, we can find an estimate of our probabilities:

[formula]

Now, as in standard Newton-Raphson, we calculate the first and second derivatives of our log-likelihood (in η). We see that

[formula]

For the second derivatives, as usual we get "independence" between observations, i.e., if j  ≠  i, for any m, l

[formula]

We also have our usual Hessian within observation

[formula]

for m  ≠  l, and

[formula]

Let Hi denote the within observation Hessian. By combining ([\ref=eq:hess1]) and ([\ref=eq:hess2]) we see that

[formula]

and we can write out a second order Taylor series approximation to our log-likelihood (centered around some value [formula]) by

[formula]

Because we have independence of observations, the second order term has decoupled into the sum of simple quadratic forms. Unfortunately, because each of these Hi are different (and not even full rank), using this quadratic approximation instead of the original log-likelihood would still be difficult. We would like to find a simple matrix which dominates all of the - Hi. To this end, we show that [formula] where

[formula]

Let [formula] be an M-vector of probabilities and define

[formula]

We have that [formula]

Define D by

[formula]

We would like to show that D diagonally dominant and thus positive semi-definite. Toward this end, choose some m  ≤  M.

[formula]

Thus D is positive semi-definite, and so [formula]. Furthermore, [formula] so we also have

[formula]

Now if we consider [formula], then the preceeding lemma gives us that [formula], and since for all i ti  ≤  t, we have our majorization [formula].

If we replace our original Hessian with this majorizing approximation, with some algebraic manipulation we can reduce our problem to the gaussian framework. Furthermore, because our new approximation dominates the Hessian, we still enjoy nice convergence properties. Toward this end, we write a new majorizing quadratic approximation (by replacing each - Hi by tI)

[formula]

which, with simple algebra, becomes

[formula]

Completing the square, we see that minimizing this with the addition of our penalty is equivalent to

[formula]

Notice that we have reduced the multinomial problem to our gaussian framework. Our plan of attack is to repeatedly approximate our loss by this penalized quadratic (centered at our current estimate of β), and minimize this loss to update our estimate of β. Multinomial Algorithm Combining the outer loop steps with our gaussian algorithm we have the following simple algorithm. We can still include all the other bells and whistles from glmnet as well.

Initialize β  =  β0.

Iterate until convergence:

Update η by η  =  Xβ.

Update P by

[formula]

Set [formula] and set [formula].

Iterate until convergence: for [formula]

Update R- k by

[formula]

Update βk  · by

[formula]

Update R by

[formula]

Path solution

Generally we will be interested in models for more than one value of λ. As per usual in glmnet we compute solutions for a path of λ values. We begin with the smallest λ such that β  =  0, and end with λ near 0. By initializing our algorithm for a new λ value at the solution for the previous value, we increase the stability of our algorithm (especially in the presence of correlated features) and efficiently solve along the path. It is straightforward to see that our first λ value is

[formula]

where P0 is just a matrix of the sample proportions in each class. We generally do not solve all the way to the unregularized end of the path. When λ is near 0 the solution is very poorly statistically behaved and the algorithm takes a long time to converge -- any reasonable model selection criterion will choose a more restricted model. To that end, we choose [formula] (with ε  =  0.05 in our implementation) and compute solutions over a grid of m values with [formula] for [formula].

Strong rules

It has been demonstrated in [\citet=tibshirani2012strong] that using a prescreen can significantly cut down on the computation required for fitting lasso-like problems. Using a similar argument as in [\citet=tibshirani2012strong], at a given λ  =  λj we can screen out variables for which

[formula]

where [formula] for the gaussian case and [formula] for the multinomial case. Now, these rules are unfortunately not "safe" (they could possibly throw out features which should be in the fit, though in practice they essentially never do). Thus, at the end of our algorithm we must check the Karush-Kuhn Tucker optimality conditions for all the variables to certify that we have reached the optimum (and potentially add back in variables in violation). In practice, there are very rarely violations.

Elastic net

We have seen that in some cases performance of the lasso can be improved by the addition of an [formula] penalty. This is known as the elastic-net [\citep=ZH2005]. Suppose now we wanted to solve the elastic-net problem

[formula]

We can again solve one row of β at a time. The row-wise solution satisfies

[formula]

Again, simple algebra gives us that

[formula]

Thus, the algorithm to fit the elastic net for multiresponse regression is exactly as before with step (b) replaced by ([\ref=eq:enet]). We can similarly apply this to multinomial regression, and replace step (ii) of multinomial algorithm with our new update.

Timings

We timed our algorithm on simulated data, and compared it to the msgl package [\citep=msgl], which implements an alternative algorithm to solve this problem, described in [\citet=vincent2012]. We also compare to a similar algorithm and implementation in our package (glmnet) for the usual multinomial lasso regression without grouping. Both glmnet implementations are written in R with the heavy lifting done in Fortran. The msgl code interfaces from R, but all of optimization code is written in c++. All simulations were run on an Intel Xeon X5680, 3.33 ghz processor. Simulations were run with varying numbers of observations n, features p, and classes M for a path of 100 λ-values with ([formula]) averaged over 10 trials. Features were simulated as standard Gaussian random variables with equicorrelation ρ. In all simulations, we set the true β to have iid N(0,4 / M2) entries in its first 3 rows, and 0 for all other entries.

From Table [\ref=tab:timings] we can see that for the grouped problem, glmnet is an order of magnitude faster than msgl. Also, though compared to the ungrouped multinomial lasso we do take a little bit of a hit, our grouped algorithm can still solve very large problems quickly, solving gene-expression sized problems in under a minute.

Discussion

We have given an efficient group descent algorithm for fitting the group-penalized multiresponse and multinomial lasso models and empirically shown the efficiency of our algorithm. It has also been included in the current version (1.8-2) of the R package glmnet.