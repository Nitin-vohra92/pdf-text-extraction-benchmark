Maximum Probability and Relative Entropy Maximization. Bayesian Maximum Probability and Empirical Likelihood

Φ-problem, MAP, MNPL

The Φ-problem can be loosely stated as follows: there is a prior distribution over a non-parametric set Φ of data-sampling distributions and a sample from unknown data-sampling distribution. The objective is to select a data-sampling distribution from the set Φ, called model.

More formally: Let P be the set of all probability mass functions (pmf's) with finite support X. The set P is endowed with the usual topology. Let Φ  ⊆  P. Let [formula] be i.i.d. sample from pmf r∈P. The 'true' sampling distribution r need not be in Φ; in other words: the model Φ might be misspecified. A strictly positive prior π(  ·  ) is put over Φ. The objective in the Φ-problem is to select a sampling distribution q from Φ, when the information summarized by {X,Xn1,π(  ·  ),Φ} and nothing else is available.

Bayesian Maximum Probability method selects the Maximum A-Posteriori Probable (MAP) data-sampling distribution(s) [formula]; there the posterior probability [formula], and ln(q) is used to denote [formula]; log  is meant with the base e. Hence the standard abbreviation, MAP, for the method.

The Bayesian Sanov Theorem (BST), through its corollary - the Bayesian Law of Large Numbers (BLLN) - provides a strong case for MAP as the correct method for solving the Φ-problem. The theorems are Bayesian counterparts of the well-known Large Deviations (LD) theorems for empirical measures: the Sanov Theorem and the Conditional Law of Large Numbers (cf. [\cite=C] and Sect. 2). In order to state the theorems it is necessary to introduce the L-divergence L(q||p) of q∈P with respect to p∈P: [formula]. The L-projection [formula] of p on Q  ⊆  P is [formula]. The value of L-divergence at an L-projection of p on Q is denoted by L(Q||p).

Let Xn1 be i.i.d. r. Let Q  ⊂  Φ  ⊆  P; L(Q  ||  r)  <    ∞  . Then for n  →    ∞  , [formula]

The posterior probability πn(Q|Xn1) decays exponentially fast (a.s. r∞) with the decay rate L(Q  ||  r)  -  L(Φ  ||  r). For a proof see [\cite=GL]. To the best of our knowledge Ben-Tal, Brown and Smith [\cite=BBS] were the first to use an LD reasoning in the Bayesian nonparametric setting. Ganesh and O'Connell [\cite=GO] proved BST for the well-specified special case; i.e., r∈Φ, by means of formal LD.

Let Φ  ⊆  P be a convex, closed set. Let B(,ε), be a closed ε-ball defined by the total variation metric, centered at the L-projection [formula] of r on Φ. Then,  lim n  →    ∞πn(q∈B(,ε)  |  Xn1)  =  1,a.s. r∞.

The BLLN is an extension of Freedman's Bayesian nonparametric consistency theorem [\cite=F] to the case of misspecified model. It shows that the posterior probability concentrates (a.s. r∞) on the L-projection of the 'true' sampling distribution r on Φ. For a book-length treatment of Bayesian non-parametric consistency see [\cite=GR].

MAP satisfies the BLLN. To see this, note that by the Strong Law of Large Numbers (SLLN), conditions for supremum of the posterior probability asymptotically turn into conditions for supremum of the negative of L-divergence. This also permits to view the L-projections as asymptotic instances of MAP distributions [formula].

There is also another method which satisfies the BLLN: Maximum Non-parametric Likelihood (MNPL). This can be shown by the above mentioned recourse to the SLLN. MNPL selects [formula].

These two (up to trivial transformations) are the only methods for solving the Φ-problem, which comply with the BLLN; hence they are consistent in the well-specified as well as in the misspecified case. Selecting a sampling distribution by some other conceivable method would, in general, asymptotically select sampling distribution which is a posteriori zero-probable. In this sense, selection of, say, the posterior mean, or selection of [formula], are ruled out.

The Φ-problem becomes more interesting when turned into a parametric setting. To this end, let X be a random variable with pmf r(x;θ) parametrized by [formula]. Assume that a researcher is not willing to specify parametric family q(X;θ) of data-sampling distributions, but is only willing to specify some of its underlying features. These features, i.e., the model Φ, can be characterized by Estimating Equations (EE): [formula], where [formula], [formula]. In the EE theory parlance, u(  ·  ) are the estimating functions, number of which is in general different than the number K of parameters θ. The 'true' data sampling distribution r(x;θ) need not belong to Φ. A Bayesian puts positive prior π over Φ, which in turn induces prior π(θ) over Θ; cf. [\cite=FR]. By the BLLN, the posterior πn(  ·  |Xn1) concentrates on a weak neighborhood of the L-projection [formula] of r(x;θ) on Φ: This thus provides a probabilistic justification for using [formula] as an estimator of θ. Thanks to the convex duality, the estimator [formula] can be obtained also as [formula]. Since r is in practice not known, following [\cite=KS], one can estimate the convex dual objective function by [formula]. The resulting estimator is just the Empirical Likelihood (EL) estimator (cf. [\cite=QL], [\cite=O], [\cite=MJM]). It can be easily seen that EL satisfies the BLLN. The same is true for the Bayesian MAP estimator [formula] For further results and discussion see [\cite=GJa], [\cite=GJb].

Π-problem, MaxProb, REM

Unlike the Φ problem, the Π problem is not a statistical problem. In the Π problem, the sampling distribution q is known, and there is a set Π  ⊆  P, into which an unavailable empirical pmf, drawn from q, is assumed to belong. The objective is to select an empirical pmf (also known as type, cf. [\cite=C]) from the set Π. Thus, the Φ and Π problems are opposite to each other.

More formally: let X be a set of m elements. Type [formula], where ni is the number of occurrences of i-th element of X (i.e., outcome), [formula], in a sample of size n, drawn from sampling distribution q. The objective in the Π-problem is to select a type(s) νn from Π, when the information summarized by {X,q,n,Π} and nothing else is available.

Maximum Probability (MaxProb) method (cf. [\cite=B], [\cite=Vi], [\cite=GG]) selects the type n  =   arg  sup νn∈Ππ(νn;q) which can be generated by the sampling distribution q, with the highest probability. If the sampling is i.i.d., then [formula]. Niven [\cite=N1] expanded MaxProb into non-i.i.d. and combinatorial settings; see also [\cite=N2], [\cite=Vi], [\cite=GN].

The Sanov Theorem (ST) (cf. [\cite=S], [\cite=CST]), through its corollary - the Conditional Law of Large Numbers (CLLN) (cf. [\cite=Va], [\cite=CC], [\cite=CST]) - provides a probabilistic justification for application of MaxProb in the i.i.d. instance of the Π-problem. The ST identifies the exponential decay rate function as the I-divergence [formula], p,q∈P. The I-projection p̂ of q on Π  ⊆  P is [formula]. The value of the I-divergence at an I-projection of q on Π is denoted by I(Π||q).

Let Π be an open set; I(Π  ||  q) <   ∞  . Then, for n  →    ∞  , [formula]

The rate of the exponential convergence of the probability π(νn∈Π;q) towards zero is determined by the information divergence at (any of) the I-projection(s) of q on Π.

Let Π be a convex, closed set that does not contain q. Let B(p̂,ε) be a closed ε-ball defined by the total variation metric that is centered at the I-projection p̂ of q on Π. Then, lim n  →    ∞π(νn∈B(p̂,ε)  |  νn∈Π;q)  =  1.

Given that a type from Π was observed, it is asymptotically zero-probable that the type was different than the I-projection of the sampling distribution q on Π.

It is straightforward to see that MaxProb satisfies CLLN. Indeed, set of MaxProb types converges to set of I-projections, as n  →    ∞  ; cf. [\cite=GAI], [\cite=GG]. Relative Entropy Maximization method (REM/MaxEnt) which maximizes, with respect to p, the negative of I-divergence (a.k.a., relative entropy) thus can be viewed as asymptotic form of MaxProb method.

Still, it is possible to solve Π-problem by selecting the type(s) with the highest value of relative entropy; in other words, to view REM as a self-standing method for solving Π-problem, rather than as an asymptotic instance of MaxProb. Obviously, REM satisfies CLLN.

MaxProb and REM/MaxEnt are the only two methods which satisfy CLLN. Selection of the mean type, which was under the name ExpOc proposed in [\cite=GG], or selection of, say the type with the highest value of Tsallis entropy, would in general, violate CLLN.

The Π-problem originated in Statistical Physics, where Π is formed by mean energy constraint; see [\cite=E]. In [\cite=GGA] feasible set of types formed by interval observations was considered.

Estimating Equations can be used to expand the Π problem into parametric setting. This time, the EE define a feasible set Π into which an unobserved parametrized type νn(θ) is supposed to belong: [formula], where [formula], [formula]. The true data-sampling distribution r(x;θ) need not belong to Π. The parametric Π-problem is framed by the information {X,r,n,Π(θ),Θ}, and the objective is now to select parametric type νn(θ) from Π. CLLN implies (cf. [\cite=KSE]) that the parametric Π-problem should be (for n  →    ∞  ) solved by selecting

[formula]

Thanks to the convex duality, the estimator [formula] can equivalently be obtained as [formula]. The estimator is known as Maximum Maximum Entropy (MaxMaxEnt) estimator.

The parametric Π-problem can be made more realistic, by assuming that a sample of size N is available to a modeler. Kitamura and Stutzer [\cite=KS] suggested to use the sample to estimate the convex dual objective function by its sample analogue [formula]. The resulting method is known as Empirical Maximum Maximum Entropy (EMME) method, or Maximum Entropy Empirical Likelihood (cf. [\cite=ISJ], [\cite=KS], [\cite=MJM], [\cite=JM]).

Acknowledgements

Valuable discussions with George Judge and Robert Niven, and a feedback from Valérie Girardin are gratefully acknowledged. Supported by VEGA 1/3016/06 and APVV RPEU-0008-06 grants.