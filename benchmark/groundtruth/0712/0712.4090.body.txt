Does Probability become Fuzzy in Small Regions of Spacetime?

Is Hilbert Space discrete? A recent paper by Buniy et al. [\cite=Buniy1] with this title claims that a possible discretization of spacetime leads to a discretization of the state space of quantum mechanics. We briefly review their arguments as follows: given some unknown quantum state, say, a spin-[formula] particle (a qubit)

[formula]

the phases θ and φ can be determined by state tomography, using appropriate measuring apparatuses. But if spacetime is discrete, there is only a finite number of measuring apparatuses that can possibly be constructed. This means that measurements can only be done within some fixed accuracy which is determined by the coarseness of spacetime. In particular, interpreting θ and φ as angular variables, there are only finitely many possible rotations of a measuring apparatus, such that those variables cannot be measured with greater accuracy than about 1 / d, where d is the size of the apparatus.

For the details of the argumentation, we refer the reader to [\cite=Buniy1] and a related paper [\cite=Buniy2]. In this paper, we give a more elementary and rigorous derivation, which shows that this result has more general validity and is not restricted to quantum theory.

We start with a simple thought experiment. Suppose an observer is confined to a small region of space, say, an astronaut is captured inside a free-falling spaceship (or elevator, as is frequently used in thought experiments on the equivalence principle). The observer is completely isolated from the outside world, that is, we assume that the spaceship is a perfectly closed system.

Moreover, suppose that the astronaut is given some random experiment with two different outcomes ("success" and "failure") that he can in principle repeat an arbitrary number of times. The probability of success is given by a real number p*∈[0,1] that the astronaut does not know. The astronaut's task is to determine the unknown probability value p*. For example,

the astronaut might possess some coin which is loaded instead of fair, and he wants to know the probability of "heads" (classical probability), or

the astronaut carries a single spin-[formula] particle that is initialized by some black box in an unknown quantum state |ψ〉, and he wants to find out the probability of "spin up" (quantum state).

The only way for the observer to estimate p* is to repeat the random experiment a large number of times, remember the number k of "success" outcomes and the number m of total trials, and use k / m as an estimator of p*.

This way, the astronaut can determine p* in principle to arbitrary accuracy by choosing m large enough, that is, by repeating the random experiment often enough. But there is a small detail that may cause problems: every trial produces one bit of information, say, "0" for "failure" or "1" for "success". After m trials, there are m useless bits that have accumulated inside the spaceship, keeping a record of the sequence of measurement outcomes.

Those bits cannot simply be disposed to the environment, because the spaceship is by assumption completely isolated. Also, since time evolution is reversible (which is true for classical as well as for quantum mechanics), the bits cannot be directly erased, but they can only be converted into different forms of information inside the spaceship. For example, if the astronaut saves the measurement results on a computer hard disc and deletes the corresponding file after a million trials, global reversibility ensures that the million bits are simply transmitted to different degrees of freedom inside the spaceship, for example to the vibrational modes of the air's molecules. The same is true for the initialization of the random experiment.

As a consequence, m bits of information are accumulated inside the spaceship if the random experiment is performed m times. For simplicity of the argument, let's assume that the value of p* is not too far away from 1 / 2, such that the outcome bit string of length m cannot be significantly compressed. If for some reason the number of bits that the spaceship can hold is bounded (e.g. by the laws of physics), then the astronaut is forced to stop performing trials at a certain point -- namely, at the point when his spaceship "runs out of memory".

Note that this kind of argumentation is similar to the resolution of the thought experiment of Maxwell's demon [\cite=Maruyama], where the fact that the demon runs out of memory and has to erase information (which is costly) preserves the second law of thermodynamics.

In fact, Bekenstein [\cite=Bekenstein] has derived an upper bound on the number of bits I that can be stored inside a bounded spatial region of effective radius R and energy E, based on calculations in quantum field theory and black hole entropy. It is given by

[formula]

This means that the actual laws of physics, as we know them, indeed force the astronaut to stop performing the random experiment after a large, but finite number of trials. Hence the accuracy to determine p* by measurements is bounded. We will now compute the resulting unavoidable uncertainty by using elementary probability theory.

To do this, we treat p* as a random variable, assuming uniform prior distribution on the interval

[formula]