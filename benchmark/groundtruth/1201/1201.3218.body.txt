Convex Optimization methods for computing the Lyapunov Exponent of matrices

The first author is supported by the RFBR grants No 10-01-00293 and No 11-01-00329, and by the grant of Dynasty foundation. This research was carried out while the first author was visiting Center of Operation Research and Econometrics (CORE) and Université Catholique de Louvain (Louvain-la-Neuve, Belgium) in Spring, 2011. That author is grateful to the institute and to the university for their hospitality. The second author is an F.R.S.-FNRS fellow.

R. M. Jungers

Introduction

Let us consider a family [formula] of linear operators acting in [formula]. To each operator Aj we associate a positive number pj so that [formula]. In the sequel we assume that every family of operators is equipped with a family of numbers. Consider a random product [formula], where all indices {dj} are independent and identically distributed random variables; each dj takes values [formula] with probabilities [formula] respectively. According to the Furstenberg-Kesten theorem [\cite=FK] the value [formula] converges with probability 1 to a number ρ, which depends only on the family A, i.e., on the operators {Aj}mj = 1 and on the probabilities {pj}mj = 1. The number λ  =   log ρ is called the largest Lyapunov exponent of the family A. In this paper we do not deal with other Lyapunov exponents, and, for the sake of simplicity, we omit the word "largest". This number can be defined by the following limit formula

[formula]

where [formula] denotes the mathematical expectation. The results of this paper can be extended to more general matrix distributions, but we restrict ourselves to i.i.d. matrices taking values on a finite set A.

We introduce a new approach for computing the Lyapunov exponent based on using special positive homogeneous functionals on [formula]. The idea is the following: for any such a functional f the minimal and maximal expected value of [formula] over all [formula], give a lower and upper bound respectively for λ. For families of nonnegative matrices those bounds can be effectively computed and then optimized over certain families of functionals f, which leads to optimal bounds βk  ≤  λ  ≤  αk that, under some general assumptions, rapidly converge to λ as k  →    ∞  . For every k both αk and βk are found by solving unconstrained convex minimization problems. The rate of convergence is proved to be at least linear in k, but in most of practical examples it is much faster. For dimensions d up to 50 it usually takes less than k  =  12 iterations to estimate λ with the relative precision 1%. All computations take a few minutes on a standard desktop computer.

In Section II we describe the new approach for operators with a common invariant cone, then in Section III we consider two special families of functionals and the corresponding bounds αk and βk. In Section IV it is shown that for nonnegative matrices both those bounds can be found and optimized over the corresponding families as solutions of certain convex minimization problems. In Theorems [\ref=th10] and [\ref=th20] we prove that under some general assumptions on matrices we have [formula], where C is an effective constant. In Section V this technique is extended to all matrices, without the nonnegativity condition. We derive an upper bound for λ, which is sharper than the classical upper bound [formula]. On the other hand, Theorem [\ref=th30] proved in that section shows that there are no good lower bounds for the Lyapunov exponent of general matrices. Finally, in Section VI we compute or estimate Lyapunov exponents of special families of matrices arising in problems of functional analysis, combinatorics, and language theory, and also report numerical results with randomly generated matrices.

Lyapunov exponents of matrices have been studied in the literature in great detail due to many applications in probability, ergodic theory, functional analysis, combinatorics, etc. (see [\cite=O] [\cite=W] [\cite=Per] [\cite=GM] [\cite=FBS] [\cite=P5] [\cite=JPB] and references therein). A special attention has been paid to the case of nonnegative matrices, i.e., matrices with nonnegative entries [\cite=I] [\cite=H] [\cite=P1]. The problem of computing or estimating the Lyapunov exponent is known to be extremely hard. It is even algorithmically undecidable in general [\cite=BT]. Nevertheless, there are several methods for approximate computation of the Lyapunov exponent that work well in most of practical cases. These are methods for special families of matrices arising in applications [\cite=FBS] [\cite=P5] [\cite=JPB], for general nonnegative matrices [\cite=Key] [\cite=GA], and for general families [\cite=DF1] [\cite=DF2] [\cite=BeynL]. The method proposed in this paper for nonnegative matrices has several advantages compared with those previously known: 1) it produces upper and lower bounds for the Lyapunov exponent λ that both converge to λ with a linear rate as the number of iterations grows. So, the Lyapunov exponent is sandwiched between two values. The rate of convergence is estimated theoretically, but in practice, as we see in numerical examples, it converges much faster. 2) The method works equally well for high dimensions. In examples with randomly generated matrices of dimension d  ≤  50 it computes λ with a relative error less than 1% within a few iterations. 3) We relax the assumptions on nonnegative matrices imposed in the previous papers on the subject.

The most popular upper bound used in the literature is

[formula]

where Ak is the set of all mk products of matrices of length k (with the corresponding probabilities). For any norm [formula] this bound converges to λ as k  →    ∞  . Usually one takes the Euclidean norm [formula]. As for the lower bounds for nonnegative matrices, the most well-known of them comes from the results of Key [\cite=Key], which uses the same formula, but with an arbitrary submultiplicative functional instead of the norm. We shall see that our bounds are closer to the real value of λ and have a guaranteed rate of convergence as k  →    ∞  . The theoretical reasons for that are the following: 1) In our bounds, we manage to interchange the Expectation- and Maximum-operations, which results in a smaller upper bound and a larger lower bound. 2) We do not restrict ourself to an a priori fixed functional (or norm), but rather we show how to optimize it over a large family of functionals.

In Section V we extend this approach to general matrices, without the nonnegativity assumption, and obtain an upper bound that is better than ([\ref=euc]). We also prove that such a lower bound for general matrices does not exist.

Operators with a common invariant cone: Bounds for the Lyapunov exponent

Assume that all operators [formula] share a common invariant cone [formula], which is supposed to be convex, closed, solid, pointed, and having its apex at the origin. For any points [formula] we write x  ≥  y   if   x  -  y∈K and x  -  y  >  0 if x  -  y∈int  K. For an operator A we write A  ≥  0 if it leaves the cone K invariant. The same notation are used for the dual cone [formula].

Consider a functional [formula]. In the sequel we impose the following assumptions on f:

1) f is positive, i.e., f(x)  >  0, whenever x  ≠  0;

2) f is homogeneous, i.e., f(tx)  =  tf(x) for any [formula].

Now we define two values Fmin and Fmax for any family A and for any functional f:

[formula]

We use the short notation Fmin(f,A)  =  Fmin, and the same with Fmax, if the functional f and the family A are fixed. Denote also [formula]. Thus,

[formula]

and similarly with F(k)max. Thus, F(k)min is the smallest expected value of the logarithm of the ratio [formula] over all x∈K,x  ≠  0. Let us first make the following simple observation:

For every natural k and n we have

[formula]

We have

[formula]

[formula]

[formula]

[formula]

which completes the proof for F(k + n)max. The proof for F(k + n)min is the same.

For an arbitrary functional f and for every k we have   Fmin    ≤      F(k)min   and   Fmax    ≥    F(k)max  .

For an arbitrary functional f and for any n we have  F(n)min    ≤    λ    ≤    F(n)max. In particular,  Fmin    ≤    λ    ≤    Fmax.

It suffices to prove that  Fmin    ≤    λ    ≤    Fmax. Then applying this inequality to the family An and taking into account that λ(An)  =  nλ(A) one obtains  F(n)min    ≤    λ    ≤    F(n)max.

By the compactness argument it follows that there are positive constants C1,C2 such that [formula]. Actually, these constants are respectively the minimum and the maximum of f(x) on the intersection of the unit sphere with the cone K. Applying Corollary [\ref=c10], we obtain for every x∈K,x  ≠  0

[formula]

[formula]

Since [formula] and [formula] as k  →    ∞  , we see that Fmin  ≤  λ. On the other hand, the same Corollary [\ref=c10] implies that

[formula]

for every x∈K such that f(x)  =  1. Furthermore, for each x∈int  K there is a positive constant C(x) such that for every operator B leaving the cone K invariant, we have [formula] (see, for instance, [\cite=P4]). Therefore, [formula], and hence

[formula]

Thus,   λ    ≤    Fmax.

By Fekete's lemma [\cite=F] for any sequence of nonnegative numbers [formula] such that [formula], the limit lim k  →    ∞ak exists and equals to [formula]. Similarly, if [formula], then [formula]. Applying Lemma [\ref=l5] we see that [formula] (denote this limit by F(  ∞  )min), and [formula] (denote this limit by F(  ∞  )max). Invoking now Lemma [\ref=l10] we obtain the following

For every family A and for every functional f we have

[formula]

Thus, to approximate the Lyapunov exponent one can take an arbitrary functional f and get the values Fmin and Fmax as a lower and upper bound respectively. Iterating, one obtains the bounds F(k)min and F(k)max, which, by Corollary [\ref=c10], are, at least, not worse. If the two inner inequalities in ([\ref=5]) become equalities, then F(k)min and F(k)max converge from different sides to λ, which allows us to compute λ with an arbitrary prescribed accuracy. Sufficient conditions for that are given in Theorem [\ref=th5] below. Finally, in the ideal case, when Fmin  =  Fmax, all the inequalities in ([\ref=5]) become equalities. In this case we get a sharp value of λ immediately, just by evaluating Fmin. Such "ideal" functionals f are called invariant.

A functional f is called invariant for a family A if

[formula]

Thus, f is invariant if and only if   Fmin  =  Fmax. In view of Lemma [\ref=l10] both these values equal to λ.

For any invariant functional f the constant in Definition [\ref=d10] is equal to the Lyapunov exponent λ of A.

Certainly, invariant functionals do not exist for all families that have invariant cones. For nonnegative matrices sufficient conditions were obtained in [\cite=P1], we shall cite that result in Theorem A (Section IV). However, even if an invariant functional exists, it may be very difficult to find or to approximate. Nevertheless, as the following theorem says, the very existence of an invariant functional guarantees that for an arbitrary functional f the values F(k)min and F(k)max both converge to λ with the linear rate.

For an arbitrary family A and for every functional f we have F(  ∞  )max    =    λ.

If, in addition, there is an invariant functional for this family, then for every functional f we have F(  ∞  )min    =    λ. In this case

[formula]

where the constant C depends only on A and on f.

There are positive constants C1,C2 such that [formula]. Taking the operator norm [formula] and using the fact that the mean of maxima is bigger than or equal to the maximum of means, we obtain for every [formula]

[formula]

[formula]

Taking limit as k  →    ∞  , we get λ  ≥  F(  ∞  )max. Comparing with ([\ref=5]) we see that λ  =  F(  ∞  )max.

Let [formula] be an invariant functional for A. By the compactness argument, for an arbitrary functional f on K there are positive constants C1,C2 such that C1(x)  ≤  f(x)  ≤  C2(x),  x∈K. Therefore,

[formula]

In the same way we show that [formula], and hence

[formula]

Taking limit as k  →    ∞  , we obtain   F(  ∞  )min    =    F(  ∞  )max, which completes the proof.

Thus, for every functional f we have F(k)max    →    λ. If the family A possesses an invariant functional on the cone K, then for every functional f the values F(k)min and F(k)max converge from two sides to the Lyapunov exponent λ, and the distance between them decays as C  k- 1. This provides a theoretical opportunity to compute the Lyapunov exponent with a given precision, using an arbitrary functional f. To realize this idea we need to compute the values F(k)max and F(k)min for large k. Each computation actually requires the resolution of an optimization problem, for which one needs to find a global optimum of the function [formula] on the cone K. Therefore, the functional f should be chosen in a special way, to obtain the objective function ψk(x) convenient for global minimizing/maximizing. In the next section we define two families of functionals f (each depending on one d-dimensional parameter), and then, in Section IV, we apply them for the cone [formula] (i.e., for the case of nonnegative matrices). Those functionals will allow us not only to evaluate the lower and upper bounds for λ, but also to optimize these bounds over all values of the parameters. This leads to a fast algorithm for computing the Lyapunov exponent λ of nonnegative matrices (Section IV). Even in relatively high dimensions (up to 60) that algorithm computes the Lyapunov exponent with a good precision (the relative error is less than 1%). The corresponding numerical examples from applications and some results with randomly generated matrices are given in Section VI. Then, making use of the semidefinite lifting technique, we partially extend our technique to general matrices, without the nonnegativity assumption. Applying a special functional f on the cone of positive semidefinite matrices, we obtain an upper bound for λ, which is, at least, not worse than the usual upper bound ([\ref=euc]) with the Euclidean norm (Section V). In practice it is much more efficient, which is confirmed by numerical examples in Section VI. As for effective lower bounds, it is shown in Section V that they actually do not exist for general matrices. This explains well-known negative theoretical results on the Lyapunov exponent computation [\cite=BT].

We have seen that for every functional f the value F(  ∞  )max actually coincides with the Lyapunov exponent. However, for F(  ∞  )min this is, in general, not the case, unless the family A possesses an invariant functional. The main problem, therefore, is the lower bound for the Lyapunov exponent. In Section V we shall see examples of matrix families that have no functionals f such that F(k)min  →  λ as   k  →    ∞  . That is why the existence of an invariant functional is crucial for deriving lower bounds that converge to the Lyapunov exponent.

Two special functionals [formula]

In this section we define two families of functionals f, which then will be applied to compute the Lyapunov exponent of nonnegative matrices.

Let A be an arbitrary finite family of matrices sharing an invariant cone K. For every point x  >  0 consider the functional f(  ·  ) = rx(  ·  ) defined on K as follows:

[formula]

Geometrically, this functional is a norm on K, whose unit ball is [formula]. If rx(y)  ≤  1  , then y  ≤  x, therefore Ay  ≤  Ax for any operator A  ≥  0, and hence rx(Ay)    ≤    rx(Ax). Consequently, for this functional we have

[formula]

Let us denote

[formula]

Similarly we define αk(x)  =  F(k)max   and   αk    =     inf   x    >    0   αk  (x). Applying now Lemma [\ref=l10], we conclude that αk(x)  ≥  λ for each x  >  0 and [formula], and therefore   αk    ≥    λ.

To obtain a lower estimate for λ we take arbitrary v∈int  K* and consider the linear functional f(x)  =  (v,x). Again, this functional is a norm on K, whose unit ball is the intersection of K with a half-space. For this functional we have

[formula]

Denote

[formula]

Similarly we define βk(v)  =  F(k)min   and   βk    =     inf   v    >    0   βk  . Lemma [\ref=l10] now implies that   βk(v)    ≤    λ   for every v∈intK *  and [formula].

Thus, we have the following bounds for the Lyapunov exponent of a matrix possessing an invariant cone:

[formula]

In general, they are not easy to compute. For instance, to evaluate β(v) we need to find the global minimum over x∈K of the function

[formula]

This function is not convex in x, it is actually quasiconcave, and hence its minimization may be hard. Nevertheless, we shall see in Section IV that in case [formula] the value βk(v) is not only computable, but can be efficiently optimized over all v∈int  K*, and the same is for αk(x). If we work with a general cone K, then it is more convenient to apply the linear functional f(x)  =  (v,x) to get not a lower bound (as βk(v)), but the upper one. Doing so, we write Fmax for the functional f(x)  =  (v,x) and obtain

[formula]

The objective function [formula] is concave, and hence its maximum on the convex set {x∈K | (v,x)  =  1} can be efficiently found. The same can be done for every k:

[formula]

The shortcoming of this estimate is that it is very hard to minimize over the set v∈int  K*, even in the case [formula]. Nevertheless, choosing appropriate v one can obtain good upper bounds γk(v) that converge fast to λ as k  →    ∞  . We use this bound in Section V for approximating Lyapunov exponents of general sets of matrices.

The Lyapunov exponent of nonnegative matrices

We are going to see that in case [formula], i.e., when all the operators Aj are written by nonnegative matrices, both estimates αk and βk are efficiently computable. We only show here how to evaluate α and β, since the computation of αk and βk is the same with replacing the family A of m matrices by the family Ak of all their mk products of length k.

We begin with α. Let us first note that [formula]. Therefore,

[formula]

Changing the variables, [formula], we get

[formula]

Interchanging log  and max  we write

[formula]

Observe that the function [formula] is convex in u. The maximum of convex functions is convex. Therefore, the value α is a solution of the following convex minimization problem:

[formula]

Let us now compute β. We have

[formula]

Thus, β(v) is the minimal value of the concave function [formula] on the simplex {x    ≥    0  ,  (v,x) = 1}. This minimal value is attained at an extreme point, i.e., at a vertex of the simplex. Since its vertices are the vectors of the canonical basis, we have

[formula]

Since   (v,ej)    =    vj   and   (v,Aej)    =    (v,aj), where aj is the jth column of the matrix A, we obtain

[formula]

For any j and for any [formula] the set of solutions v > 0 of the inequality

[formula]

coincides with the set of solutions of the inequality

[formula]

which is convex, because the function [formula] is concave in v. Hence, for any j the function [formula] is quasiconcave in v, and therefore β(v) is quasiconcave as well, as a minimum of quasiconcave functions. Thus, β is the solution of the following quasiconcave maximization problem:

[formula]

Let us remember that for each k the values αk and βk are defined by formulas ([\ref=alpha]) and ([\ref=beta]) multiplied by [formula] and with replacing A by Ak. Similarly for the values αk(x) and βk(v). Thus, for every vectors x,v  >  0 we have the following inequality:

[formula]

Two conditions for nonnegative matrices

The bounds αk(v) and βk(x) are derived for all families of nonnegative matrices, and inequality ([\ref=k]) always holds. The question, however, is do they provide really effective estimations for the Lyapunov exponent, i.e., do they converge to λ as k  →    ∞   ? It appears that the answer is affirmative, whenever the matrices Aj are not "too sparse". More precisely, the family A satisfies the following two assumptions:

(a) there is at least one strictly positive product of matrices from A (with repetitions permitted);

(b) matrices from A do not have zero rows nor zero columns.

Note that conditions (a) and (b) are assumed in most of papers studying random products of nonnegative matrices (see [\cite=W] [\cite=I] [\cite=H] [\cite=Key] [\cite=P1]). We shall see that condition (a) can always be omitted, but the situation with condition (b) is more complicated.

Let us recall that (Theorem [\ref=th5]), if there is an invariant functional for the family A, then for any other functional f the bounds F(k)min and F(k)max converge linearly to the Lyapunov exponent.

Theorem A [\cite=P1]. For every family of nonnegative matrices satisfying conditions (a) and (b) there exists an invariant functional on [formula]. This functional is, moreover, concave and monotone on the cone K.

Applying now Theorem [\ref=th5] to the functionals f(  ·  )  =  rx(  ·  ) and f(  ·  )  =  (v,  ·  ), we arrive at

For every family of nonnegative matrices satisfying (a) and (b), and for every vectors x,v  >  0 we have [formula] and

[formula]

where the constant C depends on A,x and v.

Since βk  ≥  βk(v) and αk  ≤  αk(x), we see that the estimates βk and αk tend to λ as well, and   αk    -    βk    ≤    C  k- 1. In general, there is no need to evaluate the optimal values in problems ([\ref=alpha]) and ([\ref=beta]) with a good precision. To approximate λ it suffices to find points x and v for which the difference αk(x)    -    βk(v) is small, say, less than ε, then the Lyapunov exponent λ is found with the precision ε. As we shall see in numerical examples, in practice the value   αk  -  βk   decays much faster than k- 1, and it is enough to take a reasonably small k (much smaller than 1 / ε) to compute the Lyapunov exponent λ with the precision ε.

Estimates αk and βk can be extended to any set of matrices sharing a polyhedral invariant cone K. If the cone K is spanned by vectors {hj}N1j = 1 and its dual cone K* is spanned by vectors {gi}N2i = 1, then writing formula for α(x) we replace [formula] by [formula], and writing formula for β(v) we replace [formula] by [formula]. Here it is important that both sets of vectors are finite, i.e., that the cone K is polyhedral. As we shall see in Theorem [\ref=th30], for families of matrices with a non-polyhedral invariant cone an effective lower bound for λ may not exist at all. In particular, βk is not such a bound any more.

Omitting condition (a)

The lower and upper bounds βk(v) and αk(x) respectively can be computed for every family of nonnegative matrices. However, to prove the convergence of these bounds to λ we essentially used conditions (a) and (b), because Theorem A may fail without them. Moreover, there are simple examples showing that if at least one of the conditions (a) or (b) is not fulfilled, then the difference αk  -  βk may not vanish as k  →    ∞  , in which case our bounds do not provide the Lyapunov exponent computation with a given prescribed accuracy. For every family A condition (b) can be, of course, checked immediately. Condition (a) looks more difficult to verify. Nevertheless is can be checked efficiently as well. The corresponding algorithm takes 2md3 arithmetic operations, where d is the dimension, and m is the number of matrices [\cite=PV]. Thus, if both conditions (a) and (b) are satisfied, then we apply Theorem [\ref=th10] to compute the Lyapunov exponent λ. Otherwise, if at least one of them fails, one can still use inequality ([\ref=k]), but now there is no guarantee that both parts converge to λ as k  →    ∞  . For some families this still gives good numerical estimates for λ (as for the binomial matrices from Subsection VI.1 below). However, there are examples, when αk and βk are too far from each other for all k, and these bounds become useless. A question arises: is it possible to obtain effective upper and lower bounds for the Lyapunov exponent (perhaps, different from αk and βk) without those two conditions? We do not know the answer for condition (b). Is it true that if nonnegative matrices are allowed to have zero rows and columns, then the Lyapunov exponent can be sandwiched between two efficiently computable bounds, whose difference tend to zero?

As for condition (a), the answer is affirmative. That condition can be omitted, with a special modification of the upper bound αk(x). In this subsection we extend our approach to this case, when matrices of the family A do not necessarily have a positive product. To begin with, we need the following key result proved in [\cite=PV]:

Theorem B [\cite=PV]. If a family of nonnegative matrices A satisfies condition (b), but do not have a positive product, then one of the two following cases takes place:

(1) A is reducible;

(2) there is a partition of the set [formula] to r  ≥  2 sets [formula], on which every    matrix from A acts as a permutation.

In the latter case there exists a product D of matrices from A that has a block-diagonal form: r strictly positive blocks corresponding to the sets [formula].

Property (1) means that there is a nontrivial subspace of [formula] spanned by several basis vectors, that is invariant for all matrices from A. Property (2) means that for every matrix A∈A there is a permutation σ of the set [formula] such that [formula], where [formula] is a cone spanned by the vectors {ei | i∈Ωk}. To compute the Lyapunov exponent for a family not satisfying condition (a) one needs to consider both cases of Theorem B.

Case 1. The family [formula] is reducible. In this case there is a permutation of basis vectors, after which all matrices from A take a block upper-triangular form. This permutation can be found by a combinatorial algorithm that takes O(d2) arithmetic operations (see, for instance, [\cite=J] and references therein). Now it remains to refer to the main result of the work [\cite=GMO]: for a family of block upper-triangular matrices the Lyapunov exponent equals to the largest Lyapunov exponent of the blocks. Hence, in case (1) the problem of computing the Lyapunov exponent is reduced to several analogous problems in smaller dimensions.

Case 2. There is a partition of the set [formula], on which every matrix from [formula] acts as a permutation. We first show that in this case, we can restrict our attention to the set [formula]. This is a union of faces Li of the positive orthant [formula], corresponding to the sets Ωi of the partition. Clearly, AjL  ⊂  L for each [formula]. Consider an arbitrary functional f on the set L, which is positive (f(x) > 0  ,  x∈L,x  ≠  0) and homogeneous (f(tx)  =  tf(x),  x∈L,  t  ≥  0). For this functional we define Fmin,Fmax,F(k)min,F(k)min in the same way as in Section II. Then we establish the following analogue of Lemma [\ref=l10]:

If a family A is irreducible, then for an arbitrary functional f on L and for each n we have  F(n)min    ≤    λ    ≤    F(n)max. In particular,  Fmin    ≤    λ    ≤    Fmax.

The proof is literally the same as the proof of Lemma [\ref=l10] with only one exception: in order to prove that Fmax  ≥  λ we need to show that there exists a vector x∈L, for which

[formula]

In the proof of Lemma [\ref=l10] we established the existence of such a point x in the cone K, now we need this point in the set L. To this end we apply the main result of the work [\cite=Hong]: if a family A is irreducible and its matrices have no zero columns and rows, then every nonzero vector [formula] satisfies ([\ref=need]). Thus, an arbitrary x∈L  ,  x  ≠  0 suffices. The remainder of the proof is the same as for Lemma [\ref=l10].

From [\cite=P3] it follows that for a family of matrices satisfying assumptions of the case (2) of Theorem B there exists an invariant functional [formula] on L, for which Fmin  =  Fmax  =  λ. Now, precisely as in the proof of Theorem [\ref=th5], we conclude that for every functional f on L one has

[formula]

where the constant C depends only on A and on f. To estimate the Lyapunov exponent it remains only to choose any convenient functional f in order to compute the values F(k)min and F(k)max.

For Fmin we again choose f(x)  =  (v,x) with arbitrary v  >  0. It appears that for this functional we again have the equality Fmin  =  β(v), where β(v) is defined by ([\ref=betapos]). This is not obvious, because now we take minimum not over the whole set [formula], but over a much narrower set L. We have

[formula]

Since the minimum of a concave function [formula] on the simplex {x  ∈  Ln  ,  (v,x) = 1} is attained at an extreme point, i.e., at a basis vector, we have

[formula]

For Fmax we again take the functional [formula], where x  >  0. For every [formula] we have [formula], where σ is our permutation. Whence,

[formula]

This value will be denoted by α̃(x). Interchanging max  and log  and writing j  =  σ(n), we obtain

[formula]

and

[formula]

Applying now ([\ref=k1]) and Lemma [\ref=l20] we obtain

For every family of nonnegative matrices possessing property (2) from Theorem [\ref=th20], and for every vectors x,v  >  0 we have

[formula]

and

[formula]

where the constant C depends on A,x and v.

Let us note that the values βk  =   sup v  >  0βk(v)   and α̃k  =   inf x  >  0α̃k(x)   can be efficiently found by using convex programming, because the function βk(v) is quasiconcave in v, and α̃(x) is convex in u, where [formula]. The first assertion is proved, the proof of the second one is the same as for the function αk(x). Thus, both bounds from Theorem [\ref=th20] can be optimized by parameters. In practice this significantly improves the rate of convergence to λ.

We see that for every family A of nonnegative matrices satisfying condition (b) there is an efficient method of computing the Lyapunov exponent. For families satisfying condition (a) the method is given by Theorem [\ref=th10], for a reducible family (the case (1) of Theorem B) the problem is equivalent to several problems of smaller dimensions, for a family of the case (2) of Theorem B the method is given by Theorem [\ref=th20]. The optimal parameters v and x in the bounds can be effectively found by convex programming. This covers all families of nonnegative matrices without zero rows and columns.

For every family A of nonnegative matrices that have neither zero columns nor zero rows, and for every vector v > 0 we have   λ    ≥    βk(v)   and [formula].

Is it possible to avoid condition (b) ?

According to Theorem [\ref=th5] for every family of nonnegative matrices, without any additional assumptions, the upper bound αk converges to the Lyapunov exponent λ as k  →    ∞  . For the lower bound βk this is also true, provided the family A is irreducible and condition (b) is fulfilled. Moreover, the irreducibility assumption is not essential, because if the family is reducible, then the problem of the Lyapunov exponent computation is equivalent to several problems of smaller dimension. In turn, condition (b) is, in general, unavoidable. Indeed, if one of the matrices have a zero column, then the lower bound becomes trivial: βk  =    -    ∞   for all k. In some cases (but not always!) this difficulty can be overcome by special tricks. Let us describe two of them:

1) Considering the transposed family. If all matrices of the family A have no zero columns (but may have zero rows), then the value βk is different from -    ∞   for each k, and may converge to λ as k  →    ∞  , although this convergence is not theoretically guaranteed. Hence, if the matrices of A have zero columns, then one can compute βk for the family [formula]. In many cases this leads to good lower bounds.

2) The modified estimate βk(v). In some cases one can consider the following modified value for a vector [formula]:

[formula]

where bj is the jth column of the matrix B. In contrast to the value βk(v), here the vector v may have zero components, and the minimum is taken over its nonzero components. It is shown easily that for every v this is a lower bound for λ. If we choose v so that it has zeros at all positions corresponding to zero columns of matrices from Ak, then βk(v) may converge to λ. We illustrate this trick in Section VI by a numerical example and apply it for estimating the Lyapunov exponent of large sparse matrices arising in one problem of the language theory.

Let us stress that for both these tricks there are corresponding counterexamples, when they do not work. In general, if the matrices have zero columns/rows, we do not know any satisfactory lower bound for λ. Finding such a bound can be considered as a challenging open problem.

The Lyapunov exponent of general matrices

In this section we present an efficient upper bound for the Lyapunov exponent of general matrices (not necessarily nonnegative), and argue that such a lower bound does not exist.

Let us first assume that all matrices [formula] share a common invariant cone K. The Lyapunov exponent λ is defined as the limit of the value [formula] as   k  →    ∞  . For every k this value is an upper bound for λ. More generally, for any positive homogeneous functional on K the value

[formula]

is an upper bound for λ, which by Theorem [\ref=th5] converges to λ as k  →    ∞  . This upper bound is usually applied in the literature to estimate the Lyapunov exponent. The most popular functional f here is the Euclidean norm. On the other hand, the value

[formula]

is, at least, not bigger (in most cases, actually, much smaller) than ([\ref=up1]), because the maximum of means does not exceed the mean of maxima. Therefore, the upper bound F(k)max is closer to λ than ([\ref=up1]). However, its computation may face serious difficulties, because it involves finding the maximal value of the function [formula] on the set   x∈K,f(x)   ≤   1. Basically, such a maximization problem can be efficiently solved only in the case, when the function ψk(x) is concave, or quasiconcave. Unfortunately, it does not possess this property for most of norms f(x), including the Euclidean norm. That is why we use the linear functional f(x)  =  (v,x), for which the function [formula] is concave, and can be effectively maximized. Its maximum gives the upper bound γk(v) defined in ([\ref=gamma1]) and in ([\ref=gamma2]). As we have already mentioned, the shortcoming of this upper bound is that the function γk(v) is not convex, and one is not able to efficiently minimize it over v∈K*. Nevertheless, it is still possible to pick an arbitrary v, hoping to obtain a good bound for λ.

To extend this approach to general matrices, without the common invariant cone assumption, we apply the so-called semidefinite lifting. Let us have a family [formula]. To each matrix Aj we associate an operator Ãj on the [formula]- dimensional space Md of symmetric d  ×  d-matrices defined as follows:

[formula]

We thus obtain the family [formula]. All operators Ãj now share a common invariant cone: the cone Kd of symmetric positive semidefinite d  ×  d-matrices. Let us recall that K*  =  K and that the scalar product in the space Md is defined as (X,Y)  =  tr(XY). This is shown easily that λ()    =    2  λ(A). Therefore, we can apply the upper bound γk(v) to the family [formula] and get the following upper bound for λ(A)

[formula]

where [formula] means that the matrix X is positive semidefinite. For every positive definite matrix V the value Γk(V) is an upper bound for λ(A), and it converges to it as k  →    ∞  . To compute Γk(V) one needs to find the maximum of a smooth concave function on the intersection of the cone Kd with a hyperplane {X∈Md | tr  (VX)  =  1}. This problem can be efficiently solved by standard tools of semidefinite programming (SDP, see, for instance, [\cite=VB]). In many cases the most natural choice is to take V = I (the identity matrix), which yields the following upper bound:

[formula]

Let us show that this upper bound is better that the standard upper bound with the Euclidean norm.

For every matrix family we have [formula].

We have

[formula]

The supremum of the linear function g(X)  =  tr  (BXB*) is attained at an extreme point of the set [formula], i.e., at a rank one matrix X  =  YY*, where [formula] is a vector. Since [formula] and [formula], we see that [formula], and whence the right hand side of inequality ([\ref=evkl]) equals to [formula].

In Section VI we present numerical results with randomly generated matrices showing that the bound Γk(I) can indeed be much closer to λ than the standard bound with the Euclidean norm.

Now let us explain why there is no good lower bound for general families of matrices, even if they are non-factorable (do not have nontrivial common invariant subspaces in [formula]) and share a common invariant cone (not a polyhedral cone, when we have the lower bound βk(v), see Remark [\ref=r10]). To avoid any trouble with the case λ  =    -    ∞  , we formulate the result for lower bounds of the Lyapunov radius   ρ    =    e  λ.

a) There is a non-factorable pair   =  {A1,A2} of   2  ×  2-matrices such that for every function φ on the set of all non-factorable pairs of 2  ×  2-matrices, which is continuous at the point [formula] and φ(A)  ≤  ρ(A) at any point A, we have   φ()    ≤    ρ()    -    1.

b) There is a non-factorable pair   =  {A1,A2} of   3  ×  3-matrices sharing an invariant cone K such that for every function φ on the set of all non-factorable pairs of 3  ×  3-matrices sharing the invariant cone K, which is continuous at the point [formula] and φ(A)  ≤  ρ(A) at any point A, we have   φ()    ≤    ρ()    -    1.

a) Consider a pair B  =  {B1,B2}, where B1 is a rotation of the plane by the angle [formula] about the origin, and B2 is the orthogonal projection onto the OX axis. It is easily shown that the Euclidean norm of every product of length k of matrices B1,B2 is at least 2- k. Hence, ρ(B)  ≥  1 / 2. Let now B1,n be a rotation of the plane by the angle [formula]. about the origin, and Bn  =  {B1,n,B2}. Clearly, Bn  →  B as n  →    ∞  . On the other hand, B2B3n + 11,nB2  =  0, and hence ρ(Bn)  =  0 for each [formula]. Therefore, if φ(Bn)  ≤  ρ(Bn)  =  0 for all n, then by continuity φ(B)  ≤  0. Thus, ρ(B)  -  φ(B)  ≥  1 / 2. Now it remains to take   =  {2B1,2B2}.

b) It suffices to take   =  {41,42}, where B1,B2 are the 2  ×  2-matrices from the proof of part (a), i is the semidefinite lifting of the matrix Bi. Matrices of this pair is three-dimensional, and they share a positive definite cone K3.

Thus, there are pairs of matrices, whose Lyapunov exponent cannot be well-approximated by a continuous lower bound. This means that there is no algorithm, whose output continuously depends on the data, which for an arbitrary pair of matrices computes the Lyapunov exponent with a given precision. It should also be mentioned that, as it was shown by Blondel and Tsitsiklis (1997), the problem of computing the Lyapunov exponent for matrices with integer entries is algorithmically undecidable [\cite=BT].

The statement (b) of Theorem [\ref=th30] does not hold for 2  ×  2-matrices, because every cone in [formula] is polyhedral (see Remark [\ref=r10]).

The phenomenon that the Lyapunov exponent has good upper bounds, but does not have lower ones is explained by the fact that this value is an upper semicontinuous function on the families of matrices, but not lower semicontinuous. In the proof we used a pair of matrices, where the Lyapunov exponent is not lower semicontinuous.

Numerical examples

In this section, we show the efficiency of our method on several examples. We first analyze matrices drawn from an application in combinatorics, for which the exact value is known. Then, in Subsection VI.2, we study an application in functional analysis. We then provide estimates for the Lyapunov exponent of some matrices arising in the language theory (Subsection VI.3). Finally, in Subsection VI.4, we analyze diverse kinds of randomly generated matrices. As mentioned above, all optimization problems that we need to solve for computing these estimates are convex unconstrained problems. We apply standard tools of convex optimization, namely, the matlab function fminunc, which uses a quasi-Newton procedure (the so-called BFGS-scheme), together with a cubic line search procedure. All computations took a few minutes on a standard desktop pc.

As a general observation, our upper bound generally converges faster than the Euclidean bound (that is, the bound obtained from ([\ref=euc]) with the Euclidean norm) towards the real value in the first steps of the algorithm. This allows us to have a fair upper estimate without having to compute too large products of matrices. On the other hand, for our lower bound, not only it also converges faster than the previously available lower estimates (see [\cite=Key]), but in addition, generally these other lower bounds do not allow to reach a satisfactory accuracy at all. Even though they converge asymptotically towards the exact value, they appear to be often relatively far from it for the largest values of k that a classical computer can handle (say, k  =  14 or so). We graphically present our results in terms of the Lyapunov radius   ρ    =     exp  (λ), because in most applications, it is the "physical" quantity that one wants to compute.

The binomial triangle and generalizations

Recently, the question of the density of ones in the nth row of Pascal's triangle has been studied in number theory. This question is equivalent to the number of odd coefficients in the polynomial (1 + x)n. In [\cite=FBS], the authors generalize the question to the study of odd coefficients for the polynomial [formula] They show that for any [formula] there exists a set of matrices Σm such that for large n, with probability one, the proportion of odd coefficients is given by the Lyapunov radius of Σm. As an example, we represent hereunder Σ6:

[formula]

Moreover, it is shown in [\cite=FBS] that for these particular sets of matrices, it is possible to compute the Lyapunov exponent exactly. For this reason, we start this section on numerical examples with this application, for which it is possible to refer to the exact solution. Figure [\ref=fig-finch] shows how our estimates evolve in comparison to the exact value, and to the upper bound obtained from ([\ref=euc]) with an arbitrary norm. We chose the Euclidean norm, as it most often performs best in practice. Observe that the family Σ6 does not satisfy condition (b) (the first matrix has three zero rows), hence the convergence of βk towards λ is not guaranteed theoretically. Nevertheless, both βk and αk converge rapidly towards the exact value of λ. Moreover, as one can see on figure [\ref=fig-finch] (b), it may happen that the previously known lower bounds only give the trivial zero lower bound, while our lower bound rapidly converges towards the exact value.

The regularity of de Rham curves

An important application of the joint spectral characteristics of matrices is the exponents of global and local regularity of solutions of functional equations. In particular, they measure the regularity of fractal curves, refinable functions, and wavelets. To a given refinable function, one can associate a set of matrices so that the Lyapunov exponent of this set is equal to the local regularity of the function at almost all points (in Lebesgue measure) of its domain. We consider the simplest example of refinable functions, the so-called de Rham curves, which are obtained from an arbitrary flat polygon by successive cutting off its angles, when each side is divided by the same ratio   ω:(1 - 2ω):ω, where [formula] is a given parameter. The corresponding matrices of de Rham curve are given by

[formula]

For these (very small) matrices, our upper bound and the Euclidean bound both perform well when k is large, but for small values of k, our upper bound is much better.

Words avoiding 7 / 3-powers

Our last application comes from language Theory. It has been recently shown that the asymptotic growth of some specific languages can be approximated by joint spectral characteristics of matrices [\cite=JPB] [\cite=J]. In [\cite=BCJ], this idea has been applied to the so-called language of 7 / 3-free words (reads as "seven thirds-free words"). These are words in which any subword is never repeated more than 7 / 3 times in some sense. See [\cite=BCJ] for more information. The matrices corresponding to that language have size 227  ×  227. In this reference, the authors analyze the joint and lower spectral radii of that set of matrices, but, in view of the large size of the matrices, nothing is said on the value of the Lyapunov exponent. The results are shown on Fig. [\ref=fig-7thirds]. From this figure it appears that the true value lies within the interval . We applied our techniques in order to estimate this value. The matrices in this application have zero rows and zero columns, therefore we use the modified lower bound k defined by ([\ref=betapos-mod]).

Randomly generated matrices

Nonnegative matrices

We ran our algorithms on randomly selected nonnegative matrices. We report here the results of computation on two different sizes: reasonably small matrices of dimension 5, and large matrices of dimension 60. For both these sizes, we generated dense and sparse matrices. For the dense matrices, the entries are drawn uniformly and independently between zero and one, while for the second case, we sparsify the matrices by putting each entries to zero with a probability p = 5 / 7. For matrices of size 60, computing long products rapidly becomes prohibitive. As one can see on Fig. [\ref=fig-random] (c) and (d), already with products of length 1 we have a fairly good approximation of the Lyapunov exponent, while the estimate with the Euclidean norm is still far from convergence. As one can see in figure (b), if the matrices become so sparse that zero rows can appear, then our lower bound βk may vanish. In this case we apply the modified bound k. Table [\ref=table-random] summarizes the scalability of our method when the size of the matrices increases. For the same effort of computation, the accuracy seems to be somewhat independent of the dimension.

We provide the results of computations (the lower bound βk and the upper bound αk) for diverse sizes of matrices, as well as the relative accuracy obtained. We also mention the maximal length of the products computed in order to reach this accuracy.

Nonnegative matrices with zero columns

In Fig. [\ref=fig-rand15zerolines] we demonstrate applications of the modified lower bound ([\ref=betapos-mod]) for matrices with zero columns, when [formula].

Matrices with negative entries

When the matrices do not have only nonnegative entries, no algorithm is known to squeeze the Lyapunov exponent between a lower and an upper bound. Of course, the upper iterative estimate ([\ref=euc]) is always available, for any choice of the norm, but a good lower estimate converging to λ as k  →    ∞  , most likely, does not exist at all (see Section V). The only thing that can be done in this situation is to improve the upper bound. In Proposition [\ref=p20] we showed that for every k the upper bound Γk(I) is better than ([\ref=euc]) with the Euclidean norm. Here we compare these bounds for randomly generated matrices, whose entries were all i.i.d. homogeneously between - 0.5 and 0.5. We report in Table [\ref=table-nonpositive] the results for k = 1 for matrices of size 10,20,30,40. In Fig. [\ref=fig-nonpositive], we show the accuracy of the two upper bound for a randomly generated 30  ×  30 matrix. As one can see, Γk(I) is small already for k = 1, while for k = 3 the Euclidean bound is still far from having converged.