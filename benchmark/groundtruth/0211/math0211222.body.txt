Some non-conventional ideas about algorithmic complexity

Introductory remarks

The algorithmic (program-size, or Kolmogorov) complexity of a binary string s is defined as the size in bits of the smallest computer program able to generate it:

[formula]

where p is a program string used by a universal computer U to produce the sequence s (Chaitin [\cite=sa] [\cite=ait]).

Let us now consider the following algorithm of size ⌊ log 2N⌋ + k + 1 (see also the Appendix). It lists in order of their size all strings of length less than or equal to N bits (there are 2N + 1 - 2 of them), writing them one after the other on a computer file (or on a sheet of paper). The output should look like

Let us assume also that N  ≫  ⌊ log 2N⌋ + k + 1. Thus, since

[formula]

where 2⌊ log 2N⌋ + k + 2 - 2 is the total number of different program strings which could be obtained with a number of bits less than or equal to ⌊ log 2N⌋ + k + 1, then among all the produced strings there is surely at least one that is more complex than our algorithm, i.e. it can't be produced by any program of size less than or equal to ⌊ log 2N⌋ + k + 1 bits, by definition of algorithmic complexity. Suddenly a paradox appears: an algorithm of size equal to ⌊ log 2N⌋ + k + 1 bits is able to write a list of strings which contains at least one that is more complex than ⌊ log 2N⌋ + k + 1 bits, namely than the algorithm itself.

It is even more striking to consider a similar but simpler algorithm (the same as before but without the if condition, and thus having constant size, nearly equal to k bits; more in the Appendix) which lists every natural number in binary notation, endlessly. In this context, we have that every binary string, of any length, is generated by this almost trivial algorithm. And, although every single string could be extremely complex (for instance, like that coding the collected works of Giacomo Leopardi) the whole, infinite set has a ridiculous algorithmic complexity.

Think for a moment to a god less 'complex' than a small portion of what it created! Obviously, we are not claiming that our Universe is tout court identical to a computer algorithm; what we want to suggest is that the concept of something simpler generating something incredibly more complex is not completely weird and unfounded, where simplicity and complexity must be intended in terms of algorithmic complexity.

Think for a while to the mocking and frustrating possibility that this applies to our real world too. Mankind is searching for the ultimate meaning of things, expecting to find it through a wider and more complex description than we currently have. But, maybe, that assumption could be simply wrong, and the ultimate reason, the origin of complex things might be in a simpler thing, as it happens with our listing algorithm. Saying it in other words, complexity might be simply a by-product and any wider and more complex description, far from being a step toward the final explanation of things, might exist with no finality, might be simply self-aimed. Maybe, Who/What created the Universe is not Omniscient and Almighty like we suppose It should be, but, let us say, It might be simpler than bacteria. Tegmark [\cite=te] also suggested the possibility of a Universe with almost no information content if taken as a whole, and he did it from the point of view of quantum mechanics, while El Naschie [\cite=el], using the Newton non-dimensional gravity constant [formula] as a measure of complexity for the Universe, found that the information dimension of the Universe is 128, nearly equal to the inverse of the Sommerfeld electromagnetic fine structure constant measure at the electro-weak scale [formula].

Besides, the property of being explainable to mankind might be distributed by chance over the things of our world. Some strings/patterns are reproducible by shorter (therefore simpler and more familiar) programs, and thus they are 'explainable', while others are not and they are perceived as random by us (see, for example, the model of inductive inference by Solomonoff [\cite=so]; and the notion of random string, Chaitin [\cite=sa]). But there might be no design under the distribution of what is explainable and what is not. Likewise, some strings of our listing program are reproducible by shorter program, some other not, that's all!

A similar argument on the (un)explainability of the reality was proposed by C. S. Calude in terms of lexicons (see Calude and Meyerstein [\cite=cm] and references therein). A lexicon is the infinite expansion of some real number (e.g. the infinite binary expansion of 0s and 1s obtained through the tossing of a fair coin) with the base-independent property of containing every finite string as a sub-string, infinitely many times. Calude and Meyerstein [\cite=cm] suggest that the Universe might behave like a lexicon and that we maybe 'live' on a very long finite sequence that is ordered, i.e. it may be explained through science, at least partially. But, there is no guarantee that it is anywhere, anytime the same; the order may suddenly switch to pure randomness in other portions of the lexicon/Universe.

Probability and the brain

The probability that your brain, specifically its function of giving rise to your mind, is reproducible by a finite algorithm (e.g. N-bit long) is arbitrarily close to 0.

For the sake of thought experiment, let us try to provide some arguments in favor of the above statement. Let us suppose that the processes of your brain which give rise to your mind are exactly assimilable to and reproducible by an algorithm of N bits. Thus, we are able to algorithmically reproduce your brain (and thus its product, your mind) as a program running on an 'hardware' different from your body in such a way that it does not suffer from the main limitation of the human body, namely its relatively short life. In this way, such 'brain' could in principle work for an unlimited time span.

Moreover, let us imagine that as a part of the program simulating your brain there is a very simple (from the algorithmic point of view) and fast counter able to enumerate (but not to store) all binary numbers of c bits (there are 2c of them) in increasing lexicographical order.

Now, the reproduced mind could think of a specific decimal number greater than the size (in bits) of its own generating algorithm, say N + k, and make the counter start counting in increasing order all possible binary numbers/strings of N + k bits (there are 2N + k of them).

At its own will, the simulated mind can then stop the counter whenever it wants and print the last enumerated number. The counter could be provided with a sort of counting completeness indicator, which would give the percentage of the whole count reached till that moment. This can be of some help to the simulated mind in choosing when to stop the counter (not too early for instance, since the first binary strings are surely not very complex algorithmically).

Remember that the printed number is a binary string of N + k bits, less than 2N + k in size, and it results to be somewhat blindly chosen by the simulated mind. Therefore, with probability nearly equal to

[formula]

where 2N + 1 - 2 is the total number of different strings/programs which could be obtained with a number of bits less than or equal to N, the simulated N-bit long algorithmic brain/mind would be able to generate a sequence of a complexity greater than N bits.

For [formula], such probability becomes arbitrarily close to 1. To recap, within the hypothesis that your brain (and thus its product, your mind) is reproducible by an N-bit long algorithm, the probability that such algorithm would be able to generate a string of a complexity greater than N bits, and thus leading to a logical paradox according to the definition of algorithmic complexity, is arbitrarily close to 1. Therefore, the probability that the hypothesis will be violated is arbitrarily close to 1.

Of course, there is a number of possible critiques to the above argument. Someone may argue that for big values of N the described enumeration (involving 2N + k binary numbers of N + k bits) is not physically feasible (it would require an incredibly huge amount of time), making our point physically unsound.

Others may argue that the above argument does not eliminate at all the possibility that our minds are 'algorithmic': we might be similar to machines, behaving predictably like machines (and thus 'choosing', through the procedure described above, a string of consistent algorithmic complexity), but simply and wrongly believing we are not.

But, even in such cases our argument should be of some interest: though probably physically unfeasible, our point seems to be in principle logically and mathematically sound (after all, many trusted mathematical demonstrations are physically unverifiable, for they involve the concept of infinity for instance) and maybe it might be an example of a physical status (i.e. our brains/minds actually like algorithms) for which we are able to provide a logically and mathematically sound argument of the contrary.

Digression

Let us consider the following device. A mechanical tool reads a decimal number N in input and tosses an idealized, fair coin N times. Whenever the result of the toss is a head, such device prints a 1 on a long tape, otherwise it prints a 0. One might think that the algorithmic size of this device is proportional to log 2N (the size of the binary expansion of the decimal number N), and therefore that it could be able to generate a sequence of a complexity greater than log 2N bits, with probability arbitrarily close to 1.

In that case, however, the algorithmic size of the device is comparable to the complexity of the entire physical process, of all the physical laws (plus all the relevant initial conditions) which make the toss to result each time in a head rather than a tail, or vice versa. Hence, this device is not an algorithm embedded in an electronic computer; rather, it operates under the influences of the physical world. Maybe the same argument might apply to human mind too: the peculiarity of the brain, in giving rise to mind and consciousness, might partly originate from its complex and continuous physical interaction with the surrounding physical world.

Acknowledgements

I am very grateful to Professor M. S. El Naschie for helpful suggestions and comments on the subject of this paper.

Appendix

A possible (FORTRAN-like) computer code for our listing algorithm:

(decimal number)

The size of this algorithm is a constant k (which results from the coding of its instructions different from the decimal number N) plus the number of bits of the binary coding of N, that is ⌊ log 2N⌋ + 1 (where ⌊x⌋ is the floor function of x).

A simpler, yet more powerful, listing algorithm would be the following:

Its size is nearly equal to k but it is able to list an infinite number of binary strings.