Classifying informative and imaginative prose using complex networks

Introduction

The ever-growing amount of available documents in the Web has propelled the development of statistical natural language processing methods in recent years. Examples of related applications trying to "understand" unstructured data include machine translation [\cite=hutchins1992introduction], text summarization [\cite=amancio2012extractive] [\cite=yang2008hierarchical] [\cite=patil2007text] [\cite=marcu2000theory], information retrieval [\cite=chen2013text] [\cite=chen2013text] [\cite=singhal2001modern] and content analysis [\cite=krippendorff2012content]. An application of special importance for the organization of electronic data is the classification task, which automatically assigns one or more labels for a word, sentence, paragraph or entire documents [\cite=sebastiani2002machine] [\cite=aphinyanaphongs2014comprehensive] [\cite=amancio2012structure] [\cite=amancio2012identification]. Traditional textual categorization methods usually serve to identify the relevance of texts (i.e. whether it is a spam or not) or the meaning conveyed by words and expressions [\cite=fetterly2004spam] [\cite=meyer2004spambayes] [\cite=navigli2009word] [\cite=amancio2012unveiling] [\cite=silva2012word]. Recent classification tasks, however, have emphasized other textual aspects. For example, the categorization of texts according to the their polarity (e.g. positive or negative) has become a relevant task for analyzing e.g. customer reviews [\cite=miao2010fine] or global variations in mood via polarity analysis of twitter messages [\cite=golder2011diurnal]. Note that most of these classification tasks are dependent on text content, since the presence of one or more specific words provides clues about the classes being inferred. While the semantic content is crucial for the success of these applications, the structure of texts might play an important role in classifications problems where the semantics of words is not crucial for the purpose of categorization. This is the case of identifying the style of texts, since documents on the same subject might display different writing styles. In contrast to semantic-based traditional classification tasks, in this paper we probe the relevance of textual structure to provide useful features for text classification. More specifically, we probe how textual structure depends on two distinct stylistic writing styles: imaginative and informative prose. The structure and organization of texts is studied via networked models, an well known representation of complex systems.

Networks are discrete models that basically represent the interrelations between interacting agents in a complex system. Owing to the simplicity and generality of the model, it has been employed to model a myriad of real and artificial systems [\cite=costa2011analyzing]. Despite being very different in nature, networks modelling distinct systems share several structural patterns [\cite=newman2010introduction]. Of special relevance to this paper, are the networked models of language and texts, which have been useful to unveil universal properties including the scale-free and small-world phenomena [\cite=i2001small]. In practical terms, networked models have been useful to grasp several features of texts, such as quality [\cite=antiqueira2007strong], complexity [\cite=amancio2012complex] and authenticity [\cite=amancio2013probing]. Particularly, in this study, we used the so-called word adjacency model, which is a approximation of text networks formed by syntactical links [\cite=i2004patterns]. Because the topological analysis of word adjacency networks does not depend on the interpretation of texts, it has been applied with relative success to study the underlying structure of texts, even when textual content is entirely unknown [\cite=amancio2013probing]. Here we applied such representation to discriminate informative and imaginative prose. We have extended the traditional model in a twofold fashion: (i) we analyzed the local structure of particular nodes (words); and (ii) we considered novel topological/dynamical that are able to unfold the general structure of the network of concepts. As we shall show, both extensions provided competitive classification performance when compared to traditional stylometry methodologies. In special, we have found that symmetries and accessibilities were the most important network measurements. We believe that the proposed extended model could be used to improve the performance of several related problems where the textual structure plays a prominent role in the characterization of documents [\cite=stamatatos2009survey].

This paper is organized as follows. In Section [\ref=sec.met], we present the word adjacency model. Section [\ref=sec.patt] describes the pattern recognition methods used to perform the classification. This section also presents a method for measuring feature relevance in a multivariate fashion. The results obtained with the proposed methodology are described in Section [\ref=sec.results]. Finally, Section [\ref=sec.conclusion] provides a perspective for further studies and improvements of the model.

Representation and characterization of texts as networks

In this section, we present how a text can be represented as a complex network. We also swiftly describe the main topological network measurements employed to characterize the structure of networks.

Modeling texts as complex networks

A complex network can be represented as a graph, which is defined as a set of nodes and edges [\cite=newman2010introduction]. An usual representation of a network is the adjacency matrix [formula]. The elements are defined as:

[formula]

In the text mining community, several networked representations of texts have been proposed [\cite=mihalcea2011graph]. If the stylistic properties of texts are relevant for the task being tackled, syntactical relations are employed to establish the links between words [\cite=i2004patterns]. If the application requires the extraction of semantic features, words are connected according to semantic relations, such as those present e.g. in the WordNet [\cite=miller1995wordnet] or in free association graphs [\cite=costa2003what]. In this study, we aim at grasping textual features that are independent of semantic content. For this reason, we used a model that is able to capture stylistic textual features [\cite=cong2014approaching] [\cite=amancio2012identification]. This model, henceforth referred to as word adjacency model, denotes each distinct word as a node. The edges are established between words appearing in the same context. It has been shown that if one considers the context as the interval of two adjacent words, most of the syntactical relations are recovered [\cite=i2004patterns]. This model has been successfully applied to study many language applications and related systems [\cite=cong2014approaching].

To create a word adjacency network, usually some pre-processing steps are applied. First, all punctuation marks, line breaks, spaces, numbers and special characters are removed. Particularly, we are mostly interested in the relationship between words conveying semantic information. For this reason, stopwords (or function words) can be optionally removed from the analysis. In the next step, a lemmatization step is performed in order to map words representing the same concept into the same node. To assist the lemmatization process, the part-of-speech tag of each word is extracted according to the procedure described in [\cite=greene1971automatic]. The part-of-speech labelling is required to solve ambiguities, because the same word form might be mapped into distinct lemmas. After the pre-processing step, each remaining distinct word becomes a node and edges are established between adjacent words.

Complex network measurements

Currently, there are more than a hundred measurements employed to characterize the topological structure of networks [\cite=costa2007characterization]. Some measurements might depend not only on the structure, but also on a dynamical process (e.g. random walks) occurring on the structure. Below we swiftly describe the measurements used in this study.

Number of nodes (V): in a word adjacency network, the number of nodes is the set of different words in the text. In other words, the number of nodes is the vocabulary size of the pre-processed text.

Degree (k): the simplest connectivity measurement is the node degree [\cite=boccaletti2006complex], which corresponds to the total number of edges connected with node i. This measurement is defined for directed networks as [formula] and [formula] for in- and out- degree, respectively. If one considers the undirected and unweighted version of the network, the degree [formula] can be understood as the number of distinct bi-grams that a given word appears. If one considers edges weights, then the degree is proportional to the word frequency.

Neighborhood connectivity (N): this measurement is defined as the number of nodes that can be reached when, starting from the reference node, walks of length h are performed. Note that the traditional degree measurement is recovered when h = 1.

Clustering coefficient (cc): given a node i, the probability of its neighbors to be connected is called clustering coefficient (cci) [\citep=watts1998smallworld]. This measurement is defined as

[formula]

where NÎ”(i) is the total number of triangles (i.e. a click comprising three nodes) connected with node i, and N3(i) denotes the number of connected triples, which is defined as the amount of different connections between i and each pair of nodes. This measurement is traditionally used to quantify the local connectivity of real-world networks [\cite=opsahl2009clustering]. In word adjacency networks, this measurement has been applied to quantify the specificity of words according to the number of distinct contexts in which they appear [\cite=amancio2011comparing].

Betweenness centrality (B): to define this measurement, consider all paths connecting any pair of nodes in the network are followed via shortest paths [\cite=Freeman1977Betweenness]. The betweenness of a node u is defined as being proportional to the number of paths that passes through node u. More specifically,

[formula]

where Ïƒ(i,u,j) is the number of shortest paths between i and j that passes through node u and Ïƒ(i,j) is the total amount of shortest paths between i and j. According to equation [\ref=btweq], the betweenness centrality can be interpreted as the network flow [\cite=newman2005measure] [\cite=borgatti2005centrality] [\cite=freeman1991centrality], which is a relevant quantity for the analysis of robustness of power-grid networks [\cite=Motter2002Cascade] [\cite=mirzasoleiman2011cascaded]. When applied to the analysis of text networks, this measurement has been interpreted as being useful to quantify the generality of words in which the word appears [\cite=amancio2015authorship], which is in part motivated by the use of this measurement in community detection methods [\cite=girvan2002community]. Unlike the clustering coefficient, the betweenness centrality uses the global connectivity information to quantify the specificity/generality of concepts [\cite=amancio2015authorship].

Closeness centrality (C): unlike the betwenness centrality, which is based on the number of shortest paths, the closeness centrality [\cite=freeman1979centrality] uses the length of the shortest paths. If dij is the shortest distance between nodes i and j, the closeness centrality is calculated as [formula]. Geodesic paths have been reinterpreted in the context of text networks as a measure of word relevance. Actually, a word is deemed relevant if it is very frequent in the text or if it appears related to other very frequent words [\cite=amancio2011comparing].

Eccentricity (E): this measurement quantifies the maximum geodesic distance between the reference node and all other nodes [\cite=estrada2011structure]. Therefore, the maximum eccentricity value corresponds to the network diameter. This measurement is calculated for each node i as Ei  =   max j(dij).

Eigenvector centrality (Ec): the eigenvector centrality can be understood as an extension of degree centrality [\cite=bonacich1987power], because the relevance of the reference node relies both on the number and relevance of neighbors. Considering the adjacency matrix [formula], the eigenvector centrality is defined as the eigenvector associated with the leading eigenvalue. There are many linguistic applications that uses this centrality measurement. It has been applied, for example, in the text summarization task in order to select the most relevant extracts in texts modelled as graphs [\cite=patil2007text].

PageRank (Pr ): the PageRank is widely known to be part of the Google's web search [\cite=newman2010introduction] [\cite=langville2011google]. In texts networks, this measurement has been successfully applied e.g. to disambiguate word senses [\cite=mihalcea2004pagerank]. This measure is based on the eigenvector centrality, and it is defined in matrix terms as

[formula]

where Î± and Î² are positive constants (conventionally Î² = 1), [formula] is a vector [formula] and [formula] is a diagonal matrix represented as

[formula]

In contrast with eigenvector centrality, PageRank considers a weighted sum of neighbors importance reflecting the neighbors degree. In this way, the relevance associated to a node is proportionally transferred to its neighbors.

Accessibility (A(h)): the accessibility is an extension of the concept of neighborhood connectivity because it measures the effective number of nodes reached at the h-th concentric level [\cite=travenccolo2008accessibility]. The effective number of nodes accessed after h steps is computed considering the distribution of probabilities of access via self-avoiding random walks. Mathematically, it is defined using the Shannon entropy [\cite=shannon2001mathematical] of the probabilities of access at the h-th concentric level:

[formula]

where p(h)ij is the probability of a walker starting from i to reach node j in h steps. In text networks, this measurement has been applied to generate summaries and to identify keywords and styles [\cite=amancio2012extractive]. An example of the computation of accessibility is shown in Fig. [\ref=acfig].

Generalized accessibility (Ag): the generalized accessibility is an extension of the accessibility that does not rely on a particular length of walk. Instead, the probabilities of transition are computed considering all possible lengths, which is implemented via definition of a modified random walk, the so called accessibility random walk [\cite=arruda2014role]. This measurement is defined as

[formula]

where [formula] is a quantity that depends on the probability of the transition i  â†’  j considering walks of variable length. The matrix [formula] representing the probability of transition is calculated as [formula], where

[formula]

Note that, according to equation ([\ref=www]), the highest weights are assigned to the nearest nodes. The generalized accessibility has been applied e.g. to identify influential spreaders in spatial networks [\cite=arruda2014role].

Symmetry (S): the symmetry concept is found in many real systems [\cite=finnerty2003origins] [\cite=longuet1963symmetry]. Symmetric properties can also occur in written texts as a consequence of grammatical or stylistic constraints [\cite=amancio2015concentric]. For this reason, we have quantified this property in word adjacency networks. To model such property, recently, some network measurements have been created [\cite=holme2006detecting] [\cite=rossi2013characterizing] [\cite=silva2014concentric] [\cite=amancio2015concentric]. In this paper, we used the quantities introduced in [\cite=silva2014concentric] [\cite=amancio2015concentric], as it allows to capture symmetric patterns in a multi-scale fashion. The definition of symmetry measures rely upon the characterisation of hierarchic levels. The hierarchic level Î“h(i) for a given node i is the set comprising all nodes h hops away from i. The symmetry measures are based on the accessibility measurement, because the same network dynamics is taken for the analysis. In addition, the symmetry measurement can be seen as a normalization of the accessibility. Thus, using self-avoiding random walks, a node is considered to be symmetric if the access to its neighbors (in a given hierarchic level) is symmetric. The symmetry (or regularity) of the access is measured in terms of the entropy:

[formula]

where Î·r denotes the total number of dead ends in the r-th hierarchical level and p(h)ij is the same quantity used to define the accessibility in equation ([\ref=acessa]). There are two variations of the quantity proposed in equation ([\ref=eq.sim]). The backbone symmetry (Sb), a variation of the the concept of radial symmetry, removes all edges between nodes in the same hierarchical level. The merged symmetry (Sm), on the other hand, is based on the concept of angular symmetry, which can be obtained by merging linked nodes in the same hierarchical level. To exemplify both variations of the symmetry concept, we show in Fig. [\ref=fig.symmetry] the transformations applied to a hierarchical neighborhood before the computation of equation ([\ref=eq.sim]).

Modularity (Q): a community structure is defined as a network subgraph with a large number of intra-links and a few edges connected to the others nodes of the network. To quantify whether a network is organized in communities, the modularity compares the number of internal links with the expected value of the same quantity in a equivalent random network [\cite=Newman2006Modularity]. This quantity is computed as

[formula]

where [formula] is the total number of edges in the network, ci and cj are the communities to which nodes i and j belong, and

[formula]

Usually, word adjacency networks display low values of modularity. A more consistent organization in communities can be found e.g. in semantic networks such as the WordNet [\cite=miller1995wordnet].

Characterization of texts with complex networks

So far we have presented several topological/dynamical measurements of complex networks. The objective here is to use these quantities to characterise styles in texts. Note that several measurements are locally defined, i.e. each node possess a value. There are several possibilities to use these local measurements to characterise the networks. In this paper, we have used the following three distinct methodologies:

Global strategy without stopwords (GS): in this approach, we sum up the local measurements to characterise the networks. The most natural summarisation procedure is to take the average âŒ©XâŒª, where [formula] and X is a local measurement. We also used the following quantities: the standard deviation Ïƒ(X), the median ([formula]), the maximum value (max (X)) and the minimum value (min (X)). The only global measurement, the modularity, was also used considered in this strategy. Following several approaches for grasping textual features with networks, we have removed all stopword before the formation of the networks. These words were disregarded from the analysis because they just serve to connect content words in the word adjacency model.

Local strategy without stopwords (LS): in this approach, the value of each measure X for each word becomes a feature. Similarly to the GSW, all stopwords are removed from the analysis. Because features are defined for each word, global networks measurements are not considered in this case.

Local strategy with stopwords (LSS): this is the same local approach adopted in the LS method. However, this variation also considers all stopwords in the analysis.

Pattern recognition and evaluation

In this section, we present the methodology for analyzing the relationship between the texts and the categories (informative and imaginative). More specifically, we describe the pattern recognition methods employed and the methods to compute the quality of the classification and relevance of the proposed features (see Section [\ref=section.meas]).

Pattern recognition methods

To study the relationship between complex network measurements and text style, we used a feature selection algorithm and three different supervised classifiers. The method used to select the features was the information gain [\cite=mitchell1997machine], which is a supervised attribute filter known as mutual information [\cite=Cover2006InformationTheory]. Given the random variables X and Y, the mutual information I(X,Y) is computed as

[formula]

where p(x) and p(y) are probability functions and p(x,y) is the joint probability.

The information gain corresponds to the mutual information when X is the values obtained for a given attribute and Y is a vector of corresponding classes. This technique is used to create a decreasing sorted ranking of relevance. Thus, the most relevant attributes, i.e. the ones with the highest values of information gain, are selected to perform the classification. An important characteristic of this method is that the attributes are evaluated separately, i.e. the information of a given attribute does not influence the others.

In our experiments, the following pattern recognition methods were used:

Nearest neighbors: the K nearest neighbors classifier (KNN) considers the local neighborhood of the test instance [\cite=krzysztof2007data]. Given a test instance, the class chosen is the majority class in the set of the K nearest neighbors in the training dataset. Further details concerning this method can be found in [\cite=bishop2006pattern].

Classification and regression tree: this method represents the patterns found in the dataset as a tree, a data structure storing nested rules. Even though there are several tree algorithms, we chosen to use the classification and regression tree (CART) method [\cite=breiman1984classification] because it has some advantages as it is relatively simple for interpret and the the predictor variables are not previously assumed [\cite=lewis2000introduction]. A major advantage of tree-based pattern recognition algorithms the patterns found in the dataset are not hidden from the user, as it happens in artificial neural networks methods [\cite=bishop2006pattern].

Naive Bayes: the Naive Bayes algorithm is based on the Bayes theorem [\cite=getoor2007introduction]. Assuming feature independence, the correct class [formula] of an instance is given by where ck is one of the possible classes, fjâˆˆF is a particular feature. To compute the quantity P(fj|ck) we assumed that the likelihood of the features follows a bell shape [\cite=manning1999foundations].

We have chosen the aforementioned methods because they yield good performance when set with default parameters [\cite=amancio2014systematic]. The evaluation of the performance of the methods when set with default parameters was performed with the "leave one out" algorithm [\cite=guyon2006feature]. This evaluation procedure consists in selecting one element of the dataset to be used as an test instance, while the remaining instances are used in the training phase. This procedure is then repeated until all instances of the dataset have been chosen as an test instance.

Quantifying feature relevance

To quantify the relevance of features for the classification task, the following method was used. Let [formula] be the set of attributes comprising Î¦ distinct attributes. We generate a set Fc comprising all 2Î¦ combinations of features. For a particular classification method, we compute the accuracy rate obtained for each classifier in Fc. The accuracy rate is then employed to sort (in decreasing order) the classifiers in Fc. A function is associated for each attribute in F:

[formula]

where Ï‰(fi,j)  =  1 if the j-th best classifier in Fc used the i-th attribute. If the i-th feature was not used in the j-th best classifier, then Ï‰(fi,j)  =  0. Note that the function Î© quantifies how frequent a given feature is in the best classifiers. If this function has a fast growth for low values of k, then it is a relevant feature because it appears in the classifiers with the highest accuracy rates. To quantify how frequent a given feature fi is among the best classifiers, the following index of feature relevance can be defined:

[formula]

Unlike traditional index devised to measure the relevance of attributes, the index defined in eq. ([\ref=meu]) takes into account the non-trivial inter-relationship between features [\cite=mann1947test].

Results and discussion

In this section, we analyze the proposed technique for discriminating informative and imaginative prose. We also compare the proposed technique with other traditional natural language processing methods. In our experiments, we used the Brown University Standard Corpus of Present-Day American English (a.k.a. Brown Corpus) [\cite=francis1979brown]. Because the set of informative texts comprises several short texts, for this class we have selected only the 126 longest texts. As such, in our experiments, each class is represented by the same number of instances. Each class can also be classified in subclasses. The set of informative texts used in this study comprises 80 scientific manuscripts, 30 miscellaneous texts and 16 biographies and related subjects. The set of imaginative documents comprises general fiction, romances, love stories and others. Note that we have not used this fine-grained description in our experiments.

Complex network approach

Following the steps in the methodology, we created a word adjacency network for each document in the dataset. The topological measurements were extracted and the 15 most relevant features were selected according to the information gain criterion. In the global strategy, the following features have been selected:   Vocabulary size: V; Degree connectivity: âŒ©kâŒª; PageRank: [formula], âŒ© Pr âŒª, Ïƒ(Pr) and max (Pr); Clustering coefficient: Ïƒ(cc) and âŒ©ccâŒª; Closeness centrality: min (C), Ïƒ(C), âŒ©CâŒª and [formula]; Generalized accessibility: âŒ©AgâŒª; and Betweenness centrality: âŒ©BâŒª and [formula].   In this case, the accuracy rate reached a maximum value of 78% with the Naive Bayes algorithm. Note that this result is statistically significant, as the p-value associated with this accuracy rate is p  <  1.0  Ã—  10- 10. The accuracy rate obtained for the other classifiers are shown in the first row of Table [\ref=fig.maxresults].

When the local strategy (LS) was used to perform the classification, the accuracy rate improved by a large margin: all three classifiers reached an accuracy rate of 92%. The largest improvement in performance occurred for the KNN classifier; the accuracy rate went from 72% to 92%. The features employed in this case were:     Backbone symmetry: Sb(h), for h = {2,3,4}; Merged symmetry: Sm(h), for h = {2,3,4}; and Accessibility: A(h), for h = {2,3}.     Note that these local measurements were chosen because they do not correlate with the frequency. The local strategy with stopwords (LSS) displayed an slight better classification performance. In this case, the accuracy rate reached 95% with KNN, CART, and Naive Bayes classifiers. The principal component analysis projection provided in Fig. [\ref=fig.pcaNetworkStop] confirms the suitability of this network model for discriminating informative from imaginative prose. In this case, the features employed were:     Backbone symmetry: Sb(h), for h = {2,3,4}; Merged symmetry: Sm(h), for h = {2,3,4}; Acessibility: A(h), for h = 2; and Generalized accessibility: (Ag).     Note that, in both local strategies, the accuracy rates in the classification are much higher than the ones obtained with the global strategy, which suggests that a few words account for the informativeness of the topological approach. To better understand the factors behind the network ability to discriminate informative from imaginative prose, we evaluated the relative importance of features employed in the best approach, i.e. the local strategy with stopwords. The method employed to quantify the relevance of features is described in the methodology. According to this method, the most relevant features, in decreasing order of relevance were:     (i) Merged symmetry: Sm(h = 2)(the) (ii) Merged symmetry: Sm(h = 3)(by) (iii) Backbone symmetry: Sb(h = 4)(by) (iv) Merged symmetry: Sm(h = 3)(an) (v) Generalized accessibility: Ag(have) (vi) Merged symmetry: Sm(h = 4)(by) (vii) Generalized accessibility: Ag(it) (viii) Generalized accessibility: Ag(by).     Note that the most relevant features are those related to the symmetry of specific words. Interestingly, this is consistency with recent results showing that symmetry measurements tend to be more discriminative than other traditional network measurements [\cite=comin2014framework].

It is relevant to highlight that most of the network approaches for text classification focus on the global properties of networks. Our results reveal, conversely, that the informativeness of the topological strategy concentrates in a few nodes. Particularly, the informativeness was found to be mostly hidden in the symmetry patterns of specific function words. For this reason, we believe that the local strategies (LS and LSS) could be useful not only for the studied task, but also in several related tasks, where the topology of specific words plays a prominent role in characterizing texts.

Comparison with traditional methods

To compare the performance of the proposed technique with other traditional techniques, we first analysed if the classes can be discriminated via Latent Semantic Analysis [\cite=dumais2004latent], which considers as features the frequency of words. The projection obtained with this technique is shown in Fig. [\ref=fig.lsa]. Note that a good discrimination was obtained in this case, mainly because some words are more common in informative documents (e.g. state, system and program, while others occur more often in imaginative texts (e.g. say and mr.). A more accurate classification system based on stylistic attributes can be created if one considers as features the frequency of the most informative stopwords. To select the most informative stopwords, we used the information gain criterion. Using the KNN classifier (the best classifier), the performance reached 97% of accuracy. This high accuracy level can be observed in the principal component analysis provided in Fig. [\ref=fig.pcaStop]. Another traditional strategy in stylometry consists in counting the frequency of character bigrams [\cite=stamatatos2009survey]. Considering the most informative bigrams, the accuracy rate reached 98%. A visualisation of the data provided by this set of features is shown in Fig. [\ref=fig.pcaBigrams].

All in all, the results obtained with traditional classifiers demonstrate that the local topological approach is effective as our best results differs only 3% from the most efficient traditional system. This result is consistent with similar studies showing that the topology plays a relevant role in characterising complex systems, especially those conveying information [\cite=amancio2015authorship] [\cite=amancio2012extractive] [\cite=amancio2012identification] [\cite=amancio2011comparing]. Because the proposed representation is complementary to the traditional approaches, we advocate that the combination of features of distinct nature (traditional and topological) could lead to the improvement of similar tasks relying on the accurate characterisation of stylistic marks.

Conclusion

In this paper, we have evaluated the ability of network measurements to identify two textual categories, which are related to informative and imaginative documents. We have extended previous models in a twofold manner. First, the local topology of nodes representing specific words was studied. We have thus emphasized particular network regions to characterize the local topology of texts. This approach differs from previous networked representations because traditional topological analyses consider with equal relevance the topological analysis of all nodes of the network. Another proposed extension is the use of novel network measurements that are able to grasp more relevant information than traditional measurements. Particularly, we have used symmetry measurements that are able to quantify the homogeneity of access to neighbours. The concept of node degree was also extended via introduction of accessibility measurements, which are able to measure the effective number of (accessed) neighbours.

Computational simulations revealed that the proposed extensions are able to improve the efficiency of classification tasks. The best improvement in performance when comparing the traditional model and the proposed method occurred with the KNN classifier. An improvement of 23% was observed, thus confirming the efficiency of the proposed methodology. A systematic analysis of feature relevance revealed that among the most informative attributes are the symmetry and accessibility indexes applied to particular nodes. These results confirm the complementary role played by these measurements in characterizing text networks, since they do not correlate with traditional natural language processing methods. Owing to the generality of the proposed representation and characterization, we believe that it could be extend to a myriad of related applications where the quantification of style is relevant for text categorization. As further works, we intend to combine network methods and traditional statistical methods to improve the performance of the classification. We expect, in this case, that the interwoven combination of methodologies will be able to overcome the limitations of each technique.

acknowledgments

HFA thanks CAPES for financial support. LdFC is grateful to CNPq (Brazil) (grant no. 307333/2013-2), FAPESP (grant no. 11/50761-2), and NAP-PRP-USP for sponsorship. DRA acknowledges financial support from SÃ£o Paulo Research Foundation (FAPESP) (grant no. 14/20830-0).