Prospects of GPGPU in the Auger Offline Software Framework

Introduction

Although cosmic rays have been intensively studied for more than 100 years, fundamental questions about the phenomenon remain unclear. In particular, the origin, the acceleration mechanism, and the chemical composition of the highest energetic cosmic rays with energies up to several hundred EeV (1 EeV = 1018 eV) remain open questions. As the flux of the highest energy cosmic rays is of the order of one particle per square kilometer per century, direct observation is impracticable. Instead, the Earth's atmosphere is used as a calorimeter in which the cosmic rays are detected indirectly by the particle cascades, or 'air showers', they induce. For detailed reviews on cosmic rays see e.g. references [\cite=Kotera2011] [\cite=Letessier-Selvon2011].

The currently largest detector for cosmic rays at the highest energies is the Pierre Auger Observatory in Argentina. The Pierre Auger Observatory [\cite=PAO2004] is designed as a hybrid of two complementary detector systems. The 'surface detector' [\cite=Allekotte2008] consists of 1660 water-Cherenkov stations that sample the secondary particles at the ground level. The stations are arranged in a hexagonal grid that covers an area of 3000 km2. The surface detector array is surrounded by 27 telescopes stationed at four sites to detect the fluorescence light emitted by air molecules that have been excited by the particle cascade. This 'fluorescence detector' [\cite=Abraham2010] provides a direct calorimetric measurement independent of hadronic interaction models and thus a better energy resolution than the surface detector. Furthermore, as it measures the development of the shower in the atmosphere, it is sensitive to the mass of the primary cosmic ray. However, the fluorescence detector can operate only during clear and moonless nights, whereas the surface detector has no principal constraint on the uptime.

As complementary detector without principal time constraints, the Auger Engineering Radio Array (AERA) [\cite=Neuser2014] detects the radio pulses emitted by the particle cascades, due to geomagnetic and charge-excess effects [\cite=Aab2014a]. Currently, AERA consists of 124 stations in a hexagonal grid covering an area of about 6 km2. Each station is equipped with an antenna designed for polarized measurements from 30 to 80 MHz.

The Auger [formula] Software Framework [\cite=Argiro2007] provides the tools and infrastructure for the reconstruction of events detected with the Pierre Auger Observatory. Components of the framework are also used by other experiments [\cite=Sipos2012]. It is designed to support and incorporate the ideas of physicists for the projected lifetime of the experiment of more than 20 years. This is achieved by a strict separation of configuration, reconstruction modules, event and detector data, and utilities to be used in the algorithms. The reconstruction of radio events detected with AERA is fully integrated in the Auger [formula] Software Framework which allows joint analyses of events from all detector systems [\cite=PAO2011g]. The main reconstruction algorithm for the radio reconstruction and its performance profile is described in the next section.

Radio Reconstruction in the Auger Offline Framework

The main algorithm for the reconstruction of radio events implemented in several [formula] modules is depicted in Fig. [\ref=fig:OfflineRadioReconstruction]. After a trigger, the voltage traces of the two channels at each station are read out, and, after noise filtering and signal enhancing, processed as follows. First, as an initial estimate of the event timing in each station, the maxima of the voltage traces is used. Second, from the timing information of the individual stations, the direction of the shower is obtained by triangulation. Third, the antenna pattern for this direction is evaluated, and the E-field trace at each station reconstructed from the voltage traces. Finally, the maximum of the envelope of the E-field trace is used as updated timing information for a new iteration of the procedure. On convergence, the timing information yields the incident direction of the primary particle whereas the E-field distribution on the ground allows a derivation of the energy and particle type of the cosmic ray.

The execution of this algorithm in [formula] requires an uncomfortable amount of time. Using the Linux kernel profiler 'perf' [\cite=linux_perf] we identified two main performance bottlenecks in this algorithm. First, about 15% of the computation time is spent in calculating Fourier transformations with the FFTW library [\cite=fftw]. Second, about 25% of the time is used for the interpolation of the antenna patterns. All other parts of the algorithm use less than 5% of the time. The same bottlenecks are identified using 'google-perftools' [\cite=google_perf] or 'Intel VTune amplifier' [\cite=intel_vtune] as alternative profilers.

In the next sections we discuss the elimination of these bottlenecks by implementing the relevant parts on the GPU using the Cuda framework. In both cases we followed a minimum invasive approach that leaves the general interfaces in [formula] intact. To select between the CPU and GPU implementation, a preprocessor directive is used.

Interpolations of Antenna Patterns

To reconstruct the electric field vector from the measured voltage traces the antenna pattern in the direction of the electromagnetic wave must be known. The antenna pattern can be conveniently expressed as a two dimensional complex vector, the 'vector effective length (VEL)'. For each antenna, the VEL is known for discrete frequencies and zenith and azimuth angles from measurements or simulations. Between theses nodes the VEL has to be interpolated for arbitrary directions.

The interpolation of textures is a core task of graphics cards, which have dedicated circuits for the interpolation. The usage of these promises a great speedup, but is available for single precision data of limited size only. In the baseline implementation, the antenna pattern is evaluated in double precision. As a linear interpolation requires six elementary operations, the maximum relative uncertainty from the limited floating-point precision can be estimated as 2.5  ×  10- 4 % in single precision. This is smaller than other uncertainties in the reconstruction and thus negligible here. The largest antenna pattern considered here consists of complex data for the vector effective length at 98 frequencies, 31 zenith angles, and 49 azimuth angles. In single precision this corresponds to approximately 2.4 MB of data which is small compared to the total available memory size of a few GB on modern GPUs and much below the maximal size of a 3D texture of typically at least 2048×  2048×  2048 elements. The patterns for all antennas used in AERA can be stored simultaneously on the device.

To speed up the interpolation on the CPU, the pattern has already been buffered in the baseline implementation for look up on repeated access. In the GPU implementation this is unnecessary. Here, the patterns are copied and converted to allow binding to texture memory only once on first access.

Fourier Transformations and Hilbert Envelopes

In the baseline implementation in [formula], calls to the FFTW library are wrapped in an object-oriented interface. The interface provides several distinct classes for the operation on real or complex data of given dimensionality. Shared functionality is implemented in a base class and propagated by inheritance. In the radio reconstruction, the wrapper operates within a container that stores a trace simultaneously in the time and frequency domains. After modification of either, the other is updated in a lazy evaluation scheme.

To calculate the FFT on the GPU, FFTW calls are replaced by calls to the CUDA FFT library (CuFFT). In contrast to the CPU implementation, all instances of the GPU implementation share static memory on the GPU to avoid time consuming allocations. This is safe here as memory copies are blocking and performed immediately before and after the FFT calculation.

However, in the reconstruction several FFTs are calculated in context of obtaining the Hilbert envelope of the radio traces. The envelope E(t) of a trace x(t) is [formula] with the Hilbert transform of the signal H(x(t)). The Hilbert transformation shifts the phase of negative frequencies, i.e. for band-limited signal frequencies below the mid-frequency, by [formula] and positive frequencies by [formula]. The GPU implementation as described above thus results in a non-optimal memory access pattern for the envelope calculation as shown in Fig. [\ref=fig:HilbertImplementations] (a). However, with a dedicated computing kernel not only can two memory copies be avoided, but also the phase shift and summation are calculated in parallel (cf. Fig [\ref=fig:HilbertImplementations] (b)).

Discussion

The results obtained from the new GPU implementations are consistent with the results from the baseline implementation. While the FFTs yield identical results on CPU and GPU, the interpolation is not only a different implementation but also only in single precision. This amounts to a relative difference in the directional antenna patterns between the individual implementations of typically below ±  0.8% and thus small compared to other uncertainties.

The performance improvements obtained by the modifications are summarized in Fig. [\ref=fig:SpeedupSummary]. As test systems we used here a typical recent desktop PC and a combined CPU/GPU cluster. The desktop is equipped with an AMD A8-6600K processor and an NVIDIA GeForce 750 Ti graphics card. The cluster contains 4 Intel Xeon X5650 CPUs and 4 NVIDIA Tesla M2090 GPUs. On the desktop system the GPU implementation of FFT and the Hilbert transformation yield a speedup of 1.3, doing also the interpolations on the GPU increased this speedup to approximately 2. On the cluster system the total achieved speedup is 1.5. The lower speedup on the cluster system is due to the higher relative performance of the cluster CPU and GPU compared to the desktop system.

As only selected isolated parts of the code are moved to the GPU, the time used for computing on the GPU is low compared to the time needed for memory copy, and also only 7% of the copy-time is overlapped by computing time. However, increasing the GPU utilization would require the traces to be kept permanently on the GPU so that more analysis steps can benefit from porting to the GPU. This, however, would require non-trivial changes in the [formula] framework, in particular, modifications of the internal structure and interfaces.

Conclusion

The calculation of Fourier transformations and the interpolation of antenna response patterns have been identified as bottlenecks in the AERA event reconstruction using a performance profiler. Eliminating both by re-implementating the calculation in CUDA while keeping the structure of [formula] intact yields a speedup of 1.49 to 2.04 depending on the test system. The largest speedup is obtained here on a typical desktop PC equipped with an entry level graphics card. Considering the relative costs of about € for a desktop PC and € for an entry level GPU, even such selected applications of GPGPU in existing frameworks are a possibility to be considered in planning future computing strategies.