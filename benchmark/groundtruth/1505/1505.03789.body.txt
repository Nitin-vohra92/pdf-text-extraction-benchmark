Bounds for maximum likelihood regular and non-regular DoA estimation in K-distributed noise

, Olivier Besson

and Ben A. Johnson

Problem statement

Estimating the direction of arrival (DoA) of multiple signals impinging on an array of sensors from observation of a finite number of array snapshots has been extensively studied in the literature [\cite=VanTrees02]. Maximum likelihood estimators (MLE) and Cramér-Rao bounds (CRB), derived under the assumption of additive white Gaussian noise, and either for the so-called conditional or unconditional model [\cite=Bohme86] [\cite=Stoica89] [\cite=Stoica90] [\cite=Stoica90b], serve as references to which newly developed DoA estimators have been systematically compared. In many instances however, additive noise is usually colored and, consequently, the problem of DoA estimation in spatially correlated noise fields has been studied, see e.g., [\cite=Friedlander95] [\cite=Ye96] [\cite=Nagesha96] [\cite=Viberg97] [\cite=Goransson99].

When the spatial covariance matrix of this additive noise is known a priori, maximum likelihood estimators and Cramér-Rao bounds are changing in a straightforward way with whitening operations. The new statistical problem appears when the covariance matrix of the additive noise is not known a priori and information about this matrix is substituted by a number of independent and identically distributed (i.i.d.) training samples, that form the so-called secondary training sample data set. In many cases one can assume that the statistical properties of the training noise data are the same as per noise data within the primary training set data: such conditions are usually referred to as the supervised training conditions. Therefore, under these conditions, one has two sets of measurements, one primary set [formula] which contains signals of interest (SOI) and noise, and a second set [formula] (secondary training set) which contains noise only. Examples of this problem formulation are numerous in the area of passive location and direction finding. For instance, in the so-called over-sampled 2D HF antenna arrays, ionospherically propagated external noise is spatially non white [\cite=Coleman00] [\cite=Abramovich13b], and some parts of HF spectrum (distress signals for example) with no signals may be used for external noise sampling [\cite=Abramovich14]. Despite its relevance in many practical situations, this problem has been relatively scarcely studied [\cite=Abramovich04] [\cite=Werner06]. For parametric description of the Gaussian noise covariance matrix with [formula] the unknown parameter vector, in [\cite=Werner06], the authors derive the Cramér-Rao bound for joint SOI parameters (DoA) [formula] and noise parameters [formula] estimation, assuming a conventional unconditional model, i.e., [formula] and [formula] where [formula] stands for the complex Gaussian distribution whose respective parameters are the mean, row covariance matrix and column covariance matrix. [formula] is the usual steering matrix with [formula] the vector of unknowns DoA, [formula] denotes the waveforms covariance matrix and [formula] corresponds to the noise covariance matrix, which is parameterized by vector [formula].

In many cases however, the Gaussian assumption for the predominant part of the noise cannot be advocated. Typical example is the HF external noise, heavily dominated by powerful lighting strikes [\cite=George82] [\cite=CCIR83] [\cite=Radio13]. Evidence of deviations from the Gaussian assumption has been demonstrated numerous times for different applications, with the relevance of the compound-Gaussian (CG) models being justified [\cite=Conte87] [\cite=Baker91] [\cite=Rangaswamy93] [\cite=Billingsley99] [\cite=Conte04] [\cite=Conte05]. In essence, the individual M-variate snapshot of such a noise over the face of an antenna array may be treated as a Gaussian random vector, whose power can randomly fluctuate from sample to sample. CG models belong to a larger class of distributions, namely multivariate elliptically contoured distributions (ECD) [\cite=Anderson90] [\cite=Fang90] [\cite=Ollila12]. For the sake of clarity, we briefly review the main definitions of ECD. A vector [formula] follows an EC distribution if it admits the following stochastic representation

[formula]

where [formula] means "has the same distribution as". In [\eqref=storep_CES], Q is a non-negative real random variable and is independent of the complex random vector [formula] which is uniformly distributed over the complex sphere [formula]. The matrix [formula] is such that [formula] where [formula] is the so-called scatter matrix, and we assume here that [formula] is non-singular. The probability density function (p.d.f.) of [formula] can then be written as

[formula]

where [formula] stands for proportional to. The function [formula] is called the density generator and satisfies finite moment condition [formula]. It is related to the p.d.f. of the modular variate Q by p(Q) = δ- 1M,gQM - 1g(Q).

Going back to our scenario of two data sets [formula] and [formula], we assume that they are independent, and that their columns are independent and distributed (i.i.d.) according to [\eqref=p(x)]. In other words, one has [formula] and [formula], where Qtp and Qts are i.i.d. variables drawn from [formula], and [formula] and [formula] are i.i.d. random vectors uniformly distributed on the unit sphere. It then follows that the joint distribution of [formula] is given by [formula] where [formula] and where [formula]. Additionally, we assume that [formula] depends on a parameter vector [formula] while [formula] depends on [formula]. Our objective is then to estimate [formula] from [formula]. Let us emphasize an essential difference of the problem in [\eqref=p(Xp] [\eqref=Xs)] with respect to the typical problem of target detection in CG clutter [\cite=Sangston12]. There, within each range resolution cell the clutter is perfectly Gaussian and therefore the optimum space-time processing is the same as per the standard Gaussian problem formulation. It is the data dependent threshold and clutter covariance matrix (in adaptive formulation) that needs to be calculated from the secondary data, if not known a priori [\cite=Sangston12] [\cite=Pascal08]. In the problem [\eqref=p(Xp] [\eqref=Xs)], the SOI DoA estimation should be performed on a number Tp of ECD i.i.d. primary training samples, and maximum likelihood DoA estimation algorithm and CRB should be expected to be very different from the Gaussian case.

The paper is organized in the following way. In Section [\ref=section:CRB], we derive a general expression of the FIM for elliptically distributed noise using two data sets. Section [\ref=section:doaK] focuses on the case of DoA estimation in K-distributed noise. In section [\ref=section:crbK], we derive conditions under which the FIM is bounded/unbounded, and provide a sufficient condition for unboundedness of the FIM with general elliptical distribution. The maximum likelihood estimate, as well as an approximation, are derived in section [\ref=section:mleK]. In the same section, we derive lower and upper bounds on the mean-square error of the MLE for non-regular estimation conditions, i.e., when the Fisher information matrix is unbounded. Numerical simulations serve to evaluate the performance of the estimators in Section [\ref=section:numerical] and our conclusions are drawn in Section [\ref=section:conclu].

Cramér-Rao bounds

In this section, we derive the CRB for estimation of parameter vector [formula] from the distribution in [\eqref=p(Xp] [\eqref=Xs)]. The Fisher information matrix (FIM) for the problem at hand can be written as [\cite=VanTrees02]

[formula]

where we used the fact that

[formula]

Hence, the total FIM is the sum of two matrices [formula], with straightforward definition from [\eqref=FIM]. In order to derive each matrix, we will make use of the general expression of the Fisher information matrix for ECD recently derived in [\cite=Besson13] [\cite=Greco13]. First, let us introduce

[formula]

where [formula]. Then, we have from [\cite=Besson13] that the (j,k)-th element of the Fisher information matrices is given by

[formula]

[formula]

where [formula]. Since [formula] depends only on [formula], it follows that [formula] takes the following form

[formula]

with

[formula]

where [formula]. Let us now consider [formula]. Using the fact that [formula] depends only on [formula] and [formula] depends only on [formula], [formula] is block-diagonal, i.e.,

[formula]

with The whole FIM is thus given by

[formula]

The CRB for estimation of [formula] is obtained as the upper-left block of the inverse of the FIM and is thus simply [formula]. Similarly to the Gaussian case, the CRB for estimation of [formula] in the conditional model is the same as if [formula] was known. As for the CRB for estimation of [formula], it is the same as if we had a set of T = Tp  +  Ts noise only samples.

Application to K-distributed noise

Data model

We address the specific problem where the primary data can be written as

[formula]

where τtp follows a Gamma distribution with shape parameter ν and scale parameter β, i.e., its p.d.f. is given by

[formula]

which we denote as [formula], and [formula]. The noise component is known to follow a K distribution and [formula] in [\eqref=xt_CGK] admits a CES representation similar to [\eqref=storep_CES] with [formula]. The p.d.f. of Qtp in this case is given by

[formula]

where KM - ν(.) is the modified Bessel function. Note that the μ-th order moment of Qtp is

[formula]

where we used the fact [\cite=Gradshteyn94] that

[formula]

The density generator is thus here

[formula]

where, for the sake of notational convenience, we have dropped the subscript tp.

Cramér-Rao bounds

The FIM for K-distributed noise can be obtained from the FIM for Gaussian distributed noise and the calculation of the scalar

[formula]

for [formula]. For the signal parameters part only, we indeed have [formula] where the subscript K and G stand for K-distributed and Gaussian distributed noise. Using the fact that [formula], it follows that

[formula]

It then ensues that

[formula]

and thus

[formula]

A formula for the FIM in case of K-distributed noise was derived in [\cite=ElKorso14] based on the compound Gaussian representation [\eqref=xt_CGK]. While it resembles our derivations based on the FIM for ECD derived in [\cite=Besson13], it does not match exactly our expression herein. Moreover, we study herein the existence of the FIM and derive a closed-form approximation of the FIM.

Let us investigate the conditions under which the integral

[formula]

converges. Towards this end, let us use the following inequality which holds for M + 1 - ν > 1 and z > 0 [\cite=Baricz10]

[formula]

It follows that

[formula]

The first integral converges for [formula] while the second converges for 2μ  +  ν + M - 3 - M - 1 + ν  >  0  ⇔  μ  +  ν - 2 > 0. Hence, for μ  +  ν - 2 > 0, one has

[formula]

Accordingly, one has, for z > 0

[formula]

which implies that

[formula]

The first integral converges for μ  +  ν - 2 > 0 and the second converges for [formula]. In the former case, one has

[formula]

Consequently, we conclude that the integral converges only for μ  +  ν - 2 > 0: for μ = 2 this implies that ν > 0 which is verified. In contrast, when μ = 1, one must have ν  >  1. In other words, the term in the FIM corresponding to the noise parameters is always bounded since it depends on I2 only. The situation is different for signal parameters. In an unconditional model where [formula] would depend on signal parameters as well, the FIM is bounded. In contrast, in the conditional model where signal parameters are embedded in the mean of the distribution, the FIM corresponding to signal parameters is bounded only for ν  >  1: otherwise, it is unbounded. The latter case corresponds to the so-called non regular case corresponding to distributions with singularities, as studied e.g., in [\cite=Ibragimov81].

Before pursuing our study of the FIM for the specific case of K-distributed noise, let us make an important observation. For the K distribution, we have just proven that I1 does not exist for ν  ≤  1. However, see [\eqref=E{Q^mu}], [formula] exists if and only if μ + M  >  0 and μ  +  ν > 0. The latter condition implies that, when ν  ≤  1, [formula] does not exist. Observe that convergence of the latter integral is problematic in a neighborhood of 0, since for Q0  >  1, [formula] as p(.) is a density. Therefore, at least for K-distributed noise, if [formula] does not exist, then [formula] is unbounded. At this stage, one may wonder if this property extends to any other elliptical distribution. It turns out that this is indeed the case, as stated and proved in the next proposition.

Whatever the p.d.f. of the modular variate Qtp, if [formula] then [formula].

For the sake of notational convenience, we temporarily omit the subscript tp and use Q instead of Qtp. Let us first observe that

[formula]

Since p(Q)  =  δ- 1M,gQM - 1g(Q), one can write

[formula]

which implies that

[formula]

Therefore

[formula]

The third term of the sum is always positive. In the second term, we have that lim b  →    ∞p(b)  =  0. It follows that divergence of [formula] is a sufficient condition for divergence of [formula]. As said before [formula] exists, and therefore a sufficient condition for [formula] to be undounded is that [formula] is unbounded.

Let us now go back to the K-distributed case and investigate whether it is possible to derive a simple expression for Iμ and subsequently αμ, assuming that μ  +  ν - 2 > 0. Towards this end, let us make use of

[formula]

to write that

[formula]

The last term is obviously not possible to obtain in closed-form so that we use a "large M - ν" approximation of the modified Bessel function [\cite=NIST10]

[formula]

which results in

[formula]

Therefore,

[formula]

We finally have

[formula]

If the large M - ν approximation is made from the start, then one has

[formula]

so that

[formula]

and hence

[formula]

Figure [\ref=fig:alphamu_approx] compares the approximations in [\eqref=alpha_mu_approx_3terms] and [\eqref=alpha_mu_approx_1term], as well as a method which uses random number generation to approximate αμ based on its initial definition in [\eqref=alpha_mu_ini]. More precisely, we generated a large number of random variables [formula] and replace the statistical expectation of [\eqref=alpha_mu_ini] by an average over the so-generated random variables. As can be observed from Figure [\ref=fig:alphamu_approx], the 3 approximations provide very close values, which enable one to validate the closed-form expressions in [\eqref=alpha_mu_approx_3terms] and [\eqref=alpha_mu_approx_1term].

Maximum Likelihood estimation

We now focus on maximum likelihood (ML) estimation of direction of arrival φ0, signal waveforms stp and covariance matrix [formula] in the model

[formula]

where [formula], and [formula]. The joint distribution of [formula] is given by

[formula]

where [formula]. Joint estimation of all parameters appears to be very complicated and hence we will proceed in two steps. At first, we assume that [formula] is known and derive the ML estimates of φ and stp. Then, [formula] is substituted for some estimate obtained from observation of [formula] only.

DoA estimation with known [formula]

Assuming that [formula] is known, one needs to maximize with respect to φ and stp

[formula]

where g(.) is given by [\eqref=g_K]. Since g(.) is monotonically decreasing, see [\eqref=g'(q)], it follows that [formula] is maximized when the argument of g(.) is minimized. However,

[formula]

Therefore, for any φ, [formula] is maximized when

[formula]

It ensues that one needs now to maximize, with respect to φ

[formula]

with [formula]. In order to avoid calculation of a modified Bessel function and thus in order to simplify estimation, we propose to make use of the "large M - ν" approximation of the modified Bessel function given in [\eqref=approxK_largeM] to write

[formula]

This approximation results in an approximate maximum likelihood (AML) estimator of φ which consists in maximizing

[formula]

Note that

[formula]

which should be compared to the concentrated log likelihood function in the Gaussian case, as given by

[formula]

A few remarks are in order about these estimates, in particular about the behavior of the AML estimator in the case of unbounded FIM, i.e., when 0  <  ν  <  1 . First, note that all estimates will be a function of

[formula]

where [formula] is the projection onto the orthogonal complement of [formula]. Compared to [\eqref=log_f_G(phi)], the logarithm operation in [\eqref=ftilde(phi)] will strongly emphasize those snapshots [formula] for which [formula] is small. Let us thus investigate the properties of this statistic, when evaluated at the true value of signal DOA φ0. Using the fact that [formula], where [formula] and [formula] is a short-hand notation for [formula], one has

[formula]

For small ν (0  <  ν  <  1), it follows that, in the vicinity of φ0, the snapshot with minimal [formula] is more or less the snapshot for which τtp is minimum, hence the snapshot for which noise power is minimum, which makes sense. If we let uTp  =   min 1  ≤  tp  ≤  Tpτtp, then its cumulative density function (c.d.f.) is given by

[formula]

which is shown in Figure [\ref=fig:cdf_min_tau_T=4]. Obviously, with small ν, the snapshot which corresponds to the minimum value of τtp exhibits a very high signal to noise ratio and, due to the emphasizing effect of the log  operation in [\eqref=ftilde(phi)], the performance of the AML estimator is likely to be driven mainly by this particular snapshot. This is illustrated in Figure [\ref=fig:MSE_Rknown_order_tau_vs_T_vs_nu] where we display the mean-square error (MSE) of the AML estimate which uses all Tp snapshots and the MSE of an hypothetical AML estimator which would use only the snapshot [formula] corresponding to the minimum value of τtp. The scenario of this simulation is described in the next section. This figure shows a marginal loss of the AML estimator using [formula] only, as compared to the full AML estimator, especially for small ν.

Let us thus analyze the behavior of the AML estimators. For the sake of notational convenience, let φTpK and φminK denote the AML estimator using Tp snapshots with K-distributed noise and the AML estimator using the snapshot [formula] corresponding to the minimal τtp, respectively. Observe that, when using a single snapshot [formula], minimizing [\eqref=ftilde(phi)] is equivalent to minimizing the Gaussian likelihood function in [\eqref=log_f_G(phi)] with Tp = 1. Since [formula] exhibits a high signal to noise ratio, φminK is close to φ0, one can make a Taylor expansion and relate the error φminK  -  φ0 to the error [formula] as

[formula]

where [formula] is some vector that depends essentially on the derivatives of [formula] [\cite=Renaux06] and whose expression is not needed here. One can simply notice that [formula] would be the same with Gaussian noise and a single snapshot, since maximizing [\eqref=ftilde(phi)] or [\eqref=log_f_G(phi)] is equivalent when one snapshot is used . This implies that

[formula]

Observe that [formula] is the mean-square error (MSE) that would obtained in Gaussian noise and a single snapshot, which is about Tp times the MSE obtained in the Gaussian case and using Tp snapshots, and the latter is approximately the Gaussian CRB. The MSE of φminK depends on [formula] where uTp is the minimum value of a set of Tp independent and identically distributed (actually gamma distributed) variables. Therefore, in order to obtain [formula], one must consider statistics of extreme values, a field that has received considerable attention for a long time, see e.g., [\cite=Gumbel35] [\cite=Gnedenko43] [\cite=Gumbel58]. It turns out that only asymptotic (as Tp  →    ∞  ) results are available and we build upon them to derive the rate of convergence of [formula]. First, note that

[formula]

Now since ν is small and Tp is large, T- 1 / νp is very small and we can approximate [formula], which yields

[formula]

It follows that asymptotically, vTp  =  T1 / νpuTp converges to the distribution in [\eqref=cdf_vTp], whose probability density function is

[formula]

Using integration by parts, it follows that

[formula]

One can then conclude that, as Tp goes to infinity,

[formula]

Therefore, in the case of 0  <  ν  <  1, the MSE of φminK decreases as T- 1 / νp, a rate of convergence much faster than the usual T- 1p. Note that this case corresponds to unbounded FIM. Such rates of convergence are also found with distributions possessing singularities [\cite=Ibragimov81].

As for the AML estimate obtained from Tp snapshots, namely φTpK, its MSE is upper-bounded by that φminK (since it uses all snapshots, including [formula]), and is lower-bounded by the MSE that would be obtained if τtp = uTp for [formula], and this MSE is T- 1p times the MSE of φminK. Additionally, as said before, we have [formula] where CRBTpG(φ0) is the Gaussian CRB using Tp snapshots. Hence, one can bound the MSE of φTpK as

[formula]

As will be illustrated in the next section, the upper bound is rather tight, while the lower bound is much lower than the actual MSE.

Estimation of [formula] using secondary data

When [formula] is not known, then the secondary data [formula] can be used to estimate it. The maximum likelihood estimator is obtained (for T  ≥  M) as the solution (up to a scaling factor) to the following implicit equation [\cite=Ollila12]

[formula]

[formula] can be obtained through an iterative procedure, whose convergence is guaranteed under the assumptions made [\cite=Ollila12]. In order to avoid evaluation of the modified Bessel function, one can use the large M - ν approximation of [formula] in [\eqref=approx_ratioK] to define [formula] as the solution to the fixed-point solution

[formula]

Note that [formula] is more or less the well-known Tyler fixed-point estimator [\cite=Tyler87], which again can be obtained from an iterative procedure whose convergence is guaranteed [\cite=Pascal08] [\cite=Chitour08]. The drawbacks of the two above estimators are that 1)they are suited to a K distribution for the noise and 2)Ts is required to be larger than M. In order to gain robustness against these problems, a solution is to use normalized data [formula] whose distribution is independent of that of the noise, and to use regularization. More precisely, we suggest to resort to the following scheme [\cite=Abramovich07c] [\cite=Chen11] [\cite=Wiesel12] and define [formula] since convergence of this iterative scheme has been proved [\cite=Pascal14]. The very good performance of this scheme has been illustrated in various applications, see e.g., [\cite=Chen11] [\cite=Wiesel12] [\cite=Pascal14] [\cite=Abramovich13] [\cite=Besson13b], where discussions on how to select the regularization parameter η can also be found.

Numerical simulations

We assume a linear array of M = 16 elements spaced a half-wavelength apart and we consider the simple scenario of a single source impinging from [formula] embedded in unit power K-distributed noise. The covariance matrix [formula] is given by [formula] with ρ = 0.99. The exact and approximate maximum likelihood estimators, which consists in maximizing f(φ) in [\eqref=f(phi)] (φ) in [\eqref=ftilde(phi)] were implemented using the Matlab function fminbnd, and the maximum was searched in the interval [formula] where φ3dB is the half-power beamwidth of the array. The signal waveform was generated from i.i.d. Gaussian variables with power P and the signal to noise ratio (SNR) is defined as [formula]. The asymptotic Gaussian CRB, multiplied by the scalar α1  /  M was used as the bound for K-distributed noise. For the regularized covariance matrix estimator [formula] of [\eqref=FP_R(eta)], the value of η was set to η = 0.01. 1000 Monte-Carlo simulations were used to evaluate the mean-square error (MSE) of the estimates.

In Figures [\ref=fig:MSE_vs_T_Ts=32_nu>1] and [\ref=fig:MSE_vs_T_Ts=32_nu<1] we plot the CRB (for ν  >  1) or the lower and upper bounds of [\eqref=lowerbound_besselk] when ν  <  1, as well as the MSE of the ML and AML estimators, as a function of Tp, and compare the case where [formula] is known to the case where it is estimated from [\eqref=FP_R(eta)] with Ts = 32 snapshots in the secondary data. The following observations can be made:

there is almost no difference between the MLE and the AMLE, and therefore the latter should be favored since it does not require evaluating modified Bessel functions.

the MSE in the case where [formula] is known is lower than that when [formula] is to be estimated, which is expected. However, the difference is smaller when ν  <  1: in other words, it seems that adaptive whitening is not so much penalizing with small ν while it seems more crucial for ν  >  1. Indeed, for small ν, what matters most is the fact that some snapshots are nearly noiseless, and this is more influential than obtaining a very good whitening.

the decrease of the MSE for ν  >  1 is roughly of the order T- 1p. When ν  <  1, this rate is significantly increased and the MSE decreases very quickly as T- 1 / νp, as predicted by the analysis above. This rate of convergence is also observed in Figure [\ref=fig:MSE_multi_Rknown_vs_T_vs_nu] where we consider a scenario with two sources at [formula].

the upper bound in [\eqref=lowerbound_besselk] seems to provide quite a good approximation of the actual MSE, at least for Tp large enough.

The influence of Ts is investigated in Figure [\ref=fig:MSE_vs_Ts_T=16], where one can observe that about Ts = 64 is necessary for the performance with estimated [formula] to be very close to the performance for known [formula]. However, as indicated above, this is less pronounced when ν  <  1, where the difference becomes smaller with lower Ts.

Finally, we investigate whether the rate of convergence of the MLE or AMLE when ν varies is impacted by a small amount of Gaussian noise. More precisely, we run simulations where the data is generated as

[formula]

where [formula], i.e., the noise is a mixture of K-distributed noise and Gaussian distributed noise. The covariance matrix of the noise is now [formula] and we use the AML estimator assuming that the noise has a K distribution with parameter ν and known covariance matrix [formula]. In Figure [\ref=fig:MSE_mixture_vs_T], we display the MSE of the AML estimator versus Tp and versus ν for different values of α. Clearly, the rate of convergence of the estimator is affected by a small amount of Gaussian noise, even when ν is small. This indicates that, if noise is not purely K-distributed with small ν, we recover the usual behavior of the MSE versus Tp.

Conclusions

In this paper we addressed the DoA estimation problem in K-distributed noise using two data sets. The main result of the paper was to show that, when the shape parameter ν of the texture Gamma distribution is below 1, the FIM is unbounded. On the other hand, for ν  >  1, the FIM is bounded and we derived an accurate closed-form approximation of the CRB. The maximum likelihood estimator was derived as well as an approximation, which induces non significant losses compared to the exact MLE. In the non regular case where ν  <  1, we derived lower and upper bounds on the mean-square error of the (A)ML estimates and we showed that the rate of convergence of these (A)ML estimates is about T- 1 / νp where Tp is the number of snapshots.