Assumption Corrolary

Algorithm ().

Adaptive inexact fast augmented Lagrangian methods for constrained convex optimization

First author's work has been funded by the Sectoral Operational Programme Human Resources Development 2007-2013 of the Ministry of European Funds through the Financial Agreement POSDRU/159/1.5/S/134398.

Introduction

Large-scale constrained convex optimization models are convenient tools to formulate several practical problems in modern engineering, statistics, and economical applications. Several important applications in such fields can be modeled into this class of problems such as: linear (distributed) model predictive control [\cite=NecNed:13] [\cite=NedNec:14], network utility maximization [\cite=BecNed:14], or compressed sensing [\cite=AybIye:12] [\cite=QuoSav:12]. However, solving large-scale optimization problems is still a challenge in many applications due to the limitations of computational devices and computer systems, as well as the limitations of existing algorithm techniques.

In the recent optimization literature, the primal-dual first order methods have gained a great attention due to their low complexity-per-iteration and their flexibility of handling linear operators, constraints and nonsmoothness. In the constrained case, when complicated constraints are present, many first order algorithms are combined with duality or penalty strategies in order to process them. Amongst the most popular primal-dual approaches, which behaves very well in practice, the augmented Lagrangian (AL) strategy was extensively studied, e.g., in [\cite=Roc:76] [\cite=LanMon:14] [\cite=AybIye:12] [\cite=NecPat:15] [\cite=NedNec:14] [\cite=QuoCev:14] [\cite=HeTao:12] [\cite=HeYan:04]. It is well known that the AL smoothing technique is a multi-stage strategy implying successive computations of solutions for certain primal-dual subproblems. In general, these subproblems do not have closed form solutions, which makes the AL strategies inherently related to inexact first-order algorithms. In these settings, most of the complexity results regarding AL and fast AL algorithms are given under inexact first order information [\cite=LanMon:14] [\cite=AybIye:13] [\cite=NecPat:15] [\cite=NedNec:14] [\cite=QuoCev:14]. To our best knowledge, the complexity estimates for the fast AL methods have been studied only in [\cite=NedNec:14] [\cite=QuoCev:14]. However, in these papers only outer complexity estimates are provided. It is clear that the outer complexity estimates in AL methods do not take into account the complexity-per-iteration. In many situations we can choose the smoothing parameters in order to perform only one outer iteration, see, e.g., [\cite=AybIye:13] [\cite=NecPat:15] [\cite=LanMon:14], but the overall complexity estimate is still high due to the high complexity-per-iteration. Trading off these quantities is a crucial question in the implementation of AL methods.

In this paper we aim at improving the overall iteration complexity of fast AL methods using the inexact oracle framework developed in [\cite=DevGli:14] based on a simple inner accuracy update and excessive gap-like fast gradient algorithms [\cite=Nes:excessive:05]. By using the inexact oracle framework, we are able to provide the overall computational complexity of an inexact fast AL method with constant smoothing parameter and of its adaptive variant.

Our contributions. In this paper we analyze the computational complexity of Inexact Fast Augmented Lagrangian (IFAL) method with constant and adaptive smoothing parameters. Using the inexact oracle framework [\cite=DevGli:14], our approach allow us to obtain clean and intuitive complexity results and, moreover, it facilitates the derivation of the overall computational complexity of our methods.

For the basic IFAL method with constant smoothing parameter, we derive [formula] outer complexity estimates corresponding to simple inner accuracy updates. We also derive the overall computational complexity of order [formula] projections onto a primal set, in order to obtain an ε  -  optimal solution in the sense of the objective residual and feasibility violation.

Then, we show that for an optimal choice for the smoothing parameters we need to perform only one outer iteration. Based on this result, we introduce an adaptive IFAL method with variable smoothing parameters for which we prove an overall computational complexity of order [formula] projections.

We show that our adaptive variant of inexact fast AL method is implementable, i.e. it is based on computable stopping criteria. Moreover, we compare our results with other complexity estimates for AL methods from literature and highlight the advantageous features of our methods.

Paper organization. The rest of this paper is organized as follows. In Section 2 we define our optimization model and introduce some preliminary concepts related to duality and inexact oracles. In Section 3 we introduce the inexact fast AL method with constant smoothing parameter and analyze its computational complexity. To improve this complexity, in Section 4 we study an adaptive parameter variant and provide its complexity estimate. Finally, in Section 5, we compare our results with other complexity results on AL methods from the literature.

Notations. We work in the Euclidean space [formula] composed by column vectors. For [formula] we denote the inner product 〈u,v〉  =  uTv and the Euclidean norm [formula]. For any matrix G we denote by [formula] its spectral norm.

Problem formulation and preliminaries

We consider the following linearly constrained convex optimization problem:

[formula]

where [formula] is a proper, closed and convex function, [formula] is a nonempty, closed and convex set, [formula], and [formula]. We use U* to denote the optimal solution set of set of [\eqref=problem], which will be assumed to be nonempty.

The goal of this paper is to analyze the optimization model [\eqref=problem] and to develop new inexact Augmented Lagrangian methods with convergence guarantees for approximately solving [\eqref=problem]. For this purpose, we require the following blanket assumptions which are assumed to be valid throughout the paper without recalling them in the sequel:

If f is strongly convex, it is well-known that the Lagrangian dual function associated with the linear constraint Gu  +  g  =  0 has the Lipschitz gradient. This setting has been extensively studied in the literature (see, e.g., [\cite=BecNed:14] [\cite=NecPat:14] [\cite=NecPat:15] [\cite=QuoCev:14] [\cite=NecNed:13]). Thus, in the rest of our paper we only assume f to be a convex function and not necessarily strongly convex. Due to the constraint, it is clear that the dual function d defined by [formula] is, in general, nonsmooth and concave, which induces the difficulties in the application of usual first order methods to the dual. Therefore, various (dual) subgradient schemes have been developed, with iteration complexities of order [formula] [\cite=NedOzd:09] [\cite=Nes:14].

Under additional mild assumptions, we aim in this paper at improving the iteration complexity required for solving the linearly constrained optimization problem [\eqref=problem]. Our approach relies on the combination between smoothing techniques and duality [\cite=NecSuy:08] [\cite=Nes:excessive:05] [\cite=QuoNec:15].

First, we briefly recall the Lagrangian duality framework as follows. The Lagrangian function and the dual function associated to the convex problem [\eqref=problem] are defined by: From Assumption [\ref=all_assumptions](ii) it follows that the convex problem [\eqref=problem] is equivalent with solving the dual formulation, i.e.:

[formula]

Our goal in this paper is to find an approximate primal solution for the optimization problem [\eqref=problem]. Therefore, we introduce the following definition:

Given a desired accuracy ε > 0, the point uε∈U is called an ε-optimal solution for the primal problem [\eqref=problem] if it satisfies:

This set of optimality criteria has been also adopted by Rockafellar in [\cite=Roc:76] in the context of classical augmented Lagrangian methods. Moreover, the above criteria has been also used by Nesterov in [\cite=Nes:15] for the analysis of primal-dual subgradient methods. It can be easily observed that once we have an ε-feasible point, i.e., ||Guε  +  g||  ≤  ε, then we can also obtain a lower bound on f(uε)  -  f*. Indeed, we have the relation:

[formula]

and thus [formula]. Moreover, from a practical point of view, it is sufficient to find an ε-feasible point uε satisfying f(uε)  -  f*  ≤  ε.

Given the max-min formulation [\eqref=classic_dual], one can intuitively consider a double smoothing of the convex-concave Lagrangian function. Regarding the dual function, one of the most widely known smoothing strategies for obtaining an approximate smooth dual function with Lipschitz continuous gradient is the augmented Lagrangian (AL) smoothing [\cite=AybIye:13] [\cite=Roc:76] [\cite=NecPat:15] [\cite=NedNec:14] [\cite=QuoCev:14]. Thus, we combine the AL technique with a smooth approximation of the primal function and define:

[formula]

where μ,ρ  >  0 are two smoothness parameters. Clearly, we have the relation [formula] and, in addition, Lμρ(  ·  ,x) has Lipschitz continuous gradient with the Lipschitz constant [formula] for any fixed x. Based on this approximation of the true Lagrangian function we also define two smooth approximations of the primal and dual functions f and d, respectively:

[formula]

Let define the optimal solutions of the two previous optimization problems:

[formula]

Clearly, both functions fμ and dρ are smooth approximations of f and d, respectively. In particular, we observe that the smoothed primal function fμ has Lipschitz continuous gradient with the Lipschitz constant [formula]. Moreover, the smoothed dual function dρ is concave and its gradient [formula] is Lipschitz continuous with the Lipschitz constant [formula]. We emphasize again that, in most practical cases, uρ(x) cannot be computed exactly, but within a pre-specified accuracy, which leads us to consider the inexact oracle framework introduced in [\cite=DevGli:14] for the analysis of inexact first order algorithms.

Recall that a smooth function [formula] is equipped with a first-order (δ,L)-oracle if for any y∈Q we can compute [formula] such that the following bounds hold on φ (so-called inexact descent lemma) [\cite=DevGli:14]:

[formula]

If we define ũρ(x)∈U as the inexact solution of the inner subproblem in u satisfying:

[formula]

then using the notation [formula], we are able to provide the following important auxiliary result, whose proof can be found in [\cite=NecPat:15]:

Let δ > 0 and ũμ(x)∈U satisfy [\eqref=func_approx]. Then, for all x,y, we have:

[formula]

The relation [\eqref=inexact_oracle] implies that dρ is equipped with a [formula]-oracle with φδ,L(x)   =   L0ρ(ũρ(x),x) and [formula] =  Gũρ(x) + g. It is important to note that the analysis considered in [\cite=NedNec:14] [\cite=QuoCev:14] [\cite=QuoNec:15] requires to solve the inner problem with higher accuracy of order O(δ2), i.e.:

[formula]

in order to ensure bounds on dρ(x) of the form:

[formula]

where DU is the diameter of the bounded convex domain U. It is obvious that our approach in this paper is less conservative, requiring to solve the inner problem with less accuracy than in [\cite=NedNec:14] [\cite=QuoCev:14] [\cite=QuoNec:15]. As we will see in the sequel, this will also have an impact on the total complexity of our method compared to those in the previous papers.

Inexact fast augmented Lagrangian method

In this section we propose an augmented Lagrangian smoothing strategy similar to the excessive gap technique introduced in [\cite=Nes:excessive:05] [\cite=QuoNec:15] [\cite=QuoCev:14]. Typical excessive gap strategies are based on primal-dual fast gradient methods, which maintain at each iteration some excessive gap inequality. Using this inequality, the convergence of the outer loop of the algorithm is naturally determined.

In this paper, we use an excessive gap-like inequality, which holds at each outer iteration of our algorithm. Given the dual smoothing parameter ρ, inner accuracy δ and outer accuracy ε, we further develop an Inexact Fast Augmented Lagrangian (IFAL) algorithm for solving [\eqref=problem]:

The update rules for τk are derived below. We define as in [\cite=Nes:excessive:05] [\cite=QuoNec:15] [\cite=QuoCev:14] the smoothed duality gap Δk  =  fμk(uk)  -  dρ(xk). Based on this smoothed duality gap Δk, we further provide a descent inequality, which will facilitate the derivation of a simple inner accuracy update and of the total complexity of the IFAL algorithm.

Let ρ > 0 and {(uk,xk)} be the sequences generated by the IFAL Algorithm. If the parameter τk∈(0,1) satisfies [formula] for all k  ≥  0, then the following excessive gap inequality holds:

[formula]

Firstly, we observe that, from strong convexity of [formula] results:

[formula]

Secondly, given any [formula], from the definition of L0ρ, note that we have:

[formula]

Multiplying both sides of [\eqref=estimate_deltak] with 1  -  τk and combining with [\eqref=expression_linear_dual], we obtain:

[formula]

Using the convexity of f we further have:

[formula]

Further, from the left hand side of inexact descent lemma [\eqref=inexact_oracle] we get:

[formula]

Using the assumption that [formula] and the right hand side of inexact descent lemma [\eqref=inexact_oracle] we further derive: Choosing [formula] we obtain our result.

Similar excessive gap inequalities have been proved in [\cite=Nes:excessive:05] [\cite=QuoNec:15] [\cite=QuoCev:14] and then used for analyzing the convergence rate of excessive gap algorithms. However, the main results in [\cite=Nes:excessive:05] [\cite=QuoNec:15] [\cite=QuoCev:14] only concern with the outer iteration complexity and do not take into account the necessary inner computational effort for finding ũρ(  ·  ), since it is very difficult to estimate this quantity using the approaches presented in these papers.

In the sequel, based on our approach, we provide the total computational complexity of the IFAL algorithm (including inner complexity) for attaining an ε  -  optimal solution of problem [\eqref=problem]. First, we notice that if we assume [formula], by taking into account that [formula], a simple choice of sequence τk satisfying [formula] is given by [formula].

Let [formula] and [formula]. Let {(uk,xk)} be the sequences generated by the IFAL(ρ,ε) Algorithm. If we choose the inner accuracy [formula], then the following estimates on objective residual and the feasibility violation hold:

[formula]

From [\eqref=lemma_delta] it can be derived that:

[formula]

Observing that [formula], we can further bound the cumulative error as follows:

[formula]

Using this bound and f*  ≥  dρ(x) for any [formula], from [\eqref=rate_conv_aux] we have:

[formula]

On the other hand, using the KKT conditions of [\eqref=problem], we have:

[formula]

which is the first estimate in [\eqref=eq:opt_ests] Combining [\eqref=rate_conv] and [\eqref=feas_relation_aux], we obtain:

[formula]

which is the second estimate of [\eqref=eq:opt_ests].

Now, we provide the overall computational complexity of the IFAL Algorithm in terms of the number of projections on the primal set U.

Let [formula] and [formula]. Assume that at each outer iteration k of the IFAL(ρ,ε) Algorithm, Nesterov's optimal method [\cite=Nes:04] is called for computing an approximate solution ũρ(x̂k) of the inner subproblem [\eqref=inner_subproblem] such that [formula]. Then, for attaining an ε  -  optimal solution in the sense of Definition [\ref=opt_point], the IFAL Algorithm performs at most: projections on the simple set U, where the constant γ has the following expression: [formula].

Note that from Theorem [\ref=th_outer_rate_conv] we observe that for attaining an ε  -  optimal solution, the number of outer iterations [formula] must satisfy: On the other hand, at each outer iteration k, Assumption [\ref=all_assumptions](iii) implies that Nesterov's optimal method [\cite=Nes:04] applied on the inner subproblem [\eqref=inner_subproblem] performs:

[formula]

inner iterations (i.e., projections on U). Using this estimate we can easily derive the total number of projections on the simple set U necessary for attaining an ε  -  optimal point:

[formula]

The theorem is proved.

We observe that Δ0  ≤  0, where 0  =  fμ0(u0)  -  L0ρ(ũρ(x0),x0)  +  δ0 can be computed explicitly. Then, using this upper bound in our estimates, makes Algorithm IFAL implementable, i.e., the algorithm stops with an ε  -  optimal solution at the outer iteration [formula] provided that the following two computable conditions hold: Moreover, as suggested, e.g., in [\cite=QuoNec:15] [\cite=QuoCev:14], if we choose the primal-dual starting points u0  =  ũρ(0) and [formula], respectively, then we even have Δ0  ≤  δ0.

Adaptive inexact fast augmented Lagrangian method

In this section we analyze the overall iteration complexity of the IFAL Algorithm for an optimal choice of the smoothing parameter ρ and then we introduce an adaptive variant of this with the same computational complexity (up to a logarithmic factor) that is fully implementable in practice.

First, assume that we adopt the initialization strategy suggested in Remark [\ref=remark1] such that Δ0  ≤  δ0. Therefore, using this strategy and the previous assumption that [formula], the outer complexity can be estimated as:

[formula]

Note that the variation of the smoothing parameter ρ induces a trade-off between the number of the outer iterations and the complexity of the inner subproblem, i.e. for a sufficiently large ρ we have a single outer iteration, but a complex inner subproblem. The next result provides an optimal choice for ρ (up to a constant factor), such that the best total complexity is obtained. For simplicity of the exposition, we assume [formula].

Let [formula] and [formula]. Assume that, at each outer iteration k of the IFAL(ρ,ε) Algorithm, Nesterov's optimal method [\cite=Nes:04] is called for computing an approximate solution ũρ(x̂k) of the inner subproblem [\eqref=inner_subproblem] such that: Then, by choosing the smoothing parameter: for attaining an ε  -  optimal solution of [\eqref=problem], the IFAL Algorithm performs at most: projections on the simple set U.

First, we observe that for [formula], the IFAL Algorithm performs a single outer iteration. Indeed, from [\eqref=outer_compl_optimal] one can obtain γ̃  ≤  ε1 / 2 provided that: It can be seen that any ρ such that [formula] satisfies both conditions. In this case, when a single outer iteration is sufficient, the total computational complexity is given by the inner complexity estimate [\eqref=inner_Nesterov]: On the other hand, if [formula], then from [\eqref=outer_compl_optimal] we can further bound [formula] as follows:

[formula]

Using the same inner complexity estimate [\eqref=inner_Nesterov] of the Nesterov's optimal method, the total computational complexity is given by:

[formula]

Using [\eqref=outer_bound_2] for the second term in the right hand side of the above estimate, and optimizing over ρ we obtain that, for the optimal parameter [formula], the necessary number of outer iterations is at most 2. In conclusion, the choice [formula] is optimal up to a constant factor.

If one knows a priori an upper bound on [formula], then the previous result indicates that a proper choice of ρ determines that a single outer iteration of the IFAL Algorithm to be sufficient to attain ε-primal optimality and the overall iteration complexity is of order [formula], which is better than [formula] for any ρ > 0. However, in practice [formula] is unknown and thus the optimal smoothing parameter ρ cannot be computed. In order to cope with this problem, we provide further an implementable adaptive variant of the IFAL Algorithm, which has the same total complexity given in Theorem [\ref=th_total_compl_optimal] (up to a logarithmic factor). The Adaptive Inexact Fast augmented Lagrangian (A-IFAL) algorithm relies on a search procedure which is used typically for penalty and augmented Lagrangian methods in the case when a bound on the optimal Lagrange multipliers is unknown (see, e.g., [\cite=LanMon:14]).

We notice that the A-IFAL Algorithm can be regarded as a variant of the IFAL Algorithm with variable increasing smoothness parameter ρ and constant inner accuracy [formula]. The following result provides the overall complexity of the A-IFAL Algorithm necessary for attaining an ε  -  optimal solution.

Let {(uk,xk)} be the sequences generated by the A-IFAL Algorithm. Then, for attaining an ε  -  optimal solution of [\eqref=problem], the following total number of projections on U need to be performed:

[formula]

We observe that the maximum number of outer iterations performed by the A-IFAL Algorithm is given by [formula]. Thus the overall iteration complexity is given by:

[formula]

It is important to note that the adaptive A-IFAL algorithm has the same computational complexity as the IFAL algorithm, up to a logarithmic factor. Moreover, both algorithms are implementable, i.e., they can be stopped based on verifiable stopping criteria and their parameters can be easily computed.

Comparison with other augmented Lagrangian complexity results

In this section we compare the computational complexity and other features of the IFAL/A-IFAL Algorithm with previous works and complexity results on AL methods.

Given x∈U,r > 0, we use the notations [formula] and [formula]. Then, in [\cite=LanMon:14], the authors analyze the classical AL method for the same class of problems [\eqref=problem]. They developed an implementable variant of the classical AL method to obtain an ε  -  suboptimal primal-dual pair (uε,xε) satisfying the following criteria:

[formula]

The authors provide their own iteration complexity analysis for the augmented Lagrangian method using the inexact dual gradient algorithm and without any artificial perturbation on the problem they obtained that it is necessary to perform [formula] projections on the simple set U in order to obtain a primal-dual pair satisfying [\eqref=lan_criteria]. An important remark is that, for some ρ > 0, the method in [\cite=LanMon:14] requires a priori a pre-specified number of the outer iterations in order to compute the inner accuracy and to terminate the algorithm. Moreover, to satisfy our ε  -  optimality definition, an average primal iterate [formula] has to be computed (see [\cite=NedNec:14]). Further, for any fixed ρ, [formula] outer iterations has to be performed and the inner accuracy has to be chosen of the form [formula]. In this case, the method from [\cite=LanMon:14] requires:

[formula]

total projections on the set U, provided that [formula]. However, we observe that for [formula], from [\eqref=total_lan] the overall iteration complexity is of order O(1 / ε) (as in the present paper), while for an arbitrary constant parameter ρ, the complexity estimates are much worse than those given in this paper.

It is important to note that although the inexact excessive gap methods introduced in [\cite=QuoNec:15] [\cite=QuoCev:14] are similar to the IFAL Algorithm and the authors also provide outer complexity estimates of order [formula] for a fixed ρ, the update rules for the inner accuracy in [\cite=QuoNec:15] [\cite=QuoCev:14] induce difficulties in the derivation of the total complexity. Moreover, assuming one implements the update rule of the inner accuracy δk given e.g. by [\cite=QuoCev:14], at each outer iteration it is required a primal iterate ũρ(x) satisfying:

[formula]

For a small constant ρ and high accuracy, the theoretical complexity of the algorithm can be very pessimistic. Moreover, from our previous analysis we can conclude that for an adequate choice of the parameter ρ the number of outer iterations is 1 and therefore outer complexity estimates are irrelevant for the total complexity of the method.

Other recent complexity results concerning the classical AL method are given, e.g., in [\cite=AybIye:12] [\cite=AybIye:13] [\cite=NedNec:14]. For example, in [\cite=AybIye:13] an adaptive classical AL method for cone constrained convex optimization models is analyzed. However, the method in [\cite=AybIye:13] is not entirely implementable (the stopping criteria cannot be verified) and the inner accuracy is constrained to be of order: where β > 1. The authors in [\cite=AybIye:13] show that the outer complexity is of order O( log (1 / ε)) and thus, the total complexity is similar with the estimates given in our paper (up to a logarithmic factor). However, our IFAL Algorithm represents an accelerated augmented Lagrangian method based on the excessive gap theory and moreover, it can be easily implemented in practice, i.e., it can be stopped based on verifiable stopping criteria and the parameters can be easily computed.

Concluding remarks

We have analyzed the iteration complexity of several inexact accelerated first-order augmented Lagrangian methods for solving linearly constrained convex optimization problems. We have computed the optimal choice of the penalty parameter ρ and by means of smoothing techniques and excessive gap-like condition, we provided estimates on the overall computational complexity of these algorithms. We compared our theoretical results with other existing results in the literature. The implementation of these algorithms, numerical simulations, and comparison can be found in the forthcoming full paper.