What is Learning ? A primary discussion about Information and Representation

Introduction

Although machine learning or artificial intelligence researchers know that we are not anywhere near the technique which could enable us to build a machine with real intelligence, almost all discussion about various properties of Intelligence stay on a philosophical level, such as the ability of learning, the ability of logical reasoning, self-consciousness, or even emotion. Formal mathematical definitions of these properties are very undefined, and we do not even have any binary criteria to verify whether a system really possesses any of these abilities, let alone formal methods which can be used to analyse the level of each of these abilities.

By generalising our intuitive understanding of learning ability, this paper gives a concise necessary condition for being a learning model. And this paper also presents further research directions which would enable us to analyse whether a model will be a learning model or non-learning model during the development process.

The discussion starts with intuitive introductions that prepare readers with conceptual understanding of the viewpoint of the following formal discussion.

Intuitive introduction: Information

We are surrounded and processing various information constantly. And certainty plays a critical role in representing information. Because one piece of information must be carried by at least one certain representation.

"I'm 188cm" : in this example a certain number together with a unit (cm) represents the information on height, the first person pronoun represents an identity related to the height information.

"Turn left ? Right!" : in this example, the uncertainty of context brings ambiguity.

Even when we are measuring the uncertainty of a system (entropy), we will get a certain number or a range with certain boundary; or if we cannot get a certain boundary, we still have the certain representation of the system we are measuring; or even if we do not know what system we are measuring, we still have a certain definition of entropy; and if we have no certainty about anything, then there is no information at all. In other words, the amount of certainties also equivalent to the amount of information. Therefore, we can summary Feature One of information as follows:

One piece of information is equivalent to at least one certain representation, and vice versa.

Intuitive introduction: Learning

We as human cannot sense vast amount of physical realities naturally, such as the existence of almost all submicroscopic particles, the existence of majority region of electromagnetic wave, the movement of earth, the fact that the Earth is a sphere, actually this list could keep growing and includes almost all knowledge we have learned since the born of modern science. And we are all very familiar with the process of learning new knowledge based on known knowledge, and based on new learned knowledge to learn even newer knowledge. Firstly, this paper will give a generalised definition of the relative relation between the known knowledge and new learned knowledge. The introduction of this definition starts with an example:

A man was charged with murder, judge or juror has no information about the reality of innocent or guilty. A reasonable sentence must be the consequence of implementing proper methods on all evidences.

The evidence is local information, and the reality of innocent or guilty is global information. So the same as the known knowledge (local information) and new learned knowledge (global information) as introduced above. Actually, the notion of "no observation independent reality " has been widely accepted for decades. The most important thing is, the global and local relation not only exists in learning high level abstract concept, but also exists in learning very fundamental concepts, such as object identification. Because we have no direct access to the world[\cite=luft2011subjectivity] other than though our sensors. The information of the existence of an object (global information) is the consequence of learning based on its different possible appearances (local information) received by our retina. And Feature One of information tells us there must be a certain representation of this object (global information).

Further discussion of learning (section 3) is after a formal discussion about information (section 2). These sections are all based on the explanation of mapping relations from a new viewpoint.

Information

Information Generator and Representation

Suppose there is a mapping relation between a set X and a set Y, so any subset of X can be regarded as being generated by a generator, and the corresponding elements of subset of Y are representations of this generator. The joint of any two subsets of X, or any two corresponding subsets of Y are not necessarily to be null.

Domain and Range of a Mapping Relation

When a mapping relation F is defined between domain D and range O. Then, with respect to F, any subset of its domain could correspond to an unknown generator, and the corresponding elements in the range of that domain are the representation of that unknown generator. Then, with respect to F (could be a function), we can define local information and global information as follows:

Local information: Any input x∈D is a local information with respect to F.

Global information: Any element of subset Y  ⊂  O and the corresponding subset X  =  {x|y∈Y,y  =  F(x)} are global information.

So the global information y∈Y,Yn  ⊂  O represents an unknown generator Gn. It is obvious that an unknown information generator Gn could have a lot global representations: all elements of Yn, and due to the Feature One of information, we can say that all elements of Yn should follows a certain constraint c. And if the corresponding elements of domain subset Xn do not follow the same constraint c, we can say that the global representation (y∈Yn) of the information generator Gn is the invariant representation with respect to its appearances ([formula]).

Example One:

Domain: A CCD array, or our retina all provide a very large domain. If the value of each pixel is between 0-255 and the size of the image is 200*200 pixels, than the corresponding size of the domain is 25640000, which is a very large number.

Local information: Every image of a dog, every possible moment captured by our retina, these are all local information.

Global information: The existence of the dog, and all its appearances are global information. But we can learn the invariant representation form local information, a camera cannot.

Information (generator) Type

It is obvious that different mapping relation pairs different subsets between domain and range. Therefore, we can say that a mapping relation along with its domain define the type of unknown generator Gn, or in other words defines the type of information.

Common constraint:Linear separable hypersurface

As introduced in section 2.2, global information must follow a certain constraint c, so the certainty of information can be guaranteed. There are a large number of possible constraints, and we denote the set of all possible constraint as C. Now we would like to know which constraint is preferable? and Why? The following discussion gives the answer of this two questions.

Assumption One:

Within certain type of information, there exists a mapping relationship that can define the differences between information generators.

Based on Assumption One and Feature One of information, we know that a mapping relation, denoted as FC, map different subsets generated by unknown generators to global information that distinguishable by applying constraint cR. By choosing the simplest constraint cR that each y∈O represent a distingusiable information generator, so the expression of FC is:

[formula]

According to this expression, the differences of information generators can be identified as follow:

A set X1  ⊂  D is the global information of an unknown information generator G1, then y1  =  FC(x):x∈X1,y1∈R ; another set X2  ⊂  D is the global information of an unknown information generator G2, then we have y2  =  FC(x):x∈X2,y2∈R.

If y1  ≠  y2, then we can say that G1 and G2 are different information generators, and the type of information is defined by FC.

Furthermore, for real numbers in range O, they can be represent by parallel hyperplanes defined by a vector set VC, so we can say that points on these hyperplanes are population representations (global information) of different unknown information generators, and this also explain why researchers prefer the property of linear separable so much, because it is not only easy understanding, but also is the common constraint that every element of constraint set C can convert to.

Learning

Previous section shows a formal way of explaining local information and its global representation. What being learned is nothing more than information, or in other words, is nothing more than global representation, so the analysis of learning ability should based on the analysis of the mapping relation as introduced in previous section. And again this section starts with an intuitive example.

Example Two:

select dob from table1 where name = "wu hao"

y  =  2x, when x  =  2, we have y  =  4

P(is human | a picture of me) = 0.9999

People see picture 1, they know it is a pig. People see picture 2, they know it is a dragon. People see picture 3, even though they may have never seen this before and do not know the name, they still know it is different.

These four models in example two all include one input, a mapping relation (function), and one output. A is a database, B is a linear function, C is a powerful probabilistic model, and D is us. By analysing example two from the mapping relation point of view we know that:

: The database will return NULL for all inquiries that do not match what have been stored previously.

: Outputs will be different when the inputs are different. And for almost all regression problems, we have assumptions about the mapping relation between two subsets of our observations, and after modelling our hypothesis of the mapping relation, for a new set of input we can almost always expect the output set contains new element which we have never seen before.

: When applying probabilistic model, our hypothesis is based on the belief that there are unknown statistical laws which represent different concepts [\cite=vapnik2000nature] . Assume mapping relation Fmodel is able to mimic the unknown statistical law of the appearance of a cup perfectly , so the mapping relation we get is as follow:

At this stage, Fmodel is able to give the chance of being a cup for any element of the domain. What if we want Fmodel to have the ability to give the chance of being other object, such as a dog. Different from what we observed in regression problem, there is no real number naturally related to different objects. Therefore we chose another way of constructing our hypothesis of observation as follow:

Fmodel:RN  →  R  ×  I(I is indicator set)

But the introduction of this "Indicator Set" brings a problem. The size of this indicator set represent the information of the amount of objects Fmodel can effectively identify. This is a global information (denoted as #  I) with respect to identities of each object, and it is given by us. For objects which are not included in our hypothesis of Fmodel, the outputs are almost all clustered around 0.

We, as human with well developed vision, are always able to identify object which have never been seen before, or in other words, the function of our vision system are able to define new global information.

Necessary condition of Learning

Therefore, combining the discussion of information and its representation with above discussion, the intuitive understanding of the ability of learning is:

Learning means the ability to define new information generators

From this point of view, B and D are learning model, A and C are non-learning model. And by generalising the description above, we have a formal necessary condition of a mapping relation Fmodel to be a learning model as follow:

Condition S:

[formula]

For any subset XS of the domain D, the corrsponding subset of range is YS, exists a set XN which is a subset of the complement set of XS, so that we have a subset YN = Fmodel(XN), the symmetric differences of YN and YS is neither YS nor YN.

Generally, if a mapping relation Fmodel satisfies Condition S, then we can say Fmodel is able to learn its domain and it is a learning model, otherwise it is not able to learn its domain and it is a non-learning model. But there are actually two strategies which enable a non-learning model Fmodel to satisfy condition S, thus behaves like it is able to learn its domain.

Shrinking the scale of the domain.

Iterating over the domain

Example Three:

As shown in figure 8, in picture 1, function F* is defined on domain D, and it is not able to learn its domain because it does not satisfy condition S. But by shrinking its domain as shown in picture 2, F* behaves like it is able to learn most of its domain. This is strategy A

As shown in figure , in picture 5, a set of function {F1,F2,F3,F4} forms an approximation of function FL which satisfies condition S over its domain, but neither of these four functions satisfy condition S. This is strategy B.

Usually these two strategies are being applied together.

For neutralising the effect brought by strategy A and B, condition S needs to be extended a little bit.

Condition S*:

Suppose domain D is defined on N dimensional space, extend the scale of dimension M of the domain to infinite and mapping relation Fmodel still satisfies condition S, then we can say Fmodel is a learning model on domain D, dimension M.

If Fmodel satisfies condition S* on every dimension of its domain D, then we can say Fmodel is able to learn its domain D, and it is a complete learning model. On the other hand, as shown in figure 8 picture 5, the iteration process behaves similar with a database systems as shown in picture 3 and 4. And a database system is a typical memory system, so we can also say:

Not being able to satisfy condition S* is a sufficient condition for a model to be a Non-Learning model.

Non-Learning model and memory system are equivalent.

AI Effect

Fmodel contains our hypothesis about the mapping relation between different subsets of our observation. The ideal situation is our hypothesis could be as close to the unknown mapping relation as possible. One method of measuring the validity of our hypothesis is as follow:

Suppose unknown mapping relation is U:D  →  O, and our guess if H. So for any o  ⊂  O, we have T  =  U(o), and G  =  H(o), then J(T,G) is the validity of our hypothesis.

For simple problems, such as regression problem, our hypothesis works fine , but its only learn simple mapping relation. But when facing complex optimisation problems, for getting a desired mapping relation Fmodel, strategies A and B are usually being applied together and if researchers do not exam the property of information being used carefully, it is possible that information comes from outside of this structure can be introduced. Usually the choice of introducing information from outside the structure will help to reduce the difficulties of constructing a desired Fmodel or improve its performance. But this behaviour will eventually cause Fmodel fail to satisfy condition S*, as shown by the example of figure 7 (the probabilistic model).This somehow explains Rodney Brooks complain:

" Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation "

Problems and Feature Works

More necessary conditions or a sufficient condition

In this paper, condition S* is a necessary condition for a mapping relation to be a learning model, this does not rule out the possibility that some mapping relation MF could satisfy condition S* but still against our understanding of learning. Therefore, further discussion about condition on mapping relation is necessary.

Necessary condition on structural level

The discussion of previous section indicates that Condition S* of mapping relation is insufficient for analysing the structural detail of a given model, necessary condition on structural level is required to guide the construction process of getting a desired mapping relation and presumably would also give us a formal explanation about why introducing global information from outside the structure could make the problem easy to be solved or improve the performance but also causes losing the ability of learning at the same time.

Verifying state-of-the-art artificial intelligence systems.

Because of the overheated expectation of an realistic AI system and the following failure in 1970s, researchers usually avoid using the term "AI" since then. Instead they express the similar idea by implying that their systems are able to learn similar features as human brain could do or be able to exhibit unexpected behaviours.

These days, some systems have demonstrated impressive performances which not only make the discussion of AI a hot topic again but also raised the alert over the possibility of people may losing control of Artificial Intelligence. But no matter how powerful these systems could be, from machine learning point of view, they are still solutions of optimisation problems which based on different hypothesises of our observation. Therefore their ability of learning is independent from their performance and can be verified in the same way as the discussion of probabilistic model in section 3. The research results of problem one and two will provide more powerful tools for verifying the learning ability of a give model.

The common learning model

As discussed in section 2.4, linear separable is the common constraint that all constraints can convert to, therefore all learn models (include us) can convert to the common learning model which follows this common constraint. A prototype which demonstrates the common learning model theory is the zero to one step for harvesting information learned by future artificial intelligence system.

Begin with object recognition

The ability of learning is one property of an intelligent system, the learning result is not necessary to be right all the time, and the concept of "right" is also a global information which needs to be defined in future research.

The process of learning highly abstract information is tightly related to other part of intelligence, after all, during the past 300000 years' development of homo sapiens, most of our learned highly abstract realities are "Wrong" and comparing with realities we know today, it seems "overfitting" is a common phenomenon of the highly abstract information learning process (World Elephant, then Geocentric Theory). Therefore, further study of learning will focus on the learning process of relatively independent and low level abstract information, such as the object recognition problem.

Dataset separation and merge problem

When explaining the object recognition problem using the theoretical framework proposed in previous sections, two seemingly counterintuitive deductions are dataset separation and merge problems.

Dataset separation problem: When dataset generated by one object is separated , there could be two different set of invariant representations.

Dataset merge problem: When two dataset of different object are merged together, there could be a new set of invariant representations.

The description above is the common intuitive understanding of these two problem, but there are two mistakes about this understanding:

The information generators is defined by the mapping relation, so these two problems are equivalent.

Since there is no observation independent reality, this is not a problem, it is a phenomenon which can be used to verify the validity of future implementation based on this theoretical framework.

Note

It seems particularly hard for people to understand the dataset merge phenomenon in object recognition. It indicates that if we could combine two objects together, such as a dog and a cat, and guarantee the smoothness of the transformation of their appearances, then we will see them as a same thing(things). This confused conclusion exists only because we have the information of the existence of these two distinguishable objects at first, and there is no real world experience which bring us into this mental experiment. But we can always almost recognise "Optimus Prime", "Bumblebee", "Jazz" and other characters in transformers (the movie or the cartoon), no matter they appear as cars or human form robots. In this scenario, the recognition process of a transformer is no more difficult than recognise anything else and we take it for granted. Therefore, the confusion about cat and dog could be completely solved someday in the future which "Marvel" or "DC" decide to bring us a new superhero whose appearance transforms between cat and dog. And this phenomenon should be able to be verified experimentally if there are corresponding neuroscientific approaches.

Other related topics

With the progress of future works, some other related topics will help to provide a better understanding about the essence of learning and its relation with intelligence. These topics include: detailed analysis of non-learning model, formal discussion of overfitting problem in the process of learning highly abstract information, strategies that could make a learning model behaves like a non-learning model.

Conclusion

John Connor asked T-800:

"Can you learn stuff that you haven't been programmed with? so you could be...you know, more human? And not such a dork all the time?"

Even though that is just a scenario of a movie, still we all want to known the answer of this question. And for researchers, the most interesting part is how this question can be answered.

Machine Learning theories focus on solving different optimisation problems, so we could model hypothesises of our observation. While, the discussion of learning should focus on the behaviour of the model. In this paper, by analysing the relations among observable appearance(local information), reality (global information), their representation and the generalisation of our intuitive understanding of learning, we have a necessary condition S* which can be used to get the answer of John's question.

Starting from a viewpoint which has been missed out, the analysis in this report shows that, Learning is not an action of absorbing existed information. In contrast, learning is an action of define information generators, or more specifically, is the ability of always being able to define new information generators. And preliminary verification indicates that our hypothesis of what we observed could be a learning model which only learn trivial information, and when facing complex problems , lack of learning ability is a trade-off for reducing the complexity or improving the performance through introducing global information from outside the structure.

It is possible in the future that we could harvest the information generated by a learning model which can learn like us but with greater learning ability. And focusing on the direction of getting a common learning model which can learn like us, this paper illustrates the logic dependencies of many topics that related to understanding the essence of learning.