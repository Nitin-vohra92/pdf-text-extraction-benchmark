Synchronization in dynamical networks with unconstrained structure switching

Networked structures, in which sets of distributed dynamical systems interact over a wiring of links with non-trivial topology, are a key tool to investigate the emergence of collective organization in many systems of interest [\cite=Alb02] [\cite=Boc06] [\cite=Boc14]. The analysis of synchronized states is particularly relevant, as they play a crucial role in many natural systems, such as brain networks [\cite=Var01], or ecological communities [\cite=Ber99]. In the past decade, the emergence of synchronized states has been extensively reported and studied [\cite=Boc02], with a notable emphasis on the effect of complex static topologies on synchronization properties [\cite=Pec98] [\cite=Lag00] [\cite=Bar02] [\cite=Nis03] [\cite=Bel04] [\cite=Cha05] [\cite=Hwa05] [\cite=Mot05] [\cite=Yao09] [\cite=Li13] [\cite=Mas13]. Nonetheless, static network models do not adequately describe the processes that arise because of mutations in some biological systems, such as infectious bacterial population, which are known to have adaptive mutation rates that can become very highly elevated [\cite=Den06] [\cite=Csa07]. These require, instead, the use of time-dependent topologies whose evolution occurs over time scales that are commensurate with those of the node dynamics [\cite=And88] [\cite=Han04]. A powerful tool to assess the stability of the synchronous solution in networks of N identical nodes with diffusive coupling, is the so-called Master Stability Function (MSF) [\cite=Pec98]. If the evolution of such networks is along structures whose Laplacians commute at all times, synchronization can be significantly enhanced. In fact, studies have shown that it can be achieved even when the connection structure at each time would not allow a synchronized state, in the static case [\cite=Boc06_2] [\cite=Amr06]. The far more realistic situation of networks whose coupling matrices do not necessarily commute has also generated significant interest. This has led to the development of several results about synchronizability in systems with specific properties, including the study of the synchronous state in the case of moving neighbourhood networks [\cite=Por06], the rigorous derivation of sufficient conditions for synchronization in fast switching networks [\cite=Sti06], and the analysis of the system dynamics in the so-called blinking limit [\cite=Has13] [\cite=Has13_2]. In this Article, we propose a framework that allows to assess the stability of a synchronous solution by means of an MSF when the evolution of the topology is fully general and unconstrained, and without assuming any hypothesis on the time scales of the topological evolution.

To this purpose, let us consider a network of N identical systems, evolving according to

[formula]

Here dot denotes time derivative, [formula] is an m-dimensional row vector describing the state of the [formula] node, σ is the interaction strength, and [formula] and [formula] are two vectorial functions describing the local dynamics of the nodes and the output from a node to another, respectively. Also, [formula] is the Laplacian of the network describing the time evolution of the connections. In this expression, W is the weighted adjacency matrix, and S is the diagonal strength matrix of the network: [formula]. To fix the ideas, assume that the network has an initial structure, with Laplacian L0, that is constant from time t = 0 to t = t0. Also assume that, at time t1 > t0, the network is found in a configuration with Laplacian L1 ≠ L0. Note that in the following we consider Laplacians with Hermitian structure. Also, as [formula] is a Laplacian matrix, the sum of each row vanishes, its diagonal elements are strictly positive and its off-diagonal elements are non-positive. Now, let [formula] be the state vector indicating the synchronized solution, and define the mN-dimensional column vector [formula], representing the global deviation from the synchronized state. From Eq. [\ref=eq1], to linear order in [formula], one has

[formula]

where [formula] is the N-dimensional identity matrix, [formula] denotes the direct product, and [formula] is the Jacobian operator. The vector [formula] can be written at each time as a sum of direct products of the eigenvectors [formula] of [formula] and a time-dependent set of N m-dimensional row-vectors [formula]: Multiplying [formula] from the left to both sides of Eq. [\ref=eq:var], one gets

[formula]

where, for the sake of brevity, we defined [formula] and [formula], in which [formula] is the [formula] eigenvalue of [formula]. To assess the stability of this dynamical system, one can compute the Master Stability Function, which represents the dependence on ν of the largest Lyapunov exponent Λmax associated to the equations [\ref=variation]. Then, the stability criterion for a given ν is that the time averages of Λmax in the direction of all eigenvectors are negative. This allows to study systems with highly non-trivial behaviour. As an example, one can consider an evolving system where each "frozen" connection topology has at least one direction in which Λmax is positive, but the synchronization manifold is transversely stable [\cite=Boc06_2]. Similarly, one can detect instabilities introduced by the evolving nature of the Laplacian in systems where the synchronization manifold in each frozen configuration is attracting [\cite=Jos08]. This is particularly useful, as it is well-known that non-linear perturbations in a system can destroy the stability of the synchronized state [\cite=Bar02_2].

Note that using the MSF is not the only possible method to assess the stability of a synchronized state. For instance, one could construct the Lyapunov function for the synchronization manifold. This would guarantee the stability since it is a necessary and sufficient condition, while in the general case the MSF is only a necessary one. However, while it is certainly possible to build the Lyapunov function in some specific cases [\cite=Wu07], a general construction method is not known. Also, it should be noted that in the absence of fixed points or other attracting sets far from the synchronization manifold, the MSF provides a sufficient stability condition as well, thereby becoming a widely used approach [\cite=Sor11].

To use the MSF method, one must first note that the boxed term in Eqs. [\ref=variation] explicitly depends on the time variation of the eigenvectors of L. If the Laplacians L0 and L1 commute, one can choose to study the problem in the common basis of eigenvectors. In this reference frame the eigenvectors do not change. Thus, the boxed term vanishes and Eqs. [\ref=variation] reduce to a set of variational equations. However, if we allow the network to switch to a different structure without imposing the extra requirement of commutativity, the eigenvector variation must be taken into account. Note that this forbids instantaneous jumps between the two structures, as a sudden change in the eigenvectors would cause their derivatives to be not defined. Therefore, the goal is constructing a smooth evolution process from t = t0 to t = t1 to allow the system to evolve between between the two topologies while keeping the eigenvector elements differentiable. To achieve this, we first consider the matrices A and B, consisting of the eigenvectors of L0 and L1, respectively, and describe how to transform one into the other via a proper rotation around a fixed axis. Then, we use this framework to find a transformation between L0 and L1. The rotation evolving A into B takes the form of a one-parameter transformation group Gs such that G0A = A and G1A = B. Note that, as we will use this mapping to build the Laplacian, we must also impose the extra requirement that the vector [formula], corresponding to the null eigenvalue, is kept constant by Gs for all [formula].

In general, the transformation O from A to B is a rotation, which can be found by solving the linear system of N2 equations in N2 variables OA = B. It is convenient to work in the basis defined by the matrix A. In this basis, [formula]. Without loss of generality, assume that the conserved vector [formula] is the first vector of A. Then, the transformation matrix O has the form

[formula]

As [formula], it is a proper rotation if its determinant is 1, or a composition of rotations and reflections if its determinant is - 1. The determinant of O equals the determinant of the minor O' obtained by removing the first row and the first column from O. Thus, we only need to find a solution to the problem in N - 1 dimensions. We will henceforth use primes when referring to objects in N - 1 dimensions.

From the considerations above, it is [formula] G'1 = O', and, of course, [formula]. Thus, the problem is equivalent to determining the possibility of finding a path between the identity and O'. If [formula], then [formula]. But [formula] is the connected identity component of the orthogonal group, and additionally, since it is a manifold, it is path-connected [\cite=War71]. Thus, for every orthogonal [formula] matrix in it, there is a continuum of orthogonal matrices of the same dimension connecting it to the identity. Each point along this path corresponds to an orthogonal matrix that can be embedded in [formula] by adding a 1 in the top left corner. Since every such embedded matrix keeps the synchronization manifold vector invariant, a parametrization of the path provides a direct solution to the original problem.

If, instead, [formula], then [formula]. While [formula] is also a connected topological space, the identity does not belong to it [\cite=War71]. Thus, no path connects the identity to O'. However, in our case, the labeling of the vectors is irrelevant. In other words, provided that the vector [formula] is kept constant, one can arbitrarily swap two vectors in the basis given by the matrix A, obtaining a new matrix C. Of course, this imposes a swap of the corresponding columns in the transformation matrix O as well. But swapping two rows in a matrix changes the sign of its determinant. This means that the new matrix [formula], and a path connecting it to the identity can be found. The only consequence of the swap is a change of the order of the eigenvalues. As we are considering unlabelled networks, the problem can always be solved. To build an explicit solution, we factor the transformation O' into rotations and reflections in mutually orthogonal subspaces.

Before describing the actual procedure, we recall a useful, twofold result. First, any orthogonal operator X in a normed space over [formula] induces a 1- or 2-dimensional invariant subspace. To find one such subspace, first define the unitary operator U acting on [formula] as [formula], where [formula] and [formula] belong to [formula]. Then, find a non-vanishing eigenvector [formula] of U. The span of [formula] and [formula] defines the invariant subspace: applying X to any linear combination of [formula] and [formula] produces a vector that is still a linear combination of [formula] and [formula]. Also, if the corresponding eigenvalue is complex, [formula] and [formula] are orthogonal.

Using this, we can describe the following algorithmic procedure to build a transformation O of A into B:

Express the problem in the basis A, in which O has the form of Eq. [\ref=tmat].

Consider the operator O' obtained from O by removing the first row and the first column, and let d be its dimension.

Build the operator U that acts on [formula] as [formula], where [formula] and [formula] belong to [formula].

Find an eigenvector [formula] of U, with eigenvalue λ.

Normalize [formula] and [formula].

If [formula] then

Pick the non-vanishing component between [formula] and [formula]. If both are non-zero, choose one randomly. Without loss of generality, assume this is [formula].

Create d - 1 other orthonormal vectors, all orthogonal to [formula], and arrange all these vectors so that [formula] is the last of them. This set of vectors is an orthonormal basis C of [formula].

Change the basis of the d-dimensional sub-problem to C. In this basis, all the elements in the last row and in the last column of O' will be 0, except the last one, which will be ±  1.

If d > 1, consider a new operator O' obtained from the old O' by removing the last row and the last column. Let d be its dimension, and restart from step 3. Otherwise stop.

If instead [formula], then

Create d - 2 other orthonormal vectors, all orthogonal to [formula] and [formula], and arrange all these vectors so that [formula] and [formula] are the first two of them. This set of vectors is an orthonormal basis C of [formula].

Change the basis of the d-dimensional sub-problem to C. In this basis, all the elements in the first two rows and in the first two columns of O' will be 0, except the first two.

If d > 2, consider a new operator O' obtained from the old O' by removing the first two rows and the first two columns. Let d be its dimension, and restart from step 3. Otherwise stop.

At each iteration of the steps above, 1 or 2 dimensions are eliminated from the problem. All the subsequent changes of base leave the already determined elements of O unchanged, because they act on orthogonal subspaces to those already eliminated. The procedure reconstructs O piece by piece with a block-diagonal form, in which the blocks correspond to the action of the orthogonal operator on the invariant subspaces. If the subspace is 1-dimensional, then the block is a single [formula] element. If instead the subspace is 2-dimensional, then its block is either a rotation or a reflection, i.e., it is either [formula] or [formula]. Thus, the 1-dimensional invariant subspaces induced by the operator correspond either to leaving one direction untouched, or to reflecting the system about that direction. Conversely, the 2-dimensional invariant subspaces correspond to rotations in mutually orthogonal planes.

Once this form of O is found, permute the basis vectors from the second onwards so they correspond, in order, first to all the actual rotation blocks, then to the - 1 elements, and finally to the + 1 elements. The new form of the transformation matrix, ON, is simply ON = TOT- 1, where T is the required change-of-basis matrix.

Note that the determinant of ON could still be - 1. However, as seen before, in this case, one can relabel two vectors of the original basis, inducing a swap of the corresponding columns of ON. To perform this, first note that, if [formula], then the number of - 1 elements in ON must be odd. Then, there are three possible cases.

If O'N has at least one + 1 element, swap the basis vectors corresponding to the first - 1 and the first + 1 elements in O'. Then, the first block after the "sin -cos " blocks is [formula]. Swapping the labels of the the two corresponding vectors, makes the block in O' become [formula].

If instead O'N has no + 1 elements and only one - 1 element, the basis vectors to swap are those corresponding to the first two vectors in the last 3  ×  3 block of O', that becomes Now, perform one more basis change, leaving all the basis vectors unchanged, but mapping the last three into the eigenvectors of M, which are The new form of M becomes

Finally, if O'N has no + 1 elements and at least three - 1 elements, swap the basis vectors corresponding to the first two after the "sin -cos " blocks. Their 3  ×  3 block is now Next, do a change of basis as described in the previous case. The eigenvector matrix of M is so, after the basis change, the new form of M is once more

Regardless of the original value of [formula], take now every two subsequent - 1 elements, if there are any, and change their diagonal block into [formula]. This yields the final general form for the transformation matrix, which can be turned into the required transformation group via the introduction of a parameter [formula]:

Notice that when s = 0, [formula], and when s = 1, Gs = G1 = O. Also, for every value of s, the determinant of Gs is always 1, and the first vector is kept constant, which means that Gs describes a proper rotation around the axis defined by the eigenvector corresponding to the synchronization manifold. Moreover, the first vector has always been left untouched by all the possible basis transformations. Thus, as s is varied continuously between 0 and 1, the application of Gs sends A into B continuously, as needed.

Finally, to describe how to obtain a transformation between the two Laplacians L0 and L1, let R be the change-of-basis matrix resulting from the composition of all basis changes done in step 6 of the method, the permutation of the vectors to obtain O, and the possible final adjustment in case of negative determinant. Also, to simplify the formalism, in the following we let [formula] and [formula]. Since B0 and B1 are matrices of eigenvectors, it is [formula] and [formula], where D0 and D1 are the diagonal matrices of the eigenvalues of L0 and L1. Also, as Gs is a group of orthogonal transformations, it is [formula], where Bs is a basis of [formula]. Multiplying this by [formula] on the left yields

[formula]

where we used [formula].

Now, note that all the basis changes performed are between orthonormal bases. Thus R is an isometry, as is any Gs, since they are all proper rigid rotations, and any Bs defines an orthonormal basis of [formula]. Then, the Laplacian for the parameter s is given by the matrix Ls that solves the equation

[formula]

where Ds is a diagonal matrix whose elements are the eigenvalues of Ls, and Bs consists of the eigenvectors of Ls. However, the equation above has two unknowns, namely Ls and Ds.

This provides a certain freedom in describing the evolution of the eigenvalues of the Laplacians. For instance, one can choose the simplest evolution, which is given by a set of linear transformations. Then, for all [formula],

[formula]

where λ(0)i and λ(1)i are the [formula] eigenvalues of L0 and L1, respectively. Note that this allows for the possibility of degeneracy of some eigenvalues for some particular value of the parameter [formula]. However, the transformation we described leaves all the eigenvectors distinct and separate throughout the evolution. Thus, the Laplacian can always be diagonalized for any value of s. Multiplying Eq. [\ref=BTDT] on the right by [formula] yields [formula], hence [formula]. Multiplying this on the left by Bs, it is [formula], hence [formula]. Substituting Eq. [\ref=BTdef] into this last equation gives

[formula]

In the equation above, B0 is known, R and Gs have been explicitly built, and Ds is completely determined by Eq. [\ref=valueevol]. Thus, Eq. [\ref=LvsD] defines the Laplacian for any given value of the parameter s. The evolution of the eigenvalues is imposed by Eq. [\ref=valueevol], and the evolution of the eigenvectors is given by Eq. [\ref=BTdef].

It is important to stress that the solution given by the linear evolution of the eigenvalues is not unique. There could be, in principle, many allowed transformations of L0 into L1, each characterized by a specific eigenvalue evolution. However, the difference between solutions is only in the [formula] term in Eq. [\ref=variation], since the boxed term does not depend on the eigenvalues. Thus, the specific choice of eigenvalue evolution could change the phenomenology of the system studied, but would not modify how the switching between general structures affects the stability. This is akin to the results presented in Ref. [\cite=Boc06_2], which can be considered a special case of the present treatment that occurs when all the Laplacians commute.

With this result, one can finally describe the system evolution through unconstrained topologies. From t = 0 to t = t0, the boxed term in Eq. [\ref=variation] vanishes. To compute its value during the switch, first note that the [formula] eigenvector at time t is the [formula] column of Bs, with [formula]. But then, using Eq. [\ref=BTdef], the [formula] element of the [formula] eigenvector is

Notice that in the equation above the only term that depends on time is [formula], since R is just a change-of-basis matrix, and B0 is the matrix of eigenvectors of L0 at time t = 0. Therefore, it is

This allows a fully explicit expression for the boxed term in Eqs. [\ref=variation] that accounts for the time variation of the eigenvectors: However, for all practical purposes, one does not need to use the expression above directly. In fact, considering that most elements of Gs are 0, it is quite simple to compute and store Bs in a symbolic form. Similarly, most of the [formula] terms are 0. In fact, they vanish if r = 1, if q = 1, if [formula], if r > 2b + 1, and if q > 2b + 1, where b is the number of "sin -cos " blocks in Gs. Also, for all other cases [formula] is proportional to a sine or a cosine. Then, define [formula] to be the matrix whose [formula] element is [formula]; also, define [formula]. Again, [formula] can be easily computed and stored in a symbolic form. Then, Eq. [\ref=NCbox1] becomes

[formula]

Once more, the equation above can be computed symbolically, and evaluated at any particular t, when needed. Note that, despite the seemingly complex expressions, Eqs. [\ref=NCbox1] and [\ref=NCred] are straightforward to deal with. This is due to the fact that Gs and [formula] are always represented as tridiagonal matrices, and, as mentioned above, many of the non-trivial elements of [formula] vanish as well. Thus, numerical applications of this approach can benefit not only from a restricted amount of needed memory, but also from sparse matrix methods that result in a small computational complexity.

The treatment we built is valid for every positive, finite switching time [formula] between configurations. As explained above, this time cannot vanish, lest the derivatives in Eq. [\ref=variation] be not defined. Nonetheless, one can wonder about the behaviour of a system when the switching time becomes very small, although non-zero. To this purpose, first note that this time only appears as a multiplicative factor in Eq. [\ref=NCbox1]. Thus, a very small [formula] would have the effect of making the boxed term in Eq. [\ref=variation] much larger than the purely variational term. In this regime, the effects on the the stability of the synchronous solution are due mostly, if not exclusively, to the switching process. In other words, if the expression in Eq. [\ref=NCbox1] yields positive results, the synchronized state is made more stable in the corresponding direction, and vice versa for negative results, regardless of the contribution coming from the variational term. Note that this is in agreement with the finding that blinking networks can greatly facilitate synchronization [\cite=Bel04] [\cite=Sti06]. Similarly, one can consider the opposite limit, namely that of a secular switching for which [formula] becomes very large, while still remaining finite. In this case, for large enough switching time, the boxed term in Eq. [\ref=variation] becomes negligible compared to the rest, and the stability is determined entirely by the variational term. This case is very similar to that of an evolution along commutative structures [\cite=Boc06_2]. In fact, in this regime of quasi-static evolution, the structure at any given time t is, to first order, equal to the structure at time [formula]. Thus, it is [formula]. Therefore, one can treat this case as the commutative one with the addition of a small perturbation. Note that this perturbation does not change the stability of the synchronized state: for a positive variational term, instability is maintained, and for a negative one, the synchrony remains stable. The only uncertainty happens for the critical condition corresponding to a vanishing variational term, for which the perturbation can have either effect on stability.

To illustrate the use of our method, we consider the example of a weighted network of N = 10 chaotic Rössler oscillators, switching back and forth between two topologies (Fig. [\ref=topotopotopo]). Letting the state vector [formula], each of the oscillators obeys the local dynamics

[formula]

with the output function

[formula]

The switching times and the time periods for which the network remains in each of the two configurations (permanence times) are all set to 0.1. We perform two simulations, one with interaction strength σ = 1 and one with σ = 0.1, to illustrate two different cases and the sensitivity of our method. To estimate the largest Lyapunov exponent associated to the system of Eq. [\ref=variation], we compute the time-average of the logarithm of the norm of the vector

[formula]

at each integration step. The value to which [formula] converges is Λmax. The results, in Fig. [\ref=LyaEta], show that for the σ = 1 case the convergence value is approximately - 0.3, indicating that the synchronized state is stable. Conversely, when σ = 0.1, the estimated Lyapunov exponent is just positive, with a value of approximately 0.022, corresponding to an unstable synchronized state. To verify this numerical result, we simulated the actual network evolution for the two cases according to Eq. [\ref=eq1], and with [formula] and [formula] given by Eqs. [\ref=numf] and [\ref=numh] above. Figure [\ref=GlobErr] shows the time evolution of the global synchronization error

[formula]

For the σ = 1 case, after a certain transient, the synchronization error decays to 0. When the interaction strength is lowered to σ = 0.1, instead, χ eventually starts growing and oscillates wildly, always taking non-zero values. These results indicate that the system is indeed able to synchronize in the first case, while it never does in the second, in agreement with the numerical calculations of Λmax. Thus, the simulations not only confirm the validity of our treatment, but provide an example for which the stability of the synchronized state can be changed by the tuning the parameters controlling the topological evolution.

In summary, we demonstrated how to explicitly solve the problem of constructing an appropriate time evolution of a system of networked dynamical units switching between different topologies. Our method builds the evolution from a mapping of the eigenvectors of the graph Laplacians of the individual structures, and it ensures that the elements of the eigenvectors are differentiable at each intermediate time. This enables the use of the Master Stability Function for network topologies that evolve in time in a fully general and unconstrained way. While the connection pathway is not unique, different solutions only affect the variational part of the linearized system. It has to be remarked that our treatment is valid regardless of the time scales involved. There is no restriction on the permanence times of the network in each configuration, and the only constraint on the switching times is that they do not vanish. In addition, our method introduces a numerical advantage, in that one only needs to integrate a set of linear equations coupled with a single non-linear one, rather than having to deal with a system entirely composed of non-linear differential equations. Also, this approach does not rely on particular assumptions concerning the structures visited by the systems, and contains the regimes of blinking networks and commutative evolution as its limiting cases. Thus, our results have a natural application in the study of synchronization events in systems for which the temporal scales of the topology evolution are comparable with (or even secular with respect to) those characterizing the evolution of the dynamics in each networked unit. This is a common occurrence in many real-world systems, such as neural networks, where synchronization can become possible due to mutations [\cite=Noe03], or financial market, where global properties are affected by adaptive social dynamics.