Evaluating the Potential of a Dual Randomized Kaczmarz Solver for Laplacian Linear Systems

Sandia is a multi-program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-AC04-94AL85000. Supported by Contract #618442525-57661 from Intel Corp. and Contract #DE-AC02-05CH11231 from DOE Office of Science.

Introduction

Graph Laplacians

The Laplacian matrix of a weighted, undirected graph is defined as L = D - A, where D is the diagonal matrix containing the sum of incident edge weights and A is the weighted adjacency matrix. The Laplacian is symmetric and positive semidefinite. The Laplacian is also defined for directed graphs [\cite=AgaevChebotarev2005]. Because they are not symmetric, most of the solvers discussed in this paper do not apply, and efficient solution techniques remain an open problem. Solving linear systems on the Laplacians of structured graphs, such as two and three dimensional meshes, has long been important in finite element analysis (with applications in electrical and thermal conductivity, and fluid flow modeling [\cite=BHV2008]) and image processing (with applications in image segmentation, inpainting, regression, and classification [\cite=KMT2011] [\cite=McCannPollard2008] [\cite=Grady2006]). More recently, solving linear systems on the graph Laplacians of large graphs, with irregular degree distributions, has emerged as an important computational task in network analysis (with applications to maximum flow problems [\cite=CKMST2011], graph sparsification [\cite=SpielmanSrivastava2011], and spectral clustering [\cite=KhoaChawla2015]).

Most applied work on Laplacian solvers has been on preconditioned conjugate gradient (PCG) solvers, including support graph preconditioners [\cite=GrembanPHD1996] [\cite=BGHNT2006] [\cite=BCHT2004], or on specialized multigrid methods [\cite=LivneBrandt2012] [\cite=KMT2011]. Several of these methods seem efficient in practice, but none have asymptotic performance guarantees based on the size of the graph.

The theoretical computer science community has developed several methods, which we refer to generally as combinatorial Laplacian solvers. These solvers have good complexity bounds but, in most cases, no reported experimental results. Spielman and Teng [\cite=SpielmanTeng2004] first showed how to solve these problems in linear times polylogarithmic work, later improved upon by Koutis, Miller, and Peng [\cite=KMP2010], but their algorithms do not yet have a practical implementation. An algorithm proposed by Kelner et al. [\cite=KOSZ2013] has the potential to solve these linear systems in linear times polylogarithmic work with a simple, implementable algorithm.

The Dual Randomized Kaczmarz Algorithm

The inspiration for the algorithm proposed by Kelner et al. [\cite=KOSZ2013], which we refer to as Dual Randomized Kaczmarz (DRK), is to treat graphs as electrical networks with resistors on the edges. For each edge, the weight is the inverse of the resistance. We can think of vertices as having an electrical potential and a net current at every vertex, and define vectors of these potentials and currents as v and f respectively. These vectors are related by the linear system Lv  =  f. Solving this system is equivalent to finding the set of voltages that satisfies the net "injected" currents. Kelner et al.'s DRK algorithm solves this problem with an optimization algorithm in the dual space, which finds the optimal currents on all of the edges subject to the constraint of zero net voltage around all cycles. They use Kaczmarz projections [\cite=Kaczmarz1937] to adjust currents on one cycle at a time, iterating until convergence.

We will also refer to the Primal Randomized Kaczmarz (PRK) method that applies Kaczmarz projections in the primal space [\cite=StrohmerVershynin2009]. One sweep of PRK performs a Kaczmarz projection with every row of the matrix. Rows are taken in random order at every sweep.

DRK iterates over a set of fundamental cycles, cycles formed by adding individual edges to a spanning tree T. The fundamental cycles are a basis for the space of all cycles in the graph [\cite=Diestel2010]. For each off-tree edge e, we define the resistance Re of the cycle Ce that is formed by adding edge e to the spanning tree as the sum of the resistances around the cycle,

[formula]

which is thought of as approximating the resistance of the off-tree edge re. DRK chooses cycles randomly, with probability proportional to Re / re.

The performance of the algorithm depends on the sum of these approximation ratios, a property of the spanning tree called the tree condition number

[formula]

The number of iterations of DRK is proportional to the tree condition number. Kelner et al. use a particular type of spanning tree with low tree condition number, called a low stretch tree. Specifically, they use the one described by Abraham and Neiman [\cite=AbrahamNeiman2012] with Ï„ = O(m log n log log n), where n refers to the number of vertices and m refers to the number of edges of the original graph. The work of one iteration is naively the cycle length, but can be reduced to O( log n) with a fast data structure, yielding O(m log n2 log log n) total work.

Overview

The rest of the paper is organized as follows. In Section 2 we survey the related experimental work. Section 3 is an initial evaluation of the DRK algorithm as compared to PCG and PRK. We present our new ideas for improving the performance of DRK in Section 4. In this section we also consider how to perform cycle updates in parallel. Section 5 is an evaluation of the new ideas proposed in Section 4.

Related Experimental Work

As the DRK algorithm is a recent and theoretical result, there are few existing implementations or performance results. Hoske et al. implemented the DRK algorithm in C++ and did timing comparisons against unpreconditioned CG on two sets of generated graphs [\cite=HLMW15]. They concluded that the solve time of DRK does scale nearly linearly. However, several factors make the running time too large in practice, including large tree stretch and cycle updates with unfavorable memory access patterns. They cite experimental results by Papp [\cite=Paap2014], which suggest that the theoretically low stretch tree algorithms are not significantly better than min-weight spanning trees in practice, at least on relatively small graphs.

Chen and Toledo [\cite=ChenToledo2003] experimented with an early and somewhat different combinatorial approach to Laplacians called support graph preconditioners. They demonstrated that support graph preconditioners can outperform incomplete Cholesky on certain problems. There has also been some experimental work in implementing the local clustering phase of the Spielman and Teng algorithm [\cite=ZLM2013].

Initial Evaluation of DRKand Comparison to PCG and PRK

Experimental Design

Our initial study of DRK measures performance in terms of work instead of time, and uses a somewhat more diverse graph test set than Hose et al. [\cite=HLMW15]. We implemented the algorithm in Python with Cython to see how it compared against PCG (preconditioned with Jacobi diagonal scaling) and PRK. However, we did not implement a low stretch spanning tree. Instead we use a low stretch heuristic that ranks and greedily selects edges by the sum of their incident vertex degrees (a cheap notion of centrality). We have found that this works well on unweighted graphs. We also did not implement the fast data structure Kelner et al. use to update cycles in O( log n) work.

We do not report wall clock time, since our DRK implementation is not highly optimized. Instead we are interested in measuring the total work. For PCG the work is the number of nonzeros in the matrix for every iteration, plus the work of applying the preconditioner at every iteration (number of vertices for Jacobi). For PRK the work is the number of nonzero entries of the matrix for every sweep, where a sweep is a Kaczmarz projection against all the rows of L. As the DRK work will depend on data structures and implementation, we consider four different costs for estimating the work of updating a single cycle, which we refer to as cost metrics. The first metric is the cost of updating every edge in a cycle, which is included because it is the naive implementation we are currently using. The second metric relies upon the data structure described by Kelner et al., which can update the fundamental cycles in O( log n) work. This may be an overestimate when the cycle length is actually less than log n. The third metric considers a hypothetical log  of cycle length update method which we do not know to exist, but is included as a hopeful estimate of a potentially better update data structure. The last metric costs O(1) work per cycle, which is included because we surely cannot do better than this.

We ran experiments on all the mesh-like graphs and irregular graphs shown in Appendix Table [\ref=table:graphsizes]. Mesh-like graphs come from more traditional applications such as model reduction and structure simulation, and contain a more regular degree distribution. Irregular graphs come from electrical, road, and social networks, and contain a more irregular, sometimes exponential, degree distribution. Most of these graphs are in the University of Florida (UF) sparse matrix collection [\cite=DavisHu2011]. We added a few 2D and 3D grids along with a few graphs generated with the BTER generator [\cite=KPPS2014]. We removed weights and in a few cases symmetricized the matrices by adding the transpose. We pruned the graphs to the largest connected component of their 2-core, by successively removing all degree 1 vertices, since DRK operates on the cycle space of the graph. The difference between the original graph and the 2-core is trees that are pendant on the original graph. These can be solved in linear time so we disregard them to see how solvers compare on just the structurally interesting part of the graph.

We solve to a relative residual tolerance of 10- 3. The Laplacian matrix is singular with a nullspace dimension of one (because the pruned graph is connected). For DRK and PRK this is not a problem, but for PCG we must handle the non-uniqueness of the solution. We choose to do this by removing the last row and column of the matrix. We could also choose to orthogonalize the solution against the nullspace inside the algorithm, but in out experience the performance results are similar.

We also ran a set of PCG vs. DRK experiments where the convergence criteria is the actual error within 10- 3. We traded off accuracy in the solution to run more experiments and on larger graphs. We do this knowing the solution in advance. One of the interesting results of the DRK algorithm is that, unlike PCG and PRK, convergence does not depend on the condition number of the matrix, but instead just on the tree condition number. Since higher condition number can make small residuals less trustworthy, we wondered whether convergence in the actual error yields different results.

Experimental Results

We compare DRK to the other solvers by examining the ratio of DRK work to the work of the other solvers. The ratio of DRK work to PRK work is plotted in Figure [\ref=fig:prkworkcompare], separated by graph type. Each vertical set of four points are results for a single graph, and are sorted on the x axis by graph size. The four points represent the ratio of DRK work to PRK work under all four cost metrics. Points above the line indicate DRK performed more work while points below the line indicate DRK performed less work. Similar results for the PCG comparison are shown in Figure [\ref=fig:pcgworkcompare]. Another set of PCG comparisons, converged to the actual error, is shown in Figure [\ref=fig:pcgactualerr].

An example of the convergence behavior on the USpowerGrid graph is shown in Figure [\ref=fig:actualerrexample]. This plot indicates how both the actual error and relative error behave during the solve for both PCG and DRK. A steeper slope indicates faster convergence. Note this only shows metric 1 work for DRK.

Experimental Analysis

In the comparison to PRK (shown in Figure [\ref=fig:prkworkcompare]), DRK is often better with cost metrics 3 and 4. On a few graphs, mostly in the irregular category, DRK outperforms PRK in all cost metrics (all the points are below the line.) In the comparison to PCG (shown in Figure [\ref=fig:pcgworkcompare]), DRK fares slightly better for the irregular graphs, but on both graph sets these results are somewhat less than promising. PCG often does better (most of the points are above the line). Even if we assume unit cost for cycle updates, PCG outperforms DRK. The performance ratios also seem to get worse as graphs get larger.

The results concerning the actual error (shown in Figure [\ref=fig:pcgactualerr]) are very interesting as they are quite different than those with the residual tolerance. For all of the mesh graphs, considering the actual error makes DRK look more promising. The relative performance of cost metrics 3 and 4 are now typically better for DRK than PCG. However, PCG is still consistently better with cost metrics 1 and 2. For some of the irregular graphs, the convergence behavior is similar, but for others things look much better when considering actual error. Informally the number of edges updated by DRK did not change much when switching convergence criteria, but PCG work often increased. The USpowerGrid example (shown in Figure [\ref=fig:actualerrexample]) gives a sense of this. The residual error and actual error decrease similarly for DRK, but the actual error curve for PCG decreases much more slowly for the actual error.

New Algorithmic Ideas

We consider ways in which DRK could be improved by altering the choice of cycles and their updates. Our goals are both to reduce total work and to identity potential parallelism in DRK. To this end we are interested in measuring the number of parallel steps, the longest number of steps a single thread would have to perform before before convergence, maximized over all threads. Parallel steps are measured in terms of the four cost metrics described in section 2. We will also define the span [\cite=CLRS2009], or critical path length, which is the number of parallel steps with unbounded threads.

Expanding the Set of Cycles

Sampling fundamental cycles with respect to a tree may require updating several long cycles which will not be edge-disjoint. It would be preferable to update edge-disjoint cycles, as these updates could be done in parallel. The cycle set we use does not need to be a basis, but it does need to span the cycle space. In addition to using a cycle basis from a spanning tree, we will use several small, edge-disjoint cycles. We expect that having threads update these small cycles is preferable to having them stand idle.

2D Grid Example

A simple example of a different cycle basis is the 2D grid graph, shown in Figure [\ref=grid]. In the original DRK, cycles are selected by adding off-tree edges to the spanning tree as in Figure [\ref=grid](a). As the 2D grid graph is planar, the faces of the grid are the regions bounded by edges, and we refer to the cycles that enclose these regions as facial cycles. We consider using these cycles to perform updates of DRK, as the facial cycles span the cycle space of a planar graph [\cite=Diestel2010]. Half of these cycles can be updated at one iteration and then the other half can be updated during the next iteration, in a checkerboard fashion, as in Figures [\ref=grid](b)(c). Furthermore, to speed up convergence, smaller cycles can be added together to form larger cycles (in a multilevel fashion) as in Figure [\ref=grid](d).

We implemented such a cycle update scheme using the grid facial cycles, and performed experiments to see how the facial cycles affected the total work measured in both the number of cycles updated (metric 4) and edges updated (metric 1). With the facial cycles, the span per iteration is the cost of updating two cycles at each level. We ran experiments with and without the hierarchical combination of the facial cycles against the original set of fundamental cycles. In the case of the fundamental cycles we use H trees [\cite=AKPW1995], which have optimal stretch O( log n). Solutions were calculated to a residual tolerance of 10- 6. The accuracy here is slightly better than the rest of the experiments since these experiments were faster.

The results shown in Figure [\ref=plot:gridresults] indicate that the facial cycles improve both the work and span. Using a hierarchical update scheme reduces the total number of edges updated. However as this requires updating larger cycles it has a worse span than simply using the lowest level of cycles.

Extension to General Graphs

We refer to small cycles we add to the basis as local greedy cycles. Pseudocode for finding these cycles is shown in Algorithm 1. We construct this cycle set by attempting to find a small cycle containing each edge using a truncated breadth-first search (BFS). Starting with all edges unmarked, the algorithm selects an unmarked edge and attempts to find a path between its endpoints. This search is truncated by bounding the number of edges searched so that each search is constant work and constructing the entire set is O(m) work. If found, this path plus the edge forms a cycle, which is added to the new cycle set, and all edges used are marked. Table [\ref=table:graphsizes] shows the number of local greedy cycles found for all the test graphs when the truncated BFS was allowed to search 20 edges. Greedy cycles were found in all the graphs except for tube1, all of whose vertices had such high degree that searching 20 edges was not enough to find a cycle.

Adding additional cycles to the cycle basis means we need new probabilities with which to sample all the cycles. Since in the unweighted case, the stretch of a cycle is just its total length, it seems natural to update cycles proportional to their length.

Cycle Sampling and Updating in Parallel

In the original DRK algorithm, cycles are chosen one at a time with probability proportional to stretch. We propose a parallel update scheme in which multiple threads each select a cycle, at every iteration, with probability proportional to cycle length. If two threads select cycles that share an edge, one of the threads goes idle for that iteration. In Figure [\ref=parallelcycle], threads 1, 2, and 4 select edge-disjoint cycles. However the third processor selects a cycle which contains edge 3, which is already in use by the cycle on thread 1. Processor 3 sits this iteration out while the other processors update their cycles.

We compute several measures of parallel performance. The first is simply the number of iterations. The second is the total work across all threads at every iteration. Lastly we measure the span, or critical path length. This is the maximum of the work over all threads, summed over all the iterations.

We envision threads working in a shared memory environment on a graph that fits in memory. This might not be realistic in practice as there must be some communication of which edges have already been used which might be too expensive relative to the cost of a cycle update. However we are simply interested in measuring the potential parallelism, thus we ignore any communication cost.

The parallel selection scheme conditions the probabilities with which cycles are selected on edges being available

[formula]

This scheme creates a bias towards smaller cycles with less conflicting edges as more threads are added, which can increase total work.

Experiments and Results

Experimental Design

We performed experiments on a variety of unweighted graphs from the UF Sparse Matrix Collection (the same set as in Section 2, shown in Table [\ref=table:graphsizes]). Again we distinguish between mesh-like graphs and irregular graphs. We also use a small test set for weak scaling experiments, consisting of 2D grids and BTER graphs.

We continue to use our Python/Cython implementation of DRK, without a guaranteed low stretch spanning tree or a cycle update data structure. The code does not run in parallel, but we simulate parallelism on multiple threads by selecting and updating edge-disjoint cycles at every iteration as described above.

Our experiments consist of two sets of strong scaling experiments, the spanning tree cycles with and without local greedy cycles, up to 32 threads. We set a relative residual tolerance of 10- 3. Again we sacrifice accuracy to run more experiments on larger graphs. We consider the same four cycle update cost metrics as in Section 2: , log n, [formula], and unit cost update. However in the case of the local greedy cycles, which cannot use the log n update data structure, we always just charge the number of edges in a cycle. For all the cost models, we measure the total work required for convergence and the number of parallel steps taken to converge. For metric 4 these will be the same. A condensed subset of the scaling results is shown in Table [\ref=table:results].

Experimental Results

We first examine the effects of using an expanded cycle set in the sequential algorithm. We estimate the usefulness of extra cycles as the length of the largest cycle in the fundamental set normalized by the number of cycles in the fundamental set. This is because we suspect the large cycles to be a barrier to performance, as they are updated the most frequently, and at the highest cost. We plot the performance of the local greedy cycles for the two different graph types, using metrics 1 and 4 in Figure [\ref=plot:cyclestats]). These plots show the ratio between the work of the expanded cycle sets as a function of the estimated usefulness. Points below the line indicate that adding local greedy cycles improved performance.

We examine how the local greedy cycles perform as graph size increases with weak scaling experiments on 2D grid graphs and BTER graphs. The 2D grids used for this experiment are the same as in Figure [\ref=plot:gridresults], and the BTER graphs were generated with the parameters: average degree of 20, maximum degree of [formula], global clustering coefficient of 0.15, and maximum clustering coefficient of 0.15. We plot the performance of cost Metric 1 as graph size scales in Figure [\ref=plot:weakresults].

Figure [\ref=plot:example] shows examples of our results on three of the graphs. In Figure [\ref=plot:example](a) the parallel steps (with the four different metrics) is plotted as a function of the number of threads used for the barth5 graph. The total edges (metric 1) is at the top of the plot, while the unit cost (metric 4) is at the bottom. These results are shown for both fundamental and extended cycle sets. Figure [\ref=plot:exampleemailwork] shows the effect of adding threads to the total work.

To measure the parallel performance across multiple graphs we look at the average speedup of the parallel steps across all graphs. Speedup is defined as the sequential work using one thread over the number of parallel steps using a number of multiple threads. The speedup with and without extended cycles is shown in Figure [\ref=plot:speedup]. Note that without local greedy cycles metric 2 and metric 4 speedup are the same as the costs differ by log n. We compare the speedup between the different cycle sets for the different graph types in Figure [\ref=plot:speedup8]. Results are shown only for metric 1. The speedup of using 8 threads without local greedy is plotted against the speedup of using 8 threads with local greedy.

Experimental Analysis

In the sequential results shown in Figure [\ref=plot:cyclestats], there seems to be a threshold of largest cycle length above which local greedy cycles can be useful, but below which there is not much difference. However, there is not a clear scaling with the cycle length ratio, indicating that this is still a crude guess as to where the extended cycle set is useful. Also note that mesh-like graphs tend to have larger girth (max cycle length) than irregular graphs, leading to local greedy cycles working better on meshes. The local greedy cycle improvement is slightly better for metric 1 where we count every edge update. At the other extreme, when updating large cycles is the same cost (unit) as small cycles added by local greedy, the local cycles are less effective. However there is still an improvement in number of cycles updated. We were unable to find some measure of the usefulness of a single local greedy cycle.

The weak scaling experiments shown in Figure [\ref=plot:weakresults] show that, with the exception of a BTER outlier, the work scales nicely with graph size. Also results for both cycle sets scale similarly. The extended cycle set benefits the 2D grid graphs while the BTER graphs see little improvement or are worse. This is consistent with Figure [\ref=plot:cyclestats] since the BTER graphs are irregular and the 2D grids are mesh-like.

The scaling of parallel steps plots show a variety of different behavior on the example graphs. On the mesh-like barth5 graph (shown in Figure [\ref=plot:example](a)), the local greedy cycles improve both sequential performance and the scaling of parallel steps performance. At the left of this plot we see the extra cycles improve sequential results. Then as threads are added in parallel, the steeper slope indicates the local greedy cycles improved the scaling of the parallel steps. On the tuma1 graph (shown in Figure [\ref=plot:example](b)), the local greedy cycles improve sequential performance, but result in similar or worse scaling. At the left of this plot we see the extra cycles improve results sequentially, but when scaled to 32 threads performance is similar. On the email graph (shown in Figure [\ref=plot:example](c)), the local greedy cycles do not improve sequential performance, and scaling is poor with both cycle sets. There is little difference between the different cycle sets in this plot. Furthermore scaling is poor and quickly flattens out by about four threads. For a better understanding of the poor parallel steps scaling on the email graph, we examine the total work scaling (shown in Figure [\ref=plot:exampleemailwork]), showing how much extra work we have to do when skewing the probability distribution. This extra work quickly increases, limiting the parallel performance.

In the average parallel steps speedup plot (shown in Figure [\ref=plot:speedup]), we see similar speedup for both cycle sets. On mesh-like graphs the local greedy cycles do slightly better on all cost metrics beyond 16 threads. However on the irregular graphs, only with cycle cost metric 4 do the local greedy cycles perform better, and under metric 3 they perform worse. (Again note that without local greedy cycles metric 4 and metric 3 speedups are the same.) We hypothesized that giving the solver smaller, extra cycles would improve the parallel performance compared to the fundamental cycles. However this seems to only be true for mesh-like graphs, and even then the improvement is minimal. An interesting thing to note is that the speedup is better with the log n cost model. This is probably due to overcharging small cycles, which is less problematic when there are more threads to pick potentially larger cycles.

Taking a snapshot of the parallel steps speedup results on eight threads (shown in Figure [\ref=plot:speedup8]), we see that there are some irregular graphs which do not have much speedup for either cycle set (bottom left of the plot). However there are mesh-like and irregular graphs which enjoy a speedup for both cycle sets (top right of the plot). It is difficult to say on which graphs will different cycles aid with parallelism.

Conclusions

We have done an initial comparison of Kelner et al.'s DRK algorithm with PCG and PRK. These preliminary results, measuring algorithm work by number of edges touched or by number of cycles updated, do not at present support the practical utility of DRK. For mesh-like graphs, PCG usually takes less work than DRK, even if DRK is charged only one unit of work per cycle update. This suggests that the fast cycle update data structure proposed by Kelner et al. (or any undiscovered fast update method) will not be enough to make DRK practical. It does seem that DRK is an improvement to PRK on several graphs, mostly irregular graphs. One promising result of these experiments is that DRK converges to small actual error similarly to residual error, while PCG sometimes does not. More PCG iterations are required when solving to a low actual error, while DRK work does not increase very much. More work should be done to understand this behavior.

The experiments in this paper were limited to unweighted graphs for simplicity. Experiments with weighted graphs should be run for more complete results. An open question is whether there is a class of graphs with high condition number, but with practical low stretch trees, where DRK will perform significantly better in practice.

We suggest techniques for improving DRK in practice. We consider using a spanning set, including non-fundamental cycles, to accelerate convergence. Using facial cycles of a two-dimensional grid graph greatly reduces the required number of edge updates compared to the fundamental cycle basis. We attempt to generalize these cycles by finding small local greedy cycles. We found these cycles can accelerate convergence, especially for mesh-like graphs. It is difficult to measure the usefulness of any one cycle in the basis, so it is difficult to determine where and which extra cycles are useful.

We also consider how DRK could be implemented in parallel to take advantage of simultaneous updates of edge disjoint cycles. We describe a model in which threads select cycles, and go idle if a conflicting edge is found. While this can increase total work, it can often reduce the number of parallel steps. However there is a limit to this parallelism. Furthermore, this scaling behavior seems to be similar with or without local greedy cycles.