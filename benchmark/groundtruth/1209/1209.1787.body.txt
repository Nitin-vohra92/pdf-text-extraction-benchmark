Beyond inverse Ising model: structure of the analytical solution for a class of inverse problems

Introduction

The estimation of the direct interactions among the microscopic constituents of an extended system is a problem that has recently received considerable interest from the literature of several communities (biology [\cite=Socolich:2005vy] [\cite=Weigt:2009on], genetics [\cite=Braunstein:2008sa] [\cite=Bailly-Bechet:2010kx], neuroscience [\cite=Schneidman:2006vg] [\cite=Shlens:2006uq] [\cite=Cocco:2009mb], economy [\cite=Lillo:2008ht] [\cite=Moro:2009wy] [\cite=Lachapelle:2010kv]). This is mainly due to the availability of large datasets across various fields, which have introduced the possibility to directly fit from data the interaction structure (e.g., the wiring pattern of neurons, the regulatory network of genes, the cross-influence of traders) of a system which is operating according to an unknown, or a partially known, mechanism. Despite this abundance of data, the problem of estimating the most appropriate model in order to describe a system is far from being solved: even if the maximum entropy principle (MEP) is invoked in order deal with the issue of model selection [\cite=Jaynes:1957uq] [\cite=Jaynes:1957fk], one is typically left with a difficult inference problem. In particular fitting a model under the MEP requires matching a set of observables estimated from data with the ones predicted by the model (the so-called inverse problem), a task which is known to be generically hard [\cite=Wainwright:2008kx]. While the Boltzmann learning algorithm can be used to obtain accurate solutions for the inverse problem only by investing a large amount of computational power [\cite=Ackley:1985hc], mean-field techniques [\cite=Kappen:1998dt] [\cite=Tanaka:1998tg] [\cite=Sessak:2009lf] [\cite=Roudi:2009qm] [\cite=Roudi:2011rr] [\cite=Ricci-Tersenghi:2012uq], message-passing methods [\cite=Mezard:2009ul] [\cite=Higuchi:2009lq] [\cite=Aurell:2010vn] [\cite=Marinari:2010vn] and pseudo-likelihood approximations [\cite=Wainwright:2007zr] [\cite=Aurell:2012ys] have proved to be very effective in order to fit approximately the interactions of the system in a short amount of time. In this work I show that for a specific class of models the inverse problem can be solved exactly and efficiently. In particular, if the probability distribution of the system has a factorized form, the entropy of the system can be written as a sum of cluster contributions which are easy to manipulate analytically. This is consistent with the results of [\cite=Cocco:2011vn] [\cite=Cocco:2012uq], who show that efficient, fast inference procedures can be constructed if the entropy of the system can be approximated by the sum of a suitable set of small cluster contributions. For these systems one can see that the robustness of the inverse problem is linked with the factorization property of the probability distribution (equivalently, the additivity of the entropy). This is a consequence of a general fact: inverse problems are typically more stable with respect to the direct ones because the response of couplings due to a change in the observables is much more localized than the response of the observables due to a shift of the couplings. In the following section I introduce a very generic setup for the inverse problem, in order to stress the generality of the results. In section [\ref=sec:TwoModels] its exact solution is presented for two specific cases. In [\ref=sec:Appl] the results are scrutinized against numerical simulations involving synthetic data, and in [\ref=sec:General] the method introduced is extended to more general cases. In section [\ref=sec:Concl] some conclusions are drawn.

The inverse problem

I consider the problem of estimating a set of M couplings [formula] describing a probability density for a set of N binary spins [formula] of the form

[formula]

where [formula] is a set of known functions of s (usually referred as potentials, or sufficient statistics), and the partition function Z(g) enforces the normalization of p(s). In the following discussion we will assume all of the φμ(s) to be monomials, i.e., to be functions of the form

[formula]

where Γμ is a given subset (cluster) of the vertex set [formula] associated with the operator μ. Within this general formulation, the Ising (or graphical) model can for example be constructed by using single-body clusters {i}Ni = 1 and two-body clusters {i,j}Ni < j = 1, together with their associated couplings usually denoted with {hi}Ni = 1 and {Jij}Ni < j = 1. Figure [\ref=fig:System] shows factor graph associated with the probability density [\eqref=eq:PDens] in a generic case. In order to estimate such model, a set of T i.i.d. observations   =  {s(t)}Tt = 1of the system is provided, so that the log-likelihood function for the model can be written as

[formula]

and depends on the data through the empirical averages [formula]. Maximizing such function in order to find the coupling set [formula] best describing the data [formula] in absence of prior leads to the set of M conditions

[formula]

where the quantities 〈φμ〉g  =  ∂ log Z(g)  /  ∂gμ describe the averages of the functions φμ(s) under the model parametrized by g. Thus, in order to find the best model in order to describe data [formula] one has to match empirical averages μ with ensemble averages [formula]. An equivalent characterization of the problem of finding the optimal [formula] is provided by computing the Legendre transform

[formula]

which can be identified as the Shannon entropy of the optimal model expressed as a function of the empirical averages [formula]. Such quantity is the cumulant generating function for the maximum likelihood estimator of the couplings [formula]. From this perspective, the entropy S() plays a role similar to the one of the free-energy F(g)  =    -   log Z(g) in a statistical mechanics problem (i.e., estimating the ensemble averages 〈φμ〉g given the couplings g). In fact, F(g) satisfies the relations

[formula]

being χμ,ν(g) the covariance matrix 〈φμφν〉g  -  〈φμ〉g〈φν〉g. Such matrix χμ,ν is a central object in the field of information theory, where it is customarily referred as Fisher information [\cite=Cover:1991fk] [\cite=Mezard:2009ko]. For S() it holds

[formula]

Just as the quantity χμ,ν(g) can be used to express how fast in T the empirical averages [formula] converge to the ensemble values 〈φ〉, its matrix inverse expresses the rate of convergence of the inferred couplings [formula] to the asymptotic values [formula]. In particular the covariance matrix for the estimator [formula] is given by

[formula]

where [formula]. Hence the matrix χ- 1() expresses the stability of the inferred couplings with respect to the statistical error due to finite sampling. I will show in the following section how equations [\eqref=eq:GenFuncEst] and [\eqref=eq:InvFish] can be used to determine analytically [formula]. In particular, if it is possible to compactly express the entropy of a model as a function of the empirical averages [formula], the maximum likelihood estimator [formula] can be found easily.

Two factorizable models

In this section I present two models for which the inverse problem can be exactly solved. The solution strategy can be straightforwardly extended to more general cases. In order to discuss this approach to the inverse problem, it is nevertheless necessary to introduce the notion of marginal pΓ, which given a set Γ  ⊆  V is defined as

[formula]

and allows to define the cluster entropy SΓ as

[formula]

It is easy to prove that marginals and cluster entropies can be exactly expressed as functions of the averages of all the possible monomials contained in the cluster Γ. More precisely, given the set of 2|Γ| monomials [formula] associated with the clusters [formula], it is easy to show that

[formula]

This formula characterizes the marginals pΓ(sΓ) as local objects, as they are uniquely determined by the averages of potentials contained inside Γ.

Tree-like graphs

Consider a model of the form

[formula]

where E  =  {(i,j)∈V  ×  V  |  i  <  j} is a given set of edges. Suppose additionally that the graph associated with the edge set E is acyclic, so that the model [\eqref=eq:IsingMod] corresponds to an Ising model on a tree (more precisely, a forest). In that case, it can be shown by induction (see [\cite=Mezard:2009ko] [\cite=Wainwright:2008kx]) that the total probability density p(s) can be factorized according to

[formula]

where ∂i  =  {(j,k)∈E  |  i∈(j,k)}. Then, the entropy of the model is additive and can be written as

[formula]

where one has defined the magnetizations [formula] and the correlations [formula]. After the use of formula [\eqref=eq:Marg], one finds by differentiation that the solution of the inverse Ising model on an acyclic graph is given by

[formula]

and the stability of the solution is determined by

[formula]

The quantities

[formula]

appearing in the previous expression describe the empirical frequencies associated with the clusters {i} and {i,j}. Notice that the problem of finding any of the mean magnetizations 〈si〉(h,J) or a correlations 〈sisj〉(h,J) usually requires the use of iterative algorithms such as belief propagation, which converge in a number of steps linear with the number of vertices |V| [\cite=Mezard:2009ko]. For the inverse problem no iterative algorithm is required, as the solution can be found by simply evaluating formula [\eqref=eq:SolTree]. This is a consequence of the fact that - as observed in [\cite=Cocco:2011vn] [\cite=Cocco:2012uq] - inverse problems tend to be more local than their direct counterparts. Equivalently, due to sparsity of χ- 1, by shifting an empirical average (either mi or Jij) just neighboring couplings are changed, whereas for the direct problem the change of a coupling (say, hi), has an effect on a finite number of vertices (being the size of the perturbation roughly determined by the correlation length of the system). These results are consistent with the ones of [\cite=Ricci-Tersenghi:2012uq] [\cite=Welling:2003kx], where the solution of the inverse Ising problem under the Bethe approximation is derived. In particular, equation [\eqref=eq:SolTree] can be recovered by specializing the results in [\cite=Ricci-Tersenghi:2012uq] [\cite=Welling:2003kx] to the case of a tree, for which the Bethe approximation is exact. This solution requires indeed the knowledge of the full topology specified by E, so that this approach cannot be used to determine the graph structure from data. Rather, it allows to exactly recover the coupling strength given an acyclic set of edges E.

One-dimensional models

Within this approach it is also possible to find the exact solution of the inverse problem for a one-dimensional chain with interactions of arbitrary order. Even if a heuristic solution for this model was first proposed in [\cite=Gori:2011ly], its detailed derivation is provided in the following. The model is defined by a probability distribution p(s) on a set of binary spins s∈{ - 1,1}N specified by a family of potentials acting on the first R spins [formula] subject to the periodic boundary conditions si  =  si + N. By using translation operators T = {Tn}N / ρ - 1n = 0, defined through their action on the φ

[formula]

one can define a one-dimensional chain as the probability distribution

[formula]

Suppose that the family [formula] contains all and only the 2R(1 - 2-  ρ) monomials describing the unit cell (i.e., any potential of range smaller than R along the chain can be uniquely obtained by translating one of the φμ by mean of a Tn operator). Then one can exploit the factorization

[formula]

where one has defined the sets [formula] and [formula]. This leads to the additivity formula for the entropy

[formula]

proved in appendix [\ref=app:1DimChain]. Translational symmetry and formula [\eqref=eq:Marg] lead finally to

[formula]

where the Γ(sΓ) are monomials used to expand the marginals as in [\eqref=eq:Marg], while cμ,Γ  =  1 if it exists an n such that Tnφμ  =  Γ and cμ,Γ  =  0 otherwise. The couplings are given by

[formula]

while the inverse susceptibilities can be written as

[formula]

The structure of this solution is analogous to the one of [\eqref=eq:SolTree], in the sense that once that the S(p) is written as a sum of cluster entropies SΓn(pΓn) and Sγn(pγn), the expression for the couplings is obtained by summing their separate contributions to the estimator [formula]. The main difference in this case is that one can exploit translational symmetry in order to equate the cluster contributions due to different unit cells. Thus, the number of sampled unit cells is [formula], rather that [formula]. This rules the convergence of the couplings to their asymptotic values: if the system is large translational symmetry allows to obtain reliable estimations of the couplings even with a small number of observations.

Applications

In order to validate the results shown above, some examples involving synthetic datasets are discussed. In the case of a tree-like model, a set of T = 106 configurations of a systems of size N = 50 has been simulated. The configurations have been obtained by MonteCarlo sampling from a distribution p(s) specified by a random tree-like graph E, while the couplings J and h have been uniformly drawn from the interval

[formula]

and multiplied by a β parameter in the range

[formula]

). The results are shown in figure [\ref=fig:1DimChainError], where the variance of the inferred couplings is plotted against the expected scaling ρ / NT. Also for the one dimensional case I analyzed the effect of an inverse temperature parameter β∈[0.01,1] modulating an interaction strength gμ randomly drawn from the interval

[formula]

General structure of exact solutions

What has been shown for models [\eqref=eq:IsingMod] and [\eqref=eq:1DimChain] can be easily extended to more general scenarios. In particular any factorization property for the probability distribution p(s) analogous to [\eqref=eq:TreeFact] and [\eqref=eq:ChainFact] breaks the entropy in a set of cluster contributions of the form

[formula]

where G is a suitably chosen collection of clusters associated with a set of {cΓ}Γ∈G coefficients. Equation [\eqref=eq:Marg] implies that cluster entropies SΓ(pΓ) are local in the empirical observables associated with Γ. Hence, if the model is large enough (i.e., if φ includes all the monomials located within the clusters Γ∈G), the cluster entropies can be expressed as functions of the observables [formula] conjugated to the couplings g. In this case the following facts hold:

Due to linearity of the derivative any inferred coupling [formula] receives contributions just by clusters containing its conjugated potential φμ(s).

The matrix χ- 1 is sparse, so that by shifting any of the empirical observables μ the only clusters which are affected are the ones including variables contained inside the monomial φμ(s).

The expressions [\eqref=eq:FisherInfoPairTree] and [\eqref=eq:Fisher1DChain] for the stability matrix χ- 1 have then a clear interpretation in term of cluster contributions: if the clusters necessary to describe p(s) are uniformly sampled (i.e., the empirical frequencies Γ are approximately flat), the error on the inferred couplings is small. Conversely, as soon as one ore more configurations sΓ are not sampled, the asymptotic error diverges. This is consistent with the observation that if a pΓ(sΓ) assigns zero mass to some configurations, infinite couplings are required to describe the marginals (in that case either some regularization can be used to enforce finiteness of the solution, or infinite solutions may be accepted as describing a hard constraint on the set of accessible configurations). As a further comment, notice that a reliable estimation of the couplings g doesn't require sampling global fluctuations in the configuration space: it is simply necessary to sample fluctuations which are local with respect to clusters in G in order to accurately estimate the couplings g. These ideas can be useful even in the case in which the entropy doesn't strictly break in the sum of small cluster terms. In particular it has been observed in [\cite=Cocco:2011vn] [\cite=Cocco:2012uq] that for several models it is possible to account for a large fraction of the entropy S(p) by using just a few local terms (corresponding to the remark that in many cases the matrix χ- 1 is approximately sparse). In this case fast, although approximate, inference schemes could be obtained by using these techniques. Notice that the price to pay in order to analytically solve this inverse problem is the introduction of a potentially high number of parameters: a coefficient has to be fitted for each of the monomials included in any of the clusters Γ∈G. In particular, the number of parameters to be considered is bound by M  ≤  |G|  ×  2max Γ∈G|Γ|. Thus, even if a number of clusters polynomial in N is usually sufficient to properly describe the entropy of a system, their size enters exponentially in the number of fitted parameters.

Conclusions

The task of determining a probability distribution whose associated momenta match a given set of values is a known hard problem, with relevant implications in the field of high-dimensional inference. In this work I have shown that its exact solution can be found whenever the probability density for the system has a factorized structure. The sparsity of the Fisher information matrix χ- 1() and the additivity of the entropy can be shown to rely upon this independence property. These result is completely general (in particular, it is independent of the interaction structure of the system). In order to provide illustrative examples, these ideas have been used to solve two specific inverse problems: the inverse Ising model on a tree and the one-dimensional periodic chain with arbitrary order interactions. The interactions of both systems can be accurately reconstructed if the clusters describing the system are uniformly sampled, while the inferred couplings diverge as soon as some relevant micro states are unobserved. These findings have been confirmed by numerical simulations, which show an excellent agreement with the analytical predictions. Techniques for the approximate solution of generic inverse problems could in principle be derived by developing the methods presented in this work.

Factorization property for the one-dimensional chain

I show in the following that for a one-dimensional periodic chain defined as in [\eqref=eq:1DimChain], the factorization property

[formula]

holds, where [formula] and [formula]. To obtain this result, one can consider the auxiliary two-dimensional model defined by the log-probability

[formula]

in which the configuration space contains the original degrees of freedom are sni∈{ - 1,1} (with [formula] and [formula]) and the auxiliary ones tni∈{ - 1,1} (with [formula] and [formula]). The relation between the original model and the auxiliary one is sketched in figure [\ref=fig:2DMap]. In particular the coupling λ controls the strength of the bond in the auxiliary dimension (labeled by n), so that the limit λ  →    ∞   describes the original chain with the obvious identification sni  →  si and tni  →  si. By defining the row variables n  =  {sni}i = R + nρi = 1 + nρ and n  =  {tni}i = R + nρi = 1 + (n + 1)ρ, the log-probability for the two dimensional model can be written as

[formula]

Hence the distribution over the degrees of freedom n and n defines a tree, because only successive row of variables interact. The marginals associated with the clusters Γn and γn can be used in order to express the probability pλ(s,t) as

[formula]

where for the two-dimensional model Γn and γn are analogously defined. By taking the λ  →    ∞   limit, the identification

[formula]

allows to recover the factorization property [\eqref=eq:ChainFact].