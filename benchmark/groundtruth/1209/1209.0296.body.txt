Simulating lattice spin models on GPUs

Introduction

In most common cases, computer programs are written serially: to solve a problem, an algorithm is constructed and implemented as a serial stream of coded instructions. These instructions are executed on a Central Processing Unit (CPU) on one computer. Momentarily disregarding specific cases for which modern CPU hardware is optimized, only one instruction may be executed at a time; on its termination, the next instruction begins its execution. On the other hand, parallel computing involves the simultaneous use of multiple computational resources. In recent years, massively parallel computing has become a valuable tool, mainly due to the rapid development of the Graphics Processing Unit (GPU), a highly parallel, multi-threaded, many core processor. However, due to the specialized nature of the hardware, the technical difficulties involved in GPU programming for scientific use stymied progress in the direction of using GPUs as general-purpose parallel computing devices. This situation has begun to change more recently, as NVIDIA introduced [formula], a general purpose parallel computing architecture with a new parallel programming model and instruction set architecture (similar technology is also available from ATI but was not studied by us). CUDA comes with a software environment that allows developers to use C (C++, CUDA FORTRAN, OpenCL, and DirectCompute are now also supported [\cite=ProgrammingGuide2010]) for program development, exposing an application programming interface and language extensions that allow access to the GPU without specialized assembler or graphics-oriented interfaces. Nowadays, there are quite a few scientific applications that are running on GPUs; this includes quantum chemistry applications [\cite=Ufimtsev+Martinez2008] [\cite=Ufimtsev2009], quantum Monte Carlo simulations [\cite=Anderson2007] [\cite=Meredith2009], molecular dynamics[\cite=Meel2007] [\cite=Stone2007] [\cite=Anderson2008] [\cite=Davis2009] [\cite=Friedrichs2009] [\cite=Genovese2009] [\cite=Dematte2010] [\cite=Eastman2010], hydrodynamics [\cite=Bernaschi2010], classical Monte Carlo simulations [\cite=Lee2009] [\cite=Tobias+Preis2009] [\cite=Tobias+Preis2010], and stochastic processes [\cite=Juba2008] [\cite=Januszewski+Kostur2010] [\cite=Balijepalli2010].

In this work we revisit the problem of parallel Monte Carlo simulations of lattice spin models on GPUs. Two generic model systems were considered:

The two-dimensional (2D) Ising model [\cite=Onsager1944] serves as a prototype model for describing critical phenomena and equilibrium phase transitions. Numerical analysis of critical phenomena is based on computer simulations combined with finite size scaling techniques [\cite=Newman1999]. Near the critical point, these simulations require large-scale computations and long averaging, while current GPU algorithms are limited to small lattice sizes [\cite=Tobias+Preis2009] or to spin [formula] systems [\cite=Tobias+Preis2010]. Thus, in three-dimensions (3D), the solution of lattice spin models still remains a challenge in statistical physics.

The North-East model [\cite=Reiter1992] serves as a prototype for describing the slowing down of glassy systems. The computational challenge here is mainly to describe correlations and fluctuations on very long timescales, as the system approaches the glass temperature (concentration) [\cite=Reiter1992]. As far as we know, simulations of facilitated models on GPUs have not been explored so far, but their applications on CPU has been discussed extensively [\cite=Garrahan2003] [\cite=Jack2006] [\cite=Garrahan2007] [\cite=Chandler2010a].

We develop two new algorithms useful to simulate these lattice spin models. For certain conditions, we obtain speedups of two orders of magnitude in comparison with serial CPU simulations. We would like to note that in many GPU applications, speedups are reported in Gflops, while in the present work, we report speedups of the actual running time of the full simulation. The latter provides a realistic estimate of the performance of the algorithms. The paper is organized as follows: Section [\ref=sec:gpuarch] comprises a brief introduction to GPUs and summarizes their main features. Section [\ref=spin_models] contains a short overview of lattice spin models and the algorithms developed. Section [\ref=PRNG] includes a short discussion of the random number generator used in this work. Section [\ref=sec:result] presents the results and the comparisons between CPU and GPU performance. Finally, Section [\ref=sec:conclusions] concludes.

GPU architecture

In order to understand GPU programming we provide a sketch of NVIDIA's GPU device architecture. This is important for the development of the algorithms reported below, and in particular for understanding the logic and limitations behind our approach. By now three generations of GPUs have been released by NVIDIA (G80, GT200 and the latest architecture codename "Fermi"). In table [\ref=tab:3architectures] we highlight the main features and differences between generations.

Hardware specifications

On a GPU device one finds a number of Scalar Multiprocessors (SMs). Each multiprocessor contains 8 / 32 (architecture dependent) Scalar Processor cores (SPs), a Multi-threaded Instruction Unit (MTIU), special function units for transcendental numbers and the execution of transcendental instructions such as sine, cosine, reciprocal, and square root, 32-bit registers and the shared memory space. In general, unlike a CPU, a GPU is specialized for compute-intensive, highly parallel computation - exactly the task graphics rendering requires - and is therefore designed such that more transistors are devoted to data processing rather than data caching and flow control. This is schematically illustrated in figure [\ref=fig:Different_philosophies], and makes GPUs less general-purpose but highly effective for data-parallel computation with high arithmetic intensity. Specifically, they are optimized for computations where the same instructions are executed on different data elements (often called Single Instruction Multiple Data, or SIMD) and where the ratio of arithmetic operations to memory operations is high. This puts a heavy restriction on the types of computations that optimally utilize the GPU, but in cases where the architecture is suitable to the task at hand it speeds up the calculations significantly.

Memory architecture

NVIDIA's GPU memory model is highly hierarchical and is divided into several layers:

CUDA programing model

As has been mentioned, CUDA includes a software environment that allows developers to develop applications mostly in the C programming language. C for CUDA extends C by allowing the programmer to define C functions (kernels) that, when called, are executed N times in parallel by N different CUDA threads on the GPU scalar processors. Before invoking a kernel, the programmer needs to define the number of threads (N) to be created. Moreover the programmer needs to decide into how many blocks these threads will be divided. When the kernel is invoked, blocks are distributed evenly to the different multiprocessors (hereby, having p multiprocessors, requires a minimum of p blocks to achieve 100% utilization of the GPU). The blocks might be 1, 2 or 3 dimensional with up to 512 threads per block. Blocks are organized into a 1 or 2 dimensional grid containing up to 65,535 blocks in each dimension. Each of the threads within a block that execute a kernel is given a unique thread ID. To differentiate between two threads from two different blocks, each of the blocks is given a unique ID as well. Threads within a block can cooperate among themselves by sharing data through shared memory. Synchronizing thread execution is possible within a block, but different blocks execute independently. Threads belonging to different blocks can execute on different multiprocessors and must exchange data through the global memory. There is no efficient way to synchronize block execution, that is, in which order or on which multiprocessor they will be processed, thus, an efficient algorithm should avoid communication between blocks as much as possible. Much of the challenge in developing an efficient GPU algorithm involves determining an efficient mapping between computational tasks and the grid/block/thread hierarchy. The multiprocessor maps each thread to one scalar processor, and each thread executes independently with its own instruction address and register state. The multiprocessor SIMT (Single Instruction Multiple Threads) unit creates, manages, schedules, and executes threads in groups of 32 called warps. When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that get scheduled by the SIMT unit. The SIMT unit selects a warp that is ready to execute and issues the next instruction to the active threads of the warp. A warp executes one common instruction at a time, so full efficiency is realized when all threads of a warp agree on their execution path (on the G80 and GT200 architectures, it takes 4 clock cycles for a warp to execute, while on the Fermi architecture it takes only one clock cycle). If threads of a warp diverge via a data dependent conditional branch, the warp serially executes each branch path taken, disabling threads that are not on that path. When all paths terminate, the threads converge back to the same execution path. Branch divergence occurs only within a warp: different warps execute independently regardless of whether they are executing common or disjointed code paths. More information can be found in Ref.[\cite=ProgrammingGuide2010].

Lattice spin models

We consider a general lattice model of spins that are placed on a square lattice of dimensions d = 1,2,3.... The spins may be of any dimension and may acquire continuous or discrete values. In the applications reported below, we focus, for simplicity, on the case where the spins at lattice site i take discrete values of [formula], but this can be easily extended to any spin dimension and value. The interactions between the spins is given by the Hamiltonian:

[formula]

In the above equation, the first sum is usually carried over nearest neighbors only (which we will denote 〈ij〉). Jij is the interaction parameter and may be constant, discrete or continuous and Bi is an external field. The above Hamiltonian can be used to study equilibrium properties as in the Ising model and spin glass models (Edwards-Anderson model [\cite=Edwards1975], Sherrington-Kirkpatrick model [\cite=Sherrington1975], random orthogonal model [\cite=Marinari1994] etc.), or the dynamic behavior as in facilitated spin models (Fredrickson and Andersen [\cite=Fredrickson+Andersen1984], Jackle-Eisinger north-east model [\cite=Jackle1991] [\cite=Reiter1992], etc.). Simulations of such systems are based on Monte Carlo techniques [\cite=Landau2000]. In most CPU implementations, the algorithms for equilibrium or dynamic simulations do not differ significantly. However, as will become clear below, they become very different when implemented on a GPU. Though the implementations described below are for two simple cases, the extension of our approach to the spin glass models mentioned above or to other facilitated spin models is straightforward.

Monte Carlo simulation of the 2D Ising model

The simplest 2 Ising model [\cite=Newman1999] describesN magnetic dipoles (or spins) placed on a 2 square lattice with one spin per cell. We limit the discussion to the spin [formula] case, where each spin has only two possible orientations, "up"and "down". Each spin interacts with its nearest neighbors only, with a fixed interaction strength. In the absence of an external magnetic field, the Hamiltonian is given by

[formula]

where 〈ij〉 represents a sum over nearest neighbors and J determines the energy scale. Monte Carlo simulation techniques based on the Metropolis algorithm [\cite=Metropolis+al:1953] are perhaps the most popular route to obtain the thermodynamic properties of this model. In a CPU implementation of the Metropolis algorithm, a spin is selected at random and an attempt to flip the spin is accepted with the Metropolis probability Pacc  =   min [1, exp (  -  ΔH  /  T)], where T is the temperature in units of energy. In the GPU algorithm developed here, we will take advantage of the fact that spins interact only with their nearest neighbors, and thus the problem can be divided into non-interacting domains. The generalization to the case of finite interacting regions is straightforward. The algorithm is as follows:

Randomly initialize the lattice (this is done on the CPU).

Copy lattice to the GPU.

Divide the lattice into Q sub-lattices, each with P spins.

A grid of Q thread-blocks is formed. Every thread-block contains [formula] threads.

Every block copies a sub-lattice including its boundaries from the global memory to the shared memory. To avoid bank conflicts and save precious shared memory space, the short data type was used to form the lattice (figure [\ref=fig:2D_Ising]).

Within the block, every thread is in charge of 4 spin sites (a sub block of [formula]). At first all red spins are updated (figure [\ref=fig:sequence(a)]), i.e. all threads are active. Once a thread finished updating the red spin it continues to update its blue spin (figure [\ref=fig:sequence(b)]). Since no native block synchronization exists, before updating the remaining spins, we make sure that all blocks finished the first two steps. This is done by recopying the data from the shared memory (boundaries excluded) on to the global memory and ending the kernel.

Relaunch the kernel with the configuration generated in the previous step. Redo steps 5 and 6, only this time the green and white spins are being updated (figure [\ref=fig:sequence(c)]).

Recopy data to global memory.

This completes one Monte Carlo step (or one lattice sweep). In our implementation we chose sub-lattices of [formula] in size and blocks of 256 threads, as this choice turned out to be the most efficient. Using more threads does in fact reduce the time it takes the block to copy a sub-lattice to the shared memory, but then the ratio of arithmetic operations to memory operations is low and performance is poor. To obtain thermodynamic average properties, we use the fact the CPU and GPU can work in parallel and the lattice is copied from the device to the host from time to time, so averages can be calculated on the CPU while the GPU continues to sweep the lattice. We note in passing that similar algorithms have been proposed by Tobias et al. [\cite=Tobias+Preis2009] and Block et al..[\cite=Tobias+Preis2010] The former approach is restricted to lattices with up to [formula] spins in 2D (assuming spins are stored as integer data type), whereas the algorithm presented in this work is applicable to larger systems, is designed for coalesced global memory access and avoids bank conflicts, all of which makes better use of the GPU architecture. The limit of system size in our approach is related to the size of the global memory on the GPU, which is typically on the order of 1 - 4. This amounts to maximum system sizes of [formula] - [formula] spins. The algorithm of Block et al. [\cite=Tobias+Preis2010] is applicable to much larger systems and deals with the issue of multi-GPU programming, but is currently restricted to spin [formula] systems while the algorithm presented in this work is suitable for the more generalized Potts model[\cite=Wu1982Potts] and gives approximately the same speedups in comparison to an equivalent CPU code.

To acquire the correct thermodynamic equilibrium state, the chosen set of Monte Carlo moves must satisfy either detailed balance or the weaker balance condition. On the CPU detailed balance is rigorously satisfied when randomly selecting a spin within the Metropolis algorithm. For the GPU, however, the approach we developed breaks detailed balance and only balance is satisfied [\cite=Manousiouthakis1999] [\cite=Ren2006]. This is a sufficient condition to ensure that the algorithm describes the correct Boltzmann distribution.

The North-East model

The North-East model is based on the Ising model Hamiltonian with special constraints that are used to model facilitated dynamics [\cite=Fredrickson+Andersen1984] [\cite=Nakanish+Takano1986] [\cite=Reiter1992]. The Hamiltonian is given by:

[formula]

where si, J, B and 〈ij〉 are described above. What makes this model different from the previous one is a constraint imposed on the transition probability to flip a spin. This probably is zero unless the spin's upper (north) and right (east) neighbors point "up". In the latter case, one accepts a flip with the same Metropolis probability given by Pacc  =   min [1, exp (  -  ΔH  /  T)]. Thus, the thermodynamics of the model are the same as the Ising model, but the Monte Carlo dynamics generated by the above rule is quite different. In most applications reported in the literature one takes J = 0. For [formula] the dynamics generated by this model are richer and show an interesting re-entrant transition [\cite=Geissler2005]. When J = 0 the thermodynamics are trivial and the coupling between neighboring spins depends only on the aforementioned dynamical constraint. The lattice is initialized so spins point "up" with the probability c, which is also the equilibrium density of spins pointing "up", and can be expressed as [formula], where the partition function is z = e1 / T + e- 1 / T and T is the unit-less temperature. Due to the dynamical constraints, if c is too low, one finds domains of spins that are stuck. For a high values of c on the other hand, all spins are flippable. As a consequence, there is a critical concentration c* below which the system is not ergodic. This transition from ergodic to non-ergodic behavior is modeled by the spin-spin autocorrelation function

[formula]

where 〈si〉 = 2c - 1 is the average spin polarization and si(t) is the spin polarization at Monte Carlo step t for site i. We expect the function to decay to zero for an initial concentration c > c* and to decay to a finite value f (which is the fraction of spins that are stuck) for c < c*. The CPU implementation of the North-East model is identical to that described for the Ising model, with the additional constraint for the flipping probability. The GPU implementation for this model, similar as it may seem to the Ising model, is a bit cannier. We found out that applying the checkerboard algorithm (described in subsection [\ref=subsec:MC2Dising]) does not yield the same relaxation dynamics as the serial CPU implementation. This is understandable, since this model imitates a diffusion process: for a spin to be able to change its configuration it is necessary that its north and east neighbors point up. If they do not, they in turn will also need their neighbors to point up to be able and change their configuration. Equilibration takes place by an up spin diffusion from "north-east" to "south-west". By sequentially (instead of randomly) sweeping the lattice we change the dynamics of this process. Such being the case, the corrected algorithm we developed is:

Initialize the lattice so spins point up with probability c (on the CPU).

Copy lattice to the GPU.

Divide the lattice into Q sub-lattices, each with P spins.

A grid of Q thread-blocks is formed. Every thread-block contains [formula] threads.

Blocks then randomly pick a sub-lattice (figure [\ref=fig:NE]) in such a way that two different blocks cannot pick the same sub-lattice. Every block copies a sub-lattice including its boundaries from the global memory to the shared memory.

Within each block, λ threads concurrently pick λ spins from the sub-lattice randomly and update them. λ is chosen to be a small fraction of P.

Synchronize the block.

Reiterate steps 6 - 7 q times, such that [formula].

Copy back data from shared memory to global memory (to allow block synchronization).

Relaunch the kernel with the same configuration and redo steps 5 - 8.

Recopy data to global memory.

This completes one lattice sweep. Again we use the fact the CPU and GPU can work in parallel, and the lattice is copied from the device to the host from time to time to store the spin's configuration for the computation of the autocorrelation function. The algorithm presented here does not preserve detailed balance nor the weaker balance condition, but since we are interested in its dynamics the given algorithm is correct. One should note that in order to obtain the correct dynamics an initialization of the parameter λ is needed. It is obvious that in the limit where Q = λ = 1 the algorithm is in fact serial and preserves detailed balance (results for this limit are given by the black line in figure [\ref=fig:Con_of_parameter]). We can now use this result as a reference, and increase the number of threads and blocks that are working in parallel. There is an upper limit above which the results will diverge from the desired reference and the dynamics will no longer be correctly reproduced. Once λ has been evaluated, the simulation can be performed. In figure [\ref=fig:Con_of_parameter] we provide a consistent test on the value of λ for the North-East model at c = 0.42.

It is possible to write an algorithm that maintains the weaker balance condition and at the same time preserve the dynamical relaxation of the model. The drawback of such an algorithm, however, is that it is ≈  2.5  ×   slower then the one presented above. The only changes are in steps 6 - 9:

Within each block, λ threads concurrently pick λ red spins (figure [\ref=fig:NE]) from the sub-lattice randomly and update them. λ is chosen to be a small fraction of P.

Synchronize the block and pick λ white spins from the sub-lattice randomly and update them.

Copy back data from shared memory to global memory (to allow block synchronization).

Relaunch the kernel with the same configuration and redo steps 6 - 8 three more times (this will complete one MC step).

Pseudo random numbers generation

Monte Carlo simulations rely heavily on the availability of random or pseudo-random numbers. In this work, random numbers were used to determine whether spin flips are accepted or rejected, in accordance with the Metropolis algorithm. As is widely known, the use of a poor quality PRNG (Pseudo Random Numbers Generator) may lead to inaccurate simulations.[\cite=Ferrenberg1992] In the course of this work three different PRNGs were utilized:

L'Ecuyer with Bays-Durham shuffle and added safeguards.[\cite=NR] This PRNG has a long period (2  ×  1018) and is easy to implement on a CPU. Unfortunately, porting it to GPUs causes a dramatic decrease in performance: this is mostly due to the fact that the implementation of this algorithm requires too many registers.

The "minimal" random number generator of Park and Miller.[\cite=NR] This PRNG has a period of 2  ×  109 and is portable to GPUs. In practice, however, it proved to work poorly and the GPU simulation results were in poor agreement with reference values obtained with better PRNGs.

Linear Congruential Random Number Generator (LCRNG).[\cite=NR] This PRNG has a period of 106 - 109 and provided good results even for very long runs. The LCRNG algorithm is very easy to port to the GPU and has the advantage of being very fast, requiring only a few operations per call.

In our implementations (section [\ref=spin_models]), each thread used a separate LCRNG, creating its own sequence of pseudo-random numbers with a unique seed. The sequence (of thread i) is created as follows:

[formula]

[formula]

An appropriate choice of the coefficients is responsible for the quality of the LCRNG. We used [\cite=NR] a = 1,664,525 , c = 1,013,904,223 and m = 232. The different seeds were created per thread according to

[formula]

with xTh00  =  1. Similar PRNGs were used for other GPU applications.[\cite=Tobias+Preis2009] [\cite=Tobias+Preis2010]

Results

For comparison, CPU codes were executed on a PC with Intel Core2 Duo E7400 [formula] processor, Intel RaisinCity motherboard with Intel G41 chipset and Kingstone [formula] RAM (only a single core was used for the calculations). The operating system was CENTOS. Codes were compiled with Intel C++ compiler (ICC) using all optimizations provided for best performance. Speedups reported here are given in terms of the complete application running time (from initialization till results are processed), rather than Gflops or spin updates per second. This choice is important, as it most closely describes the "real" gain in practical simulations by the use of a GPU rather than a CPU.

Ising model

In order to verify the GPU implementation, we compared values of magnetization, energy and heat capacity as a function of temperature (temperature was taken in energy units) between GPU and CPU versions. In figure [\ref=fig:Comparison_cpu_vs_gpu] results from a [formula] spin lattice are presented. We find the results to clearly agree. In terms of acceleration we achieved a 15  ×   factor for lattices with [formula] spins on the GT 9600 GPU (G80 architecture). Implementing the same code on the new GTX 480 GPU, we achieved a factor of 150  ×  . This factor reduces to 2  ×   for small lattices ([formula]). The reason for this is that in small problems it becomes harder to hide memory access latencies, because there are not enough threads to execute between memory access operations. A theoretical analysis of our implementation shows that the GPU reaches full occupancy (a useful tool to check this is the "CUDA GPU Occupancy Calculator" which can be freely downloaded from Ref.[\cite=Occupancy]). The 10 fold factor obtained by the GTX 480 in comparison with the GT 9600 is easily understood when taking into consideration that the GTX 480 has15 SMs instead of 8 (on the GT 9600), 4 blocks can be active simultaneously instead of 3 and a warp executes in one clock cycle instead of 4. Thus the relative speedup is [formula]. From figure [\ref=fig:Running_times(a)] it is obvious that the GPU reaches full occupancy only for lattices bigger than [formula]. Note also that near the critical temperature, the fluctuations and noise increase. Since the present work is not concerned with determining the critical behavior, we have used the same number of Monte Carlo sweeps for all temperatures. Estimation of the critical behavior requires much longer runs, and perhaps also larger systems. In this respect, the GPU approach developed here provides the means to increase numerical accuracy by more than one order of magnitude (noise scales with the square root of the number of MC steps), either by increasing the system size or by simulating longer Monte Carlo runs.

North-East model

The North-East model has a critical concentration [formula], below which the dynamics break ergodicity. In figure [\ref=fig:N-E_results] we show the results for spin-spin autocorrelation function for different concentrations above the critical value. Two lattice sizes were studied. Similar to the previous case, we find that the GPU results agree well with the CPU results, indicating that the proposed algorithm reproduces the correct dynamics. This is not trivial and depends on the value chosen for λ. Moreover, as pointed out above, the algorithm used for the Ising model fails to produce correct relaxation times. As the system approached the critical concentration the dynamics become sluggish and the autocorrelation function decays slowly to zero.

In figure [\ref=fig:Running_times(b)] we compare the running times between the CPU and two GPU architectures. We achieved a 2  ×   factor on the GT 9600 GPU and a [formula] speedup running the same code on the new GTX 480 GPU. This factor reduces to 8  ×   for smaller lattices ([formula] and less). The GPU acceleration is nearly two orders of magnitude, implying that one can simulate the system closer to the critical density. This is important, since the behavior of relaxation near the critical density is not necessarily universal, and thus extrapolations are often tricky.

Conclusions and Summary

We have developed algorithms to simulate lattice spin models on consumer-grade GPUs. Two prototype models have been considered: The Ising model describing critical phenomena at equilibrium and the North-East model describing glassy dynamics. We showed that for equilibrium properties of lattice models an impressive speedup of 150  ×   can be achieved. To simulate the dynamics of such models, a more sophisticated approach was developed in order to preserve the dynamical rules and outcome. Our algorithm for the dynamic model reaches a [formula] factor in comparison to the serial CPU implementation for large system sizes. Though the algorithms were performed on specific models, we feel they can be easily extended to a larger class of similar systems. Since the gain in computational power embodied by these results is in some cases two orders of magnitude, while the required GPU hardware is priced similarly to a CPU and can often be added to existing systems, the algorithms are certainly of interest. On the other hand, taking full advantage of it still requires savvy knowledge of the device and its capabilities and limitations, and the development of specific numerical algorithms. As the advantages of this useful technology become clear and knowledge about its implementation continues to build up and spread, we hope it will become more accessible to a wide variety of computationally-minded scientists.

Acknowledgments

We would like to thank Prof. Sivan Toledo for discussions. This work was supported by the Israel Science Foundation (grant no. 283/07). GC is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. ER thanks the Miller Institute for Basic Research in Science at UC Berkeley for partial financial support via a Visiting Miller Professorship.