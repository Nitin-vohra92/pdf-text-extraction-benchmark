Kullback-Leibler Penalized Sparse Discriminant Analysis for Event-Related Potential Classification

Introduction

A brain computer interface (BCI) is a system that measures brain activity and converts it into an artificial output which is able to replace, restore or improve any normal output (neuromuscular or hormonal) used by a person to communicate and control his/her external or internal environment. Thus, BCI can significantly improve the quality of life of people with severe neuromuscular disabilities [\cite=BCIcommunication].

Communication between the brain of a person and the outside world can be appropriately established by means of a BCI system based on event-related potentials (ERPs), which are manifestations of neural activity as a consequence of certain infrequent or relevant stimuli. The main reason for using ERP-based BCI are: it is non-invasive, it requires minimal user training and it is quite robust (in the sense that it can be use by more than 90 % of people) [\cite=wolpawBCI]. One of the main components of such ERPs is the P300 wave, which is a positive deflection occurring in the scalp-recorded EEG approximately 300 ms after the stimulus has been applied. The P300 wave is unconsciously generated and its latency and amplitude vary between different EEG records of the same person, and even more, between EEG records of different persons [\cite=Electroph]. By using the "oddball" paradigm [\cite=Don-Far] an ERP-based BCI can decode desired commands from the subject by detecting those ERPs in the background EEG. In other words, a ERP-based BCI deals with a pattern recognition problem where two classes are involved: EEG with ERP (target class) and EEG without ERP (non-target class).

Several authors have investigated in regards to which classification technique performs best for separating target from non-target EEG records [\cite=Lottereview] [\cite=Blankertz-tutorial] [\cite=AdvanceChallenges] [\cite=gareis2011]. All of these studies have concluded that linear discriminant analysis (LDA) is a very good classification scheme, resulting most of the times in the optimal performance while keeping the solution simple. It is timely to mention here that in a BCI classification scheme there are two main problems: the curse-of-dimensionality and the bias-variance trade-off [\cite=Lottereview]. While the former is a consequence of working with a concatenation of multiple time points from multiple channels, the latter depicts the generalization capability of the classifier.

However, it has been shown that LDA performs poorly when high dimensional data with small training sample is used. Different regularized solutions have been proposed [\cite=krusienskicomparison] [\cite=meanshrinkage] [\cite=aggregation] and all of them have demonstrated that a regularized version of LDA can substantially improve the classification performance of the standard LDA. However, there is still much room for further improvement. In this regards, in the present work, we propose to achieve significant gain by building up a model which takes into account the following issues, which, to our knowledge, have not been jointly considered in any of the previous works:

Solution sparsity needs to be kept as high as possible in order to maximize robustness while achieving good classification performance. It is timely to remember that the generalizability of a model tends to decrease as the sparsity decreases, i.e., as the number of parameters in the model increases [\cite=bookstats].

Since randomly selecting one variable out of a group of correlated variables may produce neglecting important information, groups of "similar" variables should be allowed into the model [\cite=enet].

Model should allow for the inclusion of discriminant a priori information since that could greatly enhance separability between classes.

With the above in mind, in this work we develop a penalized version of the sparse discriminant analysis (SDA) [\cite=SLDA], which we call Kullback-Leibler Penalized sparse discriminant analysis (KLSDA), with the main objective of solving the binary ERP classification problem. As far as we know SDA has never been used before in ERP-based BCI classification problems. Also, we shall compare our penalized version with the standard SDA in this context.

The organization of this article is as follows. In Section [\ref=reviewLDA] we made a brief review on discriminant analysis. Our proposed new approach is presented in its full general formulation in Section [\ref=ourKLSDA]. In Section [\ref=datasets] the ERP-EEG databases used in the experiments are described. In Section [\ref=Experiments] the experiments and results of the ERP-based EEG classification problem solved by KLSDA are shown. Finally, concluding remarks and future works are presented in Section [\ref=Discussion].

Discriminant Analysis: a brief review

The LDA criterion is a well-known dimensionality reduction tool in the context of supervised classification. Its popularity is mainly due to its simplicity and robustness which lead to very high classification performances in many applications [\cite=Duda].

Let [formula] be p-dimensional random vectors whose distributions uniquely characterize each one of the K classes of a given classification problem. In addition, let [formula] be an n   ×   p data matrix such that each row [formula] is a realization of one and only one of the aforementioned random vectors, and let [formula] be a categorical variable accounting for class membership; i.e., such that if pattern [formula] is a realization of [formula], then zi = k.

The LDA method consists of finding q  <  K discriminant vectors (directions), [formula] such that by projecting the data matrix [formula] over those directions, the "classes" will be well separated one from each other. It is assumed that the random vectors [formula] are independently and normally distributed with a common covariance matrix [formula]. The procedure for finding the vectors [formula] requires estimates of the within-class, the between-class and the total covariance matrices, [formula], [formula] and [formula], respectively. These estimates are given by:

[formula]

[formula]

[formula]

where Ik and nk are the set of indices and the number of patterns belonging to class k, respectively, [formula] [formula] is the class k sample mean and [formula] is the common sample mean. Note that t  =  w  +  b.

Let [formula] be a p  ×  q matrix whose jth columns is [formula]. The LDA method seeks to find the vectors [formula] in such a way that they maximize separability between classes, which is achieved by simultaneously maximizing b and minimizing w. Since t  =  w  +  b, this is equivalent to simultaneously maximizing b and minimizing t, thus:

[formula]

Since the rank of b is at most K - 1, there are at most K - 1 non-trivial solutions of problem ([\ref=LDA]), and therefore there are at most K - 1 discriminative vectors. Observe that, these vectors are precisely the directions along which the classes show maximum between-class covariance relative to their within-class covariance. Usually q = K - 1.

It is known that LDA fails when the number of patterns is low in relation to the number of variables (n <  < p) [\cite=penalized]. In this situation, the matrix t becomes usually ill-conditioned, what produces a poor estimation of [formula] and, as a consequence, a bad classification performance.

In the particular case K = 2 (and therefore q = 1), the solution to ([\ref=LDA]) has the following explicit formulation:

[formula]

This special case is known as Fisher linear discriminant analysis (FLDA) [\cite=fisher]. The FLDA approach can be formulated as a linear regression model [\cite=fisher] [\cite=Duda]. Let [formula] be as before and let [formula] be a n-dimensional vector such that [formula] or [formula], depending on whether the ith observation belonging to class 1 or to class 2, respectively, and let us consider the following ordinary least squares problem (OLS):

[formula]

whose solutions are all the vectors in the set [formula], where "†" denotes the Moore-Penrose generalized inverse and [formula] denotes the null space of [formula]. If [formula] is invertible, then Eq. ([\ref=OLS]) has a unique solution given by [formula]. For convenience it is assumed that [formula], and therefore [formula] and [formula]. Hence [formula], where [formula] is given by ([\ref=FLDA]). Since the proportionality constant [formula] is irrelevant to the direction of the solution, this proves that OLS (Eq. ([\ref=OLS])) is equivalent to the FLDA method (Eq. ([\ref=FLDA])).

Several works ([\cite=flexlda] [\cite=LSLDA] [\cite=bookstats], to cite a few) have extended the above OLS-LDA formulation to multiclass problems. It has been shown that the LDA solution can be obtained from a multivariate regression fit. In particular, Hastie et al. in [\cite=flexlda], introduced a richer and more flexible classification scheme to LDA, called optimal scoring, which we briefly describe below.

Let [formula] be as before and [formula] be a n  ×  K matrix of binary variables such that yij is an indicator variable of whether the ith observation belongs to the jth class. Let us define [formula], where the vectors [formula] are recursively obtained, for [formula], as the solution of the following constrained least squares problem which resumes the optimal scoring:

[formula]

Note that when j = 1 the orthogonality condition in Eq. ([\ref=optimalscoring]), which is imposed to avoid trivial solutions, is vacuous and hence it is not enforced. Details about the computational implementation to solve Eq. ([\ref=optimalscoring]) can be found in [\cite=flexlda].

In the sequel we shall refer to [formula] as the "score vector". Observe that [formula] is the vector in [formula] for which the mapping from [formula] to [formula] defined by [formula], results optimal for the constrained least squares problem defined by ([\ref=optimalscoring]). This mapping is precisely what introduces more flexibility into the LDA framework since it transforms binary variables into real ones.

Clemmensen et al. [\cite=SLDA] introduced a regularized version of the optimal scoring problem by adding two penalization terms to the functional in ([\ref=optimalscoring]). These penalization terms on one side induce sparsity and on the other side allow correlated variables into the solution. This regularized LDA formulation, named SDA, consists on recursively solving for [formula], the following problem:

[formula]

where λ1 and λ2 are predefined positive constants, called regularization parameters, which balance the amount of sparsity and the correlation of variables, respectively. Later on we shall analyse appropriate forms for selecting those regularization parameters.

Problem ([\ref=SDA]) is alternately and iteratively solved as follows. At first [formula] is hold fixed and optimization is performed with respect to [formula]. Then [formula] is hold fixed and optimization is performed with respect to [formula]. The following two steps are iterated:

For given (fixed) [formula], solve:

[formula]

For given (fixed) [formula], solve:

[formula]

For computational implementation details of the above steps we refer the reader to [\cite=SLDA] and [\cite=spasm].

The solution of problem ([\ref=SDA]) provides q discriminant direction, [formula], allowing standard LDA procedure with the n  ×  q matrix [formula] to be performed.

Solving Eq. ([\ref=enet]) involves the well-known elastic-net problem (e-net) [\cite=enet], which is similar to LASSO (least absolute shrinkage and selection operator) [\cite=lasso], since e-net performs automatic sparse variable selection and it also allows selection of groups of correlated variables (this is due to the introduction of [formula]-norm term).

In this work we propose the use of a penalized version of the SDA method to efficiently solve the binary classification problem appearing in BCI systems based on ERPs. This new method seeks to increase classification performance by taking into account information about the difference between classes by means of the inclusion of appropriate anisotropy matrices into the penalizing terms. The use of adaptive penalizers and, in particular of anisotropy matrices in regularization method for inverse ill-posed problems is a new approach that has shown to produce significantly better results than those obtained with the corresponding non-adaptive or isotropic penalizers [\cite=mazzieri2015mixed].

A new approach: Kullback-Leibler Penalized Sparse Discriminant Analysis

In a pattern recognition problem is very important to analyze the data and be able to extract from them as much prior information as possible since, needless to say, a good classification performance will largely depend on how well the problem is understood through the available data.

Let [formula], [formula], [formula], [formula], λ1 and λ2 be as before, and let [formula] and [formula] be p  ×  p diagonal positive definite matrices. The KLSDA scheme consists of recursively solving, for [formula], the following regularized constrained least squares problem:

[formula]

Here again, for j = 1 the orthogonality condition is vacuous. As in the SDA case, the solution of problem ([\ref=KLSDA]) is approximated by alternatively iterating the following two steps (with an adequate initialization):

Given [formula] solution of ([\ref=lalala]), solve:

[formula]

Given [formula] solution of ([\ref=Genet]), solve:

[formula]

The vector [formula] solution of ([\ref=Genet]), not only inherits both the correlated variables selection and sparsity properties of SDA, but it also contains in each one of its components appropriate discriminative information which is suitable for improving separability between classes. As before, the classification rule is constructed based upon the n  ×  q matrix [formula]. In the following subsection we show how the Kullback-Leibler divergence can be used for constructing the anisotropy matrices [formula] and [formula] in such a way that they properly incorporate discriminative information into KLSDA.

Kullback-Leibler anisotropy matrix

Discriminative information can be incorporated into KLSDA by appropriately quantifying the "distances" between classes, or more precisely, between their probability distributions. Although there is a wide variety of "metrics" for comparing probability distributions [\cite=Distances], we shall use here the well-known Kullback-Leibler divergence [\cite=kullback]. The decision to use this particular "metric" is due not only to its nice mathematical properties, but also to the fact that it was already successfully applied in many classification problems [\cite=automatic] [\cite=kullbackSVM] [\cite=KullbackClassification].

Let us suppose first that we are dealing with a binary classification problem (K = 2). Let [formula] be a discrete random variable defined on a discrete outcome space N and consider two probability functions f1(n) and f2(n), n∈N. Then, the Kullback-Leibler "distance" (KLD) of f1 relative to f2 is defined as:

[formula]

with the convention that [formula]. Although [formula] quantifies the discrepancy between f1 and f2, it is not a metric in the rigorous mathematical sense, because it is not symmetric and it does not satisfy the triangle inequality. If, for any reason, symmetry is desired then a modified KLD, called J-divergence, can be defined as follows:

[formula]

For measuring discrepancy between K probability distributions, [formula], since KLD is "additive" [\cite=kullback], an appropriate measure can be build up by adding up the J-divergences between all possible pairs of distributions, that is:

[formula]

Let fij(  ·  ) be the probability function of the jth class in the ith sample, with [formula] and [formula]. We define the J-divergence at sample i as

[formula]

This function quantifies the discrepancy between the K classes at sample i. A value of JKL(i) close to zero means that there is very little discriminative information at sample i, while a large value of JKL(i) means that sample i contains a significant amount of discriminative information which we definitely want to take into account for constructing the solution vectors [formula]. In Section [\ref=resultsKLD] we show in detail how J-divergence is able to highlight the most discriminative samples.

The available a priori discriminative information can be incorporated into the KLSDA formulation (Eq. ([\ref=KLSDA])) by means of appropriately constructed anisotropy matrices [formula] and [formula]. Since we wish to stand out those samples containing significant amounts of discriminative information, the matrices [formula] and [formula] must be constructed so as to strongly penalize those samples where there is little or none discriminative information while avoiding penalization at the remaining ones.

Computational implementation

Our computational implementation of KLSDA bellow is made by appropriately modifying the original SDA algorithm [\cite=spasm]. Thus KLSDA is mainly solved in two steps. In the first step, Eq. ([\ref=Genet]), which is a generalized version of the e-net problem [\cite=Genet], is solved. The second step consists of updating the optimal score vector [formula] by solving Eq. ([\ref=lalala]). It is shown in [\cite=SLDA] that the solution of Eq. ([\ref=lalala]) is given by [formula], where [formula] is the K  ×  q matrix containing the score vectors [formula], [formula] and s is a proportionality constant such that [formula].

In regard to the first step, it is known that the e-net problem can be reformulated by means of LASSO. In fact by defining the following augmented variables:

[formula]

the generalized e-net problem (Eq. ([\ref=Genet])) can be re-written as:

[formula]

which is known as generalized LASSO [\cite=Glasso]. If [formula] is invertible the solution of ([\ref=glasso]) can be found as [formula], where j is the solution of:

[formula]

The LARS-EN algorithm, presented in [\cite=enet], provides an efficient way of solving problem ([\ref=ec2]).

Regularization parameters

It is well-known that in every regularization method the choice of the regularization parameters is crucial. For Tikhonov-type functionals a popular and widely used method for approximating the optimal parameters is the so called L-curve criterion. One of the main advantages of this selection criterion is the fact that it does not require any prior knowledge about the noise. Roughly speaking, the method finds an optimal compromise between the norm of the residual and the norm of the regularized solution by selecting the point of maximal curvature in a log-log plot of those two quantities, parametrized by the regularization parameter. For details see [\cite=lcurve].

Despite its popularity, the L-curve method cannot be directly applied to multi-parameter penalization functionals like ([\ref=Genet]). In 1998, Belge et al. proposed and extension of the L-curve technique for approximating the optimal regularization parameters in those cases, called L-hypersurface technique [\cite=hyperlcurve], which we briefly describe next.

Let [formula], [formula], [formula] for [formula], be all given, and consider the following multi-parameter regularized least squares problem:

[formula]

where 1  ≤  r  ≤  2 and let [formula] denote the regularization parameter vector. Define [formula] and [formula], for [formula]. The L-hypersurface associated to problem ([\ref=Lhyper]) is the subset [formula] of [formula] parametrized by [formula] defined as [formula] [formula].

The L-hypersurface criterion consists of finding [formula] such that [formula] is the point of maximal Gaussian curvature of the L-hypersurface [formula]. Although approximating [formula] is most of the times very costly from a computational point of view, in [\cite=hyperlcurve] the authors show that a good approximation to [formula] is given by the minimizer of the residual norm, i.e., by the vector [formula] satisfying [formula].

We describe next the details of how the optimal regularization vector (λ̂1,λ̂2) given by the L-hypersurface approach is approximated within the KLSDA context formalized in Eq. ([\ref=KLSDA]) or, more precisely, in the context of the generalized e-net problem (Eq. ([\ref=Genet])). For [formula], define [formula], [formula] and [formula]. The L-hypersurface associated to ([\ref=KLSDA]) is then defined as [formula] [formula].

Although generalized e-net is defined in terms of λ1 and λ2, there are other possible choices for tuning parameters [\cite=enet]. For example, the [formula]-norm of the coefficients (t) can be chosen instead of λ1, In fact, this can be achieved by re-writing the LASSO version (Eq. ([\ref=ec2])) of our generalized e-net as a constrained optimization problem with an upper bound on [formula]. Similarly, since the LARS-EN algorithm is a forward stagewise additive fitting procedure, the number of steps κ of the algorithm can also be used as a tuning parameter replacing λ1. This is so because, for each fixed λ2, LARS-EN produces a finite number of vectors [formula] which are approximations of the true solution at each step κ. In our numerical experiment we adopted λ2 and κ as tuning parameters and, in accordance to Belge's remark described above, the best [formula] solutions were selected as those minimizing the residual norm. All the steps of the algorithm solving KLSDA with this proposal (together with automatic parameter selection) are presented in Algorithm [\ref=KLSDAalgSup].

P300 speller databases

Two real ERP-EEG databases were used to evaluate the classification performance of our KLSDA method.

Dataset-1

Dataset-1 is an open-access P300 speller database from Laboratorio de Investigación en Neuroimagenología at Universidad Autónoma Metropolitana, Mexico D.F., described in [\cite=base-datos]. This database consists of EEG records acquired from 25 healthy subjects, recorded by 10 channels (Fz, C3, Cz, C4, P3, Pz, P4, PO7, PO8, Oz) at 256 Hz sampling rate using the g.tec gUSBamp. During the acquisition, the EEG records were filtered with a Chebyshev Notch 4th order filter with cutoff frequencies of 58-62 Hz and a Chebyshev band-pass 8th order filter with cutoff frequencies of 0.1-60 Hz. A 6-by-6 matrix containing letters and numbers was presented to each subject on a computer screen. During the experiment, the subject was asked to spell different words. The person had to focus on one character at the time. As stimulus, a row or a column of the matrix was randomly highlighted for a period of 62.5 ms with inter-stimuli intervals of 125 ms. In each stimulating block (consisting in 12 consecutive flashings), every row and every column of the matrix was intensified only once. For each character to be spelled this stimulating block was repeated 15 times. If the person was well concentrated, a relevant event occurred when the chosen character was illuminated, i.e. an ERP was elicited. Thus, in a binary classification problem, the 6-by-6 matrix generates twelve possible events, of which only two are labelled as target.

Each subject participated in 4 sessions, of which the first two were copy-spelling runs, i.e, they contained the true label data vector, with 21 characters to be spelled. For this reason, in the present work we used those two copy-spelling sessions as our dataset. The EEG records were filtered from 0.1 Hz to 12 Hz by a 4th order forward-backward Butterworth band-pass filter. A 1000 ms data segment was extracted (windowed) from the EEG records at the beginning of each stimulus. A total of 3780 EEG patterns (of which only 630 are target) with dimensionality of 10  ×  256 = 2560 conforms the dataset per each subject.

Dataset-2

Dataset-2 is a P300 speller database of patients with amyotrophic lateral sclerosis (ALS) obtained from the Neuroelectrical Imaging and BCI Laboratory, IRCCS Fondazione Santa Lucia, Rome, Italy. The EEG data were recorded using g.MOBILAB and g.Ladybird with active electrode (g.tec, Austria) from eight channels (Fz, Cz, Pz, Oz, P3, P4, PO7 and PO8). All channels were referenced to the right earlobe and grounded to the left mastoid. The EEG signals were digitized at 256 Hz. Eight participants with ALS were required to copy-spell seven predefined words of five characters each using a P300 speller paradigm. As in Dataset-1, a 6-by-6 matrix containing alphanumeric characters was used. The rows and columns of the matrix were randomly intensified for 125 ms, follow by an inter stimulus interval of the same length. In each stimulating block all rows and columns were intensified 10 times. For more details about this dataset we refer the reader to [\cite=P300ALS].

In a pre-processing stage, the EEG records from each channel were band-pass filtered from 0.1 Hz to 10 Hz using a 4th order Butterworth filter. Then, data segments of 1000 ms were extracted at the beginning of each intensification. The ALS P300 speller dataset consists of 4200 patterns, of which 700 are target, with 256  ×  8 = 2048 sample points.

Experiments and results

In this section we show how the KLSDA method is implemented in the context of the aforementioned real ERP-based BCI classification problem with both datasets described in Section [\ref=datasets]. Since we are dealing here with a binary classification problem (K = 2), there is only one direction vector [formula] in Eq. ([\ref=KLSDA]), with which a linear classifier has then to be implemented with the n  ×  1 projected data vector [formula].

In the computational implementation of Algorithm [\ref=KLSDAalgSup], the parameter λ2 was allowed to vary between 10- 8 and 10- 1 in a log-scale, and an upper bound of 800 was chosen for the [formula]-norm of the coefficients (t  ≤  800). All codes were run in MATLAB on an AMD FX(tm)-6300 Six-Core PC with 4GB of memory. It is timely to mention here that although the CPU-time needed to find the direction vector was around 2800 secs, once [formula] was found, training and testing the linear classifier with the projected data vector [formula] took only about 3.36 secs.

In experiments we decided to use the symmetric version of KLD, JKL, as a measure of discrepancy. The probability distribution of each class was estimated by using histogram with centered bins between the most minimum and most maximum value in target and non-target training data. We constructed an anisotropy matrix [formula], where [formula], with C being the constant that makes [formula], i.e., [formula]. Note that with this choice, di is large where JKL(i) is small and vice-versa. If there exist i0,1  ≤  i0  ≤  p, such that JKL(i0) = 0 then the matrix [formula] cannot be formally defined as above. This case, however, can be overcomed by simply replacing JKL(i0) by JKL(i0) + ε, for [formula], with ε very small. The anisotropy matrix [formula] so defined is clearly diagonal, symmetric and positive definite.

Four different configurations of KLSDA, denoted by KLSDA0, KLSDA1, KLSDA2 and KLSDA3, were implemented. The first one KLSDA0 correspond to the standard SDA approach with automatics parameter selection. The second and third ones, KLSDA1 and KLSDA2, incorporate the anisotropic matrix [formula] in the [formula] and [formula] penalizers, respectively. Finally, KLSDA3 incorporate the anisotropic matrix [formula] in both the [formula] and [formula] terms. Table [\ref=cuadro] summarizes these four configurations (where I denotes the p  ×  p identity matrix). The classification performances of KLSDA method is compared with those obtained with the original SDA approach.

Kullback-Leibler divergence for ERP detection

It is reasonable to think that the KLD is an appropriate measure for enhancing the impact of the P300 wave in the KLSDA solution, by selecting both the most discriminative channels and the most discriminative time samples. With this in mind, for both databases, the J-divergence at each sample was estimated as described in Section [\ref=KLAniso] (Eq. ([\ref=JKL])). An analysis of this J-divergence as a function of channel and time allowed us to detect which samples were the most discriminative ones. The corresponding plots for six selected subjects of Dataset-1 and six selected subjects of Dataset-2 are presented in Figure [\ref=KLD].

Note that although for most of the subjects the discriminative samples are mostly located in the 250-500 ms window (which in fact corresponds to the latency window of the P300 wave), in other cases (e.g. subject 7 and 14 from Dataset-1 and subject 1 from Dataset-2) the most discriminative samples are somewhat randomly distributed all over the plot. Moreover, there are cases in which for some channels the J-divergence shows no contribution at all to class separation (e.g. subject 8 from Dataset-1 and subject 2 from Dataset-2). Figure [\ref=KLD] also shows the high variability of the ERP morphology between subjects as it was pointed out in Section [\ref=Intro].

Classification results

There are different available measures for evaluating a BCI classification method [\cite=significant]. The receiver operator characteristics (ROC) curve is a powerful tool for evaluating a two-class unbalanced problem [\cite=ROC]. In the present work the Area Under the ROC Curve [\cite=AUC], denoted by AUC, was used as the classification performance measure. For avoiding classification bias, a 3-fold cross-validation procedure was implemented.

Figure [\ref=clasi] shows the classification results obtained for each subject from Dataset-1 with the KLSDA0, KLSDA1, KLSDA2 and KLSDA3 methods. Several remarks are in order. First of all, the method which results in the best classification performance seems to be subject dependent. A second observation is that the poorest classification performance corresponds precisely to those subjects with "randomly spread J-divergence". A final observation is that in nineteen of the twenty five cases there is at least one no trivial KLSDA configuration that outperforms the pure SDA (KLSDA0), while in the remaining six cases the performances are essentially equal.

A similar analysis can be made on the results obtained with Dataset-2 as depicted in Figure [\ref=clasi2]. Note that in this dataset, in only three cases one KLSDA configuration outperforms SDA.

The average classification results for each KLSDA configuration are presented in the last column of Figure [\ref=clasi] and Figure [\ref=clasi2] for Dataset-1 and Dataset-2, respectively. It is worth noting that for Dataset-1 the average classification result were over 77% and for Dataset-2 were over 75% for all KLSDA configuration. These results are very encouraging since an efficient BCI system requires of an accuracy above 70% to allow communication and device control [\cite=AdvanceChallenges].

We have also used both datasets to test the FLDA classifier with 3-fold cross-validation. As expected, due to the curse-of-dimensionality, the classification results were very poor. In fact, for Dataset-1 the average classification performance was only around 60% while for Dataset-2 it was near 65%. These results clearly indicate that regularization improves classification performance.

Sparsity analysis

Given that solution sparsity was desired, the mean of the number of non-zeros values for each KLSDA configuration was analysed. For the KLSDA0, KLSDA1, KLSDA2 and KLSDA3 methods, the mean of the percentage of non-zero values respect to the number of sample points for Dataset-1 were found to be 5.75%, 7.48%, 5.78% and 7.79%, respectively, while for Dataset-2 those values were found to be 12.27%, 13.03%, 12.98% and 13.19%. Note that for Dataset-2 sparsity is consistently and significantly lower. This fact most probably reflects the fact that ALS patients database (non-healthy subjects) involved more complex patterns. Finally, it is highly remarkable that all KLSDA configurations with such low percentages of non-zero values achieved such satisfactory classification performances. This observation allows us to conclude that KLSDA constitutes a very robust classifier method.

Figure [\ref=betas] depicts the morphology of the solution vectors for one subject of Dataset-1 in the four different KLSDA configurations. Between parentheses the number of non-zero values in the solution vector is shown. Note that this number is higher when the anisotropy matrix [formula] is used in the [formula] penalization term (KLSDA1 and KLSDA3), and also the amplitudes of the coefficients increase in those two cases.

Discussion

It is well known that LDA is a commonly used method for ERP classification purposes. Its wide use is mainly due to its robustness, simplicity and good classification performances. When the number of observations is significantly lower than their dimensionality, LDA performs poorly, reason for which several alternatives to this method have been proposed. In the present work we described different LDA approaches from the statistical literature and developed a new penalized sparse discriminant analysis method called Kullback-Leibler Penalized Sparse Discriminant Analysis. This new method not only inherits the good properties of SDA, but it also allows us to incorporate KLD-based discriminative information, in order to enhance classification performance.

It is important to highlight that our implementation of KLSDA incorporates automatic tuning parameter selection. In light of the sparsity degree of the solution and the classification performances obtained, this procedure has proved to be very adequate for choosing the two regularization parameters in the model.

We tested the new KLSDA approach with two real ERP-EEG datasets. An analysis of the classification results indicates that the KLSDA configuration leading to the best performance is subject depended. In particular, the classification results from Dataset-2 (ALS patients), show that adding KLD information into the solution may not necessarily result beneficial, specially when it provides no clear discriminative information. In regard to this, we shall compare these results with the ones obtained by using another discriminant information measure, as the asymmetric KLD or the ones presented in [\cite=johnson2001symmetrizing] [\cite=reid2011information], to cite a few.

It is remarkable that in those cases where KLSDA outperformed SDA there were no computational cost added. In fact, in our experiments we found that SDA's computational cost was the same or even higher than KLSDA's with two anisotropic penalizing terms (KLSDA3).

The results achieved by applying KLSDA in the context of ERP classification problems are high enough (over 75%) to ensure good communication between the brain of the person and the device being controlled. These results encourage us to continue research efforts. There is clearly much room for improvement. Further research is currently underway in several directions. For instance, different discrepancy measures, anisotropy matrices and penalizing terms can be considered.

A final remark is that, although the KLSDA method was inspired by the idea of solving the binary ERP classification problem in BCI systems, it can clearly be applied to any type of classification problem.

Acknowledgements

This work was supported in part by Consejo Nacional de Investigaciones Científicas y Técnicas, CONICET, through PIP 2014-2016 No. 11220130100216-CO, the Air Force Office of Scientific Research, AFOSR/SOARD, through Grant FA9550-14-1-0130 and and by Universidad Nacional del Litoral, UNL, through CAID-UNL 2011 Project No.525 within PACT "Señales, Sistemas e Inteligencia Computacional".

References