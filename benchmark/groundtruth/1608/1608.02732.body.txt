On Lower Bounds for Regret in Reinforcement Learning

Introduction

This is a brief technical note to clarify the state of lower bounds on regret for reinforcement learning. In particular, this paper:

Reproduces a lower bound on regret for reinforcement learning, similar to the result of Theorem 5 in the journal UCRL2 paper [\cite=Jaksch2010].

Clarifies that the proposed proof of Theorem 6 in the REGAL paper [\cite=Bartlett2009] does not hold using the standard techniques without further work. We suggest that this result should instead be considered a conjecture as it has no rigorous proof.

Suggests that the conjectured lower bound given by [\cite=Bartlett2009] is incorrect and, in fact, it is possible to improve the scaling of the upper bound to match the weaker lower bounds presented in this paper.

Problem formulation

We consider the problem of learning to optimize an unknown MDP M*  =  (S,A,R*,P*). S  =  {1,..,S} is the state space, A  =  {1,..,A} is the action space. In each timestep t = 1,2,.. the agent observes a state st∈S, selects an action at∈A, receives a reward rt  ~  R*(st,at)∈[0,1] and transitions to a new state st + 1  ~  P*(st,at). We define all random variables with respect to a probability space [formula].

A policy μ is a mapping from state s∈S to action a∈A. For MDP M and any policy μ we define the long run average reward starting from state s:

[formula]

where [formula]. The subscripts M,μ indicate the MDP evolves under M with policy μ. A policy μM is optimal for the MDP M if μM∈ arg  max μλMμ(s) for all s∈S. For the unknown MDP M* we will often abbreviate sub/superscripts to simply *  , for example λ** for λM*μM*.

Let Ht = (s1,a1,r1,..,st - 1,at - 1,rt - 1) denote the history of observations made prior to time t. A reinforcement learning algorithm is a deterministic sequence {πt|t = 1,2,..} of functions each mapping Ht to a probability distribution πt(Ht) over policies, from which the agent sample policy μt at timestep t. We define the regret of a reinforcement learning algorithm π up to time T

[formula]

The regret of a learning algorithm shows how worse the policy performs that optimal in terms of cumulative rewards. Any algorithm with o(T) regret will eventually learn the optimal policy. Note that the regret is random since it depends on the unknown MDP M*, the random sampling of policies and, through the history Ht on the previous transitions and rewards. We will assess and compare algorithm performance in terms of the regret.

Finite horizon MDPs

We now spend a little time to relate the formulation above to so-called finite horizon MDPs [\cite=Osband2013] [\cite=dann2015sample]. In this setting, an agent will interact repeatedly with a environment over [formula] timesteps which we call an episode. A finite horizon MDP M*  =  (S,A,R*,P*,H,ρ) is defined as above, but every [formula] timesteps the state will reset according to some initial distribution ρ. We call [formula] the horizon of the MDP.

In a finite horizon MDP a typical policy may depend on both the state s∈S and the timestep h within the episode. To be explicit, we define a policy μ is a mapping from state s∈S and period h = 1,..,H to action a∈A. For each MDP [formula] and policy μ we define the state-action value function for each period h:

[formula]

and VMμ,h(s): = QMμ,h(s,μ(s,h)). Once again, we say a policy μM is optimal for the MDP M if [formula] for all s∈S and [formula].

At first glance this might seem at odds with the formulation in Section [\ref=sec:_prob_form]. However, finite horizon MDPs can be thought of as a special case of Section [\ref=sec:_prob_form] in the expanded state space : = S  ×  {1,..,H}. In this case it is typical to assume that the agent knows about the evolution of time h deterministically a priori. To highlight this time evolution within episodes, with some abuse of notation, we let skh  =  st for t = (k - 1)H + h, so that skh is the state in period h of episode k. We define Hkh analogously.

Multi-armed bandit

We call the degenerate MDP with only one state S = 1 a multi-armed bandit with independent arms [\cite=lai1985asymptotically]. In this setting the actions at∈A are often called "arms" and the optimal average reward is simply the average reward of the highest reward,

[formula]

We now reproduce a lower bound on regret for any learning algorithm in a multi-armed bandit [\cite=Bubeck2012regretBandit].

Let sup  be the supremum over all distributions of rewards such that for each a = 1,..,A the rewards r(1)t,..,r(A)t∈{0,1} are i.i.d. and let inf  be the infimum over all reinforcement learning algorithms. Then

[formula]

At a high level Theorem [\ref=thm:_bandit_lower] says that no matter what learning algorithm you choose, there will always be some environment which gives your algorithm [formula] regret. This is a pretty powerful result, since it means that if we can design an algorithm with upper bounds on regret [formula] then this algorithm is in some sense near-optimal [\cite=Bubeck2012regretBandit].

The intuition for the proof is relatively simple and presented in [\cite=Bubeck2012regretBandit]. After any T timesteps there must be some arm which is pulled less than T  /  A times. Standard concentration results state that the estimates of a random variable can only be accurate up to [formula] where n is the number of observations. Therefore, for the arm with n  ≤  T  /  A it is difficult to distinguish between a Ber(1 / 2) and [formula]. This means that, if every arm is Ber(1 / 2) but one [formula], any algorithm would incur [formula] regret. In the next section we will see how to make this argument more rigorous.

Proof of Theorem [\ref=thm:_bandit_lower]

We consider the problem where all arms are i.i.d. Bernoulli with parameter δ, but one arm a* has parameter δ  +  ε for some δ,ε  >  0. We define an auxilliary t(a)  =  rt(a) for all a  ≠  a*, but with the rewards of the action a = a* replaced by the draw t  ~  Ber(δ). We consider an auxilliary sequence of actions ãt  ~  πt(H̃t) for H̃t  =  (ã1,1,..,ãt - 1,t - 1) as the history generated by an agent with no feedback informing them about a*.

We introduce the notation nT(a): = |{at  =  a|t = 1,..,T}| and ñT(a): = |{ãt  =  a|t = 1,..,T}| to denote the number of times arm a have been selected by time T under at and ãt respectively. The following lemma establishes a lower bound on the regret realized by action ãt.

For all δ,ε  >  0 and all learning algorithms π,

[formula]

We have,

[formula]

where the last step follows from a symmetry argument, since a* is independent of ñt(a) for all actions a.

We now establish that, if ε is sufficiently small, then over a limited time horizon the distributions of t(at) cannot be significantly different from the outcomes rt(at). We compare the conditional distributions over the choice of action P with the choice of actions P̃ which would have arisen under the uninformative data Ht. To be more precise we define [formula] with [formula]. We write rTt: = (rt(at),..,rT(at)) for the sequence of rewards from time t to T and similarly for Tt. To quantify the difference between two distributions we will employ the following notion of KL divergence:

[formula]

For all δ,ε  >  0 and all learning algorithms π,

[formula]

We can apply the chain rule of KL divergence [\cite=Bubeck2012regretBandit] to obtain

[formula]

It follows that

[formula]

We conclude the proof by noting that the actions ãt are selected indepedently of of a* together with a symmetry argument.

We now use Pinsker's inequality to show that, if the distribution of actions P is close to the choice of actions under uninformative data P̃ then the resulting regret is close to the regret of the uninformative policy.

For all δ,ε  >  0 and all learning algorithms π,

[formula]

Pinsker's inequality gives us

[formula]

Since [formula], it follows that [formula] We complete the proof of through a simple substitution in Lemma [\ref=lem:_regret_uninformed].

To complete the proof of Theorem [\ref=thm:_bandit_lower] we can use Lemma 20 from [\cite=Jaksch2010].

For any [formula] and ε  ≤  1  -  2δ we have

[formula]

We combine Proposition [\ref=prop:_bound_kl] with Lemma [\ref=lem:_regret_kl] to say,

[formula]

We can choose δ  =  0.25 to complete the proof of Theorem [\ref=thm:_bandit_lower]. We note that better constants are available through a more careful analysis, but this is not our focus in this work.

Reinforcement learning

In this section we will work to extend the lower bound arguments from bandits to reinforcement learning with S  ≥  2. As in common in the literature, we will begin with a simple two state MDP with known rewards and unknown transitions [\cite=Jaksch2010] [\cite=Bartlett2009] [\cite=dann2015sample]. It is relatively straightforward to extend this flavour of result to MDPs with S  >  2 simply by concatenating ⌈S / 2⌉ copies of these smaller systems.

State 0 gives a reward of 0 and state 1 gives a reward of 1. All actions from the state 0 follow the same law P(0,a)  =  (1 - δ0,δ0). In state 1 P(1,a)  =  (δ1,1 - δ1) for all actions apart from P(1,a*)  =  (δ1  -  ε,1  -  δ1  +  ε). For this simple MDP we will distinguish policies in terms of their action upon s = 1, since this is the only action which can influence the evolution of the MDP.

We define [formula] to be the average expected reward under the policy a  ≠  a*. For convenience we write δ*1: = δ1  -  ε for the distinguished optimal action and correspondingly [formula] for the average expected reward under the optimal policy a*.

Sketch at REGAL-style lower bounds

In this section we present a quick overview of the style of argument that attempts to solidify the lower bound of Theorem 6 in [\cite=Bartlett2009]. We assume that δ0  ≥  δ1 to bound the difference in optimal value,

[formula]

Broadly speaking, this indicates that the agent should obtain expected regret Ω(ε  /  δ0) every timestep it selects action at  ≠  a* whilst in state s = 1. All other actions in any other state produce zero regret. We now note that the problem described by Figure [\ref=fig:_lower_bound] is quite similar to the bandit example from Section [\ref=sec:_mab]. The difference here is that actions of the suboptimal arm a  ≠  a* give expected regret [formula], rather than ε.

The arguments we present in this section can be thought of as an attempt to make the sketch proof for Theorem 6 of [\cite=Bartlett2009] more explicit, if not entirely rigorous. Our arguments will follow the same structure as Section [\ref=sec:_mab]: we consider an auxilliary MDP where the optimal action a* has been replaced by another action with identical transition dynamics. We will write ãt for the actions which are taken by this uninformed policy and t for the uninformative history that it generates. We begin with a result of a similar flavour to Lemma [\ref=lem:_regret_kl].

In the environment of Figure [\ref=fig:_lower_bound], for all δ,ε  >  0 and all learning algorithms π,

[formula]

We note that the uninformed agent can only incur regret when it makes a sub-optimal decision, which is only possible in state s = 1. The proportion of the time the agent spends in state s = 1 is lower bounded by θ1. The regret for any sub-optimal decision while in state s = 1 is at least [formula] by [\eqref=eq:_mdp_step_regret]. We follow the arguments from Lemma [\ref=lem:_regret_uninformed] to obtain our desired result.

We now note that the problem of learning a 2-state transition function is equivalent to estimating a Bernoulli reward. Therefore, we can use Lemma [\ref=lem:_mdp_regret_uninformed] in place of Lemma [\ref=lem:_regret_uninformed] and repeat a similar argument to the proof of Theorem [\ref=thm:_bandit_lower] for multi-armed bandits. At a high level we can bound the regret of any agent in terms of the deviation in KL from the distribution of the uninformed agent. For ε small, and over a short enough time window T, the distribution of actions chosen by the learning algorithm cannot differ significantly from the actions chosen from the uninformative system. As such, using Pinsker's inequality, the resulting regret from any learning algorithm cannot differ significantly from that of the uninformed algorithm.

To make this argument explicit, we use Lemma [\ref=lem:_kl_uninformed] and Lemma [\ref=lem:_regret_kl] together with Proposition [\ref=sec:_prob_form] and optimize over the resulting bound over ε. That is to say, for any learning algorithm π,

[formula]

Now, we are left with a problem to complete the argument for Theorem 6 from REGAL. We introduce the notation, TMμ(s,s') for the expected number of timesteps to get from state s to s' in MDP M under policy μ. The one-way diameter of an MDP is defined

[formula]

The claim in Theorem 6 of REGAL is that, for any learning algorithm π there exists and MDP M such that [formula] for some c0  >  0.

From construction of the MDP in Figure [\ref=fig:_lower_bound] it is clear that [formula], since the only state with optimal value bias is s = 1 and the expected time from s = 0 to s = 1 is [formula]. We now examine behaviour of the remaining free parameters using the definition θ1  =  δ0  /  (δ0  +  δ1):

[formula]

This completes the demonstration that the standard proof techniques for lower bounds do not address the problems in the proof REGAL Theorem 6. In fact, we are only able to establish a lower bound [formula] and not [formula] as [\cite=Bartlett2009] had claimed. Further, these bounds are actually weaker than the established results in [\cite=Jaksch2010] [formula], where D(M): =  max s,s' min μTMμ(s,s')  ≥  Dow is the diameter of the MDP.

Where do the lower bounds lie?

The arguments in Section [\ref=sec:_regal_bound] show that existing machinery is not sufficient to establish a proof of Theorem 6 in [\cite=Bartlett2009]. In light of this we suggest that this published result be considered a conjecture, rather than an established theorem. In this note we present another alternative conjecture, that the results of Theorem 6 in [\cite=Bartlett2009] are not correct. The spirit of this conjecture is similar to Conjecture 1 of [\cite=osband2016posterior] given for finite horizon MDPs.

The lower bounds of [\cite=Jaksch2010] [formula] are unimprovable in the sense that there exists some learning algorithm π such that, for any MDP M* and any δ > 0

[formula]

with probability at least 1  -  δ.

What is wrong the REGAL lower bound?

In order for Conjecture [\ref=conj:_tight_lower] to be true, the sketched proof in [\cite=Bartlett2009] must be false. Although the arguments of Section [\ref=sec:_regal_bound] show that this proof is not yet rigorous, they do not pinpoint any step of the appealing sketched argument which is incorrect. However, we will now present an intuitive argument for what may be going wrong in the sketched proof:

For every timestep t in state s = 1 the worst possible decision the agent could make will contribute regret O(Dow) in terms of the value. The proposed sketch proof argues that the agent effectively incurs this regret every timestep until it learns the optimal arm.

If we measure regret in terms of actual shortfall in the instantaneous regret λ**  -  rt must be bounded O(1) per timestep. The bad decisions in state s = 1 are just worth O(Dow) value because it might lead to O(Dow) of these O(1) instantaneous regret steps to occur in a row.

Alternatively, we might think of regret in terms of the future value O(Dow which a bad decision at s = 1 may be worth - this is the argument that REGAL uses [\cite=Bartlett2009]. However, if we do this then that means this bad decision must be followed by O(Dow) timesteps in which we count no additional regret.

At the moment, the argument for Theorem 6 in [\cite=Bartlett2009] is doing a type of double-counting for regret. It assigns the maximum O(Dow(M*)) regret in terms of value at each timestep. However, this analysis ignores that for every one of these bad actions there will be O(Dow)(M*) periods of time within s = 0 where, in terms of the value shortfall, these actions will not incur further regret than has been counted already.

Comparison to existing tight PAC bounds

Another piece of tangentially supporting evidence for Conjecture [\ref=conj:_tight_lower] comes from the recent PAC-analysis for finite horizon MDPs [\cite=dann2015sample]. The problem formulation given by this paper differs from [\cite=Bartlett2009] in several ways, but they produce an algorithm LUCFH which matches upper and lower bounds for the horizon H in finite horizon MDPs. In finite horizon MDPs, the horizon H is an upper bound on Dow. A similar flavour of result is available in discounted MDPs [\cite=lattimore2012pac] where the horizon H is replace with an equivalent timeframe [formula].

The analysis for LUCFH in finite horizon MDPs implies that the number of episodes required for ε-optimal episodes is [formula], where we view all variables other than H and ε as fixed. According to their definition, this would imply [formula] timesteps until ε-optimal episodes, which is roughly equivalent to [formula] timesteps until ε-optimal timesteps.

At a high level the algorithm and analysis from [\cite=dann2015sample] leverages the sort of phenomenon we describe in Section [\ref=sec:_bad_regal]. This essential argument is refined and made more rigorous through the Bellman equation for local variance, first used in [\cite=lattimore2012pac]. It is not generally possible to go from PAC bounds to regret guarantees, however, the spirit of previous analyses and comparable results suggest that the tight bounds [formula] timesteps until ε-optimal timesteps are suggestive of a tight regret scaling [formula].

Conclusion

This technical note aims to clarify the current state of lower bounds for regret in reinforcement learning. We reproduce a clear step by step argument for the lower bound on regret given in [\cite=Bartlett2009]. We show that, using standard machinery, this leads to a provable lower bound [formula] and currently there is no proof available for the bound [formula] as conjectured in that earlier work. To stimulate thinking on this topic, we present Conjecture [\ref=conj:_tight_lower], that the lower bound [formula] is in fact unimprovable. Definitively proving these results one way or another is an exciting area for future research.

Acknowledgements

We would like to thank the authors of [\cite=Bartlett2009] for their help and dialogue in the discussion of these delicate technical issues. We would also like to thank Daniel Russo for the many hours of discussion and analysis spent in the office on issues like these.