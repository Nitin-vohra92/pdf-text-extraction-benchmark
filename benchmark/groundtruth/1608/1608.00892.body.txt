Knowledge Distillation for Small-footprint Highway Networks

Index Terms: Knowledge distillation, Highway deep neural networks, Small-footprint

Introduction

Recent years have witnessed wide applications of speech technology in embedded devices like mobile phones, thanks to deep learning that has significantly advanced state-of-the-art in this area. For scenarios that internet connections are unavailable or for privacy concerns, it is desirable that speech recognisers can run locally in such kind of resource constrained platforms. However, state-of-the-art neural network models are either computationally expensive or consume large amount of memory, and are therefore unsuitable for this purpose. Recently, there have been a number of works on small footprint acoustic models to address this problem such as using low-rank matrices [\cite=xue2013restructuring] [\cite=sainath2013low], structured linear layers [\cite=le2013fastfood] [\cite=sindhwani2015structured] [\cite=moczulski2015acdc], and the use of low rank displacement of structured matrices [\cite=sindhwani2015structured]. Instead of manipulating the model parameters, another approach is based on the teacher-student architecture [\cite=li2014learning] [\cite=ba2014deep] [\cite=romero15_fitnet], which is also known as model compression [\cite=bucilu2006model] or knowledge distillation [\cite=hinton2015distilling]. In this approach, the teacher may be a large-size network or an ensemble of several different models, which is used to predict the soft targets for training the student model that is much smaller. As pointed out in [\cite=hinton2015distilling], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these pseudo labels is observed to perform better than the same model trained independently using the ground truth labels [\cite=hinton2015distilling].

Previously, we studied a compact acoustic model using highway deep neural network (HDNN) for resource constrained speech recognition [\cite=llu2016a]. HDNN is a type of network with shortcut connections between hidden layers [\cite=srivastava2015training]. Compared to the plain networks with skip connections, HDNNs are equipped with two gate functions - transform and carry gate - to control and facilitate the information flow over all the whole network. In particular, the transform gate is used to scale the output of a hidden layer and the carry gate is used to pass through the input directly after elementwise rescaling. The gate functions are the key to train very deep networks [\cite=srivastava2015training] and to speed up convergence as experimentally validated in [\cite=llu2016a]. We have shown that the gate functions can manipulate the behaviour of the whole neural networks in sequence training and adaptation [\cite=lu2016sequence]. With the gate functions, we can train much thinner and deeper networks with much smaller number of model parameters, which can achieve comparable recognition accuracy compared to much larger plain DNNs.

In this paper, we investigate teacher-student training to further improve the accuracy of the small-footprint HDNN acoustic model. In particular, we use a large size plain DNN acoustic model to provide soft labels for training the student HDNN model. As mentioned before, there have been a number of work on teacher-student training for speech recognition [\cite=li2014learning] [\cite=ba2014deep] [\cite=hinton2015distilling]. The one that is closest to our study is [\cite=li2014learning]. However, the student model investigated in this paper is much smaller due to the highway architecture. In addition, we present further analysis and experimental study on hybrid loss functions that interpolate the cross-entropy and teacher-student costs, the use temperature to smooth the soft labels and sequence training and adaptation results in this context.

Highway Deep Neural Networks

In this work, we focus on feed-forward neural networks - also known as DNNs - as target student models. Although long short-term memory based recurrent neural networks (LSTM-RNNs) and convolutional neural networks (CNNs) may obtain higher recognition accuracy compared to DNNs [\cite=sak2014long] [\cite=abdel2014convolutional], they are more suitable for teacher models because they are usually computational more expensive for applications on resource constrained platforms. A plain DNN with L hidden layers may be represented as

[formula]

where [formula] is an input vector to the network at the time step t; [formula] denotes the transformation of the input [formula] with the parameter θl followed by a nonlinear activation function, e.g., sigmoid; g(  ·  ,φ) is the output function that is parameterised by φ in the output layer, which usually uses the softmax to obtain the posterior probability of each class given the input feature.

Highway deep neural networks [\cite=srivastava2015training] augment the feature extractor with gate functions, in which the hidden layer may be represented as

[formula]

where T(  ·  ) is the transform gate that scales the original hidden activations; C(  ·  ) is the carry gate, which scales the input before passing it directly to the next hidden layer; [formula] denotes elementwise multiplication; The outputs of T(  ·  ) and C(  ·  ) are constrained to be within

[formula]

Model Training

Cross-Entropy Training

The most common criterion to train neural networks for classification is cross-entropy (CE), which defines the loss function as

[formula]

where j is the index of the hidden Markov model (HMM) state; [formula] is the output of the nerual network as Eq. [\eqref=eq:sm], while t denotes the ground truth label that is a one-hot vector. Note that, the loss function is defined with one training utterance here for the simplicity of notation. Supposing that ŷjt  =  δij, where δij is the Kronecker delta function and i is the ground truth class at the time step t, the CE loss becomes

[formula]

In this case, minimising L(CE)(θ) is equivalently to minimise the negative log posterior probability of the correct class, and it is equal to maximising the probability yit, while the posterior probabilities of other classes are ignored. However, maximising yit will also result in minimising the posterior probabilities of other classes since they sum to one.

Teacher-Student Training

Instead of using the ground truth labels, the teacher-student training approach defines the loss function as

[formula]

where jt is the output of the teacher model, which works as a pseudo label. As pointed out in [\cite=li2014learning], the loss function as Eq. [\eqref=eq:kd] is equivalent to minimise the Kullback-Leibler divergence between the posterior probabilities of each class from the teacher and student models. Here, jt is no longer a one-hot vector, instead, the competing classes will have small but nonzero posterior probabilities for each training example. Hinton et al. [\cite=hinton2015distilling] suggested that the small posterior probabilities are valuable information that encodes correlations among different classes. However, their roles may be very small in the loss function as these probabilities are close to zero due to the softmax function. To address this problem, they suggested to use a large temperature to flatten the posterior distribution as

[formula]

where [formula] are parameters in the softmax layer, and [formula] is the temperature. Following [\cite=hinton2015distilling], we applied the same temperature to the softmax functions in both the teacher and student networks in our experiments, as only increasing the temperature in the teacher network resulted in much higher error rates in our experiments.

One particular advantage of the teacher-student training approach is that unlabelled data can be used easily. However, when the ground truth labels are available, it may be beneficial to incorporate the ground truth information into the loss function, which can be done by interpolating the two loss functions as

[formula]

where [formula] is the tuning parameter. We denote this as the hybrid loss, and it will be studied in the experimental section.

Sequence Training

While the previous two loss functions are defined at the frame level, sequence training defines the loss at the sequence level, which usually yields significant improvement for speech recognition [\cite=kingsbury2012scalable] [\cite=Vesely:IS13] [\cite=su2013error]. If we denote [formula] as the sequence of acoustic frames [formula] and [formula] as the sequence of labels, where T is the length of the signal, the loss function from the scalable minimum Bayesian risk criterion(sMBR) [\cite=gibson2006hypothesis] [\cite=kingsbury2009lattice] is defined as

[formula]

where [formula] measures the state level distance between the ground truth and predicted labels; Φ denotes the hypothesis space represented by a denominator lattice, and W is the word-level transcription; k is the acoustic score scaling parameter. In this paper, we only focus on the sMBR criterion since it can achieve comparable or slightly better results compared to the maximum mutual information (MMI) or minimum pone error (MPE) criterion [\cite=Vesely:IS13].

For sequence training, the acoustic model is normally firstly trained with the CE loss function, which is then fine tuned with the sequence-level loss for a few iterations. While for knowledge distillation, the model is firstly trained with the loss function as Eq. [\eqref=eq:kd]. This may raise the question that if the improvement will diminish in sequence training, and we will perform experimental study to unswear this question. Note that, only applying the sequence training criterion without regularisation may lead to overfitting as observed in [\cite=Vesely:IS13] [\cite=su2013error]. To address this problem, we interpolate the sMBR loss function with the CE loss [\cite=su2013error]. However, for the case of knowledge distillation, we apply the following interpolation:

[formula]

where [formula] is the smoothing parameter.

Experiments

System Setup

Our experiments were performed on the individual headset microphone (IHM) subset of the AMI meeting speech transcription corpus [\cite=renals2007recognition].The amount of training data is around 80 hours, corresponding to roughly 28 million frames. We followed the experimental setup in [\cite=lu2016sequence]. We used 40-dimensional fMLLR adapted features vectors normalised on the per-speaker level, which were then spliced by a context window of 15 frames (i.e. [formula]). The number of tied HMM states is 3927. The HDNN models were trained using the CNTK toolkit [\cite=yu2014introduction], while the results were obtained using the Kaldi decoder [\cite=povey2011kaldi]. We also used the Kaldi tookit to compute the alignment and lattices for sequence training. We set the momentum to be 0.9 after the 1st epoch for CE training, and we used the sigmoid activation for all the networks. The weights in each hidden layer of HDNNs were randomly initialised with a uniform distribution in the range of

[formula]

Loss Function and Temperature

We firstly compare the teacher-student loss function as Eq. [\eqref=eq:kd] and the hybrid loss function as Eq. [\eqref=eq:hybrid]. We used a CE trained plain DNN-H2048L6 as the teacher model, and used the HDNN-H128L10 as the student model. Figure [\ref=fig:kd] shows the convergence curves when training the model with different loss functions, while Table [\ref=tab:kd] shows the WERs. We observe that by teacher-student training without the ground truth labels, we can achieve significantly lower frame error rate on the cross validation set as shown in Figure [\ref=fig:kd], corresponding to moderate WER reduction (31.3% vs. 32.0 on the eval set). However, using the hybrid loss function as Eq. [\eqref=eq:hybrid], we do not obtain further improvement. In fact, it converges slower when q > 0 during training as shown in Figure [\ref=fig:kd]. Our interpretation is that it may be because the probabilities of uncorrected classes played a smaller role in this case, which supports the argument that they encode useful information for training the student model [\cite=hinton2015distilling]. This hypothesis encouraged us to investigate the use of a large temperature to flatten the posterior probability distribution of the labels from the teacher model. The results are also shown in Table [\ref=tab:kd]. Contrary to our expectation, using large temperatures results in higher WERs. In the following experiments, we fixed q = 0 and T = 1.

Teacher Model

We then improved the teacher model by sMBR-based sequence training, and used this model to supervise the training of the student model. Similar to the observations in [\cite=li2014learning], the sMBR-based teacher model can significantly improve the performance of the student model. In fact, the error rate is lower than that achieved by the student model trained independently with sMBR as shown in Table [\ref=tab:seq] (28.8% vs. 29.4% on the eval set). Note that, since the sequence training criterion is not to maximise the frame accuracy, training the model with this criterion normally reduces the frame accuracy, as shown explicitly by Figure 6 in [\cite=heigold2014asynchronous]. Interestingly, we observed the same pattern in the case of teacher-student training. Figure [\ref=fig:seq] shows the convergence curves of using CE and sMBR based teacher models, where we see that the student model achieves much higher frame error rate on the cross validation set when supervised by sMBR-based teacher model, though the loss function as Eq. [\eqref=eq:kd] is at the frame level.

We then study if the accuracy of the student model can be further improved by the sequence level criterion. Here, we set the smoothing parameter p = 0.2 in Eq. [\eqref=eq:reg] and the default learning rate to be 1  ×  10- 5 following our previous setup in [\cite=lu2016sequence]. Table [\ref=tab:seq] shows the sequence training results of student models supervised by the CE and sMBR-based teacher models respectively. Note surprisingly, the student model supervised by the CE-based DNN model can be significantly improved by the sequence training. Notably, the WER obtained by this approach is lower compared to the model trained independently with sMBR (28.4% vs. 29.4% on the eval set). However, this configuration did not work for the student model supervised by the sMBR-based teacher model. After inspection, we found that it was due to overfitting. We then increased the value of p for stronger regularisation and reduced the learning rate. Lower WERs can be obtained as the table shows, however, the improvement is less significant as the sequence level information has already been integrated into the teacher model.

Unsupervised Adaptation

Our final experiments concern adaptation. Neural network acoustic models are less adaptable due to large number of unstructured model parameters compared to conventional acoustic models using Gaussian mixtures. However, a smaller model may be easier to adapt. In particular, the gate functions in HDNNs are more adaptable since they have much smaller number of model parameters, e.g., the total number of parameters in [formula] of an HDNN-H128L10 acoustic model is around 0.03 million. Furthermore, only updating the gate functions does not easily yield overfitting with small amount of adaptation data and pseudo labels [\cite=lu2016sequence]. We performed similar adaptation experiments for HDNN trained by the teacher-student approach. We applied the second-pass adaptation approach for the standalone HDNN model, i.e., we decoded the evaluation utterances to obtain the hard labels first, and then used these labels to adapt the model using the CE loss as Eq. [\eqref=eq:ce]. However, using the teacher-student loss as Eq. [\eqref=eq:kd], only one pass decoding is required because the pseudo labels for adaptation are provided by the teacher, which does not need the word level transcription. This is a particular advantage of the teacher-student training technique. However, for resource constrained application scenarios, the student model should be adapted offline, because the teacher model needs to be accessed to generate the labels. This requires another set of unlabelled speaker-dependent data, however, is is usually inexpensive to collect.

The standard AMI corpus does not have an additional set of speaker-dependent data, we only show online adaptation results. We used two sequence trained speaker-independent seed models, which was either trained independently or by teacher-student training. During adaptation, we updated the model by 5 iterations with fixed learning rate as 2  ×  10- 4 for sample following our previous setup [\cite=lu2016sequence]. We also compared the CE loss as Eq. [\eqref=eq:ce] and the teacher-student loss as Eq.[\eqref=eq:kd] for adaptation. Results are given in Table [\ref=tab:adap]. For the independent model, only updating the gates yields slightly better results. While for the teacher-student trained model, only updating the gates results in relatively much higher accuracy compared to updating all the model parameters when using the CE loss function for adaptation. Interestingly, we have a different observation when using the teacher-student loss, i.e. updating all the model parameters yield lower WER. These results may agree with the argument in [\cite=hinton2015distilling] that the soft targets can work as a regulariser and can prevent the student model from overfitting.

Conclusions

In this paper, we investigated the teacher-student training for small-footprint acoustic models using HDNNs. We observed that the accuracy of the student acoustic model could be improved under the supervision of a high accuracy teacher model, even without additional unsupervised data. In particular, the student model supervised by a sMBR-based teacher model achieved lower WER compared to the model trained independently using the sMBR-based sequence training approach. Unsupervised speaker adaptation further improved the recognition accuracy by around 5% relative for our model with less then 0.8 million model parameters. However, we did not obtained improvements by using the hybrid loss function by interpolating the CE and teacher-student loss functions, and using higher temperature to smooth the pseudo labels did not help either. In the future, we shall evaluate this model on low resource conditions where the amount of training data is much smaller.