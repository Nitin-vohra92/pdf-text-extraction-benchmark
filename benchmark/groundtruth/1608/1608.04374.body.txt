Lemma

A Geometric Framework for Convolutional Neural Networks

Introduction

Machine Learning algorithms have long worked with multi-dimensional vector data and parameters, but have not exploited the underlying inner product space structure. A recent paper on deep learning in Nature called for "new paradigms" involving "operations on large vectors" [\cite=lecun2015deep] to propel the field forward. This approach is taken to describe the convolutional neural network (CNN) in this paper. In particular, the layers are described as vector-valued maps, and gradients of these maps with respect to the parameters at each layer are taken in a coordinate-free manner. This approach promotes a greater understanding of the network than a coordinate-based approach, and allows for loss function gradients to be calculated compactly using coordinate-free backpropagation of error. This paper also considers a higher-order loss function, as in [\cite=rifai2011manifold] and [\cite=simard1992tangent], and provides a method for gradient descent for that loss. The framework developed in this paper can be extended to cover other types of network structures and even inspire the creation of new networks.

Mathematical Preliminaries

Some prerequisite notation and concepts are introduced here before CNNs can be fully described.

Multilinear Algebra & Derivatives

Every individual vector space is assumed to be an inner product space, with the inner product represented by 〈  ,  〉. The inner product naturally extends to the direct product [formula] of inner product spaces [formula] and their tensor product [formula] as follows [\cite=werner1978multilinear]:

[formula]

where ei,ēi∈Ei, [formula]. The symbol [formula] is exclusively used to denote the tensor product operator in this paper. An inner product space E is canonically identified here with its dual space E* using the inner product on E, so dual spaces will rarely be used in this paper. The set of r-linear maps from [formula] to a vector space F is denoted by [formula]. For a linear map L∈L(E;F), its adjoint map, denoted by L*, is a linear map in L(F;E) defined by the relationship 〈L*f,  e〉  =  〈f,  Le〉 for all e∈E and f∈F. For each vector e1∈E1 and any bilinear map B∈L(E1,E2;F), define a linear map [formula] by

[formula]

for all e∈E2. Likewise, for each vector e2∈E2 and any bilinear map B∈L(E1,E2;F), define a linear map [formula] by

[formula]

for all e∈E1.

Now, notation for derivatives in accordance with [\cite=marsden1988manifolds] is presented. Consider a map f:E1  →  E2. The (first) derivative [formula] of f at a point x∈E1 is a linear map from E1 to E2, i.e. [formula], and it can be defined as

[formula]

for any v∈E1. The derivative [formula] can be viewed as a map from E1 to L(E1;E2), defined by [formula]. Let [formula] denote the adjoint of [formula] so that [formula] for all v∈E1 and w∈E2.

Now consider a map f:E1  ×  F1  →  E2 written as f(x;θ) for x∈E1 and θ∈F1, where the semi-colon is inserted between x and θ to distinguish the state variable x from the parameters θ. Let [formula] denote the derivative of f with respect to x evaluated at (x;θ), and let [formula] denote the derivative of f with respect to θ evaluated at (x;θ). It is easy to verify that [formula] and [formula] and that

[formula]

for all e∈E1 and u∈F1. The adjoints of [formula] and [formula] are denoted by [formula] and [formula], respectively. Sometimes, [formula] is written instead of [formula], to emphasize differentiation of f with respect to the parameter variable θ.

The second derivative [formula] of f with respect to x evaluated at (x;θ) is a bilinear map in L(E1,E1;E2) defined as follows: for any e,ē∈E1,

[formula]

It is assumed that every function that appears in this paper is (piecewise) twice continuously differentiable. The second derivative [formula] is symmetric, i.e. [formula] for all e,ē∈E1. The second derivative [formula] of f with respect to x and θ at the point (x;θ) is a bilinear map in L(E1,F1;E2) defined as follows: for any e∈E1 and u∈F1,

[formula]

On the other hand, the second derivative [formula] of f with respect to θ and x at the point (x;θ) denotes the bilinear map in L(F1,E1;E2) defined as follows: for any u∈F1 and e∈E1,

[formula]

Note that for all e∈E1 and u∈F1, it is easy to verify that

[formula]

Backpropagation in a Nutshell

Now, backpropagation will be presented in a coordinate-free form. Given two maps f1(x;θ1)∈E2 for x∈E1,θ1∈F1 and f2(z;θ2)∈E3 for z∈E2,θ2∈F2, the composition [formula] is the map defined as follows:

[formula]

for x∈E1,θ1∈F1,θ2∈F2. In this framework, functions are composed with respect to the state variables. By the chain rule,

[formula]

which are evaluated at a point (x;θ1,θ2) as follows:

[formula]

where the dependency on the parameters θ1 and θ2 is suppressed for brevity, which shall be understood throughout the paper. In particular, taking the adjoint of [formula] produces

[formula]

which is backpropagation in a nutshell. This can be seen by the following: consider a loss function J defined by

[formula]

for some vector y∈E3 that may depend on x, along with f as in ([\ref=eqn:f_comp]). Then, for any u∈F1, with [formula] representing the parameters,

[formula]

Since this holds for any u∈F1, the canonical identification of an inner product space with its dual is used to obtain

[formula]

where ([\ref=backprop:nutshell]) is used for the second equality. This shows that the error (f(x;θ)  -  y) propagates backward from layer 2 to layer 1 through multiplication by [formula]. The adjoint operator reverses the direction of composition, i.e. (L1L2)*  =  L*2L*1, which is the key to backpropagating the error.

The second derivative [formula] of [formula] is given by

[formula]

for all e,ē∈E1. The second derivative [formula] is given by

[formula]

for all e∈E1 and u∈F1, which is equivalent to the following: for any fixed e∈E1

[formula]

which is a linear map from F1 to E3, or by ([\ref=mixed:partials])

[formula]

The adjoint of ([\ref=backprop2:NabD]) or ([\ref=backprop2:DNab]) yields higher-order backpropagation of error, say for a loss function [formula] with some e∈E1 and y∈E3 that may depend on x, but not on the parameters. Higher-order backpropagation will be studied in more detail in the next section.

Backpropagation can be expressed recursively for the composition of more than two functions. Consider L functions ft(xt;θt)∈Et + 1 for xt∈Et,θt∈Ft, [formula]. Define the composition [formula]. Let [formula] and [formula] for [formula] so that

[formula]

for all [formula]. The first and second derivatives of ([\ref=eqn:recursive_rel]) and their adjoints can be easily obtained.

Convolutional Neural Networks

This section will describe how the above framework can be applied to convolutional neural networks; refer to [\cite=huang2006large] or [\cite=krizhevsky2012imagenet], for example, for more on the theory of CNNs. First, the actions of one layer of a generic CNN will be described, and then this will be extended to multiple layers. A coordinate-free gradient descent algorithm will also be described. Note that in this section, all bases of inner product space will be assumed to be orthonormal.

Single Layer Formulation

The actions of one layer of the network will be denoted f(X;W,B), where [formula] is the state variable, and [formula] and [formula] are the parameters. Throughout this section, let {ei}i (resp. {ẽa}a) be a basis for [formula] (resp. [formula]), and let {Ejk}jk (resp. {Ējk}jk,{Ẽjk}jk,{Êjk}jk) be a basis for [formula] (resp. [formula]). Then X, W and B can be written as follows:

[formula]

Each [formula] is called a feature map, which corresponds to an abstract representation of the input for a generic layer. Each [formula] is a filter used in convolution, and each [formula] is a bias term. The actions of the layer are then a new set of feature maps, [formula], with explicit form given by:

[formula]

where Ψ is a pooling operator, S is an elementwise nonlinear function, and C is the convolution operator, all of which will be defined in this section.

Cropping, Embedding & Mixing Operators

The cropping and mixing operators will be used to define the convolution operator C appearing in ([\ref=eqn:single_layer]). The cropping operator, [formula], is defined as:

[formula]

where [formula] is defined as:

[formula]

Define the embedding operator [formula] by

[formula]

for [formula], which corresponds to embedding Y into the zero matrix when {Ejk}jk is the standard basis. The adjoints of Kjk and κjk are calculated as follows:

For any [formula],

[formula]

where

[formula]

For [formula], the mixing operator [formula] defines how the cropped feature maps are combined into the next layer of feature maps, which is useful in a framework such as [\cite=lecun1998gradient]. It can be explicitly represented as:

[formula]

where [formula]. The adjoint operator Φ*v has a compact form, as the following lemma describes.

Let [formula] and [formula]. Then,

[formula]

Convolution Operator

The C operator in ([\ref=eqn:single_layer]) is known as the convolution operator. The convolution [formula] is defined as:

[formula]

where [formula] is a bilinear operator that defines the mechanics of the convolution. The specific form of Ca is defined using ([\ref=eqn:bigcrop]) and ([\ref=eqn:Phi]) as follows:

[formula]

with [formula]. The fixed vectors {Aa}m2a = 1, where [formula] for each a, define the action of ΦAa and thus the mixing of feature maps. The choice of Δ defines the stride of the convolution.

The adjoints of the operators [formula], [formula], and [formula] will be used in gradient calculations. The following theorems describe how to calculate them:

Let [formula] and [formula]. Then,

[formula]

Let [formula], [formula]. Then,

[formula]

Furthermore, for any [formula],

[formula]

Elementwise Nonlinearity

The S operator in ([\ref=eqn:single_layer]) is an elementwise nonlinear function, [formula], that operates as follows:

[formula]

where [formula] is some elementwise nonlinear function, which can be written as [formula]. The map [formula] defines the nonlinear action. Common choices for [formula] include the ramp function max (0,x) (also known as the rectifier), the sigmoidal function, or hyperbolic tangent, for example. The adjoint of [formula] is easy to calculate.

[formula] is self-adjoint for all [formula], i.e. [formula].

Pooling Operator

The Ψ operator in ([\ref=eqn:single_layer]) is known as the pooling operator, and its purpose is to reduce the size of the feature maps at each layer. Only linear pooling is considered in this paper (the framework does extend to the nonlinear case though), so that [formula] operates as:

[formula]

for [formula]. Here [formula] operates in the same way for each feature map Ya. The operator ψ acts on disjoint r  ×  r neighbourhoods that form a partition of the input Ya, with one output from each neighbourhood. This implies that n̄1  =  rn2 and [formula] (assuming that r|n̄1 and r|1).

One type of linear pooling is average pooling, which involves taking the average over all elements in the r  ×  r neighbourhoods. This can be represented using ([\ref=eqn:smallcrop]) as:

[formula]

where the operator [formula] is defined in ([\ref=eqn:smallcrop]) with p  =  q  =  r and

[formula]

If {Ējk}jk is the standard basis, [formula] is the all-ones matrix.

The adjoint Ψ* of the average pooling operator Ψ can be computed using the following theorem.

Let [formula]. Then, using ([\ref=eqn:embed]) with [formula],

[formula]

Single-Layer Derivatives

The derivatives of a generic layer f(X;W,B), as described in ([\ref=eqn:single_layer]), with respect to X, W, and B are presented in the following theorem.

[formula]

[formula]

[formula]

Note that the adjoints of the above operators can be calculated using the reversing property of the adjoint operator *  .

Multiple Layers

Suppose now that the network consists of L layers. Denote the actions of the tth layer as Xt + 1  =  ft(Xt), where [formula] and X1 is one point in the input data. The layer map [formula] can be given explicitly as:

[formula]

Here, [formula] and [formula]. Note that the pooling operator Ψt, the nonlinearity St, and the convolution operator Ct are layer-dependent. The entire network's actions can be denoted as:

[formula]

where [formula] is the parameter set and [formula] is the input data.

Final Layer

Classification is often the goal of a CNN, thus assume that there are N classes. This implies the following: mL + 1  =  N, [formula], and [formula]. The final layer is assumed to be fully connected, which aligns with the form given in ([\ref=eqn:f_t]) if the cropping operator ([\ref=eqn:bigcrop]) and pooling operator ([\ref=eqn:bigpool]) for the final layer -- KLjk and ΨL, respectively -- are identity maps. Also, [formula] defining the mixing operator ΦLALa in ([\ref=eqn:conv]) is [formula] for each a. Then, the final layer is given as:

[formula]

where {eL + 1a}a is a basis for [formula], and [formula]. Note that [formula]. It is also important to note that this shows that simpler, fully-connected neural networks are just a special case of convolutional neural networks.

Loss Function & Backpropagation

While training a CNN, the goal is to optimize some loss function J with respect to the parameters θ. For example, consider

[formula]

where y represents the given data and F(X;θ) is the prediction. Gradient descent is used to optimize the loss function, so it is important to calculate the gradient of J with respect to each of the parameters. For this, define maps ωt and αt as:

[formula]

for [formula], which satisfy ([\ref=eqn:recursive_rel]). Assume ωL + 1 and α0 are identity maps for the sake of convenience. Then, for any [formula],

[formula]

Since this holds for any Ut,

[formula]

by the same logic used to derive ([\ref=eqn:nabla_J]) from ([\ref=eqn:nabla_J_u]). Differentiating [formula] with respect to Wt produces

[formula]

where Xt  =  αt - 1(X) and Xt + 1  =  ft(Xt)  =  αt(X). Taking the adjoint of ([\ref=eqn:dFdW]) yields

[formula]

which can be substituted into ([\ref=eqn:dJdW]). Then, the final step in computing ([\ref=eqn:dJdW]) involves computing [formula] in ([\ref=eqn:dFdW:adjoint]), which can be done recursively:

[formula]

This comes from taking the derivative and then the adjoint of the relationship [formula]. Note that [formula] and [formula] in ([\ref=eqn:dFdW:adjoint]) and ([\ref=Domega:adjoint:recursion]) are calculated by taking the adjoint of the equations in statements 1 and 2 in Theorem [\ref=thm:Df].

The gradient descent update to optimize J over the weights Wt, for a single point X, is given by:

[formula]

where [formula] is the learning rate. This weight update extends to a batch of data points. Note that [formula], so the descent is performed in a coordinate-free manner directly over the entire Wt-parameter space. A gradient descent update rule can similarly be obtained for Bt.

Higher-Order Loss Functions

Suppose that another term is added to the loss function to penalize the first-order derivative of F(X;θ), as in [\cite=rifai2011manifold] or [\cite=simard1992tangent] for example. This can be represented using

[formula]

for some [formula] and [formula]. When βX  =  0, minimizing R(X,θ) promotes invariance of the network in the direction of VX. This can be useful in image classification, for example, where the class of image is expected to be invariant with respect to rotation. In this case, VX would be an infinitesimal generator of rotation. This new term R can be added to J to create a new loss function

[formula]

where [formula] determines the amount that the higher-order term contributes to the loss function. Note that R could be extended to contain multiple terms as:

[formula]

where BX is a finite set of pairs (VX,βX) for each X.

The gradient of R with respect to the parameters must now be taken. This can be calculated as:

[formula]

for all [formula]. Again, in the same way that ([\ref=eqn:nabla_J]) was derived from ([\ref=eqn:nabla_J_u]),

[formula]

Before ([\ref=eqn:dRdW]) can be computed, some preliminary results will be given.

Let f be as in ([\ref=eqn:single_layer]), and [formula]. Let Z  =  C(W,X)  +  B. Then,

[formula]

For any [formula] with S defined in ([\ref=eqn:bignl]), [formula] is self-adjoint, i.e. [formula].

The following theorem provides a recursive update formula for [formula] which backpropagates the error through the tangent network, as in [\cite=simard1992tangent].

Let ft be defined as in ([\ref=eqn:f_t]) and ωt and αt be defined as in ([\ref=eqn:omega_phi]). Then, for any [formula],

[formula]

where Xt  =  αt - 1(X) and Xt + 1  =  ft(Xt)  =  αt(X).

Since [formula], the derivative [formula] can be updated recursively:

[formula]

which is forward propagation of the tangent network [\cite=simard1992tangent]. Calculating ([\ref=eqn:dd_lefthook]) involves taking the adjoint of ([\ref=eqn:D2f]), which can be done using the reversing property of the adjoint, along with Theorems [\ref=thm:W_lh_C] and [\ref=thm:bigpool] and Lemma [\ref=lem:D2S]. The above results are crucial for the next theorem:

Suppose [formula]. Then,

[formula]

Then, ([\ref=eqn:dRdW]) can be computed with Theorem [\ref=thm:main], where [formula] is computed recursively by ([\ref=eqn:D_phi]).

Gradient descent to optimize J defined in ([\ref=eqn:curly_J]) can now be given for one point X as:

[formula]

This formula extends to a batch of updates, and for R defined with multiple (VX,βX) pairs as in ([\ref=eqn:R_mult_terms]). A similar formula can be obtained for updating Bt, which is left to the reader.

Conclusion & Future Work

This work has developed a geometric framework for convolutional neural networks. The input data and parameters are defined over a vector space equipped with an inner product. The parameters are learned using a gradient descent algorithm that acts directly over the inner product space, avoiding the use of individual coordinates. Derivatives for higher-order loss functions are also explicitly calculated in a coordinate-free manner, providing the basis for a gradient descent algorithm. This mathematical framework can be extended to other types of deep networks, including recurrent neural networks, autoencoders and deep Boltzmann machines. Another interesting future direction is to expand the capabilities of automatic differentiation (AD) into this coordinate-free realm, strengthening the hierarchical approach to AD [\cite=walter2013algorithmic].

This paper has shown how to express a particular deep neural network, end-to-end, in a precise format. However, this framework should not be limited to only expressing previous results, and it should not be written off as simply a derivative calculation method. The stronger mathematical understanding of neural networks provided by this work should promote expansion into new types of networks.