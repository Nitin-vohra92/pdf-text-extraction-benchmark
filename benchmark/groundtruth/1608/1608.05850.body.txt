Steganalyzer performances in operational contexts

Introduction

Information hiding is a recent computer science security field that focuses on the capability to hide hidden messages in digital media like pictures or movies [\cite=bgh13:ip] [\cite=bcfg+13:ip]. This discipline encompasses the design of algorithms, called steganographiers, that aim at discreetly inserting secret messages into innocent like cover media, the output being called stego-content. Conversely, steganalysers are tools that aim at detecting the possible presence of a secret message in a given document. Available steganalysers are mainly based on ensemble classifiers that have learned to separate between real natural images and stego contents.

Steganalyzers of the literature are usually evaluated as follows. A state-of-the-art steganographier s is firstly chosen, while the BOSS images [\cite=BOSS] are separated in two sets, half of each two parts being steganographied using s. Then the first set is used during the learning stage, while the steganalysis method is evaluated using the second set. Such an evaluation corresponds to the particular situation where the warden Eve (the steganalyzer) has the knowledge of which steganographier has been used, with which parameters (embedding payload, etc.) In this article, we will investigate a more realistic scenario where Eve only knows that images contain secret messages: she does not know which steganographic algorithm has been used, and the game consists of separating well original from stego contents. More precisely, in this research work, we show what happens when the learning stage has been realized with a wrong steganographier, and we ask whether it is useful to use more than one steganographier during the learning stage to face this problem.

The remainder of this article is constituted as follows. In the next section, we briefly recall the functioning of state-of-the-art steganographiers and steganalyzers studied in this article. Then, in Section [\ref=sec:Eve], we first investigate the effect of a wrong assumption on the steganographier during the learning stage. In Section [\ref=sec:mixingStego], we wonder whether it is possible to solve this problem by mixing more than one steganographier during the learning stage, in order to design a kind of universal detector. Errors on payload assumption are then discussed in Section [\ref=sec:payload]. All these situations are merged in Section [\ref=sec:operational], leading to what can be expected for operational contexts.

State of the art

Let us now recall some famous algorithms that will be investigated in this article. The first next paragraph focuses on steganographiers while the second one is about steganalyzers. Readers wanting more details about these schemes are referred to the provided citations.

In F5 algorithm [\cite=f5], the absolute value of some randomly selected DCT coefficients is decreased by one. However, to avoid errors during decoding, F5 algorithm skips over all the coefficients equal to +/-1 which is denoted as shrinkage. The nsF5 [\cite=nsF5] algorithm is introduced as a modified version of F5 by alleviating the shrinkage. HUGO [\cite=hugo1] method, for its part, focuses on so-called efficient SPAM features on spatial domain. Finally, in universal distortion function J-UNIWARD [\cite=J-uniward], the embedding is performed on specific regions of the cover objects, more precisely on texture and noisy ones, where the distortion function is computed according to wavelet domain. This method avoid the embedding in clean edges and smooth regions.

Ensemble classifiers have been proposed as steganalysis systems in [\cite=ensemble]. They are built by fusing decisions of an ensemble of simple base learners that are inexpensive to train, leading to a steganalyzer of low complexity. To achieve this goal, these schemes explored several different possibilities for the base learners and fusion rules for designing the final classifier. This latter has been improved in CC-PEV, whose functioning is detailed in [\cite=pevny2007merging].

Training and testing stages use not the same steganographier

Let us first measure the effects of modifying the steganographic method between the training and the testing stage. To investigate this question, 2,000 original JPEG images have been used in our experiments. They are taken from the BOSS contest [\cite=BOSS], their size is equal to 512  Ã—  512, and they have been converted to JPEG. For stego images, an embedding payload of 0.1 is used. The method used for extracting the features from the images is CC-PEV. The same ensemble classifier has been used both in the training and in the testing stage, namely the one of [\cite=ensemble].

In the first experiment, the ensemble classifier is trained using 50% of the natural images and 50% of the same images steganographied by nsF5, while it is tested using the same rate of natural and J-UNIWARD images. Conversely, in the second set of experiments, J-UNIWARD is used during the training stage and nsF5 during the testing one. Obtained results are presented in Figure [\ref=fig:f1]. It can be seen that the presence of J-UNIWARD hidden messages is more or less detected when the steganalyzer has been trained by using nsF5. Conversely, the detection of nsF5 is impossible when learning with J-UNIWARD. This asymmetric behavior may be explained by the fact that the use of nsF5 affects more the general aspect of the embedding image compared to J-UNIWARD. So, the ensemble classifier can learn more from the former than from the latter, and its classification is thus more efficient and trustworthy. This result has been obtained again when considering all other possible combinations, see Table [\ref=tab:mixingStego]: the only acceptable performances are obtained when nsF5 is used during the training stage.

Trying to improve steganalyzer score by mixing learning steganographiers

In this new scenario, we wonder whether the steganalysis performance can be improved by using more than one steganographier during the learning stage: if two or three steganographiers are suspected by Eve, can she use such a suspicion to produce a more accurate steganalyzer ? Or, to say this differently, is it possible to create a kind of universal steganalyzer by using a large set of steganographiers during the learning stage ?

Results of these experiments are given in Table [\ref=tab:my_label] and partially illustrated in receiver operating characteristic (ROC) curves of Figure [\ref=fig:mix22]. In this table, each row corresponds to an experiment where more than one steganographier has been used during the learning stage. Each tuple in this table gives the proportion of, respectively, natural images, HUGO, J-UNIWARD, and nsF5 stego-contents that has been used to constitute the set of 2,000 images, either during training or during testing stage. A payload of 0.1 has been used as previously. However, the area under the curve (AUC) obtained here never becomes larger than 0.7, while it was the case in Table [\ref=tab:mixingStego], setting at naught the hope to constitute universal steganalyzer by mixing several tools when training.

Uncertainty effects regarding payload

The objective is now to emphasize the possible effects of payload ignorance on steganalyzer performances. Indeed, a large payload of 0.1 is always chosen for evaluating steganalyzers of the literature. By doing so, steganalyzer designers made strong assumptions that make life less complicated, and the game totally unfair in their own advantage. These two assumptions are that the steganographier will absurdly use a very large payload, and additionally this payload is known by the steganalyzer. Everything happens as if steganalyzer designers claim to be able to detect if a communication channel possibly contains stego images, while they finally answer to the challenge: "knowing the set of images, the presence of hidden information, the steganographier, and the payload, can we separate with a good accuracy the natural from the stego images." On our side, we argue that it is not possible to expect exactly the payload value chosen by the steganographier in operational contexts.

In this new run of tests, images are steganographied by using respective payloads of 0.005, 0.05, and 0.1 (see Figure [\ref=fig:f4] to understand the effects of such payloads on host contents). CC-PEV features are used with ensemble classifier in both training and testing stages. nsF5, J-UNIWARD, and HUGO have been successively tested using the 3 payloads listed above, to illustrate the effects of such an error for both spatial and frequency embedding. Obtained results are summarized in Table [\ref=tab:table1]. As can be seen, the only situation where the separation is acceptable is the nsF5 one, and when training with a large payload that helps the ensemble classifier to learn the embedding effects.

Operational contexts

We now consider the most realistic scenario where the steganalyzer side only knows that one of the 3 most famous steganographier tools are used. But he is not sure about the chosen payload. Obtained results when mixing both the steganographier and its payload between training and testing stages have then been computed, and obtained results are summarized in Table [\ref=tab:table2].

As can be deduced from this table, the classification is acceptable only when the learning process has been realized with nsF5 and with a larger payload than the one that has been used during the tests. In this situation, it has been possible to separate, with a medium accuracy, images steganographied by either HUGO or J-UNIWARD. Remark that, obtained results are better than what has been found in Table [\ref=tab:table1].

Conclusion

This paper has focused on experiments in Kerckhoffs's context: everything about the used steganographic schemes, except the key, are known by steganalysis systems. Thanks to a large number of experiments, we indeed have shown that even J-UNIWARD can be detected but while learning with other steganographic tools, namely HUGO and NSF5. This is observed even if the objective is to analyse a small payload based steganographic tool. In such a situation, it is sufficient to set a large payload in the learning step. To learn the behavior of a dedicated steganographic scheme, we will consider in a future work to study why less efficient steganographic tools are more convenient than this dedicated steganographic scheme in the learning process.