Algorithm Lemma Definition Corollary Proposition Proposition Assumption Algorithm Example

Optimality Conditions for Nonlinear Semidefinite Programming via Squared Slack Variables

This work was supported by Grant-in-Aid for Young Scientists (B) (26730012) and for Scientific Research (C) (26330029) from Japan Society for the Promotion of Science.

Ellen H. Fukuda

Masao Fukushima

Introduction

We consider the following nonlinear semidefinite programming (NSDP) problem:

[formula]

where [formula] and [formula] are twice continuously differentiable functions, Sm is the linear space of all real symmetric matrices of dimension m  ×  m, and Sm+ is the cone of all positive semidefinite matrices in Sm. Second-order optimality conditions for such problems were originally derived by Shapiro in [\cite=shapiro97]. It might be fair to say that the second-order analysis of NSDP problems is more intricated than its counterpart for classical nonlinear programming problems. That is one of the reasons why it is interesting to have alternative ways for obtaining optimality conditions for [\eqref=eq:sdp]; see the works by Forsgren [\cite=FA00] and Jarre [\cite=Jarre12]. In this work, we propose to use the squared slack variables approach for deriving these optimality conditions.

It is well-known that the squared slack variables can be used to transform a nonlinear programming (NLP) problem with inequality constraints into a problem with only equality constraints. For NLP problems, this technique was hardly considered in the literature because it increases the dimension of the problem and may lead to numerical instabilities [\cite=Rob76]. However, recently, Fukuda and Fukushima [\cite=FF14] showed that the situation may change in the nonlinear second-order cone programming context. Here, we observe that the slack variables approach can be used also for NSDP problems, because, like the nonnegative orthant and the second-order cone, the cone of positive semidefinite matrices is also a cone of squares. More precisely, Sm+ can be represented as

[formula]

where [formula] is the Jordan product associated with the space Sm, which is defined as for any W,Z∈Sm. Note that actually [formula] for any Z∈Sm.

The fact above allows us to develop the squared slack variables approach. In fact, by introducing a slack variable Y∈Sm in [\eqref=eq:sdp], we obtain the following problem:

[formula]

which is an NLP problem with only equality constraints. Note that if [formula] is a global (local) minimizer of [\eqref=eq:sdps], then x is a global (local) minimizer of [\eqref=eq:sdp]. Moreover, if [formula] is a global (local) minimizer of [\eqref=eq:sdp], then there exists Y∈Sm such that (x,Y) is a global (local) minimizer of [\eqref=eq:sdps]. However, the relation between stationary points, or Karush-Kuhn-Tucker (KKT) points, of [\eqref=eq:sdp] and [\eqref=eq:sdps] is not so trivial. As in [\cite=FF14], we will take a closer look at this issue, and investigate also the relation between constraint qualifications for [\eqref=eq:sdp] and [\eqref=eq:sdps], using second-order conditions.

We remark that second-order conditions for these two problems are vastly different. While [\eqref=eq:sdps] is a run-of-the-mill nonlinear programming problem, [\eqref=eq:sdp] has nonlinear conic constraints, which are more difficult to deal with. Moreover, it is known that second-order conditions for NSDPs include an extra term, which takes into account the curvature of the cone. For more details, we refer to the papers of Kawasaki [\cite=Kawasaki88], Cominetti [\cite=Cominneti1990] and Shapiro [\cite=shapiro97]. The main objective of this work is to show that, under appropriate regularity conditions, second-order conditions for [\eqref=eq:sdp] and [\eqref=eq:sdps] are essentially the same. This suggests that the addition of the slack term already encapsulates most of the nonlinear structure of the cone. In the analysis, we also propose and use a sharp characterization of positive semidefiniteness that takes into account the rank information. We believe that such a characterization can be useful in other contexts as well.

Finally, we present results of some numerical experiments where NSDPs are reformulated as NLPs using slack variables. Note that we are not necessarily advocating the use of slack variables and we are, in fact, driven by curiosity about its computational prospects. Nevertheless, there are a couple of reasons why this could be interesting. First of all, conventional wisdom would say that using squared slack variables is not a good idea, but, in reality, even for linear SDPs there are good reasons to use such variables. In [\cite=BM03] [\cite=BM05], Burer and Monteiro transform a linear SDP [formula] into [formula], where V is a square matrix and [formula] denotes the trace map. The idea is to use a theorem, proven independently by Barvinok [\cite=Barvinok95] and Pataki [\cite=Pataki98], which bounds the rank of possible optimal solutions. By doing so, it is possible to restrict V to be a rectangular matrix instead of a square one, thereby reducing the number of variables. Another reason to use squared slack variables is that the reformulated NLP problem can be solved by efficient NLP solvers that are widely available. In fact, while there are a number of solvers for linear SDPs, as we move to the general nonlinear case, the situation changes drastically [\cite=HYY15].

Throughout the paper, the following notations will be used. For [formula] and [formula], [formula] and [formula] denote the ith entry of x and the (i,j) entry (ith row and jth column) of Y, respectively. The identity matrix of dimension [formula] is denoted by [formula]. The transpose, the Moore-Penrose pseudo-inverse, and the rank of [formula] are denoted by [formula], and [formula], respectively. If Y is a square matrix, its trace is denoted by [formula]. For square matrices W and Y of the same dimension, their inner product is denoted by [formula]. We will use the notation [formula] for Z∈Sm + . In that case, we will denote by [formula] the positive semidefinite square root of Z, that is, [formula] satisfies [formula] and [formula]. For any function [formula], the gradient and the Hessian at [formula] with respect to x are denoted by [formula] and [formula], respectively. Moreover, for any linear operator [formula] defined by [formula] with [formula], [formula], and [formula], the adjoint operator [formula] is defined by Given a mapping [formula], its derivative at a point [formula] is denoted by [formula] and defined by where [formula] are the partial derivative matrices. Finally, for a closed convex cone K, we will denote by [formula] the largest subspace contained in K. Note that [formula].

The paper is organized as follows. In Section [\ref=sec:prel], we recall a few basic definitions concerning KKT points and second-order conditions for [\eqref=eq:sdp] and [\eqref=eq:sdps]. We also give a sharp characterization of positive semidefiniteness. In Section [\ref=sec:kkt], we prove that the original and the reformulated problems are equivalent in terms of KKT points, under some conditions. In Section [\ref=sec:cq], we establish the relation between constraint qualifications of those two problems. The analyses of second-order sufficient conditions and second-order necessary conditions are presented in Sections [\ref=sec:sosc] and [\ref=sec:sonc], respectively. In Section [\ref=sec:comp], we show some computational results. We conclude in Section [\ref=sec:conclusion], with final remarks and future works.

Preliminaries

A sharp characterization of positive semidefiniteness

It is a well-known fact that a matrix Λ∈Sm is positive semidefinite if and only if 〈Λ,W2〉  ≥  0 for all W∈Sm. This statement is equivalent to the self-duality of the cone Sm+. However, we get no information about the rank of Λ. In the next lemma, we give a new characterization of positive semidefinite matrices, which takes into account the rank information.

Let Λ∈Sm. The following statements are equivalent:

Λ∈Sm + ;

There exists Y∈Sm such that [formula] and Y∈Φ(Λ), where

[formula]

For any Y satisfying the conditions in (ii), we have [formula]. Moreover, if σ and σ' are nonzero eigenvalues of Y, then σ  +  σ'  ≠  0.

Let us prove first that (ii) implies (i). Since the inner product is invariant under orthogonal transformations, we may assume without loss of generality that Y is diagonal, i.e., where D is a k  ×  k nonsingular diagonal matrix, and [formula]. We partition Λ in blocks in a similar way: where [formula], and C∈Sm - k. We will proceed by proving that A  =  0, B  =  0 and C is positive definite.

First, observe that, by assumption,

[formula]

holds. Since D is nonsingular, this implies B  =  0. Now, let us prove that A  =  0. From [\eqref=eq:lemma_psd.1] and the fact that D is diagonal, we obtain

[formula]

Again, because D is nonsingular, it must be the case that all diagonal elements of A should be zero. Now, suppose that Aij is nonzero for some i and j, with i  ≠  j. In face of [\eqref=eq:lemma], this can only happen if Dii  +  Djj  =  0. Let us now consider the following matrix: where [formula] is a submatrix containing only two nonzero elements, [formula] and [formula]. Then, easy calculations show that [formula], which also implies [formula]. Moreover, [formula] because [formula] is the diagonal matrix having 1 in the (i,i) entry and 1 in the (j,j) entry, and since Aii and Ajj are zero. We conclude that Y∉Φ(Λ), contradicting the assumptions. So, it follows that A must be zero. Similarly, we have that Dii  +  Djj is never zero, which corresponds to the statement about eigenvalues σ and σ' in the lemma. In fact, if Dii  +  Djj is zero, then, by taking W exactly as before, we have [formula] and [formula]. Once again, this shows that Y∉Φ(Λ), which is a contradiction.

It remains to show that C is positive definite. Taking an arbitrary nonzero H̃∈Sm - k, and defining we easily obtain [formula]. Since Y∈Φ(Λ), we have [formula]. But this shows that 〈H̃2,C〉  >  0, which implies that C is positive definite. In particular, the rank of Λ is equal to the rank of C, which is [formula].

Now, let us prove that (i) implies (ii). Similarly, we may assume [formula], with C positive definite. Then, we can take [formula], where E is any positive definite matrix. It follows that any matrix W∈Sm satisfying [formula] must have the shape [formula], for some matrix F. Since C is positive definite, it is clear that [formula], whenever W is nonzero.

The statement about the sum of nonzero eigenvalues might seem innocuous at first, but it will be very useful in Section [\ref=sec:sosc]. In fact, the idea for this new characterization of positive semidefiniteness comes from the second-order conditions of [\eqref=eq:sdps]. For now, let us present another result that will be necessary. Given A∈Sm, denote by LA:Sm  →  Sm the linear operator defined by for all E∈Sm. There are many examples of invertible matrices A for which the operator LA is not invertible. This is essentially due to the failure of the condition on the eigenvalues. The following proposition is well-known in the context of Euclidean Jordan algebra (see [\cite=Sturm2000]), but we include here a short-proof for completeness.

Let A∈Sm. Then, LA is invertible if and only if σ  +  σ'  ≠  0 for every pair of eigenvalues σ,σ' of A; in this case, A must be invertible.

The statements in the proposition are all invariant under orthogonal transformations. Thus, we may assume without loss of generality that A is already diagonalized, and so Akk is an eigenvalue of A for every [formula].

Let us show that the invertibility of LA implies the statement about the eigenvalues of A. We will do so by proving the contrapositive. Take i and j such that Aii  +  Ajj  =  0. Let W be such that all the entries are zero except for Wij  =  Wji  =  1. Then, we have [formula]. This shows that the kernel of LA is non-trivial and consequently, LA is not invertible.

Reciprocally, since we assume that A is diagonal, for every W∈Sm, we have 2(LA(W))ij  =  Wij(Aii  +  Ajj) for all i and j. Due to the fact that Aii  +  Ajj is never zero, the kernel of LA must only contain the zero matrix. Hence LA is invertible, and the result follows.

In view of Proposition [\ref=prop:nonsigular], the matrix D which appears in the proof of Lemma [\ref=lemma:psd] is such that LD is invertible. This will play an important role when we discuss the relation between the second-order sufficient conditions of problems [\eqref=eq:sdp] and [\eqref=eq:sdps].

KKT conditions and constraint qualifications

Now, let us consider the following lemma, which will allow us to present appropriately the KKT conditions of problems [\eqref=eq:sdp] and [\eqref=eq:sdps].

The following statements hold.

For any matrices [formula], let [formula] be defined by [formula]. Then, we have [formula].

For any matrix A∈Sm, let [formula] be defined by [formula]. Then, we have [formula].

For any matrix [formula] and function [formula], let [formula] be defined by ψ(x)  =  〈θ(x),A〉. Then, we have [formula].

Let A,B∈Sm. Then, they commute, i.e., AB  =  BA, if and only if A and B are simultaneously diagonalizable by an orthogonal matrix, i.e., there exists an orthogonal matrix Q such that [formula] and [formula] are diagonal.

Let A,B∈Sm+. Then, AB  =  0 if and only if [formula].

(a) See [\cite=Ber09].

(b) Note that [formula]. Let [formula] and [formula]. Then, from item (a), we have [formula] and [formula]. Taking into account the symmetry of A, we have [formula] and [formula]. Hence we have [formula].

(c) Observe that [formula] for any [formula]. Then, we have where the last equality follows from the definition of adjoint operator.

(d) See [\cite=Ber09].

(e) See [\cite=Ber09].

We can now recall the KKT conditions of problems [\eqref=eq:sdp] and [\eqref=eq:sdps]. First, define the Lagrangian function [formula] associated with problem [\eqref=eq:sdp] as We say that [formula] is a KKT pair of problem [\eqref=eq:sdp] if the following conditions are satisfied:

[formula]

where, from Lemma [\ref=lem:matrices](c), we have [formula]. Applying the trace map on both sides of [\eqref=eq:kkt_sdp.4], we see that condition [\eqref=eq:kkt_sdp.4] is equivalent to 〈Λ,G(x)〉  =  0. This result, together with the fact that [formula] and [formula], shows that [\eqref=eq:kkt_sdp.4] is also equivalent to ΛG(x)  =  0, by Lemma [\ref=lem:matrices](e). Moreover, the equality [\eqref=eq:kkt_sdp.4] implies that Λ and G(x) commute, which means, by Lemma [\ref=lem:matrices](d), that they are simultaneously diagonalizable by an orthogonal matrix. The following definition is also well-known.

If [formula] is a KKT pair of [\eqref=eq:sdp] such that then (x,Λ) is said to satisfy the strict complementarity condition.

As for the equality constrained NLP problem [\eqref=eq:sdps], we observe that [formula] is a KKT triple if the conditions below are satisfied:

[formula]

where [formula] is the Lagrangian function associated with [\eqref=eq:sdps], which is given by From Lemma [\ref=lem:matrices](b),(c), these conditions can be written as follows:

[formula]

For problem [\eqref=eq:sdp], we say that the Mangasarian-Fromovitz constraint qualification (MFCQ) holds at a point x if there exists some [formula] such that where [formula] denotes the interior of Sm + , that is, the set of symmetric positive definite matrices. If x is a local minimum for [\eqref=eq:sdp], MFCQ ensures the existence of a Lagrange multiplier Λ and that the set of multipliers is bounded. A more restrictive assumption is the nondegeneracy condition discussed in [\cite=shapiro97], where it is presented in terms of a transversality condition on the map G. However, at the end, it boils down to the following condition.

Suppose that [formula] is a KKT pair of [\eqref=eq:sdp] such that where [formula] denotes the image of the linear map [formula], TSm  + (G(x)) denotes the tangent cone of Sm+ at G(x), and [formula] is the lineality space of the tangent cone TSm  + (G(x)), i.e., [formula] (see, for instance, the observations on page 310 in [\cite=shapiro97]). Then, (x,Λ) is said to satisfy the nondegeneracy condition.

A good thing about the nondegeneracy condition is that it ensures that Λ is unique.

For problem [\eqref=eq:sdps], a common constraint qualification is the linear independence constraint qualification (LICQ), which simply requires that the gradients of the constraints be linearly independent. In Section [\ref=sec:cq], we will show that LICQ and the nondegeneracy are essentially equivalent.

Second-order conditions

Since [\eqref=eq:sdps] is just an ordinary equality constrained nonlinear program, second-order sufficient conditions are well-known and can be written as follows.

Let [formula] be a KKT triple of problem [\eqref=eq:sdps]. The second-order sufficient condition (SOSC-NLP) holds if

[formula]

for every nonzero [formula] such that [formula].

The second-order sufficient condition for [\eqref=eq:sdps] holds if for every nonzero [formula] such that [formula]; see [\cite=Ber99] or [\cite=NW99]. Since we have the desired result.

Similarly, we have the following second-order necessary condition. Note that we require the LICQ to hold.

Let (x,Y) be a local minimum for [\eqref=eq:sdps] and [formula] be a KKT triple such that LICQ holds. Then, the following second-order necessary condition (SONC-NLP) holds:

[formula]

for every [formula] such that [formula].

See [\cite=NW99].

Second-order conditions for [\eqref=eq:sdp] are a more delicate matter. Let (x,Λ) be a KKT pair of [\eqref=eq:sdp]. It is true that a sufficient condition for optimality is that the Hessian of the Lagrangian be positive definite over the set of critical directions. However, replacing "positive definite" by "positive semidefinite" does not yield a necessary condition. Therefore, it seems that there is a gap between necessary and sufficient conditions. In order to close the gap, it is essential to add an additional term to the Hessian of the Lagrangian. For the theory behind this see, for instance, the papers by Kawasaki [\cite=Kawasaki88], Cominetti [\cite=Cominneti1990], and Bonnans, Cominetti and Shapiro [\cite=BonnansCS]. The condition below was obtained by Shapiro in [\cite=shapiro97] and it is sufficient for (x,Λ) to be a local minimum, see Theorem 9 therein.

Let [formula] be a KKT pair of problem [\eqref=eq:sdp] satisfying strict complementarity and the nondegeneracy condition. The second-order sufficient condition (SOSC-SDP) holds if

[formula]

for all nonzero d∈C(x), where is the critical cone at x, and H(x,Λ)∈Sm is a matrix with elements

[formula]

for [formula]. In this case, (x,Λ) is a local minimum for [\eqref=eq:sdp]. Conversely, if x is a local minimum for [\eqref=eq:sdp] and (x,Λ) is a KKT pair satisfying strict complementarity and nondegeneracy, then the following second-order necessary condition (SONC-SDP) holds:

[formula]

for all d∈C(x).

Equivalence between KKT points

Let us now establish the relation between KKT points of the original problem [\eqref=eq:sdp] and its reformulation [\eqref=eq:sdps]. We start with the following simple implication.

Let [formula] be a KKT pair of problem [\eqref=eq:sdp]. Then, there exists Y∈Sm such that (x,Y,Λ) is a KKT triple of [\eqref=eq:sdps].

Let Y∈Sm +  be the positive semidefinite matrix satisfying [formula]. Let us show that (x,Y,Λ) is a KKT triple of [\eqref=eq:sdps]. The conditions [\eqref=eq:kkt_sdps.1] and [\eqref=eq:kkt_sdps.3] are immediate. We need to show that [\eqref=eq:kkt_sdps.2] holds.

Recall that [\eqref=eq:kkt_sdp.4] along with [\eqref=eq:kkt_sdp.2] and [\eqref=eq:kkt_sdp.3] implies G(x)Λ  =  0, due to Lemma [\ref=lem:matrices](e). It means that every column of Λ lies in the kernel of G(x). However, G(x) and Y share exactly the same kernel, since G(x)  =  Y2. It follows that YΛ  =  0, so that [formula].

The converse is not always true. That is, even if (x,Y,Λ) is a KKT triple of [\eqref=eq:sdps], (x,Λ) may fail to be a KKT pair of [\eqref=eq:sdp], since Λ need not be positive semidefinite. This, however, is the only obstacle for establishing equivalence.

If [formula] is a KKT triple of [\eqref=eq:sdps] such that Λ is positive semidefinite, then (x,Λ) is a KKT pair of [\eqref=eq:sdp].

The only condition that remains to be verified is [\eqref=eq:kkt_sdp.4]. Due to [\eqref=eq:kkt_sdps.2], we have

[formula]

Since G(x) and Λ are both positive semidefinite, we must have [formula].

The previous proposition leads us to consider conditions which ensure that Λ is positive semidefinite. It turns out that if the second-order sufficient condition for [\eqref=eq:sdps] is satisfied at (x,Y,Λ), then Λ is positive semidefinite. In fact, a weaker condition is enough to ensure positive semidefiniteness.

Suppose that [formula] is a KKT triple of [\eqref=eq:sdps] such that Y∈Φ(Λ), where Φ(Λ) is defined by [\eqref=eq:lemma:psd], that is,

[formula]

for every nonzero W∈Sm such that [formula]. Then (x,Λ) is a KKT pair of [\eqref=eq:sdp] satisfying strict complementarity.

Due to Lemma [\ref=lemma:psd], Λ is positive semidefinite and [formula]. Now, since G(x)  =  Y2, we have [formula]. Therefore (x,Λ) must satisfy the strict complementarity condition.

Suppose that SOSC-NLP is satisfied at a KKT triple [formula]. Then (x,Λ) is a KKT pair for [\eqref=eq:sdp] which satisfies the strict complementarity condition.

If we take v  =  0 in the definition of SOSC-NLP, we obtain Y∈Φ(Λ). So, the result follows from Proposition [\ref=prop:kkt_slack].

The next result is a refinement of Proposition [\ref=prop:sdp_to_sdps].

Suppose that [formula] is a KKT pair of [\eqref=eq:sdp] which satisfies the strict complementarity condition. Then there exists some Y∈Φ(Λ) such that (x,Y,Λ) is a KKT triple of [\eqref=eq:sdps].

Without loss of generality, we may assume that G(x) has the shape [formula], where A∈Sk +  and [formula]. Since G(x) and Λ are both positive semidefinite, the condition [formula] is equivalent to G(x)Λ  =  0. It follows that Λ has the shape [formula] for some matrix C∈Sm  -  k + . However, strict complementarity holds only if C is positive definite. Therefore, it is enough to pick Y to be the positive semidefinite matrix satisfying Y2  =  G(x).

Finally, note that if [formula], with W∈Sm,W1∈Sk, [formula], W3∈Sm - k, then the condition [formula] together with Proposition [\ref=prop:nonsigular] implies W1  =  0, W2  =  0. Since C is positive definite, we must have [formula], if W  ≠  0. From [\eqref=eq:lemma:psd], this shows that Y∈Φ(Λ).

Relations between constraint qualifications

In this section, we shall show that the nondegeneracy in Definition [\ref=def:nondeg] is essentially equivalent to LICQ for [\eqref=eq:sdps]. In [\cite=shapiro97], Shapiro mentions that the nondegeneracy condition for [\eqref=eq:sdp] is an analogue of LICQ, but he also states that the analogy is imperfect. For instance, when G(x) is diagonal, [\eqref=eq:sdp] naturally becomes an NLP, since the semidefiniteness constraint is reduced to the nonnegativity of the diagonal elements. However, even in that case, LICQ and the nondegeneracy in Definition [\ref=def:nondeg] might not be equivalent (see page 309 of [\cite=shapiro97]). In this sense, it is interesting to see whether a correspondence between the conditions can be established when [\eqref=eq:sdp] is reformulated as [\eqref=eq:sdps]. Before that, we recall some facts about the geometry of the cone of positive semidefinite matrices.

Let A∈Sm +  and let U be an m  ×  k matrix whose columns form a basis for the kernel of A. Then, the tangent cone of Sm +  at A is written as (see [\cite=pataki_handbook] or [\cite=shapiro97]). For example, if A can be written as [formula], where D is positive definite, then the matrices in TSm  + (A) have the shape [formula], where the only restriction is that H should be positive semidefinite.

Our first step is to notice that nondegeneracy implies that the only matrix which is orthogonal to both [formula] and [formula] is the trivial one, i.e.,

[formula]

where [formula] denotes the orthogonal complement.

On the other hand, the LICQ constraint qualification for [\eqref=eq:sdps] holds at a feasible point (x,Y) if the linear function which maps (v,W) to [formula] is surjective. This happens if and only if the adjoint map has trivial kernel. The adjoint map takes W∈Sm and maps it to [formula]. So the surjectivity assumption amounts to requiring that every W which satisfies both [formula] and [formula] must actually be 0, that is,

[formula]

The subspaces [formula] and [formula] are closely related. The next proposition clarifies this connection.

Let V  =  Y2, then [formula]. If Y is positive semidefinite, then [formula] as well.

Note that if Q is an orthogonal matrix, then [formula]. The same is true for ker LY, i.e., [formula]. So, without loss of generality, we may assume that Y is diagonal and that where D is an r  ×  r nonsingular diagonal matrix. Then, we have

[formula]

This shows that every matrix [formula] satisfies YZ  =  0 and therefore lies in ker LY. Now, the kernel of LY can be described as follows: If Y is positive semidefinite, then D is positive definite and the operator LD is nonsingular. Hence [formula] implies A  =  0. In this case, ker LY coincides with [formula].

If [formula] satisfies LICQ for the problem [\eqref=eq:sdps], then nondegeneracy is satisfied at x for [\eqref=eq:sdp]. On the other hand, if x satisfies nondegeneracy and if [formula], then (x,Y) satisfies LICQ for [\eqref=eq:sdps].

It follows easily from Proposition [\ref=prop:nondeg_licq].

Analysis of second-order sufficient conditions

In this section, we examine the relations between KKT points of [\eqref=eq:sdp] and [\eqref=eq:sdps] that satisfy second-order sufficient conditions.

Suppose that [formula] is a KKT triple of [\eqref=eq:sdps] satisfying SOSC-NLP. Then (x,Λ) is a KKT pair of [\eqref=eq:sdp] that satisfies strict complementarity and [\eqref=eq:sosc_sdp.1]. If additionally, (x,Y,Λ) satisfies LICQ for [\eqref=eq:sdps] or (x,Λ) satisfies nondegeneracy for [\eqref=eq:sdp], then (x,Λ) satisfies SOSC-SDP as well.

In Corollary [\ref=col:strict], we have already shown that (x,Λ) is a KKT pair of [\eqref=eq:sdp] and strict complementarity is satisfied. In addition, if (x,Y,Λ) satisfies LICQ for [\eqref=eq:sdps], then Corollary [\ref=col:licq_nondeg] ensures that (x,Λ) satisfies nondegeneracy for [\eqref=eq:sdp]. It only remains to show that [\eqref=eq:sosc_sdp.1] is also satisfied. To this end, consider an arbitrary nonzero [formula] such that [formula] and [formula]. We are thus required to show that

[formula]

where H(x,Λ) is defined in [\eqref=eq:sosc_sdp.2]. A first observation is that, due to [\eqref=eq:kkt_sdp.1], we have [formula], that is, [formula]. We recall that H(x,Λ) satisfies

[formula]

The strategy here is to first identify the shape and properties of several matrices involved, before showing that [\eqref=eq_objective] holds. Without loss of generality, we may assume that G(x) is diagonal, i.e., where D is a k  ×  k diagonal positive definite matrix. We also have with E an invertible diagonal matrix such that E2  =  D. Since SOSC-NLP holds, considering v  =  0 in [\eqref=eq:sosc_nlp.1], we obtain [formula] for all nonzero W∈Sm such that [formula], which, by [\eqref=eq:lemma:psd], shows that Y∈Φ(Λ). From [\eqref=eq:kkt_sdps.2], we also have [formula]. Thus, Lemma [\ref=lemma:psd] ensures that every pair σ,σ' of nonzero eigenvalues of Y satisfies σ  +  σ'  ≠  0. Since the eigenvalues of E are precisely the nonzero eigenvalues of Y, it follows that LE is an invertible operator, by virtue of Proposition [\ref=prop:nonsigular]. Moreover, due to the strict complementarity condition, we obtain where Γ∈Sm  -  k +  is positive definite. The pseudo-inverse of G(x) is given by We partition [formula] in blocks in the following fashion: where A∈Sk, [formula] and C∈Sm - k. Inasmuch as [formula] lies in the tangent cone TSm  + (G(x)), C must be positive semidefinite. However, as observed earlier, we have [formula], which yields [formula]. Since Γ is positive definite, this implies C  =  0, and hence We are now ready to show that [\eqref=eq_objective] holds. We shall do that by considering v  =  d in [\eqref=eq:sosc_nlp.1] and exhibiting some W such that [formula] and [formula]. Then SOSC-NLP will ensure that [\eqref=eq_objective] holds. Note that, for any Z∈Sm - k, is a solution to the equation [formula]. Moreover, any solution to that equation must have this particular shape. Therefore, the proof will be complete if we can choose Z such that [formula] holds. Observe that, by [\eqref=eq:hessian],

[formula]

Thus, taking Z  =  0 yields [formula]. This completes the proof.

Suppose that [formula] is a KKT pair for [\eqref=eq:sdp] satisfying [\eqref=eq:sosc_sdp.1] and the strict complementarity condition. Then, there exists Y∈Sm such that (x,Y,Λ) is a KKT triple for [\eqref=eq:sdps] satisfying SOSC-NLP.

Again, we assume without loss of generality that G(x) is diagonal, so that where D is a k  ×  k diagonal positive definite matrix. Take Y such that where E2 = D and E is positive definite; in particular LE is invertible. Then (x,Y,Λ) is a KKT triple for [\eqref=eq:sdps]. Due to strict complementarity, we have where Γ∈Sm  -  k +  is positive definite. We are required to show that

[formula]

for every nonzero (v,W) such that [formula]. So, let (v,W) satisfy [formula]. Let us first consider what happens when v  =  0. Partitioning W in blocks, we have Recall that LE as well as E is invertible. So, [formula] implies W1  =  0 and W2  =  0. If W  ≠  0, then W3 must be nonzero, which in turn implies that [formula] must also be nonzero. We then have [formula]. But [formula] must be greater than zero, since Γ is positive definite. Thus, in this case, [\eqref=eq_objective2] is satisfied.

Now, we suppose that v is nonzero. First, we will show that [formula] lies in the tangent cone TSm  + (G(x)) and that [formula] is orthogonal to Λ, which implies [formula]. This shows that v lies in the critical cone C(x).

Note that the image of the operator LY only contains matrices having the lower right (m - k)  ×  (m - k) block equal to zero. Therefore, [formula] implies that [formula] has the shape Hence, [formula] and [formula] is orthogonal to Λ. Due to SOSC-SDP, we must have Thus, if [formula] holds, then we have [\eqref=eq_objective2]. In fact, since W satisfies [formula], the chain of equalities finishing at [\eqref=eq_non_negative] readily yields [formula].

Here, we remark one interesting consequence of the previous analysis. The second-order sufficient condition for NSDPs in [\cite=shapiro97] is stated under the assumption that the pair (x,Λ) satisfies both strict complementarity and nondegeneracy. However, since [\eqref=eq:sdp] and [\eqref=eq:sdps] share the same local minima, Proposition [\ref=prop:sosc_sdp_to_sdps] implies that we may remove the nondegeneracy assumption from SOSC-SDP. We now state a sufficient condition for [\eqref=eq:sdp] based on the analysis above.

Let [formula] be a KKT pair for [\eqref=eq:sdp] satisfying strict complementarity. Assume also that the following condition holds:

[formula]

for every nonzero [formula] such that [formula]. Then, x is a local minimum for [\eqref=eq:sdp].

Apart from the detail of requiring nondegeneracy, the condition above is equivalent to SOSC-SDP, due to Propositions [\ref=prop:sosc_sdps_to_sdp] and [\ref=prop:sosc_sdp_to_sdps].

Analysis of second-order necessary conditions

We now take a look at the difference between second-order necessary conditions that can be derived from [\eqref=eq:sdp] and [\eqref=eq:sdps]. Since the inequalities [\eqref=eq:sonc_nlp] and [\eqref=eq:sonc_sdp] are not strict, we need a slightly stronger assumption to prove the next proposition.

Suppose that [formula] is a KKT triple of [\eqref=eq:sdps] satisfying LICQ and SONC-NLP. Moreover, assume that Y and Λ are positive semidefinite. If (x,Λ) is a KKT pair for [\eqref=eq:sdp] satisfying strict complementarity, then it also satisfies SONC-SDP.

Since (x,Y,Λ) satisfies LICQ for [\eqref=eq:sdps] and Y is positive semidefinite, Corollary [\ref=col:licq_nondeg] implies that (x,Λ) satisfies nondegeneracy. Under the assumption that (x,Λ) is strictly complementary, the only thing missing is to show that [\eqref=eq:sonc_sdp] holds. To do so, we proceed as in Proposition [\ref=prop:sosc_sdps_to_sdp]. We partition G(x),Y,Λ, [formula] and [formula] in blocks in exactly the same way. The only difference is that, since [\eqref=eq:sonc_nlp] does not hold strictly, we cannot make use of Lemma [\ref=lemma:psd] in order to conclude that LE is invertible. Nevertheless, since we assume that Y is positive semidefinite, all the eigenvalues of E are strictly positive anyway. So, as before, we can conclude that LE is an invertible operator, by Proposition [\ref=prop:nonsigular]. Due to strict complementarity, we can also conclude that Γ∈Sm  -  k +  is positive definite and that C  =  0.

All our ingredients are now in place and we can proceed exactly as in the proof of Proposition [\ref=prop:sosc_sdps_to_sdp]. Namely, we have to prove that, given d∈C(x), the inequality [formula] holds. As before, the way to go is to craft a matrix W satisfying both [formula] and [formula]. Then SONC-NLP will ensure that [\eqref=eq:sonc_sdp] holds. Actually, it can be done by taking and following the same line of arguments that leads to [\eqref=eq_non_negative].

Suppose that [formula] is a KKT pair for [\eqref=eq:sdp] satisfying SONC-SDP. Then, there exists Y∈Sm such that (x,Y,Λ) is a KKT triple for [\eqref=eq:sdps] satisfying SONC-NLP.

It is enough to choose Y to be [formula]. If we do so, Corollary [\ref=col:licq_nondeg] ensures that (x,Y,Λ) satisfies LICQ. We now have to check that [\eqref=eq:sonc_nlp] holds. For this, we can follow the proof of Proposition [\ref=prop:sosc_sdp_to_sdps] by considering [\eqref=eq:sonc_sdp] instead of [\eqref=eq:sosc_sdp.1]. No special considerations are needed for this case.

Assume that (x,Λ) is a KKT pair of [\eqref=eq:sdp] satisfying nondegeneracy and strict complementarity. Then, Proposition [\ref=prop:sonc_sdps_to_sdp] gives an elementary route to prove that SONC-SDP holds. This is because if we select Y to be the positive semidefinite square root of G(x), all the conditions of Proposition [\ref=prop:sonc_sdps_to_sdp] are satisfied, which means that [\eqref=eq:sonc_sdp] must hold. Moreover, if we were to derive second-order necessary conditions for [\eqref=eq:sdp] from scratch, we could consider the following.

Let [formula] be a local minimum of [\eqref=eq:sdp]. Assume that [formula] is a KKT pair for [\eqref=eq:sdp] satisfying strict complementarity and nondegeneracy. Then the following condition holds:

[formula]

for every [formula] such that [formula].

Propositions [\ref=prop:sonc_sdps_to_sdp] and [\ref=prop:sonc_sdp_to_sdps] ensure that the condition above is equivalent to SONC-SDP. Comparing Propositions [\ref=prop:sosc_slack] and [\ref=prop:sonc_slack], we see that the second-order conditions derived through the aid of slack variables have "no-gap" in the sense that, apart from regularity conditions, the only difference between them is the change from ">  " to "≥  ".

Computational experiments

Let us now examine the validity of the squared slack variables method for NSDP problems. We tested the slack variables approach on a few simple problems. Our solver of choice was PENLAB [\cite=Penlab], which is based on PENNON [\cite=Pennon] and uses an algorithm based on the augmented Lagrangian technique. As far as we know, PENLAB is the only open-source general nonlinear programming solver capable of handling nonlinear SDP constraints. Because of that, we have the chance of comparing the "native" approach against the slack variables approach using the same code. We ran PENLAB with the default parameters. All the tests were done on a notebook with the following specs: Ubuntu 14.04, CPU Intel i7-4510U with 4 cores operating at 2.0Ghz and 4GB of RAM.

In order to use an NLP solver to tackle [\eqref=eq:sdp], we have to select a vectorization strategy. We decided to vectorize an n  ×  n symmetric matrix by transforming it into an [formula] vector, such that the columns of the lower triangular part are stacked one on top of the other. For instance, the matrix [formula] is transformed to the column vector [formula].

Modified Hock-Schittkowski problem 71

There is a known suite of problems for testing nonlinear optimization problems collected by Hock and Schittkowski [\cite=HS80] [\cite=S09]. The problem below is a modification of problem 71 of [\cite=S09] and it comes together with PENLAB. Both the constraints and the objective function are nonconvex. The problem has the following formulation:

[formula]

We reformulate the problem [\eqref=eq:Hock] by removing the positive semidefiniteness constraints and adding a squared slack variable Y. We then test both formulations using PENLAB. The initial point is set to be x  =  (5,5,5,5,0,0) and the slack variable to be the identity matrix Y  =  I4. This produces infeasible points for both formulations. Nevertheless, PENLAB was able to solve the problem via both approaches. The results can be seen in Table [\ref=table:hock]. The first three columns count the numbers of evaluations of the augmented Lagrangian function, its gradients and its Hessians, respectively. The fourth column is the number of outer iterations. The "time" column indicates the time in seconds as measured by PENLAB. The last column indicates the optimal value obtained. It seems that there were no significant differences in performance between both approaches.

The closest correlation matrix problem -- simple version

Given an m  ×  m symmetric matrix H with diagonal entries equal to one, we want to find the element in Sm +  which is closest to H and has all diagonal entries also equal to one. The problem can be formulated as follows:

[formula]

This problem is convex and, due to its structure, we can use slack variables without increasing the number of variables. We have the following formulation:

[formula]

In our experiments, we generated 100 symmetric matrices H such that the diagonal elements are all 1 and other elements are uniform random numbers between - 1 and 1. For both [\ref=eq:cor1] and [\ref=eq:cor2], we used X  =  Im as an initial solution in all instances. We solved problems with m  =  5,10,15,20 and the results can be found in Table [\ref=table:ex1]. The columns "mean", "min" and "max" indicate, respectively, the mean, minimum and maximum of the running times in seconds of all instances. For this problem, both formulations were able to solve all instances. We included the "mean time" column just to give an idea about the magnitude of the running time. In reality, for fixed m, the running time oscillated highly among different instances, as can be seen by the difference between the maximum and the minimum running times. We noted no significant difference between the optimal values obtained from both formulations.

We tried, as much as possible, to implement gradients and Hessians of both problems in a similar way. As [\ref=eq:cor1] is an example that comes with PENLAB, we also performed some minor tweaks to conform to that goal. Performance-wise, the formulation [\ref=eq:cor2] seems to be competitive for this example. In most instances, [\ref=eq:cor2] had a faster running time. In Figure [\ref=fig:corr_20], we show the comparison between running times, instance-by-instance, for the case m  =  20.

The closest correlation matrix problem -- extended version

We consider an extended formulation for [\ref=eq:cor1] as suggested in one of PENLAB's examples, with extra constraints to bound the eigenvalues of the matrices:

[formula]

where κ is some positive number greater than 1 and the notation [formula] means X  -  κIm∈Sm + . This is a nonconvex problem, and using slack variables, we obtain the following formulation:

[formula]

In our experiments, we set κ  =  10. As before, we generated 100 symmetric matrices H whose diagonal elements are all 1 and other elements are uniform random numbers between - 1 and 1. For [\ref=eq:cor_ext1], we used z  =  1 and X  =  Im as initial points. For [\ref=eq:cor_ext2], we used an infeasible starting point z  =  1, X  =  Y2  =  Im and Y1  =  3Im. We solved problems with m  =  5,10,15,20 and the results can be found in Table [\ref=table:ex2]. The columns have the same meaning as in Section[\ref=sec:ccm1]. This time, we saw a higher failure rate for the formulation [\ref=eq:cor_ext2]. We tried a few different initial points, but the results stayed mostly the same. The best results were obtained for the case m  =  5 and m  =  10, where [\ref=eq:cor_ext2] had a performance comparable to [\ref=eq:cor_ext1], although the latter seldom failed. For m  =  15 and m  =  20, [\ref=eq:cor_ext2] was slower than [\ref=eq:cor_ext1], which is expected, because the number of variables increased significantly. However, it was still able to solve the majority of instances. In Figure [\ref=fig:corr_ext], we show the comparison of running times, instance-by-instance, for the cases m  =  10 and m  =  20.

Final remarks

In this article, we have shown that the optimality conditions for [\eqref=eq:sdp] and [\eqref=eq:sdps] are essentially the same. One intriguing part of this connection is the fact that the addition of squared slack variables seems to be enough to capture a great deal of the structure of Sm + . The natural progression from here is to expand the results to symmetric cones. In this article, we already saw some results that have a distinct Jordan-algebraic flavor, such as Lemma [\ref=lemma:psd]. It would be interesting to see how these results can be further extended and, whether clean proofs can be obtained without recoursing to the classification of simple Euclidean Jordan algebras.

As for the computational results, we found it mildly surprising that the slack variables approach was able to outperform the "native" approach in many instances. This warrants a deeper investigation of whether this could be a reliable tool for attacking NSDPs that are not linear. These are precisely the ones that are not covered by the earlier work by Burer and Monteiro [\cite=BM03] [\cite=BM05].