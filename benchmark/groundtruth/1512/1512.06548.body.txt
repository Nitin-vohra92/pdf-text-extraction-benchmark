Interferometric Radio Transient Reconstruction in Compressed Sensing Framework

Introduction

The study of the radio sky at radio wavelengths has increased since the arrival of new sensitive instrumentation. The timescale to produce images in radio was reduced by the development of new imaging techniques taking the full advantage of the instrument. At the same time, the study of known class of transient sources (e.g. pulsars for general relativity tests, Active Galactic Nuclei, etc) and the recent discovery of new class of transients (e.g. Rotating Radio Transients, Fast Radio Bursts, Lorimer type bursts, see [\cite=Lorimer_2007]) has motivated further development for transient detection and characterization.

Imaging via aperture synthesis with interferometric data has been an active field of research for ~  40 years. A radio interferometer gives a limited set of noisy Fourier samples of the sky (the visibilities [\citep=wilson09]) inside the field of view of the instrument. An approximate of the sky can be obtained (assuming the small field approximation) by simply taking the inverse Fourier Transform (FT) of those visibilities. Due to the limited number of baselines, the Fourier plane is incomplete and one requires to process this incomplete Fourier map either by solving a deconvolution problem, using tools such as CLEAN and its derivates (e.g. [\citet=clean] [\citet=cleancs] [\citet=MSMFS]) or by solving the "inpainting" problem by recovering missing information in the Fourier plane. Several teams have addressed this issue within the framework of the recent Compressed Sensing (CS) theory (e.g. [\citet=Garsden_2015] [\citet=Girard_2015] [\citet=Dabbech2015] and other references therein). In addition, next generation of giant interferometers such as LOFAR [\citep=haarlem], suffers from "direction-dependent" effects [\citep=tasse2012] which distort the Fourier relationship between the measurements and the sky (such as array non-coplanarity and dipole projection). In [\citet=Garsden_2015], a new imager compatible with LOFAR combined both a sparse approach given by the CS theory and corrections for A and W effects [\citep=tasse13]. It also demonstrated better angular resolution and lower residuals as compared to classical methods, on simulated and real datasets.

A lot of effort has been put into the development of detection pipelines (e.g. the LOFAR TRAnsient Pipeline - TRAP [\citep=Swinbank_2015], based on fast iterative closed-loop performing calibration / imaging / source detection / catalogue cross-matching). However, being variable and mostly point-like, the transients imaging suffers from the imaging rate. On the one hand, short time integration enables temporal monitoring of a transient, but each snapshot provides poor visibility coverage. Therefore, the image has low signal-to-noise ratio (SNR) due to large fraction of missing data. On the other hand, long time integration ensures a good sampling, but it will average out the temporal variability of the transient by mixing and diluting "ON" state periods with "OFF" state periods. As a result, a variety of transient radio sources might be missed due to uncertainty or timescale filtering. Consequently, it is difficult to use classical imagers to detect and image transient source when the temporal variability of the transient source is unknown. There is an interest in developing fast imagers, enable to cope with the time variability of the sources. Such imagers can rely on the CS framework to give a quick approximate of the true sky, giving access to faster transients. This motivated the development of a 2D-1D sparse reconstruction imager on the experience obtained in the 2D imaging case.

Radio transient reconstruction in compressed sensing framework

Compressed sensing theory

Shannon theory is commonly known in the domain of signal processing to perfectly reconstruct a regularly sampled signal. However, the innovative sampling and compression theory of recent years, Compressed Sensing (CS) [\citep=Candes2008], or Compressive Sensing could go beyond the Shannon limit, at a rate significantly below the Nyquist rate, to capture and represent compressible signals based on the sparsity of observed signals.

The CS theory is a paradigm for finding a nearly exact reconstruction in the case of an undetermined problem. In the radio interferometry imaging problem, as we have fewer observations than unknowns (i.e. the sparsely sampled FT of the sky), the CS theory applies and could enable to produce accurate maps possibly with improved angular resolution. To achieve the perfect reconstruction from few samples, the CS theory relies on two principles: sparsity and incoherence. First, in general, the CS theory exploits the fact that the signal can be sparse or compressive in some dictionary [formula]. For instance, a signal [formula] may be not sparse in the direct space, but can be sparse in the wavelet space. In such case, [formula] can be decomposed as its sum of few, but significant, coefficients, as [formula], with T relatively small. Second, the incoherence principle states that a sparse signal in the dictionary [formula] must be as dense as possible in the domain where it is acquired. It means that the sensing vectors must be as different as possible from the atoms of [formula].

2D-1D inverse problem formulation

As indicated in the subsection [\ref=ssec:CS], the interferometry imaging problem constitutes an ill-posed inpainting problem which can be described mathematically in Eq. [\eqref=eq:visibilities]:

[formula]

with [formula], the measured visibility vector, [formula] the sampling mask which accounts for incomplete sampling in the Fourier space, [formula] the FT operator, [formula] the sky, and [formula] the noise. The sky [formula], expressed in the direct space, is a real quantity while the noise [formula] is complex as it alters both amplitude and phase of the visibility measurements.

By extension, the application to transient imaging leads to recast the entities of Eq. [\eqref=eq:visibilities] as time-dependent entities. In that scope, the data model of the sky [formula], containing a transient source, will be a cube. At a given frequency, two dimensions are associated with the spatial information and the third dimension describe its time dependence. In the classical 2D case, the masking operator [formula] is a given, depending on the frequency and time integration. In our case, we have to account for its time-dependence as we considered a ground-based radio interferometer that rotates with Earth (leading to the apparent motion of the sky). [formula] will sample different region of the sky FT with time which is a cumulative effect to the variation of the sampled FT of the varying sky. The imaging of a transient radio sky, can be regarded as a 2D-1D spatial-temporal image reconstruction problem.

To comply with the CS framework, the choice of the corresponding 2D-1D sparse representation Φ is critical for our problem. Fortunately, as the temporal information is not correlated to the spatial information, we can separate the 2D-spatial dictionary and 1D-temporal dictionary rather than looking for a 3D dictionary. Therefore, as described in [\citep=starckfermi], an ideal wavelet function would be ψ(x,y,t) = ψ(xy)(x,y)ψ(t)(t) where the space (xy) and time (t) are independent. As in [\cite=Garsden_2015], we selected the 2D starlets [\citep=starlet] which have proven to be adapted to astronomical sources. For the 1D temporal transform ψ(t), we used decimated wavelet functions such as Haar or biorthogonal CDF 9/7 wavelets, depending on the temporal profile of the transient. The whole 2D-1D decomposition scheme is illustrated in Fig. [\ref=2d1d-scheme]. Firstly, assuming a cube of size Nx  ×  Ny  ×  Nz where Nz denotes the number of time frames, the starlet transform is operated on each time frame, yielding N2D spatial scales for each frame. Secondly, For each spatial scale, the temporal 1D transform is performed on each pixel column in the temporal direction of each scale. The 1D transform is decimated and will not increase the size of coefficients in time. Thus, we obtain a 2D-1D coefficient set of size N2D  ×  Nx  ×  Ny  ×  Nz.

Given this 2D-1D dictionary Φ, the minimization problem can be formulated from Eq. [\eqref=eq:visibilities] in the analysis framework:

[formula]

where ε denotes the error radius. The objective minimization function takes the form of [formula] where the [formula]-norm (summation of absolute values of coefficients) is used to reinforce the sparsity of the solution and ensure the convexity of the problem. However, the [formula]-norm involves a soft-thresholding operator which induce bias is solutions [\citep=starlet]. This is particularly unsuitable for scientific data analysis involving photometry. According to  [\cite=Candes2007], the reweighted [formula] scheme is one way to handle this issue. To address this issue, we adopted a reweighted [formula] scheme [\citep=Candes2007], by defining a weighting vector [formula]. In addition, as the source photometry is always assumed positive, we impose a positivity constraint as well. Thus, our convex minimization problem can be formulated as:

[formula]

where [formula], a scale-dependent vector, is a Lagrangian parameter which depends implicitly on ε of the data fidelity in [\eqref=eq:min], the operator [formula] denotes the element-by-element multiplication and [formula] is the indicator function (which is 0 if x belongs to [formula], +    ∞   otherwise)

As [formula] is not explicitly related to the error radius ε, the mathematical relation between [formula] and ε is not easy to find out. However, since the real dataset [formula] is noisy and makes the sparsity constraint decline in the 2D-1D decomposition space, [formula] is closely related to the statistical distribution of decomposition coefficients. Thus, the study of the statistical distribution is important. In practice, several noise driven strategies are available to estimate the statistical distribution. One of the strategies is noise-driven strategy from the residual, which is used hereof. As we will see in section [\ref=ssec:algo], the residual is obtained by [formula] in the n-th iteration. Consequently, the statistical distribution [formula], and [formula] is accessible by the reliable noise estimator MAD (Median of the Absolute Deviation). Then, [formula] where k defines the level of the significant coefficients which lie within the band kσ of the Gaussian distribution.

Algorithms

According to [\cite=Candes2007], the reweighted [formula] scheme is applied as follows:

Set the iteration count [formula] and initialize [formula].

Solve the minimization problem [\eqref=eq:anaPos] yielding a solution [formula], and [formula] is obtained by [formula].

Update the weights (see later on).

Terminate on convergence or when reaching the maximum number of iterations [formula]. Otherwise, increment n and go to step 2.

First, a biased solution [formula] is obtained by the non-reweighted convex optimization, then a weighting step is performed using the following weighting strategy: if |αi,j|  ≥  k'σj then wi,j=k'σj / |αi,j|, else, wi,j=1 (operation later described as function f(|αi,j|). We update the weights for each entity i at scale j, σj is the noise standard deviation at scale j. k' acts as a reweight level in scale j. Then, we subsequently apply the proximal theory to solve the minimization problem [\eqref=eq:anaPos] with the Condat-Vũ splitting method (CVSM -  [\cite=condat2013] [\cite=vu2013splitting]). The CVSM introduces a primal-dual pair (x,u) to solve the convex optimization problem [\eqref=eq:anaPos] using forward-backward algorithm. Thus, in summary, the adapted CVSM with reweighted scheme is presented in Algo [\ref=algo:vu_ana], where the parameters τ and η respect the convergence condition such as [formula], and μ is a relaxation parameter used to accelerate the algorithm. If μ = 1, the algorithm is in the unrelaxed case or no acceleration case. In the Algo [\ref=algo:vu_ana], the line 3 can be considered as the forward step to converge the non-negative solution from the residual [formula], while the line 4 can be regarded as the backward step to enforce the sparsity constraint by using the soft-thresholding proximity ([formula]).

Experiments

We simulated a sky model of size 32×  32×  64, i.e. 32×  32 pixels on image plane and 64 2-min frames on time. The time-dependent sky model is constituted of a control steady source at the center of the field and a transient source with a gaussian light curve (FWHM = 20 min located at time slice T=24). Both sources have the same peak flux density of 10 arbitrary unit in the sky model. Then, to do the realistic simulation, we generated a 2-hour Fourier sampling mask cube by using an uniform random distribution of 20 antennas observing at the zenith at the latitude of the Nan�ay Radio Observatory.

To generate the observed visibilities in terms of noise levels, we take the FT of the sky model and apply the mask cube, and then add white gaussian noise with various magnitude (σ  =  0.0,0.5,1.0,1.5 flux unit) on the complex visibilities. Then, by FT inversion, we can obtain the characteristic dirty cube for each case: Fig. [\ref=dirtyrecons] (left) shows the transient "OFF" state (first row) and its "ON" state (second row) of the dirty cube. We notice that when the noise level is high, the transient source can be confused with background artifacts.

Figure [\ref=dirtyrecons] (right) illustrates the light curve of the transient source central pixel from the sky model cube (dashed curve), the dirty cube (red curve) and the reconstructed cube obtained by the Condat-Vũ algorithm (green curve) described in Sect. [\ref=ssec:algo]. We can conclude that with no additional noise (but with the sampling noise due to missing data), our 2D-1D CS method gives a perfect profile reconstruction, with flux unit relative error ~  10- 5. As the noise level increases, the flux density of the transient is more spread around the central pixel, resulting in a bias of the peak. However, by summing the flux of the transient on nearby pixels (over a surface equal to the source size), the profile of the transient is again well recovered. Meanwhile, the steady source (not shown) is also well recovered except in the high noise regime where fluctuations are present. From these preliminary results, it seems that our 2D-1D CS reconstruction method is efficient reconstruct the transient in a noisy dataset. The reconstruction time was not monitored but is subject to further study.

Conclusions

CS theory offers new tools for solving ill-posed problems such as the imaging problem in radio interferometry. In previous studies, such as [\cite=Garsden_2015], we have shown that the 2D CS method can outperform classical tools used for deconvolution. In this work, we present an extension of this 2D imager, by recovering the 3D data with the third dimension being the temporal information to detect and reconstruct radio transients. We used respectively a 2D and a 1D wavelet dictionaries to perform 2D-1D reconstruction. In addition, we minimize the convex problem using Condat-Vũ splitting method in the proximal theory framework. The preliminary results based on simulated data cubes containing both steady and transient sources show a good potential for transient imaging. For next steps, we will compare our method with classical deconvolution methods and develop a "time-agile" imager addressed to the next generation of interferometers, such as LOFAR and SKA, to enable the detection of radio transients. A featured paper is in preparation, and contains extended tests on various dataset (simulated and real datasets containing transient source). As the search for known and unknown transient is a emerging field in radio astronomy, the development of such tools may have a strong impact on transient studies.