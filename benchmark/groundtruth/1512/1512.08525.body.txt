Introduction

Probabilistic graphical models (PGM) consist of a structural model and a set of conditional probabilities [\cite=1] [\cite=2]. They are widely used in machine learning and data mining techniques, like classification, speech recognition [\cite=korayem2007optimizing], bioinformatics [\cite=eddy1994rna] [\cite=soding2005protein], Natural Language Processing (NLP) [\cite=christodoulopoulos2011bayesian] [\cite=kupiec1992robust], etc. Scalability and restricted domain size (e.g., propositional domain) are the major challenges for PGMs. To overcome these challenges one would expect extensions to the existing PGMs. One extension is offered by the hierarchical probabilistic graphical models (HPGM) which aim to extend the PGM to work with more structured domains [\cite=15] [\cite=fine1998hierarchical]. However, this extension tackles the restricted domain size problem, but not scalability. For hierarchical data, where data can be divided into several levels arranged in tree-like structures, data items in each level depend on or are influenced only by the data items in the upper levels while a Bayesian Network (BN) is the most appropriate PGM to represent a probability distribution since the dependencies in this kind of data are not bidirectional, a BN is often infeasible, as it may not provide a concise enough representation of a large probability distribution.When dealing with the kind of massive hierarchical data that is becoming increasingly common in the big data era, this is true because each level represents a random variable, while each node in that level represents an outcome (possible value) of that random variable, so the data can grow horizontally (number of values) faster than vertically (number of random variables). Moreover, since the dependency between the random variables is pre-defined in the hierarchical data, the structure of the network is predefined. Hence, the first phase of building Bayesian Network to find the optimal structure is not applicable

For example, consider the glycan ontology "GlycO" [\cite=thomas2006modular] which describes 1300 glycan structures (see section [\ref=sec:Exp]) whose theoretical tandem mass spectra (MS) can be predicted by GlycoWorkbench [\cite=24]. If the maximum of cleavages is set to two and the number of cross-ring cleavages is set to one, then the theoretical MS2 spectrum contains 2,979,334 ions, which themselves can be fragmented to form tens of millions of ions in MS3. To represent this data set of only two levels of the MS data using a Bayesian Network (BN) the network will be composed of two nodes, MS1 and MS2 with a single path MS1  →  MS2 while the conditional probability table (CPT) for the MS2 will contain 3,873,134,200 (2,979,334 ×   1300) entries. For this kind of data, we propose a simple probabilistic graphical model for massive hierarchical data (PGMHD) which we consider as an extension to the Bayesian Network (BN,) that can represent massive hierarchical data in a more efficient way. We successfully apply the PGMHD in two different domains: bioinformatics (for multi-label classification) and search log analytics (for latent semantic discovery of related terms, as well as, semantically ambiguous terms).

The main contributions of this paper are as follows: We propose a simple, efficient and scalable probabilistic model that extends Bayesian Network for massive hierarchical data. We successfully apply this model to the bioinformatics domain in which we automatically classify and annotate high-throughput mass spectrometry data. We also apply this model for large-scale latent semantic discovery and semantically ambiguous terms discovery using 1.6 billion search log entries provided by CareerBuilder.com, using the Hadoop Map/Reduce framework.

Background

Graphical models can be classified into two major categories: (1) directed graphical models (the focus of this paper), which are often referred to as Bayesian Networks, or belief networks, and (2) undirected graphical models which are often referred to as Markov Random Fields, Markov networks, Boltzmann machines, or log-linear models [\cite=3]. Probabilistic graphical models (PGMs) consist of both graph structure and parameters. The graph structure represents a set of conditionally independent relations for the probability model, while the parameters consist of the joint probability distributions [\cite=1]. Probabilistic graphical models are often considered to be more convenient than numerical representations for two main reasons [\cite=4]:

To encode a joint probability distribution for P(X1,...,Xn) for n propositional random variables with a numerical representation, we need a table with 2n entries.

Inadequacy in addressing the notion of independence: to test independence between X and Y, one needs to test whether the joint distribution of x and y is equal to the product of their marginal probability.

PGMs are used in many domains. For example, Hidden Markov Models (HMM) are considered a crucial component for most of the speech recognition systems [\cite=korayem2007optimizing]. In bioinformatics, probabilistic graphical models are used in RNA sequence analysis [\cite=eddy1994rna]. In natural language processing (NLP), HMM and Bayesian models are used for part of speech (POS) tagging [\cite=christodoulopoulos2011bayesian]. The problem with PGMs in general, and Bayesian Networks in particular, is that they are not suitable for representing massive data due to the time complexity of learning the structure of the network and the space complexity of storing a network with thousands of random variables or random variables taking in many values. In general, finding a network that maximizes the Bayesian score which maximizes the posterior probability and Minimum Description Length (MDL) score which gives preference to a simple BN over a complex one, is an NP-hard problem [\cite=5].

Bayesian Network

A Bayesian Network is a concise representation of a large probability distribution to be handled using traditional techniques such as tables and equations [\cite=6]. The graph of a Bayesian Network is a directed acyclic graph (DAG) [\cite=2]. A Bayesian Network consists of two components: a DAG representing the structure (as shown in Figure [\ref=bn]), and a set of conditional probability tables (CPTs). Each node in a Bayesian Network must have a CPT which quantifies the relationship between the variable represented by that node and its parents in the network. Completeness and consistency are guaranteed in a Bayesian Network since there is only one probability distribution that satisfies the Bayesian Network constraints [\cite=6]. The constraints that guarantee a unique probability distribution are the numerical constraints represented by CPT and the independence constraints represented by the structure itself. The independence constraints is shown in Figure [\ref=bn]. Each variable in the structure is independent of any other variables other than its parents, once its parents are known. For example, once the information about A is known, the probability of L will not be affected by any new information about F or T, so we call L independent of F and T once A is known.

Bayesian Networks are widely used for modeling causality in a formal way, for decision-making under uncertainty, and for many other applications [\cite=6].

Related Work

Our research is related closely to Bayesian Network classifiers. In this section we review different forms of Bayesian Network classifiers to understand how the PGMHD extends BN in a way different than the existing models. We will cover the following BN classifiers:

Naive Bayes Classifier (NB).

Selective Naive Bayes (SNB)

Tree Augmented Naive Bayes (TAN).

Hidden Naive Bayes (HNB).

we will also cover how we applied PGMHD to other data mining problems, such as, latent semantic discovery of related search terms in users search logs, and of semantically ambiguous keywords by analyzing users' search logs.

Naive Bayes (NB)

Naive Bayes is the simplist form of the BN classifiers and the most common one. This classifier is based on an assumption that all the features are independent given the class. Figure [\ref=nb] shows an example of NB. A NB classifier is defined as follows:

[formula]

Where [formula]. P(c) is the prior probability of class c and P(xj|c) is the conditional probability of feature/variable xj. The value of c that maximizes the right hand side is chosen. A Naive Bayes classifier's performance depends upon the quality of the predictor features, such that the performance is improved when the predictor features are relevant and non redundant.

Selective Naive Bayes (SNB)

In order to improve the performance of BN classifier by selecting the predictive features that are relevant and not redundant, Selective Naive Bayes (SNB) [\cite=bielza2014discrete] is proposed as a feature subset selection problem. Let us define [formula] as the projection of [formula] onto a selected feature subset [formula]. The classification equation becomes

[formula]

Tree Augmented Naive Bayes (TAN)

This form of Bayesian Network classifier extends the NB by allowing each attribute to have at most one attribute parent in addition to its class parent. This extension tends to represent the fact that in some cases there is dependency or influence between features in a way that a value of a feature xj depends on a value of feature y. TAN classifier is defined as follows:

[formula]

Where pxj is the attribute parent of xj. Figure [\ref=tan] shows an example of TAN.

Hidden Naive Bayes (HNB)

HNB (Figure [\ref=hnb]) is another extension of NB. In this extension each attribute Ai gets a hidden parent Ahpi to integrate the influences from all other attributes. The definition of the HNB classifier is as follows:

[formula]

where,

[formula]

Probabilistic Graphical model for Massive Hierarchical Data Problems (PGMHD)

In this section we describe PGMHD. We discuss the structure of the model, its learning algorithm, and how it extends BN.

Model Structure

Consider a multi-level directed graph G = (V,A) where V and A  ⊂  V  ×  V denote the sets of nodes and arcs, respectively, such that:

V is partitioned into m levels [formula] such that [formula], and [formula] for [formula].

The arcs in A only connect one level to the next, i.e., if a∈A then a∈Li - 1  ×  Li for some [formula].

An arc a = (vi - 1,vi)∈Li - 1  ×  Li represents the dependency of vi with its parent vi - 1, [formula]. Moreover, let :V  →  P(V) be the function that maps every node to its parents, i.e.,

[formula]

The nodes in each level Li represent all the possible outcomes of a finite discrete random variable, namely Xi, [formula].

Note that the nodes in the first level L0 can be seen as root nodes and the ones in Lm - 1 as leaves. Also, an observation x in our probabilistic model is an outcome of a random variable, namely [formula], defined as

[formula]

which represents a path from L0 to Lm - 1 such that (Xi - 1,Xi)∈A.

In addition, we assume that there are t observations of X, namely [formula], and let [formula] be a frequency function defined as f(w,v) =  Frequency of Co-Occurrence w and v. Moreover, these latter t observations are the ones used to train our model, so that f(w,v) > 0 for every (w,v)∈A.

It should be observed that the proposed model can be seen as a special case of a Bayesian Network by considering a network consisting of directed predefined paths. However, we believe that a leveled directed graph that explicitly defines one node per outcome of the random variables (as described above): i) leads to an easily scalable (and distributable) implementation of the problems we consider; ii) improves the readability and expressiveness of the implemented network; and iii) simplifies and facilitates the training of the model.

Probabilistic-based Classification

Given an outcome at level [formula], namely v∈Li, we calculate the classification score i(w|v) of v to the parent outcome w∈Li - 1 by estimating the conditional probability P(Xi - 1 = w|Xi = v) as follows

[formula]

where

[formula]

and

[formula]

Probabilistic-based Similarity scoring

Fix a level [formula], and let [formula] be identically distributed random variables as in [\eqref=randX]. We define the probabilistic-based similarity score CO (Co-Occurrence) between two outcomes xi,yi∈Li by computing the conditional joint probability

[formula]

as

[formula]

where pi(w,v) = P(Xi - 1 = w,Xi = v) for every (w,v)∈Li - 1  ×  Li. We can naturally estimate the probabilities pi(v,w) with p̂(v,w) defined as

[formula]

Hence, we can obtain the related outcomes of xi∈Li (at level i) by finding all the y∈Li with a large estimated probabilistic similarity score (xi,yi).

Progressive Learning

PGMHD is designed to allow progressive learning which is shown in Algorithm [\ref=alg:learn]. Progressive learning is a learning technique that allows a model to learn gradually over time. Training data does not need to be given at one time to the model. Instead, the model can learn from any available data and integrate the new knowledge incrementally. This learning technique is very attractive in the big data age for the following reasons:

Training the model does not require processing all data upfront

It can easily learn from new data without the need to re-include the previous training data in the learning.

The training session can be distributed instead of doing it in one long-running session.

It supports recursive learning which allows the results of the model to be used as new training data, provided they are judged to be accurate by the user.

PGMHD an extension to NB

PGMHD extends NB in different directions to improve its scalability and ability to handle massive hierarchical data as follows:

It enables multi-label classification.

It enables multi-level representation of the predictive features.

It enables lazy classification.

The first dimension PGMHD extends is the multi-label classification. Our model allows more than one class to be in the root level of the classifier where any instance can be classified to more than one class. The second dimension of this extension is the multi-level classification which allows the classifier to represent the predictive features in m levels instead of only 2 levels as in the regular NB. This extension allows the hierarchical modeling to preserve the structure of the data, which our experiments show is important for improving the quality of the classification. The last dimension of this extension is lazy classification against the eager NB. PGMHD is considered a lazy classifier since the calculation of the classification score of a new instance is all done during the classification process, unlike NB where all the CPT are pre-calculated and stored. This extension makes the PGMHD suitable for progressive learning, which can be very important for scalability.

Experiments and Results

Glycans (Figure [\ref=glycan]) are the third major class of biological macro-molecules besides nucleic acids and proteins [\cite=9]. Glycomics refers to the scientific attempts to characterize and study glycans, as defined in [\cite=9] or an integrated systems approach to study structure-function relationships of glycans as defined in [\cite=10].

Mass spectrometry (MS) is an analytical technique used to identify the composition of a sample [\cite=ma2010challenges]. Although (MS) has become the major analytical technique for glycans, no general method has been developed for the automated identification of glycan structures using MS and tandem MS data. MSn refers to the sequence of MS selection with some form of fragmentation, it is also called tandem MS. The relative ease of peptide identification using tandem MS is mainly due to the linear structure of peptides and the availability of reliable peptide sequence databases. In proteomic analyses, a mostly complete series of high abundance fragment ions is often observed. In such tandem mass spectra, the mass of each amino acid in the sequence corresponds to the mass difference between two high-abundance peaks, allowing the amino acid sequence to be deduced. In glycomics MS data, ion series are disrupted by the branched nature of the molecule, significantly complicating the extraction of sequence information. In addition, groups of isomeric monosaccharides commonly share the same mass, making it impossible to distinguish them by MS alone. Databases for glycans exist but are limited, minimally curated, and suffer badly from pollution from glycan structures that are not produced in nature or are irrelevant to the organism of study. PGMHD attempts to employ machine learning techniques (mainly probabilistic-based multi-label classification) to find a solution for the automated identification of glycans using MS data.

We recently implemented the Glycan Elucidation and Annotation Tool (GELATO), which is a semi-automated MS annotation tool for glycomics integrated within our MS data processing framework called GRITS (http://www.grits-toolbox.org/). Figures [\ref=MS1], and [\ref=MS2_1] show screen shots from GELATO for annotated spectra. Figure [\ref=MS1] shows the MS profile level and Figure [\ref=MS2_1] shows the annotation of MS2 peaks using fragments of a selected candidate glycan for annotation of the MS1 data. The output GELATO represents all the possible annotations to the given spectra. The user may select a subset of those possible annotations as the correct ones, but then he/she needs a smarter tool that can learn the correct selection and eliminate the incorrect ones in the future. PGMHD is successfully applied for that purpose as we show in this section.

To represent the MS data annotation using PGMHD, each annotation of MS1 data (which is a glycan) is represented as a node in the top-layer of PGMHD. All the fragments generated by that glycan and used to annotate peaks in MS2 are represented by nodes in the lower layer and connected by edges with the parent node in the upper layer, and this pattern can be extended until MSn. Each fragment at level MSi is represented by a node in layer Li - 1 and connected by an edge with its parent node at layer Li - 2. The edge's weight represents the co-occurrence frequency between a child and a parent, and storing frequencies rather than probabilities facilitates progressive learning. Figure [\ref=pgmhd_ms] shows the PGMHD for MS data with three levels (MS1, MS2, and MS3) in these figures. As shown in the model, three layers are created: one for the MS1 level, a second one for the MS2 level, and a third for MS3. Several different nodes at the MS1 level can be annotated with the same fragment ion at the MS2 level, so MS2 nodes can have several parents. The frequency values are shown on the edges.

We annotated 3314 MS scans of banceriatic cancer samples using GELATO. Then an expert manually approved 1990 scan annotations which we used to train and test PGMHD. We split this data set into training data and test data sets. The size of the training data is 1779 scans and 121 scans for testing. We trained PGMHD and compared it against leading classifiers including Naive Bayes [\cite=rish2001empirical], SVM [\cite=cortes1995support], Decision Tree [\cite=quinlan1987simplifying], K-NN [\cite=altman1992introduction], Neural Network [\cite=werbos1988generalization], Radial Basis Function network (RBF Network) [\cite=poggio1989theory] and Bayesian Network [\cite=pearl1985bayesian] from Weka [\cite=hall2009weka]. Then we provide the test list to each classifier to predict the best glycan that annotates the scans in the test set. We also used Mulan [\cite=tsoumakas2011mulan], which is a Java library that extends Weka classifiers to handle multi-label classification problems. Also, we applied the m-estimate as a probability estimation technique To help PGMHD overcome the common problem for any Bayesian model which is the zero-frequency problem [\cite=jiang2007scaling]. Figure [\ref=Prec_Rec_MS] shows the precision and recall for the different classifiers compared to PGMHD after we used Mulan for multi-label classification and m-estimate for PGMHD. Another important aspect in our experiment besides accuracy is the scalability. In order to measure the scalability of PGMHD compared to the other classifiers we measure the space and time complexity. Figure [\ref=Training] shows how PGMHD was the fastest model in the training phase, however it was not the best in the classification time as shown in Figure [\ref=Classification], though it did get the third best time. Most important is the space complexity used by each model which is shown in Figure [\ref=Memory]. The memory usage which is the most important aspect in the scalability shows that PGMHD is much better than all the other classifiers especially the bayesian ones. Due to the difficulty of getting more manually curated MS annotations dataset for testing the scalability of our model in comparison to other machine learning models, we synthesized a dataset using GELATO. To synthesize a dataset with a massive number of MS annotations we used GELATO to generate all the possible annotations for the MS experiments which were manually curated before. We assume that all the generated annotations by GELATO are valid and correct annotations. Our focus in this part of the experiment is the scalability not the accuracy since the accuracy was already tested using the manually curated dataset. The new dataset includes 6776 instances for training and 392 instances for testing. The number of features is 2952 while the number of classes is 1340. As a result of this extension in the training data, the Baysian Network classifer, K-NN, and RBF ran out of memory which means they can not handle this dataset in 4 GB of main memory. On the other hand PGMHD used only 160 MB to represent this dataset in memory. Figure [\ref=exp2] shows the memory usage of the models which scale successfully to handle the new dataset.

Semantically Related Keywords in Search Logs

Semantic similarity is a metric that is defined over documents or terms in which the distance between them reflects the likeness of their meaning  [\cite=harispe2013semantic], and it is widely used in Natural Language Processing (NLP) and Information Retrieval (IR) [\cite=mihalcea2006corpus]. Generally, there are two major techniques used to compute semantic similarity: one is computed using a semantic network (Knowledge-based approach) [\cite=budanitsky2001semantic], and the other is based on computing the relatedness of terms within a large corpus of text (corpus-based approach) [\cite=mihalcea2006corpus]. The major techniques classified as corpus-based approaches are Pointwise Mutual Information (PMI) [\cite=bouma2009normalized] and Latent Semantic Analysis (LSA) [\cite=dumais2004latent], though PMI outperforms LSA on mining the web for synonyms [\cite=turney2001mining]. A group of Google researchers proposed two efficient models which can discover semantic word similarities [\cite=mikolov2013efficient]. The two novel models are the following:

Continuous Bag-of-Words model

Continuous Skip-gram model

These models aim to use large scale Neural Network to learn word vectors. The two models have restrictions that make them not suitable in our usecase. The first restriction is that both models require words and context in which those words are used. In their experiments, the authors built vectors of at least 50 words around the given word (words before and after the given word from the text in which that word was used). One more restriction is that they allow only single token words to be processed (no phrases). In our case, the two models are not applicable since the searches conducted by the users usually contains a single phrase with no context or other words surrounding it. Also, we care about phrases as opposed to single words, since small phrases are most commonly used in our search engine. For example "Java Developer" should be considered as a single phrase when we discover the semantically related phrases. In our experiment, we discovered high quality semantic relationships using a data set of 1.6 billion search logs (search keywords used to search for jobs on CareerBuilder.com). PGMHD completed this task in 45 minutes.

It is tempting to suggest using a synonym dictionary since the problem sounds like finding synonyms, but the problem here is more complicated than finding synonyms since the search terms or phrases on our site are often job titles, skills, and company names which are not, in most cases, regular words from any dictionary. For example if a user searches for "java developer", we would not find any synonyms for this phrase in a dictionary. Another user may search for "hadoop" which is also not a word that would be found in a typical English dictionary.

Experiment Setup and Results

The experiment performing latent semantic discovery among search terms using PGMHD was run on a Hadoop cluster with 69 data nodes, each having a 2.6 GHz AMD Opteron Processor with 12 to 32 cores and 32 to 128 GB RAM. Table [\ref=pgmhdResults] shows sample results of 10 terms with their top 5 related terms discovered by PGMHD. To evaluate the model's accuracy, we sent the results to data analysts at CareerBuilder who reviewed 1000 random pairs of discovered related search terms and returned the list with their feedback about whether each pair of discovered related terms was "related" or "unrelated". We then calculated the accuracy (precision) of the model based upon the ratio of the number of related results to the total number of results. The results show the accuracy of the discovered semantic relationships among search terms using the PGMHD model to be 0.80.

Discovering Semantic Ambiguity of a Keyword

The semantic ambiguity of a keyword can be defined as the likelihood of seeing different meanings of the same keyword in different contexts [\cite=jayadianti2013solving] [\cite=gracia2007solving]. The techniques mentioned in the literature focuses on utilization of ontologies and dictionaries like Wordnet as described in [\cite=jayadianti2013solving] [\cite=gracia2007solving]. Those solutions are not applicable when the keywords are from a domain like job search. In the job search domain the used keywords are typically job titles, skills, company names, etc. which are not regular English keywords. For example, java can mean a programming language, as well as, coffee but an English dictionary would not provide both of those meanings.

PGMHD is applied successfully to discover the semantic ambiguity of a keyword. About 1.6 billion search logs used in this experiment. The search keywords extracted from those 1.6 billion logs were used to train PGMHD, which was then used to calculate the normalized PMI score for each term with all of its parents. The initial results of this use case are promising, though work to improve the implementation are is still ongoing. We plan to publish a separate paper about this use case of PGMHD soon.

Table [\ref=DisAmbResults] shows sample results of the discovered semantically ambiguous terms using PGMHD.

Conclusions and Future Work

Probabilistic graphical models are very important in many modern applications such as data mining and data analytics. The major issue with existing probabilistic graphical models is their scalability to handle large data sets, making this a very important area for research, especially given the tremendous modern focus on big data due to the number of data points produced by modern computer systems and sensors. PGMHD is a probabilistic graphical model that attempts to solve the scalability problems with existing models in scenarios where massive hierarchical data is present. PGMHD is designed to fit hierarchical data sets of any size, regardless of the domain in which the data belongs. PGMHD can represent the hierarchical data with any number of levels, it can handle multi-label classification, and it is suitable for progressive learning since it is considered to be a lazy classifier. In this paper we present three experiments from different domains: one being the automated tagging of high-throughput mass spectrometry data in bioinformatics, the other being latent semantic discovery using search logs from the largest job board in the U.S, and the last one being identification of semantically ambiguous keywords. The three use cases in which we tested PGMHD show that this model is robust and can scale from a few thousand entries to billions of entries, and that it can also run on a single computer (for smaller data sets), as well as in a parallelized fashion on a large cluster of servers (69 were used in our experiment). PGMHD is used in production at CareerBuilder.com for discovery of semantically related keywords and semantically ambiguous keywords. The work on discovering semantically ambiguous keywords is ongoing, and we plan to publish a separate paper about it. We plan to compare machine learning algorithms implemented in Apache Spark with PGMHD.

Competing interests

The authors declare that they have no competing interests.

Author's contributions

KA carried out the design, implementation, and experiments related to MS annotation, Discovering semantically related keywords, and discovering semantically ambiguous keywords. MK participated in the design, implementation, and experiments related to discovering semantically related keywords, and discovering semantically ambiguous keywords. CO participated in the design, implementation, and experiments related to discovering semantically related keywords, and discovering semantically ambiguous keywords. TG participated in the design, and experiments related to discovering semantically related keywords, and discovering semantically ambiguous keywords. RR, WY, and MP participated in the design and validation of the MS annotation experiments. JM, KR, KK, and WY they all contributed to writing this manuscript and validating the model as well as the results. All authors read and approved the final manuscript.

Acknowledgements

The authors would like to thank David Crandall from Indiana University for providing very helpful comments and suggestions to improve this paper. We also would like to thank Kiyoko Aoki Kinoshita from Soka University for the valuable discussions and suggestions to improve this model. Deep thanks to the search team, the big data team, and the data science team at CareerBuilder.com for their support while implementing and test this model over their Hadoop cluster.