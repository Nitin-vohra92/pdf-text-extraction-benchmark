Semisupervised Autoencoder for Sentiment Analysis

Introduction

In machine learning, documents are usually represented as Bag of Words (BoW), which nicely reduces a piece of text with arbitrary length to a fixed length vector. Despite its simplicity, BoW remains the dominant representation in many applications including text classification. There has also been a large body of work dedicated to learning useful representations for textual data [\cite=vsm] [\cite=lda] [\cite=lsa] [\cite=word2vec] [\cite=xavier]. By exploiting the co-occurrence pattern of words, one can learn a low dimensional vector that forms a compact and meaningful representation for a document. The new representation is often found useful for subsequent tasks such as topic visualization and information retrieval. In this paper, we investigate the application of one of the most popular representation learning methods, namely autoencoders [\cite=bengio], to learn task-dependent representations for textual data. Our model differs from most of the existing work as it naturally incorporates label information into its objective function, which allow the learned representation to be directly coupled with the task of interest.

In this paper we focus on a specific class of task in text mining: Sentiment Analysis (SA). We further focus on a special case of SA as a binary classification problem, where a given piece of text is either of positive or negative attitude. This problem is interesting largely due to the emergence of online social networks, where people consistently express their opinions about certain subjects. Also, it is easy to obtain a large amount of clean labeled data for SA by crawling reviews from websites such as IMDB or Amazon. Thus, SA is an ideal benchmark for evaluating text classification models (and features).

Autoencoders have attracted a lot of attention in recent years as a building block of Deep Learning [\cite=bengio]. They act as the feature learning methods by reconstructing inputs with respect to a given loss function. In a neural network implementation of autoencoders, the hidden layer is taken as the learned feature. While it is often trivial to obtain good reconstructions with plain autoencoders, much effort has been devoted on regularizations in order to prevent them against overfitting [\cite=bengio] [\cite=dae] [\cite=cae]. However, little attention has been devoted to the loss function, which we argue is critical for modeling textual data. The problem with the commonly adopted loss functions (squared Euclidean distance and element-wise KL Divergence, for instance) is that they try to reconstruct all dimensions of input independently and undiscriminatively. However, we argue that this is not the optimal approach when our interest is text classification. The reason is two folds. First, it is well known that in natural language the distribution of word occurrences follows the power-law. This means that a few of the most frequent words will account for most of the probability mass of word occurrences. An immediate result is that the Autoencoder puts most of its effort on reconstructing the most frequent words well but (to a certain extent) ignores the less frequent ones. This may lead to a bad performance especially when the class distribution is not well captured by merely the frequent words. For sentiment analysis, this problem is especially severe because it is obvious that the truly useful features (words or phrases expressing a clear polarity) only occupy a small fraction of the whole vocabulary; and reconstructing irrelevant words such as 'actor' or 'movie' very well is not likely to help learn more useful representations to classify the sentiment of movie reviews. Second, explicitly reconstructing all the words in an input text is expensive, because the latent representation has to contain all aspects of the semantic space carried by the words, even if they are completely irrelevant. As the vocabulary size can easily reach the range of tens of thousands even for a moderate sized dataset, the hidden layer size has to be chosen very large to obtain a reasonable reconstruction, which causes a huge waste of model capacity and makes it difficult to scale to large problems.

In fact, the reasoning above applies to all the unsupervised learning methods in general, which we argue is one of the most important problems to address in order to learn task-specific representations. This naturally leads us to the semisupervised approach, where label information is introduced to guide the feature learning procedure. In particular, we propose a novel loss function for training autoencoders that are directly coupled with the classification task. We first train a linear classifier on BoW, then a Bregman Divergence [\cite=bregman] is derived as the loss function of a subsequent autoencoder. The new loss function gives the autoencoder the information about directions along which the reconstruction should be accurate, and where larger reconstruction errors are tolerated. Informally, this can be considered as a weighting of words based on their correlations with the class label: predictive words should be given large weights in the reconstruction even they are not frequent words, and vice versa. Furthermore, to reduce the bias introduced by the linear classifier, we take a Bayesian view by defining a posterior distribution on the weights of the classifier. We then approximate the posterior with Laplace approximation and derive the marginalized loss function for the autoencoder. We show that our model successfully learns features that are highly discriminative with respect to class labels, and also outperform all the competing methods evaluated by classification accuracy. Moreover, the derived loss can also be applied to unlabeled data, which allows the model to learn further better representations.

Model

Denoising Autoencoders

Autoencoders learn functions that can reconstruct the inputs. They are typically implemented as a neural network with one hidden layer, and one can extract the activation of the hidden layer as the new representation. Mathematically, we are given a collection of data points X  =  {xi},xi∈Rd,i∈[1,m], the objective function of an autoencoder is thus:

[formula]

where W∈Rk  ×  d,b∈Rk,W'∈Rd  ×  k,b'∈Rd are the parameters to be learned; D is a loss function, such as the squared Euclidean Distance [formula]; g and f are predefined nonlinear functions, which we set as g(x)  =   max (0,x), f(x)  =  (1  +  exp( - x))- 1 in this paper; hi is the learned representation; i is the reconstruction. A common approach is to use tied weights by setting W  =  W'; this usually works better as it speeds up learning and prevents overfitting at the same time. For this reason, we always use tied weights in this paper.

Autoencoders transform an unsupervised learning problem to a supervised one by the self reconstruction criteria. This enables one to use all the tools developed for supervised learning such as back propagation to efficiently train the autoencoders. Moreover, thanks to the nonlinear functions f and g, autoencoders are able to learn non-linear and possibly overcomplete representations, which give the model much more expressive power than their linear counter parts such as PCA (LSA) [\cite=lsa].

In this paper, we adopt one of the most popular variants of autoencoders, namely Denoising Autoencoder. Denoising Autoencoder works by reconstructing the input from a noised version of itself. The intuition is that a robust model should be able to reconstruct the input well even in the presence of noises, due to the high correlation among features. For example, imagine deleting or adding a few words from/to a document, the semantics should still remain unchanged, thus the autoencoder should learn a consistent representation from all the noisy inputs. In the high level, Denoising Autoencoders are equivalent to ordinary autoencoders trained with dropout [\cite=dropout], which has been shown as an effective regularizer for (deep) neural networks. Formally, let q(x̄|x) be a predefined noising distribution, and x̄ be a noised sample of x: x̄  ~  q(x̄|x). The objective function takes the form of sum of expectations over all the noisy samples:

[formula]

where we have slightly overloaded the notation to let i denote the reconstruction calculated from the noised input x̄i. While the marginal objective function requires infinite many noised samples per data point, in practice it is sufficient to simulate it stochastically. That is, for each example seen in the stochastic gradient descent training, we randomly sample a x̄i from q(x̄i|xi) and calculate the gradient with ordinary back propagation.

Loss Function as Bregman Divergence

We then discuss the proper choice of the loss function D in [\eqref=eq:daeloss] as a specific form of Bregman Divergence. Bregman Divergence [\cite=bregman] generalizes the notion of distance in a d dimensional space. To be concrete, given two data points ,x∈Rd and a convex function f(x) defined on Rd, the Bregman Divergence of [formula] from x with respect to f is:

[formula]

Namely, Bregman Divergence measures the distance between two points ,x as the deviation between the function value of f and the linear approximation of f around x at [formula].

Two of the most commonly used loss functions for autoencoders are the squared Euclidean distance and element-wise KL divergence. It is not difficult to verify that they both fall into this family by choosing f as the squared [formula] norm and the sum of element-wise entropy respectively. What the two loss functions have in common is that they make no distinction among dimensions of the input. In other words, each dimension of the input is pushed to be reconstructed equally well. While autoencoders trained in this way have been shown to work very well on image data, learning much more interesting and useful features than the original pixel intensity features, they are less appropriate for modeling textual data. The reason is two folds. First, textual data are extremely sparse and high dimensional, where the dimensionality is equal to the vocabulary size. To maintain all the information of the input in the hidden layer, a very large layer size must be adopted, which makes the training cost extremely large. Second, ordinary autoencoders are not able to deal with the power law of word distributions, where a few of the most frequent words account for most of the word occurrences. As a result, frequent words naturally gain favor to being reconstructed accurately, and rare words tend to be reconstructed with less precision. This problem is also analogous to the imbalanced classification setting. This is especially problematic when frequent words carry little information about the task of interest, which is not uncommon. Examples include stop words (the, a, this, from) and topic related terms (movie, watch, actress) in a movie review sentiment analysis task.

Semisupervised Autoencoder with Bregman Divergence

To address the problems mentioned above, we propose to introduce supervision to the training of autoencoders. To achieve this, we first train a linear classifier on Bag of Words, and then use the weight of the learned classifier to define a new loss function for the autoencoder. Now let us first describe our choice of loss function, and then elaborate the motivation later:

[formula]

where θ∈Rd are the weights of the linear classifier, and we have omitted the bias for simplicity. Before we delve into more details, note that Equation [\eqref=eq:df] is a valid distance, as it is non-negative and reaches zeros if and only if   =  x. Moreover, the reconstruction error is only measured after projecting on θ; this guides the reconstruction to be accurate only along directions where the linear classifier is sensitive to. Note also that Equation [\eqref=eq:df] on the one hand uses label information (θ has been trained with labeled data), on the other hand no explicit labels are directly referred to (only requires xi). Thus one is able to train an autoencoder on both labeled and unlabeled data with the loss function in Equation [\eqref=eq:df]. This subtlety distinguishes our method from pure supervised or unsupervised learning, and allows us to enjoy the benefit from both worlds.

As a design choice, we consider SVM with squared hinge loss (SVM2) and [formula] regularization as the linear classifier, but other classifiers such as Logistic Regression can be used and analyzed similarly. Let us denote {xi},xi∈Rd as the collection of samples, and {yi},yi∈{1, - 1} as the class labels; the objective function SVM2 is:

[formula]

Here θ∈Rd is the weight; λ is the weight decay parameter.

Equation [\eqref=eq:svm] is continuous and differentiable everywhere with respect to θ, so the model can be easily trained with stochastic gradient descent. The next (and most critical) step of our approach is to transfer label information from the linear classifier to the autoencoder. With this in mind, we examine the loss induced by each sample as a function of the input, while with θ fixed:

[formula]

Note that f(xi) is defined on the input space Rd, which should be contrasted with L(θ) in Equation [\eqref=eq:svm] which is a function of θ. We are interested in f(xi) because if we consider moving each input xi to i, f(xi) indicates the direction along which the loss is sensitive to. If we think of [formula] as the reconstruction of xi obtained from an autoencoder, a good i should be in a way such that the deviation of i from xi is small evaluated by f(xi). In other words, we would like i to still be correctly classified by the pretrained linear classifier. Therefore, f(xi) should be a much better function to evaluate the deviation of two samples. if we can derive a Bregman Divergence from f(xi) and use it as the loss function of the subsequent autoencoder training, the autoencoder should be guided to give reconstruction errors that do not confuse the classifier. Note that f(xi) is a quadratic function of xi whenever f(xi)  >  0, so we only need to derive the Hessian matrix in order to achieve the Bregman Divergence. The Hessian follows as:

[formula]

Recall that for a quadratic function with Hessian matrix H, the Bregman Divergence is simply (  -  x)TH(  -  x); then we have:

[formula]

In words, Equation [\eqref=eq:df0] says that we measure the reconstruction loss for difficult examples (those that satisfy 1  -  yiθTxi  >  0) with Equation [\eqref=eq:df]; and there is no reconstruction loss at all for easy examples. This discrimination is undesirable, because in this case the Autoencoder would completely ignore easy examples, and there is no way to guarantee that the i can be correctly classified. Actually, this split is just an artifact of the hinge loss and the asymmetrical property of Bregman Divergence. Hence, we perform a simple correction by ignoring the condition in Equation [\eqref=eq:df0], which basically pretends that all the examples induce a loss. This directly yields the loss function as in Equation [\eqref=eq:df].

The Bayesian Marginalization

In principle, one may directly apply Equation [\eqref=eq:df] as the loss function in place of the squared Euclidean distance and train an autoencoder. However, doing so might introduce a bias brought by one single classifier. As a remedy, we resort to the Bayesian approach, which defines a probability distribution over θ. Although SVM2 is not a probabilistic classifier like Logistic Regression, we can borrow the idea of Energy Based Model [\cite=bengio] and use L(θ) as the negative log likelihood of the following distribution:

[formula]

where β  >  0 is the temperature parameter which controls the shape of the distribution p. Note that the larger β is, the sharper p will be. In the extreme case, p(θ) is reduced to a uniform distribution as β approaches 0, and collapses into a single δ function as β goes to positive infinity.

Given p(θ), we rewrite Equation [\eqref=eq:df] as an expectation over θ:

[formula]

Obviously there is now no closed form expression for D(,x). To solve it one could use sampling methods such as MCMC, which provides unbiased estimates of the expectation but could be slow in practice. Instead, we use the Laplace approximation, which approximates p(θ) by a Gaussian distribution p̃(θ)  =  N(,Σ). As estimating the full covariance matrix is prohibitive, we further constrain Σ to be diagonal. The benefit of doing so is that the expectation can now be computed directly in closed form. To see this, by simply replacing p(θ) with p̃(θ) in Equation [\eqref=eq:int]:

[formula]

where D now involves two parts, corresponding to the mean and variance term of the Gaussian distribution respectively. Now let us derive p̃(θ) for p(θ). In Laplace approximation, [formula] is chosen as the mode of p(θ), which is exactly the solution to the SVM2 optimization problem. For Σ, we have:

[formula]

Here we have overridden diag but letting it denote a diagonal matrix induced either by a square matrix or a vector; [formula] is the indicator function; (  ·  )- 1 denotes matrix inverse. Interestingly, the second term in Equation [\eqref=eq:int] is now equivalent to the squared Euclidean distance after performing element-wise normalizing the input using all difficult examples. The effect of this normalization is that the reconstruction errors of frequent words are down weighted; on the other hand, discriminative words are given higher weights as they would occur less frequently in difficult examples. Note that it is important to use a relatively large β in order to avoid the variance term dominating the mean term. In other words, we need to ensure p(θ) to be reasonable peaked around [formula] to effective take advantage of label information.

Experiments

Datasets

We evaluate our model on six Sentiment Analysis benchmarks. The first one is the IMDB dataset [\cite=mass], which consists of movie reviews collected from IMDB. The IMDB dataset is one of the largest sentiment analysis dataset that is publicly available; it also comes with an unlabeled set which allows us to evaluate semisupervised learning methods. The rest five datasets are all collected from Amazon [\cite=amazon], which corresponds to the reviews of five different products: books, DVDs, music, electronics, kitchenware. All the six datasets are already tokenized as either uni-gram or bi-gram features. For computational reasons, we only select the words that occur in at least 30 training examples. We summarize the statistics of datasets in Table [\ref=tb:datasets].

Methods

Bag of Words (BoW). Instead of using the raw word counts directly, we take a simple step of data normalization:

[formula]

where ci,j denotes the number of occurrences of the jth word in the ith document, xi,j denotes the normalized count. We choose this normalization because it preserves the sparsity of the Bag of Words features; also each feature element is normalized to the range

[formula]

Note that except for BoW and LrDrop, all the other methods require a predefined dimensionality of representation. We use fixed sizes on all the datasets. For SBDAE and NN, a small hidden size is sufficient, so we use 200. For DAE, we observe that it benefits from very large hidden sizes; however, due to computational constraints, we take 2000. For BoW, DAE, SBDAE, we use SVM2 as the classifier. All the models are trained with mini-batch Stochastic Gradient Descent with momentum of 0.9.

Results

We first summarize the results as in classification error rate in Table [\ref=tb:results]. First of all, our model consistently beats BoW with a margin, and it achieves the best results on four (larger) datasets out of six. On the other hand, DAE, DAE+ and NN all fail to outperform BoW, although they share the same architecture as nonlinear classifiers. This suggests that SBDAE be able to learn a much better nonlinear feature transformation function by training with a more informed objective (than that of DAE). Moreover, note also that finetuning on labeled set (DAE+) significantly improves the performance of DAE, which is ultimately on a par with training a neural net with random initialization (NN). However, finetuning offers little help to SBDAE, as it is already implicitly guided by labels during the training.

LrDrop is the second best method that we have tested. Thanks to the usage of dropout regularization, it consistently outperforms BoW, and achieves the best results on two (smaller) datasets. Compared with LrDrop, it appears that our model works better on large datasets (≈  10K words, more than 10K training examples) than smaller ones. This indicates that in high dimensional spaces with sufficient samples, SBDAE benefits from learning a nonlinear feature transformation that disentangles the underlying factors of variation, while LrDrop is incapable of doing so due to its nature as a linear classifier.

As the training of the autoencoder part of SBDAE does not require the availability of labels, we also try incorporating unlabeled data after learning the linear classifier in SBDAE. As shown in Table [\ref=tb:results], doing so further improves the performance over using labeled data only. This justifies that it is possible to bootstrap from a relatively small amount of labeled data and learn better representations with more unlabeled data with SBDAE.

To gain more insights of the results, we further visualize the filters learned by SBDAE and DAE on the IMDB dataset in Table [\ref=tb:topics]. In particular, we show the top 5 most activated and deactivated words of the first 8 filters (corresponding to the first 8 rows of W) of SBDAE and DAE, respectively. First of all, it seems very difficult to make sense of the filters of DAE as they are mostly common words with no clear co-occurrence pattern. By comparison, if we look at the filters from SBDAE, they are mostly sensitive to words that demonstrate clear polarity. In particular, all the 8 filters seem to be most activated by certain negative words, and are most deactivated by certain positive words. In this way, the activation of each filter of SBDAE is much more indicative of the polarity than that of DAE, which explains the better performance of SBDAE over DAE. Note that this difference only comes from reweighting the reconstruction errors in a certain way, with no explicit usage of labels.

Related Work and Discussion

Our work falls into the general category of learning representations for text data. In particular, there have been a lot of efforts that try to learn compact representations for either words or documents  [\cite=vsm] [\cite=lda] [\cite=lsa] [\cite=word2vec] [\cite=pv] [\cite=mass]. LDA [\cite=lda] explicitly learns a set of topics, each of which is defined as a distribution on words; a document is thus represented as the posterior distribution on topics, which is a fixed-length, non-negative vector. Closely related are matrix factorization models such as LSA [\cite=lsa] and Non-negative Matrix Factorization (NMF) [\cite=nmf]. While LSA factorizes the doc-term matrix via Singular Value Decomposition, NMF learns non-negative basis and coefficient vectors. Similar to these efforts, our model also works directly on the doc-term matrix. However, thanks to the usage of autoencoder, the representation for documents are calculated instantly via direct matrix product, which eliminates the need of expensive inference. Our work also distinguishes itself from other work as a semisupervised representation learning model, where label information can be effectively leveraged.

Recently, there has also been an active thread of research on learning word representations. Notably, [\cite=word2vec] shows that we can learn interesting word embeddings via very simple architecture on a large amount of unlabeled dataset. Moreover, [\cite=pv] proposed to jointly learn representations for sentences and paragraphs together with words in a similar unsupervised fashion. While our work does not explicitly model the representations for words, it is straightforward to incorporate this idea by adding an additional linear layer at the bottom of the autoencoder.

From the perspective of machine learning methodology, our approach resembles the idea of layer-wise pretraining in deep Neural Networks [\cite=bengio]. Our model differs from the traditional training procedure of autoencoders in that we effectively utilize the label information to guide the representation learning. Related idea has been proposed in [\cite=semi-ae], where they train Recursive autoencoders on sentences jointly with prediction of sentiment. Due to the delicate recursive architecture, their model only works on sentences with given parsing trees, and could not generalize to documents. MTC [\cite=mtc] is another work that models the interaction of autoencoders and classifiers. However, their training of autoencoders is purely unsupervised, the interaction comes into play by requiring the classifier to be invariant along the tangents of the learned data manifold. It is not difficult to see that the assumption of MTC would not hold when the class labels did not align well with the data manifold, which is a situation our model does not suffer from.

Conclusion

In this paper, we have proposed a novel extension to autoencoders for learning task-specific representations for textual data. We have generalized the traditional autoencoders by relaxing their loss function to the Bregman Divergence, and then derived a discriminative loss function from the label information. Experiments on text classification benchmarks have shown that our model significantly outperforms Bag of Words, traditional Denoising Autoencoder, and other competing methods. We have also qualitatively visualized that our model successfully learns discriminative features, which unsupervised methods fail to do.

Acknowledgments

This work is supported in part by NSF (CCF-1017828).