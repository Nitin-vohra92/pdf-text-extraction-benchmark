Boolean functions whose Fourier transform is concentrated on pairwise disjoint subsets of the input

Muli Safra

Introduction

Perhaps the simplest characteristic of functions is linearity , i.e. functions which are simply (weighted) sums of their variables. The set of linear Boolean functions is rather limited: the only linear Boolean functions are constant functions and dictatorships, i.e. functions that depend on only one variable.

Relaxing the notion of linearity, we say that a Boolean function [formula] is approximately linear if it can be approximated by an affine function of its variables, i.e. [formula]. Another, equivalent formulation of approximately linear asserts that f's Fourier coefficients are concentrated on the 1-st and 0-th levels, i.e. [formula]. For example, from the latter definition it is not hard to see that such functions in particular have low noise sensitivity. Informally, low noise sensitivity means that adding a small random perturbation to the input x, is unlikely to change the value of [formula].

A theorem of Friedgut, Kalai, Naor proves that those approximately linear functions have a unique structure: they are approximated by dictatorships. Intuitively, one may expect such results to be true, because a linear combination that is "well-spread" among many independent variables (i.e. far from dictatorship of one variable) should be distributed similarly to a "bell-curved" Gaussian; in particular, it should be far from the [formula] distribution of a Boolean function which is bimodal, i.e. has two distinct modes or "peaks" at - 1 and + 1.

The long code and related works

One of the most important historical driving forces in the study of Boolean functions has been their applications to testing of error correcting codes [\cite=Bou]. In particular, the long code [\cite=Long_code] can be viewed as evaluations of dictatorships functions. Each codeword in the long code corresponds to the evaluation of a dictatorship [formula] on all the points on the n-dimensional Boolean hypercube. Indeed, the long code is highly inefficient - since there are only n possible dictatorships, the long code encodes log n bits of information in a 2n-bit codeword. Despite its low rate, the long code is an important tool in many results on hardness of approximation and probabilistically checkable proofs (such as [\cite=Long_code] [\cite=long_code_app] [\cite=long_code_app_2] [\cite=KS_thesis] [\cite=Fri_app_NP_hardness_VC] [\cite=Bou_app_UG_hardness_Cuts_embeddability] [\cite=FKN_app_dinur_PCP] [\cite=Fri_app_hardness_MultiCut] [\cite=Fri_app_UG_hardness_VC] [\cite=Fri_app_UG_hardness_2IS]).

The great virtue of the long code is that it is a locally testable code: It is possible to distinguish, with high probability, between a legal codeword and a string that is far from any legal codeword, by querying just a few random bits of the string. Naturally, this is a highly desirable property when constructing probabilistically checkable proofs, which are proofs that must be verified by reading a few random bits of the proof. Using local queries, it is possible to estimate whether a Boolean function is approximately linear. These properties can be used by long-code testers [\cite=FKN_app_dinur_PCP] together with the FKN Theorem described above.

Our results

In this work extend the intuition from the FKN Theorem, that a well-spread sum of independent variables must be far from Boolean. In particular we ask the following questions:

What happens when the variables are not uniformly distributed over [formula]? In particular, we consider variables which are not even Boolean or symmetric.

In a social choice setting, it may be intuitive to consider a mechanism that takes into account how strong is each voter's preference. For example, in some countries the elections are known to be highly influenced by the donations the candidates manage to collect ("argentocracy").

In the context of computational complexity, Boolean analysis theorems that consider non-uniform distributions have proven very useful. In particular, Dinur and Safra use the p-biased long code in their proof of NP-hardness of approximation of the Vertex Cover problem [\cite=Fri_app_NP_hardness_VC]. In the p-biased long code each codeword corresponds to a dictatorship, in a population where each voter independently chooses - 1 with probability [formula] and + 1 with probability 1 - p. An extension of Friedgut's Junta Lemma [\cite=Fri98] to such non-uniform product distributions was key to Dinur and Safra's proof.

In this work we prove that even when the variables are not uniformly distributed over [formula], every Boolean function that is close to their sum must be close to one of them:

What happens when rather than a sum of variables, we have a sum of functions over disjoint subsets of Boolean variables?

In a social choice setting, it may be intuitive to consider a situation where the population is divided into tribes; each tribe has an arbitrarily complex internal mechanism, but the outcomes of all the tribes must be aggregated into one communal decision by a simple (i.e. almost linear) mechanism.

Furthermore, this theorem may lead to interesting applications in computational theory settings where such special structures arise. In fact, this was our original motivation for this work.

Observe that this question is tightly related to the previous question because each arbitrary function over a subset of Boolean variables can be viewed as an arbitrarily-distributed random variable.

In this work we prove that any balanced function that is close to a sum of functions over disjoint subsets of its variables is almost completely determined by a function on a single subset:

As we will see later, the precise statement of the FKN Theorem does not require the Boolean function to be balanced. If we do not require the function to be balanced, there is an obvious exception to the theorem - constant functions, [formula] and [formula], are not dictatorships but are considered linear. The general statement of the FKN Theorem says that a Boolean function that is almost linear is either almost a dictatorship or almost a constant function. More precisely, it says that the distance of any Boolean function from the nearest linear (not necessarily Boolean) function is smaller by at most a constant multiplicative factor than the distance from either a dictatorship or a constant function.

One may hope to extend this relaxation to non-Boolean random variables or subsets of Boolean random variables. E.g. we would like to claim that the distance of any Boolean function from a sum of functions on mutually exclusive subsets of the variables is at most the distance from a function on a single subset or a constant function. However, it turns out that this is not the case - in Lemma [\ref=tightness_of_main_result-1] we show that this naive extension of the FKN Theorem is false!

The variance of a Boolean function measures how far it is from a constant (either - 1 or 1). For example, the variance of any balanced Boolean function is 1, whereas any constant function has a variance of 0. In order to extend our results to non-balanced Boolean functions, we have to correct for the low variance. In Theorem [\ref=sum_of_sequence] and Corollary [\ref=fkn-like] we prove that the above two theorems extend to non-balanced functions relatively to the variance:

Intuitively these amendments to our main theorems mean that in order to prove that a Boolean function is close to a dictatorship, we must show that it is very close to linear.

Finally, in Lemma [\ref=tightness_of_main_result-1] we show that this dependence on the variance is necessary and tight.

Hypercontractivity

Many theorems about Boolean functions rely on hypercontractivity theorems such as the Bonami-Beckner Inequality ([\cite=Bonami] [\cite=Beckner]). Writing a real-valued function over [formula] as a polynomial yields a distribution over the monomials' degrees [formula], where the weight of k is the sum of relative weights of monomials of degree k. Hypercontractivity inequalities bound the ratios between norms of real-valued functions over [formula] in terms of this distribution of weights over their monomials' degrees. In this work it is not clear how to use such inequalities because the functions in question may have an arbitrary weight on high degrees within each subset.

All of the proofs presented in this work are completely self-contained and based on elementary methods. In particular, we do not use any hypercontractivity theorem. This simplicity makes our work more accessible and intuitive. This trend is exhibited by some recent related works, e.g. [\cite=Sums_of_ind_symmetric_variables] [\cite=Sums_of_iid_variables] [\cite=Bou_for_Gaussian_variables], that also present proofs that do not use hypercontractivity.

Organization

We begin with some preliminaries in Section [\ref=sec:Preliminaries]. In Section [\ref=sec:Related-Works] we give a brief survey of related works. In Section [\ref=sec:Our-Main-Results] we formally state our results. In Section [\ref=sec:High-Level-Outline] we give an intuitive sketch of the proof strategy. The interesting ingredients of the proof appear in Section [\ref=sec:Proofs], whereas some of the more tedious case analyses are postponed to Section [\ref=sec:Proofs-of-technical]. Tightness for some of the results is shown in Section [\ref=sec:Tightness-of-results]. Finally, in Section [\ref=sec:Conjectures-and-extensions] we make some concluding comments and discuss possible extensions.

Preliminaries

L2-squared semi-metric

Throughout the paper, we define "closeness" of random variables using the squared L2-norm:

[formula]

It is important to note that this is a semi-metric as it does not satisfy the triangle inequality. Instead, we will use the 2-relaxed triangle inequality:

[formula]

[formula]

Although it is not a metric, the squared L2-norm has some advantages when analyzing Boolean functions. In particular, when comparing two Boolean functions, the squared L2-norm does satisfy the triangle inequality because it is simply four times the Hamming distance, and also twice the L1-norm ("Manhattan distance"): [formula].

Additionally, the squared L2-norm behaves "nicely" with respect to the Fourier transform:

[formula]

(Proofs of Facts [\ref=fac:l2_norm-ft]-[\ref=fac:var-min_l2_norm] are standard and are included in the appendix for completeness.)

Variance

The variance of random variable X is defined as

[formula]

Observe that for a function f the variance can also be defined in terms of its Fourier coefficients,

[formula]

Another useful way to define the variance is the expected squared distance between two random evaluations:

For any random variable X,

[formula]

We can also view the variance as the L2-squared semidistance from the expectation

[formula]

Recall also that the expectation [formula] minimizes this semi-distance [formula]:

[formula]

Finally, for any two functions f,g that are closed in L2-squared semimetric, we can use the 2-relaxed triangle inequality (Fact [\ref=fac:_relaxed_triangle_ineq]) to bound the difference in variance:

[formula]

[formula]

Related Work

In their seminal paper [\cite=FKN], Friedgut, Kalai, and Naor prove that if a Boolean function is ε-close to linear, then it must be [formula]-close to a dictatorship or a constant function. The FKN Theorem quickly found applications in social choice theory [\cite=Kalai_social]. More importantly, it has since been applied in other fields; a good example is Dinur's combinatorial proof of the PCP theorem [\cite=FKN_app_dinur_PCP].

There are also many works on generalizations on the FKN Theorem. Alon et al. [\cite=FKN_for_Z_r_1] and Ghandehari and Hatami [\cite=FKN_for_Z_r_2] prove generalizations for functions with domain [formula] for [formula]. Friedgut [\cite=FKN_for_any_deg_and_non-uniform_measure] proves a similar theorem that also holds for Boolean functions of higher degrees and over non-uniform distributions; however, this theorem requires bounds on the expectation of the Boolean function. In [\cite=FKN_quantum], Montanaro and Osborne prove quantum variants of the FKN Theorem for any "quantum Boolean functions", i.e. any unitary operator f such that f2 is the identity operator. Falik and Friedgut [\cite=FKN_for_representation_theory] and Ellis et al. [\cite=EFF-FKN_balanced_Sn] [\cite=EFF-FKN_Sn] prove representation-theory variants of the FKN Theorem, for functions which are close to a linear combination of an irreducible representation of elements of the symmetric group.

The FKN Theorem is an easy corollary once the following proposition is proven: If the absolute value of the linear combination of Boolean variables has a small variance, then it must be concentrated on a single variable. Formally, Intuitively, this proposition says that if the variance was spread among many of the variables, i.e. the "weights" [formula] were somewhat evenly distributed, then one would expect the sum of such independent variables to be closer to a Gaussian rather than a bimodal distribution around [formula].

This proposition has been generalized in several ways in a sequence of recent works by Wojtaszczyk [\cite=Sums_of_ind_symmetric_variables] and Jendrej, Oleszkiewicz, and Wojtaszczyk [\cite=Sums_of_iid_variables], which are of particular interest to us. Jendrej et al. prove extensions of the FKN Proposition to the following cases:

The case where Xi's are independent symmetric

The case where all the Xi's are identically distributed

Let us note that the works of Wojtaszczyk [\cite=Sums_of_ind_symmetric_variables] and Jendrej, Oleszkiewicz, and Wojtaszczyk [\cite=Sums_of_iid_variables] have been eventually extended and transformed into [\cite=Krzysztof_article], which is conditionally accepted Theory of Computing.

The extension [\cite=Krzysztof_article] -carried out independently of our work- has resulted in a theorem which is our Theorem [\ref=sum_of_sequence], however for the case of bounded-variance variables.

Furthermore, it is worthwhile noting that the proof there could be amended so as to achieve Theorem [\ref=sum_of_sequence], and can thus be considered as an alternative technique for such purposes.

Following the work of Jendrej et al. and the announcement of our results, Nayar [\cite=Nayar13_biased-FKN] proved a variant of the FKN Theorem for the biased hypercube, which builds on ideas from [\cite=Krzysztof_article].

Formal Statement of Our Results

In this work we consider the following relaxation of linearity in the premise of the FKN Theorem: Given a partition of the variables [formula] and a function fj (not necessarily Boolean or symmetric) on the variables in each subset, we look at the premise that the Boolean function f is close to a linear combination of the fj's. Our main result (Corollary [\ref=fkn-like]) states, loosely, that any such f must be close to being completely dictated by its restriction fk to the variables in a single subset Ik of the partition.

While making a natural generalization of the well-known FKN Theorem, our work also has a surprising side: In the FKN Theorem and similar results, if a function is ε-close to linear then it is [formula]-close to a dictatorship, for some constant K. We prove that while this is true in the partition case for balanced functions, it does not hold in general. In particular, we require f to be [formula]-close to linear in the fj's, in order to prove that it is only [formula]-close to being dictated by some fk. We show (Lemma [\ref=tightness_of_main_result-1]) that this dependence on [formula] is tight.

Our first result is a somewhat technical theorem, generalizing the FKN Proposition. We consider the sum [formula] of a sequence of independent random variables. In particular, we do not assume that the variables are Boolean, symmetric, balanced, or identically distributed. Our main technical theorem, which generalizes the FKN Proposition, states that if this sum does not "behave like" any single variable Xk, then it is also far from Boolean. In other words, if a sum of independent random variables is close to a Boolean function then most of its variance comes from only one variable.

We show that [formula] is far from Boolean by proving a lower bound on the variance of its absolute value, [formula]. Note that for any Boolean function f, [formula] everywhere, and thus [formula]. Therefore the lower bound on [formula] is in fact also a lower bound on the (semi-)distance from the nearest Boolean function:

[formula]

By saying that the sum [formula] "behaves like" a single variable Xk, we mean that their difference is almost a constant function; i.e. that

[formula]

is small.

Furthermore, the definition of "small" depends on the expectation and variance of the sum of the sequence, which we denote by E and V

[formula]

Formally, our main technical theorem states that

Let [formula] be a sequence of independent (not necessarily symmetric) random variables, and let E and V be the expectation and variance of their sum, respectively. Then for some universal constant [formula] we have that there exists [formula] such that,

[formula]

[formula]

The main motivation for proving this theorem is that it implies a generalization of the FKN Theorem, Corollary [\ref=fkn-like] below.

Intuitively, while the FKN Theorem holds for Boolean functions that are almost linear in individual variables, we generalize it to functions that are almost linear with respect to a partition of the variables.

Formally, let [formula] and let [formula] be a partition of [formula]; denote by fj the restriction of f to each subset of variables:

[formula]

Our main corollary states that if f behaves like the sum of the fj's then it behaves like some single fk:

Let f, Ij's, and fj's be as defined above. Suppose that f is concentrated on coefficients that do not cross the partition, i.e.:

[formula]

Then for some [formula], f is close to [formula]:

[formula]

[formula]

In particular, notice that it implies that f is concentrated on the variables in a single subset Ik.

Unlike the FKN Theorem and many similar statements, it does not suffice to assume that f is ε-close to linear. Our main results require a dependence on the variance of f. We prove in Section [\ref=sub:Tightness-of-the-main] that this dependence is tight up to a constant factor by constructing an example for which [formula] and f is [formula]-close to linear with respect to a partition, but f is still [formula]-far from being dictated by any subset.

Corollary [\ref=fkn-like] is tight up to a constant factor. In particular, the factor [formula] is necessary.

More precisely, there exists a series of functions [formula] and partitions [formula] such that the restrictions [formula] of [formula] to variables in [formula] satisfy

[formula]

but for every [formula]

[formula]

From our results to the FKN Theorem

We claim that our results generalize the FKN Theorem. For a constant variance, the FKN Theorem indeed follows immediately from Corollary [\ref=fkn-like] (for some worse constant [formula]). However, because the premise of Corollary [\ref=fkn-like] depends on the variance, it may not be obvious how to obtain the FKN Theorem for the general case, where the variance may go to zero. Nonetheless, we note that thanks to an observation by Guy Kindler [\cite=FKN] the FKN Theorem follows easily once the special case of balanced functions is proven.

Given a Boolean function [formula], we define a balanced Boolean function [formula] that will be as close to linear as f,

[formula]

First, notice that g is indeed balanced because: where the second equality holds because under uniform distribution taking the expectation over X is the same as taking the expectation over - X.

Observe also that every monomial [formula] in the Fourier representation of [formula] is multiplied by [formula] in the Fourier transform of [formula]. (The [formula] in the exponent comes from [formula] for all the variables that appear in the monomial, and another 1 for the xn + 1 outside the function). Since [formula], for odd [formula] we have that [formula], and the monomial does not change, i.e. [formula]; for even [formula], [formula], so [formula]. In particular, the total weight on the first and zeroth level of the Fourier representation is preserved because

If f satisfies the premise for the FKN Theorem, i.e. if [formula], then from [\eqref=eq:guys_trick] it is clear that the same also holds for g. From the FKN Theorem for the balanced special case we deduce that g is [formula]-close to a dictatorship, i.e. there exists [formula] such that [formula]. Therefore by [\eqref=eq:guys_trick] f is also [formula]-close to either a dictatorship (when [formula]) or a constant function (when k = n + 1). The FKN Theorem for balanced functions follows as a special case of our main results, and therefore this work also provides an alternative proof for the FKN Theorem.

High-Level Outline of the Proof

The main step to proving Theorem [\ref=sum_of_sequence] for a sequence of n variables, is Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY] below which handles the special case of only two random variables. The main theorem then follows by partitioning the n variables into two subsets, and labeling their sums X and Y, respectively (Subsection [\ref=sub:main-thm]).

Let X,Y be any two independent random variables, and let E and V be the expectation and variance of their sum, respectively. Then for some universal constant [formula],

[formula]

[formula]

Intuitively, in the expression on the left-hand side of [\eqref=eq:_lemma_statement] we consider the sum of two independent variables, which we may expect to variate more than each variable separately. Per contra, the same side of [\eqref=eq:_lemma_statement] also has the variance of the absolute value, which is in general smaller than just the variance (without absolute value). Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY] bounds this loss of variance.

On a high level, the main idea of the proof of Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY] is separation to two cases, depending on the variance of the absolute value of the random variables, relative to the original variance of the variables (without absolute value):

If both [formula] and [formula] have relatively small variance, then [formula] and [formula] can be both approximated by random variables with constant absolute values. In this case we prove the result by case analysis.

If either [formula] or [formula] has a relatively large variance, we prove an auxiliary lemma that states that the variance of the absolute value of the sum, [formula], is not much smaller than the variance of the absolute value of either variable ([formula], [formula]):

Let X,Y be any two independent random variables, and let E be the expectation of their sum. Then for some universal constant [formula],

[formula]

[formula]

Note that in this lemma, unlike the former statements discussed so far, the terms on the right-hand side also appear in absolute value. In particular, this makes the inequality hold with respect to the maximum of the two variances.

We find it of separate interest to note that this lemma is tight in the sense that it is necessary to take a non-trivial constant K0 > 1:

A non-trivial constant is necessary for Lemma [\ref=lemma:_|X+Y|_>_K*|X+E=00005BY=00005D|]. More precisely, there exist two independent balanced random variables [formula], such that the following inequality does not hold for any value K0 < 4 / 3:

[formula]

In particular, it is interesting to note that K0 > 1.

Discussion and proof appear in Section [\ref=sub:Tightness-of-the-aux_lemma].

[formula]

Proofs

From variance of absolute value to variance of absolute value of sum: proof of Lemma [\ref=lemma:_|X+Y|_>_K*|X+E=00005BY=00005D|]

We begin with the proof of a slightly more general form of Lemma [\ref=lemma:_|X+Y|_>_K*|X+E=00005BY=00005D|]:

Let [formula] be any two independent balanced random variables, and let E be any real number. Then for some universal constant [formula],

[formula]

Lemma [\ref=lemma:_|X+Y|_>_K*|X+E=00005BY=00005D|] follows easily by taking [formula] and [formula] and [formula].

This lemma is relatively easy to prove partly because the right-hand side contains the maximum of the two variances. Thus, it suffices to prove separately that the left-hand side is greater or equal to [formula] and to [formula]. Without loss of generality we will prove:

[formula]

Separating into two inequalities is particularly helpful, because now [formula] no longer appears in the right-hand side.

Our next step is to reduce to the special case where [formula] is a balanced variable with only two values in its support. Every balanced variable can be written as a mixture of balanced random variables [formula], each with support at most two; this follows by applying the Krein-Milman theorem to the space of balanced random variables. Now use the convexity of the variance to get:

[formula]

Thus [formula] is in particular greater or equal to [formula] for some α. Therefore in order to prove Lemma [\ref=lemma:_var|X+Y+E|_>_var|X+E|], it suffices to prove the lower bound [formula] (with respect to [formula]) for every balanced [formula] with only two possible values.

Recall (Fact [\ref=fac:_(z1-z2)]) that we can express the variances of [formula] and [formula] in terms of the expected squared distance between two random evaluations. We use a simple case analysis to prove that adding any balanced [formula] with support of size two preserves (up to a factor of [formula]) the expected squared distance between any two possible evaluations of [formula].

For every two possible evaluations x1,x2 in the support of [formula],

[formula]

The proof appears in Section [\ref=cla:For-every_x] [\ref=x] [\ref=y] [\ref=y].

[formula]

Finally, in order to achieve the bound on the variances (inequality [\eqref=eq:_var|X+Y+E|_>_var|X+E|]), take the expectation over all choices of [formula]:

[formula]

(Where the two equalities follow by Fact [\ref=fac:_(z1-z2)], and the inequality by Claim [\ref=cla:For-every_x] [\ref=x] [\ref=y] [\ref=y].)

From variance of absolute value of sum to variance: proof of Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY]

We advance to the more interesting Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY], where we bound the variance of [formula] with respect to the minimum of [formula] and [formula]. Intuitively, in the expression on the left-hand side of [\eqref=eq:var|x+y|_>_k_varx] [\eqref=vary] we consider the sum of two independent variables, which we may expect to variate more than each variable separately. Per contra, the same side of [\eqref=eq:var|x+y|_>_k_varx] [\eqref=vary] also has the variance of the absolute value, which is in general smaller than just the variance (without absolute value). We will now bound this loss of variance.

We change variables by subtracting the expectation of X and Y,

[formula]

Note that the new variables [formula] are balanced. Also observe that we are now interested in showing a lower bound for

[formula]

On a high level, the main idea of the proof is separation to two cases, depending on the variance of the absolute value of the random variables, relative to the original variance of the variables (without absolute value):

If either [formula] or [formula] has a relatively large variance, we can simply apply Lemma [\ref=lemma:_var|X+Y+E|_>_var|X+E|] that states that the variance of the absolute value of the sum, [formula], is not much smaller than the variance of the absolute value of either variable ([formula] and [formula]).

If both [formula] and [formula] have relatively small variance, then [formula] and [formula] can be both approximated by random variables with constant absolute values, (i.e. random variables with supports [formula] and [formula] for some reals dX and dY, respectively). For this case we prove the result by case analysis.

Formally, let 0 < a < 1 / 10 be some parameter to be determined later, and denote

[formula]

If either of the variances of the absolute values is large, i.e.

[formula]

then we can simply apply Lemma [\ref=lemma:_var|X+Y+E|_>_var|X+E|] to obtain:

[formula]

On the other hand, if both the variances of the absolute values are small, i.e.

[formula]

then [formula] and [formula] are almost constant in absolute value.

In particular, let the variables X' and Y' be the constant-absolute-value approximations to [formula] and [formula], respectively:

[formula]

From the precondition [\eqref=var|X+E|_<_var_X] it follows that [formula] and [formula] are close to X' and Y', respectively:

[formula]

In particular, by the 2-relaxed triangle inequality (Facts [\ref=fac:_relaxed_triangle_ineq] and [\ref=fac:_L2_norm_and_var]) we have that the following variances are close:

[formula]

Hence, it will be useful to obtain a bound equivalent to [\eqref=eq:var|x+y|_>_k_varx] [\eqref=vary], but in terms of the approximating variables, X',Y'. We will then use the similarity of the variances to extend the bound to [formula] and complete the proof of the lemma.

We use case analysis over the possible evaluations of X' and Y' to prove the following claim:

Let [formula] be balanced random variables and let X',Y' be the constant-absolute-value approximations of [formula], respectively:

[formula]

Then the variance of the absolute value of X' + Y' - E is bounded by:

[formula]

The proof appears in Section [\ref=sub:Proof-of-analisys_for_const_|_|].[formula]

Now we use the closeness of the approximating variables X',Y' to recover a bound for the balanced variables [formula]:

[formula]

(Where the first line follows by equation [\eqref=eq:var_x+y_is_close]; the second line from Claim [\ref=cla:_const_|_|]; the third from [\eqref=eq:var_x_is_close] and [\eqref=eq:var_y_is_close]; the fourth from the definition of MXY [\eqref=eq:M_XY-def]; and the fifth is true because [formula] and [formula].)

Combining the two cases, we have that

[formula]

Finally, we set [formula]. Then,

[formula]

and thus [\eqref=eq:_lemma_statement] holds for [formula].

Proof of the main theorem

Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY] bounds the variance of the absolute value of the sum of two independent variables, [formula], in terms of the variance of each variable. In the following theorem we generalize this claim to a sequence of n independent variables.

In order to generalize Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY] to n variables, consider the two possible cases:

If for every i, [formula], then we can partition [formula] into two sets A,B such that

[formula]

(If [formula], we can iteratively add variables to A until [\eqref=eq:13<V<23] is true; if [formula] for some i, we can simply take [formula].) Thus, substituting [formula] and [formula] in Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY], we have that for every k

[formula]

Otherwise, if [formula], apply Lemma [\ref=lemma:_Var|X+Y+c|_>_K_*_VarX_*_VarY] with X = Xk and [formula] to get:

[formula]

The theorem follows for K2 = 3K1.

Proof of the extension to FKN Theorem

Corollary [\ref=fkn-like], the generalization of the FKN Theorem, follows easily from Theorem [\ref=sum_of_sequence].

From the premise it follows that f is [formula]-close to the sum of the fj's and the empty character:

[formula]

Since f is Boolean, this implies in particular that

[formula]

Thus by the main theorem, for some [formula]

[formula]

Rearranging and using [formula], we have

[formula]

From the premise, we have [formula], and therefore

[formula]

Finally, we can assume without loss of generality that [formula], and thus Plugging back into [\eqref=eq:k2-div-(1-epsilon)], we get:

[formula]

Finally,

[formula]

Proofs of technical claims

Expected squared distance: proof of Claim [\ref=cla:For-every_x] [\ref=x] [\ref=y] [\ref=y]

We use case analysis to prove that adding any balanced [formula] with support of size two preserves (up to a factor of [formula]) the expected squared distance between any two possible evaluations of [formula].

(Claim [\ref=cla:For-every_x] [\ref=x] [\ref=y] [\ref=y]) For every two possible evaluations x1,x2 in the support of [formula],

[formula]

Denote

[formula]

Because [formula] is balanced, its two possible values must be of the form [formula], for some [formula]. Assume without loss of generality that [formula] and [formula].

We divide our analysis to cases based on the value of pY (see also Figure [\ref=fig:4_pt_argument]):

If [formula] then with probability at least [formula] both evaluations of [formula] are non-negative, in which case the distance between [formula] and [formula] can only increase:

[formula]

If [formula] then with probability at least [formula], y1 is non-negative; we also use [formula] implies [formula]:

[formula]

If [formula] and [formula], we can prove the claim by focusing on the case [formula]:

[formula]

Notice that [formula] implies that

[formula]

Furthermore, since [formula] and [formula], we have that

[formula]

Plugging back into [\eqref=eq:Pr=00005By1>0] [\eqref=y2<0=00005D] we have

[formula]

Else, if [formula] and [formula], we need to sum over the possible signs of y1,y2, and use the fact that [formula] is much larger than [formula]:

[formula]

where we used the condition [formula] in the third line.

We have

[formula]

and Therefore,

[formula]

Constant absolute value: proof of Claim [\ref=cla:_const_|_|]

We use a brute-force case analysis to prove a relative lower bound on the variance in absolute value of a sum of two variables with constant absolute values:

(Claim [\ref=cla:_const_|_|]) Let [formula] be balanced random variables and let X',Y' be the constant-absolute-value approximations of [formula], respectively:

[formula]

Then the variance of the absolute value of X' + Y' - E is bounded by:

[formula]

Denote [formula] and [formula] (and analogously for pY,dY).

Observe that

[formula]

Thus we can bound [formula] from below by:

[formula]

[formula]

Also, for Y' we have

[formula]

Assume without loss of generality that dY < dX and E > 0. Recall (Fact [\ref=fac:_(z1-z2)]) that we can write the variance in terms of the expected squared distance between evaluations. Then, summing over the different possible signs of X' and Y' we have

[formula]

(Where the first inequality follows by taking the expectation over the different possible signs of X', Y' (see also Figure [\ref=fig:const_abs_value]); the second follows by taking the minimum over the possible signs of the quantities in absolute values;)

We next claim that

[formula]

If 2E  ≥  dY, [\eqref=eq:(a)+(b)>dY] is immediate. If 2E < dY, then [\eqref=eq:(a)+(b)>dY] follows because 2dX - 2E  ≥  dx  ≥  dY.

Therefore,

[formula]

(Where the first inequality follows from [\eqref=eq:(a)+(b)>dY] and [formula]; and the second inequality follows by [\eqref=eq:_(1-px)_*_px_lower_bound].)

Tightness of results

Tightness of the main result

The premise of main result, corollary [\ref=fkn-like], requires f to be [formula]-close to a sum of independent functions. One may hope to avoid this factor of [formula] and achieve a constant ratio between ε in the premise and [formula] in the conclusion, as in the FKN Theorem. However, we show that the dependence on [formula] is necessary.

By example. Let

[formula]

where we think of - 1 as "true" and 1 as "false".

The variance of f is [formula]: Also, f is [formula]-close to a sum of independent functions:

[formula]

Yet, f is [formula]-far from any function that depends on either only the xi's or only the yi's.

Tightness of Lemma [\ref=lemma:_|X+Y|_>_K*|X+E=00005BY=00005D|]

Lemma [\ref=lemma:_|X+Y|_>_K*|X+E=00005BY=00005D|] compares the variance of the absolute value of a sum of independent variables, to the variance of the absolute value of each variable. Since both sides of the inequality consider absolute values, it may seem as if we should only be increasing the variation on the left side by summing independent variables. In particular, one may hope that the inequality should hold trivially, with K0 = 1. We show that this is not the case.

(Claim [\ref=cla:_tightness_of_aux_lemma]) A non-trivial constant is necessary for Lemma [\ref=lemma:_|X+Y|_>_K*|X+E=00005BY=00005D|]. More precisely, there exist two independent balanced random variables [formula], such that the following inequality does not hold for any value K0 < 4 / 3:

[formula]

(In particular, it is interesting to note that K0 > 1.)

By example. Let

[formula]

Then we have that

[formula]

[formula]

and therefore

[formula]

Conjectures and extensions

While the dependence on the variance in Corollary [\ref=fkn-like] is tight, it seems counter-intuitive. We believe that it is possible to come up with a structural characterization instead.

Observe that the function used for the counter example in Lemma [\ref=tightness_of_main_result-1] is essentially the (non-balanced) tribes function, i.e. OR of two AND's. All the extreme examples we have discovered so far have a similar structure of an independent Boolean function on each subset of the variables (e.g. AND on a subset of the variables), and then a "central" Boolean function that takes as inputs the outputs of the independent functions (e.g. OR of all the AND's).

We conjecture that such a composition of Boolean functions is essentially the only way to construct counterexamples to the "naive extension" of the FKN Theorem. In other words, if a Boolean function is close to linear with respect to a partition of the variables, then it is close to the application of a central Boolean function g on the outputs of independent Boolean functions gj's, one over each subset Ij. Formally,

Let [formula] be a Boolean function, [formula] a partition of [formula]. Suppose that f is concentrated on coefficients that do not cross the partition, i.e.:

[formula]

Then there exist a "central" Boolean function [formula] and a Boolean function on each subset [formula] such that the composition of g with the hj's is a good approximation of f. I.e. for some universal constant K,

[formula]

Intuitively, this conjecture claims that the central function only needs to know one bit of information on each subset in order to approximate f.

We believe that such a conjecture could have useful applications because one can often deduce properties of the composition of independent functions [formula] from the properties of the composed functions g and h. For example if f, g, and h are as above, then the total influence of f is the product of the total influences of g and h.

In fact, we believe that an even stronger claim holds. It seems that for all the Boolean functions that are almost linear with respect to a partition of the variables, the "central" function g is either an OR or an AND of some of the functions on the subsets hj. Formally,

(Stronger variant) Let [formula] be a Boolean function, [formula] a partition of [formula]. Suppose that f is concentrated on coefficients that do not cross the partition, i.e.:

[formula]

Then there exist Boolean functions [formula] for each [formula] such that either the OR or the AND of those hj's is a good approximation of f. I.e. for some universal constant K,

Proofs of preliminary facts

Below, we bring missing proofs of facts from Section [\ref=sec:Preliminaries]. All these proofs can be found elsewhere (e.g. [\cite=ODonnell14-book]), and are brought here only for completeness.

(Fact [\ref=fac:l2_norm-ft])

[formula]

[formula]

(Fact [\ref=fac:var-ft])

[formula]

[formula]

(Fact [\ref=fac:_(z1-z2)]) For any random variable X,

[formula]

[formula]

(Fact [\ref=fac:var-l2_norm])

[formula]

[formula]

(Fact [\ref=fac:var-min_l2_norm])

[formula]

Differentiate twice with respect to E:

[formula]