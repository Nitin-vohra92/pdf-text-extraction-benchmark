>m >m >m

=1

=100000

Pose for Action - Action for Pose

Introduction

Human pose estimation from RGB images or videos is a challenging problem in computer vision, especially for realistic and unconstrained data taken from the Internet. Popular approaches for pose estimation [\cite=Wang_CVPR2013] [\cite=eichner_accv2012] [\cite=desai_eccv2012] [\cite=pishchulin_cvpr2013] [\cite=pishchulin2013strong] [\cite=yang_tpami2014] [\cite=dantone_tpami2014] [\cite=cherian_cvpr2014] [\cite=tompson2014joint] adopt the pictorial structure (PS) model, which resembles the human skeleton and allows for efficient inference in case of tree structures [\cite=Felzenszwalb_ijcv2005] [\cite=felzenszwalb_tpami2010]. Even if they are trained discriminatively, PS models struggle to cope with the large variation of human pose and appearance. This problem can be addressed by conditioning the PS model on additional observations from the image. For instance, [\cite=pishchulin_cvpr2013] and [\cite=vela_bmvc2014] detect poselets, which are examples of consistent appearance and body part configurations, and condition the PS model on these.

Instead of conditioning the PS model on predicted configurations of body parts from an image, we propose to condition the PS model on high-level information like activity. Intuitively, the information about the activity of a person can provide a strong cue about the pose and vice versa the activity can be estimated from pose. There have been only few works [\cite=yao_ijcv2012] [\cite=Yu_cvpr2013] [\cite=bruce_cvpr2015] that couple action recognition and pose estimation to improve pose estimation. In [\cite=yao_ijcv2012], action class confidences are used to initialize an optimization scheme for estimating the parameters of a subject-specific 3D human model in indoor multi-view scenarios. In [\cite=Yu_cvpr2013], a database of 3D poses is used to learn a cross-modality regression forest that predicts the 3D poses from a sequence of 2D poses, which are estimated by [\cite=yang_tpami2014]. In addition, the action is detected and the 3D poses corresponding to the predicted action are used to refine the pose. However, both approaches cannot be applied to unconstrained monocular videos. While [\cite=yao_ijcv2012] requires a subject-specific model and several views, [\cite=Yu_cvpr2013] requires 3D pose data for training. More recently, [\cite=bruce_cvpr2015] proposed an approach to jointly estimate action classes and refine human poses. The approach decomposes the human poses estimated at each video frame into sub-parts and tracks these sub-parts across time according to the parameters learned for each action. The action class and joint locations corresponding to the best part-tracks are selected as estimates for the action class and poses. The estimation of activities, however, comes at high computational cost since the videos are pre-processed by several approaches, one for pose estimation [\cite=park_iccv2011] and two for extracting action related features [\cite=wang:2011] [\cite=wang2014cross].

In this work, we present a framework for pose estimation that infers and integrates activities with a very small computational overhead compared to an approach that estimates the pose only. This is achieved by an action conditioned pictorial structure (ACPS) model for 2D human pose estimation that incorporates priors over activities. The framework of the approach is illustrated in Figure [\ref=fig:overview]. We first infer the poses for each frame with a uniform distribution over actions. While the binaries of the ACPS are modeled by Gaussian mixture models, which depend on the prior distribution over the action classes, the unaries of the ACPS model are estimated by action conditioned regression forests. To this end, we modify the approach [\cite=dantone_tpami2014], which consists of two layers of random forests, on two counts. Firstly, we replace the first layer by a convolutional network and use convolutional channel features to train the second layer, which consists of regression forests. Secondly, we condition the regression forests on a distribution over actions and learn the sharing among action classes. In our experiments, we show that these modifications increase the pose estimation accuracy by more than 40% compared to [\cite=dantone_tpami2014]. After the poses are inferred with a uniform distribution over actions, we update the action prior and the ACPS model based on the inferred poses to obtain the final pose estimates. Since the update procedure is very efficient, we avoid the computational expensive overhead of [\cite=bruce_cvpr2015].

We evaluate our approach on the challenging J-HMDB [\cite=Jhuang_iccv2013] and Penn-Action [\cite=zhang2013actemes] datasets, which consist of videos collected from the Internet and contain large amount of scale and appearance variations. In our experiments, we provide a detailed analysis of the impact of conditioning unaries and binaries on a distribution over actions and the benefit of appearance sharing among action classes. Our approach achieves state-of-the-art pose estimation and action recognition performance on both datasets. Compared to [\cite=bruce_cvpr2015], the pose estimation accuracy is improved by over 30%.

Related Work

State-of-the-art approaches for pose estimation are mostly based on neural networks [\cite=jain2014learning] [\cite=toshev2014deeppose] [\cite=jain_accv2014] [\cite=tompson_cvpr2015] [\cite=fan2015combining] [\cite=ouyang2014multi] or on the pictorial structure framework [\cite=tran2010improved] [\cite=Eich12] [\cite=yang_tpami2014] [\cite=pishchulin_cvpr2013] [\cite=andriluka_ijcv2012] [\cite=dantone_tpami2014]. A few works also combine both concepts [\cite=chen_nips2014] [\cite=tompson2014joint]. In this work, we focus on PS models with a tree structure due to their exact and efficient inference [\cite=Felzenszwalb_ijcv2005] [\cite=felzenszwalb_tpami2010].

Several approaches have been proposed to improve the accuracy of PS models for human pose estimation. For instance, joint dependencies can be modeled not only by the PS model, but also by a mid-level image representation such as poselets [\cite=pishchulin_cvpr2013] [\cite=pishchulin2013strong] [\cite=vela_bmvc2014], exemplars [\cite=sap_cvpr2010] or data dependent probabilities learned by a neural network [\cite=chen_nips2014]. Pose estimation in videos can be improved by taking temporal information or motion cues into account [\cite=shen2014_eccv2014] [\cite=cherian_cvpr2014] [\cite=park_iccv2011] [\cite=batra_eccv2012diverse] [\cite=ramakrishna_cvpr2013] [\cite=zuffi_iccv2013].

In [\cite=park_iccv2011] [\cite=batra_eccv2012diverse] several pose hypotheses are generated for each video frame and a smooth configuration of poses over time is selected from all hypotheses. Instead of complete articulated pose, [\cite=ramakrishna_cvpr2013] and [\cite=cherian_cvpr2014] track individual body parts and regularize the trajectories of the body parts through the location of neighboring parts. A similar approach is adopted in [\cite=shen2014_eccv2014] where poses at each frame are tracked across multiple frames, and subsequently refined by introducing spatio-temporal smoothing constraints. Similar in spirit, the approach in [\cite=dong2015cvpr] jointly tracks symmetric body parts in order to better incorporate spatio-temporal constraints, and also to avoid double-counting. Optical flow information has also been used to enhance detected poses at each video frame by analyzing body motion in adjacent frames [\cite=sapp2011parsing] [\cite=fragkiadaki2013pose] [\cite=zuffi_iccv2013] [\cite=jain_accv2014] [\cite=Pfister15a]. In contrast to these approaches, we utilize the detected poses in each video frame to extract high-level information about the activity and use it to refine the poses. Action recognition based on 3D human poses has been investigated in many works [\cite=ye2013survey]. With the progress in the area of 2D human pose estimation in recent years, 2D poses have also gained an increased attention for action recognition [\cite=Jhuang_iccv2013] [\cite=pishculin_gcpr2014] [\cite=tran_bmvc2011] [\cite=singh_iccv2011action] [\cite=desai_eccv2012] [\cite=YangWM10] [\cite=cheron2015p]. However, utilizing action recognition to aid human pose estimation is not well studied, in particular not in the context of 2D human pose estimation. There are only a few works [\cite=yao_ijcv2012] [\cite=Yu_cvpr2013] [\cite=ukita2013iterative] [\cite=Bangpeng_TPAMI2012] [\cite=bruce_cvpr2015] that showed the benefit of it. The approaches in [\cite=yao_ijcv2012] [\cite=Yu_cvpr2013] rely on strong assumption. The approach [\cite=yao_ijcv2012] assumes that a person-specific 3D model is given and considers pose estimation in the context of multiple synchronized camera views. The approach [\cite=Yu_cvpr2013] focuses on 3D pose estimation from monocular videos and assumes that 3D pose data is available for all actions. The approach [\cite=ukita2013iterative] adopts a mixture of PS models, and learns a model for each action class. For a given image, each model is weighted by the confidence scores of an additional action recognition system and the pose with the maximum weight is taken. A similar approach is adopted in [\cite=Bangpeng_TPAMI2012] to model object-pose relations. These approaches, however, do not scale with the number of action classes since each model needs to be evaluated.

The closest to our work is the recent approach of [\cite=bruce_cvpr2015] that jointly estimates the action classes and refines human poses. The approach first estimates human poses at each video frame and decomposes them into sub-parts. These sub-parts are then tracked across video frames based on action specific spatio-temporal constraints. Finally, the action labels and joint locations are inferred from the part tracks that maximize a defined objective function. While the approach shows promising results, it does not re-estimate the parts but only re-combines them over frames , only the temporal constraints are influenced by an activity. Moreover, it relies on two additional activity recognition approaches based on optical flow and appearance features to obtain good action recognition accuracy that results in a very large computational overhead as compared to an approach that estimates activities using only the pose information. In this work, we show that additional action recognition approaches are not required, but predict the activities directly from a sequence of poses. In contrast to [\cite=bruce_cvpr2015], we condition the pose model itself on activities and re-estimate the entire pose per frame.

Overview

Our method exploits the fact that the information about the activity of a subject provides a cue about pose and appearance of the subject, and vice versa. In this work we utilize the high-level information about a person's activity to leverage the performance of pose estimation, where the activity information is obtained from previously inferred poses. To this end, we propose an action conditioned pictorial structure (PS) that incorporates action specific appearance and kinematic models. If we have only a uniform prior over the action classes, the model is a standard PS model, which we will briefly discuss in Section [\ref=sec:PS_model]. Figure [\ref=fig:overview] depicts an overview of the proposed framework.

Pictorial Structure

We adopt the joint representation [\cite=dantone_tpami2014] of the PS model [\cite=Felzenszwalb_ijcv2005], where the vector [formula] represents the 2D location of the jth joint in image [formula], and X = [formula] is the set of all body joints. The structure of a human body is represented by a kinematic tree with nodes of the tree being the joints j and edges E being the kinematic constraints between a joint j and its unique parent joint p as illustrated in Figure [\ref=fig:overview]. The pose configuration in a single image is then inferred by maximizing the following posterior distribution:

[formula]

where the unary potentials [formula] represent the likelihood of the jth joint at location [formula]. The binary potentials [formula] define the deformation cost for the joint-parent pair (j,p), and are often modeled by Gaussian distributions for an exact and efficient solution using a distance transform [\cite=Felzenszwalb_ijcv2005]. We describe the unary and binary terms in Section [\ref=sec:unary_potentials] and Section [\ref=sec:binary_potentials], respectively. In Section [\ref=sec:action_cond_ps], we then discuss how these can be adapted to build an action conditioned PS model.

Unary Potentials

Random regression forests have been proven to be robust for the task of human pose estimation [\cite=shotton2011real] [\cite=girshick_iccv2011] [\cite=minsun_cvpr2012] [\cite=dantone_tpami2014]. A regression forest F consists of a set of randomized regression trees, where each tree T is composed of split and leaf nodes. Each split node represents a weak classifier which passes an input image patch P to a subsequent left or right node until a leaf node LT is reached. As in [\cite=dantone_tpami2014], we use a separate regression forest for each body joint. Each tree is trained with a set of randomly sampled images from the training data. The patches around the annotated joint locations are considered as foreground and all others as background. Each patch consists of a joint label c∈{0,j}, a set of image features FP, and its 2D offset [formula] from the joint center. During training, a splitting function is learned for each split node by randomly selecting and maximizing a goodness measure for regression or classification. At the leaves the class probabilities [formula] and the distribution of offset vectors [formula] are stored.

During testing, patches are densely extracted from the input image [formula] and are passed through the trained trees. Each patch centered at location [formula] ends in a leaf node [formula] for each tree T∈F. The unary potentials φj for the joint j at location [formula] are then given by

[formula]

In [\cite=dantone_tpami2014] a two layer approach is proposed. The first layer consists of classification forests that classify image patches according to the body parts using a combination of color features, HOG features, and the output of a skin color detector. The second layer consists of regression forests that predict the joint locations using the features of the first layer and the output of the first layer as features. For both layers, the split nodes compare feature values at different pixel locations within a patch of size [formula] pixels.

We propose to replace the first layer by a convolutional network and extract convolutional channel features (CCF) [\cite=yang_iccv2015] from the intermediate layers of the network to train the regression forests of the second layer. In [\cite=yang_iccv2015] several pre-trained network architectures have been evaluated for pedestrian detection using boosting as classifier. The study shows that the "conv3-3" layer of the VGG-16 net [\cite=Simonyan14c] trained on the ImageNet (ILSVRC-2012) dataset performs very well even without fine tuning, but it is indicated that the optimal layer depends on the task. Instead of pre-selecting a layer, we use regression forests to select the features based on the layers "conv2-2", "conv3-3", "conv4-3", and "conv5-3". An example of the CCF extracted from an image is shown in Figure [\ref=fig:ccf]. Since these layers are of lower dimensions than the original image, we upsample them using linear interpolation to make their dimensions equivalent to the input image. This results in a 1408 (128+256+512+512) dimensional feature representation for each pixel. As split nodes in the regression forests, we use axis-aligned split functions. For an efficient feature extraction at multiple image scales, we use patchwork as proposed in [\cite=iandola2014densenet] to perform the forward pass of the convolutional network only once.

Binary Potentials

Binary potentials [formula] are modeled as a Gaussian mixture model for each joint j with respect to its parent joint p in the kinematic tree. As in [\cite=dantone_tpami2014], we obtain the relative offsets between child and parent joints from the training data and cluster them into [formula] clusters using k-means clustering. Each cluster k takes the form of a weighted Gaussian distribution as

[formula]

with mean μkjp and covariance Σkjp, where [formula]. The weights wkjp are set according to the cluster frequency [formula] with a normalization constant α = 0.1 [\cite=dantone_tpami2014].

For inference, we select the best cluster k for each joint by computing the max-marginals for the root node and backtrack the best pose configuration from the maximum of the max-marginals.

Action Conditioned Pose Estimation

As illustrated in Figure [\ref=fig:overview], our goal is to estimate the pose X conditioned by the distribution [formula] for a set of action classes a∈A. To this end, we introduce in Section [\ref=sec:action_cond_ps] a pictorial structure model that is conditioned on [formula]. Since we do not assume any prior knowledge of the action, we estimate the pose first with the uniform distribution [formula]. The estimated poses for N frames are then used to estimate the probabilities of the action classes [formula] as described in Section [\ref=sec:action_recognition]. Finally, the poses Xn are updated based on the distribution [formula].

Action Conditioned Pictorial Structure

In order to integrate the distribution [formula] of the action classes obtained from the action classifier into [\eqref=eqt:ps], we make the unaries and binaries dependent on [formula]:

[formula]

While the unary terms are discussed in Section [\ref=sec:cond_unary_potentials], the binaries [formula] are represented by Gaussians as in [\eqref=eq:binary]. However, instead of computing mean and covariance from all training poses with equal weights, we weight each training pose based on its action class label and [formula]. In our experiments, we will also investigate the case where [formula] is simplified to

[formula]

Conditional Joint Regressors

Figure [\ref=fig:appearance_sharing] shows examples of patches of the wrist extracted from images of different action classes. We can see a large amount of appearance variation across different classes regardless of the fact that all patches belong to the same body joint. However, it can also be seen that within individual activities this variation is relatively small. We exploit this observation and propose action specific unary potentials for each joint j. To this end we adopt conditional regression forests [\cite=dantone_cvpr2012] [\cite=minsun_cvpr2012] that have been proven to be robust for facial landmark detection in [\cite=dantone_cvpr2012] and 3D human pose estimation in [\cite=minsun_cvpr2012]. While [\cite=dantone_cvpr2012] trains a separate regression forest for each head pose and selects a specific regression forest conditioned on the output of a face pose detector, [\cite=minsun_cvpr2012] proposes partially conditioned regression forests, where a forest is jointly trained for a set of discrete states of a human attribute like human orientation or height and the conditioning only happens at the leaf nodes. Since the continuous attributes are discretized, interpolation between the discrete states is achieved by sharing the votes.

In this work we resort to partially conditional forests due to its significantly reduced training time and memory requirements. During training we augment each patch P with its action class label a. Instead of [formula] and [formula], the leaf nodes model the conditional probabilities [formula] and [formula]. Given the distribution over action classes [formula], we obtain the conditional unary potentials:

[formula]

Since the terms [formula] need to be computed only once for an image [formula], [formula] can be efficiently computed after an update of [formula].

Appearance Sharing Across Actions

Different actions sometimes share body pose configurations and appearance of parts as shown in Figure [\ref=fig:appearance_sharing]. We therefore propose to learn the sharing among action classes within a conditional regression forest. To this end, we replace the term [formula] in [\eqref=eqt:conditional_unary_potentials] by a weighted combination of the action classes:

[formula]

where the weights γa(a') represent the amount of sharing between action class a and a'. To learn the weights γa for each class a∈A, we apply the trained conditional regression forests to a set of validation images scaled to a constant body size and maximize the response of [\eqref=eqt:unary_with_cond] at the true joint locations and minimize it at non-joint locations. Formally, this can be stated as

[formula]

subject to [formula] and γ(a')  ≥  0. [formula] denotes the nth scaled validation image of action class a, [formula] is the annotated joint position for joint j in image [formula], and [formula] is a set of image locations which are more than 5 pixels away from [formula]. The set of negative samples is obtained by computing [formula] and taking the 10 strongest modes, which do not correspond to [formula], for each image. For optimization, we use the smoothed unaries

[formula]

with σ  =  3 and replace max  by the softmax function to make [\eqref=eqt:sharing_cost_fun] differentiable. The last term in [\eqref=eqt:sharing_cost_fun] is a regularizer that prefers sharing, , ||γ||2 attains its minimum value for uniform weights. In our experiments, we use λ = 0.4 as weight for the regularizer. We optimize the objective function by constrained local optimization using uniform weights for initialization γ(a') = 1 / |A|. In our experiments, we observed that similar weights are obtained when the optimization is initialized by γ(a) = 1 and γ(a') = 0 for a'  ≠  a, indicating that the results are not sensitive to the initialization. In [\eqref=eqt:sharing_cost_fun], we learn the weights γa for each action class but we could also optimize for each joint independently. In our experiments, however, we observed that this resulted in over-fitting.

Action Classification

For pose-based action recognition, we use the bag-of-word approach proposed in [\cite=Jhuang_iccv2013]. From the estimated joint positions [formula], a set of features called NTraj+ is computed that encodes spatial and directional joint information. Additionally, differences between successive frames are computed to encode the dynamics of the joint movements. Since we use a body model with 13 joints, we compute the locations of missing joints (neck and belly) in order to obtain the same 15 joints as in [\cite=Jhuang_iccv2013]. We approximate the neck position as the mean of the face and the center of shoulder joints. The belly position is approximated by the mean of the shoulder and hip joints. For each of the 3,223 descriptor types, a codebook is generated by running k-means 8 times on all training samples and choosing the codebook with minimum compactness. These codebooks are used to extract a histogram for each descriptor type and video. For classification, we use an SVM classifier in a multi-channel setup. To this end, for each descriptor type t, we compute a distance matrix Dt that contains the χ2-distance between the histograms (hti,htj) of all video pairs (vi,vj). We then obtain the kernel matrix that we use for the multi-class classification as follows

[formula]

where L is the number of descriptor types and μt is the mean of the distance matrix Dt. For classification we use a one-vs-all approach with C  =  100 for the SVM.

Experiments

In order to evaluate the proposed method, we follow the same protocol as proposed in [\cite=bruce_cvpr2015]. In particular, we evaluate the proposed method on two challenging datasets, namely sub-J-HMDB [\cite=Jhuang_iccv2013] and the Penn-Action dataset [\cite=zhang2013actemes]. Both datasets provide annotated 2D poses and activity labels for each video. They consist of videos collected from the Internet and contain large amount of scale and appearance variations, low resolution frames, occlusions and foreshortened body poses. This makes them very challenging for human pose estimation. While sub-J-HMDB [\cite=Jhuang_iccv2013] comprises videos from 12 action categories with fully visible persons, the Penn-Action dataset comprises videos from 15 action categories with a large amount of body part truncations. As in [\cite=bruce_cvpr2015], we discard the activity class "playing guitar" since it does not contain any fully visible person. For testing on sub-J-HMDB, we follow the 3-fold cross validation protocol proposed by [\cite=Jhuang_iccv2013]. On average for each split, this includes 229 videos for training and 87 videos for testing with 8,124 and 3,076 frames, respectively. The Penn-Action dataset consists of 1,212 videos for training and 1,020 for testing with 85,325 and 74,308 frames, respectively. To evaluate the performance of pose estimation, we use the APK (Average Precision of Keypoints) metric [\cite=yang_tpami2014] [\cite=bruce_cvpr2015].

Implementation Details

For the Penn-Action dataset, we split the training images half and half into a training set and a validation set. Since the dataset sub-J-HMDB is smaller, we create a validation set by mirroring the training images. The training images are scaled such that the mean upper body size is 40 pixels. Each forest consists of 20 trees, where 10 trees are trained on the training and 10 on the validation set, with a maximum depth of 20 and a minimum of 20 patches per leaf. We train each tree with 50,000 positive and 50,000 negative patches extracted from 5,000 randomly selected images and generate 40,000 split functions at each node. For the binary potentials, we use k = 24 mixtures per part. For learning the appearance sharing among action classes (Section [\ref=sec:appearance_sharing]) and training the action classifier (Section [\ref=sec:action_recognition]), we use the 10 trees trained on the training set and apply them to the validation set. The action classifier and the sharing are then learned on the validation set.

For pose estimation, we create an image pyramid and perform inference at each scale independently. We then select the final pose from the scale with the highest posterior [\eqref=eq:ps_cond]. In our experiments, we use 4 scales with scale factor 0.8. The evaluation of 260 trees (20 trees for each of the 13 joints) including feature extraction takes roughly 15 seconds on average. Inference with the PS model for all 4 scales takes around 1 second. The action recognition with feature computation takes only 0.18 seconds per image and it does not increase the time for pose estimation substantially.

Pose Estimation

We first evaluate the impact of the convolutional channel features (CCF) for pose estimation on split-1 of sub-J-HMDB. The results in Table [\ref=tab:mcf_vs_ccf] show that the CCF outperform the combination of color features, HOG features, and the output of a skin color detector, which is used in [\cite=dantone_tpami2014].

In Table [\ref=tab:frame_work_analysis_a] we evaluate the proposed ACPS model under different settings on split-1 of sub-J-HMDB when using CCF features for joint regressors. We start with the first step of our framework where neither the unaries nor the binaries depend on the action classes. This is equivalent to the standard PS model described in Section [\ref=sec:PS_model], which achieves an average joint estimation accuracy of 51.5%. Given the estimated poses, the pose-based action recognition approach described in Section [\ref=sec:action_recognition] achieves an action recognition accuracy of 66.3% for split-1.

Having estimated the action priors [formula], we first evaluate action conditioned binary potentials while keeping the unary potentials as in the standard PS model. As described in Section [\ref=sec:action_cond_ps], we can use in our model the probabilities [formula] or replace them by the distribution [\eqref=eqt:best_ps], which considers only the classified action class. The first setting is denoted by "Cond. ([formula])" and the second by "Cond. [\eqref=eqt:best_ps]". It can be seen that the conditional binaries based on [\eqref=eqt:best_ps] already outperform the baseline by improving the accuracy from 51.5% to 53.8%. However, taking the priors from all classes slightly decreases the performance. This shows that conditioning the binary potentials on the most probable class is a better choice than using priors from all classes.

Secondly, we analyze how action conditioned unary potentials affect pose estimation. For the unaries, we have the same options "Cond. ([formula])" and "Cond. [\eqref=eqt:best_ps]" as for the binaries. In addition, we can use appearance sharing as described in Section [\ref=sec:appearance_sharing], which is denoted by "AS". For all three binaries, the conditional unaries based on [\eqref=eqt:best_ps] decrease the performance. Since the conditional unaries based on [\eqref=eqt:best_ps] are specifically designed for each action class, they do not generalize well in case of a misclassified action class. However, adding appearance sharing to the conditional unaries boost the performance for both conditioned on [\eqref=eqt:best_ps] and [formula]. Adding appearance sharing outperforms all other unaries without appearance sharing, , conditional unaries, independent unaries and the unaries conditioned on [formula]. For all unaries, the binaries conditioned on [\eqref=eqt:best_ps] outperform the other binaries. This shows that appearance sharing and binaries conditioned on the most probable class performs best, which gives an improvement of the baseline from 51.5% to 55.1%.

In Table [\ref=tab:frame_work_analysis_b], we also evaluate the proposed ACPS when using the weaker features from [\cite=dantone_tpami2014]. Although the accuracies as compared to CCF features are lower, the benefit of the proposed method remains the same. For the rest of this paper, we will use CCF for all our experiments.

In Table [\ref=tab:final_acc] we compare the proposed action conditioned PS model with other state-of-the-art approaches on all three splits of sub-J-HMDB. In particular, we provide a comparison with [\cite=dantone_tpami2014] [\cite=yang_tpami2014] [\cite=bruce_cvpr2015] [\cite=park_iccv2011] [\cite=cherian_cvpr2014] [\cite=chen_nips2014]. The accuracies for the approaches [\cite=yang_tpami2014] [\cite=bruce_cvpr2015] [\cite=park_iccv2011] [\cite=cherian_cvpr2014] are taken from [\cite=bruce_cvpr2015] where the APK threshold 0.2 is used. We also evaluated the convolutional network based approach [\cite=chen_nips2014] using the publicly available source code trained on sub-J-HMDB. Our approach outperfroms the other methods by a margin, and notably improves wrist localization by more than 5% as compared to the baseline.

Table [\ref=tab:final_acc_penn] compares the proposed ACPS with the state-of-the-art on the Penn-Action dataset. The accuracies for the approaches [\cite=yang_tpami2014] [\cite=bruce_cvpr2015] [\cite=park_iccv2011] are taken from [\cite=bruce_cvpr2015]. We can see that the proposed method improves the baseline from 75.5% to 81.1%, while improving the elbow and wrist localization accuracy by more than 7% and 10%, respectively. The proposed method also significantly outperforms other approaches.

Action Recognition

In Table [\ref=tab:AR_JHMDB], we compare the action recognition accuracy obtained by our approach with state-of-the-approaches for action recognition. On sub-J-HMDB, the obtained accuracy using only pose as feature is comparable to the other approaches. Only the recent work [\cite=cheron2015p] which combines pose, CNN, and motion features achieves a better action recognition accuracy. However, if we combine our pose-based action recognition with Fisher vector encoding of improved dense trajectories [\cite=wang2013action] using late fusion, we outperform other methods that also combine pose and appearance. On the Penn-Action dataset, the results are similar. Although the approach [\cite=wang2013action] already achieves an accuracy of 92%, combining it with our pose-based approach increases the accuracy to 92.9%.

In Table [\ref=tab:class_wise_acc], we report the effect of different action recognition approaches on pose estimation. We report the pose estimation accuracy for split-1 of sub-J-HMDB when the action classes are not inferred by our framework, but estimated using improved dense trajectories with Fisher vector encoding (IDT-FV) [\cite=wang2013action] or the fusion of our pose-based method and IDT-FV. Although the action recognition rate is higher when pose and IDT-FV are combined, the pose estimation accuracy is not improved. If the action classes are not predicted but are provided (GT), the accuracy improves slightly for sub-J-HMDB and from 64.8% to 68.1% for the Penn-Action dataset. We also experimented with several iterations in our framework, but the improvements compared to the achieved accuracy of 73.8% on all three splits of sub-J-HMDB with one iteration were minor. A few qualitative results are shown in Figure [\ref=fig:Qualittive] along with some typical failure cases in Figure [\ref=fig:Failure].

Conclusion

In this paper, we have demonstrated that action recognition can be efficiently utilized to improve human pose estimation on realistic data. To this end, we presented a pictorial structure model that incorporates high-level activity information by conditioning the unaries and binaries on a prior distribution over action labels. Although the action priors can be estimated by an accurate, but expensive action recognition system, we have shown that the action priors can also be efficiently estimated during pose estimation without substantially increasing the computational time of the pose estimation. In our experiments, we thoroughly analyzed various combinations of unaries and binaries and showed that learning the right amount of appearance sharing among action classes improves the pose estimation accuracy. On two challenging datasets for pose estimation and action recognition, our proposed model achieves state-of-the-art performance. We expect that combining the proposed ACPS with the approach of [\cite=bruce_cvpr2015] can lead to further improvements and leave this for the future work.