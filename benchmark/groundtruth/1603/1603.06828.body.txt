Introduction

In this paper, we consider a classical problem: how to approximate a finite set D in Rm for relatively large m by a finite subset of a regular low-dimensional object in Rm. In applications this problems arises in many areas: from data visualization to fluid dynamics.

A variety of data approximators extending the classical Principal Component Analysis (PCA) [\citep=jolliffe2002principal] and k-means [\citep=macqueen1967some] clustering have been suggested in the last decades: principal curves [\citep=kegl2002], principal manifolds [\citep=hastie1989] [\citep=Gorban2001ihespreprint] [\citep=GorbanBook] [\citep=GorbanHand], Self Organized Maps (SOM) [\citep=kohonen1982self], elastic maps (EM) [\citep=gorban1999neural] [\citep=zinov2000] [\citep=GorbanZin2000] [\citep=gorban2005elastic] [\citep=GorbanVidaExpert2014], principal graphs (PG) [\citep=gorban2005elastic] [\citep=gorban2007topological] [\citep=zinovyev2013data] [\citep=GorbanHand] [\citep=GorbanPMaGiP], neural gas (NG) [\citep=martinetz1993neural] and many others.

A typical data approximation task starts with the following question: whether the dataset D is situated near a low-dimensional affine manifold (plane) in Rm? If we look for a point, straight line, plane, ... that minimizes the average squared distance to the datapoints, we immediately come to the Principal Component Analysis (PCA). PCA is one of the most seminal inventions in data analysis. Now it is textbook material. Nonlinear generalization of PCA is a great challenge, and many attempts have been made to answer it. One of the earliest algorithm suggested in this direction was Self-Organizing Maps (SOM) [\citep=kohonen2001self] approach with its multiple generalizations and implementations such as Growing SOM (GSOM) [\citep=Alahakoon2000]. However, unlike classical PCA and k-means, SOM algorithm is not based on optimization of any explicit functional [\citep=erwin1992self].

With the SOM algorithm we take a finite metric space Y with metric ρ and try to map it into Rm with (a) the best preservation of initial structure in the image of Y and (b) the best approximation of the dataset D. The SOM algorithm has several setup variables to regulate the compromise between these goals. We start from some initial approximation of the map, φ1:M  →  Rm. On each (k-th) step of the algorithm we have data set D and a current approximation φk:M  →  Rm. For each x∈D and φk we define an "owner" of x in Y: [formula]. The next approximation, φk + 1, is

[formula]

where Ky  =  {x∈D,yx = y} is the set of data points with the same owner,[formula] is the number of elements in the set, [formula], and hyt = w(ρ(y,t)), 0  ≤  hyt  ≤  1 is a monotonically decreasing cutting function. The idea of SOM is very flexible and seminal, has plenty of applications and generalizations, but, strictly speaking, we don't know what we are looking for: we have the algorithm, but no independent definition: SOM is a result of the algorithm work. The attempts to define SOM as solution of a minimization problem for some energy functional were not very successful [\citep=erwin1992self].

Provide references to GSOM, batch SOM!!!

For a known probability distribution, principal manifolds were introduced as lines or surfaces passing through "the middle" of the data distribution [\cite=hastie1989]. Several algorithms for construction of principal curves [\cite=kegl2002] and surfaces for finite datasets were developed during last decade, as well as many applications of this idea. In the end of 90s, a method of multidimensional data approximation based on elastic energy minimization was proposed (see [\cite=gorban1999neural] [\cite=zinov2000] [\cite=gorban2005elastic] and the bibliography there). This method is based on the analogy between the principal manifold and the elastic membrane (and plate). Following the metaphor of elasticity, we introduce two quadratic regularization terms penalizing non-smoothness of data approximators. This allows one to apply the standard expectation-minimization strategy with quadratic form of the optimized functionals at the minimization step (i.e., solving a system of linear algebraic equations with a sparse matrix). Later on, the elastic energy was applied to constructing principal elastic graphs [\cite=gorban2007topological].

This intuitive vision was transformed into the mathematical notion of self-consistency: every point x of the principal manifold M is a conditional expectation of all points z that are projected into x. Neither manifold, nor projection should be linear: just a differentiable projection π of the data space (usually it is Rm or a domain in Rm) onto the manifold M with the self-consistency requirement for conditional expectations: [formula] For a finite dataset D, only one or zero data points are typically projected into a point of the principal manifold. In order to avoid overfitting, we have to introduce smoothers that become an essential part of the principal manifold construction algorithms.

SOMs give one of the most popular approximations for principal manifolds: we can take for Y a fragment of a regular k-dimensional grid and consider the resulting SOM as the approximation to the k-dimensional principal manifold (see, for example, [\cite=Mulier95] [\cite=Ritter92]).

The method of elastic energy minimization allows creating analogs of SOM and neural gas with explicit functional to minimize: elastic map is an analog of SOM and principal graph is an analog of neural gas. The main advantage of optimization-based analogs is ability to explicitly control for smoothness (or other types of regularity, such as harmonicity) of data approximators.

However, the main drawback of all described methods of approximation is sensitivity to outliers and noise, which is caused by the very nature of Euclidean distance (or quadratic variance): data points distant to the approximator have very large relative contributions. There exist several widely used ideas for increasing approximator's robustness in the presence of strong noise in data such as: (1) using medians instead of mean values, (2) substituting Euclidean norm by L1 norm (e.g. [\citet=Ding2006] [\citet=hauberg2014]) and (3) outliers exclusion or fixed weighting or iterative reweighting during the construction of data approximators (e.g. [\citet=Xu1995] [\citet=allende2004robust] [\citet=kohonen2001self]). In some works, it was suggested to utilize "trimming" averages, e.g. in the context of the k-means clustering or some generalizations of PCA [\citet=cuesta1997] [\citet=hauberg2014]).

The general idea of trimming consists in penalizing the contribution of distant from the mean value data points to the estimation of variance. In the simplest scenario the points that are too distant from the mean value are completely neglected; in more complex scenario the distant points contribute less than the close ones. This way of robustification probably goes back to the notion of truncated (or trimmed) mean value [\citet=Huber1981]. The strategy of trimming can be used in construction of SOMs, elastic maps or almost any other data approximators.

In this paper we apply a particular variant of "impartial trimming" [\citep=gordaliza1991best] or "data-driven trimming" suitable for principal graphs. We restrict the attraction area of data points by robustness radius R0. It means that a data point x located further than R0 from the position of it's owner node yx, ([formula]) has constant contribution R20 into the minimized energy functional. With this penalty, the trimmed energy functional remains quadratic and the procedure of energy minimisation leads to solving a linear algebraic equations system, which preserves fast computations at the step of estimating position of graph nodes given assignment of data points to the closest graph nodes.

Comparison of robust version of principal graphs versus non-robust version as well as comparison of the effect of introducing robustification by trimming for several popular data approximators is provided in section [\ref=Result], for several benchmark data distributions with non-trivial structures and presence of noise.

Graph grammars and elastic principal graphs

Below in the description of basic algorithms we follow [\citep=gorban2007topological].

Let G be a simple undirected graph with set of vertices Y and set of edges E. For k  ≥  2 a k-star in G is a subgraph with k + 1 vertices [formula] and k edges [formula]. Suppose for each k  ≥  2, a family Sk of k-stars in G has been selected. We call a graph G with selected families of k-stars Sk an elastic graph if, for all E(i)∈E and S(j)k∈Sk, the correspondent elasticity moduli λi  >  0 and μkj  >  0 are defined. Let E(i)(0),E(i)(1) be vertices of an edge E(i) and [formula] be vertices of a k-star S(j)k (among them, S(j)k(0) is the central vertex). For any map φ:Y  →  Rm the energy of the graph is defined as

[formula]

For a given map φ:Y  →  Rm we divide the dataset D into node neighborhoods Ky,  y∈Y. The set Ky contains the data points for which the node φ(y) is the closest one in φ(y). The energy of approximation is:

[formula]

where w(x)  ≥  0 are the point weights. Simple and fast algorithm for minimization of the energy

[formula]

is the splitting algorithm, in the spirit of the classical k-means clustering: for a given system of sets {Ky | y∈Y} we minimize Uφ (optimization step, it is the minimization of a positive quadratic functional), then for a given φ we find new {Ky} (re-partitioning), and so on; stop when no change.

The next problem is the elastic graph construction. Here we should find a compromise between simplicity of graph topology, simplicity of geometrical form for a given topology, and accuracy of approximation [\citep=zinovyev2013data]. Geometrical complexity is measured by the graph energy Uφ(G), and the error of approximation is measured by the energy of approximation UφA(G,D). Both are included in the energy Uφ. Topological or construction complexity will be represented by means of elementary transformations: it is the length of the energetically optimal chain of elementary transformation from a given set applied to initial simple graph.

The graph grammars [\cite=Nagl] [\cite=Loewe] provide a well-developed formalism for the description of elementary graph transformations. An elastic graph grammar is presented as a set of production (or substitution) rules. Each rule has a form A  →  B, where A and B are elastic graphs. When this rule is applied to an elastic graph, a copy of A is removed from the graph together with all its incident edges and is replaced with a copy of B with edges that connect B to graph. For a full description of this language we need the notion of a labeled graph. Labels are necessary to provide the proper connection between B and the graph.

In practice the structure and complexity of the optimal graph for approximation of a complex dataset is not known. To learn it from the data themself, the principal elastic graphs are constructed using a growing schema. All possible graph structures are defined by a graph grammar. The optimal graph structure is obtained by sequential application of graph grammar operations to the simplest initial graph. A link in the energetically optimal transformation chain is added by finding a transformation application that gives the largest energy descent (after an optimization step), then the next link, and so on, until we achieve the desirable accuracy of approximation, or the limit number of transformations (some other termination criteria are also possible [\citep=zinovyev2013data]).

The selection of an energetically optimal application of transformations by the trial optimization steps is time-consuming. There exist alternative approaches. The preselection of applications for a production rule A  →  B can be done through comparison of energy of copies of A with its incident edges and stars in the transformed graph G.

As simple (but already rather powerful) example we use a system of two transformations: "add a node to a node" and "bisect an edge". These transformations act on a class of primitive elastic graphs: all non-terminal nodes with k edges are centers of elastic k-stars, which form all the k-stars of the graph.

The transformation "add a node to a node" can be applied to any vertex y of G: add a new node z and a new edge (y,z). The transformation "bisect an edge" is applicable to any pair of graph vertices y,y' connected by an edge (y,y'): Delete edge (y,y'), add a vertex z and two edges, (y,z) and (z,y'). The transformation of elastic structure (change in the star list) is induced by the change of topology, because the elastic graph is primitive. This two-transformation grammar with energy minimization builds principal trees (and principal curves, as a particular case) for datasets. Top row in Figure [\ref=GORBANEXAMPLE] shows several steps of graph construction. For applications, it is useful to associate one-dimensional continuums with these principal trees. Such a continuum consists of node images φ(y) and of pieces of straight lines that connect images of linked nodes.

Robust elastic principal graphs

For robust version of principal graphs we change only the data approximation energy term, because Uφ(G) term is independent of data. Data approximation energy term based on trimming is modified as following:

[formula]

where R0 is the robustness radius. All other terms in energy function are the same: Uφ = UφR(G,D) + Uφ(G). It means that all optimization strategies used for construction of principal graphs are applicable for robust principal graphs too.

What is the effect of such simple changing? Figure [\ref=GORBANEXAMPLE] shows two examples of principal graph and robust principal graph construction for clear and noisy data distributions. The level of noise is relatively small (56 randomly placed points for the 560 points of tree): however, we can see that quality of data approximation for principal graphs is significantly worse than quality of robust principal graphs for noised data.

Figure [\ref=GORBANEXAMPLE] shows different growth behaviours of principal graphs versus robust principal graphs. Principal graph tries to reproduce the global structure of data. For the clear tree-like data distribution (top row in Figure [\ref=GORBANEXAMPLE]), eight iterations is enough to reconstruct the global data structure. Robust principal graph catches the nearest fragment of data and traces the local data structure. As a result, the global structure of the data distribution is detected only to the end of graph growth. We can conclude that principal graphs search for the global data structure and then add details. Robust principal graphs trace the local data structures and then approximate the global data structure as a union of local fragments.

For noisy data (third and fourth rows in Figure [\ref=GORBANEXAMPLE]), the behaviour of robust principal graphs is the same as for the clear tree-like distribution and the noise points are simply ignored. Without robustification, the influence of noise points is very significant. As a result of this influence, the reconstructed global data structure is contaminated by approximation of noisy points.

As it is mentioned above, an alternative to ([\ref=robustApproximationTerm]) could be using L1-metrics, i.e. the sum of absolute values of coordinate differences instead of the sum of of squared coordinate differences:

[formula]

The huge disadvantage of this approach is the non-quadratic form of UφL(G,D). It means that minimization of the principal graph energy cannot be reduced to the solution of a system of linear algebraic equations leading to more complex and computationally expensive algorithms. Figure [\ref=GORBANROBUST] illustrates finding the optimal position of graph node y for one particular subset Ky in the case of standard quadratic, L1-based and trimmed quadratic data approximation term. This figure shows that the difference between the optimal positions of the y node in case of quadratic and L1 form of data approximation energy is relatively small: hence, robustification is moderate in this case.

The methods of robustification introduced in [\citet=allende2004robust] and [\citet=kohonen2001self] are specific to the non-batch SOM implementation and cannot be used for principal graphs.

Convergence of robust elastic principal graphs

Adding trimming in data approximation term as in ([\ref=robustApproximationTerm]) does not change the property of elastic principal graph's energy to converge to a local energy minimum. Both energy functions defined by ([\ref=globalStandardEnergy]) and the robust one

[formula]

are Lyapunov functions for the optimization splitting-based algorithm used for constructing the elastic principal graphs. Existence of Lyapunov function guarantees convergence of the optimization algorithm based on the splitting schema.

Each graph optimization algorithm step is split in two parts. First, with fixed partitioning of the dataset D into graph node neighbourhoods Ky,y∈Y, the quadratic function ([\ref=globalRobustEnergy]) is minimized which leads to the new positions of nodes φ'(y). At this step, the energy U can not increase because it is minimized: U(φ'(y))  ≤  U(φ(y)).

Secondly, a new partitioning into sets Ky' of data points x is computed with respect to the new positions of nodes φ'(y). Let us denote Kyc the set of points from Ky which are not more distant than R0 from φ(y): [formula], and let us denote the set of "distant" points as [formula]. Of course, the whole neighbourhood is a union of these two sets, and their intersection is empty: [formula]. After new partitioning, we will have a new [formula]. Let us consider one particular neighbourhood Ky1 and any other one Ky2. During the first step, φ(y1)  →  φ'(y1) and φ(y2)  →  φ'(y2). After re-partitioning, we might have several possible re-assignment of a data point x∈Ky1 (see Figure [\ref=FIGUREENERGY]):

(1) Ky1  →  Ky'1: in this case the energy UφR(G,D) does not change since the point x remains in the neighbourhood of y1.

(2) Ky1c  →  Ky'2: in this case the energy term related to y2 node in UφR(G,D) decreases because by definition of neighbourhood [formula].

(3) Ky1f  →  Ky'2c: in this case the energy term related to y2 node in UφR(G,D) will not increase because it will change from R20 to [formula].

(4) Ky1f  →  Ky'2f: in this case the energy term related to y2 node in UφR(G,D) does not change being equal R20 before and after re-partitioning.

The same four scenario are valid for any pair yi  ≠  yj: therefore, the total energy UφR(G,D) can not increase while Uφ(G) is not affected by re-partitioning. It is also clear that non-negative derivative of the trimmed approximation energy function (Figure [\ref=FIGUREENERGY]A) is essential for non-increase of U, because otherwise case (3) can lead to increasing UφR(G,D). For example, disregarding "distant" points completely for their contribution to the approximation energy would lead to violating property (3).

At the same time, it is possible to modify ([\ref=robustApproximationTerm]) in such a way that the "distant" points xf∈Kyf will influence the new positioning of the nodes y at the graph optimisation step. For example, it can look like

[formula]

where

[formula]

and position of y in w(x) is fixed during the computation of energy derivative at the graph optimization step.

Comparing various types of robust and non-robust data approximators

The results of application of usual and robust versions of SOM, elastic maps and principal graphs for two benchmarks (spiral and kappa-like data distributions) are presented in Figure [\ref=GORBANROTHER]. Level of noise is 10% (the fraction of randomly positioned points not belonging to the data pattern is 0.1 of the number of points in the pattern). All robust versions are trimmed with the same robustness radius. Figure [\ref=GORBANROTHER] shows that robustification of approximators leads to more adequate data approximations in all cases.

Implementation details and computational protocols

All illustrations used in this paper are created by a Java applet, developed by the authors, for constructing non-linear approximations of 2D data, using various algorithms [\citep=Applet]. The Parameters of the methods used are:

Growing Neural Gas (GNG): ripening period is 300, edge lifetime is 88, step of winner learning is 0.05, step of neighbours learning is 0.0006, error decreasing after bisection is 0.5,error forgetting ratio is 0.0005, maximal number of neurons is 100.

Growing SOM (GSOM): new node is added with the same curvature, maximal number of nodes is 500, half of neighbour B-spline width is 3.

Robust Growing SOM (RGSOM): new node is added with the same curvature, maximal number of nodes is 500, half of neighbour B-spline width is 3, robustness radius is 30.

EM: stretching module is 0.01, bending module is 0.05, new node is added with the same curvature, maximal number of nodes is 500.

Robust EM (REM): stretching module is 0.01, bending module is 0.05, new node is added with the same curvature, maximal number of nodes is 500, robust radius is 30.

Principal graph: stretching module is 0.001, bending module is 0.01, scaling exponent is 0, growing grammars Add node and edge and Add node by edge bisection applied twice and shrinking grammars Delete leaf node and Delete edge by nodes union applied once in each cycle, as described in [\citet=GorbanHand], maximal number of nodes is 500.

Robust principal graph (RPG): stretching module is 0.001, bending module is 0.01, scaling exponent is 0, growing grammars Add node and edge and Add node by edge bisection applied twice and shrinking grammars Delete leaf node and Delete edge by nodes union applied once in each cycle, maximal number of nodes is 500, the robustness radius is 20.

Discussion

We propose a robust version of principal elastic graph algorithm which is based on trimming the data approximation term in the elastic energy of the graph. Growing principal graphs proceeds by approximation of local data structures and tracing them till the global structure is detected. The algorithm contains an additional parameter R0 which is called the robustness radius. Only the data points inside this radius around a graph node y can influence position of y at the next iteration step. The algorithm shows good performance in the case when the global data structure is spoiled by noisy background distribution of data points, which makes the algorithm more suitable in many practical applications (such as image recognition). Existence of Lyapunov function guarantees convergence of the optimization algorithm based on the splitting schema.

For those data distributions which contain several isolated clusters, it is necessary to restart robust principal graphs several times (one graph for each cluster). The suggested data trimming can be applied for other data approximators such as elastic maps and SOMs.