Lemma

Latent Embeddings for Zero-shot Classification

, Quynh Nguyen3, Matthias Hein3 and Bernt Schiele1

Introduction

Zero-shot classification [\cite=HEEY15] [\cite=LNH13] [\cite=LEB08] [\cite=RSS11] [\cite=YA10] is a challenging problem. The task is generally set as follows: training images are provided for certain visual classes and the classifier is expected to predict the presence or absence of novel classes at test time. The training and test classes are connected via some auxiliary, non visual source of information e.g attributes.

Combining visual information with attributes [\cite=DPCG12] [\cite=FEH10] [\cite=FZ07] [\cite=KKTH12] [\cite=LNH13] [\cite=PG11] [\cite=PCKF14] has also supported fine grained classification. In fine grained image collections, images that belong to different classes are visually similar to each other, e.g different bird species. Image labeling for such collections is a costly process, as it requires either expert opinion or a large number of attributes. To overcome this limitation, recent works have explored distributed text representations [\cite=MSCCD13] [\cite=PSM14] [\cite=WordNet] which are learned from general (or domain specific) text corpora.

Substantial progress has been made for image classification problem in the zero-shot setting on fine-grained image collections [\cite=ARWLS15]. This progress can be attributed to (i) strong deep learning based image features [\cite=KSH12] [\cite=SzegedyCVPR2015] and (ii) learning a discriminative compatibility function between the structured image and class embeddings [\cite=APHS15] [\cite=ARWLS15] [\cite=FCSBM13] [\cite=romera2015embarrassingly]. The focus of this work is on the latter, i.e on improving the compatibility learning framework, in particular via unsupervised auxiliary information.

The main idea of structured embedding frameworks [\cite=APHS15] [\cite=ARWLS15] [\cite=FCSBM13] [\cite=romera2015embarrassingly] is to first represent both the images and the classes in some multi-dimensional vector spaces. Image embeddings are obtained from state-of-the-art image representations e.g those from convolutional neural networks [\cite=KSH12] [\cite=SzegedyCVPR2015]. Class embeddings can either (i) be obtained using manually specified side information e.g attributes [\cite=LNH13], or (ii) extracted automatically [\cite=MSCCD13] [\cite=PSM14] from an unlabeled large text corpora. A discriminative bilinear compatibility function is then learned that pulls images from the same class close to each other and pushes images from different classes away from each other. Once learned, such a compatibility function can be used to predict the class (more precisely, the embedding) of any given image (embedding). In particular, this prediction can be done for images from both seen and unseen classes, hence enabling zero-shot classification.

We address the fine-grained zero-shot classification problem while being particularly interested in more flexible unsupervised text embeddings. The state-of-the-art methods [\cite=APHS15] [\cite=ARWLS15] [\cite=FCSBM13] [\cite=romera2015embarrassingly] use a unique, globally linear compatibility function for all types of images. However, learning a linear compatibility function is not particularly suitable for the challenging fine-grained classification problem. For fine-grained classification, a model that can automatically group objects with similar properties together and then learn for each group a separate compatibility model is required. For instance, two different linear functions that separate blue birds with brown wings and from other blue birds with blue wings can be learned separately. To that end, we propose a novel model for zero-shot setting which incorporates latent variables to learn a piecewise linear compatibility function between image and class embeddings. The approach is inspired by many recent advances in visual recognition that utilize latent variable models, e.g in object detection [\cite=FelzenszwalbPAMI2010] [\cite=HussainBMVC2010], human pose estimation [\cite=YangCVPR2011] and face detection [\cite=Zhu2012face] etc.

Our contributions are as follows. (1) We propose a novel method for zero-shot learning. By incorporating latent variables in the compatibility function our method achieves factorization over such (possibly complex combinations of) variations in pose, appearance and other factors. Instead of learning a single linear function, we propose to learn a collection of linear models while allowing each image-class pair to choose from them. This effectively makes our model non-linear, as in different local regions of the space the decision boundary, while being linear, is different. We use an efficient stochastic gradient descent (SGD) based learning method. (2) We propose a fast and effective method for model selection, i.e through model pruning. (3) We evaluate our novel piecewise linear model for zero-shot classification on three challenging datasets. We show that incorporating latent variables in the compatibility learning framework consistently improves the state-of-the-art.

The rest of the paper is structured as follows. In Sec. [\ref=sec:bg] we detail the bilinear compatibility learning framework that we base our method on. In Sec. [\ref=sec:method] we present our novel Latent Embedding (LatEm) method. In Sec. [\ref=sec:experiments] we present our experimental evaluation and in Sec. [\ref=sec:conclusions] we conclude.

Related Work

We are interested in the problem of zero-shot learning where the test classes are disjoint from the training classes [\cite=HEEY15] [\cite=LNH13] [\cite=LEB08] [\cite=RSS11] [\cite=YA10] [\cite=ZS16]. As visual information from such test classes is not available during training, zero-shot learning requires secondary information sources to make up for the missing visual information. While secondary information can come from different sources, usually they are derived from either large and unrestricted, but freely available, text corpora, e.g word2vec [\cite=MSCCD13], glove [\cite=PSM14], or structured textual sources e.g wordnet hierarchies [\cite=WordNet], or costly human annotations e.g manually specified attributes [\cite=DPCG12] [\cite=FEH10] [\cite=FZ07] [\cite=KKTH12] [\cite=LNH13] [\cite=PG11] [\cite=PCKF14]. Attributes, such as 'furry', 'has four legs' etc for animals, capture several characteristics of objects (visual classes) that help associate some and differentiate others. They are typically collected through costly human annotation [\cite=DPCG12] [\cite=KKTH12] [\cite=PG11] and have shown promising results [\cite=APHS15] [\cite=CGG13] [\cite=DRS11] [\cite=LNH13] [\cite=LKS11] [\cite=SKBB12] [\cite=SFD11] [\cite=YJKLGL11] in various computer vision problems.

The image classification problem, with a secondary stream of information, could be either solved by solving related sub-problems, e.g attribute prediction [\cite=LNH13] [\cite=RSS11] [\cite=RSS10], or by a direct approach, e.g compatibility learning between embeddings [\cite=APHS15] [\cite=FCSBM13] [\cite=WBU11]. One such factorization could be by building intermediate attribute classifiers and then making a class prediction using a probabilistic weight of each attribute for each sample [\cite=LNH13]. However, these methods, based on attribute classifiers, have been shown to be suboptimal [\cite=APHS15]. This is due to their reliance on binary mappings (by thresholding attribute scores) between attributes and images which causes loss in information. On the other hand, solving the problem directly, by learning a direct mapping between images and their classes (represented as numerical vectors) has been shown to be better suited. Such label embedding methods [\cite=APHS15] [\cite=ARWLS15] [\cite=FCSBM13] [\cite=Hastie:Tibshirani:Friedman:2008] [\cite=NMBSSFCD13] [\cite=palatucci2009zero] [\cite=romera2015embarrassingly] [\cite=socher2013zero] aim to find a mapping between two embedding spaces, one each for the two streams of information e.g visual and textual. Among these methods, CCA [\cite=Hastie:Tibshirani:Friedman:2008] maximizes the correlation between these two embedding spaces,  [\cite=palatucci2009zero] learns a linear compatibility between an fMRI-based image space and the semantic space,  [\cite=socher2013zero] learns a deep non-linear mapping between images and tags, ConSe [\cite=NMBSSFCD13] uses the probabilities of a softmax-output layer to weight the vectors of all the classes, SJE [\cite=ARWLS15] and ALE [\cite=APHS15] learn a bilinear compatibility function using a multiclass [\cite=CS02] and a weighted approximate ranking loss [\cite=Jo06] respectively. DeViSE [\cite=FCSBM13] does the same, however, with an efficient ranking formulation. Most recently, [\cite=romera2015embarrassingly] proposes to learn this mapping by optimizing a simple objective function which has closed form solution.

We build our work on multimodal embedding methods. However, instead of learning a linear compatibility function, we propose a nonlinear compatibility framework that learns a collection of such linear models making the overall function piecewise linear.

Background: Bilinear Joint Embeddings

In this section, we describe the bilinear joint embedding framework [\cite=WBU11] [\cite=APHS15] [\cite=ARWLS15], on which we build our Latent Embedding Model (LatEm) (Sec. [\ref=sec:method]).

We work in a supervised setting where we are given an annotated training set [formula] where [formula] is the image embedding defined in an image feature space X, e.g CNN features [\cite=KSH12], and [formula] is the class embedding defined in a label space Y that models the conceptual relationships between classes, e.g attributes [\cite=FEHF09] [\cite=LNH13]. The goal is to learn a function f:X  â†’  Y to predict the correct class for the query images. In previous work [\cite=WBU11] [\cite=APHS15] [\cite=ARWLS15], this is done via learning a function [formula] that measures the compatibility between a given input embedding (âˆˆX) and an output embedding (âˆˆY). The prediction function then chooses the class with the maximum compatibility, i.e

[formula]

In general, the class embeddings reflect the common and distinguishing properties of different classes using side-information that is extracted independently of images. Using these embeddings, the compatibility can be computed even with those unknown classes which have no corresponding images in the training set. Therefore, this framework can be applied to zero-shot learning [\cite=APHS15] [\cite=ARWLS15] [\cite=palatucci2009zero] [\cite=romera2015embarrassingly] [\cite=socher2013zero]. In previous work, the compatibility function takes a simple form,

[formula]

with the matrix [formula] being the parameter to be learnt from training data. Due to the bilinearity of F in [formula] and [formula], previous work [\cite=APHS15] [\cite=ARWLS15] [\cite=WBU11] refer to this model as a bilinear model, however one can also view it as a linear one since F is linear in the parameter W. In the following, these two terminologies will be used interchangeably depending on the context.

Latent Embeddings Model (LatEm)

In general, the linearity of the compatibility function (Eq. [\eqref=eqnLinComp]) is a limitation as the problem of image classification is usually a complex nonlinear decision problem. A very successful extension of linear decision functions to nonlinear ones, has been through the use of piecewise linear decision functions. This idea has been applied successfully to various computer vision tasks e.g mixture of templates [\cite=HussainBMVC2010] and deformable parts-based model [\cite=FelzenszwalbPAMI2010] for object detection, mixture of parts for pose estimation [\cite=YangCVPR2011] and face detection [\cite=Zhu2012face]. The main idea in most of such models, along with modeling parts, is that of incorporating latent variables, thus making the decision function piecewise linear, e.g the different templates in the mixture of templates [\cite=HussainBMVC2010] and the different 'components' in the deformable parts model [\cite=FelzenszwalbPAMI2010]. The model then becomes a collection of linear models and the test images pick one from these linear models, with the selection being latent. Intuitively, this factorizes the decision function into components which focus on distinctive 'clusters' in the data e.g one component may focus on the profile view while another on the frontal view of the object.

Objective. We propose to construct a nonlinear, albeit piecewise linear, compatibility function. Parallel to the latent SVM formulation, we propose a non-linear compatibility function as follows,

[formula]

where [formula], with [formula], indexes over the latent choices and [formula] are the parameters of the individual linear components of the model. This can be rewritten as a mixture of bilinear compatibility functions from Eq. [\eqref=eqnLinComp] as

[formula]

Our main goal is to learn a set of compatibility spaces that minimizes the following empirical risk,

[formula]

where [formula] is the loss function defined for a particular example (n,n) as

[formula]

where Î”(,n) = 1 if   â‰   n and 0 otherwise. This ranking-based loss function has been previously used in [\cite=FCSBM13] [\cite=WBU11] such that the model is trained to produce a higher compatibility between the image embedding and the class embedding of the correct label than between the image embedding and class embedding of other labels.

Optimization. To minimize the empirical risk in Eq. [\eqref=eqn:wsabie], one first observes that the ranking loss function L from Eq. [\eqref=eqnRankLoss] is not jointly convex in all the Wi's even though F is convex. Thus, finding a globally optimal solution as in the previous linear models [\cite=APHS15] [\cite=ARWLS15] is out of reach. To solve this problem, we propose a simple SGD-based method that works in the same fashion as in the convex setting. It turns out that our algorithm works well in practice and achieves state-of-the-art results as we empirically show in Sec. [\ref=sec:experiments].

We explain the details of our Algorithm [\ref=alg:sgd] as follows. We loop through all our samples for a certain number of epochs T. For each sample (n,n) in the training set, we randomly select a [formula] that is different from n (step 3 of Algorithm [\ref=alg:sgd]). If the randomly selected [formula] violates the margin (step 4 in Algorithm [\ref=alg:sgd]), then we update the Wi matrices (steps 5 - 13 in Algorithm [\ref=alg:sgd]). In particular, we find the Wi that leads to the maximum score for [formula] and the Wj that gives the maximum score for [formula]. If the same matrix gives the maximum score (step 7 in Algorithm [\ref=alg:sgd]), we update that matrix. If two different matrices lead to the maximum score (step 9 in Algorithm [\ref=alg:sgd]), we update them both using SGD.

Model selection. The number of matrices K in the model is a free parameter. We use two strategies to select the number of matrices. As the first method, we use a standard cross-validation strategy - we split the dataset randomly into disjoint parts (in a zero-shot setup) and choose the K with the best cross-validation performance. While this is a well established strategy which we find to work well experimentally, we also propose a pruning based strategy which is competitive while being faster to train. As the second method, we start with a large number of matrices and prune them as follows. As the training proceeds, each sampled training examples chooses one of the matrices for scoring - we keep track of this information and build a histogram over the number of matrices counting how many times each matrix was chosen by any training example. In particular, this is done by increasing the counter for Wj* by 1 after step 6 of Algorithm [\ref=alg:sgd]. With this information, after five passes over the training data, we prune out the matrices which were chosen by less than 5% of the training examples, so far. This is based on the intuition that if a matrix is being chosen only by a very small number of examples, it is probably not critical for performance. With this approach we have to train only one model which adapts itself, instead of training multiple models for cross-validating K and then training a final model with the chosen K.

Discussion. LatEm builds on the idea of Structured Joint Embeddings (SJE) [\cite=ARWLS15]. We discuss below the differences between LatEm and SJE and emphasize our technical contributions.

LatEm learns a piecewise linear compatibility function through multiple Wi matrices whereas SJE [\cite=ARWLS15] is linear. With multiple Wi's the compatibility function has the freedom to treat different types of images differently. Let us consider a fixed class [formula] and two substantially visually different types of images 1,2, e.g the same bird flying and swimming. In SJE [\cite=ARWLS15] these images will be mapped to the class embedding space with a single mapping [formula]. On the other hand, LatEm will have learned two different matrices for the mapping i.e [formula]. While in the former case a single W has to map two visually, and hence numerically, very different vectors (close) to the same point, in the latent case such two different mappings are factorized separately and hence are arguably easier to perform. Such factorization is also expected to be advantageous when two classes sharing partial visual similarity are to be discriminated e.g while blue birds could be relative easily distinguished from red birds, to do so for different types of blue birds is harder. In such cases, one of the Wi's could focus on color while another one could focus on the beak shape (in Sec. [\ref=subsec:qualitative] we show that this effect is visible). The task of discrimination against different bird species would then be handled only by the second one, which would also arguably be easier.

LatEm uses the ranking based loss [\cite=WBU11] in Eq. [\eqref=eqnRankLoss] whereas SJE [\cite=ARWLS15] uses the multiclass loss of Crammer and Singer [\cite=CS02] which replaces the [formula] in Eq. [\eqref=eqnRankLoss] with max . The SGD algorithm for multiclass loss of Crammer and Singer [\cite=CS02] requires at each iteration a full pass over all the classes to search for the maximum violating class. Therefore it can happen that some matrices will not be updated frequently. On the other hand, the ranking based loss in Eq. [\eqref=eqnRankLoss] used by our LatEm model ensures that different latent matrices are updated frequently. Thus, the ranking based loss in Eq. [\eqref=eqnRankLoss] is better suited for our piecewise linear model.

Experiments

We evaluate the proposed model on three challenging publicly available datasets of Birds, Dogs and Animals. First, we describe the datasets, then give the implementation details and finally report the experimental results.

Datasets. Caltech-UCSD Birds (CUB), Stanford Dogs (Dogs) are standard benchmarks of fine-grained recognition [\cite=DPCG12] [\cite=DKF13] [\cite=CaltechUCSDBirdsDataset] [\cite=StanfordDogsDataset] and Animals With Attributes (AWA) is another popular and challenging benchmark dataset [\cite=LNH13]. All these three datasets have been used for zero-shot learning [\cite=ARWLS15] [\cite=RSS11] [\cite=KKTH12] [\cite=YA10]. Tab. [\ref=tabDBStats] gives the statistics for them.

In zero-shot setting, the dataset is divided into three disjoint sets of train, val and test. For comparing with previous works, we follow the same train/val/test set split used by [\cite=ARWLS15]. In zero-shot learning, where training and test classes are disjoint sets, to get a more stable estimate in our own results, we make four more splits by random sampling, while keeping the number of classes the same as before. We average results over the total of five splits. The average performance over the five splits is the default setting reported in all experiments, except where mentioned otherwise, e.g in comparison with previous methods.

Image and class embeddings. In our latent embedding (LatEm) model, the image embeddings (image features) and class embeddings (side information) are two essential components. To facilitate direct comparison with the state-of-the-art, we use the embeddings provided by [\cite=ARWLS15]. Briefly, as image embeddings we use the 1,024 dimensional outputs of the top-layer pooling units of the pre-trained GoogleNet [\cite=SzegedyCVPR2015] extracted from the whole image. We do not do any task specific pre-processing on images such as cropping foreground objects.

As class embeddings we evaluate four different alternatives, i.e attributes (att), word2vec (w2v), glove (glo) and hierarchies (hie). Attributes [\cite=LNH13] [\cite=FEHF09] are distinguishing properties of objects that are obtained through human annotation. For fine-grained datasets such as CUB and Dogs, as objects are visually very similar to each other, a large number of attributes are needed. Among the three datasets used, CUB contains 312 attributes, AWA contains 85 attributes while Dogs does not contain annotations for attributes. Our attribute class embedding is a vector per-class measuring the strength of each attribute based on human judgment.

In addition to human annotation the class embeddings can be constructed automatically from either a large unlabeled text corpora or through hierarchical relationship between classes. This has certain advantages such as we do not need any costly human annotation, however as a drawback, they tend not to perform as well as supervised attributes. One of our motivations for this work is that the class embeddings captured from a large text corpora contains latent relationships between classes and we would like to automatically learn these. Therefore, we evaluate three common methods for building unsupervised text embeddings. Word2Vec [\cite=MSCCD13] is a two-layer neural network which predicts words given the context within a skip window slided through a text document. It builds a vector for each word in a learned vocabulary. Glove [\cite=PSM14] is another distributed text representation method which uses co-occurrence statistics of words within a document. We use the pre-extracted word2vec and glove vectors from wikipedia provided by [\cite=ARWLS15]. Finally, another way of building a vectorial structure for our classes is to use a hierarchy such as WordNet [\cite=WordNet]. Our hierarchy vectors are based on the hierarchical distance between child and ancestor nodes, in WordNet, corresponding to our class names. For a direct comparison, we again use the hierarchy vectors provided by [\cite=ARWLS15]. In terms of size, w2v and glo are 400 dimensional whereas hie is â‰ˆ  200 dimensional.

Implementation details. Our image features are z-score normalized such that each dimension has zero mean and unit variance. All the class embeddings are [formula] normalized. The matrices Wi are initialized at random with zero mean and standard deviation [formula] [\cite=APHS15]. The number of epochs is fixed to be 150. The learning rates for the CUB, AWA and Dog datasets are chosen as Î·t = 0.1,0.001,0.01, respectively, and kept constant over iterations. For each dataset, these parameters are tuned on the validation set of the default dataset split and kept constant for all other dataset folds and for all class embeddings. As discussed in Sec [\ref=sec:method], we perform two strategies for selecting the number of latent matrices K: cross-validation and pruning. When using cross-validation, K is varied in {2,4,6,8,10} and the optimal K is chosen based the accuracy on a validation set. For pruning, K is initially set to be 16, and then at every fifth epoch during training, we prune all those matrices that support less than 5% of the data points.

Comparison with State-of-the-Art

We now provide a direct comparison between our LatEm and the state-of-the-art SJE [\cite=ARWLS15] method. SJE (Sec. [\ref=sec:bg]) learns a bilinear function that maximizes the compatibility between image and class embeddings. Our LatEm on the other hand learns a nonlinear, i.e piece-wise linear function, through multiple compatibility functions defined between image and class embeddings.

The results are presented in Tab. [\ref=tab:soa]. Using the text embeddings obtained through human annotation, i.e attributes (att), LatEm improves over SJE on AWA (71.9% vs 66.7%) significantly. However, as our aim is to reduce the accuracy gap between supervised and unsupervised class embeddings, we focus on unsupervised embeddings, i.e w2v, glo and hie. On all datasets, LatEm with w2v, glo and hie improves the state-of-the-art SJE [\cite=ARWLS15] significantly. With w2v, LatEm achieves 31.8% accuracy (vs 28.4%) on CUB, 61.1% accuracy (vs 51.2%) on AWA and finally 22.6% (vs 19.6%) on Dogs. Similarly, using glo, LatEm achieves 32.5% accuracy (vs 24.2%) on CUB, 62.9% accuracy (vs 58.8%) on AWA and 20.9% accuracy (vs 17.8%) on Dogs. Finally, while LatEm with hie on Dogs improves the result to 25.2% from 24.3%, the improvement is more significant on CUB (24.2% from 20.6%) and on AWA (57.5% from 51.2%). These results establish our novel Latent Embeddings (LatEm) as the new state-of-the-art method for zero-shot learning on three datasets in ten out of eleven test settings. They are encouraging, as they quantitatively show that learning piecewise linear latent embeddings indeed capture latent semantics on the class embedding space.

Following [\cite=ARWLS15] we also include a comparison when combining supervised and unsupervised embeddings. The results are given in Tab [\ref=tab:cmb]. First, we combine all the embeddings, i.e att,w2v,glo,hie for AWA and CUB. LatEm improves the results over SJE significantly on AWA (76.1% vs 73.9%). Second, we combine the unsupervised class embeddings, i.e w2v,glo,hie, for all datasets. LatEm consistently improves over the combined embeddings obtained with SJE in this setting. On CUB combining w2v,glo,hie achieves 34.9% (vs 29.9%), on AWA, it achieves 66.2% (vs 60.1%) and on Dogs, it obtains 36.3% (vs 35.1%). These experiments show that the embeddings contain non-redundant information, therefore the results tend to improve by combining them.

Stability evaluation of zero-shot learning. Zero-shot learning is a challenging problem due to the lack of labeled training data. In other words, during training time, neither images nor class relationships of test classes are seen. As a consequence, zero-shot learning suffers from the difficulty in parameter selection on a zero-shot set-up, i.e train, val and test classes belong to disjoint sets. In order to get stable estimates of our predictions, we experimented on additional (in our case four) independently and randomly chosen data splits in addition to the standard one. Both with our LatEm and the publicly available implementation of SJE [\cite=ARWLS15] we repeated the experiments five times.

The results are presented in Tab [\ref=tab:5fold]. For all datasets, all the result comparisons between SJE and LatEm hold and therefore the conclusions are the same. Although the SJE outperforms LatEm with supervised attributes on CUB, LatEm outperforms the SJE results with supervised attributes on AWA and consistently outperforms all the SJE results obtained with unsupervised class embeddings. The details of our results are as follows. Using supervised class embeddings, i.e attributes, on AWA, LatEm obtains an impressive 72.5% (vs 70.5%) and using unsupervised embeddings the highest accuracy is observed with w2v with 52.3% (vs 49.3%). On CUB, LatEm with w2v obtains the highest accuracy among the unsupervised class embeddings with 33.1% (vs 27.7%) On Dogs, LatEm with hie obtains the highest accuracy among all the class embeddings, i.e 25.6% (vs 24.6%). These results insure that our accuracy improvements reported in Tab [\ref=tab:soa] were not due to a dataset bias. By augmenting the datasets with four more splits, our LatEm obtains a consistent improvement on all the class embeddings on all datasets over the state-of-the-art.

Note that, for completion, in this section we provided a full comparison with the state-of-the-art on all class embeddings, including supervised attributes. However, there are two disadvantages of using attributes. First, since fine-grained object classes share many common properties, we need a large number of attributes which is costly to obtain. Second, attribute annotations need to be done on a dataset basis, i.e the attributes collected for birds do not work with dogs. Consequently, attribute based methods are not generalizable across datasets. Therefore, we are interested in the unsupervised text embeddings settings, i.e w2v, glo, hie. Moreover, with these unsupervised embeddings, our LatEm outperforms the SJE on nine out of nine cases in all our datasets. For the following sections, we will present results only with w2v, glo and hie.

Interpretability of latent embeddings

In Sec. [\ref=subsec:soa], we have demonstrated that our novel latent embedding method improves the state-of-the-art of zero-shot classification on two fine-grained datasets of birds and dogs, i.e CUB and Dogs, and one dataset of Animals, i.e AWA. In this section, we zoom into the challenging CUB dataset and aim to investigate if individual Wi's learn visually consistent and interpretable latent relationships between images and classes. We use word2vec and glove as text embeddings. Fig [\ref=fig:qualitative] shows the top scoring images retrieved by three different Wi for the two embeddings i.e w2v and glo.

For w2v, we observe that the images scored highly by the same Wi (each row) share some visual aspect. The images in the first row are consistently of birds which have long and pointy beaks. Note that they belong to different classes; having a long and pointy beak is one of the shared aspect of birds of these classes. Similarly, for the second row, the retrieved images are of small birds with brown heads and light colored breasts and the last row contains large birds with completely black plumage. These results are interesting because although w2v is trained on wikipedia in an unsupervised manner with no notion of attributes, our LatEm is able to (1) infer hidden common properties of classes and (2) support them with visual evidence, leading to a data clustering which is optimized for classification, however also performs well in retrieval.

For glo, similar to the results with w2v, the top-scoring images using the same Wi consistently show distinguishing visual properties of classes. The first row shows blue birds although belonging to different species, are clustered together which indicates that this matrix captures the "blue"ness of the objects. The second row has exclusively aquatic birds, surrounded by water. Finally, the third row has yellow birds only. Similar to w2v, although glo is trained in an unsupervised manner, our LatEm is able to bring out the latent information that reflect object attributes and support this with its visual counterpart.

These results clearly demonstrate that our model factorizes the information with visually interpretable relations between classes.

Pruning vs cross-validation for model selection

In this section we evaluate the performances obtained with the number of matrices in the model is fixed with pruning vs cross-validation.

Tab. [\ref=tabPruCV] presents the number of matrices selected by two methods along with their performances on three datasets. In terms of performance, both methods are competitive. Pruning outperforms cross validation on five cases and is outperformed on the remaining six cases. The performance gaps are usually within 1-2% absolute, with the exception of AWA dataset with att and w2v with 72.5% vs 70.7% and 52.3% vs 49.3%, respectively for cross validation and pruning. Hence neither of the methods has a clear advantage in terms of performance, however cross validation is slightly better.

In terms of the model size, cross validation seems to have a slight advantage. It selects a smaller model, hence more space and time efficient one, seven cases out of eleven. The trend is consistent for all class embeddings for the AwA dataset but is mixed for CUB and Dogs. The advantage of pruning over cross-validation is that it is much faster to train - while cross validation requires training and testing with multiple models (once each per every possible choice of K), pruning just requires training once. There is however another free parameter in pruning i.e choice of the amount of training data supporting a matrix for it to survive pruning. Arguably, it is more intuitive than setting directly the number of matrices to use instead of cross validating.

Evaluating the number of latent embeddings

In Sec. [\ref=subsec:soa], when we use multiple splits of the data, although the relative performance difference between the state of the art and our method has not changed, for some cases we observe a certain increase or decrease in accuracy. In this section, we investigate the experiments performed with five-folds on the CUB dataset and provide further analysis for a varying number of K. For completeness of the analysis, we also evaluate the single matrix case, namely Kâˆˆ{1,2,4,6,8,10} using unsupervised embeddings, i.e w2v, glo, hie.

Fig. [\ref=figPerfVsK] shows the performance of the model with a different number of matrices. We observe that the performance generally increases with increasing K, initially, and then the patterns differ with different embeddings. With w2v the performance keeps increasing until K = 6 and then starts decreasing, probably due to model overfitting. With glo the performance increases until K = 10 where the final accuracy is â‰ˆ  5% higher than with K = 1. With the hie embedding the standard errors do not increase significantly in any of the cases, are similar for all values of K and there is no clear trend in the performance. In conclusion, the variation in performance with K seems to depend of the embeddings used, however, in the zero-shot setting, depending on the data distribution the results may vary up to 5%.

Conclusions

We presented a novel latent variable based model, Latent Embeddings (LatEm), for learning a nonlinear (piecewise linear) compatibility function for the task of zero-shot classification. LatEm is a multi-modal method: it uses images and class-level side-information either collected through human annotation or in an unsupervised way from a large text corpus. LatEm incorporates multiple linear compatibility units and allows each image to choose one of them - such choices being the latent variables. We proposed a ranking based objective to learn the model using an efficient and scalable SGD based solver.

We empirically validated our model on three challenging benchmark datasets for zero-shot classification of Birds, Dogs and Animals. We improved the state-of-the-art for zero-shot learning using unsupervised class embeddings on AWA up to 66.2% (vs 60.1% )and on two fine-grained datasets, achieving 34.9% accuracy (vs 29.9%) on CUB as well as achieving 36.3% accuracy (vs 35.1%) on Dogs with word2vec. On AWA, we also improve the accuracy obtained with supervised class embeddings, obtaining 76.1% (vs 73.9%). This demonstrates quantitatively that our method learns a latent structure in the embedding space through multiple matrices. Moreover, we made a qualitative analysis on our results and showed that the latent embeddings learned with our method leads to visual consistencies. Our stability analysis on five dataset folds for all three benchmark datasets showed that our method can generalize well and does not overfit to the current dataset splits. We proposed a new method for selecting the number of latent variables automatically from the data. Such pruning based method speeds the training up and leads to models with competitive space-time complexities cf the cross-validation based method.

Appendix

Here, we provide further quantitative and qualitative results as well as more analysis on our proposed LatEm model.

Comparison With The State-Of-The-Art

In this section, we provide more analysis with [\cite=LNH13] and quantitative comparisons with [\cite=romera2015embarrassingly] and [\cite=SGSBMN13] which are among the most relevant related work to ours. [\cite=LNH13] proposes a two-step method that follows a different principle than ours: (1) Learning attribute classifiers and (2) combining the scores of these attribute classifiers to make a class prediction. Typically, the positive/negative samples to train an attribute classifier are obtained by binarizing the class-attribute matrix through thresholding. It is not clear how to extend this idea to unsupervised class embeddings, therefore, we compare [\cite=LNH13] and LatEm using attributes and our image embeddings on AWA. We obtain 56.2% accuracy with [\cite=LNH13], and 71.9% accuracy with our LatEm model. We would like to emphasize that we focus on unsupervised embeddings which may not be easily employed in [\cite=LNH13].

We re-implemented [\cite=romera2015embarrassingly] following the paper because their method is embrassingly simple. In [\cite=romera2015embarrassingly], they define a binary matrix Y of size m  Ã—  z to denote the groundtruth labels of m training instances belonging to any of the z classes. The scale of this matrix has been given as Yâˆˆ{ - 1,1}m  Ã—  z in [\cite=romera2015embarrassingly]. As shown in Tab. [\ref=tab:cub-awa-dog], we denote the results with Yâˆˆ{ - 1,1}m  Ã—  z as [\cite=romera2015embarrassingly] and obtain significantly lower results than our LatEm method with the default Y. On the other hand, considering that the scale of matrix Y is another parameter to tune, we also validated the results with Yâˆˆ{0,1}m  Ã—  z. We denote the experiment that uses Yâˆˆ{0,1}m  Ã—  z in [\cite=romera2015embarrassingly] as  [\cite=romera2015embarrassingly]* in Tab. [\ref=tab:cub-awa-dog]. We observe a significant increase in accuracy by changing the scaling factor as such. However, even with Yâˆˆ{0,1}m  Ã—  z, our LatEm still outperforms [\cite=romera2015embarrassingly]* in 8 out of 11 cases. For [\cite=SGSBMN13], we got the code from the authors and then we repeated the experiments as described in section 5. As it can be seen from Tab [\ref=tab:cub-awa-dog], our LatEm consistently outperforms [\cite=SGSBMN13] on all the datasets.

Results with Combination of Embeddings

Here, we provide results with direct comparison with [\cite=ARWLS15] where class embeddings are combined through early fusion (cnc) and late fusion of compatibility scores calculated by averaging the scores obtained with different class embeddings (cmb). We use the same combination of class embeddings as [\cite=ARWLS15] for fair comparison.

Further Qualitative Results

Finally, we provide qualitative results with Latent Embeddings (LatEm) using w2v embeddings on Fig [\ref=fig:w2v], using glo embeddings on Fig [\ref=fig:glo], using att embeddings on Fig [\ref=fig:att] and using hie embeddings on Fig [\ref=fig:hie]. We show the highest scoring images retrieved using all learned latent embeddings Wi.