Averaging Results with Theory Uncertainties

This work supported in part by the U.S. Department of Energy under grant DE-FG02-92-ER40701.

ABSTRACT

Combining measurements which have "theoretical uncertainties" is a delicate matter, due to an unclear statistical basis. We present an algorithm based on the notion that a theoretical uncertainty represents an estimate of bias.

Introduction

Combining measurements which have "theoretical uncertainties" is a delicate matter. Indeed, we are often in the position in which sufficient information is not provided to form a completely principled combination. Here, we develop a procedure based on the interpretation of theoretical uncertainties as estimates of bias. We compare and contrast this with a procedure that treats theoretical uncertainties along the same lines as statistical uncertainties.

Suppose we are given two measurements, with results expressed in the form:

[formula]

Assume that Â has been sampled from a probability distribution of the form pA(Â;Ā,σA), where Ā is the mean of the distribution and σA is the standard deviation. We make the corresponding assumption for B̂. The tA and tB uncertainties in Eq. [\ref=eqn:ABmeas] are the theoretical uncertainties. We may not need to know exactly what that means here, except that the same meaning should hold for both tA and tB. We suppose that both Â and B̂ are measurements of the same quantity of physical interest, though possibly with quite different approaches. The question is: How do we combine our two measurements?

Let the physical quantity we are trying to learn about be denoted θ. Given the two results A and B, we wish to form an estimator, [formula] for θ, with "statistical" and "theoretical" uncertainties expressed separately in the form:

[formula]

The quantities [formula], σ, and t are to be computed in terms of Â,B̂,σA,σB,tA, and tB.

Forming the Weighted Average

In the absence of theoretical uncertainties, we would normally combine our measurements according to the weighted average:

[formula]

For clarity, we are assuming for now that there is no statistical correlation between the measurements; such correlations will be incorporated later.

In general, Â and B̂ will be biased estimators for θ:

[formula]

where bA and bB are the biases. We adopt the point of view that the theoretical uncertainties tA and tB are estimates related to the possible magnitudes of these biases. That is,

[formula]

We wish to have t represent a similar notion.

Without yet specifying the weights, assume that we continue to form [formula] as a weighted average of Â and B̂:

[formula]

where wA and wB are the non-negative weights. The statistical error on the weighted average is computed according to simple error propagation on the individual statistical errors:

[formula]

The bias for [formula] is:

[formula]

If the theoretical uncertainties are regarded as estimates of the biases, then the theoretical uncertainty should be evaluated with the same weighting:

[formula]

It may be noted that this posesses desirable behavior in the limit where the theoretical uncertainties are identical (completely correlated) between the two measurements: The theoretical uncertainty on V̂ is in this case the same as tA = tB; no reduction is attained by having multiple measurements.

However, it is not quite true that the theoretical uncertainties are being regarded as estimates of bias. The tA and tB provide only estimates for the magnitudes, not the signs, of the biases. Eq. [\ref=eqn:weightedt] holds when the biases are of the same sign. If the biases are opposite sign, then we obtain

[formula]

Thus, our formula [\ref=eqn:weightedt] breaks down in some cases. For example, suppose the theoretical uncertainties are completely anticorrelated. In the case of equal weights, the combined theoretical uncertainty should be zero, because the two uncertainties are exactly canceled in the combined result. Only a statistical uncertainty remains.

Unfortunately, we don't always know whether the biases are expected to have the same sign or opposite sign. As a default, we adopt the procedure of Eq. [\ref=eqn:weightedt]. In the case of similar measurements, we suspect that the sign of the bias will often have the same sign, in which case we make the right choice. In the case of quite different measurements, such as inclusive and exclusive measurements of Vub, there is no particular reason to favor either relative sign; we simply don't know. The adopted procedure has the property that it errs on the side of "conservatism" - we will sometimes overestimate the theoretical uncertainty on the combined result.

There is still a further issue. The results of the measurements themselves can provide information on what the theoretical uncertainty could be. Consider two measurements with negligible statistical uncertainty. Then the difference between the two measurements is the difference between the biases. If the measurements are far apart, on the scale of the theoretical uncertainties, then this is evidence that the theoretical uncertainties are of opposite sign. We make no attempt to incorporate this information, again erring on the conservative side.

We turn to the question of choice of weights wA and wB. In the limit of negligible theoretical uncertainties we want to have

[formula]

Using these as the weights in the presence of theoretical uncertanties can lead to undesirable behavior. For example, suppose tA  ≫  tB and σA  ≪  σB. The central value computed with only the statistical weights ignores the theoretical uncertainty. A measurement with small theoretical uncertainty may be given little weight compared to a measurement with very large theoretical uncertainty. While not "wrong", this does not make optimal use of the available information. We may invent a weighting scheme which incorporates both the statistical and theoretical uncertainties, for example combining them in quadrature:

[formula]

Any such scheme can lead to unattractive dependence on the way measurements may be associatively combined. In order to have associativity in combining three measurements A,B,C, we must have that the weight for the combination of any two to be equal to the sum of the weights for those two, e.g., wAB = wA + wB. This is inconsistent with our other requirements. We shall adopt the procedure in Eq. [\ref=Eqn:weights], with the understanding that it is best to go back to the original measurements when combining several results, rather than making successive combinations.

Inconsistent Inputs

It may happen that our measurements are far enough apart that they appear inconsistent in terms of the quoted uncertainties. Our primary goal may be to test consistency between available data and a model, including whatever theoretical uncertainties exist in the comparison. We prefer to avoid making erroneous claims of inconsistency, even at the cost of some statistical power. Thus, we presume that when two measurements of what is assumed to be the same quantity appear inconsistent, something is wrong with the measurement or with the thoeretical uncertainties in the computation. If we have no good way to determine in detail where the fault lies, we adopt a method similar to that used by the Particle Data Group (PDG)[\cite=bib:PDG] to enlarge the stated uncertainties.

Given our two measurements as discussed above, we define the quantity:

[formula]

In the limit of purely statistical and normal errors, this quantity is distributed according to a chi-square with one degree of freedom. In the more general situation here, we don't know the detailed properties, but we nonetheless use it as a measure of the consistency of the results, in the belief that the procedure we adopt will still tend to err toward conservatism.

If χ2  ≤  1, the measurements are deemed consistent. On the other hand, if χ2 > 1, we call the measurements inconsistent, and apply a scale factor to the errors in order to obtain χ2 = 1. We take the point of view that we don't know which measurement (or both) is flawed, or whether the problem is with the statistical or theoretical error evaluation. If we did have such relevant information, we could use that in a more informed procedure. Thus, we scale all of the errors (σA, σB, tA, tB) by a factor:

[formula]

This scaling does not change the central value of the averaged result, but does scale the statistical and theoretical uncertainties by the same factor.

Relative Errors

We often are faced with the situation in which the uncertainties are relative, rather than absolute. In this case, the model in which θ is a location parameter of a Gaussian distribution breaks down. However, it may be a reasonable approximation to continue to think in terms this model, with some modification to mitigate bias. We also continue to work in the context of a least-squares minimization, although it might be interesting to investigate a maximum likelihood approach.

Thus, suppose we have additional experimental uncertainties sA and sB, which scale with θ:

[formula]

If sk is what we are given, we infer the proportionality constants according to rA = sA  /  Â and rB = sB  /  B̂.

The weights that are given in Eqn. [\ref=Eqn:weights] are modified to incorporate this new source of uncertainty according to:

[formula]

Note that, as we don't know θ, we use [formula] instead. This means that the averaging process is now iterative, until convergence to a particular value of [formula] is obtained.

Likewise, there may be a theoretical uncertainty which scales with θ, and we may treat this similarly. Thus, suppose that, for example, t2A  =  t2aA  +  t2rA, where taA is an absolute uncertainty, and trA  =  ρAθ. We simply replace θ by [formula] and substitute this expression wherever tA appears, e.g., in Eqn. [\ref=eqn:weightsWithRelativeErrors]. That is:

[formula]

Summary of Algorithm

We summarize the proposed algorithm, now including possible statistical correlations: Suppose we have n measurements [formula] with covariance matrix

[formula]

and mean values

[formula]

Note that, in the non-correlated case, Mij  =  σ2iδij, or including relative uncertainties, [formula]. The parameter we are trying to learn about is θ, and the bi is the bias that is being estimated with theoretical uncertainties ti.

The present notion of the weighted average is that we find a θ which minimizes:

[formula]

This is based on the premise that we don't actually know what the biases are, and we do the minimization with zero bias in the (x - θ) dependence. The possible size of bias is taken into account in the weighting, giving more weight to those measurements in which the size of the bias is likely to be smaller.

The "weight matrix" W in principle could be taken to be:

[formula]

That is, W- 1 is an estimate for

[formula]

However, we don't assume that we know the relative signs of bi and bj. Hence, the off-diagonal titj term in Eqn. [\ref=Eqn:weightMatrixTry] could just as likely enter with a minus sign. We therefore use the weight matrix:

[formula]

If we do know the relative signs of the biases, for example because the theoretical uncertainties are correlated, then the off-diagonal terms in Eqn. [\ref=Eqn:weightMatrixTry] should be included, with the appropriate sign. When using the term "correlated" with theoretical uncertainties, it should be kept in mind that it does not necessarily have a statistical interpretation.

Setting [formula] gives the central value ("best" estimate):

[formula]

The statistical uncertainty is

[formula]

Note that this reduces to

[formula]

in the case of only statistical uncertainites. The theoretical uncertainty is

[formula]

where

[formula]

Finally, if χ2  >  n - 1, these error estimates are scaled by a factor:

[formula]

where χ2 here is the value after the minimization.

Comparison with treating theoretical uncertainties on same footing as statistical

Another approach to the present problem is to simply treat the theoretical uncertainties as if they were statistical.[\cite=bib:HFAG] This procedure gives the same estimator as above for θ. However, the results for statistical and theoretical uncertainties differ in general.

Let [formula] be the estimated statistical uncertainty on the average for this approach, and let [formula] be the estimated theoretical uncertainty. Also, let Tij be the "covariance matrix" for the theoretical uncertainties in this picture. Then the statistical and theoretical uncertainties on the average are given by:

[formula]

Note that the weights are given, as before, by

[formula]

That is, the weights are the same as the treatment earlier, if the same assumptions about theoretical correlations are made in both places.

The estimates for the statistical and theoretical uncertainties differ between the two methods. That is, in general, [formula] and [formula].

The statistical uncertainty σ is computed from the individual statistical uncertainties according to simple error propagation. The statistical uncertainty [formula] is evaluated by identifying a piece of the overall quadratic combination of statistical and theoretical uncertainties as "statistical".

The difference between t and [formula] is that t is computed as a weighted average of the individual t's, while [formula] is evaluated by identifying a piece of the overall quadratic combination of statistical and theoretical uncertainties as "theoretical". The approach for t is based on the notion that the theoretical uncertainties are estimates of bias, but with a conservative treatment of any unknown correlations. The [formula] approach may be appropriate if the theoretical uncertainties are given a probablistic interpretation.

Let's consider some possible special cases. Suppose that all of the ti's are the same, equal to t1, and suppose that the theory uncertainties are presumed to be "uncorrelated". In this case,

[formula]

Which is more reasonable? That depends on how we view the meaning of "uncorrelated" in our assumption, and on whether we assign a probabilistic interpretation to the theoretical uncertainties. If we are supposing that the acutal theoretical uncertainties are somehow randomly distributed in sign and magnitude, then it is reasonable to expect that the result will become more reliable as more numbers are averaged. However, if we consider the theoretical uncertainties as estimates of bias, which could in fact all have the same sign, then the weighted linear average is plausible. It is at least a more conservative approach in the absence of real information on the correlations.

Note that if the correlation in theoretical uncertainty is actually known, the weighted linear average will take that into account. For example, suppose there are just two measurements, with t2 =  - t1. If the weights are the same (that is, we also have σ1  =  σ2) then t = 0. The other approach also gives [formula].

A different illustrative case is when t2 = 0, t1  ≠  0, and [formula]. In this case, we find

[formula]

To understand the difference better, consider the limit in which t1  ≫  σ1,σ2:

[formula]

In this limit, both methods agree that the important information is in x2. The first method assigns a statistical error corresponding to the statistical uncertainty of x2, and a theoretical uncertainty of zero, reflecting the zero theoretical uncertainty on the x2 measurement. The second method, however, assigns equal statistical and theoretical uncertainites to the average. Their sum in quadrature is a plausible expression of the total uncertainty, but the breakdown into theoretical and statistical components is not reasonable in the second method.

Another limit we can take in this example is σ1  ≪  t1  ≪  σ2, obtaining:

[formula]

Similar observations may be made in this case as in the previous.

Conclusions

We suggest a procedure to treat theoretical uncertainties when combining measurements of a quantity. It must be emphasized that, at least without more information about the nature of the theoretical uncertainty, there is no rigorous procedure, e.g., in the context of frequencies. The interpretation we adopt is that theoretical errors represent estimates of bias. This leads to a straightforward algorithm. If the sign of the bias is not specified (the usual situation), the procedure is designed to be "conservative", in the sense that we may err on the side of overstating theory uncertainties on the combined result. There is also some arbitrariness in construction of the weights from the statistical and theoretical uncertainties; we suggest simply adding the uncertainties in quadrature.

Our procedure is compared with a method that treats theoretical uncertainties as if they are of statistical origin. While there are some reassuring similarities, there are also differences. For example, the two procedures lead to different scaling behavior for the theoretical uncertainty with the number of measurements. Depending on interpretation, either result could be regarded as reasonable; our procedure yields a more "conservative" scaling. Our procedure also does a better job of keeping meaningful separation between statistical and theoretical uncertainties as results are combined. In the case where there are no theoretical uncertainties, both procedures yield identical, conventional results. It may be useful in practice to compute the uncertainties via both approaches, giving an idea for how sensitive the result is to the assumptions.

It would, of course, be nice to have a test of the procedure. For example, does it lead to appropriate frequency coverage? Given the lack of clarity (and perhaps lack of consistency) in what theoretical uncertainties really mean, a meaningful test is difficult. We thus rely on the conservative nature of our procedure - the intervals obtained for the combined results will "probably" over-cover if theoretical uncertainties are present.

ACKNOWLEDGMENTS

I am grateful to Gregory Dubois-Felsmann, Gerald Eigen, David Hitlin, and Robert Kowalewski for stimulating discussions on this topic.