=1

Lemma Theorem Definition Corollary Proposition Claim Conjecture

The Structure of Information Pathways in a Social Communication Network

Categories and Subject Descriptors: H.2.8 Database Management: Database Applications - Data Mining

General Terms: Measurement, Theory

Keywords: social networks, communication latency, strength of weak ties

Acknowledgments: This research was supported in part by the Institute for Social and Economic Research and Policy at Columbia University, the Institute for the Social Sciences at Cornell University, the James S. McDonnell Foundation, the John D. and Catherine T. MacArthur Foundation, a Google Research Grant, a Yahoo! Research Alliance Grant, and NSF grants SES-0339023, CCF-0325453, IIS-0329064, CNS-0403340, and BCS-0537606.

Introduction

Large social networks serve as conduits for communication and the flow of information [\cite=adamic-huberman-survey] [\cite=granovetter-weak-ties]; but information only spreads on these networks as a result of discrete communication events--such as e-mail or text messages, conversations, meetings, or phone calls--that are distributed non-uniformly over time [\cite=gibson-scheduling] [\cite=white-everyday-life]. Just because two individuals are acquainted does not imply that they have communicated within some particular time interval, in which case no information could have passed directly between them. Correspondingly, the indirect flow of information between individuals requires a sequence of communication events along a path of intermediaries linking them. Although straightforward to state, these observations pose additional complications for the analysis of social networks, and can have important consequences for the relation between network structure and information flow [\cite=holme-reachability] [\cite=onnela-phone-data].

It has been a challenge to build reasonable models for the patterns of communication within a social network: it is difficult to obtain data on social network structure at a large scale, and more difficult still to obtain complete data on the dynamics of a network's communication events over time. Recent research working with such datasets has primarily studied communication of an event-driven nature, looking at communication within a social network triggered by a particular event or activity; such investigations have typically focused on communication events that ripple through many nodes over short time-scales following the triggering event. Examples of this include cascades of e-mail recommendations for products [\cite=leskovec-ec06], cascades of references among bloggers [\cite=adar-blogspace] [\cite=gruhl-blogspace] [\cite=leskovec-blogspace-sdm07], the spread of e-mail chain letters [\cite=liben-nowell-pnas08], and the search for distant targets in a social network [\cite=dodds-swn-expmt] [\cite=travers-swn].

These types of event-driven communication, however, take place against the backdrop of a much broader set of natural communication rhythms, a kind of systemic communication that circulates information continuously through the network. Pairs of individuals communicate over time at very different rates, for an enormous range of different reasons. Viewed cumulatively, this background pattern enables information to piggy-back on everyday communication and thus spread generally through the network. This type of systemic communication has remained essentially invisible in analyses of social networks over time, but its properties arguably determine much about the rate at which people in the network remain up-to-date on information about each other.

The present work: Systemic communication and information pathways

We propose a framework for analyzing this kind of systemic communication, based on inferring structural measures from the potential for information to flow between different nodes. To motivate this by an example, suppose we have the complete communication history for a group of five people over three days, as illustrated in Figure [\ref=fig:timestamps]. (Edges are annotated with the one or more times at which directed communication took place.) For the sake of this example, let us assume that there are no communication events outside the group that are relevant to the analysis. We can now ask questions such as the following: At 5pm on Friday, what is the most recent information that node B could possibly have about node A? Clearly B could have learned about A's state as of Wednesday at 11am, when their last direct communication took place. However, further inspection of the figure reveals that the most recent opportunity for information to flow from A to B was in fact the Friday 9am communication from A to C, which was then followed by the Friday 3pm communication from C to B.

Without knowing anything about the content of the messages, we will not necessarily know what, if anything, flowed between nodes, but this sequence of timestamps gives us a global picture of the information pathways, providing the full set of potential conduits for information to flow through the group of people. From this structure, we can draw several conclusions. First, still without knowing the message content, we can conclude that anything that has happened to A in the past eight hours will be unknown to B: at Friday 5pm, B is in a strong sense (at least) eight hours "out-of-date" with respect to A. Second, assuming this interval of three days is typical of the communication dynamics within this group of five people, we can infer that direct communication does not generally provide B with the strongest opportunities to learn information about A; rather, the indirect A-C-B path has the potential to transmit information from A to B much faster than the direct link.

We argue here that these latter two issues -- out-of-date information and indirect paths -- are central to an understanding of the patterns of systemic communication within a social network. The notion of individuals being out-of-date with respect to each other's information is an intuitively natural one, and one finds implicit reflections of it in settings ranging from the study of physical systems to social processes and fictional narratives. The physical world, for example, is governed by principle that we are at least k years out-of-date with respect to any point in space k light-years away; the notion of the light cone more generally characterizes the regions of space-time between which information can possibly have flowed [\cite=taylor-relativity]. In sociology, the premise that occasional encounters with distant acquaintances can provide important information about new opportunities helps form the basis for Granovetter's celebrated theory of the strength of weak ties [\cite=granovetter-weak-ties]. And in yet a different direction, the idea that two individuals sometimes cannot know what has happened to one another, over short time spans, arises as a literary device; for example, in his novel The Gift, Vladimir Nabokov provides the following grim but memorable image to convey the idea that it took the character Yasha's family several hours to learn of his suicide:

... no sooner had he reached her than both of them heard the dull pop of the shot, while in Yasha's room life went on for a few more hours as if nothing had happened ... [\cite=nabokov-gift]

The role of indirect paths in social communication is also a crucial issue that has received relatively little formal attention. If we look at a social network represented simply as an unweighted graph, then any time two nodes are joined by an edge, this edge provides the most direct path between them. If we have data, however, on the times or rates at which communication actually takes place across edges, then we can discover -- as in Figure [\ref=fig:timestamps] -- that often information has the potential to flow much more rapidly via multi-step paths. In a sense, then, the A-C-B path in Figure [\ref=fig:timestamps] can be viewed as a "triangle-inequality violation", in that a two-step path can be faster than a one-step path. One finds intuitively natural reflections of this principle in everyday life: a manager who talks to each of two employees much more frequently than they talk to each other, or a parent who talks to each of two adult children much more frequently than they talk to each other. We will see later that the structure of communication in real social networks is in fact dominated by such violations of the triangle inequality.

The present work: Vector clocks and backbone structures

We now proceed to study these notions of out-of-date information and indirect paths using data for which we have complete histories of communication events over long periods of time. Our main dataset is a complete set of anonymized e-mail logs among all faculty and staff at a large university over two years [\cite=kossinets-email]. We will use this university e-mail dataset as the primary focus of discussion in the present work; but at the end, we also discuss the results of our analyses on two other sources of data: the Enron e-mail corpus [\cite=klimt-enron], a widely-used dataset containing of e-mail communication among executives from the (now-defunct) Enron corporation; and also, in a quite different domain, the complete set of user-talk communications among admins and high-volume editors on Wikipedia. Taken together, these datasets thus represent a range of different settings in which the patterns of systemic communication within a large group are integral to the workflow of the group. We find broadly similar patterns of results across all of them.

We analyze the issue of out-of-date information by adapting ideas from the field of distributed computing, which has also had to deal with the problem of potential information flow among different computing hosts -- determining, for example, which machines might be affected if a given host is compromised at a given point in time. In particular, we use the notion of vector clocks introduced by Lamport and refined by Mattern to study how information spreads in distributed systems [\cite=lamport-clocks] [\cite=mattern-clocks]. (Mattern's development, among other things, draws interesting analogies with notions of simultaneity and light-cones from special relativity [\cite=mattern-clocks].)

Next, we formalize the notion of indirect paths by defining the network backbone -- the subset of edges in the social network that are not bypassed by a faster alternate path. We propose several related definitions of the backbone, and for all formulations we find that the backbone is a very sparse subgraph consisting of a mixture of highly embedded edges and longer-range bridges. Finally, we consider how potential information flow would be affected if communication were sped up or slowed down on certain backbone edges, and use this to draw conclusions about the effect of local communication rates on the global circulation of information.

In the end, it is important to reiterate, we are using these notions of potential information flow to draw structural conclusions about social communication networks in their everyday operation. We do not attempt to map the actual contents of messages as they are being sent, nor are we focusing on the effects of one-time, "special" events that can generate novel communication flows. Rather, our goal is to approach a dual, and largely unstudied, issue -- how everyday patterns of communication suggest certain temporal notions of distance that are distinct from the picture that an unweighted graph provides, and how these patterns cause certain sparse sets of pathways to emerge as the lines along which information has the ability to flow the quickest.

Further related work

The complete traces of communication within a network of people has been studied at moderate scales in recent years [\cite=adamic-how-search] [\cite=eckmann-email], and very recently there have been analyses of very large-scale networks based on phone calls [\cite=onnela-phone-data] and instant messaging [\cite=leskovec-msn-im]. These studies, however, have focused on structural properties of the networks different from the definitions we propose here. As noted above, a number of other recent lines of research have focused on cascading communication triggered by specific events [\cite=adar-blogspace] [\cite=dodds-swn-expmt] [\cite=gruhl-blogspace] [\cite=leskovec-ec06] [\cite=leskovec-blogspace-sdm07] [\cite=liben-nowell-pnas08], but this work too addresses issues that are quite different from our focus here.

The notion of a graph annotated with the times at which the nodes communicated has been studied at a theoretical level [\cite=berman-temporal] [\cite=cheng-temporal] [\cite=holme-reachability] [\cite=kempe-temporal]. Holme has explored some of the theoretical definitions on network datasets [\cite=holme-reachability], though in different directions from what we do here.

Finally, the sub-field of distributed computing concerned with epidemic or gossip-based algorithms has focused on designing communication patterns that spread information quickly [\cite=demers-epidemic]. In contrast, we focus here on systems that are not designed, but where analyses of the communication patterns over time can nonetheless provide us with insights into underlying structures in the network.

Vector Clocks and Latency

The basic structure of the data we consider is as follows. We are given a set V of people (nodes) communicating over a time interval

[formula]

.

We begin by briefly reviewing the approach of Lamport, Mattern, and others in the line of distributed computing research aimed at formalizing temporal lags between nodes in a network [\cite=lamport-clocks] [\cite=mattern-clocks]. To start, we consider a node v at time t and try to determine how "up-to-date" its information about another node u could be. We can quantify this by asking the following question: what is the largest t'  <  t for which a piece of information originating at time t' at u could be transmitted through a sequence of communications and still arrive at v by time t? We call this largest t' the view that v has of u at time t, and denote it by φv,t(u). The amount by which v's view of u is "out-of-date" at time t is given by t  -  φv,t(u); we will call this the information latency of u with respect to v at time t. For example, in Figure [\ref=fig:timestamps], the views that B has of A, C, D, and E at Friday 5pm are, respectively, Fri 9am, Fri 3pm, Thu 3pm, and Fri 11am; and hence the latencies are 8 hours, 2 hours, 26 hours, and 6 hours. (We will define φv,t(v)  =  t for all v and t: v is always completely up-to-date with respect to itself.) Finally, we can take all the views of other nodes that v has at time t and write it as a single vector φv,t  =  (φv,t(u):u∈V). We refer to φv,t as the vector clock of v at time t [\cite=lamport-clocks] [\cite=mattern-clocks].

There is a simple and efficient algorithm to compute the vector clocks for all nodes at all times in

[formula]

Latencies in Social Network Data

We now examine these latency measures in the context of real social communication data. Again, we focus on our university e-mail dataset, but in the final section we also discuss our other datasets -- the Enron corpus and the communications among Wikipedia editors.

For the university e-mail study we start from the complete set of communication events among the 8160 faculty and staff at a large university over two years, and then we preprocess this set in two ways. First, it is an interesting open question to consider the appropriate role for messages with large recipient lists in this type of analysis; however, for the present study, we eliminate them by considering only messages with at most c recipients other than the sender, for small values of c (ranging between 1 and 5). Messages with a single recipient account for 82% of all messages, while messages with at most c = 5 recipients account for 97%; the results here are stable across all these values of c, and in this discussion we focus on the case of single-recipient messages.

Our second type of preprocessing is the follows. Because not all members of the full population used their e-mail addresses actively during this time, we focus on the q-fraction of highest-volume e-mail users in this set, for various values of q. In this discussion we use q  =  .20, defining a set in which each user sent or received a message at least approximately once an hour during working hours for the full two-year time period. However, the results discussed here are robust as q varies over a wide range.

We begin our analysis by considering the distribution of information latencies -- in other words, measuring how far out-of-date the rest of the world is with respect to different nodes. For a time difference τ, we define the ball of radius τ around node v at time t, denoted Bτ(v,t), to be the set of all nodes whose latency with respect to v at time t is ≤  τ days. Now, for fixed t, the distribution of ball-sizes over nodes can be studied using a function ft(τ), defined as the median value of |Bτ(v,t)| over all v; this is simply the number of people who are within τ days out-of-date of a typical node. In Figure [\ref=fig:latencies] the lower curve plots (on a log-linear scale) the average value of ft over 21 fixed values of t, equally spaced around one week to account for weekly variation. We see that after an initial 12-hour ramp-up, the the number of people at τ days latency from a typical node grows in an approximately piecewise exponential fashion. The effect is that for a typical person v in this community, there are only about 12 other people who are within a day and a half out-of-date with respect to v, while there are over 200 people within four days.

Extending this curve until the ball-size is half the community, we find that the median latency between node pairs is 7.5 days. Now, to put the quantity 7.5 days in context, we can compare it to other possible measures of "distance" in the network. If we look at unweighted distances (i.e. simple "hop-counts") in the communication skeleton G, we find that the median distance between nodes is 3, a very small number characteristic of the small-world properties of such networks [\cite=travers-swn] [\cite=watts-strogatz]. But the simple fact that people in this community are "three degrees of separation" apart cannot be directly translated into statements about the potential for information flow, since that requires the temporal data that forms the basis for our definition of information latency.

With temporal data in hand, we see that latency depends both on the variation in who people communicate with and also the on the variation in how frequently they communicate. We can thus put the observed quantities in perspective by holding the frequency of communication fixed, and studying how the latencies change as we vary the choice of communication partners. In particular, we compare the observed information latencies with the results of a randomized baseline, as follows. Suppose that we simulate the sequence of e-mail exchanges, except that for each communication event, we have the sender contact a uniformly random person rather than their true recipient in the data. In this way, the potential for information flow occurs at the speed of a random epidemic, rather than according to the actual trace of e-mail communication. The randomized latencies are generally shorter than the real latencies, and the upper curve in Figure [\ref=fig:latencies] plots the median ball-sizes for this baseline.

These ball-sizes also grow in a roughly piecewise exponential fashion, and the median latency among node pairs under randomized communication is 4.6 days. Interestingly, the local exponential growth rates of the real latencies and the randomized baseline are roughly the same after about 36 hours; it is the faster exponential "head start" within this first 36 hours that allows the randomized baseline to spread so much more quickly. Essentially, under the real communication pattern, the typical person resides in a kind of temporal "bubble" at the 36-hour radius, in which they can only be aware of information from about 12 other people. With randomized communication, on the other hand, information breaks out quickly to many people; the median ball-size at 36 hours is already 50 people. This initial difference plays a significant role in the different ball-sizes multiple days later.

Open worlds vs. closed worlds

Any dataset of communicating people V will be typically embedded in some much larger, unobserved set V'. If we could watch the communication in this larger set V', the latencies even just among nodes in V would decrease, due to quick paths between members of V that snake in and out of V'  -  V. We wish to understand this effect, so that we know how to interpret latencies as we measure them in the "closed world" V rather than the "open world" where V is embedded in a larger V'. In sociology, this is known as the boundary specification problem [\cite=kossinets-missing-data] [\cite=laumann-boundary], and it is inherent in essentially any study of a social network embedded in some larger world.

We can address the effects of this issue in our context as follows. Since we are studying the q-fraction of most active users in our university e-mail set, we can ask how median latencies differ depending on whether we study this q-fraction in isolation, or embedded in the full set of faculty and staff (the q  =  1.0 fraction). We show this in Figure [\ref=fig:open-world]: the upper curve plots median latency as a function of q when the most active q-fraction is observed on its own, and the lower curve plots the median latency in the same set when it is observed embedded in the full community. For extremely small values of q, the effect is considerable, but once q exceeds 0.1, the effect becomes surprisingly negligible.

In addition to providing validation for the analysis of different q-fractions in isolation, we believe this implicitly supports a broader type of approximation -- specifically, when an active e-mail network implicitly defines a natural community on its members (as in the university community in this case), it suggests ways to reason about it as a free-standing object despite the fact that it is embedded in the unobservable global e-mail network.

Quantifying the strength of weak ties

In a paper that has been very influential in sociology, Granovetter proposed that weak ties -- connections to people who form weaker acquaintance relationships, rather than close friendships -- play an important role in conveying information to each of us from parts of the social network that are inaccessible to our circles of close friends [\cite=granovetter-weak-ties]. As a concrete example, Granovetter found that people very often reported receiving information leading to new jobs not from close friends, but from more distant acquaintances; the close friends were perhaps more motivated to help in tracking down job leads, but the more useful information came through the distant acquaintances.

Granovetter formalized this by introducing a parameter that we call the range of an edge e  =  (v,w), defined to be the unweighted shortest-path distance in the social network between v and w if e were deleted; the range is thus the (unweighted) length of the shortest "alternate path" between the endpoints [\cite=granovetter-weak-ties] [\cite=watts-swn-book]. Most edges in a typical social network will have range two, indicating that v and w have at least one friend in common. Granovetter's argument was that edges of range greater than two are generally weak ties -- i.e., edges connecting us to acquaintances with whom we have less frequent communication -- and that these long-range edges are the sources of important information to their endpoints. However, he noted [\cite=granovetter-swt-revisited] that despite interview-based methods to explore this principle, it has been an open question to provide quantitative evidence for it on social-network datasets.

We argue here that our vector-clock analysis can provide evidence for this phenomenon. If we recall the algorithm that computes the vector clocks, the basic step is to update the clock of a node w when it receives a message from some other node v. Let us define the advance in w's clock to be the sum of coordinatewise differences between φw before the update from v and φw after the update from v. Intuitively, the advance is then the potential for new information about the rest of the world that w has gained as a result of this single communication with v -- a way of formalizing the type of information-flow that Granovetter's work addresses. To get at his observation, we can thus ask: if (v,w) is an edge in the communication skeleton G of range greater than two, does each communication from v result in an unusually large advance to w's clock?

While this is a subtle effect to capture, we see evidence for precisely this in Figure [\ref=fig:weak-ties]. As a function of edge range r, we plot the median clock-advance per message over all edges in G at the given range r (the open circles in the plot), as well as the 25th and 75th percentiles (the vertical line segments). Due to the active communication within this group over two years, there are no edges of finite range larger than four; the infinite-range edges are bridges whose removal would disconnect the network. (Since one side of each of these bridges is typically an extremely small set of nodes, it is not necessarily surprising to see a typical clock advance that is smaller than the case of range 4.) In summary, we see that the clock-advance per message increases with edge range, particularly for edges of range 4, thus suggesting that long-range bridges can indeed be effective in transferring information from otherwise distant parts of the network.

Backbone Structures

Having considered methods for analyzing the notion of out-of-date information, we now use this to study the second issue mentioned at the outset -- the structure of fast indirect paths -- by introducing a concept that we call the backbone.

Defining the backbone

To develop this idea, we start by recalling the observation from the example in Figure [\ref=fig:timestamps], where the direct A-B edge was a slower conduit for potential information from A to B than the indirect path A-C-B. Let us say that an edge (v,w) in the communication skeleton G is essential at time t if the value φw,t(v) is the result of a vector-clock update directly from v, via some communication event (v,w,t') where t'  ≤  t. In other words, the edge is essential if w's most up-to-date view of v is the result of direct communication from v, rather than a sequence of updates along an indirect path from v to w. Thus for example, in Figure [\ref=fig:timestamps], consider all edges linking to B in the communication skeleton: the edges (C,B) and (E,B) are essential at Friday 5pm, but the edges (A,B) and (D,B) are not.

We define the backbone Ht at time t to be the graph on V whose edge set is the collection of edges from G that are essential at time t. (Although Ht is a directed graph, we will also sometimes study properties of it as an undirected graph, simply by suppressing the directions of the edges.) Thus the backbone reflects those communications responsible for all nodes' up-to-date views at a given time t -- i.e., those that are not "bypassed" by some indirect path.

As a visual illustration, Figure [\ref=fig:vis] depicts a small part of a backbone Ht computed from the university e-mail data, drawing only the portions induced on a particular node v and all nodes within 48 hours latency of v.

An aggregate backbone

The backbone is defined at each point in time via vector-clocks; but it is also useful to have a single graph that summarizes in an analogous but simpler way the "aggregate" communication over the full two-year period, and to be able to compare this simplified structure to the backbones defined thus far. We can define such an aggregate structure by approximating communication between pairs of nodes as perfectly periodic. For each edge (v,w) in the communication skeleton G such that v has sent ρv,w  >  0 messages to w over the full time interval

[formula]

Density and node degrees of the backbones

While the communication skeleton is a fairly dense graph, we find that the backbones and the aggregate backbone are surprisingly sparse -- in other words, from the point of view of potential information flow, a significant majority of all edges in the social network are bypassed by faster indirected paths.

In particular, Figure [\ref=fig:density] shows the average degree in the instantaneous backbones Ht as a function of time. Note that there are clear boundary effects as the vector clocks get "up to speed," but after this initial phase the average degree stabilizes to approximately 13 even as the backbone itself changes over time. The aggregate backbone H* is sparser still: its average degree is approximately five (the horizontal line in Figure [\ref=fig:density]). For comparison, the average node degree in the communication skeleton is approximately 50. In summary, even in this community of active users of e-mail, the typical person has only five contacts that are not bypassed by shorter paths in steady-state over a long time period.

The fact that the instantaneous backbones Ht are roughly 2.5 times as dense as the aggregate backbone indicates the local burstiness of communication in the network: at any particular point in time, people have essential communication with certain contacts that are not sustained in steady-state over the full two-year interval. It thus becomes natural to ask how much overlap there is between the instantaneous backbones Ht and the sparser aggregate backbone. We find in fact that the overlap is substantial: each backbone Ht, on average, contains roughly 3 / 4 of the edges from H*. Of course, which particular edges of H* appear in any one Ht varies considerably with t. Thus, it is reasonable to think of the instantaneous backbones Ht as roughly consisting of a large but varying piece of the aggregate backbone, supplemented with transient edges whose membership in the backbone changes more rapidly over time.

Considering the backbone also sheds further light on the role of high-degree nodes in the social network. It has been argued that high-degree nodes play a crucial function in the structure of short paths in unweighted graphs [\cite=albert-scale-free-deletion]. It has also been argued, however, that the importance of these "hubs" diminishes considerably once temporal effects are taken into account [\cite=gibson-scheduling]. We find support for both arguments: high-degree nodes in the full communication skeleton G indeed have many incident edges in the aggregate backbone; however the fraction of a node's edges that are declared essential strictly decreases with degree. As Figure [\ref=fig:degree-scatter] illustrates, nodes of degree k in G have an average degree of approximately k0.6 in the aggregate backbone H*; thus, the fraction of a node's edges that are essential is decreasing in its degree as k- 0.4. A corresponding effect holds for the instantaneous backbones Ht, where nodes of degree k in G have average degree approximately k0.65, an exponent that remains stable over time after an initial start-up period. Thus the backbones have a kind of "leveling" effect on the degrees, in which the spread between low and high degrees is contracted faster than just proportionally when we move from G to its backbones.

Structure of the backbone

Intuitively, the backbone is trying to balance two competing objectives: representing edges that span different parts of the network, which transmit information at long ranges; and representing very rapidly communicating edges, which will typically be embedded in denser clusters and transmit information at short ranges over quick time scales. In fact, we will see in Figures [\ref=fig:range] and [\ref=fig:embed] that the mixture of edges in the backbone achieves precisely a version of this trade-off. For this discussion, we view the backbones as undirected graphs simply by suppressing the directions of the edges.

In Figure [\ref=fig:range], we show the proportion of edges from G that belong to the backbones, as a function of their range. (Recall that the range of an edge e is defined as the distance between the endpoints of e, when e itself is deleted.) The lower curve depicts the aggregate backbone, while the upper curve depicts the average over instantaneous backbones. In each case, we see that there is an underrepresentation of edges of the intermediate range 3, with a greater density at the two extremes of range 2 and range 4. The large proportion of range-4 edges in the backbone is another reflection of the strength-of-weak-ties principle discussed earlier -- long-range edges serve as important conduits for information. To understand the picture at the other extreme, with edges of range 2, it is useful to further refine this set of edges using the notion of embeddedness.

We define the embeddedness of an edge to be, roughly, the fraction of its endpoints' neighbors that are common to both. Formally, for an edge e  =  (v,w), let Nv and Nw denote the sets of neighbors of the endpoints v and w respectively. We define the embeddedness of e to be [formula]. Thus, highly embedded edges intuitively occupy dense clusters, in that their endpoints have many neighbors in common. We see in Figure [\ref=fig:embed] that highly-embedded edges are also overrepresented in both the aggregate and instantaneous backbones. This may be initially surprising, since edges of large embeddedness have many possible two-step paths that could short-cut around them; their presence in the backbone is thus a reflection of the generally elevated rate of communication that takes places on such edges.

Taken together, then, these results on range and embeddedness indicate a striking sense in which the backbone balances between two qualitatively different kinds of information flow: flows that arrive at long range over weaker ties, and flows that travel quickly through densely clustered regions in the network.

Varying Speed of Communication

We note that although the social communication patterns we are studying arise organically (rather than being centrally designed), one can nevertheless study how the resulting latencies depend on local variations in communication styles. One could ask this question in the context of communication within a large organization, for example: how do individuals' decisions about communication strategies affect the overall rate of potential information flow in the organization? Of course, analysis of such questions can also potentially provide insight into the design of information-spreading mechanisms in engineered networks as well [\cite=demers-epidemic].

In particular, we study what happens to information latencies when each node keeps its set of contacts the same, but varies the relative rates of its communication with these contacts. Suppose we assume the communication skeleton G represents the complete set of potential communication partners for each person, and we allow people to change the individual rates at which they send messages to these partners, while keeping their total daily volume fixed. Are there simple ways to change individual rates that will reduce the shortest-path delays among pairs in the aggregate backbone?

As a baseline for comparison, we can consider the optimal reduction in delay, given a central planner with complete knowledge of the network. Here is a concrete way to formalize this optimization question in general. We are given a directed graph G, with a total rate ρv for each node v. We are also given a set S of pairs of nodes in G whose shortest-path delays we want to reduce. Each node v can choose a rate ρv,w at which to communicate to each of its neighbors w, subject to the constraint that [formula]. These rates define delays δv,w  =  T  /  ρv,w as in our construction of the aggregate backbone. (Rates ρv,w can be set to zero, in which case the resulting edge (v,w) is taken to have infinite delay.) Now, the question is: for a given bound δ, can we choose rates for each node so that the median shortest-path delay between pairs in S in the aggregate backbone is at most δ?

As formulated, this optimization problem is intractable.

The delay minimization problem defined above is NP-complete.

Proof Sketch. We reduce from the 3-SAT problem. Given a set of variables x1,...,xn and clauses to satisfy, we construct a graph G, pairs S, and node rates ρv as follows. For each variable xi, we construct three nodes ui,vi,wi with edges (ui,vi) and (ui,wi). For each clause Cj, we construct nodes sj and tj; then, for each variable xi in clause Cj, we add edges (sj,ui) and (vj,tj) if xi occurs positively in Cj, and we add edges (sj,ui) and (wj,tj) if xi occurs negatively in Cj. Each node is given a rate of 1. Finally, we evaluate the median shortest-path delay for the pairs S  =  {(sj,tj)}.

Now, if there is a satisfying truth assignment, then we can put a rate of 1 on edge (ui,vi) if xi is set to True, and a rate of 1 on edge (ui,wi) if xi is set to False. We can also put a rate of 1 on edges (sj,ui) where xi is a variable that satisfies Cj. This makes all shortest-path delays between pairs in S equal to 3; conversely, if the median shortest-path delay between pairs in S can be made equal to 3, then each pair in S must have delay 3, in which case a satisfying assignment can be determined from the paths that are used.

Load-leveling vs. load-concentrating

While this intractability shows the difficulty in optimally accelerating communication, a more realistic goal is to consider simple local rules by which individuals in a network might vary their rates of communication so as to influence the potential for information flow. A basic qualitative version of this question is the following: for accelerating potential information flow, is it better to talk even more actively to one's most frequent contacts, or to balance things out by increasing communication with the less frequent contacts? We could refer to the former strategy as load-concentrating, since it pushes more traffic onto the already-high-volume edges, and we could refer to the latter strategy as load-leveling, since it tries to level out the traffic across edges.

We can study this in the university e-mail data by choosing a re-scaling exponent γ and modifying the rates of communication on the edges emanating from each node v, changing ρv,w to ργv,w and then normalizing all rates from v to keep its total outgoing message volume the same. Varying γ thus smoothly parametrizes a family of different strategies, with values γ  >  1 corresponding to load-concentrating strategies -- since already-large rates are amplified -- while values γ  <  1 correspond to load-leveling strategies.

In Figure [\ref=fig:gamma], we show the effect of these strategies on the median shortest-path delay in the aggregate backbone. We note, first of all, the interesting fact that γ  =  1 is close to the best possible for shortest-path delays; in other words, the existing rates of communication are close to optimal, in terms of potential information flow, over this class of strategies. However, there is still room for improvement in the shortest-path delay: the optimal median, over all γ, occurs at γ*  ≈  1.2. The fact that γ*  >  1 indicates an interesting and perhaps unexpected result: that increasing the rate of communication to the most frequent contacts actually has the effect of reducing shortest-path delays -- a result at odds with the intuition that making stronger use of infrequent contacts and weak ties is the way to reduce latency.

Node-dependent delays

There is an extension of the model that sheds further light on this finding. Suppose we extend the notion of delay to have not just delays δv,w on each edge, but also a fixed delay of ε at each node, so that the total delay on a path becomes the sum of the edge and node delays. In other words, as information flows it incurs additional delays from each node that handles it.

Naturally, as ε increases, there is a larger penalty for paths that take more hops, and minimum-delay paths increasingly come to resemble those of minimum hop-count. This leads to a denser backbone, as fewer edges are rendered inessential. The value of γ at which network latency is optimized decreases with ε, crossing γ* = 1 at ε  ≈  4 days. Thus, as the speed of diffusion pathways is determined increasingly by node-specific (rather than edge-specific) delays, the backbone becomes denser, and the importance of quick indirect paths diminishes. Moreover, as node delays increase, the optimal re-scaling of communication for reducing network latency transitions from load-concentrating to load-leveling.

Conclusions

The basic definitions of social network analysis have been primarily built on graph-theoretic foundations rooted in unweighted graphs. Here we have explored how this perspective changes when one makes integral use of information about how nodes communicate over time. Rather than explicitly tracking the content of this communication, we develop structural measures based on the potential for information to flow; in this way, we can get at elusive notions around the network's everyday rhythms of communication.

With this view, some of the direct connections in the network become much longer, due to low rates of communication, while other multi-step paths become much shorter, due to the rapidity with which information can flow along them. We find that adapting the notion of vector-clocks from the analysis of distributed systems provides a principled way to measure how "out-of-date" one person is with respect to another, and we find that the sparse subgraph of edges most essential to keeping people up-to-date -- the backbone of the network -- provides important structural insights that relate to embeddedness, the role of hubs, and the strength of weak ties. Finally, this style of analysis allows us to study the effects on information flow as nodes vary the rate at which they communicate with others in the network, ranging from strategies in which communication is concentrated on heavily-used edges to those in which it is leveled out across many edges.

This style of analysis is applicable to any setting in which a group of individuals is engaged in active communication with the goal of exchanging information, and when there is data available on the temporal sequence of communication events. As discussed earlier, we have also explored the measures defined here in other e-mail datasets (the Enron corpus), as well as in settings that are quite different from e-mail networks -- in particular, we have applied vector-clock and backbone analysis to the communications among admins and other high-activity editors on Wikipedia, using edits to user-talk pages as communication events. Wikipedia is a setting where it is particularly easy to get public data with complete communication histories, but it is also representative of communities that maintain themselves through on-line communication and coordination (large open-source projects and large media-sharing sites are other examples).

Although the dynamics and patterns of communication in all three of our datasets are quite different, we find that a large number of the qualitative findings discussed for the university e-mail domain carry over to the other settings studied, including the sparsity of the aggregate and instantaneous backbones and the variation in node degrees. In particular, the typical aggregate backbone degrees around 5 and the recurring sub-linear "compression" of degrees -- where nodes of degree k in the full skeleton have typical backbone degree ~  kc for c  ≈  0.5-0.6 in all three datasets -- are common patterns that seem to call for a deeper theoretical explanation. On the other hand, compared to the university e-mail dataset, we find that the "core" of active communicators is much smaller in both the Enron corpus (since it is data from a limited set of employees' mailbox folders) and in Wikipedia (due to the specifics of community dynamics), and this makes the range of an edge in the unweighted communication skeleton harder to interpret and to correlate with other measures for both these other domains. In a sense, this is natural: principles about long-range edges and their effects are derived from properties of large populations with natural sub-communities -- as we find in the university e-mail data -- and it is not clear that long-range edges carry the same meaning in much smaller populations.

In general, we see the analysis framework proposed here as a way of comparing the different kinds of communication dynamics within different communities. Further investigation of these notions could ultimately shed light on the principles that govern the dynamics of different types of information, and how these principles interact with the directed, weighted nature of social communication networks.