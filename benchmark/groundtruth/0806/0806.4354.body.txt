Some improvements of the ART method for finding transition pathways on potential energy surfaces

Introduction

The Activation-Relaxation Technique (ART) [\cite=BM96] [\cite=BM98] [\cite=MB98] [\cite=BM99] [\cite=MB99] [\cite=MB00] [\cite=BM01] is a powerful method for searching saddle points and transition pathways of a given potential energy surface (PES). Search methods for saddle points and transition pathways can actually be classified in two main categories. In the first class of methods, one assumes that two local minima of the PES are known. The main objective of the methods in this class is to find the minimum energy path to go from one local minimum to the other one. The Replica Chain method [\cite=EK87] [\cite=AS97], the Nudged Elastic Band [\cite=MJ94] [\cite=neb_revue] [\cite=HJ00], the String method [\cite=e_ren_eve] [\cite=string_ext], the Transition Path Sampling [\cite=dellago1] [\cite=dellago2] [\cite=dellago3] [\cite=dellago4] and the Discrete Path Sampling [\cite=W02] are some methods belonging to this class (note that the Nudged Elastic Band method has been generalized to the finite temperature setting [\cite=crehuet], as well as the String method [\cite=string_Tpos]). In the second class of methods, one assumes that only one local minimum of the PES is known. The aim of methods in this class is to find a saddle point of the PES, from which the exploration will be pursued toward a different local minimum, yielding a transition path. Probably the first method in that class is the EigenVector Following method [\cite=CM81]. The Dimer method [\cite=HJ99], the Conformational Flooding method [\cite=grubmuller], the Hyperdynamics method [\cite=hyper1] [\cite=hyper2] [\cite=miron], the Parallel Replica method [\cite=replica], the Temperature Accelerated method [\cite=tad1] [\cite=tad3] [\cite=tad2], the Scaled Hypersphere Search method [\cite=Ohno1] [\cite=Ohno2] [\cite=Ohno3], are other examples. In this article, we study the Activation-Relaxation Technique, which belongs to this second class. We focus here on the zero temperature case, the so-called ART nouveau (ARTn) method [\cite=MB00] [\cite=MM00] [\cite=mousseau], and do not consider the finite temperature case, the so-called POP-ART method [\cite=popart].

The ART method is composed of two main steps, the activation step and the relaxation step. The activation step consists in moving the system from a local minimum to a saddle point. The relaxation step consists in relaxing the system, from the computed saddle point, to another local minimum. Of course, this relaxation step is very fast (and easy to perform) in comparison with the activation step.

The activation step itself can be divided into two substeps. The first substep aims at finding some region of the PES with one direction of negative curvature, which hopefully contains a first order saddle point, and that we will call the "attracting region". The basic idea for finding a point on the PES with one direction of negative curvature is to choose a random vector r, and next to repeat the two following operations: (i) move the system according to r, (ii) relax the system in the hyperplane orthogonal to r, until a point with one direction of negative curvature has been found (see section 3 for details). The second substep consists in finding a saddle point in the reached attracting region. From a numerical viewpoint, these two substeps are of very different nature. In this article, we focus on the second substep, namely the local convergence to a saddle point, starting from a configuration with one direction of negative curvature. In section [\ref=sec:algo], we present a simple, prototypical, ART-like algorithm, which has better local convergence properties than existing ones. Loosely speaking, this algorithm is optimal in the principal direction of negative curvature, but suboptimal in the transverse directions. This is why this algorithm has to be considered as a prototype, on the basis of which more complex numerical strategies can be elaborated. Some numerical results are reported on in section [\ref=sec:numerics], where we consider the problem of vacancy diffusion in crystalline materials. The numerical results obtained on this problem demonstrate the efficiency of our approach. We gather in the Appendix a convergence and robustness analysis of this new algorithm.

A new type of ART-like algorithms

From a mathematical viewpoint, a PES for an isolated molecular system with N atoms is a function [formula], N being the number of atoms in the system and [formula] the group of rigid body movements which act on [formula] in the following way: for all [formula], and for all [formula],

[formula]

This viewpoint takes into account the fact that the potential energy E(X) of the system is invariant upon rigid body movements. In the simulation of the condensed phase, artificial periodic boundary conditions are usually introduced. In this case, the system is translation invariant, but not rotation invariant, and a PES then has to be regarded as a function [formula], where [formula] is a 3N dimensional torus.

For our purpose, namely for the analysis of ART-like methods, there is no restriction in assuming that the PES under consideration is a function [formula] with isolated critical points. For [formula], we denote by [formula] the gradient of f at the point x and by [formula] the hessian of f at the point x. For [formula], let [formula] be the eigenvalues of H(x) counted with their multiplicity, and let [formula] be an orthonormal basis of associated eigenvectors.

Contrarily to second order methods, such as the one proposed in Ref. [\cite=CM81], the ART method does not rely on a complete knowledge of the spectral decomposition of the Hessian matrix. Instead, it only makes use of the direction of negative curvature. We consider here various modifications of the ART method, differing from the original ART algorithm by the fact that they also make use of the associated eigenvalue (i.e. of the curvature itself). A prototype of such algorithm reads

[formula]

where λc  >  0 and μt  >  0 are fixed numerical parameters, and [formula] is the orthogonal projector on the hyperplane [formula].

In order to clarify the behavior of the algorithm ([\ref=eq:algo1]) and the role of the numerical parameters λc  >  0 and μt  >  0, let us consider the simple example of a quadratic function f:

[formula]

with [formula] and [formula]. In this simple case, ([\ref=eq:algo1]) reads as a system of d decoupled scalar equations

[formula]

yielding

[formula]

where x0 is the initial guess of the algorithm.

Assume that all the λj are different from zero. In this case, f has a unique critical point (the origin), and the algorithm converges to this critical point for all choices of the initial guess if and only if

[formula]

This means that if the algorithm converges, it will be toward a critical point with Morse index equal to one (a first order saddle point). Conversely, if 0 is a saddle point with Morse index equal to one (i.e. if [formula]), the algorithm will converge to zero if and only if λd  <  2μ- 1t. The numerical parameter μt controls the convergence in the hyperplane x1 = 0. If μt is too small, convergence will be slow, if μt is too large, the algorithm will be unstable. Note that if λ1  <    -  λc, convergence in the e1 direction (the direction of negative curvature) will be obtained in a single iteration, while linear convergence will be observed if -  λc  <  λ1  <  0. The role of the parameter λc is to prevent the algorithm, when applied to a non-quadratic energy landscape, from becoming unstable in the region where |λ1(xk)| is small.

Let us now come back to the case of practical interest when f is the PES of some molecular system. As mentioned in the introduction, we focus here on the local convergence properties and henceforth assume that the iterates have reached the neighborhood of a first order saddle point. One can then prove (see the Appendix) that the algorithm ([\ref=eq:algo1]) converges to the saddle point, quadratically in the principal direction of negative curvature, and linearly in the perpendicular directions. Let us note that quadratic convergence is obtained under the assumption that the smallest eigenvalue λ1(xk) of the hessian matrix H(xk) and the corresponding eigenvector v1(xk) are computed exactly. However, a key ingredient in ART-like algorithms is that λ1(xk) and v1(xk) are computed approximately, by iterative methods. Thus, for instance, the eigenelement (λ1(xk),v1(xk)) can be computed by Lanczos or Arnoldi methods, which are based on repeated evaluations of matrix-vector products of the form H(xk)  v. In turn, such matrix-vector products can be approximately computed using a finite-difference formula, such as the first-order formula

[formula]

or the second order formula

[formula]

In summary, the eigenelement (λ1(xk),v1(xk)) is computed approximately by repeated evaluations of forces [formula] for a collection of configurations y close to the reference configuration xk. However, one can prove (see again the Appendix) that the algorithm ([\ref=eq:algo1]) is robust, in the sense that it can accomodate approximate evaluations of (λ1(xk),v1(xk)). The price to pay is a lower convergence rate in the principal direction of negative curvature.

The prototypical algorithm ([\ref=eq:algo1]) is not far from being optimal in the direction of negative curvature, even in presence of numerical errors in the evaluation of (λ1(xk),v1(xk)). On the other hand, it is clearly suboptimal in the transverse directions, where it behaves as a basic fixed step-size gradient. Improvements can be obtained by resorting to conjugate gradient, quasi-Newton or trust-region methods in the transverse direction [\cite=NW00]. It is also possible, in principle, to take into account the p lowest eigenvalues of H(xk) obtained from the Lanczos or Arnoldi partial diagonalization procedure, to construct a surrogate function that will provide a better model for f in the neighborhood of xk. Such improvements of the current ART-like algorithms will be considered in a future work.

Numerical results: Migration of point defects in α-iron

In this section we discuss the practical implementation of algorithm ([\ref=eq:algo1]) in the case of basic defects in α-iron: small self interstitial (SIA) and vacancy (VAC) clusters (1 to 3 defects). The crystal of α-Fe is modeled by the EAM potential developed by Mendelev et al. [\cite=mendelev2003] [\cite=mendelev2004] which has been the most widely used in recent years to study interstitial loops  [\cite=terentyev2007] [\cite=terentyev2008]. Marinica et al. have previously used the same potential and the standard ARTn method to test and reveal the energy landscape of small interstitial clusters (1 to 4 self-interstitials) in α-Fe [\cite=marinica]. It therefore gives us a good basis for comparison. The crystal consists of 1024±  n atoms (n=1,2,3).

Starting from a local minimum configuration, the first stage of the activation step is to push the system out of the basin. In order to do this, the system is slightly deformed using

[formula]

where Δx is a fixed normalised deformation of the system, which is for the moment chosen randomly, and μA is a user-defined fixed step. Possibilities for well-chosen initial deformations will be explored in future work. In this paper we use the defect centered deformation [\cite=marinica] instead of global deformation. This means that the random deformation Δx is applied only on the atoms within a certain radius around the defect. The reason for this choice is that the efficiency of the algorithm in the defect centered deformation is independent of the size of the system and provides the best rate of successful to unsuccessful activation processes (this will be elaborated on later on in the section). At each iteration the system is relaxed in the hyperplane orthogonal to the direction Δx. If, after this relaxation, the lowest eigenvalue is still positive, we continue the deformation. As soon as λ1(xk) becomes sufficiently negative (λ1(xk)  <  λd for some threshold λd  <  0), we move onto the next stage of the activation process. The threshold is used in order not to be misguided by numerical errors of the eigenvalue calculation. The lowest eigenvalue is computed using the Lanczos algorithm with 15 iterations, a small number compared to the size of the Hessian matrix (recall that [formula], where N is the number of atoms in the system).

Once the system is out of the basin, we begin to move the system towards the saddle point, in the hope of following the minimum-energy reaction path. The previously used method (slightly modified ARTn) [\cite=marinica] for this stage is:

[formula]

where μa is a user-defined constant and [formula] ensures that the step size gets smaller as we approach the saddle point. The direction of the eigenvector v1 is chosen such that it points in the same direction as the force i.e. [formula]. This is then followed by a relaxation in the hyperplane, which is discussed in the next paragraph. Algorithm ([\ref=eq:algo_old]) was an improvement to some previous methods [\cite=MB00]. However it has several drawbacks. The constant parameter μa in the algorithm needs to be defined according to the PES in study, and even so may be suited for some saddle point searches but not for others (very tightly positioned saddle points may force μa to be small for the whole system, which may in turn impede results when the surface becomes relatively smooth). With v1(xk) unitary and μa fixed, it is clear that decreasing the step size according to the number of iterations is not ideal. In fact it would be better to use the first and second derivative information of the energy surface. Taking as a simple example the function ([\ref=eq:quad]) with d  =  2 (solution x* at the origin), we can position ourselves at a point xn where the displacement from x* is in the direction of negative curvature. The further along this direction we are positioned, the more iterations would be needed to approach x* since algorithm ([\ref=eq:algo_old]) would take smaller and smaller steps. In this simple case the proposed algorithm ([\ref=eq:algo1]) would jump to the solution in one step.

The minimization of the forces in the orthogonal hyperplane consists of following a fixed step steepest descent method but with the force projected onto [formula], until the forces in this plane are zero or a maximum number of steps M is reached. In the case where we reach a configuration xk with λ1(xk)  >  0, we restart and depart from the current local minimum and make a displacement in a different randomly chosen direction (see Ref. [\cite=marinica] for details on relaxation). The number of minimization steps taken is limited due to the potentially costly force evaluations. The success of the ARTn method however relies strongly on good minimization in the hyperplane. Possible improvements including trust region and conjugate gradient methods will be explored in future work.

The main contribution of algorithm ([\ref=eq:algo1]) is the step taken in the direction of the negative curvature. In the numerical results reported later on in this section, we have implemented this step in the attracting region. On the other hand, we continue to use equation ([\ref=eq:basin]) for leaving the basin and the same algorithm as described in the previous paragraph for the relaxation in the hyperplane.

The efficiency of these ART-type algorithms depends on two main points: the number of force evaluations required during the activation stage and the ratio of successful to unsuccessful searches. The failure to find a saddle point can be determined in several ways. If minimization in the hyperplane is not done sufficiently well, the system risks climbing the energy surface too high. Once settled at a saddle point, it could be one which is not associated with the local minimum where the activation process began. It could also be the case that we fall on a saddle point where the energy is lower than the starting point, which is an immediate indication that we have overlooked at least one adjacent saddle point of the local minimum and fallen beyond. Finally, another sign of failure is when relaxation in the hyperplane yields a positive λ1(xk), in which case we have reached another local minimum. It remains a challenge to be certain that a saddle point falls in the first of the three categories mentioned. For the purposes of this study therefore, we will only reject stationary configurations if the energy is below that of the initial local minimum or if we are in fact at another minimum configuration.

Comparisons between algorithms ([\ref=eq:algo_old]) and ([\ref=eq:algo1]) are done on interstitial and vacancy defects using the parameters shown in Table [\ref=tab_params]. The results are shown in Table [\ref=tab_results]. Over a total number of 1000 successful events, [formula] is the average number of force evaluations per activation process and η is the ratio of successful events to unsuccessful events. It can be observed that the proposed algorithm ([\ref=eq:algo1]) improves performance by a large margin both in terms of the average number of force evaluations and the proportion of successful events. The elimination of the constant factor μa in algorithm ([\ref=eq:algo_old]) not only makes the algorithm more efficient but also more versatile. It may be applied to a wide range of potential energy surfaces without the need for parameter manipulation.

Aknowledgements

This work was partially supported by the Agence Nationale de la Recherche (LN3M project, contract No. ANR-05-CIGC-0003). The authors are grateful to Normand Mousseau for helpful discussions.

Appendix: Local convergence analysis

In this Appendix, we prove that algorithm ([\ref=eq:algo1]) is locally convergent, even when the eigenelement in the direction of negative curvature is approximately computed.

Let [formula] such that [formula] and [formula]. We introduce the notation [formula], [formula], [formula],

[formula]

Note that

[formula]

In the analysis below, we often use that zk  =  O(|ek|).

We consider algorithm ([\ref=eq:algo1]), where the eigenelement (λ1(xk),v1(xk)) is now computed approximately. The resulting algorithm, that we analyze below, reads

[formula]

where λ̃1(xk) and ṽ1(xk) are approximations of λ1(xk) and v1(xk):

[formula]

where the errors αk and βk are supposed to be small (i.e. |αk|  ≪  1 and |βk|  ≪  1). We assume that |ṽ1(xk)|  =  1. Note that we have made no assumption on the hessian matrix H(xk). Hence, the errors αk and βk take into account both a possible approximation in the computation of H(xk) (see ([\ref=eq:err_H1]) and ([\ref=eq:err_H2])), and an approximate partial diagonalization of this matrix (by a Lanczos or Anoldi algorithm).

We assume that [formula] and that xk is close enough to [formula] such that λ1(xk)  ≤    -  λc for all k sufficiently large. We also assume that the error βk is small enough such that λ̃1(xk)  ≤    -  λc for all k sufficiently large.

It follows from ([\ref=eq:algo2]) that

[formula]

and

[formula]

Assuming that f is [formula] with bounded first and second derivatives, it holds

[formula]

Besides, using perturbation theory, one obtains

[formula]

From ([\ref=eq:exp_grad]) and ([\ref=eq:exp_v1]), we deduce

[formula]

Inserting these equations in ([\ref=eq:zk]) and ([\ref=eq:yk]), we obtain

[formula]

on the one hand, and, on the other hand,

[formula]

As [formula] and as [formula] is positive definite on [formula], we get

[formula]

Thus, if μt  <  2 / λd, we infer from ([\ref=eq:conv_zk]) and ([\ref=eq:conv_yk]) that

[formula]

with [formula]. Under the assumption that the errors αk and βk are uniformly bounded by a small constant, this proves that algorithm ([\ref=eq:algo2]) locally converges, and that the convergence speed is at least linear.

In the case when the eigenelement (λ1(xk),v1(xk)) is exactly computed, algorithm ([\ref=eq:algo2]) reduces to algorithm ([\ref=eq:algo1]). We hence have proved that algorithm ([\ref=eq:algo1]) locally converges, and that this convergence is robust with respect to errors in the computations of the lowest eigenvalue (and the associated eigenvector) of H(xk).

Estimates for the convergence of algorithm ([\ref=eq:algo1]) are readily obtained from ([\ref=eq:conv_zk]) and ([\ref=eq:conv_yk]), by setting αk  =  0 and βk  =  0. We obtain

[formula]

Note that the convergence for zk (e.g. in the principal direction of negative curvature) is quadratic. If errors are introduced in the computation of the eigenelement (λ1(xk),v1(xk)), the rate of convergence of zk becomes linear, as can be seen in ([\ref=eq:conv_zk]).