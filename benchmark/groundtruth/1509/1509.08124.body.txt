Keywords: differential correlation mining, association mining, biostatistics, genomics, high dimensional data

Introduction

In many statistical problems one has two datasets that measure the same variables under different conditions. In the analysis of such data, it is common to assume that the samples in each dataset are generated from two underlying distributions. Even when the data is high dimensional, differences between the distributions may only be present for a small number of variables, and it is often of interest to identify these key variables. In this paper, we present a new method of second order comparative analysis, called Differential Correlation Mining (DCM), which identifies sets of variables such that the average pairwise correlation between variables in the set is higher in one sample condition than in another. The method does not make use of auxiliary information, apart from the separation of samples into pre-determined groups (e.g. treatment vs control). DCM is applicable to both low and high dimensional datasets.

Most often, differential behavior between sample groups is measured by first-order statistics, which are functions of a single variable. Familiar first-order statistics include the sample mean and the sample variance. A well-studied example of first-order differential analysis is the study of differential gene expression in microarrays (see [\citet=de1] for a canonical example, or [\citet=de2] and the references therein for an overview of several methods). Other applications of first-order differential analysis include text analysis for authorship identification [\citep=authorship], studies of brain functionality based on regional activation [\citep=diffbrains], and investigation of cultural bias in standardized testing [\citep=testbias].

The use of first-order statistics only allows for analysis of a single variable at a time. To study relationships between pairs of variables, one requires functions of two variables, which specify second-order statistics. Examples of second-order statistics include correlation, covariance, and distance. When one wishes to understand interactions between many variables (as in clustering problems), data may be summarized in matrix form, where each entry in the matrix represents the observed value of a second-order statistic. It is common to look within a matrix of relational data for groups of variables that have high pairwise association. Interconnected variable sets may represent, e.g., social groups in a communication network [\citep=facebook], genes in common protein pathways [\citep=geneclust], functionally similar brain regions [\citep=brainclust].

While there is a large literature on clustering and networks, to the best of our knowledge, there is relatively little work comparing second-order behavior across two sample conditions. The many insights obtained from ordinary second-order variable set selection lead us to believe that a second-order differential approach will be of scientific interest. The methods introduced in this paper fall under the broader heading of differential association mining. As in ordinary association mining, we are interested in the pairwise behavior of variables; however, in the differential setting, we must consider two different relational matrices. In some cases, simply taking the difference of the matrices and applying ordinary clustering methods would suffice. However, most second-order statistics - including the focus of this paper, the linear correlation coefficient - require a more careful treatment. For instance, two sample correlation matrices will exhibit vastly different random behavior based on the sample sizes of the corresponding datasets, and will have a complex dependency structure when the population correlation matrix is not the identity.

The DCM method proposed here addresses differential correlation mining in a direct way. (Section [\ref=exist] considers possible alternatives based on existing work.) DCM seeks variable sets that form differentially correlated (DC) cliques. In a graph, a clique is a set of nodes that is fully connected, in the sense that there is an edge between every pair of nodes in the set. Informally, a DC clique is a set of variables such that each variable in the set has a positive (usually large) average differential correlation with the other variables in the set.

More formally, let [formula] be the p  ×  p population correlation matrices of the distributions underlying sampling conditions 1 and 2, respectively. Let J  ⊂  [p], where

[formula]

An example

To motivate our definition of DC cliques, we provide an illustrative real-world example. Figure [\ref=fig:example] shows an empirical DC clique identified by DCM in real data from The Cancer Genome Atlas (TCGA) Research Network (http://cancergenome.nih.gov/). The two sample conditions under consideration are Her-2 type breast cancer tumors and Luminal B type tumors, as classified by [\cite=perou]. (Further results for the TCGA dataset are provided in Section [\ref=realdata].)

Figure [\ref=fig:example] shows the sample correlation matrices within each tumor type, restricted to a set of 365 variables consisting of an empirical DC clique of size 165 selected by DCM (A), and 200 randomly chosen variables (B). The variables B are included for contrast, and to show that the differential correlation observed in A is not present in the entire dataset. The figure illustrates the second-order behavior and the differential nature of the identified DC clique A. The block pattern in the upper left corner of the Her-2 matrix shows that every entry in the correlation matrix of A is large, suggesting that all the variables of A are strongly pairwise correlated. The Luminal B sample correlation shows a similar pattern, but it is much less pronounced. No such pattern is seen among the variables in B.

In general, the results of DCM are distinct from those found by first-order analysis (e.g. differential expression). For example, Figure [\ref=fig:diagnostic] shows the relative differential expression, overall expression level, and differential variation for the above estimated DC clique A. For this plot, we ranked all genes in the study (p  =  15,785) by (a) t statistic of differential mean expression between Her-2 and Luminal B samples, (b) overall expression in Her-2 samples, and (c) ratio of sample variations (F statistic) for Her-2 versus Luminal B samples. The histograms in Figure [\ref=fig:diagnostic] show the ranking of the genes in A. The overall uniformity of the histograms indicates that the variables in the observed DC clique A do not exhibit standard first-order differential behavior. Similar results were observed for all other data studied in this paper.

By targeting DC cliques, the DCM method identifies variables whose joint behavior is different across sample conditions. The results are readily interpretable as sets of variables that interact strongly under one sample condition but only weakly (or not at all) under another. In this paper, we will demonstrate DCM is an effective and efficient way to identify differentially correlated variable sets from observed data.

Related work

There is a substantial body of recent work concerning estimation and testing of covariance and correlation matrices in high-dimensional settings. However, to the best of our knowledge, none of this work addresses the search for DC cliques or related differential structures in a direct way. Below we provide an overview of work that is related to DCM. In what follows, let [formula] denote the population correlation matrices of two data distributions, and let 1,2 denote the corresponding sample correlation matrices.

Mining from single correlation matrices. Non-differential correlation mining, in which one searches for highly associated variables from a single dataset, has been well-studied, typically in the context of clustering. [\citet=corrclust] provides a survey of clustering methods for high-dimensional data based on correlation distance. [\citet=survey] and [\citet=geneclust] and the references therein give an overview of methods developed specifically for clustering of gene expression. In general, typical clustering or community detection methods must be adapted for application to correlation distances to correct for bias (see e.g. [\citet=commdet] for an illustrative example).

Estimation and hypothesis testing. There has been a great deal theoretical work devoted to testing equality of high-dimensional covariance and correlation matrices. When the sample size n is substantially larger than the dimension p, classical results are applicable, e.g., likelihood ratio tests as discussed in [\citet=anderson] and [\citet=muirhead], or results like those of [\citep=steiger] for testing individual sample correlation. In the high-dimensional (p  >  n) setting, [\citet=Cai_Covmat] [\citet=Cai_limiting] [\citet=Cai_Yin] have developed minimax rate optimal tests for the equality of covariance matrices under sparsity assumptions. Results for correlation (rather than covariance) are less prevalent; recent work includes tests for sets of sample correlation coefficients [\citep=equality], tests for rank-based correlation matrices [\citep=rankbased], and tests for detecting overall dependence [\citep=hero].

In some cases, optimal testing procedures can inform methods for estimation of high-dimensional covariance and correlation matrices. Particularly relevant is the work of [\citet=caizhang], which yields an estimator for the difference matrix [formula]. This estimator is implemented and discussed further in Section [\ref=sims]. Other approaches to high-dimensional estimation include the work of [\citet=bickel], who discuss a thresholding estimator for covariance matrices; [\citet=partial], who estimate partial correlations in sparse regression models; and [\citet=covest], who make use of graphical model techniques for covariance matrix estimation.

Detection of isolated changes in correlation structure. Existing approaches to differential correlation mining are based on examining individual variables for changes in second-order structure across two sample conditions. For example, one may treat 1 and 2 as the adjacency matrices of two fully connected, weighted networks, and then look for variables whose connectivity pattern is very different across the two networks [\citep=yin2] [\citep=gill]. Most methods approach differential correlation mining by developing a statistic to measure the change in pairwise correlations of an individual variable. [\citet=Nstat] uses the covariance distance (total difference of covariances); [\citet=gsca] uses a direct difference of sample correlations; [\citet=diffcorr] uses the difference of Fisher transformed sample correlations; and [\citet=dcgl] use a filtration (or thresholding) step before summing square correlation differences. These methods then permute samples across the two classes to measure the significance of the original differential correlation. Significant variables may then be selected by an appropriate multiple testing procedure.

Outline

This remainder of the paper is organized as follows. In the next section, we describe in detail the three main steps of the DCM procedure. Section [\ref=theory] provides a closer examination of the test statistic used in the procedure, including a discussion of its asymptotic distribution. We apply DCM to simulated data in Section [\ref=sims], and compare the results to possible alternative procedures based upon existing work. Finally, in Section [\ref=realdata] we present the results of two applications of DCM, to the aforementioned TCGA dataset and to brain activity data from the multi-institutional Human Connectome Project.

The DCM Procedure

In this section, we present details of the three components of the proposed DCM procedure: initialization, set update, and residualization. The initialization step employs a simple greedy algorithm to select an initial variable set A. Once the initial set is determined, it is passed to an update algorithm that iteratively refines the set, making use of a hypothesis testing framework to test variables for differential correlation. When an estimated DC clique is found, the residualization step prepares the data for further search by removing the differential correlation of the discovered set.

The DCM procedure is summarized below. For detailed pseudocode, see Appendix [\ref=pseudocode].

We now provide a more in-depth discussion of each step of the procedure.

Notation

We assume that the data under condition 1 consists of n1 independent samples drawn from a distribution F1 with correlation matrix [formula], and that the data under condition 2 consists of n2 independent samples drawn from a distribution F2 with correlation matrix [formula]. Let [formula] and [formula] denote the resulting data matrices in standard sample-by-variable form. Thus [formula] denotes the measurements of variable j under condition 1, while [formula] denotes the measurements of variable j under condition 2. Let [formula] and [formula] denote the restriction of [formula] and [formula], respectively, to a variable set A  ⊂  [p]. Similarly, let [formula] and [formula] denote the correlation matrices of the restricted datasets.

Let j and j be the standardized versions of [formula] and [formula], respectively, and define 1  =  (1,...,p) and 2  =  (1,...,p). Finally, let 1 and 2 denote the usual sample correlation matrices of [formula] and [formula], respectively (and 1,A and 2,A those of the appropriate restricted datasets). Thus [formula] and a similar relation holds for 2.

Initialization

The set update procedure in the second step of DCM readily identifies variables that are significantly differentially correlated relative to a given variable set A, and is most effective when the initial set of variables exhibits at least low levels of differential correlation. (When applied to a randomly chosen set of variables, the set update procedure typically returns an empty set.) The core search procedure could be run exhaustively, beginning with every variable set A  ⊂  [p], but this is not computationally feasible for data sets of high or moderate dimension. As an alternative, we identify initial variable sets exhibiting a moderate degree of differential expression using a greedy search procedure. We then can pass this initial skeleton clique to the set update process to be fleshed out into a final estimated DC clique.

The initialization procedure seeks a local maximum of the score function

[formula]

where φ is the element-wise Fisher transformation of sample correlations, namely

[formula]

The Fisher transformation and subsequent weighting by degrees of freedom ensures that the first and second terms in the sum have approximately equal variances for each i,j. In this way, we account for the natural variance of the individual elements of 1 and 2 as well as possible imbalance in sample sizes across the two datasets.

To find a local maximizer of S(  ·  ), we begin with a random seed A. We consider only pairwise swaps, replacing an element of A with one from Ac; the set A is then updated by making the swap that produced the largest increase in the score. Since exactly one element is added and removed at each stage, the size of the variable set remains constant. The cardinality of A is user-specified (with a default of 50). Due to the subsequent set update procedure, we find that a wide range of initial cardinalities result in the same final outcome. Because of the random seeding, the algorithm is not purely deterministic; however, in practice the same local maximum is reached from most seeds.

Further discussion of the initializing algorithm is available as supplemental material; pseudocode for its implementation may be found in Appendix [\ref=pseudocode]. A closely related method is implemented in Section [\ref=sims] for comparison with DCM.

Core set update procedure

The heart of the DCM procedure is the set update algorithm, which makes use of multiple testing principles to iteratively refine a variable set A. Recall that the goal of DCM is to estimate DC cliques from the data. To this end, the set update procedure is designed to identify variable sets exhibiting the properties of a true DC clique up to a level of statistical significance.

Consider a single iterative step, at which we update a given variable set A. We wish to determine whether each variable i (including those in A itself) ought to be included in the updated set A'. Since our eventual goal is to discover a DC clique, we perform hypothesis tests based upon the principles of Definition [\ref=def:DCc]. For fixed A, the tests for variable i may be written as

[formula]

Recall that Δ(i,A), as defined in [\eqref=eq:delta], is a difference of average pairwise correlations between i and elements of A from [formula] and [formula], so this amounts to a test of differential correlation relative to a fixed set A. Our updated set is then given by

[formula]

In order to test the hypotheses in [\eqref=eq:hypo], we require a test statistic. A natural choice is the corresponding sample quantity,

[formula]

In addition to being a straightforward choice, this test statistic exhibits several desirable properties discussed in Section [\ref=theory].

Let δ(i,A) denote the realized value of the test statistic for a particular dataset. It is clear that large positive values of δ(i,A) provide support for the alternate hypothesis in [\eqref=eq:hypo], while values that are negative or close to zero provide evidence in favor of the null. Thus, in order to test the hypotheses, we calculate a p-value of the form

[formula]

where the probability [formula] is the (unknown) distribution of (i,A) under the null hypothesis Δ(i,A)  =  0. Since we make no assumptions about the distributions of data (F1 and F2), we instead make use of asymptotic results to approximate the above probability. We show in Section [\ref=sec:distn] that, under appropriate regularity assumptions, and for large enough sample sizes n1,n2,

[formula]

where σ̂20 is an estimate of the variance of (i,A).

The collection of p-values [formula] measure the significance of the differential correlation of each variable relative to A. In order to select a set of significant variables A', we apply the modified FDR procedure of [\citeauthor=bhy] to the p-values. Specifically, we carry out the following steps

Order the p-values [formula] as [formula].

Define the cutoff index k* by

[formula]

Let [formula].

Recall that we impose no assumptions on the structure of correlation matrices [formula] and [formula]. In particular, it is possible that p-values [formula] and [formula] may be negatively correlated, and negative correlation is common in real data sets. Most common multiple testing methods assume independence or positive dependency between p-values. The possibility of negative dependency of p-values necessitates the more conservative multiple testing method of [\citet=bhy], which controls the expected False Discovery Rate at level α.

The main search procedure terminates when it degenerates ([formula]) or converges ([formula]). For the degenerate case, the interpretation is simple: the initial set (chosen in the first step of the DCM procedure) was not sufficiently differentially correlated to yield an interesting result. In the second case, we have identified an empirical DC clique, in the sense that by design, a nonempty fixed point A meets the first requirement of a DC clique in Definition [\ref=def:DCc] up to a level of statistical significance. The only other possible outcome of the iterative search procedure is a multi-set cycle; this corner case is discussed in Section [\ref=sec:special].

Remark. Iterative updating using multiple testing was applied by [\citet=essc] in the context of community detection for binary networks. DCM employs a similar search structure in the context of differential correlation. Whereas the method of [\citeauthor=essc] relies on a null network distribution that is based on observed node degrees, here we make use of a central limit theorem that does not rely on the structural properties of the observations.

Residualization

In general, we expect multiple DC cliques to be present in data. The residualization step allows the DCM procedure to search the same dataset many times, avoiding repeat identification of prior results. This is accomplished by generating new residualized data matrices [formula] after each (non-degenerate) termination of the set update algorithm.

For clarity, let us restrict our attention to [formula] (the same process is applied, separately, to [formula]). Suppose the set update procedure converges to a set A, with |A|  =  k  >  0. We model the correlation matrix of the variables in A as the combination of a rank-one shared group correlation, characterized by [formula], and an underlying residual structure [formula], so that [formula] The problem of "factor analysis", or representing correlation matrices by the best lower-rank approximation, is well studied [\citep=factora], and efficient methods for estimating Λ are readily available. Given an estimate Λ̂, we generate a new submatrix [formula] such that

[formula]

We then build [formula] by replacing [formula] with the residualized data [formula] and leaving the rest of the data matrix unaltered. In this way, neither [formula] nor [formula] contain the groupwise structure of A captured by Λ. We are then free to apply the DCM procedure to [formula] and [formula] and search for secondary empirical DC cliques.

Special Cases

Minimality: A nonempty fixed point A of the set update procedure has the property that, analogously to Definition [\ref=def:DCc], H0(i,A) is rejected if and only if i∈A. The second condition of Definition [\ref=def:DCc], however, is not guaranteed in general. It is possible that DCM may select a large set that in truth consists of two (or more) disjoint population DC cliques. These cases are well addressed by the residualization step. When a conglomerate estimated DC clique is residualized, the joint structure is removed, leaving behind the individual structure of the disjoint DC cliques. Further runs of the DCM algorithm are then able to identify the separate DC cliques.

In extreme cases, the sampled data may be such that the disjoint DC cliques are, by chance, correlated enough to have negligible remaining individual structure after residualization. This renders the multiple DC cliques indistinguishable in the data from a combined DC clique.

Cycles: Under certain conditions, the main search procedure may be caught in a cycle of two or more sets, so that termination to a fixed point never occurs. When the set update procedure oscillates between two sets A1 and A2, we restart the search on the intersection [formula]. Usually, this results in true convergence to a fixed point in the vicinity of the intersection. On occasion, the same oscillation (A1 to A2) re-emerges, in which case the overlap [formula] is selected as the final output. For this overlap set, H0(i,A) will be rejected for all i∈A1,A2. It is not quite a fixed point; nevertheless, it shares many properties of a true DC clique, and we consider it worthy of attention.

Cycles of length greater than two are rarely observed in real or simulated data. However, to protect against longer cycles leading to infinite loops, the algorithm is built with an adjustable maximum iteration limit.

Properties of the Test Statistic

We now discuss some properties of the test statistic (i,A) used in the calculation of p-values for the set update procedure.

Geometric Interpretation

The equation for (i,A) given in ([\ref=eq:test]) expresses the test statistic directly in terms of average differential correlation. However, we may also write (i,A) in an alternate form that yields an informative geometric interpretation. Let [formula] and [formula] be the standardized measurements of variable i under sample conditions 1 and 2, respectively; and let

[formula]

be the vector means of the standardized measurements of the variables in A under each condition. It is easily shown that

[formula]

Thus,

[formula]

Note that the vector i and the vectors {j:j∈A} lie on the surface of an (n1 - 2)-dimensional sphere in [formula], and that [formula] is the geometric center (centroid) of the latter collection. The norm [formula] is between 0 and 1; large values of [formula] place the centroid closer to the surface of the sphere, indicating that the vectors {j:j∈A} are tightly clustered, or equivalently, highly intercorrelated. Thus the quantity [formula] weights the similarity of [formula] and the centroid [formula] according to the overall similarity of the vectors {j:j∈A}. Similar remarks apply to {j:j∈A} and [formula]. The statistic (i,A) is the difference of the summary measures in conditions 1 and 2.

Figure [\ref=fig:circles] gives a simple two-dimensional representation of the geometric picture discussed above. In condition 1, [formula] is not strongly correlated with [formula], but [formula] is large because the vectors indexed by A are tightly clustered. In condition 2, [formula] is strongly correlated with [formula], but [formula] is small because the vectors indexed by A are not tightly clustered. In this example, (i,A) is close to zero, and we would likely conclude no differential correlation is present.

Asymptotic distribution of (i,A)

We now discuss the asymptotic distribution of (i,A), from which the p-values used in Section [\ref=sec:mult] are derived. First, we make note of a classical result concerning sample correlations.

[\citep=steiger2] Let [formula] be a p  ×  p correlation matrix, and [formula] the corresponding sample correlation matrix based on n i.i.d. samples of p-variate data with finite 4th moment. Let [formula] and [formula] be the vectorized versions of the matrices, of dimension p2  ×  1. Then, as n tends to infinity

[formula]

where Σ is a p2  ×  p2 covariance matrix for which a general form is given equations (3.1-3.5) in [\citet=southafrica].

Using Theorem [\ref=thm:rp] one may evaluate the asymptotic distribution of (i,A), which is a function of [formula] and [formula].

Let A be a fixed index set and let (i,A) be defined as in [\eqref=eq:test], with sample correlation matrices 1 and 2 based on n1 and n2 independent samples from distributions F1 and F2 respectively. Let [formula], where H0 is the null hypothesis in ([\ref=eq:hypo]). Then, under H0,

[formula]

as min (n1,n2)  →    ∞  .

Proof: For clarity, we first examine only one "half" of (i,A). Let

[formula]

Note that 1(i,A) is a linear function of 1 and that 1(i,A) is the same function applied to the population correlation matrix [formula]. It follows from Theorem [\ref=thm:rp] that

[formula]

with [formula], which has a finite limiting value that can be expressed as the mean of appropriate elements of the covariance matrix Σ in the theorem. To apply this result for the full test statistic, we note that (i,A)  =  1(i,A)  -  2(i,A). Samples from F1 are independent of those from F2, so 1(i,A) is independent of 2(i,A), and thus (i,A) is asymptotically normal.

Under the null hypothesis in [\eqref=eq:hypo], 1(i,A)  =  2(i,A), and therefore the mean of the limiting distribution of (i,A) is 0. The variance of (i,A) can be expressed as the weighted sum

[formula]

In practice, the variances τ21(i,A) and τ22(i,A) are not known. Thus, we make use of the results in [\citet=steiger2] to identify the asymptotic variance of 1(i,A), and a consistent estimator τ̂1.

Let rij be the sample correlation of [formula] and [formula], and let [formula]. For [formula] let

[formula]

and define

[formula]

If 1(i,A) is defined as in [formula], then under any sampling distribution satisfying the assumptions of Theorem [\ref=thm:rp], we have

[formula]

where [formula].

Lemma [\ref=lem:varest] is proven in Appendix [\ref=app:varest]. The results of this lemma allow us to make an efficient and straightforward calculation to estimate the variances of 1(i,A) and 2(i,A) for every i∈[p], used in the testing step of the DCM algorithm. Note that regardless of the size of A, the derived formula for τ̂1 involves basic operations on only three p  ×  n1 vectors: [formula] and i. Such simplification is important, since in practice the estimates τ̂1 and τ̂2 must be calculated for every variable i∈[p] and for multiple iterative steps of A.

The results of Corollary [\ref=cor:DCLT] and Lemma [\ref=lem:varest] apply to variable sets of fixed cardinality (|A|  =  k) as n1 and n2 tend to infinity. In practice, one may encounter variables sets for which k  >  n1,n2. Simulations suggest that the DCM algorithm still identifies DC cliques with high success and controls false discovery in such settings even when the cardinality of |A| greatly exceeds the sample size.

Simulation Study

To test the DCM method against possible alternatives, we implemented a simple study of performance on simulated data. We created artificial datasets containing a single DC clique and compared the results of several methods to the known truth. Although the simulated setting is not a perfect representation of real data situations, it readily illustrates the advantages of DCM as opposed to adaptations of existing methods.

Methods implemented

In the absence of comparable methods, we adapted several existing methods to search for DC cliques. These adaptations are described below.

Mining a single correlation matrix (WGCNA). We implemented the Weighted Gene Co-Expression Network Analysis (WGCNA) method of [\citet=wgcna] for clustering correlation matrices of gene expression data. This method is a hybrid analysis, which mines for clusters (or "modules") in a single correlation matrix, then tests each module for differential expression. Thus, although the WGCNA method involves both differential and second-order elements, it is not designed to search for DC cliques or similar structures. For the purposes of this simulation study, we applied WGCNA to samples from class 1 only. We then tested the output module for differential correlation via a standard t-test over sample correlations in classes 1 and 2. In this way, we attempted to only select variable sets exhibiting differential correlation, even though WGCNA does not naturally identify modules with this property.

Estimation of differential correlation matrices (D-EST and FISH). We implemented the method of [\citet=caizhang] to estimate [formula] by the suggested estimator [formula]. To search for the seeded DC clique, we converted [formula] to a

[formula]

Simulated Data

We simulated data with a single embedded DC clique, consistent with Definition [\ref=def:DCc]. Our study varied the following parameters: size of the DC clique (k), total number of variables (p), strength of the correlations (ρ1 and ρ2), samples sizes of the two datasets (n1 and n2), and background data. We considered three background types: uncorrelated, positive, and real data. To elaborate, let [formula] and [formula] be matrices that are 1 on the diagonal, ρ1 or ρ2 (respectively) on the off-diagonal for indices 1 to k, and 0 otherwise. Then, the three types of data simulations were:

Uncorrelated Gaussian. The data was generated from multivariate Normal distributions with correlations [formula].

Positively correlated Gaussian. The correlation matrices of the data were [formula], [formula]. That is, the correlation matrix was boosted everywhere (except on the diagonals) in equal part for both datasets.

Real data. The dataset was derived from a real-world data source with samples randomly assigned one of two sample classes. By adding a common vector to the first k rows of the data matrix, we induced further correlation ρ1 and ρ2 in part the data.

We found that all methods behaved similarly with regard to changes in sample sizes n1,n2 and clique size k (relative to p). Here, we present only the results regarding ρ1 vs. ρ2 and different background types, to illustrate key differences in performance between methods. By default, the other parameters were set to be n1  =  n2  =  100, k  =  100, and p  =  1000.

Results

We applied the five proposed methods (DCM, WGCNA, D-EST, FISH, DCP, and CLASSIC) to a variety of values of ρ1 and ρ2 for each of the three background data types. For each (ρ1,ρ2), we ran 100 random trials and averaged the results. The success of the methods was measured by the false positive rate (FPR), the percentage of variables in a selected set that were not in the seeded DC clique, and the true discovery rate (TDR), the percentage of detected variables from the true DC clique. That is, if B was the output variable set of a procedure and [formula] was the embedded DC clique, then

[formula]

Figure  [\ref=fig:type1] shows the False Positive rate for each method as the seeded differential correlation gets stronger (ρ1  -  ρ2 grows), with other parameters fixed at default values, for each background type. Figure [\ref=fig:type2] shows the same for False Negative rate. In both cases, values close to zero are desirable, as they represent low error in identifying the true seeded DC clique.

We note that DCM controls the rate of false positives in all cases, except for some error when there is very low signal size in the real data case, which may be due to actual signal being present in the randomized real data. DCM also begins to reliably detect DC cliques at a lower signal size - around a correlation difference of 0.2 - in every setting except for the simple uncorrelated background setting, where WGCNA has slightly better performance. When the background is highly correlated, but not differentially correlated, WGCNA is not able to control the false positive rate. This is because WGCNA is designed to be a single-dataset mining method. When the background data shows strong correlation, WGCNA correctly identifies a large correlated module in the first dataset. Even if these modules could be tested for differential correlation, there is no method inherent to WGCNA for extracting the DC clique from surrounding correlated (but not differentially correlated) data.

The D-EST and FISH methods behave as expected; because our approach necessarily returns a nonempty variable set, the false positive rate is high for small signal. However, even if the false positives were perfectly controlled in some way, these methods show a higher false negative rate than DCM. Similarly, the adjusted DCM algorithm with the classical test (CLASSIC) controls error, but is less powerful than our method.

Finally, the DCP method vastly overselects variables in the uncorrelated background case. This is likely because the mutual behavior of the variables in A induces some correlation structure in the background variables; Figure [\ref=fig:example] illustrates this phenomenon, as there is some pattern in the cross correlation between variables in B and A. This result emphasizes the danger of the common approach of looking for isolated changes in correlation structure of individual variables, rather than searching for DC cliques: vestigial correlation patterns may be misleading. (When the background has structure, as in positively correlated or real data, the induced pattern is overshadowed, so the high false positive rate of DCP does not occur.)

We wish to note in particular the behavior of these methods when the correlation difference is zero. These simulations represent settings where correlation is present in both datasets, but the structure is identical across datasets rather than differential. We expect such scenarios to arise commonly in real data, where groups of variables may be universally correlated without regard to the particular sample conditions being studied. It is important that a search procedure does not identify nonexistent DC cliques in these situations. Figure [\ref=fig:eqrho] shows the average size of selected sets as a function of the strength of the non-differential correlation. Values at zero represent cases where the method successfully avoided identifying the misleading correlation as a DC clique; large values represent the discovery of a false positive set.

It is clear that all methods except DCM and CLASSIC are prone to error when non-differential correlation is present. This striking difference in performance illustrates the ways in which existing methods are simply not designed for the specific case of DC cliques. Only DCM-type algorithms include a testing-based element, which allow them to dismiss observed correlation for which there is not enough evidence of differential behavior. (In the real data case, DCM sometimes selects small nonempty sets in spite of the lack of seeded DC clique; again, this is likely due to the fact that permuting the samples in real data will sometimes produce signal.)

A note about computation

Of the methods tested, only DCM, CLASSIC, and WGCNA are computationally practical for large p (~  105 or more). The methods FISH and D-EST have memory demands on the order of p2, as they are based upon the estimation of the full p by p dissimilarity matrices, Fisher and [formula]. Permutation-based methods such as DCP are even more infeasible, since they require the computation of a p by p correlation matrix for each of many permutations.

Data Analyses

TCGA

We now expand upon the real data application referenced in Figure [\ref=fig:example]. Recall that we applied the DCM procedure to samples from two pre-determined breast cancer subtypes: Her-2 and Luminal B. A total of 18 empirical DC cliques (more correlated in Her-2 than in Luminal B) were discovered, ranging in size from 13 to 165 genes. To illustrate how this information may be useful to genomic research, we briefly discuss one of the discovered gene sets. The set of interest contained 46 genes, listed alphabetically in Table [\ref=tab:genes]. These genes are found to be highly associated with immune response, particularly the HLA (Human Leukocyte Antigen) gene class, represented by six of the genes in the set. Researchers are interested in understanding how and why some cancer subtypes trigger immune response while others do not. For example, [\citet=iglesia] showed that prognosis was improved for patients with Her-2 and basal-like subtypes showing higher immunoreactive response. Further exploration of DC cliques such as the one in Table [\ref=tab:genes] may further understanding of the gene interactions that drive immune response.

The Human Connectome Project

The Human Connectome Project is a multi-institutional venture aimed at mapping functional connections between parts of the human brain. The project has collected vast amounts of brain scan data, all of which is publicly available to researchers online at www.humanconnectome.org. In this analysis, we made use of a dataset from the "500 Subjects MR" data release, which consists of functional magnetic resonance imaging (fMRI) brain scans for 542 healthy adult subjects. Participants performed a variety of tasks during the MR scan, designed to isolate certain types of brain functionality. Activation levels were recorded over time for ~  30,000 voxels (3D coordinate locations in the brain's white matter interior) and ~  60,000 greyordinates (indexed locations over the grey matter brain surface).

In this paper, we applied DCM to data from a single subject. We compared two task categories:

Language-based tasks: During the scan, subjects were told brief stories and asked to answer questions after each one about what they were told.

Motor-based tasks: Subjects were attached to motion sensors at the hands, feet, and tongue. They were then asked to move one appendage at a time, in blocks of repetitions.

DCM to searched amongst 91,282 brain locations (or nodes) for DC cliques that exhibit more correlation over time during language tasks than during motor tasks.

The first empirical DC clique selected by DCM contained 913 nodes located on the cortical surface. These nodes, or "greyordinates", are visualized as points on the smoothed exterior of the brain in Figure [\ref=fig:brains]. The clear locational pattern in the nodes - despite the fact that the analysis did not take location into account - is striking. Additionally, the empirical DC clique in Figure [\ref=fig:brains] includes a concentrated group in the rear of the left cortex. This general brain region is known to be specifically associated with language processing and auditory input (Wernicke's Area, see [\citet=wernicke]).

We also studied two other artifacts of the data for comparison. First, we identified the 1000 nodes exhibiting the strongest differential first-order behavior. These show higher mean activation during the language tasks than during the motor tasks, as measured by standard two-sample t-tests. We saw a clear grouping of nodes in the right frontal lobe. This pattern is unsurprising and appears in many studies of brain functionality that examine differential activation for language processing [\citep=rfront]. This basic first-order analysis suggests that differential correlation is not redundant. None of the empirical DC cliques selected by DCM show high frontal lobe concentration; instead, they exhibit "trail-like" patterns such as the ones shown in Figure [\ref=fig:brains].

Second, we identified 1000 nodes found to be highly correlated over time for the language task data, irrespective of their behavior in the motor task data. These nodes were observed to be very tightly grouped in the interior left hemisphere. This is likely due to the nature of data measurement: fMRI brain scans measure oxygen flow in the brain, so measurements for adjacent regions tend to "blur" and show high artificial correlation [\citep=fmri_depend]. In this case, the same node set is also highly correlated during motor tasks, suggesting that it is likely a byproduct of data collection. Even if this node set does represent a meaningful result - regions, perhaps, that are universally correlated regardless of task - it is not differential.

This example illustrates the advantage of taking a differential approach like DCM. Effects due to fMRI-driven spatial correlation or strong universal correlation can drown out signal that is truly specific to a particular sample condition. By comparing language tasks to the similar but distinct condition of motor tasks, we are able to isolate signals that are unique to language processing. The fact that the identified DC cliques show emergent locational patterns suggests that DCM is capturing a true facet of the data rather than arbitrary correlation. Since this output is unique in form, while maintaining some consistency with known brain functionality, we believe it merits further scientific investigation.

Conclusion

In this paper, we have introduced a new statistical method, DCM, to identify differentially correlated variable sets from observed data. The DCM algorithm has been shown to be built on statistical principles with a theoretical basis, and to perform accurately and efficiently in a variety of settings. Unlike existing methods, DCM is specifically built to discover DC cliques, and its underlying testing process controls error. Additionally, the DCM software can be run on extremely high dimensional data without large memory demands or long runtimes. Preliminary data analysis results in the application areas of both gene expression data and brain activation are encouraging.

Code for public use of DCM is freely available at http://kbodwin.web.unc.edu/software/.

Acknowledgements

The authors wish to thank Yin Xia, Andrey Shabalin, Katherine Hoadley, and Kimberly D.T. Stachenfeld for their contributions.

Variance Estimator

Proof of Lemma [\ref=lem:varest]

We wish to show that if 1(i,A) is defined as in [\eqref=hia], for fixed i∈[p] and A  ⊂  [p], then

[formula]

is equal to the consistent estimator for the variance of 1(i,A) given in [\citet=steiger2]. Define

[formula]

Then, for the variance of an average of sample correlations sample correlations, [formula], equation (5.1) of [\citet=steiger2] gives the consistent estimator

[formula]

Using the notation of Section [\ref=sec:distn], we can write

[formula]

Thus, we can write τ̂1 as a simple inner products of the vectors [formula] and i,

[formula]

Lemma [\ref=lem:varest]

We note that although the estimator τ̂1 is consistent for a very general set of sampling distributions, it may in some cases converge slowly. For very small sample sizes, we find the estimator to be negatively biased; that is, tests involving this estimator may be anticonservative. Although the full DCM procedure appears in simulations to control false positive rate even for small sample sizes, we caution against its use when min (n1,n2)  <  30.

Detailed Pseudocode