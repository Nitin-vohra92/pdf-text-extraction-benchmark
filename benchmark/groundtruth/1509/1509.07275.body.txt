Ten reasons why a thermalized system cannot be described by a many-particle wave function

Introduction

Publications on the foundations of statistical mechanics [\cite=Emch2007-EMCQSP] [\cite=lindenpopescuetal2010] [\cite=ShortFarrelly2012] [\cite=reimann2013quantum] [\cite=eisert2015quantum] make us believe that reality is very different from how we perceive macroscopic systems and from what information we can gain about them by measurements. Due to the overwhelming success of quantum mechanics, its validity is supposed even for situations where this leads to massive conceptual problems. For instance, a gas at finite temperature that consists of a macroscopic number N (of the order of 1023) of atoms is generally believed to be correctly described by a wave function [formula] (if we choose the position basis) that evolves according to the Schrödinger equation. This time evolution is unitary, i.e., it is deterministic and time reversible. Due to the interaction with the environment, which imposes the temperature on the gas, the gas becomes entangled with the environment. When the trace over the environmental degrees is taken, one obtains the reduced density matrix of the gas, and this density matrix is that of a mixed state, in accordance with the description in quantum statistical physics. When the gas is completely isolated from the rest of the world (this can be imagined by theoreticians....), then similar arguments can be applied to small subvolumes of the system: due to the interaction with the rest of the gas, this subvolume becomes entangled with it, and its reduced density matrix is that of a mixed state. The full system remains in a pure state and retains all information about the initial state [\cite=popescu2006entanglement].

However, this is a purely academic claim since we will never be able to extract detailed information about the microscopic state and to calculate from it some previous state. Neither is it possible to verify in a macroscopic system at finite temperature the incredible amount of entanglement that has supposedly developped with time. In contrast, the calculations that we employ to reproduce all the properties of such a system that we can measure are in open contradiction with these assumptions, as they are based on stochastic, irreversible processes, involving particles that are pretty well localized.

In the following, I will present ten reasons why we should abandon for macroscopic systems at finite temperature the fundamentalism based on the Schrödinger equation and take a critical realist view of statistical mechanics, accepting that the assumptions underlying its methods reflect correctly the properties of thermalized systems. Most of the following is formulated for a simple model system, namely a diluted gas of atoms with short-range interactions, which behaves in very good approximation like an ideal gas.

1. Molecular dynamics simulations, which assume localized atoms, work very well

Molecular dynamics (MD) simulations are successfully used to characterize the structure and dynamics of systems that consist of many atoms at a finite temperature [\cite=tuckerman2000understanding] [\cite=marx2000ab]. Such simulations evaluate the motion of atoms and molecules based on the forces between them and describing the system at least in some respects classically. Temperature is taken into account by coupling the system to a "thermostat" that extracts and adds energy in such a way that a Maxwell-Boltzmann distribution of velocities is obtained. The majority of methods use Newton's equations of motion to calculate the motion of the nuclei. In purely classical simulations, molecules are represented as a collection of point masses and charges with restrictions on their relative positions, and forces are effective forces (e.g., van der Waals) obtained from comparison with empirical data or from quantum mechanical calculations. This type of simulations gives very good results for structural relaxation times and concentration profiles in liquids [\cite=klameth2013structure], for many biological processes such as transport through pores in biological membranes [\cite=gumbart2005molecular], and for protein folding [\cite=karplus2005molecular]. When, however, the formation and breaking of bonds, the polarization of atoms or molecules, or excited states shall be taken into account, the quantum mechanical properties of the electrons must be considered, employing ab initio MD simulations. For given positions of the nuclei, the electronic structure of atoms and molecules is calculated using quantum mechanics. The motion of the nuclei is then calculated classically based on the force fields resulting from the electronic structure, and the electronic structure in turn is recalculated based on the changed positions of the nuclei. When quantum mechanical properties of nuclei become important, for instance with proton transfer processes that involve tunnelling, the Feynman path integral formalism of statistical mechanics is used to describe the nuclei. In this formalism, the partition function of one particle is written as [formula] with the quantum mechanical Hamiltonian H = T + U (with T being here the kinetic energy and not the temperature as in all other equations). This partition function can be rewritten such that it is identical to that of a classical harmonic chain that is closed to form a ring, in the external potential U. The typical size of this ring polymer is of the order of the thermal de Broglie wave length [formula]. For N particles, this partition function is extended such that the particles are treated like N such ring polymers, with Boltzmann statistics. This latter step is an approximation that assumes that the N particles are distinguishable and not entangled with each other.

In all these different methods for performing MD simulations, the atoms are localized: they are points for classical MD simulations, they have the extension of the electronic shell for ab initio simulations, and the extension of the thermal wavelength for path integral ab initio simulations. In none of these approaches are the atoms or molecules entangled with each other, even though the time evolution of the system according to the full Schrödinger equation for all particles would yield such an entanglement. The time evolution of a N-particle system at finite temperature is thus described by the deterministic motion of objects that are localized within a small spatial region, with added stochastic terms due to the coupling to a heat bath. The success of MD simulations suggest that they capture faithfully all those features of the considered systems that are also accessible by experimental methods.

2. In thermal equilibrium the thermal wave length sets the length scale for quantum effects

In statistical mechanics courses, sometimes the following quick derivation of the Bose-Einstein condensation temperature of an ideal Bose gas is given: Bose-Einstein condensation happens when the density of the gas becomes so large that the distance between atoms becomes of the order of the thermal wave length. This leads to [formula] and consequently T  =  (N / V)2 / 3h2 / (2πmkB), which is apart from a numerical factor of the order 1 identical with the condensation temperature derived from the fully-fledged calculation. The thermal wave length emerges naturally in calculations of the canonical partition function of an ideal gas. Its order of magnitude can also be estimated without performing calculations from the equipartition theorem [formula]. The ratio λ3th / V is often called the quantum concentration, see for instance the textbook Thermal Physics by Kittel and Kroemer [\cite=kittel1980thermal]. This quick derivation of the Bose-Einstein condensation temperature is justified by arguing that the atoms can be represented as wave packets of the extension of the thermal wave length. When the density is so high that the wave packets overlap, atoms tend to go into the same quantum state. In the opposite case that the density is so small that the wave packets are not in contact for most of the time, the gas can be approximated as a classical ideal gas of well localized atoms. Similar arguments can be made for fermionic gases: When the density is so large that all the wave packets touch each other, it cannot be further increased due to the Pauli principle, leading to a Fermi temperature that is up to a constant factor identical with the Bose-Einstein condensation temperature. Again, in the limit of very low density, quantum effects become completely irrelevant, and the Fermi gas can be treated like a classical gas. For thermally equilibrated fermions as well as for bosons, the specific quantum mechanical effects become thus only important when the concentration is not small compared to the quantum concentration.

This argument thus suggest that at sufficiently high temperatures atoms in a gas can be described as classical particles with a velocity distributed according to the Maxwell-Boltzmann distribution. This raises a puzzle: if all the wave packets that represent these atoms evolved according to the Schrödinger equation, scattering processes between them would cause them to become delocalized. Then the description as classical particles (or localized wave packets) would fail. The particles will remain localized only if there are limits of validity of the unitary time evolution of the Schrödinger equation. Note that decoherence as for instance in the theory of quantum diffusion according to Caldeira and Leggett [\cite=caldeira1983path] cannot help with this problem: Decoherence still gives a - incoherent - superposition of all the possible particle trajectories and not a single realization as in classical physics [\cite=adler2003decoherence]. It follows that there are limits of validity to the quantum mechanical description of thermal systems and that classical physics is the better description for concentrations much smaller than the quantum concentration.

3. Thermalized systems are extensive/have statistically independent subparts

Thermodynamics of homogeneous systems is extensive: if identical systems are combined to form a larger system, the extensive state variables V, N, [formula] and the thermodynamic potentials E, [formula] are the sum of those of the parts. The intensive variables p, μ, [formula] do not change. This means that the state of each system is not changed in any relevant way when the systems are combined. Conversely, parts of a larger system do not depend in any relevant way on the neighboring parts. In statistical mechanics, the statistical independence of the parts of a system is an important precondition for deriving the probability distributions associated with the different statistical ensembles and for deriving statements about the size of fluctuations. This is stated most clearly on the first pages of the textbook on statistical mechanics by Landau and Lifschitz (in § 2 of [\cite=landau2013statistical]): The probability that a (sub)system in thermal equilibrium assumes a certain configuration must be taken to be independent of the configurations of the neighboring (sub)systems. Then the extensive thermodynamic variables are simply sum variables over all subsystems, and their variance is the sum of the variances within the subsystems. In fact, one of the best interpretations of the "ensembles" introduced by Gibbs is that the systems of an ensemble are different parts of a single system. Otherwise it cannot be explained why a single macroscopic system already displays all the properties calculated within the Gibbs ensembles.

This simple assumption of statistical independence is in striking contrast to a view where there is entanglement between the different subsystems and where the system as a whole evolves deterministically and holistically. There are, of course, "derivations" of the diagonal density matrix of statistical mechanics that start from the Schrödinger equation. However, none of these derivations can succeed without making in some form the assumption of statistical independence. All these derivations, in one form or another, have to give reasons why the non-diagonal elements of the reduced density matrix become zero. In all the nondiagonal elements of the reduced density matrix vanish only when the environmental degrees of freedom are assumed to be sufficiently random or uncorrelated from each other.

Let us take as an example the simple model for decoherence given in [\cite=cucchietti2005decoherence], where a thermal environment is coupled to a spin-1/2. The environment is modelled as consisting of 2-state systems, which means that each "atom" of the environment is formally equivalent to a spin-1/2. Now, in order to obtain a diagonal reduced density matrix for the spin, the coupling constant of the environmental "atoms" to the spin are assumed to be random, and similarly the phases of the "atoms" that arise during time evolution are assumed to be uncorrelated. This is in fact an assumption of statistical independence, which means that the argument for decoherence is circular: it presumes statistical independence in order to "derive" it subsequently [\cite=kastner2014einselection].

4. The concept of entropy implies random transitions between a finite number of different states

Often, entropy is viewed as a subjective measure of our ignorance about the precise state of a system [\cite=jaynes1957information]. A popular way of explaining entropy consists in saying that it is a measure of the information required to specify the microstate of a system if only the macrostate is known. If this information is given in bits, then one obtains the entropy by multiplying the number of bits with kB ln 2. In contrast to us humans with our limited knowledge, a Laplacian demon who knows the microstate of the system assigns an entropy of zero to it, and sees that the microstate of the system evolve deterministically according to the Schrödinger equation.

This ignorance interpretation of entropy raises several problems: First, in order to fully specify the state of the system, the number of bits required is infinite and not finite. In a quantum mechanical description, a system is not in an eigenstate of the Hamiltonian, but in a superposition. The expansion coefficients that specify this superposition are real numbers, which are given by infinitely many bits. In a classical description, the state is specified by the positions and momenta of all atoms, which requires also an infinite number of bits. If the state is only given with finite precision, the future time evolution of the system is not known beyond some time horizon. Second, entropy is a thermodynamic state function that can be measured via heat exchange. This means that entropy is an objective physical quantity. A subjective interpretation of entropy does not do justice to this fact. If it did not capture something real about the system, the concept of entropy would not be useful.

Third, the definition of entropy in statistical physics is based on probabilities and not on a deterministic time evolution. The fundamental axiom of statistical mechanics, from which all of equilibrium statistical mechanics can be derived, is the axiom that in a closed system in equilibrium all accessible microstates are equally probable. This axiom suggests the idea that the system somehow moves stochastically through its different microscopic states. The second law of thermodynamics follows immediately from this idea. When a system is not closed but in thermal contact with another system, all statistical properties are obtained by assuming that the energy distributes in such a way over the two systems that the total entropy is maximized. This is again most easily justified by assuming that there are stochastic transitions between different states.

The usual way of "deriving" this stochasticity from an underlying deterministic evolution of a closed system is based on (quasi-)ergodicity (when classical mechanics is used as fundamental theory) or eigenstate thermalization (when quantum mechanics is used [\cite=deutsch1991quantum] [\cite=srednicki1994chaos]). Now, in order to argue that a system shows ergodicity or eigenstate thermalization, one must assume that the initial state of the system is not in one of the special states the time evolution of which deviates from the "typical" behavior. However, this is a strong assumption that is definitely not correct when applied backwards in time, since the present equilibrium states have come from very "improbable" initial states. Furthermore, this strong assumption amounts to saying that a random state and a stochastic time evolution of the system are in all respects sufficient for describing the system. Then, this should be taken as a valid description, without postulating that there is a (completely irrelevant) underlying deterministic dynamics.

Therefore, let us take the stochasticity suggested by a nonzero entropy seriously. If we imagine the diluted gas again as consisting of N wave packets of a volume of the order λ3th, then the entropy is correctly obtained by calculating kB times the logarithm of the number of different ways in which such wave packets can be distributed in the dynamically accessible phase space. ("Different" means that the N-particle states that are counted are orthogonal to each other.) This in turn is identical to the volume of the energy shell associated with the total energy of the system, divided by the size 3N of a unit cell in phase space.

The source of stochasticity in this system must be scattering events. The success of MD simulations suggests that wave packets move ballistically between scattering events. In order for the wave packets to remain compact, they must become localized again after scattering, and this can only be achieved by a "collapse" of the entangled configuration resulting from the scattering events, similar to what happens in a position measurement.

5. Statistical mechanics describes the properties of single systems

Statistical mechanics captures correctly the properties of single macroscopic systems. Nevertheless, the quantum statistical description of such macroscopic systems uses the concept of a mixed state, characterized by a density matrix that is diagonal in the basis of the energy eigenstates, with the probabilities of the quantum states being given by the entries of this matrix. The standard interpretation of the density matrix is that of an ensemble of systems, with the probabilities giving the proportion of systems in the different quantum mechanical states. In this interpretation, the description of the system as in a mixed state is again due to our ignorance of the full microscopic details. The probabilities are again subjective probabilities. This is in striking contrast to the fact that the mixed density matrix is so successful at describing the properties of single macroscopic systems, for instance when correlation functions or thermodynamic variables are calculated. A mixed state should therefore be taken as capturing something correct about an individual system, namely that a macroscopic system is not in a pure quantum state and cannot be described by a N-particle wave function. Let us therefore discuss which properties a system should have if it shall be described correctly by a the density matrix of a mixed state. First, let us take up the suggestion of Landau and Lifschitz to imagine the system as composed of statistically independent parts. For the sake of simplicity, let all parts be of identical size. If the density matrix is taken to be that of a part, all the parts together form an ensemble. Then, the probabilities occuring in the density matrix can be interpreted as probabilities for the different states of the different parts. Now add to this the idea that there are random transitions between the states of each part. Then the assumptions that the parts are statistically independent from each other and that the entries of the density matrix represent objective probabilities become justified, and it follows that a mixed density matrix is a good description of the system.

As an aside, let us mention that this interpretation of the mixed state solves the puzzle of ergodicity in classical statistical mechanics: the time it takes the system to visit each cell in phase space with a deterministic time evolution is incredibly much longer than the life time of the universe, which means that ergodicity (or quasi-ergodicity) cannot explain the rapid approach to equilibrium. In contrast, with a stochastic evolution each state of the system can be reached within a short time: Let us take 1023 atoms of a gas at ambient temperature and pressure. The particle density is of the order 25  ·  1024 / m3, which means that the mean distance is of the order 30 Å. If an atom has the size of 1Å, it will move about 1μm between collisions. With a speed of the order of 103m / s, there will be of the order of one collision per ns per atom. Assume that the number of possible states to which a wave packet can become localized after a collision is of the order of 10, then there are of the order of 101023 states that the system can reach within 1ns. The total number of states available to the system is of the order of (V / Nλ3th)N, which is of the order of 101024. Such a large number of states can thus be reached within 10 collision events per atom, i.e., within 10 ns.

6. A true equlibrium has forgotten the past

A thermalized system is in equilibrium. A thermal equilibrium state contains no information about the initial conditions, and it satisfies detailed balance. This means that its time evolution can in no way be distinguished from the time-reversed situation. The approach to equilibrium is an irreversible process, during which entropy increases until it is maximum in equilibrium.

This is the general understanding of equilibrium. A unitary, deterministic time evolution is fundamentally different, since it contains the full information about the initial state, and since it is therefore not invariant under time reversal.

The usual way to model the approach to equilibrium involves equations that are not invariant under time reversal, such as diffusion equations or equations with friction terms. A beautiful simple theory that leads to an equilibrium state for a gas is the Boltzmann equation, which can be found in many textbooks on statistical mechanics. In the absence of external forces, it has the form

[formula]

where [formula] is the particle density in the 6-dimensional phase space spanned by the momentum and position coordinates of a particle. The right-hand side describes the momentum changes due to collisions, with the function W depending on the scattering cross section. The collision term neglects correlations between the momenta of different particles, and this leads to an irreversible behavior of the system, with the function

[formula]

decreasing in time until the equilibrium state is reached, which satisfies the detailed balance condition

[formula]

The neglection of correlations between the momenta of different particles is another form of the assumption of statistical independence mentioned earlier. In fact, statistical independence means that the past history is not important for the present behavior, and that there are no subtle interdependencies that arise through the dynamical history.

To conclude, the usual way of modelling the transition to equilibrium involves irreversible equations. The underlying microscopic picture from which such irreversible equations can be obtained is stochastic with a limited memory of the past. The success of these theories suggests that they capture a real property of the systems, and that there are limits to a unitary, deterministic time evolution.

7. Spontaneous symmetry breaking in a phase transition is a stochastic event

When a system undergoes a phase transition, it "chooses" spontaneously and stochastically the new, symmetry-broken state into which it goes. Such a transition that is accompanied by the spontaneous breaking of a symmetry is incompatible with a unitary time evolution. A unitary time evolution that starts from a state that obeyes the symmetry under consideration and that evolves according to a Hamiltonian that also displays this symmetry must go to a final state that also has this symmetry. The final state therefore must contain all possible broken-symmetry outcomes with equal weights, and not just one of them. By taking the trace over environmental variables, such a final state would be represented by a mixed density matrix.

Even if the Hamiltonian does not strictly preserve the symmetry that is relevant for the phase transition, a unitary time evolution of a quantum system is incompatible with the spontaneous symmetry breaking occuring in a phase transition. In order to make this concrete, let us perform a Gedankenexperiment and construct a toy system, consisting of an Ising model (with the z component of neighboring spings being coupled) in contact with a heat bath, which consists in a "real" system of the phonons of the magnetic material, but for the sake of the argument, we can also imagine it as the gas of the previous sections. Let the total energy of the system be such that the maximum entropy state is one with a broken symmetry in the Ising model, with the magnetization being + M or - M (with respect to the z direction). Such a state of broken symmetry has a lower internal energy than a disordered spin system, and the energy freed by the ordering can go into the heat bath, where the entropy increases. Let us choose an initial state where the spins are oriented at random, and where the state of the spin system can be written as a product of the states of all spins. In the following, I will show that the assumption that the magnetic system orders to the + M or - M state while the time evolution of the total system is unitary leads to a contradiction. Let us start with a randomly chosen state of the spin system, and a random state of the environment. Assume that under unitary time evolution the spin system evolves to the + M state. Now prepare again the same initial state of the total system, but with one of the spins reversed. If the unitary time evolution goes again to the + M state, then take again the same initial state, but with an additional spin reversed. Eventually, an initial spin state will be generated that leads to the - M state. Then we take a linear superposition (with equal weight) of the last spin state and the one before as initial state of the spin system, again combined with the same environmental initial state. The state of the spin system is again a product state, but with the spin that was reversed last now pointing in a direction perpendicular to the ones before. Due to the linearity of the Schrödinger equation, this state must now evolve to a zero magnetization, in contradiction with our assumption that every initial state that is a product state of spins ends up with a magnetization ±  M.

A stochastic time evolution does not face this problem of a unitary time evolution. In fact, computer models based on transition probabilities between states are very successful at modelling phase transitions. Such Monte-Carlo approaches with Glauber or Metropolis dynamics capture correctly the equilibrium properties for instance of an Ising system. This suggests that the idea that a thermalized system undergoes truly stochastic transitions between its different states captures something correct about reality.

8. Quantum measurement is incompatible with a unitary time evolution

The measurement process faces exactly the same problem as a thermodynamic phase transition: linear superposition is in contradiction with the observation that a measurement always gives a specific outcome. In fact, the system of the previous section can be used as a measurement apparatus: We only need to include an additional coupling of the Ising system to an external spin 1/2 that is sent into the system and the z component of which shall be measured: if the coupling is in a suitable range of values, the Ising system will go to the + M configuration if the spin is prepared in the +   eigenstate of σz, and to the - M configuration if the spin is prepared in the -   eigenstate (for a discussion of an explicit model of this type, see [\cite=allahverdyan2005phase]). The final magnetization of the Ising system can be used as a pointer from which the measurement result is read off. When the spin is prepared with an orientation in the + x direction, time evolution according to the Schrödinger equation leads to a linear superposition of the two previous outcomes and not to a unequivocal + M or - M result, each with probability 1/2.

Now, the proposed solution of this puzzle is often ascribed to decoherence. Indeed, one obtains the density matrix of a mixed state when taking the trace over the environmental degrees of freedom. Such a mixed state correctly describes what happens in an ensemble of identically prepared systems. However, as mentioned above, decoherence theory needs an assumption of statistical independence. Furthermore, a mixed state cannot describe a single event [\cite=schlosshauer2005decoherence]. Whenever decoherence theory is used to "explain" the measurement process, it must be combined either with the many-worlds interpretation or the statistical interpretation of quantum mechanics.

There is also a close connection between the measurement process and the arguments presented in the previous seven sections about the thermalized gas consisting of localized wave packets: localization of the measured particle is an essential part of a measurement process. Now imagine an additional atom entering the thermalized gas, and assume that this atom was prepared with a sharp momentum, i.e. as a plane wave. When this atom becomes part of the gas and is eventually in equilibrium with the rest of the gas, it must also become a localized wave packet, just as the other atoms of the gas.

The "problem" of the measurement process is thus connected in two ways to the "problem" of quantum foundations of statistical physics: First, both involve a macroscopic number of degrees of freedom at finite temperature (in the measurement process these are the internal and/or environmental degrees of freedom of the measurement device), second, both show a spontaneous symmetry breaking where one of the possible outcomes is chosen, while unitary time evolution leads to a superposition of the possible outcomes (in statistical physics this occurs at a phase transition).

9. There are limits to the validity of the Schrödinger equation

The Schrödinger equation is not exact, and we have no reason to assume that any of our physical theories is exact. The history of classical mechanics has taught us an important lesson in this respect: for more than 200 years, it was generally believed that Newtons's laws provide an exact description of nature and are valid even under circumstances where they had not been tested: on the atomic scale, at high velocities close to the velocity of light, at cosmic distances. The advent of the theory of relativity and quantum mechanics has revealed that Newton's laws are only an approximation to reality, with a limited range of applicability. Nevertheless, these laws are still extremely good and useful for many purposes. This experience should make us open to the idea that probably none of our theories is exact. As far as the Schrödinger equation is concerned, we know its limits of applicability: It ignores relativistic effects and in particular the creation and annihilation of particles. Furthermore, a description of a thermalized gas by a many-particle Schrödinger equation neglects the presence of photons, which are emitted and absorbed in the system and which obeye a Planck spectrum. The electromagnetic interaction is treated classically when a system is described by a Schrödinger equation.

While the Schrödinger equation is an extremely good description for many purposes, we have no reason to expect that it applies also to a system with 1023 particles at finite temperature. On the contrary, the eight points made so far suggest strongly that the Schrödinger equation has limited validity in such systems. Otherwise, we run into contradictions.

Relativistic quantum theories do not fare better since they are also based on a deterministic, unitary time evolution. Very general arguments why a linear, unitary time evolution must have limits of validity when considering compex systems are made by G. Ellis [\cite=ellis2012limits].

10. Occam's razor

Occam's razor is a useful rule for deciding between competing explanations for the same observation: if one explanation is complicated and makes many assumptions, while another is simple and is based on less assumptions, then choose the simple explanation. This article deals with two competing explanations for the properties of thermalized systems: One explanation says that "in reality", even though we will never be able to measure this, the system is a highly entangled (not just between the particles of the system, but also with the rest of the world!) many-particle wave function which evolves deterministically and still contains all information about the initial state. Additionally, this explanation requires ad-hoc assumptions about statistical independence and runs into severe problems when confronted with the fact that a mixed density matrix captures the properties of a single system. The other explanation says that stochasticity is a generic property of finite-temperature systems. This explanation yields directly the transition to equilibrium, ergodicity, and the description of a single system by a mixed density matrix. In my view, this explanation is the one to be preferred: First, it does not postulate facts that cannot be tested, and if physics is to be an empirical science, empricial testability is an important criterion for a physical explanation. Second, it takes the area of physics that is best suited to describe a thermalized system seriously, namely statistical mechanics and thermodynamics. I see no reason why we should not take these theories to be as fundamental and correct as the many-particle Schrödinger equation, with each of these descriptions having their own limited range of applicability.

I would like to finish this section with a quotation by the physicist Walter Kohn, who won the 1998 chemistry Nobel prize for developing density functional theory and who warned us not to take the concept of a wave function too far [\cite=lecture1999electronic]: "In general the concept of a many-electron wave function [formula] for a system of N electrons is not a legitimate scientific concept, when N  >  N0, where N0≃103. I will use two criteria for defining 'legitimacy': a) That Ψ can be calculated with sufficient accuracy and b) can be recorded with sufficient accuracy".

Conclusions

The arguments presented in the foregoing ten sections suggest that there are limits to a deterministic unitary time evolution when we deal with macroscopic systems at finite temperature. This, of course, raises many follow-up issues, some of which shall be mentioned in the following.

Statistical interpretation Proponents of the statistical interpretation (also called ensemble interpretation) claim that this interpretation solves many of the problems mentioned above [\cite=Ballentine]. This interpretation holds that the wave function does not describe an individual system but an ensemble of systems. It traces back to Max Born who suggested that the wave function gives the probability amplitude for measuring the particle at a given position. According to the statistical interpretation, quantum mechanics of pure states is a special case of a more comprehensive theory, which is quantum statistical mechanics. This interpretation does not encounter some of the problems mentioned in this paper, but it has other problems: With this interpretation, quantum mechanics is incomplete and cannot describe single systems. In addition, it has an internal inconsistency: The Schrödinger equation describes the time evolution of a "probability distribution", but there is no criterion to decide when the event described by this probability distribution happens. This means that there is no distinction between an ensemble of systems in which each system evolves forever according to the Schrödinger equation, and an ensemble where in each system a "collapse" to one measurement result happens. Furthermore, there is empirical evidence that a pure quantum state does in fact represent single systems [\cite=pusey2012reality].

The meaning of temperature If temperature plays indeed an important role at determining the range of quantum interference, it is more than just a measure of the kinetic energy of particles.

Quantum mechanical chance is a collective effect Often, it is claimed that true chance enters the world on the microscale, through quantum events, for instance the radioactive decay of a nucleus or the spontaneous emission of a photon from an excited atom. However, when considered more closely, this stochasticity arises due to the interaction of an atom with the rest of the world (for instance the other atoms of a gas, or a measurement device) and is thus a collective effect at the interface to statistical physics. Chance is thus an emergent phenomenon in systems with a macroscopic number of degrees of freedom. This view is in contrast to stochastic collapse theories [\cite=ghirardi1986unified] [\cite=ghirardi1990markov], which include stochasticity already in the time evolution of an isolated particle.

Equilibrium means indifference The view that a system in equilibrium has forgotten its past is in strong contrast to the idea that such a system evolves deterministically. A deterministic system contains all the information about its future states, while a truly equilibrated system is maximally receptive for environmental influences as it has no intrinsic aspirations.

The Prigogine school points to nonunitary time evolution but not to symmetry breaking There are interesting efforts by the Prigogine school to emphasize nonunitary, decaying solutions of the Schrödinger equation as an explanation of the approach to equilibrium [\cite=petrosky1997liouville]. Such solutions occur when there is a continuum of states, and they are in fact also invoked in quantum mechanics textbooks when the transition from an excited to a ground state is discussed. However, such decaying solutions are not enough to base statistical mechanics on quantum mechanics on them, since they do not allow for the spontaneous breaking of a symmetry. The photon emitted from an excited atom is not directed in one particular direction in this description. In contrast, when the photon is measured, the symmetry that is present in the angular distribution function is broken, and the photon is found only in one of the possible directions.

Non-equilibrium Finally, the foregoing arguments and considerations apply only to systems in thermal equilibrium or systems going to equilibrium. They give any reason to believe that there are limits to quantum mechanics also in non-equilibrium. One particularly interesting related question is that of a possible information loss occuring in black holes [\cite=blackhhole]. If there is no preservation of information about the initial state in a gas at finite temperature, we have no reasons to assume that information is conserved in black holes.