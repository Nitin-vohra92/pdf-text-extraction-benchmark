An Estimation Procedure for the Hawkes Process

Introduction

In this paper, we introduce a nonparametric estimation procedure for the multivariate Hawkes point process; see Definition [\ref=estimator] for the formal definition and Figure [\ref=fig2] for an illustrative summary of the main results. The Hawkes process is a model for event streams. Its alternative name, "selfexciting point process", stems from the fact that any event has the potential to generate new events in the future. Our estimator gives substantial information on this excitement: nonmonotonicities or regime switches in the excitement of the fitted Hawkes model can be detected; the estimates may also help with the choice of parametric excitement-functions. The asymptotic distribution of the estimator can be derived so that confidence bounds are at hand. Also note that the presented estimation method is numerically less problematic than the standard likelihood-approach. Last but not least, the figures generated from the estimation results are a graphical tool for representing large univariate and multivariate event datasets in a compact and at the same time informative way. In particular, the estimation results can be interpreted as measures for interaction and stability of empirical event-streams. This will be highlighted in the data example at the end of the paper where we apply the estimation procedure to the order arrival times in an electronic market.

The Hawkes process was introduced in [\citet=hawkes71a] [\citet=hawkes71b] as a model for event data from contagious processes. Theoretical cornerstones of the model are [\citet=hawkes74], [\citet=bremaud96] [\citet=bremaud01], [\citet=liniger09] and [\citet=errais10]. For a textbook reference that covers many aspects of the Hawkes process; see [\citet=daley03]. The main theoretical reference for the following presentation is our own contribution [\citet=kirchner15a], where we show that Hawkes processes can be approximated by certain discrete-time models.

By the omnipresence of "event"-type data, the Hawkes process has become a popular model in many different contexts such as geology, e.g., earthquake modeling in [\citet=ogata88], internet traffic, e.g., youTube clicks in [\citet=crane08], biology, e.g., genome analysis in [\citet=reynaud10], sociology, e.g., crime data in [\citet=mohler11], or medicine, e.g., virus spreading in [\citet=kim11]. A most active area of scientific activity today is financial econometrics with applications of Hawkes processes to the modeling of credit defaults in [\citet=errais10], extreme daily returns in [\citet=liniger11], market contagion in [\citet=sahalia11] and numerous applications to limit-order-book modeling such as high-frequency price jumps in [\citet=bacry11b] and [\citet=chavez12], order arrivals in [\citet=bacry11a], or joint models for orders and prices on a market microstructure level in [\citet=muzy13]. Early publications applying the Hawkes model in the financial context are [\citet=bowsher02], [\citet=chavez05] and [\citet=mcneil05].

The paper is organized as follows: Section 2 explains how Hawkes processes can be approximated by specific integer-valued time series and how this approximation yields an estimation procedure. Section 3 defines the new Hawkes estimator formally and discusses its properties. Section 4 refines the procedure by giving methods for a reasonable choice of the estimation parameters Section 5 presents the data example where the ideas of the paper are applied to the analysis of intraday financial data. The last section concludes with a discussion on the implications of the presented results. Appendix [\ref=proofs] contains proofs and Appendix [\ref=figures] presents illustrating figures. Large parts of the paper are accompanied by examples with simulated data: in favor of a linear reading flow, we directly illustrate all new concepts with such examples--instead of devoting a separate section to simulations.

Approximation of Hawkes processes

In this section, after defining the Hawkes process we introduce autoregressive integer-valued time series. We clarify how this model approximates the Hawkes model and how this approximation yields an estimation procedure.

The Hawkes process

From a geometric point of view, a Hawkes process specifies a distribution of points on one or more lines. Typically, the lines are interpreted as "time" and the points as "events". Selfexciting point process is the common alternative name for the Hawkes process. It highlights the basic idea of the model: given an event, the intensity--the expected number of events in one time unit--shoots up ("selfexcites") and then decays ("forgets its past gradually"). The shape of this decay is specified by a function, namely the excitement function. The definition and the proof of existence of a Hawkes process are subtle matters. For rigorous theoretical foundation, we refer to [\citet=liniger09], Chapter 6. We assume a basic underlying probability space [formula], complete and rich enough to carry all random variables involved. On this probability space, we define stochastic point-sets [formula] of the form [formula] with Tk  ≤  Tk + 1, [formula], having almost surely no limit points. Furthermore, we assume that the σ-algebras

[formula]

are subsets of F. By setting

[formula]

any stochastic point-set P defines a random measure NP on [formula], the Borel sets of [formula]. At this point, we drop the P index; the set P is completely specified by N: = NP. In this paper, we call a random measure N of this kind point process and we call the filtration [formula] history of the point process. The conditional intensity of a point process N is

[formula]

A Hawkes process is a stationary point process N with conditional intensity

[formula]

The constant η  ≥  0 is called baseline intensity, and the function [formula], measurable, is called excitement function. Necessary existence-conditions are discussed below.

For [formula], a d-variate Hawkes process [formula] is a process with d point processes on [formula] as components, i.e., [formula] Each component process counts points from random point-sets [formula]. In this multivariate setup, the counting processes [formula] do not only selfexcite but in general also interact with each other ("crossexcite"). The baseline intensity [formula] is a d-variate vector in [formula] and the excitement function is a measurable d  ×  d matrix-valued function [formula]. The conditional intensity of a d-variate Hawkes process is [formula]-valued with

[formula]

where, for [formula],

[formula]

and [formula] In other words, the entry hi,j(t) of the matrix H(t) denotes the effect of any event T(j)k∈Pj in component j on the intensity of component i at time T(j)k + t. See Figure [\ref=fig1] for an example of a bivariate Hawkes process. In [\citet=hawkes71b], we find the following sufficient condition for existence: if

[formula]

where [formula], then a process with conditional intensity as in [\eqref=multivariate_intensity] exists. The matrix K in [\eqref=stability_criterion] is sometimes referred to as branching matrix and the entries of K as branching coefficients. These terms reflect an alternative view on the process as a special cluster process [\citep=hawkes74]:

In each of the components of a d-variate Hawkes process, we observe cluster centers that stem from independent homogeneous Poisson processes with rates [formula]. These cluster centers are also called immigrants or exogenous events. Such an immigrant [formula] in component j triggers d inhomogeneous Poisson processes in components [formula] with intensities [formula]. And each of these new points again produces d inhomogeneous Poisson processes in a similar way, so that the clusters are built up as a cascade of inhomogeneous Poisson processes. The non-immigrant events are called offspring or endogenous events. Disregarding the time component and only considering this immigrant-offspring structure, one actually has a branching process with immigration, where the number of direct offspring in component i from an event in component j is PoisKij distributed.

Parametrization and estimation of Hawkes processes

In most cases, the data analyst's choice of the excitement function H of a Hawkes process is a somewhat arbitrary parametric function--the main decision being between exponential functions or power-law functions. The function parameters are then estimated via standard likelihood maximization. Power-law decay of the excitement functions often turns out to be more "realistic" in applications; exponential decay yields a likelihood that is numerically easier to handle by recursive representation; see [\citet=ogata88]. In addition, exponential excitement functions are mathematically attractive because they yield a Markovian structure for the conditional intensity; see [\citet=errais10]. Even if the choice between exponential and power-law decay is handled carefully, these two functional families cannot catch regime switches or nonmonotonicities of excitement functions as in Figure [\ref=fig1]. So it seems important to develop methods that can identify shapes of excitement in data with less stringent assumptions. Another motivation for our research on estimation of the Hawkes model stems from numerical issues--especially encountered in the multivariate case. A third gap that we aim to close with our paper is the derivation of the asymptotic distribution of the estimates.

Note that in [\citet=bacry11b] another nonparametric method for the estimation of the multivariate Hawkes process is developed; it can be interpreted in our approximation framework. We will touch this alternative approach in Section [\ref=hawkes_estimator].

Intuition of the approximation

The main idea is simple: given a (possibly multivariate) Hawkes process, we divide the time line into bins of size Δ > 0 and count the number of events in each bin (for each component). These "bin counts" form an [formula]-valued stochastic sequence ([formula]-valued in the d-variate case). The distribution of this sequence can be approximated by a well-known time series model. We present the heuristics behind the approximation in the case of a univariate Hawkes process N with baseline intensity η > 0 and excitement function h with [formula]. For some Δ > 0, we define the bin-counts [formula] We want to argue that for small Δ > 0 and large [formula], we have that

[formula]

We divide the approximation above in three separate approximation-steps:

[formula]

The estimator we are about to present ignores the three approximations above and treats them as equalities. In doing so, we make a distributional error [\eqref=distributional_error], a cut-off error [\eqref=cutoff_error] and a discretization error [\eqref=cutoff_error]. There is an integer-valued time series that solves the approximative bin-count equation [\eqref=approx_bin_count_eq] to the point: the integer-valued autoregressive model of order [formula], the INAR(p) model.

The INAR(p) model

The INAR(p) process was first proposed by [\citet=li91] as a time series model for count data. For the history and an exhaustive collection of properties of the model; see [\citet=marques05]. For a textbook reference; see [\citet=fokianos01]. The main idea of the construction is to manipulate the standard system of autoregressive difference-equations "[formula]" in such a way that its solution [formula] is integer valued. This is achieved by giving the error terms a distribution supported on [formula] and substituting all multiplications with independent thinning-operations. The following notation from [\citet=steutel79] makes the analogy particularly obvious.

For an [formula]-valued random variable Y and a constant α  ≥  0 define the thinning operator [formula] by

[formula]

where [formula] are i.i.d. and independent of Y with [formula]. We use the convention that [formula].

We immediately present the multivariate version of the thinning operator and the multivariate version of the INAR(p):

For a d  ×  d matrix [formula] and an [formula]-valued random variable [formula], define the multivariate thinning operator �[formula] by

[formula]

where the thinnings [formula] operate independently over 1  ≤  i,j  ≤  d.

Let [formula], [formula], [formula] [formula] and [formula] an i.i.d. sequence of vectors in [formula] with mutually independent components [formula], [formula]. A d-variate INAR(p) sequence is a stationary sequence [formula] of [formula]-valued random vectors; it is a solution to the system of stochastic difference-equations

[formula]

where the " [formula]" operate independently over k and n and also independently of [formula]. We refer to [formula] as innovation-parameter vector and to [formula] as thinning-coefficient matrices.

This model has first been considered in [\citet=latour97]. In the same paper we find that if all zeros of

[formula]

lie inside the unit circle, then a multivariate INAR(p) process as in Definition [\ref=mvINAR(p)] exists.

Consider a univariate INAR(p) sequence (Xn) with innovation parameter α0 and thinning coefficients [formula]. Note that the criterion from above now simply reads [formula]. Under this condition, we have that [formula]. In particular, [formula]--which is the exact version of [\eqref=approx_bin_count_eq]. The INAR(p) sequence has a similar immigrant-offspring structure as the Hawkes process. In the time series case, the (possibly multiple) immigrants at each time step stem from i.i.d. Pois(α0) variables. Each of these immigrants produces Pois(αk) new offspring events at k time steps later. Each of these offspring events again serves as parent event for new offspring etc. A more obvious choice for the distribution of the counting sequences in Definition [\ref=thinning_operator] would be Bernoulli. Note, however, that for small thinning coefficients, the Poisson and the Bernoulli approaches are very similar. Also note that the Poisson distribution is more convenient for our purpose: we want to interpret the INAR(p) model as an approximation of the bin-count sequence of a Hawkes process and in the Hawkes model, an event can have potentially more than one direct offspring event in a future time-interval. In addition, in the Poisson case, we do not have to exclude thinning coefficients larger than one.

Approximation of the Hawkes process by the INAR(p) model

We examine the close relation between Hawkes point processes and INAR time series in [\citet=kirchner15a]. For a particularly obvious parallel, the reader may consider the analogy of the existence criteria [\eqref=stability_criterion] and [\eqref=INAR_stability_criterion]. Our cited paper gives a precise convergence statement for the univariate case. After establishing existence and uniqueness of the INAR(∞  ) process as a generalization of Definition [\ref=mvINAR(p)] with d = 1 and p =   ∞  , we prove

Let N be a univariate Hawkes process with baseline intensity η > 0 and piecewise-continuous excitement function [formula] such that [formula] for all Δ∈(0,1). Furthermore, let (X(Δ)n) be a univariate INAR(∞  ) sequence with innovation parameter α(Δ)0: = Δη and thinning coefficients [formula], and define a family of point processes by

[formula]

Then we have that, for [formula], the INAR(∞  )-based family of point processes [formula] converges weakly to the Hawkes process N.

This is Theorem 3 in [\citet=kirchner15a].

Note that weak convergence of point processes is equivalent to convergence of the corresponding finite-dimensional distributions; see [\citet=daley03], Theorem 11.1.VII. The other result from [\citet=kirchner15a] that is important for our estimation purpose is the fact that INAR(∞  ) processes can be approximated by INAR(p) processes, p <   ∞  :

Let [formula] be an INAR(∞  ) sequence with innovation parameter α0 > 0 and thinning coefficients [formula]. Furthermore, let [formula] be a corresponding INAR(p) sequence, where the thinning coefficients are truncated after the p-th lag. That is, [formula] has innovation parameter α(p)0: = α0 and thinning coefficients [formula] Then, for p  →    ∞  , the finite-dimensional distributions of [formula] converge to the finite-dimensional distributions of [formula].

This is Proposition 3 in [\citet=kirchner15a].

We have not worked out the multivariate versions of Theorem [\ref=convergence] and Proposition [\ref=INAR(p)_approximation] above. However, the simulations presented further down in the paper support the assumption that both results also hold in the multivariate case. Under this assumption, we have the following approximation: The approximation summarized in the box above is the key observation for our estimation procedure:

Choose a small bin-size Δ > 0 and calculate the bin-count sequence of the events stemming from the Hawkes process.

Choose a large support s: = pΔ and fit the approximating INAR(p) model to the bin-count sequence via conditional least-squares.

Interpret the scaled innovation-parameter estimate (Δ,p)0  /  Δ as the natural candidate for an estimate of [formula] and, for [formula], interpret the scaled thinning-coefficient matrix estimates Â(Δ,p)k  /  Δ as natural candidates for estimates of [formula].

Before giving the formal definition of the estimator in the next section, we illustrate the power of the presented method in Figure [\ref=fig2].

The estimator

In this section, we first discuss estimation of the approximating INAR(p) process. Then we define our Hawkes estimator formally and collect some of its properties. Furthermore, we present results of a multivariate simulation study that support our approach.

Estimation of the INAR(p) model

There are several possibilities to estimate the parameters of an INAR(p) process. As the margins are conditionally Poisson distributed, in principle, maximum-likelihood estimation (MLE) can be applied. In our context, however, numerical optimization of the likelihood is difficult, as the number of model parameters will typically be very large. A method-of-moments type estimator would be the Yule-Walker method (YW). A third method is the conditional least-squares estimation (CLS). We formulate the estimation in terms of CLS rather than in terms of YW for three reasons: (i) For small sample-sizes, CLS is known to have a lower variance than YW. (ii) The CLS-estimator allows to present the estimation of excitement function and baseline intensity in the same formula. (iii) The asymptotic properties of the YW- and the CLS-estimator are the same. Their derivation, however, is typically done in the CLS-setting. In any case, even for medium sample-sizes, we note only very small differences between MLE-, YW- and CLS-estimates in simulation studies (not illustrated). Inference on CLS-estimation in the univariate INAR(p) context has been discussed, e.g., in [\citet=li91] and [\citet=zhang10]. In both papers, the reasoning is performed along the lines of [\citet=klimko78], which was originally developed for CLS-estimation of time series with the very general structure "[formula]", where gθ may be nonlinear. However, as already noticed in [\citet=latour97], INAR(p) sequences can be represented as standard AR(p) models with white noise innovation terms. This yields ways for inference that are more direct.

Let [formula] be a d-dimensional INAR(p) sequence as in Definition [\ref=mvINAR(p)] with innovation-parameter vector [formula] and thinning-coefficient matrices [formula], such that [\eqref=INAR_stability_criterion] holds. Then

[formula]

defines a (dependent) white-noise sequence, i.e., [formula] is stationary, [formula], and

[formula]

This can be shown by straightforward (if lengthy) calculations; see Appendix [\ref=pf_white_noise].

As a consequence of Proposition [\ref=white_noise], a d-variate INAR(p) process can be represented as a standard d-variate autoregressive time series with (dependent) white-noise errors:

Let [formula] be the multivariate INAR(p) sequence and [formula] the white-noise sequence from Proposition [\ref=white_noise]. Then [formula] solves the system of stochastic difference-equations

[formula]

Such vector-valued time series with linear autoregressive structure have early on been examined; see, e.g., [\citet=hannan70]. However, estimation in a multivariate context requires cumbersome notation. In order to make our results comparable, we follow one reference throughout, namely the monograph [\citet=luetkepohl05]. Adapting its notation is also the reason why we work with wide matrices--i.e., matrices having a number of columns in the order of the sample size--instead of the more common long matrices.

Let [formula] be an [formula]-valued sequence, where we interpret [formula] as a column vector. Fix p and [formula], p  <  n, and define the multivariate conditional least-squares estimator as

[formula]

where

[formula]

is the design matrix and [formula]

Dealing with multivariate time series the following notations turn out to be useful:

The vec(  ·  )-operator takes a matrix as its argument and stacks its columns. The binary [formula]-operator is the Kronecker operator: for an m  ×  n matrix A  =  (ai,j) and a p  ×  q matrix B, [formula] is the mp  ×  nq matrix consisting of the block-matrices ai,jB, [formula].

The [formula]-notation arises because the estimator is matrix-valued and we have no notion of the covariance of a random matrix. As we will see the [formula]-notation is strongly related to the [formula]-operator. For a large collection of properties of these operators; see Appendix A of [\citet=luetkepohl05]. The following theorem collects all relevant information for CLS-estimation of multivariate INAR(p) sequences. Together with the approximation results from Section [\ref=approximation], this theorem is the theoretical basis for our Hawkes estimation procedure.

Let [formula] be a d-dimensional INAR(p) sequence as in Definition [\ref=mvINAR(p)] with innovation-parameter (column) vector [formula] and thinning-coefficient matrices [formula], such that [formula]. Let

[formula]

the CLS-estimator with respect to the sample [formula]. Then (n) is a weakly consistent estimator for [formula]. Furthermore, let [formula] be the design matrix from Definition [\ref=CLS] with respect to [formula]. Assume that the limit

[formula]

exists and is invertible. In addition, assume that the model is irreducible in the sense that [formula]. Then, for the asymptotic distribution of [formula], one has, for n  →    ∞  ,

[formula]

where

[formula]

with

[formula]

In view of Corollary [\ref=AR-representation], it suffices to prove Theorem [\ref=inference] for the corresponding vector-valued autoregressive time series. So the distributional properties of the CLS-estimator can be derived similarly as in [\citet=luetkepohl05], pages 70-75, where independent errors are assumed. We provide a highly self-contained proof for the dependent white-noise case in Appendix [\ref=pf_inference].

Note that the condition [formula] in Theorem [\ref=inference] above is purely technical: if we had [formula] for some i0, this would imply that in one component of our sample we cannot observe any events. We may exclude this case with a clear conscience.

The Hawkes estimator

Combining Theorem [\ref=inference] with the basic approximation from Section [\ref=connection] yields the following estimator for multivariate Hawkes processes:

Let [formula] be a d-variate Hawkes process with baseline-intensity vector [formula] and excitement function [formula] such that [\eqref=stability_criterion] holds. Let T > 0 and consider a sample of the process on the time interval (0,T]. For some Δ > 0, construct the [formula]-valued bin-count sequence from this sample:

[formula]

Define the multivariate Hawkes estimator with respect to some support s,  Δ  <  s < T, by applying the CLS-operator from Definition [\ref=CLS] with maximal lag p: = ⌈s / Δ⌉ on these bin-counts:

[formula]

We collect the main properties of the estimator in the following remark.

The following additional notation clarifies what the entries of the (Δ,s) matrix actually estimate:

[formula]

From Theorem [\ref=inference] on estimation of INAR(p) sequences together with the basic approximation in Section [\ref=connection], we see that, for 0 < t < s,

[formula]

are weakly consistent estimates (for T  →    ∞  , Δ  →  0 and s  =  Δp  →    ∞  ) for the excitement-function component value hi,j(t), respectively, for the baseline-intensity vector component [formula]. Furthermore, we find from Theorem [\ref=inference] that

[formula]

where Γ and W are defined as in [\eqref=Gamma] and [\eqref=W] with respect to the bin-count sequences. Substituting Γ and W with their empirical versions yields the covariance estimate

[formula]

where [formula] is the design matrix from Definition [\ref=CLS] with respect to the bin-count sequence and, for [formula],

[formula]

Following formulas are useful for implementation of confidence intervals:

[formula]

for [formula] and [formula].

[formula]

Applying Remark [\ref=cov] above together with Definitions [\ref=CLS] and [\ref=estimator], our Hawkes estimation procedure may be implemented in a straightforward manner. However, we emphasize that the resulting matrix (Δ,s) in [\eqref=estimator_calculation] does not completely specify a fitted Hawkes model; it only yields pointwise estimates on a grid, whereas the true excitement-parameter is a function on [formula]; see Section [\ref=Hawkes_Process]. To complete the estimation, we have to apply some kind of smoothing method over the pointwise estimated values. We work with cubic splines, normal kernel smoothers and local polynomial regression ( ksmooth(), smooth.spline() and loess() in R). We find that the results do not vary significantly. The choice of the estimation parameters bin-size Δ and support s has more impact. Therefore, we focus on the selection of these estimation parameters; see Section [\ref=refinements]. The smoothing idea will be relevant in Section [\ref=choice_of_bin_size], where we discuss variance issues. In many applications, one can even avoid choosing and applying a smoothing method: practitioners might want to use our estimation procedure from Definition [\ref=estimator] for identifying or rejecting certain parametric models. For such purposes, the pointwise estimates suffice. The same is true if the estimation procedure is used as a mere tool for representing large event datasets; see Section [\ref=interpretation]. Finally, one is often only interested in the integral of the excitement; see comments after [\eqref=stability_criterion]. In this case, it makes more sense to directly add up the estimates rather than to take the detour over some smoothing method.

Finally, we refer to the alternative nonparametric Hawkes estimation approach from [\citet=bacry11b]. Here, an implicit equation for the autocovariance density of a Hawkes process is solved for the excitement function by Fourier analysis. This approach corresponds to directly applying YW-estimation to the bin-count sequences--if somewhat in disguise. Our procedure highlights the underlying approximation principle. This explicit connection with powerful time series theory seems more fertile than the manipulations in Fourier space: it is more intuitive, simpler to implement and yields much simpler ways for inference.

Simulation studies

We check the distributional properties of the Hawkes estimator collected in Remark [\ref=cov] in a simulation study. The results are summarized in Figure [\ref=fig15]. We simulate 2  000 times from a bivariate Hawkes model with baseline intensity [formula] and excitement function

[formula]

see Figure [\ref=fig1] for this parametrization and Figure [\ref=fig2] for an estimation of a single realization. In each simulation, about 5 000 events in each component are generated and our Hawkes estimator [\eqref=estimator_calculation] is calculated. We apply a bin size Δ  =  0.2 and a support parameter s = 6. These calculations yield 2 000 matrices of the form [formula]. We examine the estimations of η1 = 0.5, i.e., the baseline-intensity for the first component, and the estimations of h2,1(1) = 0.125, i.e., the crossexcitement on component 2 from component 1 after one time unit. These values correspond to the entries (Δ,s)1,121 and (Δ,s)2,9 in the estimator matrices. We find that the 2 000 estimates are distributed symmetrically around the true values. The means of the estimates correspond almost completely to the true values. QQ-plots support the asymptotic normality result. For both estimations, we also calculate the variance estimates from [\eqref=cov_estimator]. Comparing the empirical variance of the 2 000 estimates with the 2 000 estimated variances confirms the analytic result. Furthermore, the empirical covering rates for the 95%-confidence intervals are 94.5% for the baseline-intensity estimate, respectively, 94.8% for the excitement-value estimate. Note that the applied estimation parameters Δ  =  0.2 and s  =  6 are considerably "wrong": the bin-size is quite large and the true support of H would be ∞  . We may interpret the successful estimation as a sign for the robustness of the method with respect to the estimation parameters.

Separately, we examine the impact of the choice of the bin-size Δ, the support s and the size of the sample window

[formula]

are proportional to T- 1. Variances slightly increase if we increase the support parameter s. The variance of the baseline intensity estimate with respect to Δ is roughly constant in Δ. However, the variance of the excitement estimate with respect to Δ is proportional to Δ- 1. Albeit this relation, we will see in Section [\ref=choice_of_bin_size] that the excitement estimates are still meaningful for very small values of Δ.

Refinements

Our Hawkes estimator (Δ,s) from Definition [\ref=estimator] depends on a bin size Δ > 0 and on a support s > 0. In the following section, we present procedures for sensible choices of these parameters. Furthermore, we discuss numerical and diagnostic issues.

The choice of support

Estimating the support of the excitement function of a Hawkes process corresponds to estimating the largest lag of a nonzero thinning-coefficient (matrix) of the approximating INAR sequence. In view of the VAR(p) representation of INAR(p) sequences from Corollary [\ref=AR-representation], we can use any model-selection procedure stemming from traditional time series analysis; see Chapter 4.3 of [\citet=luetkepohl05] for an overview of such procedures in the multivariate context. For comparison of different order-selection methods for univariate INAR(p) sequences; see [\citet=marques05] . As a most common example, we apply Akaike's information criterion (AIC); see [\citet=akaike73].

We work in the setup of Definition [\ref=hawkes_estimator]. The starting point is a sample of a d-variate Hawkes process on

[formula]

:= - Δ - ΔĤ ,  k = p+1,p+2,,n,

[formula]

Choice of bin size

In the following, we discuss the choice of the bin size Δ > 0 for the Hawkes estimator (Δ,s) from Definition [\ref=estimator]. We suppose a reasonable support s > 0 has already been chosen by a procedure as described in Section [\ref=choice_of_support]. One can interpret the choice of the bin size Δ as a bias/variance trade-off: the smaller Δ, the smaller the potential bias stemming from the model approximation, i.e., the smaller the errors [\eqref=distributional_error] and [\eqref=discretization_error]. At the same time, due to the 1 / Δ factor in the calculation of the estimator matrix (Δ,s) from [\eqref=estimator_calculation], its (componentwise) variance increases when Δ decreases. In a simulation study, we simulate 100 times from a Hawkes model with excitement function h(t) =  exp ( - 1.1t). For each sample, we calculate the Hawkes estimator with respect to three different bin sizes Δ∈{0.1,0.5,1}. Figure [\ref=fig5] collects the estimation results in boxplots. The bias/variance trade-off is obvious. Note, however, that we had to choose the bin-size quite large to make the bias visible at all. Concerning the large variance, we should keep in mind that the final goal may be an estimation for the excitement-function components hij--and not only a finite number of their values [formula]. When we apply some smoothing method on these values, a smaller Δ typically leads to an "averaging" over more point estimates. This balances the increase in pointwise variance. So if the goal of the estimation procedure is a completely specified Hawkes model, then the smallest Δ that is numerically convenient may be chosen; see the discussion after Remark [\ref=cov]. The following toy-example clarifies things:

We consider a smoothing method for which we can approximately calculate the variance. Namely, we define a box moving-average with window size τ > 0. For some bin size Δ > 0, we consider a univariate Hawkes selfexcitement estimate [formula]. As a smoothed function-estimate, we set

[formula]

Then, using that [formula], see Figure [\ref=fig6], [formula] and that   #  {k:  kΔ∈[t   ±   τ  /  2]}  ≈  τ  /  Δ, we obtain

[formula]

In other words, the variance of the smoothed estimate is approximately constant in Δ. The same effect can be observed empirically with more sophisticated smoothing methods: for a single large simulated sample from a Hawkes process, we calculate the pointwise Hawkes estimator from Definition [\ref=estimator] with respect to three different bin-sizes Δ. Applying a cubic smoothing-spline procedure on the three results one findes that the smoothed functions are hardly affected by the choice of Δ.

If we choose a very small bin-size Δ, computation time becomes an issue. The calculations in [\eqref=estimator_calculation] require the construction of the design matrix [formula] from Definition [\ref=CLS] with about T / Δ rows and about d  ·  s / Δ columns. Here, T is the size of the time window, d is the dimension of the process and s is the support parameter of the estimation. Then the matrix [formula] has to be inverted. This square matrix is approximately of size ⌈d  ·  s / Δ⌉  ×  ⌈d  ·  s / Δ⌉. In short, the smaller Δ, the larger the matrices involved. Note, however, that, for a very small bin-size Δ, the corresponding design-matrix is very sparse. Specialized software makes construction and manipulation of sparse matrices numerically efficient; see [\citet=maechler12].

We now understand that the trade-off related to the bin-size choice is not so much a bias/variance trade-off but more a bias/numerical-issues trade-off! To check, if we have chosen Δ small enough, we propose to calculate the (biased) estimate of the baseline intensity vector [formula] for a decreasing sequence of bin sizes [formula]  The variance of the estimates [formula] is approximately constant over the different bin-sizes; see Figure [\ref=fig6]. This makes the estimates comparable. For [formula], we plot the values [formula] against [formula]. Typically, one observes a monotone convergence in n to some constant (or d constants for d > 1). Plotting confidence intervals around the point estimates indicates when the bias is negligible in comparison to the random noise of the estimate. We will apply this method in the concluding data-example.

Diagnostics

We see a certain danger in the application of our nonparametric Hawkes estimator from Definition [\ref=estimator]. Reasonable graphical results as in Figure [\ref=fig2] might be used as an argument in favor of the Hawkes process as the true model. But this conclusion would be a misuse of the method. In fact, the proposed estimator depends only on second-order properties of the data. So, we have to expect that there is a whole family of point processes that generate the same excitement estimates, although only one of these processes is a genuine Hawkes process. As an example, consider a continuous-time, nonnegative, stationary Markov chain that has the same second-order properties as some given Hawkes process. We use this Markov chain as a stochastic intensity for another point process; see [\citet=daley03], Example 10.3(e). The resulting doubly-stochastic point process is a point process with different distributional properties than the corresponding Hawkes process. But our estimator will still yield the same results in both cases. As another example, consider a time-reversed Hawkes process. Clearly, this is not a Hawkes process anymore. However, the time-reversed version has the same autocovariance density as the original process and therefore our estimator will again yield the same result.

This means, the application of our estimation approach always ought to be followed by a model test. A most common basis for such a test in our context is a multivariate version of the random time-change theorem for point processes; see [\citet=meyer71] [\citet=brown88]: for points [formula] from a d-variate point process with conditional intensity [formula], one has that [formula] independently over �[formula] and [formula] So, after having fit the Hawkes process to point process data, we calculate the corresponding conditional-intensity estimate and time-transform the interarrival times. These transformed interarrival times ought to be compared with theoretical Exp(1)-quantiles in a QQ-plot. Next to this graphical method one ought to apply a Kolmogorov-Smirnov test and an independence test to the transformed interarrival times.

Data application

There are two contexts of growing importance where large event-datasets are not the exception but the rule: internet traffic and high-frequency data in financial econometrics. The paper concludes with an exemplary application of the estimation procedure to the latter.

The data

The data we use stem from the limit order book (LOB) of an electronic market. LOBs match buyers and sellers of a specific asset. We will consider a certain future contract. Whoever wants to buy or sell one or several of these contracts has to send his or her orders to the LOB. An order basically consists of two pieces of information: it names (a) the maximal (respectively, minimal) price at which the sender is willing to buy (respectively, to sell) and (b) the desired quantity in terms of numbers of contracts. If the order is matched to another order, the trade is executed. Such orders that immediately find counterparts are called market orders. All other incoming orders are stacked in the LOB; these are called limit orders. Limit orders either wait for getting executed by a new incoming matching (market) order or--and this happens relatively often--they are withdrawn after some time. The empirical process of time points when orders arrive we call order flow. Such an order flow can be modeled by a point process. In particular, our estimation method from Definition [\ref=estimator] allows to analyze the order flow in a Hawkes setup. For a detailed survey of order-book quantitative analysis; see [\citet=gould13]. Financial intraday histories are attractive for econometric research as there is so much data available. However, by the very differing data qualities, results are sometimes hard to compare. To clarify our starting point, we explain the context and the preparation of the data quite detailed.

We consider a sample of the LOB of E-mini S&P 500 futures with most current maturity.

The enormous liquidity makes the data attractive for quantitative analysis. Samples of these particular data have also been analyzed in the Hawkes setting, e.g., by [\citet=filimonov12] and [\citet=hardiman13]. Our particular data sample was provided by TickData inc. It stems from September 2013. We have a separate dataset for quotes and for trades. A new entry in the quotes data corresponds to one of the following three events:

Arrival of some (not marketable) limit order

Arrival of some market order, i.e., a trade takes place

Cancelation of some limit orders

In the trade data set, we see the traded price and the number of contracts traded.

In both datasets, we observe ties, i.e., multiple events with identical millisecond time-stamps. These ties require special consideration as our model, the Hawkes model, does not allow for simultaneous jumps. As data is so relatively sparse, the multiple events cannot be accidental. This leaves two possibilities: either the multiples stem from a single order that has been split (for some technical reason) or the multiples are almost instantaneous responses to each other that are reported at the same millisecond due to rounding. We may safely rule out this second possibility: as yet, it is impossible to "react" (i.e.: observe an order, send an order, let the electronic book record or match the new order) in less than a millisecond. In addition, we had the opportunity to compare our data with a snapshot of the fully reconstructed LOB. This complete data provide "match tags" for each order. This additional information shows that nearly all multiple events are in fact orders from one single market-participant. This confirms our point of view. We therefore consider each time stamp in the datasets only once. After the thinning procedure, we derive two one-dimensional event datasets from our data:

the trade data T and

the (pure) limit-order data L that collects all the times when a new non-marketable limit order has arrived or a limit order has been canceled.

In busy trading hours, i.e., between 8:30am and 3:15pm (Chicago time), we observe about 5 events per second in the trade data T, and about 12 events per second in the limit-order data L. At Chicago night time, all of these average intensities are up to twenty times smaller. All interarrival-times processes exhibit significant autocorrelation at large lags. This rules out simple standard homogenous Poisson point processes as models as well as other renewal processes. On the other hand, the autocorrelation may also stem from nonstationarities of the underlying true model; see [\citet=mikosch00].

Bivariate estimation of the market/limit order process

With our nonparametric method from Definition [\ref=estimator], we fit a bivariate Hawkes process (N(T),N(L)) on a single 30min-sample of the data (T,L), namely on data from Friday, 2013/09/06, 10:00am-10:30am (Chicago time). In this specific sample, we observe about 20 000 trades and 40 000 limit orders. Our estimation procedure from Section [\ref=hawkes_estimator] depends on a choice of support and on a choice of bin size. For a sensible choice of these parameters, we apply the methods from Sections [\ref=choice_of_support] and [\ref=choice_of_bin_size]:

As a first step, we calculate the Hawkes estimator with respect to a relatively large preliminary bin-size of [formula] and for various support candidates between 1 and 300 seconds. As proposed in Section [\ref=choice_of_support], we compare the corresponding AIC-values. This coarse analysis shows that the AIC-optimal support is surely less than 20 seconds. Repeating the analysis with respect to a much finer bin-size [formula] on the interval [formula], we find an AIC-minimizing support of about [formula]. Let us note that the obtained minimum is much more clear-cut than in the controlled simulation study from Section [\ref=choice_of_support] illustrated in Figure [\ref=fig4]. We set [formula].

In other words, our support analysis indicates that the process forgets its past after three seconds. This preliminary result is already interesting: it can be interpreted such that--in this sample--the algorithms that drive the market only take not more than the last three seconds of the LOB-history into account.

For a reasonable choice of the bin-size parameter Δ, we apply the method from Section [\ref=choice_of_bin_size]. That is, we examine the impact of the bin-size choice on the estimation. We leave the support [formula] fixed and, for different bin-size candidates Δ, calculate the baseline-intensity estimate (Δ)i,  i = 1,2, together with the corresponding confidence intervals; see [\eqref=estimator_calculation] and [\eqref=normal_approximation] for the necessary calculations. We observe a monotone relation between the bin-size candidates and the corresponding baseline-estimates. However, for [formula], the differences of the estimates are of a lower order than their (estimated) confidence intervals. So it is sensible to assume that, for this particular sample, the bias of our estimation method becomes negligible for bin-size choices of [formula]. From the bivariate event dataset, we finally calculate the Hawkes estimator from Definition [\ref=estimator] with respect to support [formula] and bin size [formula]. Figure [\ref=td2] summarizes the estimation results for this specific time thirty minute window. The baseline intensity of the limit-order process L is about four times larger than the baseline intensity of trades process T. In both processes, we observe a strong and quite similar selfexcitement. The crossexcitement, however, is obviously directed: we observe a very strong crossexcitement from T on L, but hardly an effect from L on T. The estimated interactions can be summarized in the branching-matrix estimate

[formula]

See Remark [\ref=cov] for the calculation of the point estimates as well as the 95%-confidence bounds of the branching-matrix components. Also see the explanations after [\eqref=stability_criterion] for the interpretation of the branching-matrix that is indicated in the right matrix. The largest eigenvalue of matrix [\eqref=bm], i.e., the stability-criterion estimate, is 0.72. The strong asymmetry in [\eqref=bm] may be interpreted such that the trades cause the limit orders (and cancelations) and not vice versa. In further analysis, we found that the estimated branching-matrix, and in particular the asymmetric crossexcitement, is quite stable over all thirty minute windows of the busy trading hours (not illustrated). In the crossexcitement from T on L, we observe local maxima at half and whole seconds. This effect may have two causes: it reflects a preference either for absolute or for absolute round times. To put it differently: some of the order-sending algorithms that indeed react on trade events may have an implemented lag of half or full seconds.

In a second approach, we fit the Hawkes model to the same sample as above. This time however, we ignore the best support choice and set it naively to [formula] only. In addition, we apply an extremely small bin size of [formula]. In the first milliseconds after each event, the results indicate an inhibitory effect; the Hawkes model does not allow for negative excitement. Still, this result is not surprising. It reflects the fact that it takes at least 3 or 4 milliseconds for any market participant to observe and react to a change in the LOB. In the smoothed function-estimate of the selfexcitement of the first component (the trades process), we detect local maxima at [formula] multiples. As above, this may be is a sign, that [formula] is the "resolution" of some of the algorithms that drive the market. Also note that in this naive fit, the baseline-intensity estimates are much larger than in the first fit: these large values are a compensation for the too small support choice.

Naturally, the fitted Hawkes model is only completely specified when we smooth the results from the estimation method on the grid by some kind of smoothing mechanism that yields a function [formula]. We do this with a cubic smoothing spline method. Having thus completely specified the model, we apply a Kolmogoroff-Smirnov test on the transformed interarrival-times; see Section [\ref=diagnostics]. The test rejects the fitted model for the [formula]-window. This is not surprising: given the very large sample-size, we are very likely to include "abnormal" interarrival times that our model cannot catch; the Kolmogoroff-Smirnov test is particularly sensitive to such outliers. Dividing the [formula]-windows into smaller samples of 100 events yields plausible p-values (not illustrated). For further interpretation of the diagnostics; see the discussion in Section [\ref=interpretation] below.

Interpretation of the estimation results

The interpretation of the estimation results from Section [\ref=Bivariate_estimation] is not straightforward: observing the income of an order makes people (respectively, algorithms) send other orders. In this sense, we may expect some quite direct true excitement in LOB data. In our modeling approach however, any fluctuation of exogenous processes that influence the observed event-process will also be detected as selfexcitement. Candidates for such covariate processes in our context are volume, arrival of orders away from the best bid or best ask price, spread, or even data from other assets such as options on S&P 500 E-mini futures. A most natural way to model this situation would be a joint multivariate Hawkes model. However, doing statistics with so little knowledge about the state (or even the dimensionality) of the process will yield new problems. So the best way to get rid of artificial selfexcitement in the Hawkes model is presumably to make the baseline intensity more flexible. For an example of such a Hawkes model with stochastic baseline-intensity; see [\citet=zhao12]. To summarize: our estimation method can indeed detect self- or crossexcitement in data. However, we ought to be careful with interpretation of these terms.

The Hawkes fit is meaningful and fertile despite of the criticism above and despite of the vanishing p-values in our application: plots of excitement estimates as in Figure [\ref=td2] are visualizations of huge event-datasets in a compact and at the same time informative way. In that sense, any Hawkes fit--and our estimation method in particular--can be used as a graphical tool for exploratory event-stream analysis. Furthermore, even if the Hawkes model assumption may be completely wrong, an excitement-function estimate ij(  ·  ) is also theoretically meaningful. It is an estimate for the best linear filter of [formula] which is a relevant quantity in all stationary models.

Conclusion

This paper demonstrates that applying methods from time series theory to the bin-count sequences of point process data yields a useful and intuitive nonparametric estimation method for the multivariate Hawkes process. The price for the fertile simplicity of the method is a bias due to the discretization involved. Simulation studies support that this bias can be controlled and that it is negligible for most practical means. The technique presented depends on the choice of the bin size and the assumed support of the excitement function(s). Methods for a sensible choice of these parameters are given. In any application, the robustness with respect to these choices ought to be studied.

Due to space constraints, the presentation leaves out obvious subsequent topics. Confidence bounds for the rate of endogeneity (the branching parameter of a univariate Hawkes process), estimation of the excitement function on a nonequidistant estimation-grid, derivation of power-law decay-parameter estimates in the parametric case via a linear regression on the log/log values of the pointwise estimates, and estimation of marked Hawkes processes: using the concept of our paper as a starting point, all these aims can be achieved in a straightforward manner.

Finally, note that in view of the analogy between discrete-time INAR(p) sequences and continuous-time Hawkes processes, analysts using the Hawkes model may consider to directly apply the INAR(p) model in the first place--as most event data live on relatively discrete time grids.

Acknowledgements

M.K. is indebted to Paul Embrechts for guidance and support during the preparation of the paper. The author acknowledges financial support from ETH RiskLab and the Swiss Finance Institute. Furthermore, M.K. thanks Robert Almgren for sharing his expertise on limit-order-book data, Marius Hofert as well as Martin Maechler [\citep=maechler12] for support with R, Valérie Chavez-Demoulin as well as Thibault Vatter for numerous comments on earlier versions of the paper, and Rita Kirchner as well as Anne MacKay for help with the editing.

Proofs

Proof of Proposition [\ref=white_noise]

First, we establish that

[formula]

defines a white noise sequence. Stationarity of [formula] follows from the stationarity of [formula]. For the sequel of the proof, fix any [formula]. From the property [formula] of the thinning operation from Definition [\ref=mv_thinning_operator] we get

[formula]

For the autocovariances of the sequence [formula], first note that the error [formula] is uncorrelated with any previous value [formula] of the original sequence. Indeed:

[formula]

So that we get, for n' < n (and then, by symmetry, for n'  ≠  n),

[formula]

We have established that [formula] is a white noise sequence. In a second step, we derive its marginal covariance matrix. Since [formula] and [formula], [formula], we obtain

[formula]

Using independency of [formula] from the past of the process and plugging in [formula] yields

[formula]

As the components of [formula] are Poisson distributed and mutually independent, we have [formula]. For the second term in [\eqref=u_var], we condition the difference in the expectation on the past of the process. Then the thinnings become the only source of randomness. Furthermore, the counting series of the thinnings are independent of [formula]. So:

[formula]

For fixed [formula] and [formula] we have

[formula]

The components [formula] of this vector are Poisson distributed with parameters [formula]; see Definition [\ref=mv_thinning_operator]. As the thinnings involved are all independent by definition, the components are uncorrelated. Therefore, the covariance matrix of the vector is [formula] and

[formula]

Plugging in the random variables [formula] and taking the expectation, we continue from [\eqref=u_var] and find

[formula]

[formula]

Proof of Theorem [\ref=inference]

The first part of the proof largely depends on matrix manipulations. So it is important to remind the reader that all vectors are understood as column vectors. We rewrite the INAR(p) sequence [formula] as a standard multivariate linear autoregressive time series with white-noise error sequence [formula]

[formula]

see Corollary [\ref=AR-representation]. Then the distributional properties of the CLS-estimator are derived similarly as in [\citet=luetkepohl05], pages 70-75, where independent errors are assumed. In the following, let [formula] be the design matrix from the CLS Definition [\ref=CLS] with respect to the sample [formula]. Furthermore, let [formula] Note that [formula] as well as [formula] depend on n. We work under the assumption that

[formula]

In addition, we use that, for n  →    ∞  ,

[formula]

where [formula] has the same distribution as any of the columns of the design matrix [formula]. We postpone the reasoning for [\eqref=A2] to the end of the proof. As a first step, weak consistency of [formula] is proven. To that aim, we will use that

[formula]

see Definition [\ref=CLS].

[formula]

By [\eqref=A1], the second factor converges in probability to the constant matrix Γ. By [\eqref=A2], the first factor has the same asymptotic distribution as [formula] where W̃ is a matrix consisting of jointly normally distributed entries not depending on n. So [formula] and therefore [formula]. For establishing the asymptotic distribution, we treat the difference of the estimated and true vectorized parameter-matrix in a similar way:

[formula]

In the third step of the calculation above we use that

[formula]

for matrices A,B and identity matrix I such that the calculations are consistent dimensionwise; see A.12 in [\citet=luetkepohl05]. It follows from [\eqref=prod] together with [\eqref=A1] that [formula] has the same asymptotic distribution as

[formula]

With [\eqref=A2], we then find that the asymptotic distribution of [\eqref=aux2]-and therefore of [formula]-is centered normal with covariance matrix

[formula]

We still have to establish [\eqref=A2]. To that aim, we rewrite the left-hand side of [\eqref=A2] as

[formula]

where [formula]. Note that, for [formula], [formula] is the (k - p)-th column of the design matrix [formula]. Now, let [formula]. We show that for the sequence [formula], a central limit theorem for vector-valued martingale differences can be applied. Proposition 7.9 from [\citet=hamilton94] states that if [formula] is such that

it defines a vector-valued martingale difference sequence, i.e., there is a filtration [formula] such that [formula] is Hk-measurable and [formula],  [formula],

[formula] is a positive definite matrix independent of k,

for all [formula] and for all [formula],

[formula]

where [formula] denotes the i-th component of [formula], and

[formula],

then, for n  →    ∞  ,

[formula]

Proof of (a) Define the filtration [formula] by setting

[formula]

Then one can easily check that [formula] is Hk- measurable. It suffices to prove the martingale-difference property for the sequence [formula] since [formula] for k' < k and therefore [formula] are Hk - 1-measurable. But then, because

[formula]

we obtain the martingale difference property:

[formula]

Proof of (b) Independency of k follows from stationarity of [formula]. Choose k = 0. We need to show that, for [formula],

[formula]

With [\eqref=kronecker_rule], we find

[formula]

and therefore

[formula]

To establish [\eqref=claim], we define the σ-algebra

[formula]

Note that [formula] is F-measurable and [formula] is independent of F. Using these facts when considering the expectation of the conditional variance of [formula], we obtain

[formula]

Since

[formula]

the summand [formula] is the only term that contributes to the conditional covariance matrix in [\eqref=conditional_variance]--the other summands in [\eqref=bfu] are constant with respect to F and [formula] is independent of F. So we have [formula] and continuing with [\eqref=conditional_variance] we find

[formula]

where [formula] in [\eqref=bfa_margin] is chosen in such a way that [formula]. (Remember that [formula], by assumption.) The strict inequality in [\eqref=bfa_margin] follows because, for [formula] such that bj0  ≠  0, we have that

[formula]

for some [formula] and some [formula] dependent on j0. Note that [formula] denotes the l-th component of [formula]. By stationarity, [formula] is irrelevant. And the case that [formula] for some [formula] we have excluded, so the strict inequality follows.

Proof of (c) Note that claim (c) follows if [formula] for [formula] The boundedness of these expectations is established for the univariate case in Corollary 1 of [\citet=kirchner15a]. For the multivariate case, one can argue similarly via the existence of the moment generating function in a neighborhood of zero. Proof of (d) We show that [formula] is ergodic. Then the claim of (d) follows with the Birkhoff-Khinchin Ergodic Theorem. The sequence [formula] can be represented as margin of a pd-dimensional INAR(1) sequence [formula]; see [\citet=latour97]. It is easily checked that the latter is an irreducible, aperiodic Markov chain on [formula]. So [formula] is ergodic; see [\citet=durrett95], page 338. As margins of ergodic processes are ergodic, [formula] also is ergodic. As [formula] can be written as a measurable function of the past of [formula], [formula] is also ergodic. Finally, [formula] is ergodic because it is a measurable transformation of the ergodic sequence [formula].

[formula]

Figures