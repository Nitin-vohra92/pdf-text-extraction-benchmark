Interpreting gains and losses in conceptual test using Item Response Theory

Introduction

Conceptual tests are widely used by physics instructor to asses students' conceptual understanding and compare teaching methods. In particular, the Force Concept Inventory [\cite=hestenes_force_1992] (FCI) evaluate student's mastering of Newton laws [\cite=hake_interactive-engagement_1998]. It consists of 30 multiple-choice questions where incorrect answers are based on the most frequently answers given by students in interviews. Many topics are covered by the FCI : kinematics, identification of forces and the three Newton's laws [\cite=hestenes_force_1992] [\cite=scott_exploratory_2012]. Instructors usually use the raw score or the Hake gain [\cite=hake_interactive-engagement_1998] to evaluate global student's progression. Item Response Theory (IRT) provide a more theoretically grounded measure of student's progression [\cite=wright_observations_1989] [\cite=wright_history_1997] [\cite=wallace_concept_2010]. Over the past decade, IRT have been applied with success to concept inventories, in particular to the FCI [\cite=morris_testing_2006] [\cite=planinic_rasch_2010] [\cite=wang_analyzing_2010] [\cite=morris_item_2012] [\cite=han_dividing_2015]. Student's raw score or student's proficiency given by IRT provide a global measure of the acquisition of the Newtonian concepts.

A closer look to student's answer in a test-retest situation has shown that while the total score to the test is highly reliable, 31% of the student's answers change from test to retest, suggesting weak reliability for individual answers [\cite=lasry_puzzling_2011]. Looking how answers of students change between a pre-test - before instruction - and a post-test - after instruction - using a database embedding more than 13 000 students' answers, Lasry et al. [\cite=lasry_two_2014] revealed a strong positive correlation between the initial score and the proportion of incorrect answers on the pre-test that were changed to correct answers on the post-test - the gains. A symmetric result was found for the losses - the proportion of correct answers on the pre-test that were changed to incorrect answers on the post-test, strongly and negatively correlated to the initial score. This result suggests that students with higher prior level learn more and forget less than students with lower prior level.

In this article we show that IRT can be used to qualitatively predict those experimental data while offering another interpretation of the previous results. The observed correlation mainly comes from inherent properties of the test rather than reflecting the level of progression of students. We show in particular that the student's proficiency progression, as obtained by IRT, increases for low proficiency students, a conclusion at the opposite of the previous interpretation.

The article is organized as follow : section [\ref=section:GL] provides definition of gains and losses; section [\ref=section:IRT] introduces IRT theory and the underlying assumptions; section [\ref=section:IRT_GL] compares theory's predictions with experimental data; section [\ref=section:IRT_changes] exploit IRT to predict answer's changes; finally section [\ref=section:GL_learning] and [\ref=section:conclusion] discuss and conclude this work.

gains and losses

Consider the situation of students taking a same test two times : the first one before instruction and the second one after instruction. It is hoped that the score of each student increases, so that a part of answers which were initially wrong becomes correct. Following Lasry et al. [\cite=lasry_two_2014], we define the gain G as the proportion of incorrect answers on the pre-test that change to correct answers on the post-test. Similarly, the loss L is defined as the proportion of correct answers on the pre-test that change to incorrect answers on the post-test. We then introduce ICi as the proportion of students who change from an incorrect (I) to a correct (C) answer at the question i and Ii as the proportion of initial incorrect answers. gains and losses are then defined by [formula] and [formula], where [formula] denotes the average over the questions of the test. Ci is the proportion of initial correct answers to question i so that [formula] is the average pre-test score of the students. Using data from more than 13,000 students' answers on the Force Concept Inventory (FCI), Lasry et al. [\cite=lasry_two_2014] measured dependance of gains and losses with prior knowledge (pre-test score). As shown in Fig. [\ref=fig:GLmazurIRT], students with higher prior knowledge have higher gain and smaller loss than students with lower prior knowledge. In order to interpret these results, it is first necessary to draw the same graph when no learning occurs. That is to say when the same test is taken two times consecutively, with student not memorizing their previous answers and not having learned anything between the two tests. We show in the next sections how IRT is able to answer this question.

The Item Response Theory

Item Response Theory (IRT) belongs to the family of latent trait modeling [\cite=kamata_note_2008]. In those models, each student is described by a number of latent traits, also call proficiencies. The answer of a student to a question is thought of as the result of the interaction between the capabilities of the person taking the test and the characteristics of the test items. The score of a student to an item is modeled by a probabilistic function of his proficiencies and the item's characteristics. A consequent number of knowledge and skills are always necessary to give a correct answer [\cite=reckase_multidimensional_2009] but in many cases, only one proficiency is sufficient to determine the student score. This is call unidimensional Item Response Theory but is often simply called IRT. This assumption was shown to be valid to model student's answer to the FCI [\cite=planinic_rasch_2010] [\cite=wang_analyzing_2010] and will be assumed in the following.

Let's note θ the proficiency of a student. Each question i is modeled by a function Pi(θ) which describes the probability of a student with proficiency θ to correctly answer to the question i. Pi functions, called item characteristic curves, are often assumed to be generic "S-shape" functions (see Fig. [\ref=fig:PiQ1Q13]), called logistic function, whose variations characterize each questions. In the three-parameter item model, Pi(θ) is given by

[formula]

where ai, bi and ci are parameters of the question : ai is its discrimination power, bi its difficulty and ci the probability of guessing. The parameters are estimated by statistical techniques using a large pool of students answers. Other models exist such as the two-parameter model (ci = 0), the Rasch model (ci = 0 and ai = 1) and the non-parametric kernel smoothing approach [\cite=ramsay_kernel_1991]. All these models have been applied to the FCI [\cite=morris_testing_2006] [\cite=planinic_rasch_2010] [\cite=wang_analyzing_2010] [\cite=morris_item_2012] [\cite=han_dividing_2015]. For instance, Pi functions for question 1 and 13 of the FCI are plotted in Fig. [\ref=fig:PiQ1Q13]. Question 13 is more difficult than question 1 (b13  >  b1) so its curve is more "on the right" of the graph. Its discrimination is also larger (a13 > a1) so that the S-shape is steeper. Finally, the guessing parameter is lower (c13  <  c1), as seen on the value of Pi when θ goes to -    ∞  .

The true score (in %) of a group of students with proficiency θ is given by [formula]. Because of the probabilistic nature of IRT, the score S(θ) for a given proficiency θ differs from the observed score of a student with that proficiency θ - the number of correct answer given by the student divided by the number of questions. The true score S(θ) is only recovered as an average over a large number of equal-proficiency student's individual observed scores. The observed score is also named the raw score and one strength of IRT is to convert this raw score, which is a discrete bounded variable, into a continuous unbounded variable, θ, which is assumed to be an interval scale - i.e. a scale which can be used to quantify a progression or a difference of proficiency between students [\cite=wright_observations_1989].

IRT prediction of gains and losses

The objective of a course is to increase student's proficiency. Let's write θpre the proficiency of a student before instruction and θpost its proficiency after instruction. By definition, the probability of choosing the correct answer to the question i during the pre-test is Pi(θpre). For the same reason, this probability is Pi(θpost) for the post-test. For a wide group of student with the same proficiencies, we get Ii  =  1  -  Pi(θpre) and ICi  =  (1 - Pi(θpre))  Pi(θpost). Reporting these equations into the definition of the gain and the loss leads to

[formula]

[formula]

where δPi(θ) is the difference between probability of success of question i and average test score S for a given proficiency :

[formula]

By definition [formula]. In the particular case when θpre  =  θpost (i.e. when no instruction occurs), [formula] is the variance of the Pi's for a given θ and is a characteristic of the test.

Equations ([\ref=eq:G2]) and ([\ref=eq:L2]) show that IRT enables us to predict measured values for G and L once θpre, θpost and all the Pi's are known. However, data of Lasry et al. [\cite=lasry_two_2014] give values of G and L as functions of Spre so informations about θpre, θpost and all the Pi's function are missing.

First Pi functions are taken form literature. Using the three-parameter model, Wang and Bao [\cite=wang_analyzing_2010] performed an IRT analysis of the FCI using their own database of 2 800 student's answers, leading to the knowledge of the 30 Pi functions. The measurements obtained by Wang and Bao with their students can be used for any students because characteristics of questions are independent of the population used to obtained them. This property is known as parameter invariance [\cite=rupp_understanding_2006]. Hence there Pi functions are used here.

Secondly, foreach values of Spre we estimated Spost from data of Lasry et al. [\cite=lasry_two_2014] using

[formula]

which comes from the definition of G and L and the fact that [formula].

And finally θpre and θpost are estimated by reversing the relation giving S as a function of θ : [formula]. This is an approximation where the observed raw score is assumed to be equal to the true score. The sample of Lasry et al. [\cite=lasry_two_2014] contains 13  000 students divided into 9 bins leading to an average of 1  400 students for each raw score. In this case the hypothesis of equating the raw score to the true score seems reasonable.

Figure [\ref=fig:GLmazurIRT] shows that eqs. ([\ref=eq:G2]) and ([\ref=eq:L2]) match fairly well the experimental measurements, indicating that IRT is able to correctly predict gains and losses. Discrepancies can be attributed to both uncertainties of measurements of Pi and to an unperfect parameter invariance. Such a case can occur in particular when the hypothesis of unidimensionality does not hold. As shown by Scott and Schumayer [\cite=scott_exploratory_2012], while a unique proficiency can be used to describe student's characteristic, a 5 dimensional model seems preferable. Our results show that a one-dimensional model is able to give the global tendency for the gain and the loss. A more detailed analysis is reported for future work.

As seen in Fig. [\ref=fig:GLmazurIRT], gain is an increasing function of student's initial score. A tempting interpretation is to say that students with higher initial knowledge learn more than students with lower initial knowledge. The reverse is also true for loss : students with higher initial knowledge have lower loss than students with lower initial knowledge. However this argument implicitly assumes that gains and losses are zero when no learning occurs. We now show that this is not the case, which at least makes the previous conclusion unsecured. To do so, we use IRT to estimate G and L when θpost  =  θpre, using equations ([\ref=eq:G2]) and ([\ref=eq:L2]). Results are plotted in Fig. [\ref=fig:GLTRT], which clearly show that even when no learning occurs gain is an increasing function of the pre-test score and raise up to one. Similarly, loss goes down from one to zero as pre-test score increases. For a pre-test score value of 50% both gains and losses have the same value around 35%. Such a change in student answers at the same question has been observed between two successive passes of the FCI [\cite=lasry_puzzling_2011]. Reported values of gains and losses were 18% and 20% for a population mean score of 47%. Discrepancy between their experimental measures and IRT prediction could largely be attributed to a memory effect because students took the tests two times in the same week so they may have memorized some of there initial answers. At the contrary, our IRT model assumes the independence between the test-retest, i.e. that students have not memorized any of their previous answers.

Proportion of answer's change

In order to interpret why gains and losses can have such high values even when no learning occurs, we focus directly on the global proportion of answer's change. In a test-retest situation, we have :

[formula]

where S = Spre = Spost. The explicit dependence of S and δPi with θpre  =  θpost have been omitted for clarity. The first term of the right hand side of equation ([\ref=eq:IC-TRT]) is a parabolic function of S and does not depend on the considered test. Hence, for any conceptual test, this part is identical. The second term on the right hand side of equation ([\ref=eq:IC-TRT]) depends on the item characteristic curves and consequently on the test. Values of [formula] have been plotted for the FCI as a function of the score in Fig. [\ref=fig:ICTRT]. It is clear that in this case, the contribution of [formula], while not negligible, is rather small. Consequently, for a group of students with a true score of 50 %, nearly 18 % of answers change from correct (resp. incorrect) to incorrect (resp. correct) in a test-retest situation. This result has a consequence on the reliability of the test and on the interpretation of gains and losses. In order to interpret gains and losses in term of learning outcome, their values should be as small as possible in a test-retest situation. As a consequence, values of [formula] should also be as small as possible. Because the first term of equation ([\ref=eq:IC-TRT]) does not depend on the test, one can only influence the [formula] term in order to make it as high as possible (so that [formula] decreases). It immediately leads to the conclusion that one has to choose questions - therefore the Pi's functions - in order to maximize values of [formula] for all θ.

In order to understand how to choose those Pi's functions, we consider the simple case of a test with only 3 questions. Three different cases are considered, each one corresponding to a particular set of Pi's functions. The three cases are named test A, B and C and their item characteristic functions are plotted in Fig. [\ref=fig:courbes3Tests] (left column). For each θ, the proportion of answer's change is given by [formula], where [formula] denotes the averaging over the 3 questions of the test. Hence, each individual question i has a contribution of Pi  (1 - Pi). This contribution is null when Pi = 0 or 1 and has a maximal value of 0.25 when Pi = 0.5.

Test A has three questions whose characteristic curves overlap for a wide range of θ. As a consequence, for a wide range of θ all individual questions will contribue to the proportion of answers that change. For instance, for a true score of 50% (θ = 0), P1(θ)  =  0.88, P2(θ)  =  0.5, and P3(θ)  =  0.12, leading to P1  (1 - P1)  =  P3  (1 - P3)  =  0.1 and P2  (1 - P2) = 0.25. Hence, for a score of 50%, the proportion of change, which is the average of these three values, is about 15%. The representative curve of [formula] is very similar to the one obtained for the FCI, indicating that a lot of item characteristic curves of the FCI overlap, as already noted in previous studies analyzing the FCI using a unidimensional IRT [\cite=morris_testing_2006] [\cite=planinic_rasch_2010] [\cite=wang_analyzing_2010] [\cite=morris_item_2012] [\cite=han_dividing_2015].

At the opposite, test C has three questions whose characteristic curves do not overlap - i.e. the range of θ where these functions go from a value close to 0 to a value close to 1 are well separated (see Fig. [\ref=fig:courbes3Tests]). As a consequence, each question will contribute separately to the proportion of answer's change. For instance, for a true score of 50% (θ = 0), P1(θ)≃1, P2(θ)  =  0.5, and P3(θ)≃0, leading to P1  (1 - P1)  =  P3  (1 - P3)≃0 and P2  (1 - P2) = 0.25. Hence, for a score of 50%, the proportion of change - which is the average of the Pi values - is 0.25  /  3≃0.08. This value is much smaller than for test A. In a test with N separated questions, the maximal value of [formula] is 0.25 / N and is obtained for values of S  =  0.5 / N, 1.5 / N, ... , (N - 0.5) / N. In a test with N = 30 separated-questions, maximal value for [formula] is about 1%. Hence the change of answers occurs very rarely, and values of gains and losses remain very small.

Finally test B shows the transition between test A and the extreme case of test C.

Interpretation of gains and losses when learning occurs

According to the discussion of the previous section, the interpretation of gains and losses should be separated in two extreme cases : when a wide majority of item characteristic curves overlap - like in test A - and when none of the item characteristic curves overlaps - like in test C.

In the first case, [formula] is small and equations ([\ref=eq:G2]) and ([\ref=eq:L2]) reduce to G = Spost and L = 1 - Spost. Hence the gain is more or less the post-test score and does not add any supplementary informations on student's learning. One can still want to isolate the part of the gain due to instruction by defining [formula]. In the case of type A test, ΔG = Spost - Spre = graw, leading to the so-called raw gain (because G = Spre when no learning occurs). The analysis of Lasry et al. [\cite=lasry_two_2014] data shows that graw is a decreasing function of the pre-test score. Does it mean that students with lower initial knowledge gain more than students with higher initial knowledge ? No because student's post score is limited to 100% so the raw gain graw tends to zero when the pre-test score tends to 100%. Also the score is an ordinal scale and not an interval scale [\cite=wright_observations_1989] [\cite=wright_history_1997] [\cite=wallace_concept_2010]. As a consequence, the raw score can only lead to a sorting of students but an increase of 1 point for a student with a low initial score does not reflect the same learning than an increase of 1 point for a student with a high initial score. A correct comparison of progress has to invoque an interval scale such as the student proficiency θ introduced in the previous sections [\cite=wright_observations_1989] [\cite=wright_history_1997] [\cite=wallace_concept_2010]. Fig. [\ref=fig:RawGainSpre] plots the raw gain as a function of the pre-test score for given values of student’s learning increase Δθ. As seen on this figure, a given value of graw corresponds to various value of student's progression Δθ, depending of the initial student's score.

In the second case (test of type C), where all questions are well separated, the proportion of questions that changes when no learning occurs is nearly null - it is lower than 5% for N  ≥  5. Assuming a student positive progression Δθ  =  θpost  -  θpre greater than the error range of all questions (i.e. [formula] with ai the discrimination power), the number of answers that change from incorrect to correct is Spost  -  Spre leading for the gain to

[formula]

Interestingly, one recovers in this limit the Hake's gain [\cite=hake_interactive-engagement_1998], which can be interpreted as the proportion of questions changing from incorrect to correct in a test comprising seperated item response curves (like test C). The number of answers that change from correct to incorrect is null and L = 0. However, like the raw gain, the Hake gain is not an interval scale [\cite=wallace_concept_2010] and has to be taken with due care when comparing student's progression, as already emphasized. To illustrate this, let's consider an hypothetical test where the true score is a logistic function of the proficiency : S  =  (1  +   exp ( - θ))- 1. This model is characteristic of a test where question's difficulties are distributed over the proficiency scale following a gaussian law : there are few easy questions, few hard questions and a wide majority of questions with an intermediate level of difficulty. The Hake gain is plotted on Fig. [\ref=fig:HakeGainSpre] as a function of the pre-test score for various fixed value of student's learning Δθ that are typical of student's learning (see for instance Fig. [\ref=fig:deltaThetaMazur] for typical values of Δθ in a mechanic course). As can be seen, the gain is an increasing function of the pre-test score for a fixed value of student's learning. Hence, the fact that the gain is larger for initial high level students than for initial low level students does not necessarily reveal that the initial high level students have learned more. Moreover, a given value of G corresponds to various value of student's progression Δθ, depending of the initial student's score. As shown in Fig. [\ref=fig:HakeGainSpre], a fixed value of the gain - for instance 0.34 - correspond to a strong learning for low pre-test score (Δθ = 2 for S=8%), a medium learning for medium pre-test score (Δθ = 1 for S=30%) and a low learning for high pre-test score (Δθ = 0.5 for S  =  80%). This clearly shows that the Hake gain should not be used to compare student's progression when they have different pre-test score, even in test of type C.

Table [\ref=table:ResumeGL] summarizes values of G and L for the two limit cases. As can be seen, ΔG reduces to the raw gain for type A tests and to the Hake gain for type C tests.

We conclude this section by discussing the efficiency of instruction with respect to the initial level of the students. As already emphasized, the proficiency θ has good properties [\cite=wright_observations_1989] [\cite=wright_history_1997] [\cite=wallace_concept_2010] and hence could be used to determine the learning Δθ of a student, Δθ  =  θpost  -  θpre. This increase of proficiency is plotted in Fig. [\ref=fig:deltaThetaMazur] as a function of the pre-test score for the data of Lasry et al. [\cite=lasry_two_2014]. We have evaluated θ using the scores by inverting the relation S(θ). According to Lasry et al. [\cite=lasry_two_2014], uncertainties on pre-test scores, gains and losses are about 2%, leading to uncertainties on the post-test score of the same order of magnitude. These uncertainties lead to uncertainties on the proficiencies, particularly for low or high scores due to the 'S' shape of the curve, and are represented in Fig. [\ref=fig:deltaThetaMazur]. If θ is assumed to be the good scale for measuring the learning, Fig. [\ref=fig:deltaThetaMazur] clearly shows that learning decreases as the pre-test score increases. This is an opposite conclusion with the first interpretation of the evolution of gains and losses with pre-test score, but in accordance with the evolution of [formula] with pre-test score. It seems to state that our teaching methods are more efficient on students with low prior knowledge. We recall that this result is based on data from more than 13,000 students who had taken the FCI at the beginning and at the end of an introductory physics course in a large variety of institutions: US high schools (10,007) , three Canadian two-year colleges (971), a US public university (1560) and three top-tier private universities (884) [\cite=lasry_two_2014]. Due to possible correlations between students' prior knowledge and student's institution, this could reflect a difference between institutions. But this also could mean that it is more difficult in an introductory physics course to give the same increase of learning to students with high prior level knowledge than to students with low prior level knowledge. This discussion is out of the scope of this article but in order to answer this question one would have to evaluate Δθ for each student in a group following the same course with the same teacher, plotting the same curve as in Fig. [\ref=fig:deltaThetaMazur] and finally perform a comparison across institutions.

Conclusion

We have shown that IRT is able to fairly well predict experimental measurements of gains and losses with the FCI when learning occurs. In addition, IRT shows that values of gains and losses for the FCI are rather high even when no learning occurs. The reason being that item characteristics curves overlap. All errors associated to individual questions contribute together to the probability of answer's change, leading to a difficult interpretation of gains and losses. In such a case the gain is more or less the post-test score and does not reveal that initial high level students have learned more that initial low level students.

In the case where item characteristic curves do not overlap, answer's changes are very low, the gain reduces to the Hake gain while the losses drop to zero.

We have shown that the effect of instruction can be assessed by looking to the proficiency increase instead of looking to the gain increase. The proficiency increases more for low-level student (i.e. low pre-test score).

Acknowledgments

This project was supported by the Initiative d'Excellence (IDEX) from the Université Fédérale Toulouse Midi-Pyrénées.