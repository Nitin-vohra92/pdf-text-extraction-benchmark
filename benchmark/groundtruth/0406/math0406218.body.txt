Central Limit Theorem and convergence to stable laws in Mallows distance

Running Title: CLT and stable convergence in Mallows distance

Keywords: Central Limit Theorem, Mallows distance, probability metric, stable law, Wasserstein distance

Lemma Proposition Corollary Conjecture Definition Example Remark Assumption

Proof

Proof .

Introduction and main results

The spirit of the Central Limit Theorem, that normalised sums of independent random variables converge to a normal distribution, can be understood in different senses, according to the distance used. For example, in addition to the standard Central Limit Theorem in the sense of weak convergence, we mention the proofs in of L1 convergence of densities, in of L∞ convergence of densities, in of convergence in relative entropy and in and of convergence in Fisher information.

In this paper we consider the Central Limit Theorem with respect to the Mallows distance and prove convergence to stable laws in the infinite variance setting. We study the rates of convergence in both cases.

For any r  >  0, we define the Mallows r-distance between probability distribution functions FX and FY as

[formula]

where the infimum is taken over pairs (X,Y) whose marginal distribution functions are FX and FY respectively, and may be infinite. Where it causes no confusion, we write dr(X,Y) for dr(FX,FY).

Define Fr to be the set of distribution functions F such that [formula]. show that for r  ≥  1, dr is a metric on Fr. If r  <  1, then drr is a metric on Fr. In considering stable convergence, we shall also be concerned with the case where the absolute rth moments are not finite.

Throughout the paper, we write Zμ,σ2 for a N(μ,σ2) random variable, Zσ2 for a N(0,σ2) random variable, and Φμ,σ2 and Φσ2 for their respective distribution functions. We establish the following main theorems:

Let [formula] be independent and identically distributed random variables with mean zero and finite variance σ2  >  0, and let [formula]. Then

[formula]

Moreover, Theorem [\ref=thm:mainr] shows that for any r  ≥  2, if dr(Xi,Zσ2)  <    ∞  , then  lim n  →    ∞dr(Sn,Zσ2)  =  0. Theorem [\ref=thm:main] implies the standard Central Limit Theorem in the sense of weak convergence [\cite=bickel].

Fix α∈(0,2), and let [formula] be independent random variables (where [formula], if α  >  1), and [formula]. If there exists an α-stable random variable Y such that sup idβ(Xi,Y)  <    ∞   for some β∈(α,2], then lim n  →    ∞dβ(Sn,Y)  =  0. In fact

[formula]

so in the identically distributed case the rate of convergence is O(n1 / β  -  1 / α).

See also Rachev and Rüschendorf (1992,1994), who obtain similar results using different techniques in the case of identically distributed Xi and strictly symmetric Y. In Lemma [\ref=lem:dsna] we exhibit a large class CK of distribution functions FX for which dβ(X,Y)  ≤  K, so the theorem can be applied.

Theorem [\ref=thm:main] follows by understanding the subadditivity of d22(Sn,Zσ2) (see Equation ([\ref=eq:sub])). We consider the powers-of-two subsequence Tk  =  S2k, and use Rényi's method, introduced in to provide a proof of convergence to equilibrium of Markov chains; see also . This technique was also used in to show convergence to Haar measure for convolutions of measures on compact groups, and in to show convergence of Fisher information in the Central Limit Theorem. The method has four stages:

Consider independent and identically distributed random variables X1 and X2 with mean μ and variance σ2  >  0, and write D(X) for d22(X,Zμ,σ2). In Proposition [\ref=prop:iff], we observe that

[formula]

with equality if and only if X1,X2  ~  Zμ,σ2. Hence D(Tk) is decreasing and bounded below, so converges to some D.

In Proposition [\ref=prop:rich], we use a compactness argument to show that there exists a strictly increasing sequence kr and a random variable T such that

[formula]

Further,

[formula]

where the T'kr and T' are independent copies of Tkr and T respectively.

We combine these two results: since D(Tkr) and D(Tkr + 1) are both subsequences of the convergent subsequence D(Tk), they must have a common limit. That is,

[formula]

so by the condition for equality in Proposition [\ref=prop:iff], we deduce that T  ~  N(0,σ2) and D = 0.

Proposition [\ref=prop:iff] implies the standard subadditive relation

[formula]

Now Theorem 6.6.1 of implies that D(Sn) converges to inf nD(Sn)  =  0.

The proof of Theorem [\ref=thm:mainstab] is given in Section [\ref=sec:stable].

Subadditivity of Mallows distance

The Mallows distance and related metrics originated with a transportation problem posed by Monge in 1781 [\cite=rachev2] [\cite=dudley]. Kantorovich generalised this problem, and considered the distance obtained by minimising [formula], for a general metric c (known as the cost function), over all joint distributions of pairs (X,Y) with fixed marginals. This distance is also known as the Wasserstein metric. reviews applications to differential geometry, infinite-dimensional linear programming and information theory, among many others. focused on the metric which we have called d2, while d1 is sometimes called the Gini index.

In Lemma [\ref=lem:covvalue] below, we review the existence and uniqueness of the construction which attains the infimum in Definition [\ref=def:mallows], using the concept of a quasi-monotone function.

A function [formula] induces a signed measure μk on [formula] given by

[formula]

We say that k is quasi-monotone if μk is a non-negative measure.

The function k(x,y)  =   - |x - y|r is quasi-monotone for r  ≥  1, and if r  >  1 then the measure μk is absolutely continuous, with a density which is positive Lebesgue almost everywhere. gives the following result, a two-dimensional version of integration by parts.

Let k(x,y) be a quasi-monotone function and let H1(x,y) and H2(x,y) be distribution functions with the same marginals, where H1(x,y)  ≤  H2(x,y) for all x,y. Suppose there exists an H1- and H2- integrable function g(x,y), bounded on compact sets, such that k(xB,yB)  ≤  g(x,y), where [formula]. Then

[formula]

Here [formula], where (X,Y) have joint distribution function Hi.

For r  ≥  1, consider the joint distribution of pairs (X,Y) where X and Y have fixed marginals FX and FY, both in Fr. Then

[formula]

where X*  =  F- 1X(U), Y*  =  F- 1Y(U) and U  ~  U(0,1). For r  >  1, equality is attained only if (X,Y)  ~  (X*,Y*).

Observe, as in , that if the random variables X,Y have fixed marginals FX and FY, then

[formula]

where H+(x,y)  =   min (FX(x),FY(y)). This bound is achieved by taking U  ~  U(0,1) and setting X*  =  F- 1X(U),Y*  =  F- 1Y(U).

Thus, by Lemma [\ref=lem:intprt], with k(x,y)  =   - |x - y|r, for r  ≥  1, and taking [formula] and H2  =  H+, we deduce that

[formula]

so (X*,Y*) achieves the infimum in the definition of the Wasserstein distance.

Finally, since taking r > 1 implies that the measure μk has a strictly positive density with respect to Lebesgue measure, we can only have equality in ([\ref=eq:covmax]) if [formula] Lebesgue almost everywhere. But the joint distribution function is right-continuous, so this condition determines the value of [formula] everywhere.

Using the construction in Lemma [\ref=lem:covvalue], establish that if X1 and X2 are independent and Y1 and Y2 are independent, then

[formula]

Similar subadditive expressions arise in the proof of convergence of Fisher information in . By focusing on the case r = 2 in Definition [\ref=def:mallows], and by using the theory of L2 spaces and projections, we establish parallels with the Fisher information argument.

We prove Equation ([\ref=eq:sub]) below, and further consider the case of equality in this relation. gives an equivalent construction to that given in Lemma [\ref=lem:covvalue]. If FY is a continuous distribution function, then FY(Y)  ~  U(0,1), so we generate Y*  ~  FY and take [formula]. Recall that if [formula] and [formula], we write D(X) for d22(X,Zμ,σ2).

If X1, X2 are independent, with finite variances σ21,σ22  >  0, then for any t∈(0,1),

[formula]

with equality if and only if X1 and X2 are normal.

We consider bounding D(X1  +  X2) for independent X1 and X2 with mean zero, since the general result follows on translation and rescaling.

We generate independent Y*i  ~  N(0,σ2i), and take [formula], say, for i = 1,2. Further, writing σ2  =  σ21  +  σ22, we define Y*  =  Y*1  +  Y*2 and set [formula], say. Then

[formula]

Equality holds if and only if (X*1  +  X*2,Y*1  +  Y*2) has the same distribution as (X*,Y*). By our construction of Y*  =  Y*1  +  Y*2, this means that (X*1  +  X*2,Y*1  +  Y*2) has the same distribution as (X*,Y*1 + Y*2), so [formula]. Thus, if equality holds, then

[formula]

and , showed that equality holds in Equation ([\ref=eq:linear]) if and only if h,h1,h2 are linear. In particular, Proposition 2.1 of [\cite=johnson5] implies that there exist constants ai and bi such that

[formula]

Hence, if Equation ([\ref=eq:linear]) holds, then hi(u)  =  aiu  +  bi almost everywhere. Since Y*i and X*i have the same mean and variance, it follows that ai = 1, bi = 0. Hence h1(u)  =  h2(u)  =  u and X*i  =  Y*i.

Recall that Tk  =  S2k, where [formula] is a normalised sum of independent and identically distributed random variables of mean zero and finite variance σ2.

There exists a strictly increasing sequence [formula] and a random variable T such that

[formula]

If Tkr' and T' are independent copies of Tkr and T respectively, then

[formula]

Since [formula] for all k, the sequence (Tk) is tight. Therefore, by Prohorov's theorem, there exists a strictly increasing sequence (kr) and a random variable T such that

[formula]

as r  →    ∞  . Moreover, the proof of Lemma 5.2 of shows that the sequence (T2kr) is uniformly integrable. But this, combined with Equation ([\ref=weak]) implies that lim r  →    ∞d2(Tkr,T)  =  0 [\cite=bickel]. Hence

[formula]

as r  →    ∞  . Similarly, d22(T,Zσ2)  ≤  {d2(T,Tkr)  +  d2(Tkr,Zσ2)}2, yielding the opposite inequality. This proves the first part of the proposition.

For the second part, it suffices to observe that [formula] as r  →    ∞  , and [formula], and then use the same argument as in the first part of the proposition.

Combining Propositions [\ref=prop:iff] and [\ref=prop:rich], as described in Section [\ref=sec:intro], the proof of Theorem [\ref=thm:main] is now complete.

Convergence of dr for general r

The subadditive inequality ([\ref=eq:sub]) arises in part from a moment inequality; that is, if X1 and X2 are independent with mean zero, then [formula], for r = 2. Similar results imply that for r  ≥  2, we have lim n  →    ∞dr(Sn,Zσ2)  =  0. First, we prove the following lemma:

Consider independent random variables [formula] and [formula], where for some r  ≥  2 and for all i, [formula] and [formula]. Then for any m, there exists a constant c(r) such that

[formula]

We consider independent Ui  ~  U(0,1), and set V*i  =  F- 1V(Ui) and W*i  =  F- 1W(Ui). Then

[formula]

as required. This final line is an application of Rosenthal's inequality [\cite=petrov] to the sequence (V*i  -  W*i).

Using Lemma [\ref=lem:rsub], we establish the following theorem.

Let [formula] be independent and identically distributed random variables with mean zero, variance σ2  >  0 and [formula] for some r  ≥  2. If [formula], then

[formula]

Theorem [\ref=thm:main] covers the case of r = 2, so need only consider r > 2. We use a scaled version of Lemma [\ref=lem:rsub] twice. First, we use Vi  =  Xi, Wi  ~  N(0,σ2) and m  =  n, in order to deduce that, by monotonicity of the r-norms:

[formula]

so that [formula] is uniformly bounded in n, by K, say. Then, for general n, define [formula], take m  =  ⌈n / N⌉, and u  =  n  -  (m - 1)N  ≤  N. In Lemma [\ref=lem:rsub], take

[formula]

and Wi  ~  N(0,Nσ2) for [formula], Wm  ~  N(0,uσ2) independently. Now the uniform bound above gives, on rescaling,

[formula]

and drr(Vm,Wm)  =  ur / 2drr(Su,Zσ2)  ≤  Nr / 2K. Further d22(Vi,Wi)  =  Nd22(SN,Zσ2) for [formula] and d22(Vm,Wm)  =  ud22(Su,Zσ2)  ≤  Nd22(S1,Zσ2). Hence, using Lemma [\ref=lem:rsub] again, we obtain

[formula]

This converges to zero since lim n  →    ∞d2(SN,Zσ2)  =  0.

Strengthening subadditivity

Under certain conditions, we obtain a rate for the convergence in Theorem [\ref=thm:main]. Equation ([\ref=eq:subadd]) shows that D(Tk) is decreasing. Since D(Tk) is bounded below, the difference sequence D(Tk)  -  D(Tk + 1) converges to zero, As in we examine this difference sequence, to show that its convergence implies convergence of D(Tk) to zero.

Further, in the spirit of , we hope that if the difference sequence is small, then equality 'nearly' holds in Equation ([\ref=eq:linear]), and so the functions h,h1,h2 are 'nearly' linear. This implies that if [formula] is close to its maximum, then X is be close to h(Y) in the L2 sense.

Following , we define a new distance quantity D*(X)  =   inf m,s2d22(X,Zm,s2). Notice that D(X)  =  2σ2  -  2σk  ≤  2σ2, where [formula]. This follows since F- 1X and Φ- 1 are increasing functions, so k  ≥  0 by Chebyshev's rearrangement lemma. Using results of , it follows that

[formula]

and convergence of D(Sn) to zero is equivalent to convergence of D*(Sn) to zero.

Let X1 and X2 be independent and identically distributed random variables with mean μ, variance σ2  >  0 and densities (with respect to Lebesgue measure). Defining [formula], if the derivative g'(u)  ≥  c for all u then

[formula]

As before, translation invariance allows us to take [formula]. For random variables X,Y, we consider the difference term Equation ([\ref=eq:kupper]) and write [formula], and h(u)  =  g- 1(u). The function k(x,y)  =    -  {x  -  h(y)}2 is quasi-monotone and induces the measure dμk(x,y)  =  2h'(y)dxdy. Taking [formula] and H2(x,y)  =   min {FX(x),FY(y)} in Lemma [\ref=lem:intprt] implies that

[formula]

since [formula]. By assumption h'(y)  ≤  1 / c, so

[formula]

Again take Y*1,Y*2 independent N(0,σ2) and set [formula]. Then define Y*  =  Y*1  +  Y*2 and take [formula]. Then there exist a and b such that

[formula]

where the penultimate inequality follows by Equation ([\ref=eq:brown]). Recall that D(X)  ≤  2σ2, so that D*(X)  =  D(X)  -  D(X)2 / (4σ2)  ≥  D(X) / 2. The result follows on rescaling.

We briefly discuss the strength of the condition imposed. If X has mean zero, distribution function FX and continuous density fX, define the scale invariant quantity

[formula]

We want to understand when C(X)  >  0.

If X  ~  U(0,1), then [formula]

If X has mean zero and variance σ2 then C(X)2  ≤  σ2 / (σ2  +  median(X)2).

By the Mean Value Inequality, for all p

[formula]

so that

[formula]

In general we are concerned with the rate at which fX(x)  →  0 at the edges of the support.

If for some ε  >  0,

[formula]

then lim p  →  1fX(F- 1X(p)) / φ(Φ- 1(p))  =    ∞  . Correspondingly if

[formula]

then  lim p  →  0fX(F- 1X(p)) / φ(Φ- 1(p))  =    ∞  .

Simply note that by the Mills ratio [\cite=shorack] as x  →    ∞  , Φ(x)  ~  φ(x) / x, so that as p  →  1, [formula].

The density of the n-fold convolution of U(0,1) random variables is given by fX(x)  =  xn - 1 / (n - 1)! for 0  <  x  <  1, hence F- 1X(p)  =  (n!p)1 / n, and fX(F- 1X(p))  =  n / (n!)1 / np(n - 1) / n, so that Equation ([\ref=eq:denspow]) holds.

For an Exp(1) random variable, fX(F- 1X(p))  =  1 - p, so that Equation ([\ref=eq:denspow2]) fails and C(X)  =  0.

To obtain bounds on D(Sn) as n  →    ∞  , we need to control the sequence C(Sn). Motivated by properties of the (seemingly related) Poincaré constant, we conjecture that [formula] for independent and identically distributed Xi. If this is true and C(X)  =  c then C(Sn)  ≥  c for all n.

Assuming that C(Sn)  ≥  c for all n, note that D(Tk)  ≤  (1  -  c / 4)kD(X1)  ≤  (1  -  c / 4)k(2σ2). Now

[formula]

so

[formula]

We deduce that

[formula]

or D(Sn)  =  O(nt), where t  =   log 2(1 - c / 2).

In general, convergence of d4(Sn,Zσ2) cannot occur at a rate faster than O(1 / n). This follows because [formula], where γ(X), the excess kurtosis, is defined by [formula] (when [formula]). Thus by Minkowski's inequality,

[formula]

Motivated by this remark, and by analogy with the rates discovered in , we conjecture that the true rate of convergence is D(Sn)  =  O(1 / n). To obtain this, we would need to control 1 - C(Sn).

Convergence to stable distributions

We now consider convergence to other stable distributions. review classical results of this kind. We say that Y is α-stable if, when [formula] are independent copies of Y, we have [formula] for some sequence (bn). Note that α-stable variables only exist for 0  <  α  ≤  2; we assume for the rest of this Section that α  <  2.

If X has a distribution function of the form

[formula]

where bX(x)  →  0 as x  →    ±    ∞  , then we say that X is in the domain of normal attraction of some stable Y with tail parameters c1,c2.

Theorem 5 of Section 35 of [\cite=gnedenko] shows that if FX is of this form, there exist a sequence (an) and an α-stable distribution function FY, determined by the parameters α, c1, c2, such that

[formula]

Although Equation ([\ref=eq:stabconv]) is obviously very similar to the standard Central Limit Theorem, one important distinguishing feature is that both [formula] and [formula] are infinite for 0  <  α  <  2.

We use the following moment bounds from . If [formula] are independent, then

[formula]

Now, using ideas of , we show that for a subset of the domain of normal attraction, dβ(X,Y)  <    ∞  , for some β  >  α.

We say that a random variable is in the domain of strong normal attraction of Y if the function bX(x) from Definition [\ref=def:dna] satisfies

[formula]

for some constant C and some γ  >  0.

shows that such random variables have an Edgeworth-style expansion, and thus convergence to Y occurs. However, his proof requires some involved analysis and use of characteristic functions. See also and , which use bounds based on the quantile transformation described above.

We can regard Definition [\ref=def:dsna] as being analogous to requiring a bounded (2 + δ)th moment in the Central Limit Theorem, which allows an explicit rate of convergence (via the Berry-Esséen theorem). We now show the relevance of Definition [\ref=def:dsna] to the problem of stable convergence.

If X is in the domain of strong normal attraction of an α-stable random variable Y, then dβ(X,Y)  <    ∞   for some β  >  α.

We show that Major's construction always gives a joint distribution (X*,W*) with [formula], and hence dβ(X,W)  <    ∞  . Following , define a random variable W by

[formula]

Then for w  >  1 / 2, F- 1W(w)  =  {c2 / (1 - w)}1 / α, and so for x  ≥  0,

[formula]

Now, since bX(x)  →  0, there exists K such that if x  ≥  K then bX(x)  ≥    -  c2 / 2.

By the Mean Value Inequality, if t  ≥   - 1 / 2, then

[formula]

so that for x  ≥  K

[formula]

Thus, if X is in the strong domain of attraction, then

[formula]

Hence dβ(X,W) is finite for all β if γ  ≥  1 and for β  <  α / (1 - γ), if γ  <  1. Moreover, shows that if Y is α-stable, then as x  →    ∞  ,

[formula]

and so Y is in its own domain of strong normal attraction. Thus using the construction above, dβ(Y,W) is finite for all β if α  ≥  1 and for β  <  α / (1 - α) otherwise.

Recall that the triangle inequality holds, for dβ or dββ, according as β  ≥  1 or β  <  1. Hence dβ(X,Y) is finite for all β if min (α,γ)  ≥  1 and for β  <  α / (1  -   min (α,γ)) otherwise.

Note that for random variables Xi in the same strong domain of normal attraction, dβ(Xi,Y) may be bounded in terms of the function bXi(x). In particular if there exist C,γ such that bXi(x)  ≤  C / |x|γ then sup idβ(Xi,Y)  <    ∞  , so the hypothesis of Theorem [\ref=thm:mainstab] is satisfied.

We use the bounds provided by Equations ([\ref=eq:vb1]) and ([\ref=eq:vb2]). We consider independent pairs (X*i,Y*i) having the joint distribution that achieves the infimum in Definition [\ref=def:mallows]. Then by rescaling we have that

[formula]

We deduce that in the case of identical variables, dβ(Sn,Y) (and hence dα(Sn,Y)) converges at rate O(n1 / β  -  1 / α).

We now combine Theorem [\ref=thm:mainstab] and Lemma [\ref=lem:dsna], to obtain a rate of convergence for identical variables. Note that Theorem [\ref=thm:mainstab] requires us to take β  ≤  2. Overall then we deduce that dα(Sn,Y) converges at rate O(n- t), where

if min (α,γ)  ≥  1, we take β = 2, and hence t  =  1 / α  -  1 / 2;

if min (α,γ)  <  1, we may take β  =   min [α   /   {1   -    min (α,γ)   +   ε},2] for any ε > 0, and then t  =   min (1 / α  -  1 / 2,1  -  ε,γ  /  α  -  ε).

Theorem [\ref=thm:mainr] implies that if dr(Sn,Zσ2) ever becomes finite, then it tends to zero, the counterpart of the following result.

Fix α∈(0,2), let [formula] be independent random variables (where [formula], if α  >  1), and let [formula]. Suppose there exists an α-stable random variable Y and [formula] having the same distribution as Y, and satisfying

[formula]

If α  ≠  1 then lim n  →    ∞dα(Sn,Y)  =  0, and if α  =  1 then there exists a sequence [formula] such that lim n  →    ∞dα(Sn - cn,Y)  =  0.

(Suggested by an anonymous referee). Fix ε  >  0. Suppose first that 1  ≤  α  <  2 and let [formula]. Note that di = 0 if α  >  1. Let b  >  0 and define

[formula]

Then by Equation ([\ref=eq:vb2]),

[formula]

The result follows on choosing b sufficiently large to control the second term, and then n sufficiently large to control the first.

For 0  <  α  <  1, take Ui as before, take [formula] and [formula]. Now using Equation ([\ref=eq:vb1]),

[formula]

so again since b is arbitrary, the result follows.

Note when [formula] are identically distributed, the Lindeberg condition ([\ref=eq:linde]) reduces to the requirement that dα(X1,Y)  <    ∞  .