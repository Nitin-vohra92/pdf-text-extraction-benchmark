-0.5in

Simulation-based Bayesian analysis for multiple changepoints

Abstract: This paper presents a Markov chain Monte Carlo method to generate approximate posterior samples in retrospective multiple changepoint problems where the number of changes is not known in advance. The method uses conjugate models whereby the marginal likelihood for the data between consecutive changepoints is tractable. Inclusion of hyperpriors gives a near automatic algorithm providing a robust alternative to popular filtering recursions approaches in cases which may be sensitive to prior information. Three real examples are used to demonstrate the proposed approach.

Keywords: Bayes factor; changepoint; marginal likelihood; model search.

Introduction

The range of applications of changepoint models is evident from the substantial volume of literature devoted to this problem in the econometrics, signal processing and bioinformatics literatures. A process generating data can often undergo changes over time such that one model will not be appropriate for all time periods. Here "time" refers to some natural sequential indexing of the data. Some examples are occurences of coal mining disasters during the [formula] and [formula] century [\cite=Raftery86], DNA or protein composition analysis over base number [\cite=Liu99] and winning streaks in sports [\cite=Yang04].

Markov chain Monte Carlo (MCMC) techniques can be used to estimate models with a fixed number of changepoints. When the number of changepoints is unknown, inference is more challenging.  estimates a collection of changepoint models and compares these using Bayes factors estimated from the MCMC output.  uses reversible jump MCMC (RJMCMC) to explore the number of changepoints in the coal mining disaster data. RJMCMC allows moves between models which satisfy detailed balance.

The use of alternatives to MCMC has grown in this area in recent years. uses filtering recursions to derive the posterior distribution of changepoints. This can be done for both a known and unknown number of changepoints. An advantage of this approach is that one can draw independent samples from the posterior. MCMC can only do this approximately at best. Extension to online analysis of changepoint models is also possible [\cite=Fearnhead07]. However methods based on filtering recursions rely on strong prior information in most cases. This paper aims to offer an efficient MCMC alternative which can overcome strong reliance on prior assumptions as encountered in recursive computing approaches. The class of models considered is similar to . For this reason it is possible that this could be used to give useful starting values for an analysis using filtering recursions.

Qualitatively, the work in this paper is similar in some aspects to work by  and  in terms of the class of models considered. The sampling aspect of the approach bears similarities to the samplers of  and . This paper extends these works to a broader range of data models and proposes a more efficient way of sampling changepoints. An aim is also to highlight possible shortcomings of alternatives to MCMC and how these could be overcome by using simulation approaches to inform choices for recursive computing approaches.

The remainder of the paper is organised as follows. In Section [\ref=sec_mod_and_meth] the type of changepoint model under consideration is presented. Section [\ref=sec:collapsing_cp_models] reviews the reversible jump approach to changepoint estimation and discusses how this can be simplified into a fixed dimensional sampling scheme. Section [\ref=sec:sampler] gives the moves to sample from the simpler fixed dimensional posterior. Prior specification is discussed in Section [\ref=sec:priors], and Section [\ref=sec:recursions] reviews the filtering recursion approach to generating samples of changepoints. Performance of the sampler is validated by analyzing the coal mining disasters data in Section [\ref=sec:coal_mining], while Sections [\ref=sec:tiger_woods] and [\ref=sec:well_log_data] compare qualitative aspects of the simulation based sampler approach and filtering recursions approach using two real data examples. A brief discussion concludes the article.

Changepoint models

Consider the data [formula] which is time ordered. Here yi is observed before yj if i < j. Time in this context can refer to any natural ordering of the data as it is observed. A changepoint occurs at time t if [formula] are generated differently to [formula]. Referring to ys:r(s < r) as a segment, this says that the segments y1:t and yt + 1:n are heterogeneous between but homogeneous within. Parametric changepoint models assign a different parameter for each segment to account for this heterogeneity.

This paper considers multiple changepoints which will be denoted [formula]. These split the data into k + 1 segments. The likelihood for segment j has parameter θj. Conditional on a segmentation, the data within each segment is assumed independent. It is also assumed that the regime parameters θj are independent. The likelihood of the segmentation [formula] is

[formula]

where for convenience τ0  =  0,τk + 1 = n. Instead of using τ, segmentations can be labelled with the binary latent vector [formula] with zt = 1 indicating a changepoint at time t and zn  =  0. Independent priors are assumed for each member of [formula] with hyperparameter γ and there is a prior for the changepoints with hyperparameter ξ, given by π(z|k,ξ). The posterior may be written

[formula]

where the dependence on the number of changepoints, k, is made explicit. A prior π(k) may be introduced so that the posterior of interest is the joint posterior of (k,z,θ),

[formula]

This is a hierarchical changepoint model similar to that used in .

Collapsing changepoint models

It is possible to construct a MCMC scheme to sample the posterior of ([\ref=eq:hierarchical_mod]) using RJMCMC [\cite=Green95]. The sampler will explore the product space support of this posterior:

[formula]

where Zk,Θk are respectively the sample spaces of z and θ conditional on k changepoints. A switch in the number of changepoints in the model can be made by a RJ move switching between support subspaces. For the purposes of illustration a straightforward move of this type is now discussed. When proposing a switch from k to k + 1 changepoints one possibility is to generate a random variable [formula] and form a bijection [formula] where d is the dimension of a single θj. This bijection gives the parameters for the proposed k + 1 changepoint model as a function of those for the k changepoint model; [formula]. The proposed switch in model is then accepted with probability min (1,R) where

[formula]

In the expression for R, P(  ·  ,  ·  ) denotes the proposal probability for transitions between different numbers of changepoints, and q(  ·  |θ) is the proposal density of u. The last term on the right is a Jacobian term for the bijection f. The reverse move in switching from k + 1 to k changepoints is accepted with probability min (1,R- 1). More elaborate moves between support subspaces are possible which propose changes to the model of more than one dimension or involve stochastic moves in both directions.

The key questions in a changepoint analysis are usually; how many changepoints are there and where are the changepoints? The segment parameters θ can be viewed as a nuisance parameter in this regard. Choosing conjugate priors for the θj allows these to be collapsed in the model

[formula]

where π(yτj - 1 + 1:τj|γ) is the marginal likelihood of the data segment yτj - 1 + 1:τj and is assumed to be available in closed form due to the conjugacy. The support of this posterior is

[formula]

and a switch from k to k + 1 changepoints does not require the design of a bijective function between support subspaces. The proposed switch in model is now accepted with Metropolis-Hastings probability min (1,A) where

[formula]

This idea of collapsing has been used previously in  and  for Gaussian data models.

It can be seen that the first term on the right hand side of the acceptance ratio ([\ref=acc_collapsed]) is the Bayes factor for a model with k + 1 changepoints at positions z' versus a model with k changepoints at positions z, assuming all models are equally likely, a priori. Noting this, it becomes apparent that sampling k and z is equivalent to a model search over large model space. If there can be at most k̄ changepoints, then the dimension of this space is [formula]. So searching for up to 5 changepoints in a dataset of length 200 corresponds to a dimension ~  2.5  ×  109. In the next section an MCMC scheme to search over these large model spaces, that is, sample from the posterior ([\ref=eq:collapsed_mod]), is proposed.

Sampling changepoints

The MCMC scheme to generate samples of changepoints from the posterior ([\ref=eq:collapsed_mod]) consists of three possible moves: add a changepoint; delete a changepoint; move a changepoint. Each sweep consists of the following;

Choose to add or delete a changepoint with probabilities ak and dk  =  1 - ak respectively. Clearly ak̄  =  d0  =  0.

Select a changepoint and propose to move it to a position in the range of its closest neighbouring changepoints.

Add or delete a changepoint

This move has been dicussed in Section [\ref=sec:collapsing_cp_models] but more details are given here. Suppose there is currently k changepoints at postions z. Let z correspond to changepoints at [formula]. Randomly select one of the n - k - 1 points where there could be a changepoint [formula] a t  <  n with zt  =  0. Say this is currently in segment j given by yτj - 1 + 1:τj. Relabel the proposed changepoints in z' as [formula] with τ'j  =  t. Cancellation of marginal likelihood terms then implies that

[formula]

so calculation of A in ([\ref=acc_collapsed]) only requires at most three marginal likelihood values. Conversely, for the delete move, one of the k + 1 changepoints in z' is chosen at random and the calculation of the acceptance probability involves

[formula]

Finally, the proposal one step transition probabilities for the number of changepoints will be P(k,k + 1)  =  ak / (n - k - 1) and P(k + 1,k)  =  dk + 1 / (k + 1), so that A ([\ref=acc_collapsed]) can be computed. The acceptance probability for the add move is then min (1,A) and the delete move is accepted with probability min (1,A- 1).

Move a changepoint

Gibbs update: Given the model assumption that the marginal likelihood for any segment is available in closed form, it is possible to update the position of any changepoint from its full conditional. Suppose τj is being updated. Then the conditional probability that τj = t, τj - 1  <  t  <  τj + 1 is proportional to

[formula]

where z'(t) corresponds to changepoints [formula]. The effort required for the Gibbs update is O(τj + 1  -  τj - 1) and so may be computationally expensive for large datasets with changepoints far apart, or datasets with many changepoints. In this situation a local random walk update may be preferred.

Local random walk update: t is drawn uniformly from the integers [formula] where l specifies the locality of the proposed move. The move is accepted with probability min (1,B) where

[formula]

In the event that τj - l  ≤  τj - 1 and t  <  τj, B must be multiplied by (τj  -  τj - 1 + l) / (t  -  τj - 1 + l). Similar modifications are needed if t > τj or τj + l  ≥  τj + 1.

Mixture of updates: A mixture of the two moves above should improve mixing and not be overly computationally expensive. For example, choose the Gibbs update with probability [formula] (k  ≥  1) and random walk with probability rk  =  1 - gk.

Prior specification

There are many possible choices for π(z|k,ξ).  considers a geometric distribution for the duration, d, of segments; d  ~  (p). The prior used by  has been adapted by  for the discrete time context discussed here. The k changepoint locations are distributed as the even numbered order statistics in a sample of size 2k + 1 from the integers [formula], drawn without replacement.

The geometric prior relies on specification of ξ  =  p. Ideally, one could simulate a segment specific pj in a similar vein to . However this leads to more difficult jump dynamics when adding or deleting a changepoint. The choice of p may impact the analysis. If too small, then it will assign very small probability to changepoints, meaning small changes cannot be detected with high power. If too large, then spurious changepoints are inferred. For these reasons, it desireable to introduce a hyperprior on p. For example, a (α1,α2) prior with 1 < α1  <  α2 (more weight less than 0.5), would be an ideal choice if there is enough prior information to choose α1,α2. Otherwise, a non-informative (1,1) prior would suffice.

Segment parameters share a common hyperparameter γ in Section [\ref=sec_mod_and_meth]. It is therefore possible to explore uncertainty in γ also by introducing a hyperprior π(γ).

Sampling p and γ can be easily incorporated into the MCMC scheme in Section [\ref=sec:collapsing_cp_models]. One sweep of the algorithm consists of:

Sample the changepoints.

Conditional on the changepoints sample p.

Conditional on the changepoints sample θ.

Conditional on θ sample γ and discard the θ values.

For the last step here, it will often be possible to sample γ using a Gibbs step. However, if this is not possible, a simple random walk Metropolis-Hastings could be used.

Analysis by filtering recursions

It is useful to give a brief recap of the filtering recursions analysis of  based on a point process prior for changepoint positions. ,  have also used these types of methods for the analysis of changepoint problems. Define

[formula]

It is possible to compute this quantity in a backward recursion. Defining Rγ(n)  =  π(yn|γ), for [formula]

[formula]

and

[formula]

where the dependence of Rγ(t) on the hyperparameter γ has been made explicit. Here g(  ·  ) gives the point process for the changepoint positions and G(  ·  ) the corresponding cumulative distribution function (the subscript 0 on g and G in Rγ(1) denotes the distribution of the first changepoint after 0).  takes this as geometric as do .  suggests a negative binomial family in general for this process.

After computing the recursions, a sample of size N of the changepoints can be efficiently simulated as follows:

Initialize all samples to have a changepoint at t = 0.

For [formula]

Get nt, the number of samples for which the last changepoint was at time t.

If nt > 0 compute the distribution of the next changepoint:

[formula]

Sample nt times from Pr {τ|y1:n,t} and update the nt samples that have the last changepoint at t.

There are two strengths of this approach. The first is that the samples of changepoints will be independent draws from the posterior distribution. The second is the fast sampling algorithm which avoids computing the distribution of the next changepoint for each possible time. The main weakness of this approach is that the generated samples are dependent on a fixed value of the hyperparameters γ. Updating γ using a hyperprior to correctly explore uncertainty in the value would involve recomputing the recursions Rγ(t) for each new value of γ, a computation which is quadratic in n. This would lead to an infeasible computational overhead for any reasonably large sample from the posterior.

Poisson data: coal mining disasters

The sampler of Section [\ref=sec:sampler] was applied to the coal-mining data of . This data records the dates of serious coal-mining disasters between 1851 and 1962. Disasters are assumed to arise from a Poisson process whose intensity is the height of a step function with an unknown number of steps. For comparison with , time is discretized in weeks and the intensities are taken to be (1,200 / 7), a priori. Details on the model marginal likelihood calculations are given in the Appendix. Conditional on k changepoints the prior on their positions was taken to be the same as the distribution of the even numbered order statistics of a sample of size 2k + 1 drawn without replacement from [formula] [\cite=Fearnhead06],

[formula]

where for convenience, τ0 = 0 and τk + 1 = n. The algorithm was run for 500,000 sweeps after 10,000 burn in. Every [formula] sample was taken to reduce dependency in the MCMC iterates. This took 10 seconds on a 2.5GHz processor. Figure [\ref=fig:Coal_mining_posterior] (a) shows that the posterior number of changepoints is almost identical to that obtained from long runs of a RJMCMC sampler and methods based on recursions (see , Figure 1[formula](a)).

Streakiness in sports

A sportsperson is considered "streaky" if instead of having a constant success rate over time, they have periods of high success rate. Such data will generally be a binary sequence with a "0" denoting a loss and a "1" denoting a win. The data concerning Tiger Woods' championship wins from September 1996- June 2001 was given and analyzed by , and are reanalyzed using the sampler of Section [\ref=sec:sampler]. The cumulative counts are shown in Figure [\ref=fig:tiger_woods] (a). Following  the data as is assumed to arise as a sequence of Bernoulli trials, with a possible changing probability of success. The data is ordered by subsequent tournament, and if a changepoint occurs, it is assumed to do so at some tournament. Let [formula], the number of sucesses in a segment. Then assuming a (α,β) prior for the probability of success in any segment,

[formula]

Details of this calculation are given in the Appendix. The parameters α and β were both set equal to 1. The distribution between changepoints was taken to be (p). The specification of p may have an effect on the outcome of the analysis. It is thus desirable to investigate uncertainty in its value. This is done in two ways. Firstly, a simulation study using the sampler of Section [\ref=sec:sampler] is carried out, where there is a hyperprior placed on p. Secondly, outputs of analyses using filtering recursions [\cite=Fearnhead06] for a range of values p are compared.

For the MCMC simulation study using the sampler proposed earlier, the hyperparameter given to p was uniform on

[formula]

was taken for the number of changepoints. This gives no discriminating prior weight on a particular number of changepoints. The sampler was run 100 times each for 100,000 burn in iterations and a subsequent 1,000,000 iterations. To reduce dependency in the sample, only every [formula] sample was stored. Each run took about 1.5 min on a 2.5GHz processor. Changepoints were updated using the mixture of moves discussed in Section [\ref=sec:move_changepoint]. Figure [\ref=fig:tiger_woods] (b) shows the output from one of these runs, with the posterior probability of a changepoint at any tournament indicated by the dashed line and a scaled counts curve overlain. Figure [\ref=fig:tiger_figure] (a) shows posterior probability of the number of changepoints over the 100 runs of the sampler. It can be seen that the sampler performs consistently, giving similar results over the 100 runs. Figure [\ref=fig:tiger_figure] (b) shows a histogram for the sampled values of p from the last run. Posterior support for p is highest over the range

[formula]

Gaussian changepoint models

Gaussian changepoint models are widely used and studied. Models can include those with changing mean and/or variance across segments. The model assumed for the purposes of the example here is piecewise constant, where data in any segment is Gaussian distributed. Segments share a common error variance. Data point yi in segment j is assumed to arise independently from a (μj,σ2) distribution. The segment means μj are assumed to arise from a Gaussian distribution with mean μ0 and variance ν2σ2, a priori. Denote γ = (σ2,μ0,ν2). Segment length is assumed to have a geometric distribution with parameter p. This gives the log posterior (up to a constant) as

[formula]

where [formula] and [formula]. Details of this calculation are given in the Appendix.

Application to Well-log data

The Well-log data () records measurements of nuclear-magnetic response of underground rocks obtained by lowering a probe into a bore-hole. The probe records the response at regular points in time. As well as  this data was also analyzed in . The data consists of 4050 measurements, some of which are outliers and were removed before analysis. The data are shown in Figure [\ref=fig:well_log_data].

The purpose of this example is to demonstrate how results from an analysis with filtering recursions may be sensitive to the choice of hyperparameters γ and how a short run of the sampler could possibly provide good starting values. It is possible to fit a more elaborate state space model to the Well-log data, however, this is not considered here.

chose the values p  =  0.013,σ  =  2,330,ν  =  4.3,μ0  =  115,000 when analyzing the Well-log data in the section on inclusion of hyperpriors. Two simple experiments were performed here to investigate sensitivity of the posterior distribution to prior specification. One of p (Experiment 1) or σ (Experiment 2) was varied over a grid on a small range keeping all other hyperparameter values fixed (details in Table [\ref=tab:sensitivity_exp]). The recursions of Section [\ref=sec:recursions] were computed for each value on the grid and a sample of size 100,000 was generated from the posterior of the changepoints. The empirical posterior distribution of the number of changepoints was computed for each of these samples and the modal number of changepoints recorded. The results are summarized in Figure [\ref=fig:sensitivity_p_and_sigma]. It can be seen that the modal value of the posterior number of changepoints is sensitive to the values of both p and σ. Thus choosing these values, a priori, places the posterior mass π(k,z|y,p,γ) in the area determined by p and σ and may not correctly represent the true posterior over all p,σ.

For the Well-log data it would seem most sensible to carry out an analysis with inclusion of hyperpriors on p,σ and μ0 using the scheme outlined in Section [\ref=sec:priors]. The hyperpriors used are [formula], [formula], [formula], [formula]. The bottom of Figure [\ref=fig:well_log_data] shows the posterior probability of a change output from an algorithm run for 10,000 burn-in and 100,000 subsquent iterations using a random walk update for changepoint positions. Ergodic mean estimators of the hyperparameters were σ̂  =  2360,p̂ = 0.014, = 3.99,0  =  113771.0. This took about 10 sec on a 2.5GHz processor with very diffuse starting values. This Gaussian model infers many changepoints as it picks up small changes in the mean and thus performs well for this data.

A long run of the sampler was implemented so as to obtain a near independent sample ([formula] iterations taking every [formula] sample; estimated integrated autocorrelation time of the number of changepoints ≈  1) of size 10,000 from the posterior distribution of changepoints and hyperparameters. This was compared with results from the independence proposal suggested by . In the independence proposal MCMC scheme suggested in , a sample of changepoints is generated using filtering recursions conditional on p  =  0.013,σ  =  2,330,μ0  =  115,000,ν  =  4.3. This sample is then used for an independence proposal and hyperparmeters are updated in the same way as done here. Figure [\ref=fig:comparison_independence_proposal] shows kernel density estimates constructed from samples of the hyperparameters for the sampler (dashed line) and independence proposal (solid line). It can be seen that there is a slight discrepancy in that the independence proposal leads to more peaked densities.

In our implementation an independence proposal based on a sample of size 10,000 was used. This updating scheme for hyperparameters and changepoints was then run for 50,000 iterations. Although the acceptance rate for moving between different changepoint configurations was high, the independence proposal distribution was highly degenerate. Only ten unique changepoint configurations were sampled in the 50,000 iterations of the MCMC scheme. For other datasets where less information is available to choose the hyperparameters to generate the independence proposal, it is possible that this could lead to highly biased sampling from the hyperpriors.

In the sense of hyperprior incorporation and full exploration of the posterior distribution the MCMC sampler proposed performs better than the independence proposal. However, generating independent samples may be more costly in large datasets with many changepoints. Nonetheless, it is clear that the inclusion of hyperpriors circumvents the sensitivity of posterior distribution of the changepoints to specification of the hyperparameters. This is a main advantage of the approach proposed here and makes the detection of changepoints more automatic.

Discussion

This paper has presented an MCMC method to perform retrospective inference for changepoint model which are collapsable. The multiple changepoint problem is rephrased as a stochastic model search over a large models space, with the Bayes factors for competing models appearing in the acceptance probabilities for the MCMC sampling scheme.

The performance of the sampler was verified for the benchmark coal mining disasters data. Application of the sampler to a streakiness dataset from sports revealed that posteriors for the number of changepoints can be diffuse. It was demonstrated that prior specification on the duration of segments plays a crucial role in the analysis of the models considered. Incorporation of hyperpriors to account for this revealed features of the posterior that would be missed by a popular filtering recursions analysis for changepoints. Application to the Well-log data further highlighted sensitivity of analysis by filtering recursions to prior specification. It was shown that output from a short run of our sampler can be used to give good values of the hyperparameters for this prior specification.

In conclusion, the sampling scheme presented is shown to work well and can provide further insight and account for prior uncertainty in some difficult situations. It can be used as a useful exploratory tool or for a full analysis. Computer code implementing the sampler written in C may be downloaded from www.ucd.ie/statdept/jwyse.

Appendix

Calculations for the coal-mining example

Given a segment ys:t, each [formula]. Here μ is the height of the step function that gives the intensity of the process between times s and t. Assume the prior for μ is (ρ,λ) where γ  =  (ρ,λ). The marginal likelihood for the segment is then

[formula]

where [formula] and [formula]. Completing the integral of the Gamma density gives

[formula]

Calculations for the streakiness example

Within a segment ys:t, [formula]. Taking a (α,β) prior on φ, the marginal likelihood is obtained from

[formula]

where γ = (α,β). This reduces to

[formula]

where [formula]. Completing the Beta integral gives

[formula]

Calculations for Gaussian changepoint model

The model for all the data may be written hierarchically as

[formula]

Completing the square on μj and then performing integration of μj over ( -   ∞  ,  ∞  ) gives the required posterior.

[formula]