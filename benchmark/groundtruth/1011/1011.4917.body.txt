Lemma Corollary Proposition Definition

Random multiplicative functions in short intervals

Introduction

Many of the functions of interest to number theorists are multiplicative. That is they satisfy f(mn) = f(m)f(n) for all coprime natural numbers m and n. Some examples are the Möbius function μ(n), the function nit for a real number t, and Dirichlet characters χ(n). Often one is interested in the behavior of partial sums [formula] of such multiplicative functions. For the proto-typical examples mentioned above it is a difficult problem to obtain a good understanding of such partial sums. A guiding principle that has emerged is that partial sums of specific multiplicative functions (e.g. characters or the Möbius function) behave like partial sums of random multiplicative functions. By random we mean that the values of the multiplicative function at primes are chosen randomly, and the values at all natural numbers are built out of the values at primes by the multiplicative property. For example this viewpoint is explored in the context of finding large character sums in [\cite=GS].

This raises the question of the distribution of partial sums of random multiplicative functions, and even this model problem appears difficult to resolve. The aim of this paper is to study the distribution of random multiplicative functions in short intervals

[formula]

|M(x)| ≤ c x exp d (log log x log log log x),

[formula]

. Let Z denote a Gaussian random variable with mean 0 and variance 1, and let φ denote a Lipschitz function satisfying |φ(α) - φ(β)|  ≤  |α  -  β| for all real numbers α and β. Then we have that

[formula]

is bounded by a constant times

[formula]

We recall that the Kantorovich-Wasserstein distance between two probability measures μ and ν on the real line, denoted W(μ,ν), is defined as the supremum of [formula] over all Lipschitz functions h satisfying |h(α) - h(β)|  ≤  |α  -  β| for all real numbers α and β. Thus our Theorem gives an estimate for the Kantorovich-Wasserstein distance between a normal distribution with mean zero and variance 1, and the distribution of sums of random multiplicative functions in short intervals. An intuitive way to assess the distance between two probability measures is the Kolmogorov statistic: [formula]. By a standard smoothing argument, we shall show how our estimate for the Kantorovich-Wasserstein distance can be used to bound the Kolmogorov statistic.

With notations as in Theorem [\ref=mainthm] we have that

[formula]

is bounded by a constant times

[formula]

In an interval

[formula]

are square-free. The theorem in Filaseta and Trifonov only asserts the existence of a square-free integer in such an interval, but their proof plainly gives the stronger result above. Therefore for all short intervals with [formula], our Theorem shows that the distribution of [formula] is approximately normal. Granville [\cite=G] has shown that the ABC-conjecture implies that the interval

[formula]

contains a positive proportion of square-free numbers, then Proposition 3.1 shows that the fourth moment is asymptotically the fourth moment of a normal distribution provided y  =  o(x /  log x). Further, when y is of size a constant times x /  log x, the argument there shows that the fourth moment does not match the fourth moment of a normal distribution. Thus it seems plausible that for x /  log x  ≪  y  ≤  x the distribution of [formula] is not normal, but we do not have a proof of this assertion. By modifying the conditioning argument in Harper [\cite=H] we can establish that if y is of a constant times x then the distribution of [formula] is not normal.

The method developed here could also be used to study the distribution of [formula] for other subsets S of square-free numbers in

[formula]

Beginning of the proof

Let x, y and δ be as in the statement of the Theorem, and let X denote a random multiplicative function as defined in the Introduction. We let z denote [formula]. We divide the primes below 2x into large (that is > z) and small (that is ≤  z) primes. We denote the set of large primes by L, and the set of small primes by S. Let F be the sigma-algebra generated by X(p) for all p∈S, and we denote the conditional expectation given F by [formula].

Let XL denote the vector (X(p))p∈L. Then, given F, we may think of [formula] as a function of XL, and we write this function as f(XL).

With the above notations we have

[formula]

and

[formula]

Write a square-free number n∈[x,x  +  y] as nSnL where nS is the product of the primes in S that divide n, and nL the product of the primes in L that divide n. From our choice of [formula] we note that [formula]. It follows that nL  =  n / nS  >  δx = y. From this we obtain that [formula]. Moreover, note that if n and [formula] are distinct square-free numbers in

[formula]

Let [formula] denote an independent copy of XL. For each subset A of L we write XAL to be the vector defined as XA(p)  =  X(p) for [formula], and [formula] for p∈A. For a proper subset A of L, and a prime [formula] we define

[formula]

and

[formula]

Finally define

[formula]

With these notations, and Lemma 2.1, Theorem 2.2 from [\cite=chatterjee08] enables us to get the following result.

Let Z denote a random variable with a Gaussian distribution with mean zero and variance 1. Let [formula], and let φ denote a Lipschitz function satisfying |φ(α) - φ(β)|  ≤  |α  -  β| for all real numbers α and β. We have

[formula]

Here conditioning on X means that we are conditioning on the whole vector (X(n))n  ≥  1. Actually, the bound given by Theorem 2.2 from the paper [\cite=chatterjee08] has [formula] in the first term instead of [formula]. However, the latter quantity is at least as large as the former because [formula] and conditioning reduces variance.

We shall use Proposition 2.2 to estimate [formula]. Note that this quantity is bounded by

[formula]

By the Cauchy-Schwarz inequality the first term above is

[formula]

We deduce that

[formula]

We will now focus on estimating the two terms in the RHS above. The second term will be estimated in the next section, and the first in Section 4. We now simplify the expression in the first term a little.

For each p∈L, let N(p) denote all square-free numbers in the interval

[formula]

that are not divisible p and by any prime q∈A. Write the quantity T in Proposition [\ref=normthm] as

[formula]

where

[formula]

Thus,

[formula]

where ωL(n) denotes the number of distinct prime factors of n that are in L and the equality above holds because

[formula]

The last step above involves a combinatorial identity and we leave the pleasure of proving it to the reader; a generalization of this identity appears as Problem B2 of the 1987 Putnam competition see [\cite=KPV]. Now we define

[formula]

Then we may conclude that

[formula]

The fourth moment and a parametrization of solutions

In this section we shall evaluate the fourth moment

[formula]

for a suitable range of the variables x and y. The techniques involved in this calculation will be used in the proof of our main Theorem. When we expand out the fourth moment, we find that we are counting solutions to the equation

[formula]

where n1, n2, n3, n4 are square-free integers with nj∈[x,x  +  y] and [formula] denotes a perfect square. Recall that y = xδ. We begin by parametrizing such solutions.

Write A = (n1,n2) and B = (n3,n4), and set n1 = An*1, n2 = An*2, n3 = Bn*3 and n4 = Bn*4. Then (n*1,n*2) = (n*3,n*4) = 1 and the equation [formula] is equivalent to n*1n*2 = n*3n*4. Now write r = (n*1,n*3) and s = (n*2,n*4). Then (r,s) = 1 and we see that n*1 = ru, n*3 = rv, n*2 = sv and n*4 = su where u and v are natural numbers with (u,v) = 1.

Summarizing the above paragraph, we see that the solutions to [formula] are parametrized by six variables A, B, r, s, u, v, with (r,s) = (u,v) = 1 and with

[formula]

There are additional coprimality conditions to ensure that these numbers are square-free. Since (1 + δ)- 2  ≤  n1n2 / (n3n4)  ≤  (1 + δ)2 we see that

[formula]

Similarly using n1n3 / (n2n4)  =  (r / s)2 we have

[formula]

and finally using n1n4 / (n2n3)  =  u2 / v2 we get that

[formula]

In what follows we shall make use of this parametrization and the above inequalities for the ratios A / B, r / s, u / v. One consequence of these inequalities is that if A  ≠  B then A and B are both ≥  1 / δ. Similarly if r  ≠  s then both r and s are ≥  1 / δ and if u  ≠  v then u and v are both ≥  1 / δ.

Call any solution to [formula] where the variables are equal in pairs a diagonal solution. The number of non-diagonal solutions to [formula] with nj∈[x,x(1  +  δ)] and nj square-free is at most

[formula]

Therefore, with S denoting the number of square-free integers in

[formula]

Suppose A, B, r, s, u, v parametrize a non-diagonal solution to [formula]. Then either one of u or v is not 1, or one of r or s is not 1; for if u = v = 1 and r = s = 1 then n1 = n2 and n3 = n4. Since these cases are symmetric we will only deal with the case when one of u or v is not 1, and the total number of solutions is at most twice the number of solutions in this case.

Suppose then that u or v is not 1, and since (u,v) = 1 this means that u  ≠  v and so both u and v are ≥  1 / δ. Therefore it follows that Ar  ≤  xδ(1 + δ). Further either A  ≠  B or r  ≠  s, and so either A or r must be ≥  1 / δ. Now suppose A and r are given with max (A,r)  ≥  1 / δ and Ar  ≤  xδ(1 + δ). Since (1 + δ)- 1  ≤  A / B  ≤  (1 + δ) it follows that there are at most (1 + 2Aδ) choices for B. Similarly since (1 + δ)- 1  ≤  r / s  ≤  1 + δ there are at most 1 + 2rδ choices for s. Finally since Aru∈[x,x(1  +  δ)] there are at most 1 + xδ / (Ar)  ≤  3xδ / (Ar) choices for u, and similarly there are at most 1 + xδ / (Bs)  ≤  3xδ / (Ar) choices for v. Thus the total number of such solutions is

[formula]

This may be bounded by

[formula]

proving our Proposition.

When S is of size xδ (which holds if [formula]), Proposition [\ref=fourth] shows that provided δ = o(1 /  log x), the fourth moment matches the fourth moment of a Gaussian.

We now use the ideas of this section to bound the term [formula], arising in Proposition 2.2. By the Cauchy-Schwarz inequality we have

[formula]

As before let N(p) denote the square-free integers in (x / p,(x + y) / p] which are not multiples of p. Then

[formula]

Further we have

[formula]

and arguing as in Proposition 3.1 we find that this is ≪  (1 + y / p)2 provided δ  ≤  1 /  log x, where ≪   means ≤   up to a constant multiple. Therefore we conclude that

[formula]

Using this estimate for primes z < p  ≤  y we find that

[formula]

If p > y then [formula] unless there happens to be a square-free multiple of p in

[formula]

Proof of the Theorem

We now estimate [formula] where we recall that Tp is defined in § 2. This quantity equals

[formula]

Above we allow for the possibility that p equals q. The expectation above is 1 exactly when [formula] is a square and zero otherwise. Thus writing n1 = kp, [formula], [formula], [formula] the quantity we seek is

[formula]

where nj∈[x,x(1  +  δ)], n1  ≠  n2, n3  ≠  n4, the nj are square-free with [formula], and (n1,n2) and (n3,n4) must contain at least one prime factor from L.

We use the parametrization developed in § 3 to estimate this. In the notation used there we find that our quantity above is

[formula]

The sum above is over all A, B, r, s, u, v as in our parametrization with the further restraints that Aru  ≠  Asv and Brv  ≠  Bsu, and that A and B must each contain at least one prime factor from L. Our goal is to show that the above quantity is bounded by

[formula]

We will obtain this by first fixing A and r and analyzing the restraints on the other variables.

Suppose first that A and r are chosen with Ar  >  δ(x + y). If u  ≠  v then both u and v must be ≥  1 / δ and then we would have Aru > x + y. Thus we must have u = v and since (u,v) = 1 we have u = v = 1. Now r  ≠  s (else n1 = A = n2) and so we have that both r and s are at least 1 / δ. Thus we have A  ≪  xδ and Ar∈[x,x  +  y]. Given r the condition (1 + δ)- 1  ≤  r / s  ≤  (1 + δ) shows that there are ≪  rδ choices for s. Similarly the inequality (1 + δ)- 1  ≤  A / B  ≤  (1 + δ) shows that given A there are ≪  1 + Aδ choices for B. Thus in this case our quantity is

[formula]

The final estimate follows because A must contain at least one prime factor from L, so that A  ≥  z and hence [formula].

Now suppose that Ar  <  δ(x + y). Recall that either r = s = 1 or that both r and s are at least 1 / δ. We consider these cases separately. In the former case, note that B has ≪  1 + Aδ choices, and u and v have at most xδ / A choices each. Thus this case contributes

[formula]

Now suppose that we have the second case when r  ≥  1 / δ. Here there are ≪  1 + Aδ choices for B, and given r there are ≪  rδ choices for s. Finally there are ≪  xδ / (Ar) choices for u and ≪  xδ / (Bs)  ≪  xδ / (Ar) choices for v. Thus the contribution here is,

[formula]

Putting all these estimates together gives our bound [\eqref=goal].

Using the bound [\eqref=goal], together with [\eqref=est1], [\eqref=est2] and [\eqref=est3] we conclude that [formula] is

[formula]

To deduce the Theorem we combine the above bound with the following simple estimate for [formula]. Since φ is Lipschitz we have |φ(t) - φ(0)|  ≤  |t|, and so

[formula]

Proof of the Corollary

Let ν denote a Gaussian distribution with mean 0 and variance 1, and let μ denote a probability measure. We claim that

[formula]

and Corollary 1.2 follows as a special case of this estimate.

For any real number t, and a parameter ε > 0 consider the function Φ+(ξ;t,ε) defined by

[formula]

Note that Φ+(ξ;t,ε) is Lipschitz, and moreover Φ+(ξ;t,ε)  ≥  εχ( -   ∞  ,t)(ξ). Therefore

[formula]

Choosing [formula] we obtain that

[formula]

An analogous argument, using a similar Lipschitz minorant of the characteristic function of ( -   ∞  ,t), gives that

[formula]

and so ([\ref=KKW]) follows.

in

Acknowledgements. We are happy to thank Persi Diaconis for facilitating this collaboration, and many valuable discussions. We are also grateful to Adam Harper, Zeev Rudnick and the referee for some helpful comments.