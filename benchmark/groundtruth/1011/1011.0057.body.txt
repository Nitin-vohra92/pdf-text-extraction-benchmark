>0

Discussion of "Riemann manifold Langevin and Hamiltonian Monte Carlo methods" by M. Girolami and B. Calderhead

Introduction

This technical report is the union of two contributions to the discussion of the Read Paper Riemann manifold Langevin and Hamiltonian Monte Carlo methods , presented in front of the Royal Statistical Society on October 13 2010 and to appear in the Journal of the Royal Statistical Society Series B.

The first comment establishes a parallel and possible interactions with Adaptive Monte Carlo methods. The second comment exposes a detailed study of Riemannian Manifold Hamiltonian Monte Carlo (RMHMC) for a weakly identifiable model presenting a strong ridge in its geometry.

On Adaptive Monte Carlo

The utility of RMMALA and RMHMC methodology is its ability to adapt Markov chain proposals to the current state. Many articles design Adaptive Monte Carlo (MC) algorithms to learn efficient MCMC proposals, such as the special case of controlled MCMC, [\citet=Haario2001adaptive] which utilizes a historically estimated global covariance for a Random Walk Metropolis Hastings (RWMH) algorithm. Similarly, [\citet=Atchade2006adaptive] devised global adaptation in MALA. Surveys are provided in [\cite=atchade2009adaptive], [\cite=andrieu2008tutorial] and [\cite=RobertsRosenthal2009examples].

When the proposal remains essentially unchanged regardless of the current Markov chain state, performance may be poor if the shape of the target distribution varies widely over the parameter space. A typical illustration involves the "banana-shaped" warped Gaussian (see comments by Cornebise and Bornn), ironically originally utilized to illustrate the strength of early Adaptive MC [\citep=Haario1999]. This algorithm learned local geometry by estimating the covariance matrix based on a sliding window of past states. However, [\citet=Haario2001adaptive] showed it could exhibit strong bias, perhaps connected to requirements of "diminishing adaptation" as studied in [\citet=AndrieuMoulines2006ergodicity]. Recent locally adaptive algorithms satisfy this condition, e.g. State-dependent proposal scalings [\citep=Rosenthal2010optimal] fits a parametric family to the covariance as a function of the state, or the parameterized parameter space approach of Regional Adaptive Metropolis Algorithms [\citep=RobertsRosenthal2009examples].

Riemannian approaches provide strong rationale for parameterizing the proposal covariance as a function of the state - without learning, when the FIM (or observed FIM) can be computed or estimated, (see comment by Doucet and Jacob). With unknown FIM, or to learn the optimal step size, it would be interesting to combine Riemannian Monte Carlo with adaption. A first step could involve a simplistic Riemann-inspired algorithm such as a centered RWMH via the (observed) FIM as the proposal covariance [\citep=MarinRobert2007bayesiancore] - equivalent to one step of RMMALA without drift.

An additional use of Riemannian MC could be within the MCMC step of Particle MCMC [\citep=AndrieuDoucetHolenstein2010], where adaption was highly advantageous in the AdPMCMC algorithms of [\cite=peters2010ecological].

Another interesting extension involves considering the stochastic approximation alternative approach, based on a curvature updating criterion of [\cite=Okabayashi2010], for an adaptive line search. This was proposed as an alternative to MCMC-MLE of [\cite=geyer1992markov] for complex dependence structures in exponential family models. In particular comparing properties of this curvature based condition based on local gradient information with adaptive RMMALA and RMHMC versions of the MCMC-MLE algorithms would be instructive.

Additionally, one may consider how to extend Riemannian MC to trans-dimensional MC such as reversible jump [\citep=RichardsonGreen1997], for which adaptive extensions are rare [\citep=GreenHastie2009reversible]. We wonder how a geometric approach may be extended to efficiently explore disjoint unions of model subspaces as in [\citet=NevatPetersYuan2009contour].

Finally, an open question to the community: could such geometric tools be utilized in Approximate Bayesian Computation [\citep=BeaumontCornuetMarinRobert2009], e.g. to design the distance metric between summary statistics?

RMHMC for unidentifiable models

In this comment we show how the proposed RMHMC method can be particularly useful in the case of strong geometric features such as ridges commonly occurring in nonidentifiable models. While it has been suggested to use tempering or adaptive methods to handle these ridges [\citep=Neal2001] [\citep=Haario2001adaptive], they remain a celebrated challenge for new Monte Carlo methods [\citep=Cornuet2009AMIS]. We suspect that RMHMC, by exploiting the geometry of the surface to help make intelligent moves along the ridge, is a brilliant advance for nonidentifiability sampling issues.

Consider observations [formula]. The parameters θ1 and θ2 are non-identifiable without any additional information beyond the observations: any values such that θ1  +  θ22  =  c for some constant c explain the data equally well. By imposing a prior distribution, θ1,θ2  ~  N(0,σ2θ), we create weak identifiability, namely decreased posterior probability for c far from zero. Figure [\ref=densities] shows the prior, likelihood, and ridge-like posterior for the model. For this problem, we have

[formula]

Figure [\ref=3Traj] compares typical trajectories of both HMC and RMHMC, demonstrating the ability of RMHMC to follow the full length of the ridge.

HMC and RMHMC also differ in sensitivity to the step size. As described by [\citet=Neal2010], HMC suffers from the presence of a critical step size above which the error explodes, accumulating at each leapfrog step. In contrast, RMHMC occasionally exhibits a sudden jump in the Hamiltonian at one specific leapfrog step, followed by well-behaving steps (as seen in Figure 2.(a)). This is due to the possible divergence of the fixed point iterations (FPI) in the generalized leapfrog equations

[formula]

for given momentum [formula], parameter [formula] and step size ε. Figure [\ref=validprob] shows the probability of (16) having a solution [formula] as a function of [formula], and of the derivative at the fixed point being "small enough" for the FPI to converge; the well-known sufficient theoretical threshold on the derivative [\citep=Fletcher1987] is 1, but we conservatively chose 1.2 based on typical successful runs. When the finite number of FPI diverges, the Hamiltonian explodes; however, subsequent steps may still admit a fixed point, and hence behave normally. Unsurprisingly, this behavior is much more likely to occur for larger step sizes.

While the regions of low probability can strongly decrease the mixing of the algorithm, they do not affect the theoretical convergence ensured by the rejection step. Far from being a downside, understanding this behavior can bring much practical insight when choosing the step size - possibly adapting it on-the-fly, when RMHMC already provides a clever way to adaptively devise the direction of the moves.