Illustration of iterative linear solver behavior on simple 1D and 2D problems

Introduction

When facing an optimization problem, the simplest approach is often to iterate, locally improving current solution. For example, heat diffusion can be done by iteratively replacing each value by the average value of its neighbors. With more background on numerical optimization, one would like to formulate this optimization as a linear problem. With such abstraction, it is possible to use solvers as black boxes: we do not have an interpretation of the evolution of the solution during the iterations.

We are interested in observing the behavior of basic linear solvers for very simple cases. It does not necessarily impact the way to use them, but provides some intuition about the magic of linear solvers in the context of minimizing energies on a mesh.

Our meshes are regular 1D and 2D grids of size n, and variables are attached to vertices. We address the coordinates of the unknown vector [formula] as follows: in 1D [formula] is the ith element of [formula] and in 2D, the value [formula] of the vertex located at the ith line and kth column is the (n * i + j)th element of [formula].

Our tests are performed on the minimization of two energies:

Gradient energy: The sum of squared difference between adjacent samples: [formula] in 1D and [formula] in 2D.

Laplacian energy: The sum of squared difference between a sample and the average of its neighbors: [formula] in 1D. In 2D, it is the same on energy for boundary vertices, and [formula] over other vertices.

Note that for the Laplacian energy with the right number of constraints (2 in 1D, and 3 in 2D), the minimization of the first energy is equivalent to the minimization of the second energy.

1D problems

Observing 1D problems allows to visualize the convergence by a 3D surface. In all figures in this section, the first axis (horizontal) represents the domain -- a 1D grid, the second axis (vertical) represents the value associated to each vertex, and the third axis (depth) is the time.

Gradient energy

The 1D grid has n vertices, here we develop the matrices for n = 5. The problem is formalized by this set of equations Ax = b :

[formula]

The 4 first lines represent the objective. To enforce boundary constraints [formula] and [formula], we replaced coefficients applied to [formula] and [formula] by a contribution to the right hand side. We also add the last two lines to explicitly set [formula] and [formula]. In the least squares sense, it produces the new system to solve [formula] i.e.

[formula]

The condition number of [formula] is 5.9 for n = 5, and 1000 for n = 50. These numbers are not high enough to produce numerical instabilities.

This first experience (Fig. [\ref=fig:1Dconvergence]) shows the behavior of each solver with the same matrix. The boundary conditions (locked coordinates of [formula]) are set in the way that the solution is the straight line defined by [formula]. We observe the expected relative speed of convergence i.e. Jacobi <   Gauss-Seidel <   SSOR <   Conjugate Gradient. We also visualize the impact of the order of the coordinates in Gauss-Seidel and SSOR: when only the left side is constrained (locked [formula]), the first iteration propagates the constraints to all coordinates whereas constraining only [formula] take n iteration to affect all the variables.

Laplacian energy

As for the gradient energy case, the 1D grid has n vertices, and we develop the matrices for n = 5. The problem is formalized by this set of equations [formula] :

[formula]

The 3 first lines represent the objective. As for the "gradient energy", we replaced coefficients applied to [formula] and [formula] by a contribution to the right hand side to enforce [formula] and [formula]. We also add the last two lines to explicitly set [formula] and [formula]. In least squares sense, it produces the new system to solve [formula] i.e.

[formula]

The condition number of [formula] is 34.4 for n = 5, and 920000 for n = 50 coordinates. Such numbers produce numerical instabilities slowing down the solvers.

In Fig. [\ref=fig:laplaceconvergence], we observe the same expected relative speed of convergence i.e. Jacobi <   Gauss-Seidel <   SSOR <   Conjugate Gradient. Oscillations of the solution with Gauss-Seidel and SSOR are interesting to observe: it converges faster with higher local oscillations (in the solution space, not in the time dimension).

Another interesting observation is that CG required more than 20 iterations to converge. This was unexpected because, while CG is mostly used as an iterative solver, it is also a direct solver that is supposed to converge in a worse n iterations. Our interpretation is that the poor conditionning of the matrix generates numerical instabilities that slow down the solver. This phenomena is even worst with higher dimension problem (n = 100) as illustrated in Fig. [\ref=fig:CG1D]-right.

2D problems

The 2D problems are very similar to the 1D problems: we set the linear system [formula] with the gradient (resp. Laplacian) equations and add 3 constraints (2 at corners of the grid and in the center). To solve it in the least squares senses, we have to solve the linear system [formula]. We obtained results presented in Fig. [\ref=fig:2Dgradient] and [\ref=fig:2DLaplacien].

The observations are also very similar to 1D: the speed of convergence is as usual i.e. Jacobi <   Gauss-Seidel <   SSOR <   CG as expected, and the non zero coordinates of [formula] are "discovered" at each iteration in a front propagation fashion. Each constraint has an impact on the value of all vertices after 100 iterations (L1 distance of the diagonal). For the gradient energy, the CG method already have a fair solution and could be stopped at step 100. For the Laplacian energy, that has a bad conditioning, the result at step 100 is still far from the solution. Even at step 2400, it is not converged... but at step 2500, it's done ! (as expected).

We can also observe that the conditioning affects all methods (not only CG).

Conclusion: what did I learn?

We observed two non obvious behaviors of the considered iterative linear solver:

When starting with a null solution vector [formula], all tested iterative solvers (including CG) remove the zero coefficient by front propagation from the constraints. It can be explained by the position of non zero coefficients in the matrix A that is similar to the one of the vertices adjacency matrix.

Ill-conditioned systems may take more than n iterations to converge with the conjugate gradient algorithm. Stopping them earlier does not always give a good approximation of the solution e.g. 1D Laplacian.