Attention with Intention for a Neural Network Conversation Model

Microsoft Research kaisheny@microsoft.com Geoffrey Zweig Microsoft Research gzweig@microsoft.com Baolin Peng Chinese University of Hong Kong blpeng@se.cuhk.edu.hk

Introduction

A conversation process is a process of communication of thoughts through words. It may be considered as a structural process that stresses the role of purpose and processing in discourse [\cite=Grosz1986Discourse]. Essentially, the discourse structure is intimately connected with two nonlinguistic notions: intention and attention. In processing an utterance, attention explicates the processing of utterances, for example, paying attention to particular words in a sentence. On the other hand, intention is higher level than attention and has its primary role of explaining discourse structure and coherence. Clearly, a conversation process is inherently complicated because of the two levels of structures.

A conversation process may be cast as a sequence-to-sequence mapping task. In this task, the source side of the conversation is from one person and the target side of the conversation is from another person. The sequence-to-sequence mapping task includes machine translation, grapheme-to-phoneme conversion, named entity tagging, etc. However, an apparent difference of a dialogue process from these tasks is that a dialogue process involves multiple turns, whereas usually the above tasks involve only one turn of mapping a source sequence to its target sequence.

Neural network based approaches have been successfully applied in sequence-to-sequence mapping tasks. They have made significant progresses in machine translation [\cite=sutskever2014sequence] [\cite=bahdanau2015neural] [\cite=DevlinNNJSMT], language understanding [\cite=Mesnil2015RNNLU], and speech recognition [\cite=Chan2015LAS]. Among those neural network-based approaches, one particular approach, which is called encoder-decoder framework [\cite=sutskever2014sequence] [\cite=bahdanau2015neural], aims at relaxing much requirement on human labeling.

Conversation models have been typically designed to be domain specific with much knowledge such as rules [\cite=Bohus2009Dialogue] [\cite=Young2013POMDP]. Recent methods [\cite=Wen2015SLG] relax such requirement to some extent but their whole systems are still trained with manual labels because of their sub-components that require so. Manual labels are error prone and expensive. Therefore, it is appealing to train a system end-to-end without manual labels. Recent works in [\cite=Vinyals2015NeuralConversationModel] [\cite=Sordoni2015Conversation] [\cite=Shang2015NRM] are in this approach.

In general, however, using knowledge is helpful. For example, the alignment information between the source and target side is critical in grapheme-to-phoneme conversion [\cite=Yao2015S2SG2P] to outperform a strong baseline using n-gram models [\cite=Bisani2008G2P]. In a neural network based machine translation system [\cite=DevlinNNJSMT], the alignment information is used to outperform a strong phrase-based baseline [\cite=Chiang2007Hiero].

In the context of modeling conversation process, a neural network model may be built with the knowledge of the structural information of conversation processes. In particular, the network may incorporate the notion of intention and attention. To test this, we developed a model that consists of three recurrent neural networks (RNNs). The source side RNN, or encoder network, encodes the source side inputs. The target side RNN, or decoder network, uses an attention mechanism to attend to particular words in the source side, when predicting a symbol in its response to the source side. Importantly, this attention in the target side is conditioned on the output from an intention RNN. This model, which has the structural knowledge of the conversation process, is trained end-to-end without labels. We experimented with this model and observed that it generates natural responses to user inputs.

Background

In the theory of discourse in [\cite=Grosz1986Discourse], discourse structure is composed of three separate but related components. The first is the linguistic structure, which is the structure of the sequence of utterance. The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate. The second structure is the intentional structure, which captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The third is the attentional state that is dynamic, and records the objects, properties, and relations that are salient at each point of the discourse.

In many examples we observe, there are usually just one linguistic segment that consists of all the utterances. Therefore, in the following, we consider a discourse with two structures: intention and attention.

In the example in Table [\ref=tab:exp], there is a clear flow of intentions. The user states the problem, with the user's intention of conveying the problem to the agent. The agent receives the words, processes them, and communicates back to the user. The user responds to the agent afterwards. Therefore, the whole conversation process consists of three intentions processed sequentially. The first is the intention of communication of the problem. The second intention is the process of resolving the issue. The third is the intention of acknowledgment. In processing each of the intentions, the user and the agent pay attention to particular words. For example, when resolving the issue, the agent pays attention to words such as "virus".

The model

The attention with intention (AWI) model

We propose a model that attempts to represent the structural process of intentions and the associated attentions. Figure [\ref=fig:awi] illustrates the model. It shows three layers of processing: encoder network, intention network, and decoder network.

The encoder network has inputs from the current source side input. Because the source side in the current turn is also dependent on the previous turn, the source side encoder network is linked with the output from the previous target side. The encoder network creates a representation of the source side in the current turn.

The intention network is dependent on its past state, so that it memories the history of intentions. It therefore is a recurrent network, taking a representation of the source side in the current turn and updating its hidden state.

The decoder is a recurrent network for language modeling that outputs symbol at each time. This output is dependent on the current intention from the intention network. It also pays attention to particular words in the source side.

In more details, a conversation has in totoal U turns. At turn u, a user in the source side, denoted in superscript (s), has an input sequence of [formula] with length T. An agent in the target side, denoted in superscript (t), responds to the user with [formula] with length J. The proposed model is a conditional model of the target given the source, [formula]. If there is no confusion, we may omit the session index u in the following.

Encoder network

The encoder network reads the input sentence [formula], and converts them into a fixed-length or a variable length representation of the source side sequence. There are many choices to encode the source side. The approach we use is an RNN such that

[formula]

where f(  ·  ) is an RNN. [formula] is the hidden state at time t in the source side. The initial state [formula] with t = 0 is the last hidden state activity, [formula], of the decoder network in the previous turn k - 1.

One form of the output from this encoder is the last hidden state activity [formula]. This is used as a representation of the source side in the current turn to the intention network. The other form is a variable-length representation, to be used in the attention model described in Sec. [\ref=sec:decoder]. A general description of the variable length representation is as follows

[formula]

where q(  ·  ) might be a linear network or a nonlinear network.

Intention network

The signal from the encoder network is fed into an intention network to model the intention process. Following [\cite=Grosz1986Discourse], the intention process is a dynamic process to model the intrinsic dynamics of conversation, in which an intention in one turn is dependent on the intention in the previous turn. This property might be modeled using a Markov model, but we choose an RNN.

Interestingly, the hidden state of an RNN in a certain turn may be considered as a distributed representation of the intention. Different from the usual process of training distributed representation of words [\cite=Mikolov2013Word2Vec], the distribution representation of intentions are trained with previous turns as their context. We use a first order RNN model, in which a hidden state is dependent explicitly on its previous state.

The intention model in AWI is therefore an RNN as follows

[formula]

where [formula] is the fixed dimension representation of the source side described in Sec. [\ref=sec:encoder]. k is the index of the current turn. [formula] is the last hidden layer activity of the decoder network in the previous turn k - 1.

Decoder network

The last step is to decode the sequence in the target side, which is framed as a language model over each symbol, generated left to right. In this framework, the decoder computes conditional probability as

[formula]

where the hidden state in the decoder is computed using an RNN

[formula]

The initial state [formula] at t = 0 is the last hidden state activity from the intention network.

The [formula] is a vector to represent the context to generate [formula]. It is dependent on the source side as

[formula]

where z(  ·  ) summerizes the variable-length source side representations [formula] using weighted average. The weight is computed using a content-based alignment model [\cite=bahdanau2015neural] that produces high scores if the target side hidden state in previous time [formula] and [formula] are similar. More formally, the weight αjt for the context [formula] is computed using

[formula]

where

[formula]

The alignment model enables an attention to particular words, represented as a vector [formula] in the source side. Since the decoder network generates responses on condition of the attention and also the intention, our model is called attention with intention (AWI) model.

Implementation details

All of the recurrent networks are implemented using a recently proposed depth-gated long-short-term memory (LSTM) network [\cite=Yao2015DGLSTM]. The context vector [formula] is an embedding vector of the source side word at time t.

The alignment model in Eq. ([\ref=eqn:alignment]) follows the attention model in [\cite=bahdanau2015neural], in which ejt is calculated as

[formula]

which is a neural network with one hidden layer of size A and a single output, parameterised by [formula], [formula] and [formula]. H and A are the hidden layer dimension and alignment dimension.

Evaluation

We used an in-house dialogue dataset. The dataset consists of dialogues from a helpdesk chat service. In this service, costumers seeks helps on computer related issues from human agents. Training consists of 10000 dialogues with 96913 turns or conversations. Number of tokens is 2215047 in the source side and 2378950 in the target side. The vocabulary size is 9085 including words from both side. Development set data has 1000 dialogues with 9971 turns. Test set data has 500 dialogues with 5232 turns.

We use sentence-level SGD without momentum. Learning rate is initialized to 0.1. Development set is used to control the learning rate. The learning rate is halved when perplexity on the development is increased. One epoch of training has one pass of the training data. The order of training dialogues is randomly shuffled in the beginning of each epoch. The order of turns in the dialogue is however kept.

Performances measure in perplexity

An objective comparison of different models for conversation is still an open question. We report perplexity (PPL), though it may have drawbacks, to compare different models. Table [\ref=tab:awiresults] presents results in perplexity with two models with different hiden layer sizes. Results show that a larger model with 200 hidden layer dimension has lower PPL than the model with 50 dimension.

Examples of outputs from the trained model

Table [\ref=tab:exptrainedmodel] lists an example of the conversation process between a human and the trained model. The model has two layers of LSTMs and other setups are the same as used in Sec [\ref=exp:ppl]. Similarly as observed in [\cite=Sordoni2015HRED], the model produces natural responses to user inputs. The flow of intentions is clearly seen in this example.

Related work

Our work is related to the recent works in [\cite=Shang2015NRM] [\cite=Vinyals2015NeuralConversationModel] [\cite=Sordoni2015Conversation], which use an encoder-decoder framework to model conversation. The work in [\cite=Shang2015NRM] is a model for single turn conversation. The work in [\cite=Vinyals2015NeuralConversationModel] is a simple encoder-decoder method using a fixed-dimension representation of the source side. The work in [\cite=Sordoni2015Conversation] also uses a fixed-dimension representaiton of the source side but has an additional RNN to model dialogue context. This additional RNN is similar to the intention RNN in AWI model.

However, AWI model differs from [\cite=Sordoni2015Conversation] in that it incorprates the concept of attention and intention based on the theory in [\cite=Grosz1986Discourse]. Therefore, attention mechanism is essential to AWI. The model in [\cite=Sordoni2015Conversation] doesn't have an attention model.

Because it is not yet clear what objective measure to use to compare different models, it is hard to make claims of superiority of these models. We believe AWI model is an alternative to the models in [\cite=Vinyals2015NeuralConversationModel] [\cite=Sordoni2015Conversation].

Conclusions and discussions

We have presented a model that incorporates attention and intention processes in a neural network model. Preliminary experiments show that this model generates natural responses to user inputs. Future works include experiments on common dataset to compare different models and incorporating objective functions such as goals.