Deep Cascaded Regression for Face Alignment

Introduction

Face alignment is to automatically localize the facial key points, such as eyes, mouth, nose and chin. Due to its relevance to many facial analysis tasks like facial recognition [\cite=chen2013automatic], expression recognition [\cite=ashraf2009painful] and facial pose estimation [\cite=zhu2012face], face alignment has attracted increasing interest.

Many methods have been proposed for face alignment [\cite=zhu2012face] [\cite=martinez2013local] [\cite=sdm] [\cite=sun2013deep]. Among them, the cascaded regression method [\cite=sdm] has emerged as one of the leading approaches for accurate face alignment. Given an initial shape S0, the algorithm refines the shape by estimating a shape increment ΔS. In particular, a shape increment at stage k is calculated as:

[formula]

where φk(Sk) is the shape-indexed features [\cite=dollar2010cascaded] which depends on the current estimated shape Sk. Rk is the linear regression matrix. The shape is refined as Sk + 1  =  Sk  +  ΔSk.

Despite the success, there are still some limitations in the existing methods for cascaded regression. The main limitation is that the shape-indexed features used by most of the cascaded regression methods are fixed hand-crafted features, e.g., SIFT [\cite=sift], or features learned from the shallow models. These fixed hand-crafted features may not be optimal for the specific face alignment task since they are extracted in the unsupervised fashion and are not learned during the process of face alignment. To learn the features, Cao et al. [\cite=cao2014face] proposed to jointly learn the feature mapping and the linear regression matrix by a tree-based regression. Ren et al. [\cite=lbf] presented a learning-based local binary feature via random forest.

Different from the above methods, in this paper, we propose an end-to-end deep convolutional neural network (CNN) for face alignment. The proposed architecture is shown in Fig. [\ref=framework]. We propose to use a deep convolutional network to simultaneously learn the shape-indexed features and the regressors. First, we make some structural modifications for the existing CNN architectures, including firstly reducing the size of activations through the pooling layers and then enlarging the activations through the deconvolution layers [\cite=deconvlutionnet]. For an H  ×  W image, the output feature maps of our network are also with the size of H  ×  W. With this, the mapping between image coordinates and CNN output coordinates is straightforward. Then, we propose a "Shape-Indexed Pooling" layer to extract the shape-indexed features from the last deconvolution layer of our network, which is similar to the "Spatial Pyramid Pooling" [\cite=spp]. Due to the proposed deep shape-indexed features, our method shows significant performance gains over the methods based on fixed hand-crafted features. Moreover, we propose to learn p feature maps, each of which corresponds to one landmark. Each of the feature maps is a probability matrix in the same size of the input image, indicating the probability of each pixel belonging to that landmark. With these probability maps, we can find the initialization from the shape space that contains diverse shapes. With the found initialization, we refine a shape via sequential regressions, in which the fully connected layers are used to capture the linear mapping functions.

Our contributions in this paper can be summarized as follows. First, we propose a new CNN architecture for face alignment, in which down-sampling is carried out via two pooling layers and then up-sampling is performed via two deconvolution layers. Second, we design a new pooling way, referred to as "Shape-Indexed Pooling", to extract the deep features based on the current estimated shape. Finally, we show that the proposed network significantly outperforms the sate-of-the-art methods on three databases. Note that we do not use any external data sources and report the results of 68 facial landmarks.

Related Work

Many methods have been proposed for face alignment, including CLM-based methods [\cite=martinez2013local], AAAM-based methods [\cite=gao2010review], Regression-based methods [\cite=sdm] and Deep-learning based methods [\cite=sun2013deep].

Cascaded regression methods. The cascaded regression approaches predict the facial shape in a cascaded manner, which start from an initial shape, and refine the shape stage by stage. A representative method is the Supervised Descent Method (SDM) [\cite=sdm]. SDM uses SIFT features extracted around the current shape and solves a series of linear least square problems. Ren et al. [\cite=lbf] proposed to learn the local binary features with random forest, which achieves state-of-the-art results and much faster prediction speed. Global SDM (GSDM) [\cite=globalsdm] is an extension of SDM which divides the search space into regions of similar gradient directions.

In the above methods, the initial shape is typically provided by the mean shape. If the mean shape is the frontal face, the results may be poor for some faces with large pose variations. Cao et al. [\cite=cao2014face] proposed to use different initializations to run the algorithm and take the median of all predictions as the output. Zhu et al. [\cite=cfss] proposed a coarse-to-fine method which can find the initialization from the whole shape space.

Deep learning based methods. Deep learning has shown excellent performance in various visual applications due to its strong representation power. Luo et al. [\cite=luo2012hierarchical] proposed a hierarchical face parsing method based on the deep belief network (DBN). Sun et al. [\cite=sun2013deep] proposed three-level cascaded deep convolutional networks. Each level contains several convolutional networks. The first level gives the initial estimate and the following levels refine the initial estimate and give more accurate results. Wu and Ji [\cite=wu2013facial] explored deep belief networks and the 3-way restricted Boltzmann machine for the facial feature tracking. Zhang et al. [\cite=zhang2014facial] showed that learning the face alignment together with some correlated tasks, e.g., head pose estimation, can improve the detection robustness.

Our Method

Architecture

Fig. [\ref=framework] shows the detailed configuration of the proposed deep network. Our network is composed of three modules. The first module is deep convolutional sub-network. The deep network corresponds to the feature extractor which transforms the input image to the feature maps in the same size of the input image. The second module is to find the initial shape from the shape space. In this module, we design a new loss function with label distribution to learn local mapping functions. The initialization is selected using these local masks and the shape space. The third module is cascaded regression. It uses the shape provided by the second module as the initial shape. After that, we use "shape-indexed pooling" to extract the CNN features and refine the shape via sequential regressions.

Deep Convolution Sub-network

Convolutional neural networks have shown excellent performance in various visual problems due to their representation power. In this module, we propose a new architecture that transforms the input image to the feature maps in the same size of the input image.

Our network is based on the variant of the VGG-19 layer net [\cite=vgg] as shown in Fig. [\ref=subnetwork]. We make the following structural modifications. The first modification is to remove the last three max pooling layers (pool3, pool4, pool5) and all the fully-connected layers (fc6, fc7, fc8). The second is to add two deconvolution layers [\cite=long2014fully] [\cite=noh2015learning] (deconv6, deconv7). Deconvolution, which is also called backwards convolution, reverses the forward and backward passes of the convolution. It is used for up-sampling.

Our sub-network has 16 convolution layers, 2 max pooling layers and 2 deconvolution layers. In the convolution layers, we zero-pad the layer's input with ⌊k / 2⌋ zeros on all sides, where k is the kernel size. By this padding, the input and the output can have the same size. The pooling layer filters the input with kernel size of 2  ×  2 and stride of 2 pixels, which makes the size of the output be half of the input. In the deconvolution layer, the input is filtered with 96 kernels of size 4  ×  4 and a stride size of 2 pixels, which makes the size of the output be 2 times of the input. Suppose that the input image size is H  ×  W. After passing through two pooling layers, it becomes H / 4  ×  W / 4, and then is upsampled to H  ×  W via two deconvolution layers.

Discussions. Traditional networks, such as AlexNet [\cite=alexnet], reduce the size of activations by repeated operations of pooling and convolution. The downsampling may cause the loss of spatial accuracy, especially for the face alignment problem. For instance, VGG19 takes a 224  ×  224 image as the input and the output size of the Conv5 feature maps is 14  ×  14. That is, a pixel index (x,y) in the Conv5 feature maps has a receptive field centered on the pixel (16x,16y) in the input image. If two facial key points are very close, e.g., two points in the left eye, it is hard to discriminate them because these two points are mapped to the same pixel in the CNN feature maps.

Another CNN architecture is to remove the pooling layers and keep the size of all layers the same (e.g. H  ×  W) by repeated operations of convolution. Although it can solve the loss of the spatial accuracy problem, it has a very high time complexity due to the large image size.

On the contrary to the above two architectures, our network enlarges the activations though the deconvolution operations. The size of the image is kept in the last feature map. Hence, our network can solve the loss of the spatial accuracy problem. Also our network can reduce the size of the layers, which can greatly reduce the time complexity.

After this module, the network is divided into two branches: one is finding the initialization, and the other is the cascaded regression.

Initialization Searching

Many algorithms, e.g. [\cite=sdm], use the mean shape as the initialization. In this paper, we propose a simple method to find the initialization from the shape space instead of using a specified initialization.

We firstly construct N candidate shapes [formula], which cover a wide range of the shapes including different poses, expressions, etc. To obtain these candidate shapes, we simply run k-means on the training set to find N representative shapes. Then we find the initialization from these candidate shapes.

Second, we show how to find the initialization. A good initialization should be close to the ground truth shape, whereas in the testing image the latter is unknown. In this paper, we propose a simple method to give the predicted shape. Suppose the predicted shape is S. Then we can find the initialization as [formula].

Learning local mapping functions. Now the problem is how to find the predicted shape. After the sub-network, we add a new conv layer (conv8) which has l channels. Since the size of conv8 is the same as the input image, a pixel index (x,y) in the conv8 feature maps can be mapped to same pixel in the input image. We make the i-th feature map predict the location of the i-th facial key point as shown in Fig. [\ref=local_mapping]. The location of the largest value in the i-th feature map is the facial key point.

We denote the i-th feature map as [formula] and the i-th facial key point as (xi,yi), where H / W is the height / width of the feature map. Let [formula], where [formula]. Âi is the probability matrix, and Âijk indicates the probability of this pixel belonging to the landmark.

A good possibility matrix should preserve the following information: (1) the probability for the index (xi,yi) should be the largest; and (2) the farther it is away from (xi,yi), the smaller the probability should be. Therefore, we introduce a new ground truth probability matrix [formula], which is calculated as Qijk  =  0.5max (|xi  -  j|,|yi  -  k|) and satisfies the above two principles. Finally, a normalization process is calculated as to ensure [formula].

Since Qi and Âi are two probability matrices, we propose to use the softmax loss to quantify the dissimilarity between the predicted probability matrix Âi and the ground truth probability matrix Qi, which is defined as

[formula]

Note that the local mapping functions can not only help find the initialization, but also help learn the parameters of our deep network. Since the predicted shape in this subsection is obtained by only using the local information, it needs to be refined for the more accurate results.

Cascaded Regression

In this subsection, we present how to learn the cascaded regression based on our deep neural network. Formally, suppose we are given a training set [formula], where Ii is the i-th face image and [formula] is the corresponding facial key points. (xj,yj) denotes the location of the j-th point and p is the number of facial key points.

Given the initial shape S0 from the second module, the cascaded regression approach predicts the facial shape Sk in a cascaded manner. We update the shape sequentially with the iterations [formula] as follows:

[formula]

where Ski is the current estimate of the i-th image after the k-th iteration. ΔSki  =  Rkφk(Ski)  +  bk, which is the linear combination of the feature vector φk plus a biased term bk. φk(  ·  ) represents the shape-indexed features from the current predicted landmarks. Rk is a linear regression matrix and bk is a biased term. A series of {Rk,bk} are learned in the training stage, via the following loss function:

[formula]

The problem ([\ref=least_squares]) is the well-known least square problem. This step aims to regress the Ski to a shape Sk + 1i closer to the hand-labeled landmarks S*i.

To integrate the deep network into the cascaded regression, two problems need to be solved. The first is how to corporate the learned parameters {Rk,bk} into our network and the second is how to extract the features φk(Ski).

Since {Rk,bk} represents the linear matrix and the bias, it can be rewritten as the fully-connected layer, where Rk is the weight filter and bk is the bias filter in the fully-connected layer. Hence, we add K fully-connected layers in our network. The parameters {Rk,bk} can be learned via the back propagation algorithm.

In most existing works, φik is the hand-crafted features, e.g., HOG and SIFT. These features are fixed when performing the cascaded regressions. In this paper, we propose a new way to extract the features similarly to [\cite=spp], which is referred to as "Shape-Indexed Pooling" (SIP).

The SIP layer needs two inputs. The first input is feature maps. Note that different from SIFT and HOG which take an image as input, our proposed method uses the deep network to encode the image into high level descriptors, which increases the representational power of the image. In this paper, we choose the deconv7 as the feature layer. The second input is the current estimated shape. We extract the local features for all landmarks. Fig. [\ref=deep_features] shows the network structure for extracting the features based on the shape Ski. More specially, for each point (xl,yl)∈Ski, we encode it as a bounding box where its top-left and bottom-right coordinates are

[formula]

using the max pooling. Hence, the output for each point is an M-dimensional vector and M is the number of filters in the deconv7 layer. For all landmarks, we concatenate all the vectors into to a long Mp-dimensional vector. This procedure can be viewed as the deep "shape-indexed" feature.

Experiments

Datasets and Data Augmentation

We conduct extensive evaluations of the proposed method on three benchmark datasets.

LFPW [\cite=lfpw]: The Labeled Face Parts in-the-Wild (LFPW) database contains 1,287 images downloaded from the Internet. Due to some invalid URLs, we evaluate the performance on 811 training and 224 test images provided by [\cite=300w].

HELEN [\cite=helen]: It contains 2,330 annotated images downloaded from the Flickr. We use 2,000 images as the training set and 330 images as testing.

300-W [\cite=300w]: The 300-W dataset consists of 3,148 training images from the LFPW, HELEN and the whole AFW [\cite=zhu2012face]. It performs testing on three parts: common subset, challenging subset and the full set. The common subset contains 554 images from LFPW and HELEN databases and the challenging subset contains 135 images from IBUG. The full set is the union of them (689 images).

We conduct evaluations on 68 points (provided by [\cite=300w]) on the LFPW, HELEN and 300-W datasets.

Data augmentation. We train our models only using the data from the training data without external sources. To reduce overfitting on the training data, we employ three distinct forms of data augmentation to artificially enlarge the dataset.

The first form of data augmentation is to generate image rotations. We do this by rotating the image into different angles including {  ±  30,  ±  25,  ±  20,  ±  15,  ±  10,  ±  5,0}.

The second form of data augmentation is to disturb the bounding boxes, which can increase the robustness of our results to the bounding boxes. We randomly scale and translate the bounding box for each image.

The third form of data augmentation is mirroring.

After the data augmentation, the number of training samples is enlarged to 52 times, which is shown in Table [\ref=number_training].

Evaluation. We evaluate the alignment accuracies by two popular metrics, the mean error and the cumulative errors. The mean error is measured by the distances between the predicted landmarks and the ground truths, normalized by the inter-pupil distance, which can be calculated by

[formula]

where Si is the predicted shape and S*i is the ground-truth shape for the i-th image. Di is the distance between two eyes. p is the number of landmarks and n is the total number of face images.

We also report the cumulative errors distribution (CED) curve, in which the mean error larger than l is reported as a failure. Let [formula], and CED at the error l is defined as

[formula]

where Ne  ≤  l is the number of images on which the error ei is no higher than l.

Implementation details. We implement the proposed method based on the open source Caffe [\cite=jia2013caffe] framework, which is an efficient deep neural network implementation. We first crop the image using the bounding box with the 0.2W padding on all sides (top, bottom, left, right), where W is the width of the bounding box. Then we resize it to the size of 256  ×  256 pixels. The number of candicate shapes is set to N = 5000. We set K = 8 and b  =  3. Our network is trained by stochastic gradient descent with 0.9 momentum. The min-batch size of images is 4 and the weight decay parameter is 0.0001. The network's parameters are initialized with the pre-trained VGG19 model.

Comparison with State-of-the-art Algorithms

The first set of experiments is to evaluate the performance of the proposed method and compare it with several state-of-the-art algorithms.

Zhu et al. [\cite=zhu2012face], DRMF [\cite=asthana2013robust], ESR [\cite=cao2014face], RCPR [\cite=burgos2013robust], SDM [\cite=sdm], Smith et al. [\cite=smith2014nonparametric], Zhao et al. [\cite=zhao2014unified], GN-DPM [\cite=tzimiropoulos2014gauss], CFAN [\cite=zhang2014coarse], ERT [\cite=kazemi2014one], LBF [\cite=lbf], cGPRT [\cite=lee2015face] and CFSS [\cite=cfss] are selected as the baselines. Table [\ref=map_hr] shows the comparison results of mean error on the three datasets. It can be observed that the proposed method performs significantly better than all previous methods in all settings. Specifically, on the LFPW dataset, our method obtains a mean error of 4.57, which gives an error reduction of 0.3 compared to the second best algorithm. On HELEN, our method shows an error reduction of 0.38 in comparison with the second best method. On the 300-w dataset, the mean error of our method gives a reduction of 0.54 / 1.56 on the common set and the changeling set.

Fig. [\ref=ced_common] shows the CED curves for different error levels on the 300-W dataset. Again, for all error levels, our method yields the highest accuracy and beats all the baselines. For instance, the proposed method shows a relative increase of 23% on the 300w common set compared to the second best algorithm. The example alignment results of our method are shown in Appendix.

One main reason for the good performance of our method is that instead of using traditional hand-crafted visual features (SIFT, HOG), it uses the deep network to learn the image representations and extracts the deep shape-indexed features.

Further Analyses

Results of local mapping functions. Table [\ref=mean_shape] shows the results of local mapping functions. The results are not good enough due to the fact that the mapping function only considers the local information. However, it gives us a good initial estimation compared with the mean shape.

Effects of different input sizes and networks. This set of experiments is to explore the effects of different deep sub-networks and different sizes of input images. We show the results of two different types of frameworks: VGG-S  and VGG-19, where VGG-S is a small convolutional neural network and it only has 5 convolution layers. Note that some modifications are made on the two frameworks as described above. We also report the results with two different input sizes: 256x256 and 128x128. Table [\ref=different_size] shows the comparison results, from which it can be seen that: (1) the larger network or larger input size will lead to better results; (2) our framework is capable of exploiting different types of characteristics, i.e., accuracy or speed, by using different sub-networks or different input sizes.

Conclusion

In this paper, we proposed an end-to-end deep-network-based cascaded regression method for face alignment. In the proposed deep architecture, an input image is firstly encoded into high level descriptors in the same size of the input image. Based on this representation, we proposed to learn a probability map for each facial key point and use these probability maps to find the initialization for the cascaded regression. Finally, we proposed a shape-indexed pooling layer to extract the deep features based on the deconv7 layer and the current estimated shape. Empirical evaluations on three datasets show that the proposed method significantly outperforms the state-of-the-arts.