Optimal directed searches for continuous gravitational waves

Introduction

A rapidly rotating non-axially symmetric neutron star is expected to emit long lived periodic gravitational waves (GW) signals, also known as "continuous waves" (CW). These signals could be detectable by the second generation of ground based GW observatories such as LIGO [\cite=ligoref] [\cite=ligoref3], Virgo [\cite=virgo2], GEO600 [\cite=Geo], KAGRA [\cite=kagraref] and LIGO-India [\cite=ligoindia]. These observatories are sensitive over a broad frequency range, typically O(10)-O(103)  Hz which means that the neutron star needs to be rotating fairly rapidly. The CW signal is parametrized by the sky-position of the neutron star, a frequency f and its time derivatives (the spindown parameters) [formula], all measured at some fiducial reference time. If the neutron star is in a binary system, we also need to consider the orbital parameters of the binary, though in this paper we focus on isolated systems.

There are a number of interesting astrophysical targets for CW searches. The known pulsars are particularly good examples for which we know all of the afore-mentioned parameters. Such searches, where the sky-position, frequency and spindown are all known accurately, are referred to as targeted searches in the literature. In these cases it is fairly straightforward, at least from a computational point of view, to search for possible GW signals emitted by the neutron star. A number of such searches have been carried out (see e.g. [\cite=Aasi:2013sia]). Most notably, for the Crab and Vela pulsars, the upper-limit on the GW amplitude is more constraining than the limit one derives by assuming that all of the observed spindown is due to GW emission [\cite=Abbott:2008fx] [\cite=Abadie:2011md] [\cite=Aasi:2014jln].

At the other extreme we have the blind searches where nothing is known a priori about the source parameters. One has to survey a data set which could span several months or years, a frequency range of O(103)  Hz, the entire sky and a reasonable choice of spindown parameters. Such searches are computationally limited and are, by far, the most computationally challenging GW searches of all. Results from a number of such searches have been published (see e.g. [\cite=S2ScoX1] [\cite=S2Hough] [\cite=S4PSH] [\cite=S5Powerflux2009] [\cite=S5Powerflux2011] [\cite=EatHS4R2] [\cite=EatHS5R1] [\cite=Abbott:2008uq]). Some of these results [\cite=EatHS4R2] [\cite=EatHS5R1] [\cite=Abbott:2008uq]) have utilized the public distributed computing project Einstein@Home [\cite=Einstweb].

Between these two extremes lie the directed searches where one targets interesting astrophysical objects or regions. In this case the signals parameters are partially known. In particular the sky-position is known accurately but no information is available on the spin frequency of the star and hence the GW frequency. Such searches are also computationally limited. A few such results have been published in the literature so far : a search for CW signals from the supernova remnant Cassiopeia A [\cite=S5CasA], from the galactic center [\cite=Aasi:2013jya] which could potentially harbor a number of young and rapidly rotating neutron stars and from nine young supernova remnants [\cite=owen2014]. A deep search for CW signals from Cassiopeia A (Cas A)using Einstein@Home was completed last year. All of these searches were computationally limited. We expect that similar directed searches will be of great interest in the near future.

When the searches are computationally limited (the directed and blind searches), the most commonly employed methods are semi-coherent: rather that matching the full ~  year long data set with coherent signal templates, one splits up the data set into N shorter segments (stacks) typically ~  hours or ~  days long. Each segment is matched with a set of signal templates coherently and finally the results of these N searches are combined incoherently. Descriptions of such methods can be found in [\cite=ForMetricExprRes] [\cite=HierarchP1] [\cite=HierarchP2] [\cite=Papa:2000wg] [\cite=HoughP2] [\cite=HierarchP3] [\cite=Pletsch:2009uu] [\cite=GlobCorr]. Semi-coherent methods have also been considered as parts of multi-stage hierarchical schemes for surveying large parameter spaces, see for example [\cite=HierarchP3] [\cite=HierarchP1] [\cite=Shaltev:2014toa].

It is very important to spend the computational resources wisely: what search set-up to use, what astrophysical objects to target and for each target what waveforms to search can make the difference between making a detection or missing it.

In this paper we shall focus on the directed searches though the general scheme we propose is also applicable to the blind searches. We assume that we have a list of [formula] potential targets. For each of the targets we assume that we know how far and old it is. We also make assumptions on the likelihood for different values of the signal amplitude, its frequency and the frequency-derivates, as discussed in Sec. [\ref=subsec:astroprior]. Given these priors, the question we address is: what sources should we target? What is the optimal search set-up and what is the search region in frequency and spindown that maximizes our probability of making a detection? Various parts of this problem have been partially addressed previously and here we present a complete solution. We explain how the key search pipeline parameters can be determined, which source and which part of parameter space to target taking into account the prior astrophysical knowledge, the performance of our search software, the available computational resources and the quality of the data from the GW detectors.

Previous works have typically fixed the parameter space to be searched a priori - often based on reasonable astrophysical arguments - and then optimized for the search parameters (e.g. [\cite=BC00] [\cite=HierarchP3] [\cite=Prix:2012yu]). Conversely there are studies of what parameter space to search (see e.g. [\cite=Palomba:2005fa] [\cite=Knispel:2008ue] [\cite=Wade:2012qc] [\cite=Owen:2009tj]) which have largely neglected the computational cost of the GW search. One of the aims of the present work is to integrate both aspects of the problem.

The plan for the rest of this paper is as follows. We start with a review of the expected GW signal and search methods in Sec. [\ref=sec:cw]. The general scheme for optimizing the detection probability is explained in Sec. [\ref=sec:ranking]. Finally we illustrate the general scheme with specific examples in Sec. [\ref=sec:examples] and present the application results of this scheme in Sec. [\ref=sec:Application].

The expected GW signal, astrophysical targets and search methods

The gravitational waveform

We summarize the GW signal waveform from a rapidly rotating neutron star; details can be found in [\cite=JKSPaper]. In the rest frame of the neutron star, the GW signal is elliptically polarized with constant amplitudes A+ ,  × for the two polarizations h+ ,  ×(t). Thus, we can find a frame in the plane transverse to the direction of propagation such that

[formula]

The two amplitudes are related to an overall amplitude h0 and the inclination angle ι between the line of sight from Earth to the neutron star's rotation axis:

[formula]

Numerous mechanisms can cause the GW frequency to change. These include energy loss due to the emission of gravitational radiation, electromagnetic interactions, local acceleration of the source and accretion for neutron stars which have a companion star. The spin frequency of the neutron star is assumed to vary slowly and smoothly with time for the observation duration. This assumption may not hold if the neutron star glitches, but we shall not consider this complication here. With this assumption, it is useful to expand the frequency evolution in a Taylor series expansion

[formula]

where τ is the arrival time of a wavefront at the solar system barycenter (SSB), f is the frequency at a fiducial reference time τ0, and [formula] denote the first and second time derivatives of the frequency at τ0. In this paper we shall assume that the frequency change is sufficiently small that we will not have to consider any terms beyond [formula]. This should suffice for almost all plausible CW sources over the relevant observation times.

As the detector on the Earth moves relative to the SSB, the arrival time of a wavefront at the detector, t, differs from the SSB time τ:

[formula]

Here [formula] is the position vector of the detector in the SSB frame, [formula] is the unit vector pointing to the neutron star, and c is the speed of light; [formula] and [formula] are respectively the relativistic Einstein and Shapiro time delays.

The phase of the signal as observed at the detector, φ(t), is the same as the phase observed at the source, φ(τ), at the corresponding time: φ(τ) = φ(τ(t)) = φ(t). Thus, except for an initial phase φ0, φ(t) depends only on the sky position [formula], and on [formula]. For this reason [formula] are called the phase evolution parameters.

The received signal at the detector is

[formula]

where F+ ,  × are the detector beam pattern functions which depend on the sky position [formula] and on the polarization angle ψ. It is often useful to rewrite the signal as [\cite=JKSPaper]:

[formula]

with the index i running over the different detectors whose data we are considering. The four amplitudes Aμ depend only on (h0,ι,ψ,φ0) which, for this reason are often referred-to as the amplitude parameters. The four detector dependent signals hμi(t) depend on the phase evolution parameters.

The expected GW amplitude

The value of h0 that we expect depends on the emission mechanism that we consider. We refer to [\cite=S2ScoX1] and references therein for a review of emission mechanisms. The most basic estimate of the largest amplitude that we can expect ([formula] of Eq. ([\ref=eq:spindownh0]) below) is given by energy conservation arguments which we now briefly summarize.

Neutron stars typically spindown and thus lose their rotational kinetic energy. Most of this energy loss is due to electromagnetic interactions. The simplest models (see e.g. [\cite=ShapiroTeukolsky]) take the neutron star to be a rotating magnetic dipole [formula] which is mis-aligned with the rotation axis. The system then loses energy at a rate proportional to [formula]. Consider instead a hypothetical star, for which this energy is carried away entirely in gravitational radiation. Let f be the instantaneous frequency of the emitted GW signal, G Newton's constant, and D the distance to the star. Setting the loss in rotational kinetic energy to the energy carried away by GWs leads to a limit on h0 known as the spindown limit [formula]. GWs carry energy away at a rate [formula] given by

[formula]

Here S is a large sphere of radius D centered at the neutron star, dS is the area element on this sphere, and the brackets 〈  ·  〉 denote a time average over a sufficiently large number of GW cycles. We can take the GW waveform given in Eq. ([\ref=eq:h+x]) and calculate [formula]; it is clear that [formula]. Since h0 is proportional to 1 / D, [formula] is independent of D as it should be. On the other hand, the rotational kinetic energy of the star is [formula] (we assume that the GW signal frequency is twice the rotational frequency) so that [formula]. Setting [formula] and averaging over the sphere S, the GW amplitude [formula] can be shown to be

[formula]

The value [formula] is based on energy conservation, and is independent of the actual mechanism which causes the neutron star to emit gravitational radiation. This is thus an upper limit on h0. The actual amplitude of the emitted GWs from isolated neutron stars is expected to be much smaller and depends on the emission mechanism. If we assume that not more than a fraction x of the spindown energy is carried away in gravitational waves, then the corresponding limit is smaller by a factor [formula]:

[formula]

Observational limits on GW emission from the Crab and Vela pulsars constrain x to less than 1% and 10% respectively for these two objects [\cite=Aasi:2013sia].

We concentrate on a particular emission mechanism, that due to the presence of non-axisymmetric distortions in the neutron star. The CW amplitude h0 then depends on the ellipticity ε of the star defined as

[formula]

Here Izz is the principal moment of inertia of the star, and Ixx and Iyy are the moments of inertia about the other axes. A straightforward application of Einstein's quadrupole formula yields:

[formula]

The distribution of ε for neutron stars is uncertain. In fact predictions exist for the maximum strain that a neutron star crust can sustain before breaking according to various neutron star models. However these predictions are only upper limits to the allowed ellipticities rather than predictions of the actual ellipticity values (see e.g. [\cite=NonAxNS2] [\cite=Horowitz] [\cite=JohnsonMcDaniel:2012wg]).

We can combine Eqs. ([\ref=eq:spindownh0]) and ([\ref=eq:GWampl]) to get the value of ε required for emitting at the spindown limit:

[formula]

and correspondingly for emitting in GW a fraction x of the spindown energy:

[formula]

As already pointed out, neutron star crusts cannot sustain deformations with arbitrarily high values of ε: see e.g. [\cite=NonAxNS2] [\cite=Horowitz] for discussions on the possible upper limit on ε. It is important that we take this into account as we plan our searches.

Coherent and semi-coherent search methods

We now turn to techniques for detecting the CW signals described above. We assume [formula] GW detectors labeled by an integer [formula]. We denote the calibrated strain data from the ith detector as xi(t), the detector noise by ni(t) and a possible GW signal by hi(t). In the absence of a signal xi(t) = ni(t) and in the presence of a signal, xi(t)  =  ni(t) + hi(t). We shall assume that the noise in the detector is Gaussian and stationary with zero mean. The noise is then well described by a power-spectral-density function (PSD), S(i)n(f) for the ith detector. [formula] is the total observation duration.

If computational cost were not an issue, the optimal technique for detecting the CW signal would be matched filtering; i.e. correlating the data streams xi(t) coherently with the expected signal hi(t). As shown in the previous section, hi(t) depend on both the phase and amplitude parameters. However one can eliminate the explicit dependance on the amplitude parameters analytically either by maximizing ([\cite=JKSPaper] [\cite=MultiIfoFstat]) or by marginalizing [\cite=Bstat] [\cite=Whelan:2013xka] the coherent detection statistic with respect to these. We use the maximization procedure.

The detection statistic that we obtain is known as the F-statistic. In Gaussian data the distribution of 2F is a χ2 distribution with 4 degrees of freedom: χ24(2F|ρ2). ρ2 is the non-centrality parameter determined by the two amplitudes A+ ,  × and the detector sensitivity and orientation:

[formula]

[formula] is the Fourier transform of the GW signal hi(t) given in Eq. ([\ref=eq:13]). Since the signal is narrow-band in frequency it is reasonable to assume that the PSD is constant over the frequency band of interest. We can take S(i)n outside the integral, evaluate it at the signal frequency f0, and use Parseval's identity to replace the integral over frequency by an integral over time:

[formula]

We have chosen the observation duration to be placed symmetrically about t = 0. We can substitute h(t) from Eqs. ([\ref=Eq:h(t)Signal]) and ([\ref=eq:h+x]) to get the explicit dependence of ρ2i on the amplitude parameters:

[formula]

Here the angle brackets 〈  ·  〉t refer to an average over time.

When dealing with a data set spanning a duration of several months or a year, it is not possible to carry out a purely coherent (e.g. F-statistic) search over ≥    1013 templates (waveforms) with the best sensitivity. The number of templates required to cover the parameter space grows rapidly with the observation time and soon becomes unmanageable for a fully coherent search. Semi-coherent searches have thus been applied in these cases. The general technique is to break up the full data set into shorter segments, search each segment coherently and combine the results of these coherent analyses to produce the final detection statistic. The combination of the coherent analyses will not maintain phase coherence between the segments and for this reason this method is often called semi-coherent or incoherent. There are several methods proposed for this [\cite=Schutz:1999mb] [\cite=HierarchP1] [\cite=HierarchP2] [\cite=GlobCorr] [\cite=Pletsch:2009uu]. We will not go into the details of any of these methods, but we will use the notions of computational cost and sensitivity of such methods in this context.

In semi-coherent methods the final resolution in the signal parameter space is obtained in two steps: the coherent searches and the incoherent combination of the results of the coherent searches. Both these stages require template banks to be set-up. The template bank used in the coherent analysis of the segments is called the coarse grid and comprises [formula] points. For directed searches the sky position is fixed and the coarse grid consists of points in [formula]. The semi-coherent step requires a different template grid, the fine grid, defined by refinement factors for all parameters. From these an overall refinement factor can be derived: [formula] which is the number of fine grid points for each coarse grid point. The final grid will consist of a total of [formula] points.

We consider a stack-slide-type of semi-coherent search where the detection statistic is the average of the F-statistic across the N segments:

[formula]

Here, [formula] is the F-statistic for the [formula] segment. The average [formula] is to be evaluated at a point on the fine grid, while on the right hand side the [formula] are evaluated at a coarse grid point, ideally the closest to the chosen fine grid point. Since [formula] follows a non-central χ2 distribution with 4 degrees of freedom, it follows that [formula] follows a non-central χ2 distribution with 4N degrees of freedom. The non-centrality parameter is the sum of ρ2 over the N segments. For our purposes, ρ2 is approximately constant over each segment, and thus the non-centrality parameter is well approximated by Nρ2.

The computational time for searching a data set that comprises [formula] 1800-s Short time-baseline Fourier Transforms (SFTs) and is divided in N coherent segments, is [formula], where [formula] and [formula] are the timing constants used here for the F-statistic and semi-coherent computations. τF is the time necessary to compute the F-statistic for a single coarse-grid template per SFT. [formula] is the time necessary to compute the final detection statistic per fine-grid template point and per segment. Both timing constants are derived by direct timing of the search software.

The false alarm probability, i.e. the probability of obtaining a value of [formula] above a given threshold, say [formula], in the absence of a signal is

[formula]

We set the detection criterion based on a false-alarm threshold [formula] and, from Eq. ([\ref=eq:falseAlarm]), we find the corresponding threshold [formula]. The probability η of detecting a signal with parameters λj is the probability of obtaining a value of the detection statistic [formula] higher than the detection threshold [formula] when [formula] is drawn from a distribution [formula]:

[formula]

We emphasize that even though we have eliminated the amplitude parameters from the detection statistic [formula], its distribution still depends on them through the non-centrality parameter.

The general optimization scheme

We are now ready to tackle the problem set out in the introduction: given a set of potential targets, what is the optimal choice of parameter space in [formula] we should search for each, and what should be the search set-up (in this case the coherent segment length)? To this end, we begin by discretizing the whole parameter space into many small cells such that:

The cells are non-overlapping, and the union of all the cells covers the parameter space of interest

The computing costs and detection probabilities for each target vary smoothly from one cell to the next

The cost of searching any cell for any target is much smaller that the total computational cost budget available

As long as these conditions are satisfied our optimization method will be largely insensitive to how fine the cell- discretization is chosen.

We associate to each cell, astrophysical target and search set-up a probability to detect a signal with parameters in that cell and the computing cost for searching over the cell waveform parameters with a particular semi-coherent search set-up. The goal of this work is to choose a collection of cells and targets such that:

The sum of the computational cost for searching all the chosen cells is within our computational budget, and

The sum of the probability values for the chosen cells is maximized. By this we mean that other choices of cells, search set-ups or/and targest would yield a lower detection probability.

Single set-up case

To illustrate the procedure we begin by considering a single astrophysical target, i.e. a source corresponding to a single sky-position, unknown frequency and unknown spindowns. We also restrict ourselves to a single search set-up, i.e. we assume a specific coherent segment length and number of segments.

Let us indicate the frequency-spindown parameter space as P. Based on available astrophysical information we define a prior probability density [formula] for different frequency and spindown values. We will later make a particular choice for this prior but the general method we describe now is applicable for any choice.

We break the space P into non-overlapping cells small enough so that the conditions described above are satisfied. It is simplest to consider rectangular cells defined by frequency and spindown widths [formula]. Next we assign a detection probability to each cell for a given data set from an arbitrary number of detectors and spanning a total duration [formula]. We assume that a semi-coherent method is applied with the data broken up onto N segments and the detection statistic is [formula], the average value of 2F over the segments.

We calculate the detection probability for each parameter space cell c. In order to do this we need to assume distributions (priors) for the parameters of the population of signals in that cell. In particular, we need priors for [formula] and h0. We assume that a compact object is present at the position of the astrophysical target and hence we will take the priors on (α,δ) to be 1. The standard physical priors for ψ, cos ι,φ0 are uniform, leading to an average detection probability for such population, having assumed a specific value of h0 and [formula]:

[formula]

For a population of signals with a prior distribution on the amplitude, p(h0), the average detection probability having assumed values of [formula] is:

[formula]

Finally folding in the prior [formula] with the detection probability 〈η〉, we find the total probability of detection for a cell:

[formula]

We note the difference between Pc and 〈η〉. 〈η〉 is the detection probability in a cell with an assumption that the signal is actually in that cell, and Pc is the real detection probability in a cell because it contains the prior probability density for that cell. We also note that 〈η〉h0, cos ι,psi,φ is actually independent of [formula]. In fact, it depends on [formula] only through the prior p(h0). Thus, since the prior [formula] is normalised to unity, we can drop the [formula] dependence in the above equation:

[formula]

The detection probability over the whole parameter space is

[formula]

Computational cost is the other quantity of interest. We define a computational cost density [formula] such that the cost of searching a cell is

[formula]

In practice, the cost function is strictly speaking not a density because of overhead and startup costs associated with a search which make the cost not strictly proportional to the size of the parameter space cell. However we shall neglect this because in practice these overhead costs are controlled and can be kept to a minimum by an appropriate choice of cell size.

We want to define a ranking criterion on the cells such that when we pick, according to that criterion, the top [formula] cells that exhaust the computing budget, the resulting total detection probability ([formula]) is maximum. In other words any other choice of cells would yield a lower value of the total detection probability [formula]. These top [formula] cells are then the ones that we should search.

We use the detection probability and the computational cost to define a ranking for each cell. We motivate this as follows. As explained at the beginning of this Section, the cells are small enough so that the cost for any cell is much smaller than the total available computational budget [formula]. If all the cells had the same cost, then clearly we would use the detection probability to rank the cells and we would simply pick as many top cells as we can before exhausting the computing budget. However, the cells will generally have different costs associated with them hence the ranking by detection probability does not ensure that the total detection probability is maximized. A way to fix this would be to adjust the size of the cells so that they do have the same cost. A simpler method is to instead use the ratio between the detection probability and the cost, which we call efficiency, to rank the cells. Thus for each cell we construct the ratio

[formula]

Note that the efficiency e contains information about the search set-up through 〈η〉 and C, the detector sensitivity through 〈η〉, the astrophysical priors through [formula], and the computational cost through C.

A more rigorous argument that the efficiency is the correct ranking function can be modeled on the proof of the Neyman-Pearson lemma found in most statistics textbooks. We can formulate the problem as finding a region P0  ⊂  P in the parameter space [formula], such that the cost of searching over P0 is a chosen value [formula]

[formula]

and such that the detection probability over the region P0 is larger than over any other region that satisfies the computing budget requirement (Eq. ([\ref=eq:costconstraint])):

[formula]

With the problem formulated in this way, the Neyman-Pearson lemma is directly applicable and it tells us that the optimal choice of the region P0 is to consider level sets of the efficiency function. For a given threshold [formula] on the efficiency, the condition [formula] defines a region P0. We choose [formula] so that the region P0 satisfies the computational cost constraint for a given maximum budget [formula] according to Eq. ([\ref=eq:costconstraint]).

The optimization procedure for a single source and a given set-up is then straightforward. For each cell we compute the efficiency ec, pick the cells starting from the one with the largest efficiency and continue till we have used up all the computing power [formula]. By doing this, we maximise the probability with a limited computing power budget. If we have more than a single astrophysical target we can still use this same ranking criterion: we consider the parameter space cells from the different targets all together and drop the distinction between the different targets. The same procedure described above will yield the optimal detection probability.

The general case

The efficiency ranking introduced in the previous subsection is applicable when we constrain the realm of possible searches to a single search set-up for all the cells and for all the sources. It is clear that this is not optimal and we would gain by allowing for varying set-ups. If we do this we also need to impose the additional constraint:

Each parameter space cell for a given source, must be chosen only once,

because it would clearly be wasteful to search the same cell in parameter space for the same source more than once with different set-ups. The Neyman-Pearson method used earlier cannot incorporate this additional constraint and we must modify our optimization algorithm.

We reformulate our optimization problem in such a way that the widely used method of linear programming (LP) is applicable. LP is a optimization method which extremizes a linear combination of the parameters also fulfilling a set of inequalities [\cite=GVK180926950].

We start again with the discrete form of Eqs. ([\ref=eq:costconstraint]) and ([\ref=eq:sumprob]) using the cells as constructed earlier. Let an integer i label each cell: [formula]. For simplicity we consider searches with a fixed total observation time, 300 days, and use a varying number of segments N to indicate different coherent observation time-baselines. For each cell, we can pick among different set-ups, i.e. different values of N. Let another integer s label the different set-ups: 1  ≤  s  ≤  ns. We now introduce an index j that uniquely labels every different cell-set-up combination: j  ↔  (i,s) and [formula]. Finding the optimal solution for our problem means finding which {cell,set-up} should be picked and which should be discarded. We describe this choice with an occupation index Xj:

[formula]

The ordered set of Xj values with [formula] constitutes a binary number with [formula] digits. The total probability over the set of chosen cells, which is the quantity that we want to maximize, is

[formula]

with Pj being the probability of the cell/set-up j  ↔  (i,s). The computational cost constrain can be expressed as:

[formula]

We use LP to find the values of Xj that satisfy ([\ref=eq:_discrete_sumprob]) under the constraint ([\ref=eq:SumXCleCmax]), and taking the Xj to be real numbers rather than integers. More details on this method, and the reason why the optimization procedure yields (mostly) integers rather than real numbers can be found in the Appendix [\ref=A:LP].

If we consider more than a single target and want to optimize also over targets t, the problem does not change in nature. We simply consider more (cell,set-ups) combinations, each now also labelled by a target "t" index:

[formula]

We want to find the combination of X(i,s)t values that maximizes

[formula]

with the constraints

[formula]

We emphasize that the solution to this optimization problem X(i,s)t solves the problem that we posed in the introduction: it tells us what astrophysical targets (t) we should search; for each target what frequency-spindown values (i) we should search and what semi-coherent search set-up (s) to use in each parameter space cell. Moreover, the scheme incorporates, through the priors, any astrophysical information on the distribution of the relevant signal parameters.

Examples of the optimization scheme

We now illustrate our optimization scheme with a very specific and practical example, namely searching a list of potential targets on the public distributed computing project Einstein@Home [\cite=Einstweb].

The sources that we consider are taken from [\cite=owen2014] and are listed in Table [\ref=tab:sources]. This list comprises supernova-remnants (SNR) whose position in the sky is very well known (better than sub-src second accuracy), and described by their equatorial sky coordinates α,δ. We associate with each source its estimated age τt  ±  dτt and an estimate of what we believe is the maximum intrinsic GW amplitude that it could be emitting: [formula].We label the different point sources with an index t, and [formula].

Astrophysical priors

In order to compute the detection probability in every cell we have to choose the prior on the signal amplitude h0: p(h0) (see Eqs. ([\ref=eq:cellProb]) and ([\ref=eq:11])). The most relevant parameter that h0 depends on, is the ellipticity ε defined in Eq. ([\ref=eq:ellipticitydef]). We thus recast the integral ([\ref=eq:11]) on h0 as an integral on ε. Unfortunately the ellipticity is also the least known parameter so reflecting our ignorance we take a flat probability density on log ε within a conservative range of values. Consider a cell i centered at a particular frequency fi and spindown [formula] for a particular source chosen from Table [\ref=tab:sources]. For this cell we can consider two upper limits on ε: The first is the spindown ellipticity, [formula] of Eq. ([\ref=eq:spindownEllipticity]), with x = 0.01, which is consistent with the latest limits on the emission of gravitational waves from the Crab pulsar [\cite=Aasi:2013sia]. The second limit is based on the results of [\cite=Horowitz], according to which it is unrealistic to expect ε to exceed ~  10- 4. We thus set a cell-dependent maximum acceptable value of ε as (for ease of notation we drop the subscript "x" in [formula]):

[formula]

We consider now the minimum value of ε. If the neutron star were perfectly axisymmetric then ε = 0, h0 = 0 and there would be no GW emission. However deviations from this axisymmetric configuration are expected due to the internal magnetic field, at a level that should be at least ε  ~  10- 14 [\cite=Andersson:2009yt]. We hence take

[formula]

Based on the above discussion, our prior p(ε) is:

[formula]

As an illustration of this choice of prior, consider Cas A taken to be a distance of 3.5 kpc from us. Let us assume the star to be emitting GWs at some frequency f, the fraction of the rotational energy going into GWs to be x = 0.01 and the standard value of the moment of inertia I to be [formula]. At small [formula], [formula] is given by the spindown limit [formula]. As [formula] increases, the spindown limit [formula] also increases and with it also [formula] until it reaches the value 10- 4. This happens at a crossover spin down value of

[formula]

For spindown values in absolute value larger than this crossover spindown value, [formula] ceases to increase and remains constant at a value that corresponds to the maximum ellipticity value that we have set: 10- 4. Correspondingly the detection probability 〈η〉 at a fixed search frequency will cease to increase as a function of the spindown. This is shown in Fig. [\ref=CasA_20days_limted_in_fdot] where we assumed f = 101  Hz, a 20 day coherent integration time and 300 days observation time.

What about the prior [formula]? We consider uniform and log-uniform priors on all these variables with ranges sufficiently large to cover all possible values of these parameters:

[formula]

For the second order spindown parameter we note that if the frequency evolution follows [formula], where n is the braking index, then

[formula]

For pure GW emission n = 5, for all other possible mechanisms n < 5 and in particular for pure electromagnetic emission n = 3 (see e.g. [\cite=ShapiroTeukolsky]). Hence our range for [formula] in ([\ref=eq:Priors]) encompasses all combinations of emission mechanisms.

Different ranges on [formula], [formula] and ε could have been set, based on the estimates of the age of the astrophysical targets. If we assume that the object has been spinning down by f at a spindown rate [formula] during a time [formula], its characteristic age, due to some mechanism with a braking index n, then

[formula]

By maximising Eq. ([\ref=eq:tauc]) with respect to n we derive a maximum range for [formula]. We then use that value in Eq. ([\ref=eq:ddotf]) to derive the largest range for [formula]. The conservative search ranges are then

[formula]

having taken the estimated age of the object as a proxy for its characteristic age [formula]. Note that these maximum ranges for [formula] and [formula] correspond to different n values, namely 2 and 5. This is physically inconsistent for any single source but it ensures the broadest prior range over the search values now, allowing for deviations from the constant braking index model in the past evolution of the star.

Let us assume n = 5, which means emission at the spindown limit, and recast the GW amplitude spindown upper limit (Eq. ([\ref=eq:spindownh0])) as well as the corresponding ellipticity (Eq. ([\ref=eq:spindownEllipticity])), in terms of [formula] :

[formula]

and

[formula]

When n = 5 then [formula] is the shortest lifetime compared to the characteristic ages for other emission mechanisms. Correspondingly the necessary spindown is the largest, and so are the GW amplitude upper limit and the ellipticity. Hence, choosing n = 5 allows for the highest possible value of ε. Correspondingly, if we choose to fold in the prior information on the age of the object Eq. ([\ref=eq:epsilonMax]) becomes:

[formula]

We remind the reader that the index i labels a particular [formula] cell in parameter space.

Grid spacings

Given the ranges for f, [formula] and [formula], we now need to specify the number of templates needed to cover the parameter space covered by each cell. This is a pre-requisite for estimating the computing cost for that cell. As discussed earlier, a semi-coherent search requires a set set of templates for the coherent step and a set for the semi-coherent steps. These are referred to, as the coarse and fine grids respectively. The grid spacings in each search parameter can be parametrized in terms of nominal mismatches [formula] in [formula] respectively[\cite=Pletsch:2010xb]:

[formula]

Following that, the semi-coherent grid spacing in each dimension is taken a factor γk finer that the coarse grid one:

[formula]

with the index k labelling the frequency and spindown parameters: [formula] and [formula]. The refinement factors γ(k) depend on the number of segments:

[formula]

We note that in the simplified problem that we consider here we do not include the loss of signal-to-noise ratio associated with the given mismatches and we do not optimize with respect to searches with different grids.

Application of the optimisation scheme under different assumptions

We now apply our optimization scheme to a search for a CW signal from the sources listed in Table [\ref=tab:sources]. We will consider different priors and show intermediate optimisation results: namely we firstly fix the search set-up and the target and eventually optimise also over these. In Sections  [\ref=subsec:ResSinglesetup],  [\ref=subsec:diffsetups] and [\ref=subsect:agepriorsresults] we will use uniform priors on [formula] in order to illustrate the main features of this optimization scheme. In Section [\ref=sec:LogUniformPriors] we will show the results for the more physically meaningful log-uniform priors.

Uniform priors in f and [formula]

Optimizing at fixed search set-up and separately for each target

For illustration purposes, we consider the simplest case, namely when we have a pre-determined search set-up, i.e. a fixed value for the number of coherent segments N. The optimization scheme will rank the parameter space cells of all the sources in decreasing order of detection-promise and hence yield the parameter space regions that should be searched for each source. We will consider the data to span a total observation time of 300 days, to be from the LIGO Hanford and Livingston detectors at the best sensitivity level of the S6 science run. We assume as computing budget 12-Einstein@Home months (EMs). 1EM corresponds to about 12,000 CPU cores round the clock. Here and throughout the paper we use [formula]=0.18 in Eqs. ([\ref=eq:gridSpacings]) to (46) for the grid spacings. Note, these are arbitrary but reasonable choices of values illustrative of actual searches. We shall take the coherent segments to each be 10 days long, in this section.

The result will depend on what prior we choose. We work with two choices: one that does not fold in the age information (Eq. ([\ref=eq:epsilonMax])) and one that does (Eq. ([\ref=eq:epsilonMaxAge])). We name these priors the "distance-based prior" and the "age-based prior", respectively. In this section we present results for the distance-based priors only and the age-based prior results will be discussed later.

The source in Table [\ref=tab:sources] closest to us is vela Jr with distance estimates ranging from 0.2 to 0.75 kpc. Age and distance estimates are highly uncertain due to the overlap of the SNR with the main Vela SNR and possible interaction between them. Other targets such as IC 443 and G347.3 are relatively close to Earth, with distances of 1.5 kpc and 1.3 kpc respectively. The estimated distance of Cas A is between 3.3 and 3.7 kpc, which is not very close compared with the previous three source we mentioned above. However, Cas A is the youngest source and we include it in our list as a point of comparison.

We define a quantity R as the sum of detection probabilities for the parameter space cells which are chosen for a given source:

[formula]

Note that R is also the actual highest detection probability that one can obtain for that source with the given computing budget. Clearly the highest the R, the most promising is a search for the corresponding target. For a given amount of computing power [formula], sources with higher R are more promising.

The highest value of R, about 1%, is obtained with a search that targets the closest source, Vela Jr, at 200 pc. For the other targets the detection probability is even lower and decreases with increasing distance as summarized in Fig. [\ref=fig:RversusDistAndTcoh](a).

Figs. [\ref=G2662_10days_noage] and [\ref=CasA_10days_noage] display two plots for each target. The (a) plots shows the efficiency, color-coded, for each cell : [formula]. The green curve in the efficiency plots shows [formula] as a function of f. The (b) plots display the cells selected by the optimization procedure to be searched within the computational budget, i.e. the coverage that we can afford. It is interesting to note how the shape of the covered parameter space changes as the source distance increases. As the distance decreases the detection probability per cell Pc increases, but it does so more slowly as the distance decreases because the probability cannot exceed 1. Below [formula], cells with higher [formula] have a higher detection probability through the maximum allowed h0 in Eq. ([\ref=eq:epsilonPrior]). However higher spindown also means larger computational cost due to the broader range in [formula]. So, for the farther away sources like Cas A, the gain in detection probability offsets the computational cost. However for sources which are closer, and for which the gain is smaller, this is not the case. This is the reason why in, say, Fig. [\ref=CasA_10days_noage] more cells are picked from high [formula] regions than for Fig. [\ref=G2662_10days_noage]. In general, given fixed-duration coherent segment, selected cells in farther source (Cas A) are more likely from higher [formula] region.

optimizing with respect to search set-ups and targets

We now vary the possible search set-ups and also optimize over these. Again for illustration we consider seven representative choices of coherent segment lengths: 5, 10, 20, 30, 37.5, 50 and 75 days. As before, the total observation time is 300 days. We present results for the 3 sources Vela Jr (at 200 pc), G347.3 and Cas A.

A plot of the optimal detection probability as a function of the set-up is shown in Fig. [\ref=fig:RversusDistAndTcoh](b), the non-solid lines. For Vela Jr, 20-day segment gives us the best result where the detection probability R is [formula].

Fig. [\ref=G2662_51020days_noage] shows the efficiency and the parameter space region that would be searched having optimized separately for every different set-up. This plot shows that longer coherent segment lengths disfavour very high values of [formula] because the computing power grows more rapidly with increasing [formula] than the gain in detection probability due to the larger range in [formula].

Using LP and optimizing also with respect to search set-ups we obtain the results shown in Fig. [\ref=G2662_best_noage_shortdis] for Vela Jr For illustration purposes we investigate different computing budgets: 12 EM and 24 EM.

We note three points from these results:

For all the targets considered, doubling the computing cost increases the detection probability R by a factor of about 1.8 which means that the probability associated with the cells searched with the additional 12 EM is comparable with that associated to the cells searched by the first 12 EM.

Optimizing with respect to the set-up yields a higher R as compared to a fixed set-up. However this gain is relatively small when compared to the set-up that by itself gave the highest detection probability, as it is illustrated in Fig. [\ref=fig:RversusDistAndTcoh](b). There the solid lines show the detection probability attainable by combining different set-ups for every target and the non-continuous line show the detection probability optimized at fixed set-up. For example, the R for Vela Jr. with the 20-day set-up is [formula] and it grows to [formula] by combining different set-ups; G347.3 similarly increases from [formula] to [formula] and Cas A from [formula] to [formula].

When optimizing for each target also with respect to set-up, the cells selected for Cas A's cells include the 5-day set-up (Fig. [\ref=CasA_best_noage] in Appendix [\ref=section:AppendixFigures]), unlike for the other targets. The reason is that Cas A is the farthest of the considered targets and hence we gain more detection probability by searching higher spindowns, which in turn means higher maximum h0 and hence higher detection probability, than by including more high-frequency cells. The cost of the high-spindown regions is higher than that of lower spindown ones and this is compensated by the optimization procedure by using a shorter time-baseline set-up.

Since our goal is to optimize the probability of making a detection from any source, we do not want to restrict ourselves a priori to a particular source, and hence we optimize now also with respect to the targets.

Since the distance to Vela Jr is uncertain we consider two sets of three targets: Vela Jr at 200 pc and at 750 pc, G347.3 and IC 443. We will show results for 12EM, 24EM and 48EM computing budget [formula] (Fig. [\ref=all_noage] in Appendix [\ref=section:AppendixFigures]). When Vela Jr is assumed at 200 pc, even when [formula] EM, all the picked cells are from Vela Jr. This is because Vela Jr is so much closer to us than the others that the detection probability is maximized by always targeting Vela Jr. Thus, if we really believe that Vela Jr is 200 pc away, then we should concentrate all our computing budget on it. If in the optimization process we assume that Vela Jr is 750 pc away, then the result changes. With a 12 EM budget, cells both from Vela Jr and G347.3 are picked. If we double the budget, some cells from IC 443 become worth searching and this effect becomes even more prominent if we quadruple the budget. However, even at [formula] EM most of the searched parameter space targets Vela Jr. Table [\ref=tab:_R_of_nonage] lists the R numbers and Fig. [\ref=fig:RversusDistAndTcoh](c) diplays them as a function of [formula]. Note that the highest R with respect to set-up is in bold font.

Results with age-based priors

We illustrate the results of the optimization when using the priors of Eq. ([\ref=eq:dotf_age]) that fold in the information on the age of the target. Fig. [\ref=G2662_51020days_longage_longdist] in this subsection and Figs. [\ref=CasA_51020days_age] to [\ref=G3501_51020days] in Appendix [\ref=section:AppendixFigures] show the color-coded [formula]-maps and the selected parameter space to target with a 12 EM computing budget allocated to each of the four targets Vela Jr (closest), Cas A (youngest), G347.3 and G350.1 (close and young). Because of the uncertainty in the age and the distance of Vela Jr, we have investigated the two extreme scenarios: a close and young Vela Jr (CY) and an far and old Vela Jr (FO). As done in the previous section we also optimize the search with respect to set-ups and further with respect to targets. The complete set of results is summarized in Table  [\ref=tab:_R_of_age] and Fig. [\ref=fig:RversusDistAndTcohAge]. We note the following:

The younger the target, the steeper is the slope that determines the prior [formula] volume. This means that for younger targets higher values of [formula] are allowed. At the same distance and frequency, more detection probability can be accumulated at higher [formula] values because of the higher limit in h0.

However, even when optimizing separately for every target the main factor that determines the detection probability at fixed computing cost is the distance. This is summarized in Fig. [\ref=fig:RversusDistAndTcohAge](a).

For the eldest target, Vela Jr with [formula] yrs which shows in Fig. [\ref=G2662_51020days_longage_longdist], the prior [formula] volume is small enough that with the 20-day set-up we do not exhaust the available computing budget. For shorter coherent time-baselines the computational cost is dominated by the incoherent step. As the coherent time-baseline increases the cost of the incoherent sum decreases because there are fewer segments to sum while the cost of coherent step increases rapidly, shifting the balance.

Unlike in the case where we do not fold in the age information, doubling the computing budget does not bring a significant gain in detection probability. The reason is that the parameter space that is available for searching extends just to higher frequencies, not to higher spindowns, and there the sensitivity is lower and hence the increase in detection probability is marginal.

For the older sources the optimal search set-ups with age-based priors favour longer segment durations than those found with distance-based priors because their parameter space is limited to lower [formula] regions.

We now optimize also with respect to sources. Fig. [\ref=all_age] shows that the covered parameter space increases as computing cost increases. We note the following:

At 12 EM the preferred target is Vela Jr solely, if we assume that it is CY. The cells picked by the optimization procedure are obviously the same as the cells picked in the Figure [\ref=2662_best_age](a), corresponding to 10 and 20-day set-ups that gave the highest R = 2.16  ×  10- 2.

If instead we assume that Vela Jr is FO then, at 12 EM, the detection probability is maximized by spending some fraction of the computing budget also on G350.1 and Cas A 54% of the prior parameter space of Vela Jr FO is searched leaving out high f - low [formula] cells which have a low detection probability. Again the optimal set-up is a combination of the 20, 30 and 37.5-day set-ups which yield the top three R in the fixed-set-up optimization, cfr. Figs. [\ref=G2662_51020days_longage_longdist](f), (h) and (j). 10.2 EM (84.9% of total) were spent to accumulate 1.12  ×  10- 2 (95.3%) detection probability from Vela Jr FO and 1.8 EM (15.1%) were spent to accumulate 5.48  ×  10- 4 (4.7%) detection probability jointly from G350.1 and Cas A. So one could say that the Vela Jr FO searched cells are, on average, a factor of 3.57 more efficient at accumulating detection probability per computing cost unit than the cells of the other two targets. The set-ups for the cells picked for Cas A are the same as those picked when optimizing with respect to set-up for Cas A only, cfr. Fig. [\ref=CasA_best_age](a) in Appendix [\ref=section:AppendixFigures]. This is not the case for G350.1: for the selected cells it turns out that it is more efficient to use the small computational budget on more cells with a shorter coherent time-baseline, than with a longer coherent baseline as when optimizing the 12 EM for G350.1 alone, cfr. Fig. [\ref=3501_best_age](a) in Appendix [\ref=section:AppendixFigures].

24 EM buys more parameter space cells for Vela Jr CY, nearly doubling the detection probability with respect to the 12 EM case: from 2.16  ×  10- 2 to 3.88  ×  10- 2.

Under the assumption that Vela Jr is FO, the additional 12 EM (total 24 EM) only increase the detection probability by less then 21%: from 1.17  ×  10- 2 to 1.41  ×  10- 2. This is reasonable: we know in fact that if we had 12 EM to spend just on Cas A the maximum probability that we could achieve is 2.26  ×  10- 3 and on G350.1 it is 5.59  ×  10- 4. So if we had an additional 24EM to spend, at most we could achieve an increase in detection probability of 2.82  ×  10- 3 which amounts to 24% of the 1.17  ×  10- 2. With half of that computing power we achieve just over half of this maximum gain. Regarding the set-ups picked for the different sources the same considerations hold as we made for the 12 EM case.

With 48 EM more than half of the whole prior parameter space of Vela Jr CY is covered. With such a high amount of budget the highest sensitivity cells are still searched with the most efficient search set-up: 10 and 20-day. Detection probability is nearly doubled again and still no computing budget will be spent on Cas A and G350.1. This is because Vela Jr CY has larger parameter space in [formula] and more computing power could be spent on those cells in the higher [formula] region.

Under the assumption that Vela Jr is FO, the additional 24 EM (total 48 EM) only increase the detection probability by less than 18%. Not only more cells from Cas A and G350.1 are searched, but also cells in Vela Jr trend to use longer coherent segments. This is because rather than to spend more computing power on the sources with less potential like Cas A and G350.1, it could be better to use more expense and also more efficient set-ups for Vela Jr.

Fig. [\ref=fig:investigation_age_plots] shows how R and the used computing budget C vary with age, having assumed a search for Vela Jr. at 200 and 750 pc, a coherent time-baseline of 20 days and a computing budget [formula] of 12 and 24 EM. In the young age region, C is always flat. That's because the older the object, the smaller is the prior [formula] volume available for searching. Hence there is an age [formula] at which the allocated [formula] is large enough to just cover such a volume. For higher values of the age the prior space shrinks and less computing power is needed to cover it. For lower values of the age the prior volume is larger and the optimization method will select what cells are the most promising to search while using up all the computational power, hence the plateau at low age values.

Let us now look at R. In all these four cases, R has a maximum value [formula] at a certain age [formula]. A larger computing budget gives a higher [formula] and this happens at a lower age. However [formula] does not coincide with [formula] because even though as the age increases towards [formula] the fractional covered volume of parameter space is increasing, at the same time the total volume is shrinking and the cells that are not any more included are actually the ones contributing the most to the detection probability. This is because the dropped cells are the higher spindown ones which have the highest amplitude cut-off value [formula].

Log-uniform priors in f and [formula]

We do not comment here our findings with the same level of detail used in the previous section, as that was done in order to highlight the main factors contributing to the results. Based on the material presented there, we are confident that the interested reader can do this himself/herself here. We highlight instead the following points:

The log-uniform priors favour lower frequency and lower spindown values with respect to the uniform priors.

Generally when assuming distance-based priors, this decreases the detection probability because the computing power is more eagerly invested in searching for signals with lower spindowns which typically have smaller maximum amplitudes (through Eqs. ([\ref=eq:epsilonMax]) and ([\ref=eq:epsilonMaxAge])). We note about a factor 2-7.6 decrease across the board.

This is strictly not true when assuming age-based priors in fact for Vela Jr (CY) the detection probability at 12 EM increases from 2.16% (uniform and age-based) to 3.10% (log-uniform and age-based). For all the other sources the detection probability decreases but not as much as in the distance-based case.

At fixed source, the optimisation scheme prescribes segment lengths which are higher with respect to those of the uniform-priors searches. The reason for this is that longer duration segments can be more easily afforded at lower [formula] regions, where the [formula] costs are lower.

The prescription for the parameter volumes to search is quite different: this is evident for all sources and set-ups. For example from the 3D plots shown in Figs. [\ref=all_age] and [\ref=all_age_log] we see how markedly the log-uniform priors disfavour high [formula] combinations with respect to the uniform priors.

Fig. [\ref=fig:investigation_age_plots_log] shows how R and C vary with age. The cost curve is the same as the cost curve of Fig. [\ref=fig:investigation_age_plots]. This is quite obvious because the f and [formula] log-uniform prior does not change the computing cost in each cell. However the log-uniform prior has a large influence on the detection probability: the [formula] is 700 years, the smallest. The reason is that the contribution to the detection probability from higher spindown cells that are excluded with respect to the uniform-prior case, is not compensated for by the higher fractional volume of searched parameter space. This means, for log-uniform priors, for the sources with the same distance, "The younger, the better".

Conclusions

Searches for continuous GWs, even the directed ones from sources with known sky-positions, are computationally limited and decisions regarding the parameter space, search set-up and the astrophysical target can make the difference between making or missing a detection. We have described and implemented an optimization scheme with the goal of maximizing the detection probability constrained by a limited computing budget. Specifically, we have addressed the following questions:

On which target(s) should we spend our computing resources?

What parameter space region in frequency and spindown should we search?

What is the optimal search set-up that we should use?

What is the probability of making a detection, given prior assumptions on the signal parameters?

The crucial step in our procedure is that of choosing the priors on the frequency, spindown and ellipticity of the source. We choose the broadest range of plausible values under combinations of two different assumptions: namely using or not the information on the age of the object (age-based priors) and using uniform priors or log-uniform priors for the frequency and frequency derivative. The uniform priors are useful to illustrate the method. The log-uniform priors are more realistic. With these we find the following:

Distance-based priors yield detection probabilities on average a factor of 4 smaller when used in conjunction with log-uniform priors than when used in conjunction with uniform priors. For age-based priors this difference is no more than a factor of two.

The highest detection probability for a search at the LIGO S6 run sensitivity level, using about a year of data from two detectors with a duty factor of 50% and assuming a computing budget of 12 EM, is 6.7%. This is obtained under the assumption that Vela Jr is old, 200 pc away, a uniform distribution for f and [formula], an age-based prior, and assuming that these priors reflect reality. If the f and [formula] are instead log-uniformly distributed and we match our priors to this assumption, the detection probability drops to 3.6%.

The optimisation over set-up for every cell in parameter space yields at most 15% increase in detection probability with respect to single set-up search. Given the complexity of setting up and analysing the results of a search that uses different segment lengths for different areas of parameter space, this result is relevant because it indicates that using a single set-up or at most two (a practical solution), does not significantly impact the chances of making a detection.

Independently of all prior assumptions, all optimal searches cover the broadest fraction of the prior spin down range around the instruments' maximum sensitivity frequencies.

In forthcoming work we will investigate different priors, consider a range of search set-ups including different mismatch parameters, grids, number of segments and segment durations, and optimise over all these. We will fold in the mismatch distribution arising from our choices of nominal mismatch values and of the grids, and not only work with the expected values as done here. Furthermore here we have not considered any uncertainty on the distance of the target and presented results separately having assumed different distances. A more general approach is to marginalize over the distance range using an appropriate prior, for example that given by [\cite=Allen:2014yra]. The same applies to the age estimates.

What we want to stress with this paper is that the parameter space to be searched and the targets to be searched should be part of the search optimization procedure, as well as the search parameters themselves. In previous works these aspects have been considered separately: e.g. [\cite=BC00] [\cite=HierarchP3] [\cite=Prix:2012yu] and [\cite=Palomba:2005fa] [\cite=Knispel:2008ue] [\cite=Wade:2012qc] [\cite=Owen:2009tj]. The interplay between these quantities, for some assumed prior, is very difficult to intuitively predict and hence it is important to have a rational method to do so. The method that we propose here effectively achieves this goal and lends itself to further generalisations.

Acknowledgements

J.M. acknowledges support by the IMPRS on Gravitational Wave Astronomy at the Max Planck Institute for Gravitational Physics in Hannover. M.A.P. gratefully acknowledges support from NSF PHY grant 1104902. The authors thank their colleagues Bruce Allen for useful suggestions that were adopted in this work. We acknowledge Reinhard Prix, Keith Riles, Curt Cutler, Ben Owen and Hyung Mok Lee for stimulating discussions on this work. We also thank Paola Leaci and David Keitel for their comments on the manuscript. This paper was assigned LIGO document number P1500188.

Linear programming

In this appendix we provide some further details of the method of Linear Programming (LP) and its application to our problem.

Recall that the occupation numbers Xi,s (or equivalently Xj) were originally specified as binary numbers, i.e. Xi,s could be either 0 or 1. It is however non-trivial to design an algorithm which solves the optimization problem described in Sec. [\ref=subsec:multiplesetups]. Rather than trying to do so, we have formulated the problem by taking Xi,s to be real and requiring 0  ≤  Xi,s  ≤  1. We have seen how the optimization problem can be solved using linear programming (LP).

The first question that arises is: by allowing Xi,s to be real, do the solutions which maximize [formula] have the vast majority of the Xi,s as either 0 or 1? This is observed empirically to be true in all the cases that we have studied in this paper. We shall now demonstrate that this is in fact a more general feature. We shall restrict ourselves here to the case when there are two possible set-ups for each cell.

To illustrate this we define the efficiency Ei,s = Pi,s / Ci,s. LP yields a set of non trivially occupied cells which can be ordered in decreasing values of Ei,s; thus, i = 1 corresponds to the cell with the largest efficiency, i = 2 the second largest and so on. Consider first the non-degenerate case where all Ei,s are mutually different and we assume that in each cell i only one of the two Xi,s is strictly bigger than 0. We will show in this case that the cell with the lowest efficiency (let j be the index for this cell) is the only one which can have a fractional occupation: 0  <  Xj,sj  ≤  1. If any cell with index l with a higher efficiency would have a fractional occupation, the total [formula] can be increased by decreasing Xj,sj and increasing Xl,sl until either Xl,sl = 1 or Xj,sj = 0 such that the total cost [formula] remains constant. This argument holds for all l  <  j, hence, all Xl,sl with l  <  j must be unity. If Xj,sj is set to be 0 the cell with index j - 1 is now the one with the lowest efficiency among all non-trivially occupied cells. Since [formula] the total cost will be changed marginally if we set Xj,sj either to 0 or to 1.

If a subset of non-trivially occupied cells has the same efficiency, [formula] and [formula] do not change if we decrease the occupation Xi,s by an amount Δi,s and increase the occupation of another cell Xj,sj by Δj,sj if both cells have the same efficiency and if Δj,sj  /  Δi,s  =  Pi,s / Pj,sj is fulfilled. We can shift the occupation among these cells such that one part has occupation 1 and another part 0. One cell of this subset will likely have a fractional occupation which can be set as well to either 0 or 1 without changing the total [formula] significantly. Following the previous argument, all cells with higher efficiency must have the occupation number equal to unity.

We would now like to show that for each cell i with non-trivial occupation numbers, only one of the two Xi,s  >  0, unless the cell has the lowest efficiency. We illustrate this by using the geometrical interpretation of LP. The set of inequalities described earlier define a polygon in the space of the Xi,s in which valid solutions exist. The set of inequalities can lead to either no solutions, an unbounded problem, a unique solution or infinity many solutions. In our situation only the latter two cases are possible. If only a single solution is possible the optimal point lies in one corner of the polygon. If more than one corner points were to lead to the same optimal [formula] any point in the volume enveloped by these points yield the same [formula]. The costs of our ordered set of non-trivially occupied cells can be summed from cell 1 (the one with the highest efficiency) up to the cell j - 1. The remaining cost is then [formula]. We consider now a subset of inequalities valid for cell j. There is Xj,sj  >  0, [formula] and [formula]. The polygon is either a triangle, if Cj / Cj,sj is either bigger than 1 or smaller than 1 for both sj. As depicted in Fig. [\ref=fig:LP_polygon], the polygon is a tetragon if one Cj / Cj,sj is bigger than 1 for one of the sj and smaller than 1 for the other. Both fractions being bigger than 1 means that enough remaining cost is left to fully occupy the cell with one of the two Xj,sj. Smaller than 1 means, the cell is the non-trivially occupied cell with the smallest efficiency. The remaining costs will be used in this cell.

If the enveloping polygon is a tetragon for the cell j with the lowest efficiency, [formula] is maximized if we chose the Xj,sj to be in one of the corners [formula],[formula] or [formula]. The latter case means that the LP optimization leads to a fractional occupation of both Xj,sj simultaneously. Again, setting one Xj,sj to 0, the other one to 1 or both to 0 will not change [formula] significantly. We can however not exclude that pathological cases which are not covered by our assumptions. In practice we only observed the cases decribed here and moreover, we can always shift a few Xi,s such that we only have integer occupations for only a small change in the computational cost budget.

Complete set of figures