A Still Simpler Way of Introducing the Interior-Point Method for Linear Programming

Max-Planck-Institut für Informatik, Saarbrücken, Germany Sanjeev Saxena

Computer Science and Engineering, Indian Institute of Technology, Kanpur, INDIA-208 016

Linear programming is now included in algorithm undergraduate and postgraduate courses for computer science majors. We show that it is possible to teach interior-point methods directly to students with just minimal knowledge of linear algebra.

Introduction

Terlaky [\cite=ref:7] and Lesaja [\cite=ref:4] have suggested simple ways to teach interior-point methods. In this paper, we suggest a still simpler way. Most material required to teach interior-point methods is available in popular text books [\cite=ref:5] [\cite=ref:8]. However, these books assume knowledge of calculus, which is not really required. If appropriate material is selected from these books, then it becomes feasible to teach interior-point methods as the first or only method for linear programming.

The canonical linear programming problem is to

[formula]

Here, A is an n  ×  m matrix, b and c are n-dimensional, and x is an m-dimensional vector. A feasible solution is any vector x with Ax  =  b and x  ≥  0. The problem is feasible if there is a feasible solution, and infeasible otherwise. The problem is unbounded if for every real z, there is a feasible x with cTx  ≤  z, and bounded otherwise. Infeasible problems are bounded.

Maximize cTx is equivalent to minimize - cTx.

Constraints of type [formula] can be replaced by [formula] with a new (slack) variable γ  ≥  0. Similarly, constraints of type [formula] can be replaced by [formula] with a (surplus) variable γ  ≥  0.

We first prepare the problem by deleting superfluous equations and making the rows of A linearly independent. Assume first that A contains a row i in which all entries are equal to zero. If bi is also zero, we simply delete the row. If bi is nonzero, the system of equations has no solution, and we declare the problem infeasible and stop. Now, every row of A contains a nonzero entry, in particular, the first row. We may assume that a11 is nonzero. Otherwise, we interchange two columns. We multiply the ith equation by [formula] and subtract the first equation. In this way, the first entry of all equations but the first becomes zero. If any row of A becomes equal to the all zero vector, we either delete the equation or declare the problem infeasible. We now proceed in the same way with the second equation. We first make sure that a22 is nonzero by interchanging columns if necessary. Then we multiply the ith equation (for i  >  2) by [formula] and subtract the second equation. And so on. In the end, all remaining equations will be linearly independent. Equivalently, the resulting matrix will have full row-rank.

We now have n constraints in m variables with m  ≥  n. If m  =  n, the system Ax = b has a unique solution (recalling that A has full row-rank and is hence invertible). We check whether this solution is nonnegative. If so, we have solved the problem. Otherwise, we declare the problem infeasible. So, we may from now on assume m  >  n (more variables than constraints).

We consider another problem, the dual problem, which is

[formula]

The vector y has m components and the vector s has n components. We will call the original problem the primal problem.

If x is any solution of Ax = b with x  ≥  0 and (y,s) is a solution of ATy + s = c with s  ≥  0, then

We multiply s = c - ATy with xT from the left and obtain

[formula]

As x,s  ≥  0, we have xTs  ≥  0, and hence, cTx  ≥  bTy.

Equality will hold if xTs = 0, or equivalently, [formula]. Since si,xi  ≥  0, [formula] if and only if sixi = 0 for all i.

If x is a feasible solution of the primal and (y,s) is a feasible solution of the dual, the difference cTx  -  bTy is called the objective value gap of the solution pair.

Thus, if the value of the primal and the dual problem are the same, then both are optimal. Actually, from the Strong Duality Theorem, if both primal and dual solutions are optimal, then the equality will hold. We will prove the Strong Duality Theorem in Section [\ref=ref16] (Corollary [\ref=ref5]).

We will proceed under the assumption that the primal as well as the dual problem are both bounded and feasible. We come back to this point in Section [\ref=ref18]. If the primal is unbounded, the dual is infeasible. If the dual is unbounded, the primal is infeasible. If the primal is feasible and bounded, the dual is feasible and bounded. The primal is unbounded if it is feasible and the homogeneous problem "minimize cTx subject to Ax = 0 and x  ≥  0" has a negative objective value. Equivalently, if the problem "minimize 0 subject to cTx  =   - 1, Ax  =  0, and x  ≥  0" is feasible".

Claim [\ref=ref4] implies, that if we are able to find a solution to the following system of equations and inequalities

[formula]

we will get optimal solutions of both the original and the dual problem. Notice that the constraints xisi  =  0 are nonlinear and hence it is not clear whether we have made a step towards the solution of our problem. The idea is now to relax the conditions xisi  =  0 to the conditions xisi  ≈  μ (with the exact form of this equation derived in the next section), where μ  ≥  0 is a parameter. We obtain

[formula]

We will show:

For the iterative improvement, it is important that x  >  0 and s  >  0. For this reason, we replace the constraints x  ≥  0 and s  ≥  0 by x  >  0 and s  >  0 when defining problem Pμ (see Figure [\ref=ref7]).

Note that xisi  ≈  μ for all i implies bTy - cTx  ≈  mμ by Claim [\ref=ref4]. Thus, repeated application of iterative improvement will make the gap between the primal and dual objective values arbitrarily small.

Iterative Improvement: Use of the Newton-Raphson Method

This section and the next follow Roos et al [\cite=ref:5] (see also Vishnoi [\cite=ref:9]).

Let us assume that we have a solution (x,y,s) to

[formula]

We will use the Newton-Raphson Method [\cite=ref:5] to get a "better" solution. Let us choose the next values as x'  =  x + h, y'  =  y + k, and s'  =  s + f. You should think of the steps h, k, and f as small values. Then we want, ignoring the positivity constraints for x' and s' for the moment:

Ax'  =  A(x + h) = b, or equivalently, Ax + Ah = b. Since Ax = b, this is tantamount to Ah = 0.

ATy'  +  s'  =  AT(y + k) + (s + f) = c. Since ATy + s = c, we get ATk + f = c - ATy - s = 0.

x'is'i  =  (xi + hi)(si + fi)  ≈  μ', or equivalently, xisi + hisi + fixi + hifi  ≈  μ'. We drop the quadratic term hifi (if the steps hi and fi are small, the quadratic term hifi will be very small) and turn the approximate equality into an equality, i.e., we require xisi + hisi + fixi  =  μ' for all i.

Thus, we have a system of linear equations for hi,ki,fi, namely,

[formula]

We show in Theorem [\ref=ref23] that system (S) can be solved by "inverting" a matrix.

Note that there are m variables hi, n variables kj, and m variables fi for a total of 2m  +  n unknowns. Also note that Ah  =  0 constitutes n equations, ATk  +  f  =  0 constitutes m equations, and hisi + fixi  =  μ'  -  xisi for all i comprises m equations. So we have 2m  +  n equations and the same number of unknowns. Also note that the xi and si are not variables in this system, but fixed values.

Before we show that the system has a unique solution, we make some simple observations. From the third group of equations, we conclude

(xi + hi)(si + fi)  =  μ' + hifi, and (x  +  h)T(s  +  f)  =  mμ'  +  hTf.

From the third group of equations, we obtain

[formula]

Summation over i yields

[formula]

[formula], i.e., the vectors h and f are orthogonal to each other.

Multiplying ATk + f = 0 by hT from the left, we obtain hTATk + hTf = 0. Since hTAT = (Ah)T = 0, the equality hTf = 0 follows.

cT(x  +  h)  -  bT(y  +  k) = (x + h)T(s + f) = mμ'.

From Claims [\ref=ref2] and [\ref=ref24], (x + h)T(s + f) = mμ'  +  hTf  =  mμ'. Also, applying Claim [\ref=ref4] to the primal solution x'  =  x + h and to the dual solution (y',s')  =  (y + k,s + f) yields cT(x  +  h)  -  bT(y  +  k)  =  (x  +  h)T(s  +  f).

Thus, mμ' is the objective value gap of the updated solution.

The system (S) has a unique solution.

We will follow Vanderbei [\cite=ref:8] and use capital letters (e.g. X) in this proof (only) to denote a diagonal matrix with entries of the corresponding row vector (e.g. X has the diagonal entries [formula]). We will also use e to denote a column vector of all ones (usually of length m).

Then, in the new notation, the last group of equations becomes

[formula]

Let us look at this equation in more detail.

[formula]

As XS- 1 is diagonal with positive items, the matrix [formula] is well-defined. Note that the diagonal terms are [formula]; since x  >  0 and s  >  0, we have xi / si  >  0 for all i. Thus, AS- 1XAT  =  AW2AT  =  (AW)(AW)T. Since A has full rank, (AW)(AW)T, and hence AS- 1XAT, is invertible (see Appendix). Thus,

[formula]

Then, we can find f from f =  - ATk. And to get h, we use the equation: h + S- 1Xf  =  μ'S- 1e - x, i.e.,

[formula]

Thus, system (S) has a unique solution.

What have we achieved at this point? Given feasible solutions (x,y,s) to the primal and dual problem, we can compute a solution (x',y',s')  =  (x  +  h,y  +  k,s  +  f) to Ax'  =  b and ATy'  +  s'  =  c that also satisfies hTf  =  0 and x'Ts  =  mμ' for any prescribed parameter μ'. Why do we not simply choose μ'  =  0 and be done? It is because we have ignored that we want x'  >  0 and s'  >  0. We will attend to these constraints in the next section.

Invariants in each Iteration

Recall that we want to construct solutions (x,y,s) to Pμ for smaller and smaller values of μ. The solution to Pμ will satisfy the following invariants. The first two invariants state that x is a positive solution of the primal and (y,s) is a solution to the dual with positive s. The third invariant formalized the condition xisi  ≈  μ for all i.

(primal feasibility) AxT = b with x > 0 (strict inequality).

(dual feasibility) ATy + s = c with s > 0 (strict inequality).

[formula].

Even though the variance of xisi is [formula], we still use the notation σ2.

We need to show

[formula]

We will do so for μ'  =  (1  -  δ)μ and [formula]. Claim [\ref=ref2] gives us an alternative expression for σ'2, namely,

[formula]

We first show that the positivity invariants hold if σ' is less than one.

If σ' < 1, then x' > 0, and s' > 0.

We first observe that each product x'is'i  =  (xi + hi)(si + fi)  =  μ'  +  hifi is positive. From σ' < 1, we get σ'2  <  1. Since [formula], each term of the summation must be less than one, and hence, -  μ' < hifi  <  μ'. In particular, μ' + hifi > 0 for every i. Thus, each product (xi + h)(si + f) is positive.

Assume for the sake of a contradiction that both xi + hi < 0 and si + fi < 0. But as si > 0 and xi > 0, this implies si(xi + hi) + xi(si + fi) < 0, or equivalently, μ' + xisi < 0, which is impossible because μ',xi,si are all non-negative. This is a contradiction.

We next show σ'  ≤  1 / 2. We first establish

[formula] for all i and [formula].

As [formula], each individual term in the sum is at most σ2. Thus, [formula], and hence, xisi  /  μ  ≥  1  -  σ, and further, μ  /  xisi  ≤  1 / (1  -  σ).

For the second claim, we have to work harder. Consider any m reals z1 to zm. Then [formula]; this is the frequently used inequality between the one-norm and the two-norm of a vector. Indeed,

[formula]

We apply the inequality with zi  =  1  -  xisi  /  μ and obtain the second claim.

Let us define two new quantities

[formula]

Observe that [formula] (from Claim [\ref=ref24]) and [formula]. Also,

[formula]

Finally,

[formula]

and hence,

[formula]

where the second inequality holds since the bound for σ' is increasing in σ, and σ  ≤  1 / 2. We need to choose δ such that the last inequality holds. This is why we put an exclamation mark on top of the ≤  -sign. Setting [formula] for some to be determined constant c yields the requirement

[formula]

This holds true for c  =  1 / 8 and all m  ≥  1. Thus, [formula].

Why do we require σ  ≤  1 / 2 in the invariant? Let us formulate the bound as σ  ≤  σ0 for some to be determined σ0. Then, the inequality ([\ref=ref9]) becomes

[formula]

We want this to hold for [formula] and some c  >  0. In order for the inequality to hold for c  =  0, we need σ0  ≤  2(1  -  σ0), or equivalently, σ0  ≤  2 / 3. Since we want it to hold for some positive c, we need to choose a smaller σ0; 1 / 2 is a nice number smaller than 2 / 3.

Initial Solution

This section follows Bertsimas and Tsitsiklis [\cite=ref:1]; see also Karloff [\cite=ref:3]. We have to deal with three problems: first, how to find an initial solution; second, how to make sure that we are dealing with a bounded problem; third, how to guarantee the third condition of the invariant for the initial solution. There are standard solutions for the first two problems.

Let us assume that we know a number W such that if ([\ref=ref12]) is bounded, there is an optimal solution x* with x*i  <  W for all i. Let e be the column vector of length m of all ones. We may then add the constraint eTx  <  mW to our problem without changing the optimal objective value. If ([\ref=ref12]) is unbounded, the additional constraint makes it bounded.

The standard solution for the second problem is the big M method. In the big M method, we introduce a new variable z  ≥  0, change Ax  =  b into Ax  +  bz  =  b and the objective into "minimize cTx  +  Mz", where M is a big number. We also have the constraint eTx*  <  mW. Note that x  =  0 and z  =  1 is a feasible solution to the modified problem. We solve the modified problem. If z*  =  0 in an optimal solution, we have also found the optimal solution to the original problem. If z*  >  0 in an optimal solution and M was chosen big enough, the original problem is infeasible.

We will see in Section [\ref=ref15] how to find the numbers W and M. We will now give the details and also show how to fulfill the third condition of the invariant for the initial solution, namely, [formula].

We add two new nonnegative variables xm + 1 and xm + 2 and the constraint "eTx + xm + 1  +  xm + 2 = (m + 2)W". Here, xm + 2 is used for the big M method, and xm + 1 is the slack variable for the constraint eTx  +  xm + 2  ≤  (m + 2)W. The new constraint can be satisfied by setting all variables to W. We are aiming for a particularly simple initial solution, namely xi  =  1 for 1  ≤  i  ≤  m + 2 and, therefore, scale the variable xi by xi  =  Wx'i.

Then, eTx + xm + 1  +  xm + 2  =  (m + 2)W becomes eTWx'  +  Wx'm + 1  +  Wx'm + 2  =  (m + 2)W, or equivalently, eTx' + x'm + 1  +  x'm + 2 = m + 2.

Ax = b becomes WAx' = b, or equivalently, [formula].

Finally, cTx becomes cTWx'  =  WcTx'. As W is a constant, the problem is equivalent to minimizing cTx'. After replacing primed variables with unprimed variables, the problem is

[formula]

We now come to the big M part. Let ρ = d  -  Ae. Then, Ax + ρxm + 2 = d holds for xi  =  1, 1  ≤  i  ≤  m + 2, and xm + 1 = xm + 2 = 1. We want a solution in which xm + 2 = 0. Thus, we minimize cxT + Mxm + 2 for a large M. We thus consider the artificial primal problem

[formula]

W  =  2(nU)n suffices if all entries of A and b are integral and [formula] and [formula] as we will see in Section [\ref=ref15]. Assume we also know a number L  >  0 such that in every optimal solution x* to ([\ref=ref10]), either x*m + 2  =  0 or x*m + 2  >  L. Then M  =  4mU / L suffices, if also [formula]. Indeed, if our original problem is feasible, then there is a feasible solution to ([\ref=ref10]) with xm + 2  =  0. The objective value of this solution is less than or equal to (m + 2)U  ≤  2mU since eTx  +  xm + 1  +  xm + 2  =  m + 2 and m  ≥  2. On the other hand, if x*m + 2  >  0 in an optimal solution to ([\ref=ref10]), then x*m + 2  >  L, and hence the optimal objective value is larger than ML  -  2mU  =  2mU. Thus, our original problem is feasible if and only if x*m + 2 in every optimal solution to ([\ref=ref10]). We will see in Section [\ref=ref15] how to determine L.

Assume x*m + 2  =  0 in an optimal solution to ([\ref=ref10]). Then our original problem is feasible by the preceding remark. For x*m + 1 we distinguish two cases. If x*m + 1  >  0, then our original problem is bounded. If x*m + 1  =  0, the problem may be bounded or unbounded. Remark [\ref=ref13] explains how to distinguish these cases.

The dual problem (with new dual variables yn + 1,sm + 1 and sm + 2) is

[formula]

with slack variables s  ≥  0,sm + 1  ≥  0,sm + 2  ≥  0 and unconstrained variables y.

Which initial solution should we choose? Recall that we also need to satisfy the third part of the invariant for some choice of μ, i.e., [formula]. Also, recall that we set xi to 1 for all i. As xm + 1 = 1, we choose sm + 1  =  μ  /  xm  +  1  =  μ. Then, from the last equation, yn + 1 =  - sm + 1 =  - μ. The simplest choice for the other ys is y = 0. Then, from the first equation, s = c + eμ, and from the second equation sm + 2 = M - yn + 1 = M + μ. Observe that all slack variables are positive (provided μ is large enough). For this choice,

[formula]

Thus, [formula]. We can make σ2  ≤  1  /  4 by choosing [formula].

Summary:

Let us summarize what we have achieved. For i  ≥  1, the difference between the primal and the dual objective value is exactly (m + 2)μ(i) (Claim [\ref=ref3]). The gap decreases by a factor [formula] in each iteration, and hence, can be made arbitrarily small. In the next section, we will exploit this fact and show how to extract the optimal solution. Before doing so, we show the existence of an optimal solution.

This paragraph requires some knowledge of calculus, namely continuity and accumulation point. Our sequence (x(i),y(i),s(i)) has an accumulation point (this is clear for the sequence of xi since the x-variables all lie between 0 and m + 2 and we ask the reader to accept it for the others). Then there is a converging subsequence. Let (x*,y*,s*) be its limit point. Then x* and (y*,s*) are feasible solutions of the artificial primal and its dual respectively, and xisi  =  0 for all i by continuity.

Finding the Optimal Solution

This section is similar to [\cite=ref:10] and to the approach in [\cite=ref:5]. Let us assume that we know a positive number L such that any nonzero coordinate of an optimal solution to either primal or dual is at least L. We will see later (Section [\ref=ref15]) how to find such a number in case all entries of A and b are integers.

Consider our sequence of iterates. We show: (1) if some xi becomes sufficiently small, then x*i  =  0 in all optimal solutions, and if some si becomes sufficiently small, then s*i = 0 in all optimal solutions. (2) If μ is sufficiently small, then either xi or si will be sufficiently small.

Let (x,y,s) and μ satisfy the invariants. Let x* be any optimal solution of the primal and (y*,s*) be any optimal solution of the dual. Assume that the smallest nonzero value of x*i and s*i is at least L.

If [formula], then x*i = 0 in every optimal solution.

If [formula], then s*i = 0 in every optimal solution.

By the third part of our invariant, we have [formula]. Thus, [formula], and hence, μ / 2  ≤  xisi  ≤  3μ / 2  ≤  2μ for all i. Further, [formula]. By the first two parts of the invariant, x is a feasible solution of the primal and (y,s) a feasible solution to the dual.

Since x* is an optimal solution, cTx  ≥  cTx*. We apply Claim [\ref=ref4] first to the solution pair x and (y,s) and then to the pair x* and (y,s) to obtain

[formula]

Assume xi  <  L / (4m). Since xisi  ≥  μ / 2, we have si  ≥  μ / (2xi)  >  2mμ / L  ≥  1 / L  ·  xTs. If x*i  >  0, then x*i  ≥  L, and hence,

[formula]

a contradiction. Thus, xi  <  L / (4m) implies x*i  =  0 in every optimal solution.

Since (y*,s*) is an optimal solution, bTy*  ≥  bTy. We apply Claim [\ref=ref4] first to the solution pair x and (y,s) and then to the pair x and (y*,s*) to obtain

[formula]

Assume si  <  L / (4m). Since xisi  ≥  μ / 2, we have xi  ≥  μ / (2si)  >  2mμ / L  ≥  1 / L  ·  xTs. If s*i  >  0, then s*i  ≥  L, and hence,

[formula]

a contradiction. Thus, si  <  L / (4m) implies s*i = 0 in every optimal solution.

We now define two set of indices

[formula]

Clearly, [formula].

For each i, either x*i = 0 in every optimal solution or s*i = 0 in every optimal solution. Thus, cTx*  -  bTy*  =  (x*)Ts*  =  0, and [formula].

As xisi  <  2μ, if [formula], then [formula]. Then, either [formula] or [formula], and hence, either i∈B or i∈N by the Lemma above .

By the Strict Complementarity Theorem (see e.g. [\cite=ref:6] or [\cite=ref:10]), there are optimal solutions x* and (y*,s*) in which x*i > 0 or s*i > 0; thus, both these conditions can not hold simultaneously. Thus, [formula]. Further, from Theorem [\ref=ref5], the above partition is unique (see also [\cite=ref:2]).

In the integer case (Section [\ref=ref15]), if x*i > 0 or s*i > 0, then [formula] and [formula]. Or, the lower bound [formula].

Let (x*,y*,s*) be any optimal solution. As soon as [formula], we can determine the optimal partition (B,N), i.e., x*i  =  0 for i∈N, s*i  =  0 for i∈B and [formula]. We split the variables x into xB and xN, the variables s into sB and sN, the vector c into cB and cN, and our matrix A into AB and AN. Then our system (ignoring the nonnegativity constraints) becomes

[formula]

Since we know that x*N  =  0 and s*B  =  0 in every optimal solution, the system simplifies to

[formula]

This is a system of [formula] equations in [formula] unknowns that is satisfied by every optimal solution.

Let us concentrate on the equation ABxB  =  b. If this equation has a unique solution, call it x*B, then (x*B,x*N) with x*N  =  0 must be the optimal solution, as there is an optimal solution, every optimal solution satisfies ABxB  +  ANxN  =  b and xN  =  0 in every optimal solution. In particular, x*B  ≥  0. Note that if ABxB  =  b has a unique solution, we can find it by Gaussian elimination.

What can we do if ABxB  =  b has an entire solution set? We describe a simple method, which, however, is not the most efficient. There are more efficient methods, see, for example, [\cite=ref:5] or [\cite=ref:10], which do not increase the asymptotic running time. If [formula], the problem

[formula]

has fewer variables than the original primal, and we simply use the interior point method recursively on the smaller problem.

Fortunately, we can force the situation [formula] by using a technique called perturbation. Note that [formula], implies [formula]. Thus s*i  =  0 for all i in every optimal solution and hence the system ATy  =  c must have a solution. Thus we are guaranteed [formula] if ATy  =  c does not have a solution. Assume, it does. Note that AT has n columns, c is an m-vector, and m  >  n. Instead of working with the objective direction c, we solve the problem for the direction c'  =  c  +  c'', where [formula], and ε is positive, but very close to zero. Geometrically, we perturb the optimal direction slightly so as to guarantee that the optimal solution is in a vertex of the feasible region and hence unique, see Figure [\ref=ref8]. Moreover, if ε is small enough, the optimal solution for cost vector c' is also an optimal solution for cost vector c. Using the techniques from Section [\ref=ref15], one can compute an explicit value for ε. We will refrain from doing so. The perturbation also guarantees that ATy  =  c' does not have a solution for any positive sufficiently small ε. Thus [formula] and hence [formula].

We are thus guaranteed that we eliminate at least one primal variable. We now use recursion to solve the smaller problem. As the number of variables decreases after every call, there can be at most O(m) such calls or the running time will go up by a multiplicative factor of m.

Complexity

Let us assume that the initial value of μ is μ0 and that we want to decrease μ to μf. Since every iteration decreases μ by the factor (1  -  δ), we have μ  =  (1 - δ)rμ0 after r iterations. The smallest r such that (1  -  δ)r  ≤  μf is given by

[formula]

or equivalently,

[formula]

If W is an upper bound on the coordinates of the optimal solution to our primal problem and L is a lower bound on a nonzero x*m + 2 in an optimal solution to ([\ref=ref10]), then from Section [\ref=ref18],

[formula]

From Section [\ref=ref16], [formula]. Thus, the number of iterations will be

[formula]

For the integer case, as log L  =  O(n( log n  +   log U)), the number of iterations will be

[formula]

The Bounds

In the previous sections, we used upper bounds on the components of an optimal solution and lower bounds on the nonzero components of an optimal solution. In this section, we derive these bounds. It assumes more knowledge of linear algebra, namely, determinants and Cramer's rule, and some knowledge of geometry. Unless stated otherwise, we assume that all entries of A and b are integers bounded by U in absolute value.

The determinant of a n  ×  n matrix A is a sum of products, namely,

[formula]

The summation is over all permutations π (with the appropriate sign) of n elements and the product corresponding to a permutation π selects the π(i)-th element in row i for each i. Each product is at most Un. As there are n! summands, we have [formula]; the 2 is only needed for n  =  1, see [\cite=ref:1], [\cite=ref:3] or [\cite=ref:6].

Cramer's rule states that the solution of the equation Ax = b (for a n  ×  n non-singular matrix A) is xi = ( det Ai) /  det A, where Ai is obtained by replacing the ith column of A with b.

Assume that the primal is bounded. As all constraints are linear, the solution space will be a convex polytope, and (by convexity) there will be an optimal solution that is a vertex. For each vertex, there is a submatrix A' of A obtained by keeping only n columns of A such that the corresponding coordinates of the vertex are xi = ( det A'i) /  det A'. The remaining m  -  n coordinates are zero. If we assume that each |bi|  ≤  U, the maximum value of [formula] is no more than n!Un. Also, [formula] since a nonzero integer is at least one in absolute value. Thus, x*i  <  W  =  2(nU)n for the coordinates of vertex solutions of the original primal.

In the rest of this section, we mainly discuss bounds for the artificial problem. Let us next ask how small a nonzero coordinate of a vertex solution of the artificial primal problem ([\ref=ref10]) can be? The constraint system is

[formula]

Any vertex solution is determined by some (n + 1)  ×  (n + 1) nonsingular submatrix B of the left-hand side. In the column corresponding to xm + 2, the entries are bounded by (m + 1)U, and all other entries are bounded by U. Since any product in the determinant formula for B can contain only one value of the column for xm + 2, we have [formula]. Consider next det Bi where Bi is obtained from B by replacing one of the columns with the right-hand side. We need to lower bound [formula]. The matrix Bi may contain two columns with fractional values. If we multiply these columns with W, we obtain an integer matrix. Thus, [formula] if nonzero. Thus, any nonzero coordinate of a vertex solution of ([\ref=ref10]) is greater than L, where

[formula]

The constraint system of the dual ([\ref=Ref6]) is

[formula]

The constraint matrix has m + 2 rows and n + 1  +  m + 2 columns. The last m + 2 columns contain an identity matrix, all entries in the column for yn + 1 are one, and in the first n columns most entries are bounded by U. In the row with right-hand side M, the entries are bounded by (m + 1)U. Any vertex solution is determined by some (m + 2)  ×  (m + 2) nonsingular submatrix B of the left-hand side. At most n + 1 columns of B belong to the first n + 1 columns of the left-hand side. The other columns of B contain the identity matrix. Thus, det B is equal to the determinant of a square submatrix of the first n + 1 columns of the left-hand side. We conclude that [formula]. Consider next det Bi where Bi is obtained from B by replacing one of the columns with the right-hand side. The matrix Bi may contain one column with fractional values. If we multiply this column with W, we obtain an integer matrix. Thus, [formula], if nonzero. Thus, any nonzero coordinate of a vertex solution of ([\ref=Ref6]) is also greater than L.

If all entries of A and b are integral and bounded by U in absolute value, then the coordinates of each vertex solution of the primal problem ([\ref=ref12]) are less than W. Any nonzero coordinate of a vertex solution of the artificial primal and its dual is at least L.

If the entries of A and b are rational numbers, we write the entries in each column (or row) with a common denominator. Pulling them out brings us back to the integral case. For example,

[formula]

Thus, if the determinant is nonzero, it is at least 1 / 15.

If the entries are reals, we approximate each aij by a rational number rij with 1  -  1 / n  ≤  aij / rij  ≤  1 + 1 / n. Then, any product of n aijs is upper bounded by (1  +  1 / n)n times the product of the corresponding rijs and lower bounded by (1  -  1 / n)n times the product. Since (1  +  1 / n)n  ≤  e  ≈  2.71 (e here being Euler's number) and (1  -  1 / n)n  ≥  1 / e, we can use the bounds for the rational case to get bounds for the real case.

Acknowledgments

The first author thanks Andreas Karrenbauer and Ruben Becker for intensive discussions and Andreas Karrenbauer for teaching from an earlier draft of this note. The work of the second author was inspired by an informal lecture given by Nisheeth Vishnoi at IIT Kanpur. The second author also thanks the students of CS602 (2014-15 and 2015-2016 batches) for their helpful comments and questions. Thanks also to Romesh Saigal for very prompt replies to queries.

Appendix: Result from Algebra

Assume that A is n  ×  m matrix and the rank of A is n, with n < m. Then, all n rows of A are linearly independent. Or, [formula] (0 here being a row vector of size m) has only one solution αi = 0. Thus, if x is any n  ×  1 matrix (a column vector of size n), then xTA = 0 implies x = 0. Note that (xTA)T  =  ATx. Thus, ATx  =  0 implies x  =  0.

As A is n  ×  m matrix, AT will be m  ×  n matrix. The product AAT will be an n  ×  n square matrix.

Consider the equation (AAT)x = 0. Pre-multiplying by xT we get xTAATx = 0 or (ATx)T(ATx) = 0. Now, (ATx)T(ATx) is the squared length of the vector ATx. If a vector has length zero, all its coordinates must be zero. Thus, ATx  =  0, and hence, x  =  0 by the preceding paragraph.

Thus, the matrix AAT has rank n and is invertible.

Also observe that if X is a diagonal matrix (with all diagonal entries non-zero) and if A has full row-rank, then AX will also have full row-rank. Basically, if the entries of X are [formula] then the matrix AX will have rows as [formula] (i.e., ith row of A gets scaled by xi). If rows of AX are not independent, then there are βs (not all zero) such that [formula], or there are αs (not all zero) such that [formula] with αi  =  βixi.