Correcting the estimator for the mean vectors in a multivariate errors-in-variables regression model

Introduction

[\cite=gleser81] presented the multivariate errors-in-variables regression model. We recap the formulation here and borrow the notation.

In a multivariate errors-in-variables model there are n observed random vectors [formula], where [formula] is p  ×  1 and [formula] is r  ×  1, i = 1,2,...,n. The assumed model is

[formula]

[formula]

Here, the (p + r)-dimensional random vectors [formula], i = 1,2,...,n are i.i.d. with mean [formula] and covariance matrix Σe. The vectors [formula], i = 1,2,...,n, and [formula], i = 1,2,...,n are referred to as the mean vectors. The transformation parameters are the r  ×  p matrix B and r-dimensional vector α. There are two distinct cases. The no-intercept model assumes α is known to be [formula] and B and [formula], i = 1,2,...,n are unknown and to be estimated. The more general intercept model has α, B and [formula], i = 1,2,...,n as unknown and to be estimated.

As in [\cite=gleser81], notation is conveniently condensed by letting X1 be the p  ×  n matrix with columns [formula], i = 1,2,...,n, letting X2 be the r  ×  n matrix with columns [formula], i = 1,2,...,n, letting U1 be the p  ×  n matrix with columns [formula], i = 1,2,...,n, and letting U2 be the r  ×  n matrix with columns [formula], i = 1,2,...,n. We also let E be the (p + r)  ×  n random matrix whose columns are [formula], i = 1,2,...,n. By constructing X  =  (X1',X2')' and U  =  (U1',U2')', we can represent the model as

[formula]

where [formula] is an n-dimensional column vector of ones. [\cite=gleser81] makes the following assumption:

the columns of E are i.i.d. with common mean vector [formula] and common covariance matrix Σe  =  σ2Ip + r, where scalar σ2 > 0 is unknown,

although Section 5 of [\cite=gleser81] demonstrates how this can be relaxed to a more general assumption of Σe  =  σ2Σ0, Σ0 known.

[\cite=gleser81] presents three approaches to estimating the unknown parameters U1, α, B and σ2 of the multivariate "errors-in-variables" model. Firstly, in the maximum likelihood approach the columns of E are assumed i.i.d. multivariate normal allowing estimation of all four parameters. This estimation procedure is referred to as MLE. Secondly, in the ordinary least squares approach U1, α and B are chosen to minimize an orthogonally invariant norm of the residual matrix

[formula]

This estimation procedure is referred to as OLSE. Thirdly, in the generalized least squares approach α and B are chosen to minimize an orthogonally invariant norm of the normalized residual matrix

[formula]

where (Ir  +  BB')1 / 2 is any square root of Ir  +  BB'. This estimation procedure is referred to as GLSE.

The benefit of GLSE and OLSE approaches is they make no assumption on the distribution of the columns of E beyond those given for their mean vector and covariance matrix.

Correction

In this section it becomes necessary to define the following terms in the same way as Section 2 of [\cite=gleser81]. Let

[formula]

let

[formula]

and let GDG' be the eigen-decomposition of W such that D is a diagonal matrix of the ordered eigenvalues and (p + r)  ×  (p + r) matrix G is orthogonal. We partition G as

[formula]

where G11 is p  ×  p.

[\cite=gleser81] presents the following estimators:

[formula]

[formula]

[formula]

where [formula]. In Theorem 2.1 in [\cite=gleser81] ([\ref=B]) and ([\ref=alpha]) are shown to be the OLSEs and GLSEs for B and α, and ([\ref=gleser_estim]) the OLSE for the mean vectors. For the no-intercept model this is correct. However, for the intercept model, while B̂ and [formula] are correct and it is correctly identified that the OLSE of the matrix of mean vectors U1 is given by

[formula]

it is then wrongly stated that this is equal to the key expression given in ([\ref=gleser_estim]).

In the following the term in ([\ref=gleser_min]) is manipulated to show it does not lead to the estimator in ([\ref=gleser_estim]). Expanding ([\ref=gleser_min]) we get

[formula]

Since G is orthogonal, considering the top left block of the equality G'G  =  Ip + r we get the relation G11'G11  +  G21'G21  =  Ip. Using this and [formula] the following simplification arises:

[formula]

From ([\ref=gleser_eqn2]) we have ([\ref=gleser_eqn1]) simplify to:

[formula]

giving the final result

[formula]

We are therefore left with the additional term [formula], which is not present in ([\ref=gleser_estim]). Using the correct expression ([\ref=correct]), Theorem 2.3 in [\cite=gleser81], which states this to also be the MLE, is now also correct.