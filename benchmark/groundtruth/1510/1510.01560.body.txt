=2500

Shoreline and Bathymetry Approximation in Mesh Generation for Tidal Renewable Simulations

Introduction

Ocean and coastal models are routinely used to assess the tidal energy potential of a site as well as any potential environmental impacts due to the presence of energy extraction devices (e.g. [\cite=Hasegawa2011] [\cite=Funke2014-vk] [\cite=Martin-Short2015]). Shoreline contour databases are used to define the simulation domain and create a computational mesh. However, while smaller scale structures in the shoreline may be relevant in the region of primary interest to the given study, a coarser representation is often sought in other regions. For example, small mesh-cells are required in the vicinity of tidal turbines and their wakes, for accurate calculation of power generation or scour patterns. In addition, the accurate prediction of currents in the complex geometries typical of tidal energy sites requires higher mesh resolution near the often intricate shoreline, including small islands, and near areas of steep bathymetry. However, in areas away from the region of interest the smaller scale geometries must be removed in order to alleviate the otherwise stringent requirements on mesh resolution, and therefore reduce computational costs. In addition, as discussed in [\cite=gorman:2007] and [\cite=gorman:2008] an automated shoreline simplification and parameterisation method is paramount to ensure the (reproducible) generation of high quality meshes with minimal user intervention. To facilitate the use of multi-scale simulation methods we only consider unstructured meshes in this work.

Existing methods of simplification (or smoothing) are typically based on geometric criteria, applied to all points in a piece-wise linear contour and involve the modification of point coordinates and/or the removal of points. The method proposed in [\cite=Ramer:1972] and [\cite=douglas_peucker:1973] is perhaps the most widely used example of this type of contour simplification. In fact, the simplified land masses in the Global, Self-consistent, Hierarchical, High-resolution Shoreline (GSHHS) data sets in [\cite=wessel_smith:1996] have been created using the algorithm proposed by Douglas and Peucker [\cite=douglas_peucker:1973]. However, in many cases the resulting geometry can be unsuitable for mesh generation. In particular, the lack of smoothness in the simplified geometry can force poor quality elements from the mesh generator. In addition, consistency between bathymetry and the shoreline contour is sometimes necessary, where the bathymetric map gives a value of approximately the correct value at the given contour. Thus a shoreline is extracted from a bathymetric map and subsequently simplified. In terms of bathymetry simplification, filtering of higher frequency components using Fourier analysis is the obvious method for simplification.

Here we present algorithms targeted towards simplification of shorelines and bathymetry based upon Principal Component Analysis (PCA). While PCA has been applied in many areas including raster composition from measurements, raster analysis [\cite=Demsar_et_al:2013] and beach morphodynamics studies [\cite=ruessink_et_al:2004] [\cite=medina_et_al:1992] [\cite=winant_et_al:1975], the application of PCA towards shoreline and bathymetry simplification is novel. The theoretical framework underpinning the proposed method creates a very robust and efficient geometry simplification method that simplifies the domain geometry in a reproducible way. We demonstrate methods for smoothing realistic shorelines and bathymetry and showcase the utility of such methods in mesh generation, geared towards assessment of tidal renewable energy in coastal regions around the UK.

Shoreline and Bathymetry Simplification Algorithms

Principal component analysis was developed as a multivariate analysis method through the work of Galton[\cite=galton:1889], Pearson [\cite=pearson:1901] and Hotelling [\cite=hotelling:1933] [\cite=hotelling:1936]. PCA is used to identify dominant structures or patterns in data, for example in image analysis and compression [\cite=rosenfeld_kak:1982] and turbulence structure analysis [\cite=holmes_lumley_berkooz:1996]. The aim of PCA is to reduce the dimensionality of the given data, such that the maximum possible variance of the input data is retained. This is achieved by projecting the data onto a set of uncorrelated basis functions. The projections are termed principal components. A more complete description of the theoretical background of PCA is beyond the scope of this paper, but it can be found in [\cite=jolliffe_pca_book:2002] and [\cite=wold_et_al:1987]. We here present a procedural view-point of PCA for completeness. Briefly, let u(x) be an observed variable of a given system, and we repeat M experiments using that system. We denote the outcome of the experiments as ui(x), [formula]. The discrete form of PCA is of particular interest here, where x denotes discrete points where data is provided. Each observation ui(x) is thus structured as a vector, storing the value at the N discrete points:

[formula]

The average is removed from each observation and only the vectors of fluctuations i are subsequently used. For notational convenience let [formula] denote the N  ×  M matrix of all M observations:

[formula]

PCA produces a decomposition of the data in [formula] as a linear combination of a set of N modes:

[formula]

where [formula] is an M  ×  N matrix of the PCA modes. The eigen-vectors ordered into the N  ×  N matrix [formula] are obtained from:

[formula]

where [formula] is the diagonal matrix of eigenvalues and [formula] is the N  ×  N covariance matrix:

[formula]

From an algorithmic point-of-view principal component analysis can be broadly described by the following main steps:

Collect M observations of the system and order the data from each observation as an N-dimensional vector, removing the average from each observation. Then assemble the matrix [formula].

Construct the covariance matrix [formula] using equation [\eqref=eqn:PCA_covarianceMatrix].

Orthogonalise the covariance matrix, as described in equation [\eqref=PCA_eigenproblem].

Calculate the PCA modes from an inversion of equation [\eqref=eqn:PCA_reconstruction_matrixForm]:

[formula]

A key property of PCA is that the first mode represents the most energetic "structure" in the input data, followed by the second mode, and so forth. Thus synthesis from the dominant PCA modes will preserve the most important structures while affecting geometry simplification.

In sections [\ref=ssect:ShorelineSimplification] and [\ref=ssect:BathymetrySimplification] we discuss how we have adapted the steps outlined above towards shoreline and bathymetry simplification.

Shoreline simplification

Shoreline data typically consists of a number of piece-wise linear line contours, each defined as a list of points. Our proposed method of shoreline simplification consists of applying a PCA algorithm to each contour in turn. Figure [\ref=fig:linePartitioning] shows how the decomposition of a line contour has been implemented. The M samples in equation [\eqref=eqn:PCASamples] are formed by partitioning the contour, and each column of the [formula] array corresponds to a partition. Figure [\ref=fig:linePartitioning] also shows that the x-coordinates are treated separately to the y-coordinates, with separate [formula] matrices assembled for each. Two separate eigen-problems are solved for [formula] and [formula] covariance arrays. The partitions are next reconstructed from the chosen dominant modes. As suggested in figure [\ref=fig:linePartitioning], successive partitions are allowed to overlap. The final step is the re-assembly of the contour from the reconstructed partitions and is done on a point-by-point basis: The coordinates of points where partitions overlap are calculated by averaging the coordinate values of the given point, across the overlapping reconstructed partitions. Where partitions do not overlap, the point coordinates are used directly in the corresponding assembled contour point. It has been found that best smoothing results are obtained when the number of partitions is equal to the number of points in a contour. In this way each partition corresponds to a point in the contour. However, when the number of points in a contour is less than the specified number of points per partition, the contour is not considered for decomposition and not reproduced in the output. This has been found to be a very effective way of filtering out small islands, as shown in figure [\ref=fig:shorelineReconstructionMaps].

Bathymetry simplification

The implementation of smoothing for bathymetry assumes the data to be given as a "raster" where data points are laid out as a rectangular grid, such that the points are aligned along the coordinate directions and each point can be identified by a pair of integer indices. Figure [\ref=fig:rasterPartitioning] illustrates the topological structure of the data-points and shows that, as in shoreline simplification, the M samples in [\eqref=eqn:PCASamples] are obtained by partitioning the data. Figure [\ref=fig:rasterPartitioning] also shows that partitions are allowed to partially overlap. As with shoreline reconstruction, best smoothing results are obtained when the number of partitions is equal to the number of points of the input bathymetry. Once the partitions are reconstructed from the dominant modes, the value at any point is calculated as the average of the reconstructions of all partitions overlapping at the given point.

Results

Shoreline simplification on the Western Norwegian coast

The Western Norwegian coast is used here as an exemplar of the effectiveness of the shoreline simplification method outlined in section [\ref=ssect:ShorelineSimplification]. This area was chosen due to the geometrical complexity of its shorelines, with many small islands, but also because a relatively coarse approximation to this shoreline is used in unstructured meshes aimed at tidal flow simulations discussed in section [\ref=ssect:meshGeneration] below.

Figure [\ref=fig:shorelineReconstructionMaps] identifies the region and also shows the results of various reconstructions superimposed on the "full resolution" Global, Self-consistent, Hierarchical, High-resolution Geography (GSHHG) data. Panels (b) to (e) show that many small islands have been removed. As discussed in section [\ref=ssect:ShorelineSimplification] contours whose point-count is smaller than the segment size cannot be decomposed and are not reproduced in the reconstruction. However, islands composed of too few points are likely to be of little utility to a shoreline approximation based on a larger segment size, where small scale islands are desired to be removed in the first place. Figures [\ref=fig:shorelineReconstructionMaps](b) and [\ref=fig:shorelineReconstructionMaps](c) show that increasing the segment size imparts greater smoothing on the shoreline reconstruction. The reconstruction in figure [\ref=fig:shorelineReconstructionMaps](b) was obtained using a segment size of 500 points and only the first PCA mode. Only the very large scale features are captured. The first PCA mode is also solely used in the reconstruction of figure [\ref=fig:shorelineReconstructionMaps](c), but with a smaller segment size, of 100 points. As a result, the reconstruction in panel (c) captures a lot more detail compared with that in panel (b).

Bathymetry simplification on the Orkney Islands.

The PCA-based simplification method described in section [\ref=ssect:BathymetrySimplification] was used to generate the plots in panels (a)-(d) of figure [\ref=fig:bathymetryReconstructionMaps]. The bathymetry, shown in figure [\ref=fig:bathymetryReconstructionMaps](e), is an excerpt from the GEBCO [formula] 2014 data-set, over the Orkney Islands. The region is indicated in figure [\ref=fig:simulationDomainMaps](a). The partition size was 8  ×  8 points, resulting in a total of 64 PCA modes. The reconstructions shown in figures [\ref=fig:bathymetryReconstructionMaps](a)-(d) use successively more PCA modes: Just the first mode in panel (a), 16, 32 and 48 modes in panels (b), (c) and (d) respectively. 64 modes amount to reproducing the input data, shown in panel (e). The first mode captures the most important features of the bathymetry with successive modes adding more details to the reconstruction. The highest peak in the region is in the Isle of Hoy, clearly visible as a prominent peak in all reconstructions, along with the Scottish mainland coast (just behind the Hoy peak in the surface plots) and the bathymetry troughs north-west of the Orkney Islands.

Shoreline and bathymetry simplification in meshing for tidal flow simulations of UK coastal regions.

We here showcase the shoreline and bathymetry simplification in a simulation of tidal flow around the United Kingdom. As figure [\ref=fig:simulationDomainMaps] shows, the simulation domain includes the North Sea, English Channel, Saint George's Channel, Irish Sea and part of the Northern Atlantic. However, the focus of this study is two sites of particularly high potential for renewable energy generation from tides: The Orkney Islands and the Severn Estuary [\cite=Martin-Short2015] [\cite=willis_et_al:2010]. GSHHG "full resolution" [\cite=wessel_smith:1996] data-set, at a resolution of 200 metres, was used as the source of the shorelines shown in figure [\ref=fig:simulationDomainMaps](a). Panels (b), (c) and (d) in figure [\ref=fig:simulationDomainMaps] show how shoreline data and shoreline reconstructions have been combined to define the domain boundaries for a "coarse" (panel b), "intermediate" (panel c) and "fine" (panel d) mesh. The open boundaries shown by yellow lines in figure [\ref=fig:simulationDomainMaps] are constructed by combining lines of constant bearing (loxodromes). The dashed yellow line in figure [\ref=fig:simulationDomainMaps](a) was obtained by combining two loxodromes: The first loxodrome is drawn from point [formula], [formula], at an angle to the North pointing axis (bearing) of [formula], up to [formula]. The second loxodrome is drawn from point [formula], [formula], at a bearing of [formula], up to [formula]. The two loxodromes are combined linearly, such that the loxodrome starting points are the end points of the dashed line in figure [\ref=fig:simulationDomainMaps](a). The dotted yellow line is a combination of the loxodrome from [formula], [formula] at bearing [formula] up to [formula] with the loxodrome from [formula], [formula] at bearing [formula] up to [formula]. The dashed-dotted line across Skagerrak is a loxodrome from [formula],[formula] at bearing [formula] up to [formula]. The lines are then trimmed at the intersections with the shorelines to close the domain. [\ref=fig:simulationDomainMeshes] shows the coarse (panels a and d), intermediate (panels b and e) and fine unstructured triangular mesh (panels c and f) generated from the domain boundaries in figure [\ref=fig:simulationDomainMaps]. All three meshes are generated in EPSG:4326 (the coordinate reference system axes are longitude and latitude in degrees). The element size is prescribed in terms of a target edge length. The bathymetry simplification could be used to calculate a metric based on bathymetry gradient, such that finer resolution is also focused in regions of steep bathymetry. However, the regions of interest here are relatively close to the shorelines, with relatively small slopes. Thus the optimal mesh size can be expressed in terms of proximity functions from shorelines of interest, where detailed reconstructions or full shoreline data are used in the first place. The maximum edge length is [formula] for all meshes, with angles measured along a great circle. Different mesh size gradations are used towards the various shoreline reconstructions: The edge length at shorelines reconstructed using one mode and 500 point partition size (violet lines in figure [\ref=fig:simulationDomainMaps]) was [formula], gradating linearly from the shoreline; [formula] at shorelines reconstructed using one mode and 100 point partition size (red lines in figure [\ref=fig:simulationDomainMaps]), maintained at that size [formula] from the nearest shoreline; [formula] at shorelines reconstructed using five modes and 100 point partition size (blue lines in figure [\ref=fig:simulationDomainMaps]), maintained at that size [formula] from the nearest shoreline; [formula] at shorelines reconstructed using the GSHHG full resolution shoreline (black lines in figure [\ref=fig:simulationDomainMaps]), maintained at that size [formula] from the nearest shoreline. In all gradations, the mesh size increases linearly from the minimum edge length to the maximum, across a distance of [formula]. The effect of different gradations on mesh size in the region of the Orkney Islands are shown in Panels (d), (e) and (f) of figure [\ref=fig:simulationDomainMeshes]. The prescribed edge lengths translate to an approximate length of 10km in the coarse mesh, 1km in the intermediate mesh and 50 metres in the fine mesh. The meshes were produced using Gmsh [\cite=Geuzaine2009-dd], by translating the domain boundary and element size data to formats native to the mesh generator.

Discussion and Conclusion

A new shoreline and bathymetry simplification method has been introduced, and results from its application on the shorelines and bathymetry of the NW European continental shelf have been presented. Existing simplification methods are often based on geometric criteria. The algorithm presented here is based on principal component analysis, so that the shoreline or bathymetry can be expressed in terms of a set of modes and corresponding eigen-vectors. A particularly useful feature of this method is that the modes are calculated such that the most significant structures in the data can be approximated using just a few modes. Bathymetry and shorelines are examples of multi-scale geometries where the length-scales of structures extend over multiple orders of magnitude. Thus a partial reconstruction, using just a few modes instead of all, will give a smooth approximation of the input shoreline or bathymetry, while capturing the most significant structures. The results presented in this paper show that the proposed simplification method can perform effective simplification, and the implementation allows control over the range of scales maintained in the simplified output. Further results are focused towards mesh generation for tidal flow simulations in the context of renewable energy generation. High predictive accuracy in such simulations requires meshes with typically very small element size in the region of interest, so that the smallest scales in the domain geometry are resolved. Yet in areas further away from the region of interest a larger element size is preferable, in order to reduce the computational cost, and a simplified geometry is therefore desired. The results presented here demonstrate how the developed simplification method can be used in this context.

The provision of a framework for algorithmic, or even ad-hoc, processing of geographical data is one of the targets of Geographical Information Systems (GIS). In the context of GIS, data is broadly classified into vector and raster data-structures, and the implementation of the method reflects this classification. In this way, the implementation allows for simplification of any vector or raster data-sets, albeit the focus here is on shorelines and bathymetry.

Ongoing work is aimed at extending bathymetry processing towards combinations of very high resolution data-sets over a small area with lower-resolution data over a wider area. For example, blending a high resolution, high accuracy bathymetric survey of a region earmarked for tidal turbine installations with lower resolution and less accurate bathymetry data over the rest of the simulation domain. Hydrodynamic simulations on more elaborate meshes, where a minimum element edge length is chosen so that power-extracting devices and infrastructure can be resolved or parameterised are also underway, with mesh formats being designed to work with the Fluidity, OpenTidalFarm, Telemac and MIKE models. In terms of the implementation, future work will focus on improvements such as automatic detection of intersecting shorelines, parallelisation, as well as releasing the software source code under a permissive open-source licence.

Acknowledgments

This work was supported by an EPSRC Impact Acceleration Award (EP/K503733/1) and EPSRC grants EP/J010065/1, EP/M011054/1. The authors would also like to acknowledge the support of the Imperial College High Performance Computing Service.