10.9514pt plus.8pt minus .6pt

Learning Summary Statistic for Approximate Bayesian

Computation via Deep Neural Network

Bai Jiang, Tung-yu Wu, Charles Zheng and Wing H. Wong

Stanford University

911.5pt plus.8pt minus .6pt

10.9514pt plus.8pt minus .6pt

1. Introduction

Bayesian inference is traditionally centered around the ability to compute or sample from the posterior distribution of the parameters, having conditioned on the observed data. Suppose data X is generated from a model M with parameter θ, the prior of which is denoted by π(θ). If the closed form of the likelihood function l(θ)  =  p(X|θ) is available, the posterior distribution of θ given observed data xobs can be computed via Bayes' rule

[formula]

Alternatively, if the likelihood function can only be computed conditionally or up to a normalizing constant, one can still draw samples from the posterior by using stochastic simulation techniques such as Markov Chain Monte Carlo (MCMC) and rejection sampling ([\cite=1]).

However, in many applications, the likelihood function l(θ)  =  p(X|θ) cannot be explicitly obtained, or is intractable to compute; this precludes the possibility of direct computation or MCMC sampling. In these cases, approximate inference can still be performed so long as 1) it is possible to draw θ from the prior π(θ), and 2) it is possible to simulate X from the model M given θ, using the methods of Approximate Bayesian Computation (ABC) (See e.g. [\cite=2] [\cite=3] [\cite=4] [\cite=5] [\cite=6] [\cite=7] [\cite=8]).

While many variations of the core approach exist, the fundamental idea underlying ABC is quite simple: that one can use rejection sampling to obtain draws from the posterior distribution π(θ|xobs) without computing any likelihoods. We draw parameter-data pairs (θ',X') from the prior π(θ) and the model M, and accept only the θ' such that X'  =  xobs, which occurs with conditional probability p(xobs|θ') for any θ'. Algorithm [\ref=alg:_ABC1] describes the ABC method for discrete data ([\cite=9]), which yields an i.i.d. sample {θ(i),1  ≤  i  ≤  n} of the exact posterior distribution π(θ|X = xobs).

The success of Algorithm [\ref=alg:_ABC1] depends on acceptance rate of proposed parameter θ', i.e. the probability of the event X' = xobs given θ. For continuous xobs and X', the event X' = xobs happens with probability 0, and hence Algorithm [\ref=alg:_ABC1] is unable to produce any draws. As a remedy, one can relax the acceptance criterion X' = xobs to be ||X' - xobs||  <  ε, where ||  ·  || is a norm and ε is the tolerance threshold. The choice of ε is crucial for balancing efficiency and approximation error, since with smaller ε the approximation error decreases while the acceptance probability also decreases.

However, when the observation vectors are high-dimensional, the inefficiency of rejection sampling in high dimensions results in either extreme inaccuracy, or accuracy at the expense of an extremely time-consuming procedure. To circumvent the problem, one can introduce low-dimensional summary statistic S and further relax the acceptance criterion to be ||S(X') - S(xobs)||  <  ε. The use of summary statistics results in Algorithm [\ref=alg:_ABC2], which was first proposed as the extension of Algorithm [\ref=alg:_ABC1] in population genetics application ([\cite=10] [\cite=11] [\cite=12]).

Instead of the exact posterior distribution, the resulting sample {θ(i),1  ≤  i  ≤  n} obtained by Algorithm [\ref=alg:_ABC2] follows an approximate posterior distribution

[formula]

The choice of the summary statistic is crucial for the approximation quality of ABC posterior distribution. A good summary statistic should offer a good trade-off between two approximation errors ([\cite=13]). The approximation error (1.1) is introduced when one replaces "equal to" with "similar" in the first relaxation of the acceptance criterion. Under appropriate regularity conditions, it vanishes as ε  →  0. The approximation error (1.2) is introduced when one compares summary statistics S(X) and S(xobs) rather than the original data X and xobs. In essence, this is just the information loss of mapping high-dimensional X to low-dimensional S(X). A summary statistic S of higher dimension is in general more informative, hence reduces the approximation error (1.2). But at the same time, increasing the dimension of the summary statistic slows down the rate that the approximation error (1.1) vanishes in the limit of ε  →  0. Ideally, we seek a statistic which is simultaneously low-dimensional and informative.

A sufficient statistic is an attractive option, since sufficiency, by definition, implies that the approximation error (1.2) is zero ([\cite=14]). However, the sufficient statistic has generally the same dimensionality as the sample size, except in special cases such as exponential families. And even when a low-dimensional sufficient statistic exists, it may be intractable to compute.

The main task of this article is to construct low-dimensional and informative summary statistics for ABC methods. Since our goal is to compare methods of constructing sufficient statistics (rather than present a complete methodology for ABC), the relatively simple Algorithm [\ref=alg:_ABC2] suffices. In future work, we plan to use our approach for constructing summary statistics alongside more sophisticated variants of ABC methods, such as those which combine ABC with Markov chain Monte Carlo or sequential techniques ([\cite=15] [\cite=16]). Hereafter all ABC procedures we mentioned in this paper use Algorithm [\ref=alg:_ABC2].

Existing methods for constructing summary statistics can be roughly classified into two classes ([\cite=13]), both of which require a set of candidate summary statistics. The first class consists of approaches for best subset selection. Subsets of the candidate set of summary statistics are evaluated according to various information-based criteria, e.g. measure of sufficiency ([\cite=17]), entropy ([\cite=18]), AIC ([\cite=13]) and BIC ([\cite=13]), and the "best" subset is chosen to be the summary statistic. The second class is projection approach, which constructs summary statistics by linear or non-linear regression on the candidate set ([\cite=19] [\cite=20] [\cite=21] [\cite=22]). Many of these methods require both expert knowledge and candidate summary statistics. An exception is [\cite=22]'s semi-automatic method, in which the authors take powers of individual data points as the initial candidate set and use linear regression to construct summary statistics.

Here we use deep neural networks (DNN) to automatically learn summary statistics for high-dimensional X. [\cite=21] has also considered the use of artificial neural networks for nonlinear projection; however, we are the first to consider the use of multilayer neural networks. The additional representational power offered by DNNs (compared to single-layer networks or other nonlinear function classes) enables our approach to be effective even without the need to specify an initial set of candidate statistics. In all of our experiments, we simply use the original data points as the input. Since we rely on the DNNs to automatically learn the appropriate nonlinear transformations which are necessary to construct useful summaries from the raw data, there is also no need to consider choosing a basis expansion of the original data points as in [\cite=22]'s semi-automatic method. Thus our choice of using DNNs allows to achieve a much higher degree of automation in constructing summary statistics than previous approaches.

Our procedure for constructing summary statistics is as follows:

Generate a training set {θ(i),X(i)} from prior π(θ) and the model M,

Train a DNN, with [formula] as input and [formula] as target,

Use the estimator (X) as summary statistics in ABC procedure.

The DNNs in our approach compose non-linear transformations of data in the hidden layers, and fit linear regressions in the output layer with neurons in the top hidden layer as explanatory variables.

The idea is inspired by the notable successes of deep learning approach in several areas of machine learning, especially computer vision and natural language processing ([\cite=23] [\cite=24] [\cite=25] [\cite=26]). More and more practical and theoretical results show that deep architectures composed of simple learning modules in multiple layers can model high-level abstraction in high-dimensional data. Thus it is expected that DNNs can effectively learn a good approximation to the posterior mean [formula] under squared error loss, given a sufficiently large training set.

Our motivation for using an approximation of [formula] as a summary statistic for ABC is inspired by results of [\cite=22] which demonstrate that the use of the posterior mean [formula] optimizes certain criteria for first-order optimality. In a theoretical section, we extend their results and provide a simple proof in Theorem [\ref=Theorem1] based on conditioning, which is different to [\cite=22]'s proof via density-based calculation. In two simulated experiments, we construct summary statistics for Ising model and moving-average model of order 2. Ising model has sufficient statistic, which is the ideal summary statistic but a highly non-linear function in high-dimensional space. It's a challenging task to construct a summary statistic akin to such a sufficient statistic due to high non-linearity and high-dimensionality. However, we see in our experiments that the DNN-based summary statistic approximates an increasing function of the sufficient statistic. In contrast, the semi-automatic summary statistic is unable to capture information about interactions, and hence fails to approximate the sufficient statistic. For moving-average model of order 2, the DNN-based summary statistic outperforms the auto-covariance statistic, and the semi-automatic construction. It is noteworthy that an automatically constructed summary statistic can outperform auto-covariance in MA(2) model, since the auto-covariance can be transformed to yield a consistent estimate of the parameters, and was widely used in the literature.

The rest of the article is organized as follows. In Section 2, we show how to construct summary statistics using deep neural networks. In Sections 3 and 4, we report the simulation studies on the Ising model and the moving average model, respectively. We describe in the supplementary materials the implementation details of training deep neural networks and other theoretical result, namely, how consistency can be obtained by using the posterior mean of a basis of functions of the parameters.

2. Methods

Throughout the paper, we denote by M the model, [formula] the data, and [formula] the parameter. We assume it is possible to obtain a large number of independent draws X  ~  p(  ·  |θ). Denote by xobs the observed data, π the prior of θ, S the summary statistic, ||  ·  || the norm to measure S(X) - S(xobs), and ε the tolerance threshold. Let πεABC(θ)  =  π(θ|||S(X) - S(xobs)||  <  ε) denote the approximate posterior distribution obtained by Algorithm [\ref=alg:_ABC2].

The main problem we address is how to construct a low-dimensional and informative summary statistic S for high dimensional X, which will enable accurate approximation of πεABC. We are interested mainly in the regime where ABC is most effective: settings when the dimension of X is high (e.g. p = 100) and the dimension of θ is low (e.g. q = 1,2). Given a prior π for θ, our approach is to

Generate a data set [formula] by repeatedly drawing θ(i) from π and drawing X(i) from M with θ(i).

Use Dπ to train a DNN with {X(i),1  ≤  i  ≤  N} as input and {θ(i),1  ≤  i  ≤  N} as target,

Run ABC Algorithm [\ref=alg:_ABC2] with prior π and the DNN estimator (X) as summary statistic.

Our motivation for training a DNN to construct a summary statistic is that the resulting statistic should approximate the posterior mean [formula].

2.1. Posterior Means as Summary Statistics

The main advantage of using the posterior mean [formula] as a summary statistic is that the ABC posterior distribution πεABC(θ) will then have the same mean as exact posterior distribution in the limit of ε  →  0. That is to say, [formula] does not lose any first-order information when summarizing X. As [\cite=22] discussed, using the posterior mean of the parameter itself as the summary statistic maximizes point-estimation accuracy under squared-error loss. We here provide a simple derivation for this extension in Theorem [\ref=Theorem1] based on conditioning, which is different to [\cite=22]'s proof via density-based calculation. We also provide in supplementary material an extension of Theorem [\ref=Theorem1] and show the convergence of the posterior expectation of b(θ) under the posterior obtained by ABC using [formula] as the summary statistic. Such an extension further establishes a global approximation result of posterior distribution.

Assume [formula], then statistic [formula] is well defined. ABC procedure with observed data xobs, summary statistics S, norm ||  ·  || and tolerance threshold ε produces a posterior distribution πεABC. Then we have

[formula]

and

[formula]

First, we show [formula] is a version of conditional expectation of θ given S(X), i.e. [formula]. Denote by σ(X),σ(S(X)) the σ-algebras of X and S(X), respectively. Clearly [formula] is measurable with respect to σ(S(X)). For any event A∈σ(S(X)),

[formula]

Next, write

[formula]

which, due to Jensen's inequality, implies that

[formula]

Letting ε  →  0 yields [formula].

However, users of Bayesian inference generally desire more than just point estimates: ideally, one approximates the posterior π(θ|xobs) globally. We observe that such a global approximation result is possible: if one considers a basis of functions on the parameters, b1(θ),...,bK(θ), and uses [formula] as the summary statistic, the ABC posterior distribution weakly converges to the exact posterior distribution as ε  →  0 and K  →    ∞   at the appropriate rate. We state our approximation theorem in the supplementary material.

It is also worth noting that there is a nice connection between the posterior mean and the sufficient statistics, especially minimal sufficient statistics in the exponential family. Suppose there exists sufficient statistic S* for θ, then [formula] is a function of S*. In the special case of exponential family with minimal sufficient statistic S* and parameter θ, the posterior mean [formula] is a one-to-one function of S*(X), and thus is a minimal sufficient statistic.

2.2. Structure of Deep Neural Network

At a high level, a deep neural network represents a function for transforming input vector X into output (X). The structure of a neural network can be described as a series of L nonlinear transformations applied to X. Each of these L + 1 transformations is described as a layer: where the original input is X, the output of the first transformation is the 1st layer, the output of the second transformation is the 2nd layer, and so on, with the output as the (L + 1)th layer. The layers 1 to L are called hidden layers because they represent intermediate computations, and we let H(l) denote the lth hidden layer. Then the explicit form of the network is

[formula]

where H(0) = X is the input, [formula] is the output, W(l) and b(l) are the parameters controlling how the inputs of layer l are transformed into the outputs of layer l. Let n(l) denote the size of the lth layer: then W(l) is an n(l + 1)  ×  n(l) matrix, called the weight matrix, and b(l) is an n(l + 1)-dimensional vector, called the bias vector. The n(l) components of each layer H(l) are also described evocatively as "neurons" or "hidden units". Figure [\ref=fig:_DNN_structure] illustrates an example of 2-layer DNN with 5 neurons in the 1st hidden layer and 3 neurons in the 2nd hidden layer, for input data [formula].

The role of layer l + 1 is to apply a nonlinear transformation to the outputs of layer l, H(l), and then output the transformed outputs as H(l + 1). First, a linear transformation is applied to the previous layer H(l), yielding W(l)H(l)  +  b(l). The nonlinearity (in this case tanh ) is applied to each element of W(l)H(l)  +  b(l) to yield the output of the current layer, H(l + 1). The nonlinearity is traditionally called the "activation" function, drawing an analogy to the properties of biological neurons. We choose the function tanh  as an activation function due to smoothness and computational convenience. Other popular choices for activation function are [formula] and [formula]. To better explain the activity of each individual neuron, we illustrate how neuron j in the hidden layer l + 1 works in Figure [\ref=fig:_neuron].

The output layer takes the top hidden layer H(L) as input and predicts   =  W(L)H(L)  +  b(L). Note that in many existing applications of deep learning (e. g. computer vision and natural language processing), the goal is to predict a categorical target. In those cases, it is common to use a [formula] transformation in the output layer. However, since our goal is prediction rather than classification, it suffices to use a linear transformation.

2.3. Approximate Posterior Mean by DNN

We use the DNN to construct a summary statistic: a function which maps x to an approximation of [formula]. First, we generate a training set [formula] by drawing samples from the joint distribution π(θ,x). Next, we train the DNN to minimize the squared error loss between training target θ(i) and estimation (X(i)). In order words, we minimize an objective function of the DNN parameters W(0),b(0),...,W(L),b(L), which can be written as

[formula]

We optimize the objective function by using stochastic gradient descent, computing the derivatives using backpropagation ([\cite=27]). See the supplementary material for details.

Our approach is based on the fact that any function which minimizes the squared-error risk for predicting θ from x may be viewed as an approximation of the posterior mean [formula], since the posterior mean [formula] is the minimizer of the squared-error risk [formula]. Hence, any number of supervised learning approaches could be used to construct a prediction rule for predicting θ from x, and thereby provide an approximation of [formula]. Since in many applications of ABC, we can expect [formula] to be a highly nonlinear and smooth function, it is important to choose a supervised learning approach which has the power to approximate such nonlinear smooth functions.

Therefore, deep neural networks appear to be a good choice given their rich representational power for approximating nonlinear functions. Indeed, it is speculated that by increasing the depth and width of the network, the DNN gains the power to approximate any continuous function; however, rigorous proof of the approximation properties of DNNs remains an important open problem ([\cite=28] [\cite=29] [\cite=30]). In any case, in order to take advantage of the representational power (and avoid overfitting), it is important to have a sufficiently large training set. Fortunately, in applications of Approximate Bayesian Computation, an arbitrarily large training set can be generated. Furthermore, it is worth noting that both dataset generation and training can be parallelized.

3. Example: Ising Model 3.1. Model Description and Experiment Design The Ising model consists of discrete variables (+ 1 or - 1) arranged in a lattice (Figure [\ref=fig:_Ising4]).

Each binary variable, called a spin, is allowed to interact with its neighbors. The inverse-temperature parameter θ > 0 characterizes the extent of interaction. Given θ, the probability mass function of Ising model on m  ×  m lattice is

[formula]

where Xj∈{ - 1, + 1}, j  ~  k means Xj and Xk are neighbors, and the normalizing constant

[formula]

Since the normalizing constant requires an exponential-time computation, the probability mass function p(x|θ) is intractable except in small cases. The Ising model is a natural exponential family with minimal sufficient statistics

[formula]

which is a highly non-linear function in high-dimensional space { - 1, + 1}10  ×  10.

Despite of the unavailability of probability mass function, data X can be still simulated given θ using Monte Carlo methods such as Metropolis algorithm ([\cite=1]). The Ising model on a square lattice undergoes a phase transition as θ increases. When θ is small, the spins are disordered. When θ is large enough, the spins tend to have the same sign due to the strong neighbour-to-neighbour interactions ([\cite=31]). The phase transition of Ising model on infinite lattice has a critical point θc  =  0.4406. Ising model on finite lattice is slightly different: its phase transition smoothly occurs around that critical point ([\cite=22]). So we consider Ising model on 10  ×  10 lattice and choose the prior [formula] for θ.

The DNN-based summary statistic S(X) approximates the posterior mean, which in turn is an increasing function of the sufficient statistic S*(X), since the Ising model is an exponential family. Hence, for the evaluation purpose, we compare our DNN-based summary statistic S(X) to the minimal sufficient statistic S*(X), and further compare the two resulting ABC posterior distributions. It's a challenging task to constructing such a summary statistic S(X) due to the high non-linearity of the sufficient statistic [formula] in the high-dimensional space { - 1, + 1}10  ×  10 where X lies in. Given the prior [formula] for θ, we generate a training set by Metropolis algorithm, and train a DNN to learn summary statistic S. We then compare S(X) to the known sufficient statistic S*(X). Figure [\ref=fig:_lsing_scheme] outlines this experimental scheme.

3.2. Results Given the prior [formula], we generate a training set of size 106 and a testing set of size 105 from the Ising model on [formula] lattice. As discussed in Section 3.1, large θ tends to produce X which consists entirely of either + 1 or - 1. Consistent with this, 22.8% of generated test instances have spins with the same sign, which results in sufficient statistic S* = 200, and 4.7% of the test instances have all but one spins with the same sign, resulting in S*  =  192.

A three-layer DNN with n(1)  =  500, n(2)  =  200, and n(3)  =  100 in each hidden layer is trained to predict θ from x; we define the summary statistic S(x) as the output of the DNN. Figure [\ref=fig:_Ising_scatterplot] displays a scatterplot which compares the DNN-based statistic S and sufficient statistic S*. Each point in the scatterplot represents to (S*(x),S(x)) for a single instance x in the testing set. A large number of the instances are concentrated at S*  =  200 and S*  =  192, which appear as points in the top-right corner of the scatterplot. These two cases are relatively uninteresting, so in Figure [\ref=fig:_Ising_heatmap] we display a heatmap of (S(x),S*(x)) excluding the instances with S* = 192,200. It shows that the DNN-based summary statistic approximates a strictly increasing function of sufficient statistic.

Next, two ABC posterior distributions are obtained with S* and S as summary statistic, respectively. For the sufficient statistic S*, we set the tolerance level ε  =  0 so that the ABC posterior sample follows the exact posterior distribution π(θ|X = xobs). For the DNN-based summary statistic S, we set the tolerance threshold ε small enough so that 0.1% of 106 proposed θ's are accepted. We repeat the comparison for 4 different observed data xobs, which are generated from θ  =  0.2,0.4,0.6,0.8, respectively; in each case, we compare the posterior obtained from S* with the posterior obtained from S in Figure [\ref=fig:_Ising_posterior].

It is also worth highlighting the case with true θ = 0.8 in Figure [\ref=fig:_Ising_posterior_0.8]. Since with high probability the spins Xi have the same sign when θ is large, it becomes difficult to distinguish different values of θ above the critical point θc based on the data xobs. Hence we should expect the posterior be small below θc and have a similar shape to the prior distribution above θc. Both of the ABC posteriors demonstrate this property.

4. Example: Moving-average Model 4.1. Model Description and Experiment Design The moving-average model is widely used in time series analysis. Each value in the time series is described by a linear combination of current and previous unobserved white noise error terms. Let [formula] denote the observations in time series. Then the moving-average model of order q, denoted by [formula] is given by

[formula]

where Zj are unobserved white noise error terms. In our experiments, we let [formula] in order to enable exact calculation of the posterior distribution π(θ|xobs), so that we can evaluate the accuracy of the ABC posterior distribution. In the case that Zj's are non-Gaussian (e.g. Student's t-distribution), the exact posterior distribution π(θ|xobs) becomes intractable to compute, but ABC is still applicable.

Approximate Bayesian Computation has been applied to study the posterior distribution of [formula] model using the auto-covariance as the summary statistic [\cite=6]. The auto-covariance is a natural choice for the summary statistic in the (2) model because it converges to a one-to-one function of underlying parameter θ  =  (θ1,θ2) in probability as p  →    ∞  , by the weak law of large number

[formula]

Since the [formula] model is identifiable over the triangular region

[formula]

we consider a uniform prior π over this region, and proceed to construct a summary statistic similarly to the previous example. As before, we use the prior π to generate a training set and train a three-layer DNN to predict θ based on X. The two-dimensional estimator (S1,S2) implicitly defined by DNN is taken as the summary statistic.

We perform a number of experiments to compare the auto-covariance, the DNN-based summary statistic and semi-automatic summary statistic. In each experiment, we generate some true parameter θ from the prior, and draw the observed data xobs. The exact posterior distribution given xobs is numerically computed. Then we compute ABC posterior distributions using the auto-covariance statistic (AC1,AC2), the DNN-based summary statistics (S1,S2), and the semi-automatic summary statistic, respectively. The three resulting approximate posterior distributions are compared to the exact posterior distribution and evaluated in terms of the accuracies of posterior mean of θ, posterior marginal variances of θ1,θ2, and the posterior correlation between (θ1,θ2).

4.2. Results Given the prior π, we generate a training set of size 106 and a testing set of size 105 from the MA(2) model, where each instance is a time series of length p = 100. Then a three-layer DNN with n(1)  =  500, n(2)  =  200, and n(3)  =  100 is trained to predict θ from x. The summary statistic S(x) is defined as the output of the DNN. As shown in Figure [\ref=fig:_MA2_prediction], the DNN predicts θ1,θ2 in the test set with mean squared errors (MSEs) of 0.021 and 0.024, respectively. In comparison, the semi-automatic method achieves MSEs of 0.678 and 0.189.

For observed data xobs which is generated by true parameter θ drawn from π, we run ABC procedures with three different choices of summary statistic: the DNN-based summary statistic, the auto-covariance, and also the semi-automatic summary statistic. The tolerance threshold ε is set to accept 0.1% of 105 proposed data points in ABC procedures. Figure [\ref=fig:_MA2_posterior] compares the true posterior with the posterior draws obtained by ABC, for a particular xobs with θ  =  (0.6,0.2).

Using the DNN-based summary statistic results in a more accurate ABC posterior distribution than either the ABC posterior distribution obtained by using the auto-covariance statistic or the semi-automatic construction. One of the important features of the DNN-based summary statistic is its resulting ABC posterior correctly captures the correlation between θ1 and θ2, while the auto-covariance statistic and the semi-automatic statistic appears to be insensitive to this information (Table [\ref=MA2_abc_stats]).

We further repeated the comparison for 100 different xobs. As Table [\ref=MA2_abc_stats_100] shows, the ABC procedure with the DNN-based statistic better approximates the posterior moments than the ABC posteriors using the auto-covariance statistic and semi-automatic construction.

5. Discussion The problem that we address in this article is how to automatically construct low-dimensional and informative summary statistics for ABC methods, with minimal need for expert knowledge. We base our approach on theoretical results on the desirable properties of the posterior mean as a summary statistic for ABC. However, since the posterior mean is generally intractable, we take advantage of the representational power of DNNs to construct an approximation of the posterior mean as a summary statistic. In contrast to many existing methods that select or construct summary statistics from ad-hoc candidate summary statistics, our approach automatically searches through a rich class of nonlinear transformations of the input data to yield an appropriate summary statistic.

Although we can only heuristically justify our choice of DNNs to construct the approximation (due to a lack of rigorous theory on the approximation properties of DNNs), we obtain promising empirical results. In two examples, the Ising model and the moving-average model, we find that the the DNN-based statistics are good approximations of the posterior means, and further result in high-quality approximations to the true posterior distribution.

Supplementary Materials We first present in supplementary material an extension of Theorem [\ref=Theorem1] and show the convergence of the posterior expectation of b(θ) under the posterior obtained by ABC using [formula] as the summary statistic. Such an extension further establishes a global approximation result of posterior distribution as Theorem 2. Implementation details of backpropagation and stochastic gradient descent algorithms when training deep neural network are provided. The derivatives of squared error loss function with respect to network parameters are computed. They are used by stochastic gradient descent algorithms to train deep neural networks.

Acknowledgements

The authors gratefully acknowledge National Science Foundation grants DMS1407557 and DMS1330132.

=1.7pc =2pt 914pt plus.8pt minus .6pt