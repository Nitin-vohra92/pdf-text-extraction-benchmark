Objective Bayes Covariate-Adjusted Sparse Graphical Model Selection

Introduction

Graphical models are a well-established tool in multivariate statistics. They allow to simplify high-dimensional distributions, both in terms of computations and in terms of interpretation, on the basis of a graph representing independencies between variables. We assume the reader is familiar with the basic theory of undirected and acyclic directed graphical models, as presented for instance in [\cite=Cowe:Dawi:Laur:Spie:1999], or [\cite=Laur:1996]; see also [\cite=Whit:1990].

Our interest lies in a collection of q random variables whose joint distribution, having density with respect to a product measure, embodies a conditional independence structure which can be represented by a Directed Acyclic Graph (DAG). This means that each variable is conditionally independent of its non-descendants given its parents; see Cowell et al. (1999, sect. 5.3). Such a distribution is said to be Markov with respect to the DAG. A DAG model is a (parametric) family of multivariate distributions which are Markov with respect to a DAG. We will consider in particular Gaussian DAG models. Then, the DAG structure will be reflected in the covariance matrix [formula]: if the DAG is complete, [formula] will be unconstrained; for an incomplete DAG, [formula] will present constrained entries. Notice that an unconstrained covariance matrix still has to be s.p.d. (symmetric positive definite).

Typically, the DAG structure is unknown, and we want to infer it from n joint observations of the q variables. From a Bayesian viewpoint one starts with a prior distribution on the collection of all DAGs (prior on model space), as well as with a prior distribution on the parameter space of each given DAG (parameter prior). Given these prior inputs, Bayesian inference produces a posterior probability on the space of all DAGs, which summarizes all the uncertainty in the light of the available data. Several papers have addressed this problem for the case in which the n observations are i.i.d. (independent and identically distributed) conditionally on the parameters of the model; see for instance [\citet=Dawi:Laur:1993] [\citet=Spieetal:1993] [\citet=Hecketal:1995] [\citet=Madi:Etal:1996]. Of special interest for this paper is the work by [\citet=Geig:Heck:2002]; see also [\citet=Cons:Laro:2012] and [\citet=Kuip:Moff:Heck:2014] for corrections. [\citet=Geig:Heck:2002] listed a set of assumptions on the collection of parameter priors (across DAGs) which permit their construction starting from a single parameter prior under a complete DAG (a DAG with all pairs of vertices directly connected). This represents a dramatic simplification because: i) the specification of only one distribution is required, while all the remaining priors are derived from this one; ii) the latter distribution is placed on an unconstrained parameter space describing the model with no independencies. In the Gaussian case ii) means that one can use a standard Inverse Wishart on the covariance matrix, equivalently a Wishart on the corresponding precision matrix (defined as the inverse of the covariance matrix) so that the marginal likelihood can be expressed in closed form.

Different DAGs may define the same DAG model, in which case they are called Markov equivalent. Accordingly, the set of all DAGs for the q variables can be partitioned into Markov equivalence classes (corresponding to distinct DAG models). If DAGs are meant to specify exclusively conditional independencies, as opposed to causal relationships [\citep=Laur:2001] [\citep=Dawi:2003], then all DAGs specifying the same set of conditional independencies should be regarded as indistinguishable using observational data. The method by [\citet=Geig:Heck:2002] ensures that DAGs belonging to the same equivalence class obtain the same marginal likelihood. As a consequence, their method can also be used to infer decomposable graph structures, by simply replacing each structure with an equivalent DAG (no matter which).

Despite its many advantages, the inferential procedure proposed by [\citet=Geig:Heck:2002] still requires the specification of a potentially high-dimensional parameter prior (especially in large q settings). This naturally suggests an objective Bayes approach, which is virtually free from prior elicitation. We carried out this program in [\citet=Cons:Laro:2012] for Gaussian DAG models, using the method of the fractional Bayes factor [\citep=Ohag:1995]. Our findings were consistent with, and extended, those presented in [\citet=Carv:Scot:2009] for Gaussian decomposable graphical models, which relied on the use of the hyper-inverse Wishart distribution [\citep=Leta:Mass:2007].

More recently, research has shifted towards covariate-adjusted estimation of covariance matrices. Motivation for this research stems from the analysis of genetical genomics data (eQTL analysis) where the aim is to study conditional dependence structures of gene expressions after the confounding genetic effects are taken into account. Indeed, an important finding from many genetical genomics experiments is that the gene expression level of many genes is inheritable and can be partially explained by genetic variation; see e.g. [\citet=Brem:Krug:2005]. Since some genetic variants have effects on the expression of multiple genes, they act as confounders when trying to learn the association between the genes. Accordingly, ignoring the effects of genetic variants on the gene expression levels can lead to both false positive and false negative associations in the gene network graph. The effect of genetic variants on gene expression therefore needs to be adjusted in estimating the high-dimensional precision matrix [\citep=Caietal:2013] [\citep=Chenetal:2013].

The problem is usually formulated as one of joint estimation of multiple regression coefficients and a precision matrix, with the latter assumed to be Markov with respect to some graph. Since these models are used in high-dimensional settings, both the regression and the covariance structure are assumed to be sparse. [\citet=Rothetal:2010], [\citet=Yin:Li:2011] and [\citet=Chenetal:2013] assume that the error term is multivariate normal; this assumption is relaxed in the paper by [\citet=Caietal:2013]. The literature in the area, as exemplified in all the papers above, is carried out within a constrained minimization approach (under a suitable norm). Contributions in the Bayesian framework are still very limited. A notable exception is [\citet=Bhad:Mall:2013] who perform variable and covariance selection jointly, using decomposable graphs and weakly informative hierarchical priors.

In this paper we deal with covariate-adjusted selection of Gaussian DAG models within an objective Bayes framework. Specifically, we reconsider the foundations of the approach by [\citet=Geig:Heck:2002], originally presented for the case of i.i.d. sampling, and show that it can be meaningfully extended to the multivariate regression setting. We provide closed-form expressions for the marginal likelihood of any DAG, then we propose an objective Bayes procedure, based on the fractional Bayes factor, which works for DAGs with small parent sets. Our results extend to the regression setup those of [\citet=Cons:Laro:2012] and [\citet=Carv:Scot:2009]; they also complement those of [\citet=Bhad:Mall:2013], both because they are derived within an objective framework, and because they cope with a broader family of graphs.

The paper is organized as follows. Section [\ref=sec:matrixDistrib] reviews the matrix distributions used in the paper, and section [\ref=sec:multivariateRegression] presents the Gaussian multivariate regression setup. Section [\ref=sec:ob] illustrates our objective framework, while section [\ref=sec:covsel] contains our proposal for covariance selection. Finally, section [\ref=sec:disc] briefly discusses our work.

Matrix distributions

Consider n independent observations on q continuous dependent variables, arranged in an n  ×  q response matrix:

[formula]

where [formula] is the i-th observation, and [formula] represents the observations on the j-th variable. Let [formula] be a design matrix with n rows and p + 1 columns (p predictors plus intercept) which we assume known without error; denote by [formula] its rows. We model the observations as [formula], independently over [formula], where [formula] is an unconstrained (p + 1)  ×  q matrix, [formula] is an s.p.d. q  ×  q matrix, and [formula] denotes the q-variate normal distribution with mean vector [formula] and covariance matrix [formula]. The j-th column of [formula], namely [formula], is the vector of regression coefficients for the j-th variable, and [formula]. The distribution of [formula], given [formula] and [formula], is a special case of the matrix normal distribution; the general case, reviewed in section [\ref=subsec:matrixNormal], will give a conjugate prior for [formula] (given [formula]. A conjugate prior for [formula] will be given by the Wishart distribution, which is reviewed in section [\ref=subsec:Wishart].

Matrix normal

We say that the random matrix [formula] follows the matrix normal distribution with mean matrix [formula], row covariance matrix [formula], and column covariance matrix [formula], when [formula] follows the multivariate normal distribution with mean vector [formula] and covariance matrix [formula]; recall that [formula] is the vector obtained from [formula] by stacking its columns on top of one another, while [formula] denotes the Kronecker product. If [formula] is an n  ×  q matrix, [formula] will be an n  ×  q matrix, [formula] an s.p.d. n  ×  n matrix, [formula] an s.p.d. q  ×  q matrix, and we will write

[formula]

see [\citet=Gupt:Naga:2000], and [\cite=Dawi:1981], for more information. We obtain the special case where [formula] is a response matrix, previously described and taken up in section [\ref=sec:multivariateRegression], by letting [formula], and [formula], where [formula] is the n  ×  n identity matrix.

Let [formula] denote the generic element of [formula], and [formula] the generic element of [formula]. Clearly, we have [formula]. Moreover, we have [formula], so that [formula], [formula], whereas [formula], [formula], with [formula] denoting the covariance matrix of the random vector [formula]. More generally, we find [formula] and [formula], if we denote by [formula] the cross-covariance matrix of [formula] and [formula], whose elements are the covariances between all pairs consisting of one element in [formula] and the other in [formula]. Notice that [formula].

Reparameterizing from [formula] s.p.d. to [formula] s.p.d., and from [formula] s.p.d. to [formula] s.p.d., which we will find useful for Bayesian analysis, the density of the matrix normal distribution [formula] can be written as

[formula]

where [formula] denotes the determinant of the matrix [formula], and [formula] its trace. Formula ([\ref=eq:matrixNormal]) follows from the density of [formula], keeping into account that [formula] is the value at [formula] of the bilinear form associated to [formula], which is the precision matrix of [formula], and that [formula]; see [\citet=Laur:1996]. We call [formula] the row precision matrix of [formula], and [formula] its column precision matrix. Clearly, whenever [formula], we have [formula], which means [formula].

Now let J be a proper subset of [formula], and denote by [formula] the submatrix of [formula] consisting of the columns indexed by J. It is immediate to check that [formula] is multivariate normal with mean vector [formula] and covariance matrix [formula], where [formula] is the submatrix of [formula] consisting of the rows and columns indexed by J; see [\citet=Laur:1996]. Hence, column marginalization results in

[formula]

Notice that, if [formula], then [formula].

Finally, letting [formula], it is well known that [formula] is multivariate normal with mean vector [formula], and precision matrix [formula], where [formula]; see [\citet=Laur:1996]. Since [formula], we find

[formula]

for column conditioning. In the case [formula], formula ([\ref=eq:conditioning]) returns [formula], independently over [formula], where [formula] and [formula] are the subvectors of [formula] and [formula], respectively, consisting of the elements indexed by J, while [formula] is the i-th row of [formula].

Wishart

Let [formula] be a q  ×  q unconstrained s.p.d. random matrix. We will write [formula] to mean that [formula] follows a Wishart distribution with density

[formula]

[formula] s.p.d., and [formula], otherwise, where [formula] is a q  ×  q s.p.d. matrix, a is a scalar strictly greater than q - 1, and [formula] is the q-dimensional gamma function at a / 2 (generalizing [formula]). As for parameter interpretation, it can be shown that [formula]. Our notation [formula] for the density ([\ref=eq:Wishart]) is essentially that of [\citet=DeGr:1970]; other authors [\citep=Pres:1982] [\citep=Laur:1996] would use [formula] in place of [formula].

We now recall some useful results. Let [formula] be the precision matrix of [formula], that is, [formula]. Think of [formula] as the generic row of the matrix [formula] (dropping subscript i). Partition [formula] and [formula] into the blocks corresponding to the variables indexed by J and its complement [formula], for a given proper subset J of [formula]:

[formula]

The block [formula] is the marginal covariance matrix of [formula] (obtained from [formula] by selecting the elements of [formula] indexed by J). Denote by [formula] the conditional covariance matrix [formula] of [formula] given [formula] (obtained from [formula] by complementary selection). Then

[formula]

that is, [formula] is the Schur complement of [formula] in [formula], as well as the inverse of [formula].

Formula ([\ref=eq:condVar]) expresses a relationship between four blocks of [formula] and a corresponding block of [formula]. Hence, by switching the roles of [formula] and [formula], we obtain

[formula]

where [formula] is to be interpreted as Schur complementation followed by inversion. Therefore, working with covariance matrices, marginalization corresponds to submatrix extraction and conditioning to Schur complementation, whereas, working with precision matrices, marginalization corresponds to Schur complementation and conditioning to submatrix extraction.

Now let [formula], with [formula] an s.p.d. matrix and a > q - 1. If [formula] is partitioned as in ([\ref=eq:partitionedOmega]), and [formula] is partitioned accordingly, then

[formula]

independently of [formula], where of course || = q - |J|; see [\citet=Laur:1996] who also gives the distribution of [formula].

Gaussian multivariate regression

We return to the scenario discussed in the Introduction, leading to covariate-adjusted graphical model selection, and to the response matrix [formula] introduced at the beginning of section [\ref=sec:matrixDistrib]. Denote by [formula] the [formula] matrix of all possible [formula] predictors. In eQTL analysis [formula] is typically very large, and often much larger than n. However, because of sparsity considerations, only models of the type [formula] need be taken into consideration, where [formula] is an n  ×  (p + 1) design matrix having the unit vector [formula] as first column and [formula] predictors from [formula] as remaining columns, while [formula] is an n  ×  q matrix of error terms with distribution [formula], and [formula] is a (p + 1)  ×  q matrix of regression coefficients ([formula] being the n  ×  q zero matrix). Hence, in principle, it is not unreasonable to assume n > p + 1; in practice p will be much smaller than n, as we illustrate in the Discussion. Notice that the p predictors to be used will not be known a priori, and therefore it will be necessary to carry out variable selection together with covariance selection; this will be feasible using the marginal likelihoods corresponding to different design matrices. For simplicity, we will use a single [formula] in our notation (without explicitly conditioning on it).

In section [\ref=subsec:conjugateAnalysis] we summarize the main features of a standard conjugate analysis of the model

[formula]

with [formula] unconstrained. This is done for completeness and for the benefit of the reader, so that the subsequent sections can be followed more easily; see also [\citet=Rowe:2003], whose notation is somewhat different from ours. Next, in section [\ref=subsec:marginalDataDistrib], we derive the marginal data distribution for a subset of variables (selected columns of [formula]) which represents the building block for computing the marginal likelihood of a general DAG model (as detailed in section [\ref=subsec:DAG]). We remark that, because of the theory presented in section [\ref=subsec:DAG], we need only consider an unconstrained [formula] even when we deal with covariance matrices having a graphical structure. This is indeed a major simplification characterizing the approach taken in this paper; we will return to this issue later on.

Conjugate analysis

If we denote by [formula] the least squares estimator of [formula], the likelihood function can be written as

[formula]

where [formula] is the matrix of residuals. Hence, a conjugate prior for [formula] is obtained by letting

[formula]

which results in the prior density

[formula]

where

[formula]

is the prior normalizing constant. The prior ([\ref=eq:priorDensity]) is a matrix normal Wishart.

Some algebraic manipulations show that the posterior distribution of [formula] is

[formula]

where [formula] is the posterior expectation (matrix) of [formula], and [formula] is a measure of discrepancy between [formula] and [formula] (prior and data). Prior-to-posterior updating thus takes the form

[formula]

and the posterior density [formula] is as in ([\ref=eq:priorDensity]) with hyper-parameters updated by ([\ref=eq:updatingHyperCoeff]); the posterior normalizing constant will be given by

[formula]

with the function K(  ·  ,  ·  ,  ·  ) defined in ([\ref=eq:priorNormalizConst]).

Marginal data distribution

The marginal distribution of the matrix [formula] can be obtained as

[formula]

which in light of conjugacy gives

[formula]

that is, up to a multiplicative factor, the ratio of the posterior and prior normalizing constants, ([\ref=eq:postNormalizConst]) and ([\ref=eq:priorNormalizConst]), respectively.

In the sequel, we will also need the marginal distribution of selected columns of the data matrix [formula], corresponding to a proper subset J of the full set of q response variables. Let [formula] be the n  ×  |J| selected data submatrix, and [formula] be the corresponding (p + 1)  ×  |J| submatrix of [formula], whose columns contain the regression coefficients for the selected responses. When restricted to the set J of response variables, by the results presented in section [\ref=sec:matrixDistrib], the Gaussian multivariate regression model ([\ref=eq:multivRegrModel]) can be written as

[formula]

with induced prior

[formula]

where J is the appropriate submatrix of [formula].

One readily sees that the formal structure of model and prior for a subset J of response variables is the same as for the full data matrix. As a consequence, the marginal data distribution for the submatrix [formula] is given by ([\ref=marginalDataDist]) with the following substitutions:

[formula]

while n, [formula] and [formula] remain unchanged.

Objective analysis

We assume the reader is familiar with the basic concepts of model selection from the Bayesian perspective, as described for instance in [\citet=Ohag:Fors:2004]. Here, in section [\ref=subsec:fractionalbf], we provide some background on objective Bayes model selection, focusing in particular on a proposal by [\citet=Ohag:1995]. Then, in section [\ref=subsec:fractionalMarLik], we give the expression for the marginal data distribution of a generic subset of columns of [formula] under the prior implied by such proposal; this will be instrumental in the construction of the marginal likelihood of a DAG model given in section [\ref=subsec:DAG].

Fractional parameter priors

Let [formula] be a collection of Bayesian models for the same observable [formula]. Each model Mk, [formula], consists of a family of sampling densities [formula], indexed by a model specific parameter [formula], and of a prior density [formula] on [formula], which we assume to be proper. We focus on the comparison of Mk with [formula] through the Bayes factor. The Bayes factor for Mk against [formula] is defined as [formula], where [formula] is the marginal density of [formula] under Mk, also known as the marginal likelihood of Mk.

In lack of substantive prior information, we would like to take [formula] for some objective default (non-informative) parameter prior [formula]. However, objective priors are often improper and they cannot be naively used to compute Bayes factors, even when the marginal likelihoods [formula] are finite and non-zero, because of the presence of arbitrary constants which do not cancel out in their ratios. [\cite=peri:2005] reviews several proposals put forward to address this issue. In this paper, we take advantage of the fractional Bayes factor originally introduced by [\cite=Ohag:1995]; see also [\citet=Ohag:Fors:2004].

Let b = b(n), 0 < b < 1, be a fraction of the number of observations n. Define

[formula]

where [formula] is the sampling density under model Mk raised to the b-th power, and the two integrals are assumed to be finite and non-zero. The fractional marginal likelihood ([\ref=fractionalMarginalLik]) of model Mk, can be rewritten as

[formula]

where [formula] is the implied fractional prior (actually a "posterior" based on the fractional likelihood and the default prior). The fractional Bayes factor for Mk against [formula] is then defined as the ratio of [formula] to [formula]. In essence, a fraction of the data is used to obtain a proper prior, which is then applied to the remaining fraction.

Clearly, the fractional prior depends on the choice of b. Usually b will be small, so that dependence of the prior on the data will be weak. Consistency is achieved as long as b  →  0 for n  →    ∞  . [\citet=Ohag:1995] suggests b = n0 / n as a default choice, where n0 is the minimal (integer) training sample size for which the fractional marginal likelihood is well defined, together with a couple of alternative choices, to be used when robustness is an issue. [\cite=More:1997] has an argument according to which the default choice is the only valid one, and we stick to this choice in this paper.

Fractional marginal likelihoods

Consider the Gaussian multivariate regression model ([\ref=eq:multivRegrModel]). We start from the prior

[formula]

[formula] s.p.d., which is flexible enough to accommodate different choices of default priors. In particular, aD = 0 gives [formula], equivalently [formula] for [formula], because the Jacobian of [formula] is proportional to [formula]. This is the "independence" Jeffreys prior, that is, the prior obtained by multiplying the Jeffreys priors for the two parameters assuming the other one is known; see [\citet=Pres:1982]. Alternatively, aD = q - 1 gives [formula], or [formula]. Both these priors are discussed in [\citet=Geis:Corn:1963], whereas [\citet=Geis:1965] focusses more deeply on the independence Jeffreys prior. [\citet=Sun:Berg:2007] present further objective priors for the multivariate normal model.

Using the default prior ([\ref=pDOmega]), and setting the fraction b equal to n0 / n, the fractional prior for the multivariate regression model ([\ref=eq:multivRegrModel]) is given by

[formula]

where [formula] and [formula]; this is clearly a matrix normal Wishart, having the form ([\ref=eq:priorDensity]) with

[formula]

The prior ([\ref=FBFPrior]) is proper under two conditions: i) aD + n0 - p > q, so that a > q - 1; ii) n - p - 1 > q - 1, so that [formula] is (almost surely) positive definite.

Condition ii), which simplifies to n > p + q, will not be met in our intended application setting, but we will be able to relax it in the context of sparse DAG models; see section [\ref=subsec:DAG]. Condition i) becomes n0 > p + q, if aD = 0, or n0 > p + 1, if aD = q - 1. Clearly, the fraction b = n0 / n must be larger when using the independence Jeffreys prior, rather than the prior presented in [\citet=Geis:Corn:1963], especially if q is much larger than 1. Since the fraction of the data to be used should be as small as possible, we recommend setting aD = q - 1 (and n0 = p + 2, so that a = q). Notice that, for b = n0 / n to be small, with n0 > p + 1, we need p <  < n, which is a stronger requirement than assuming n > p + 1 as in section [\ref=sec:multivariateRegression]. However, as anticipated in section 3, and illustrated in the Discussion, this requirement will be typically satisfied in our intended application setting.

Posterior updating of the hyper-parameters leads to

[formula]

keeping into account that the fractional prior is to be used on the likelihood raised to the (1 - b)-th power, that is, on data with the same [formula], [formula] and [formula], but with n - n0 in place of n. Consequently, using ([\ref=marginalDataDist]), one gets

[formula]

which after some simplifications leads to

[formula]

In order to apply the method presented in section [\ref=sec:covsel] one also needs the fractional marginal likelihood based on the submatrix [formula] which only contains the columns of [formula] belonging to the subset J, which we write as [formula]. This marginal likelihood is germane to our approach, and represents a half-way house towards computing the entire fractional marginal likelihood for a DAG model; see section [\ref=subsec:DAG]. Based on the results presented in section [\ref=subsec:marginalDataDistrib], it is immediate to conclude that [formula] can be obtained from equation ([\ref=marginalDataDistFBF]) upon making the substitutions

[formula]

These substitutions lead to

[formula]

which returns ([\ref=marginalDataDistFBF]) upon setting [formula].

Formula ([\ref=marginalDataDistFBFSubsetMatrix]) derives from [formula] with aJ = aD + n0 - p - 1 - ||, which is (almost surely) proper if n > p + |J|. The latter condition guarantees positive definiteness of [formula], while aJ = q - || = |J| using our recommended choices for aD and n0. Therefore, formula ([\ref=marginalDataDistFBFSubsetMatrix]) provides us with a valid value for [formula], whenever |J| < n - p, even if n  ≤  p + q. We will exploit this fact in section [\ref=subsec:DAG] to derive the marginal likelihood of a sparse DAG. In the next paragraph we specialize ([\ref=marginalDataDistFBFSubsetMatrix]) to the simplest regression setup, which is of some interest in its own right.

If the sampling distribution corresponds to i.i.d. observations from a q-dimensional Gaussian density with expectation [formula] and precision [formula], conditionally on [formula] and [formula], the corresponding marginal data distribution [formula] can be derived from ([\ref=marginalDataDistFBFSubsetMatrix]) upon setting p = 0 (no predictors) and [formula], where [formula] is the q-dimensional vector of sample means. In this way we obtain

[formula]

with [formula]. Expression ([\ref=marginalDataDistFBFsubsetIID]) complements formula (22) in [\citet=Cons:Laro:2012], which holds for i.i.d. q-dimensional Gaussian observations with zero expectation.

Covariance selection

So far we have analyzed the Gaussian multivariate regression model ([\ref=eq:multivRegrModel]) under the condition that [formula] is unconstrained. We now assume instead that [formula] is constrained by a DAG, aiming at graphical model (or covariance) selection after having adjusted for the presence of covariates. In section [\ref=subsec:DAG], we develop an extension of the approach by [\citet=Geig:Heck:2002] explicitly for the regression setup. An advantage of the method we present is that the computation of the marginal likelihood for each DAG only requires the results established, for an unconstrained [formula], in section [\ref=subsec:fractionalMarLik]. In section [\ref=subsec:UG_decomposable], taking advantage of the fact that any two Markov equivalent DAGs obtain the same marginal likelihood, we specify our results to the case of Gaussian decomposable graphical models, and relate them to those obtained by [\citet=Carv:Scot:2009] in the i.i.d. case.

Acyclic directed error structure

Let D be a DAG with vertex set [formula]. Denote by [formula] the parents of j in D, that is, the set of all vertices in D from which an edge points to vertex j, and by [formula] the subvector of [formula] indexed by [formula]. The multivariate normal sampling density of [formula], assumed to be Markov with respect to D, can be written as

[formula]

where [formula] is defined by

[formula]

and [formula] is the collection of all [formula]s; recall that [formula] is the i-th row of the design matrix [formula], and notice that we drop dependence on D when we move from [formula] to its components (to lighten notation). We illustrate below the reparameterization from [formula], with [formula] s.p.d., to [formula], with λj > 0, [formula], after a remark on ([formula]).

The conditional vertex density [formula] is a univariate normal density with expectation and variance given by ([\ref=cond:mean]) and ([\ref=cond:var]), respectively. It is important to remark that such density depends on D only through [formula]. In other words, if two DAGs D1 and D2 are such that [formula], then the vertex-specific parameter [formula] varies in the same space under D1 and D2, because [formula] has the same dimension under the two DAGs, and [formula]. This property, called likelihood modularity by [\cite=Geig:Heck:2002], represents a condition to be satisfied for the subsequent theory to apply.

Assume (without loss of generality) that the vertices of D are well-numbered; this means that, if [formula] is a parent of j, then [formula]. If D is complete, that is, it has all pairs of vertices joined by an edge, then the parameters indexing the last (j = q) conditional vertex density in ([\ref=jointDensity]) are: [formula], [formula], and λq  =  Ωqq, where [formula]; see the end of section [\ref=subsec:matrixNormal]. Then, since [formula], one can repeat the previous argument and recursively find [formula]. If D is incomplete, its missing edges will impose on [formula] the constraints [formula], [formula], [formula], so that a corresponding set of constraints will be imposed on [formula].

We now show that, for complete DAGs, the transformation [formula] is a smooth bijection. This fact, which is arguably not new, is reported here because it will be used below for constructing priors under general DAGs. Given the recursive definition of [formula], it is enough to show that the transformation from [formula], with [formula] s.p.d., to [formula], with [formula] s.p.d. and λq > 0, is a smooth bijection. We do this by composing a few simpler reparameterizations. First, we go from [formula], with [formula] s.p.d., to [formula], with [formula] s.p.d. and Ωqq > 0, where the smooth inverse map is provided by [formula], recalling that [formula] (unconstrained); see for instance [\citet=Laur:1996]. Then, we trivially split [formula] as [formula], and replace [formula] with [formula], where the smooth inverse map is given by [formula]. Finally, we reparameterize from [formula] to [formula], with smooth inverse map given by [formula], and we rename Ωqq as λq (constrained to be positive).

In light of the above discussion, all complete DAGs define the same statistical model, in which [formula] is unconstrained, and there is a smooth bijection between their collections of parameters; in the terminology of [\citet=Geig:Heck:2002] we have complete model equivalence, and regularity. It follows that any prior on [formula] will induce a prior on [formula], if D is complete. We now show that, if we let [formula] follow the conjugate prior ([\ref=eq:priorDensity]), then [formula], so that [formula] will be a priori independent. This property is called global parameter independence, and represents a crucial ingredient in the approach of [\citet=Geig:Heck:2002]; it can be obtained by recursive application of the following result.

If [formula] and [formula], then the pair [formula] is independent of the triple [formula].

Consider the reparameterization in terms of [formula] s.p.d., [formula], Ωqq > 0, [formula], [formula], and factorize the corresponding joint parameter density as

[formula]

We know, from our statement following ([\ref=eq:distrOmegaMar]), that [formula] is independent of [formula] under the assumed distribution for [formula]. Moreover, from the law of [formula], we obtain

[formula]

first using column marginalization ([\ref=eq:marginalization]), and ([\ref=Sigmavv]), then using column conditioning ([\ref=eq:conditioning]). Therefore, the joint density of [formula], [formula], Ωqq, [formula], and [formula], factorizes as

[formula]

which implies the desired result.

If D is incomplete, global parameter independence can be guaranteed by letting [formula], where Cj is any complete DAG such that [formula]. The actual choice of each Cj is immaterial, because all [formula], [formula], must follow j in Cj, and thus [formula] is induced by the law of [formula], where [formula] is the family of j in D. Notice that j goes necessarily last in [formula], and recall that [formula], by column marginalization, while [formula], as per ([\ref=eq:distrOmegaMar]). Assigning parameter priors in this way, we also guarantee prior modularity: [formula], if [formula]. This is the last ingredient required by the method of [\citet=Geig:Heck:2002] to compute the marginal likelihood of any DAG model, based on the assignment of the single prior ([\ref=eq:priorDensity]). We now detail the computations for our regression setting.

The marginal density of the matrix [formula] under the DAG D, equivalently the marginal likelihood of D observing [formula], can be found as [formula], where [formula] with [formula] given by ([\ref=jointDensity]), and furthermore [formula] by global parameter independence. We can thus write

[formula]

where the second equality is based on prior and likelihood modularity. It follows that

[formula]

recalling that [formula], by construction, and mCj(  ·  ) is nothing else but m(  ·  ) under our prior ([\ref=eq:priorDensity]), by complete model equivalence and regularity.

The great advantage of ([\ref=formula18OfGHnew]) is that the computations of the required terms in the rightmost product can be done under the assumption that the precision matrix [formula] is unconstrained, and thus one can use the standard matrix normal Wishart prior ([\ref=eq:priorDensity]). Notice that the DAG D enters ([\ref=formula18OfGHnew]) only through the specification of the set of parents, [formula], for each vertex j. The expressions for [formula] and [formula] are available in section [\ref=subsec:marginalDataDistrib], upon replacing J with [formula] and [formula], respectively.

Prior ([\ref=eq:priorDensity]) requires to specify the hyper-parameters [formula], [formula], a, and [formula]. This can be problematic, especially when the dimension of the problem is large, and we know that marginal likelihoods are quite sensitive to changes in the hyper-parameters; see [\citet=Ohag:Fors:2004]. We therefore suggest an objective choice, based on the fractional matrix normal Wishart prior ([\ref=FBFPrior]) applied to the Gaussian likelihood ([\ref=eq:lik]) with (n - n0) observations and the same [formula], [formula] and [formula] as the data. With this choice, the terms [formula] and [formula] in formula ([\ref=formula18OfGHnew]) can be computed from ([\ref=marginalDataDistFBFSubsetMatrix]) provided that the condition [formula] is satisfied. This condition guarantees a valid value for [formula] by granting a proper distribution to the marginal precision matrix [formula]; see section [\ref=subsec:fractionalMarLik]. In this way, formula ([\ref=formula18OfGHnew]) provides us with a valid marginal likelihood (product of q valid conditional marginal likelihoods given parent observations) for every DAG D whose parent sets have size smaller than the number of observations minus the number of columns in the design matrix [formula] (number of predictors in the model plus one). The latter is a sparsity condition on the structure of the DAG, involving the maximal number of parents across vertices, which is quite reasonable in our intended application setting (eQTL analysis) as discussed in the Introduction.

Decomposable error structure

It is often appropriate to model the conditional independence structure of a set of variables in terms of an undirected graph; see [\citet=Laur:1996] for an authoritative exposition. This is for instance the approach followed in [\citet=Caietal:2013] and [\citet=Chenetal:2013] for the analysis of genetical genomics data. With reference to the Gaussian multivariate regression model ([\ref=eq:multivRegrModel]), this means that the precision matrix [formula] of the response vector [formula] is constrained by an undirected graph G: if an edge is missing between j and [formula] in G, then [formula]. Equivalently, [formula] is Markov with respect to G, that is, if j and [formula] are not joined by an edge in G, the responses yij and [formula] are conditionally independent, under the sampling distribution, given all remaining responses; in symbols [formula] [\citep=Drto:Perl:2004].

To enhance tractability, the undirected graph G is often assumed to satisfy some conditions, such as decomposability; see for instance [\citet=Bhad:Mall:2013]. It is well known that a decomposable G is Markov equivalent to some DAG [\citep=Ande:Madi:Perl:1997]. Specifically, one can always well-number the vertices of G and construct a directed version G<, which is a DAG Markov equivalent to G; see [\citet=Laur:1996]. It follows that the methodology developed in section [\ref=subsec:DAG] can also be applied to decomposable graphs, because the marginal likelihoods given by such methodology are invariant with respect to Markov equivalence. Indeed, the proof of Theorem 4 in [\cite=Geig:Heck:2002] directly carries over into our regression setting.

In practice, the marginal likelihood of the model defined by the decomposable graph G, [formula], will be given by ([\ref=formula18OfGHnew]) with D  =  G<. Since the parameter prior used to compute ([\ref=formula18OfGHnew]) satisfies global parameter independence, [formula] is readily seen to be G<-Markov; see for instance [\citet=Cowe:Dawi:Laur:Spie:1999]. Then [formula] is also G-Markov, and thus it admits the representation

[formula]

where C is the set of cliques, and S the set of separators, of the decomposable graph G; see [\citet=Laur:1996]. The explicit expression of each factor appearing in ([\ref=factorizationUG]) can be deduced from ([\ref=marginalDataDist]) as explained in section [\ref=subsec:marginalDataDistrib].

In particular, when using the fractional matrix normal Wishart prior ([\ref=FBFPrior]), one computes [formula] and [formula] in ([\ref=factorizationUG]) by means of ([\ref=marginalDataDistFBFSubsetMatrix]), with J = C and J = S, respectively, assuming |C| < n - p (hence |S| < n - p) whenever C is a clique (S  ⊆  C a separator) of G. In this way, we cope with decomposable graphs whose clique sizes are smaller than the number of observations minus the number of predictors in the model. This is again a sparsity assumption on the graph, well-suited to our intended application setting, which grants a proper distribution to [formula] (hence to [formula]); see section [\ref=subsec:fractionalMarLik]. We remark that formulae ([\ref=factorizationUG]) and ([\ref=marginalDataDistFBFSubsetMatrix]) generalize to the multivariate regression setup the results established by [\citet=Carv:Scot:2009] for i.i.d. Gaussian observations with zero expectation. As a special case, formulae ([\ref=factorizationUG]) and ([\ref=marginalDataDistFBFSubsetMatrix]) also cover the i.i.d. Gaussian setup with unknown expectation.

Discussion

Motivated by covariate-adjusted covariance selection under sparsity, this paper proposes an objective Bayes method for computing the marginal likelihood of a multivariate regression model with normally distributed errors whose covariance matrix is constrained by a DAG. This represents an essential ingredient to obtain a posterior probability over the space of covariate-adjusted DAG models. Since the proposed method is invariant with respect to Markov equivalence, it can also be used to select covariate-adjusted decomposable models. Although we do not explicitly address variable selection, our results for the marginal likelihood can be used for Bayesian joint variable and covariance selection, as discussed in [\citet=Bhad:Mall:2013].

In practice, as we remark at the beginning of section [\ref=sec:multivariateRegression], variable selection is needed to apply our method whenever the total number of predictors [formula] is comparable to, or larger than, the number of observations; this is a typical scenario in genetical genomics applications. Restricting our attention to models including only p  ≪  n predictors, so that our objective analysis becomes feasible, turns out to be adequate for settings where sparse models are of interest. For instance, the two simulations considered by [\citet=Bhad:Mall:2013] have: i) [formula], q = 300, and n = 120, with p = 11 for the actual data generating distribution; ii) [formula], q = 100, and n = 120, with p = 3 for the actual data generating distribution. Similarly, their real data analysis (eQTL Analysis on Publicly Available Human Data) has [formula], q = 100, and n = 60, with p = 1 or p = 2 identified as the most likely values.

[\citet=Bhad:Mall:2013] currently derive their results for decomposable models under a weakly informative prior which requires to subjectively specify three scalar hyper-parameters. In particular, they use a hyper-inverse Wishart on [formula] with scale parameter equal to the identity matrix multiplied by a constant. The latter proves to be crucial and need be fixed with care, because it acts as a global shrinkage parameter. Our objective prior, with its simple method for obtaining the marginal likelihood, should provide a useful alternative to their prior specification. On the other hand, our methodology for computing the marginal likelihood can also be implemented starting from a single subjectively specified matrix normal Wishart prior under any complete DAG model, then applying the general results of section [\ref=subsec:marginalDataDistrib] in the context of DAG models as described in section [\ref=subsec:DAG]. In this case, the sparsity conditions relating the sample size n, the number of predictors p and the maximal size of the cliques, which we had to impose to make our objective Bayes analysis possible, could be relaxed.

Finally, our method does not cope with non-decomposable undirected graphical models. Bayesian covariate-adjusted covariance selection in general undirected graphical models is beyond the scope of this paper, and will present the obvious challenge of providing an efficient method for computing the marginal likelihood; see [\citet=Carvetal:2007], [\citet=Wang:Carv:2010], [\citet=Lenk:2013]. However, working within the class of decomposable graphs can still be very effective, even when the true graph is not decomposable; see [\citet=Fitchetal:2014] for asymptotic results on the posterior model probabilities, and for a high performing stochastic search of the model space.

Acknowledgements

Work partially supported by a D1-grant from Università Cattolica del Sacro Cuore. The authors are grateful to Alberto Roverato for pointing out a useful reference.