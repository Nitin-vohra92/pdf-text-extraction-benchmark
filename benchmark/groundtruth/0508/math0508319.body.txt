Example

Combinations and Mixtures of Optimal Policies in Unichain Markov Decision Processes are Optimal

Introduction

A Markov decision process (MDP) M on a (finite) set of states S with a (finite) set of actions A available in each state ∈S consists of

an initial distribution μ0 that specifies the probability of starting in some state in S,

the transition probabilities pa(i,j) that specify the probability of reaching state j when choosing action a in state i, and

the payoff distributions with mean ra(i) that specify the random reward for choosing action a in state i.

A (stationary) policy on M is a mapping π:S  →  A.

Note that each policy π induces a Markov chain on M. We are interested in MDPs, where in each of the induced Markov chains any state is reachable from any other state.

An MDP M is called unichain, if for each policy π the Markov chain induced by π is ergodic, i.e. if the matrix P = (pπ(i)(i,j))i,j∈S is irreducible.

It is a well-known fact (cf. e.g. [\cite=keme], p.130ff) that for an ergodic Markov chain with transition matrix P there exists a unique invariant and strictly positive distribution μ, such that independent of the initial distribution μ0 one has μn  =  nμ0  →  μ, where [formula]. Thus, given a policy π on a unichain MDP that induces a Markov chain with invariant distribution μ, the average reward of that policy can be defined as

[formula]

A policy [formula] is called optimal if for all policies π: [formula] It can be shown ([\cite=pute], p.360ff) that in the unichain case the optimal value [formula] cannot be increased by allowing time-dependent policies, as there is always a stationary (time-independent) policy that gains optimal average reward, which is why we consider only stationary policies.

In this setting we are going to prove that combinations of optimal policies are optimal as well.

Let M be a unichain MDP with state space S and [formula] optimal policies on M. Then any combination π of these policies where for each state i∈S either [formula] or [formula] is optimal as well.

Obviously, if two combined optimal policies are optimal, so are combinations of an arbitrary number of optimal policies. Thus, one immediately obtains that the set of optimal policies is closed under combination.

Let M be a unichain MDP with state space S. A policy π is optimal on M if and only if for each state i there is an optimal policy [formula] with [formula].

Proof of Theorem [\ref=theorem]

We start with a result about the distributions of policies that differ in at most two states.

Let M be a unichain MDP with state space S. Let π00,π01,π10,π11 be four policies on M with invariant distributions μ00 = (ai)i∈S, μ01 = (bi)i∈S, μ10 = (ci)i∈S, μ11 = (di)i∈S and s1,s2 two states in S such that

for all [formula]: π00(i) = π01(i) = π10(i) = π11(i),

π00(s1) = π01(s1)  ≠  π10(s1) = π11(s1),

π00(s2) = π10(s2)  ≠  π01(s2) = π11(s2).

Then each of the distributions μij is uniquely determined by the other three. More precisely, e.g. for all states i

[formula]

Since M is unichain, the distributions μij are all uniquely determined by the transition matrices Pij of the Markov chains induced by the policies πij. By assumption (i), the matrices Pij share all rows except rows s1,s2, which we may assume to be the first and second row, respectively. Furthermore, by (ii), P00 and P01 share the first row as well as P10 and P11. Finally, by (iii) we have equal second rows in P00 and P10 as well as in P01 and P11.

Since by assumption the distributions are invariant, we have μijPij  =  μij. Writing the probabilities in P00 as pij and those in the first two rows of P11 as qij, it follows that for each state i:

[formula]

Setting ν = (νi)i∈S with νi: = a2b1ci - aib1c2 + a1bic2, one has by ([\ref=eq1])

[formula]

Hence, normalizing ν one has an invariant distribution of P11, which by assumption is unique and consequently identical to μ11.

With this information on the distributions, we are able to tell something about the average rewards of the policies as well.

Let π00,π01,π10,π11 and s1,s2 be as in Lemma [\ref=lem:distr] and denote the average rewards of the policies by V00,V01,V10,V11. Let a,b∈{0,1} and set [formula]. Then it cannot be the case that [formula] and [formula]. Analogously, it cannot hold that [formula] and [formula].

For the sake of readability, we prove the case a = b = 0. The other cases follow by symmetry. Actually, we will show that if V00 > V01,V10, then V00 > V11. Since one has analogously the implication that if V11  ≥  V01,V10, then V11  ≥  V00, the assumptions V00 > V01,V10 and V11  ≥  V01,V10 obviously lead to a contradiction.

Similarly as in the case of transition probabilities, in the following we write for the rewards of the policy π00 simply ri instead of rπij(i)(i). For the deviating rewards in state s1 under policies π10,π11 and state s2 under π01,π11 we write r'1 and r'2, respectively. Then we have

[formula]

If we now assume that V00 > V01,V10, the first three equations yield

[formula]

while applying Lemma [\ref=lem:distr] to the fourth equation gives

[formula]

where α = a2b1 - b1c2 + a1c2. Substituting according to ([\ref=eq2]) and ([\ref=eq3]) then yields

[formula]

Obviously, replacing '>  ' with '≥  ', '<  ' or '≤  ' throughout the proof yields the analogous result for the other cases, which finishes the proof.

The following is a collection of simple consequences of Lemma [\ref=lem:comp].

Let V00,V01,V10,V11 and a,b be as in Lemma [\ref=lem:comp]. Then the following implications hold:

[formula].

[formula].

[formula].

[formula].

[formula].

[formula].

(i), (ii), (iii), (iv) are mere reformulations of Lemma [\ref=lem:comp], while (vi) is an easy consequence. Thus let us consider (v). If [formula] were [formula], then by Lemma [\ref=lem:comp] [formula], contradicting our assumption. Since a similar contradiction crops up if we assume that [formula], it follows that [formula].

Now, in order to prove the theorem, we ignore all states where the optimal policies [formula] coincide. For the remaining s states we denote the actions of [formula] by 0 and those of [formula] by 1. Thus any combination of [formula] can be expressed as a sequence of s elements ∈{0,1}, where we assume an arbitrary order on the set of states (take e.g.  the one used in the matrices Pij). We now define sets of policies or sequences, respectively, as follows: First, let Θi be the set of policies with exactly i occurrences of 1. Then set [formula], and for 1  ≤  i  ≤  s

[formula]

where d denotes the Hamming distance, and π*i is a (fixed) policy in Πi with V(π*i) =  max π∈ΠiV(π). Thus, a policy is ∈Πi, if and only if it can be obtained from π*i - 1 by replacing a 0 with a 1.

V(π*i - 1)  ≥  V(π*i) for 1  ≤  i  ≤  s.

The lemma obviously holds for i = 1, since [formula] is by assumption optimal. Proceeding by induction, let i > 1 and assume that V(π*i - 2)  ≥  V(π*i - 1). By construction of the elements in each Πj the policies π*i - 2, π*i - 1 and π*i differ in at most two states, i.e. the situation is as follows:

[formula]

Define a policy π'∈Πi - 1 as indicated above. Then V(π*i - 2)  ≥  V(π*i - 1)  ≥  V(π') by induction assumption and optimality of π*i - 1 in Πi - 1. Applying (iv) of Corollary [\ref=cor:comp] yields that V(π*i)  ≤   max (V(π*i - 1),V(π')) = V(π*i - 1), which proves the lemma.

Since the policies [formula] and [formula] are assumed to be optimal, it follows that all policies π*i are optimal as well. Now we are able to prove the Theorem by induction on the number of states s where the policies [formula] differ. For s = 1 it is trivial, while for s = 2 there are two combinations of [formula] and [formula]. One of them is identical to π*1 and hence optimal, while the other one is optimal due to Corollary [\ref=cor:comp] (v).

Thus, let us assume that s > 2. Then we have already shown that the policies π*i and hence in particular [formula] and [formula] are optimal. Since π*1 and [formula] are optimal policies that share a common digit in position k, we may conclude by induction assumption that all policies with a 1 in position k are optimal. A similar argument applied to the policies [formula] and π*s - 1 shows that all policies with a 0 in position [formula] (the position of the 0 in π*s - 1) are optimal. Note that by construction of the sets Πi, [formula]. Thus, we have shown that all considered policies are optimal, except those with a 1 in position [formula] and a 0 in position k. However, as all policies of the form

[formula]

are optimal, a final application of Corollary [\ref=cor:comp] (v) shows that these are optimal as well.

Mixing Optimal Policies

Theorem [\ref=theorem] can be extended to mixing optimal policies, that is, our policies are not deterministic (pure) anymore, but in each state we choose an action randomly. Building up on Theorem [\ref=theorem] we can show that any mixture of optimal policies is optimal as well.

Let Π* be a set of pure optimal policies on a unichain MDP M. Then any policy that chooses at each visit in each state i randomly an action a such that there is a policy π∈Π* with a = π(i), is optimal.

The theorem will be obtained with the help of the following Lemma.

Let π1,π2 be two policies on a unichain MDP M that differ only in a single state s1, i.e.  π1(i) = π2(i) for all i  ≠  s1 and π1(s1)  ≠  π2(s1). Let μ1 = (ai)i∈S, μ2 = (bi)i∈S be the invariant distributions and V1,V2 the average rewards of π1 and π2, respectively. Then the mixed policy π that chooses in s1 action π1(s1) with probability λ and π2(s1) with probability (1 - λ) and coincides with πi in all other states has invariant distribution μ = (ci)i∈S with

[formula]

and average reward

[formula]

First, note that the transition matrices P1,P2 of π1,π2 and P of π share all rows except row s1, which we assume to be the first row. Furthermore we write pij for pπ1(i)(i,j) and q1j for pπ2(s1)(s1,i), so that the entries of row 1 in P are of the form λp1j  +  (1 - λ)q1j. Now, let ν: = (νi)i∈S with νi  =  λaib1  +  (1 - λ)a1bi. Then

[formula]

since the ai's and bi's form an invariant distribution of P1,P2, respectively. Since the ci's are only a normalized version of the νi's, this finishes the first part of the proof.

Now, given the invariant distribution of π, its average reward can be written as [formula]. Thus, writing ri for rπ1(i) and r1' for rπ2(s1) one has

[formula]

Let us first assume the simplest case, where we have two optimal policies [formula] that differ only in some state single s1. By Lemma [\ref=lem:mix], the policy resulting from [formula] and [formula] when mixing actions in state s1 has the same average reward as [formula] and [formula] and therefore is optimal. Now, each mixture of actions in a single state can be interpreted as a new action in this state. Thus, proceeding by induction, the mixture of n optimal policies [formula] that differ only in a single state is optimal as well.

Now, in the general case, where we want to mix actions in s > 1 states, we have at each state i the actions (of some pure optimal policies ∈Π*) [formula] at our disposal. By Theorem [\ref=theorem] all combinations [formula] with 1  ≤  ji  ≤  ki for all i are optimal as well. Thus, we may fix the actions in s - 1 states so that we have e.g. optimal policies of the form [formula] with 1  ≤  j  ≤  k1. As we have seen above, all policies that are obtained by mixing all available actions in the first state are optimal. Furthermore, each mixture can again be interpreted as new available action, so that we may repeat our argument for each of the remaining states, thus showing that each mixed optimal policy is optimal, too.

So far, we have considered only the case where the relative frequencies with which the actions in a fixed state are chosen converge. If this does not hold, it may happen that the process does not converge to an invariant distribution. However, the average rewards after t steps converge nevertheless. Let λti(a) be the relative frequency with which action a was chosen in state i after t steps in i, and let μt be the distribution over the states after these t steps. Then the average reward Vt thereby obtained is [formula]. This is of course also the expected average reward after t steps when constantly choosing action a in state i with probability λi(a): = λti(a) for each i,a. As each of these sequences has already been shown to converge to the optimal value V*, we have the following situation. For each Vt1 of the sequence [formula] there is a sequence [formula] with lim t  →    ∞Vt(π) = V* such that Vt1(π) = Vt1. It follows that lim t  →    ∞Vt = V*.

Extensions, Applications and Remarks

Optimality is Necessary

Given some policies with equal average reward V, in general, it is not the case that a combination of these policies again has average reward V, as the following example shows. Thus, optimality is a necessary condition in Theorem [\ref=theorem].

Let S = {s1,s2} and A = {a1,a2}. The transition probabilities are given by

[formula]

while the rewards are ra1(s1) = ra1(s2) = 0 and ra2(s1) = ra2(s2) = 1. Since the transition probabilities of all policies are identical, policy (a2,a2) with an average reward of 1 is obviously optimal. Policy (a2,a2) can be obtained as a combination of the policies (a1,a2), and (a2,a1), which however only yield an average reward of [formula].

Multichain and Infinite MDPs

Theorem [\ref=theorem] does not hold for MDPs that are not unichain as the following simple example demonstrates.

Let S = {s1,s2} and A = {a1,a2}. The transition probabilities are given by

[formula]

while the rewards are ra1(s1) = ra1(s2) = 1 and ra2(s1) = ra2(s2) = 0. Then the policies (a1,a1),(a1,a2),(a2,a1) all gain an average reward of 1 and are optimal, while the combined policy yields suboptimal average reward 0.

Even though this seems to be quite a strict counterexample (note that the MDP is even communicating), we think that in certain restricted settings Theorems [\ref=theorem] and [\ref=thm2] will hold as well. For example, adding a set of states that are transient under every policy does not matter. Furthermore, if the components of a multichain MDP are the same under every policy, it is obvious that the Theorems hold as well. However, things become more complicated, if the set of transient states or the components change with the policy as in Example [\ref=ex]. Nevertheless, extensions of our results to the multichain case don't seem to be impossible as such, but may work under some clever restrictions, e.g. by combining exclusively in states that are not transient under any policy. In any case the main task when working on such extensions will probably be to determine what policy changes will result in what changes in the set of transient states and components, respectively.

The situation for MDPs with countable set of states/actions is similar. Under the (strong) assumption that there exists a unique invariant and positive distribution for each policy, Theorems [\ref=theorem] and [\ref=thm2] also hold for these MDPs. In this case the proofs are identical to the case of finite MDPs (with the only difference that the induction becomes transfinite). However, in general, countable MDPs are much harder to handle as optimal policies need not be stationary anymore (cf. [\cite=pute], p.413f).

An Application

Even though the presented results may seem more of theoretical interest, there is a straightforward application of Theorem [\ref=thm2], which actually was the starting point of this paper. Consider an algorithm operating on an MDP that every now and then recalculates the optimal policy according to its estimates of the transition probabilities and the rewards, respectively. Sooner or later the estimates are good enough so that the calculated policy is indeed an optimal one. However, if there is more than one optimal policy, it may happen that the algorithm does not stick to a single optimal policy but starts mixing optimal policies irregularly. Theorem [\ref=thm2] guarantees that the average reward of such a process again is still optimal.

Conclusion

We conclude with a more philosophical remark. MDPs are usually presented as a standard example for decision processes with delayed feedback. That is, an optimal policy often has to accept locally small rewards in present states in order to gain large rewards later in future states. One may think that this induces some sort of context in which actions are optimal, e.g. that choosing a locally suboptimal action only "makes sense" in the context of heading to the higher reward states. Our results however show that this is not the case and optimal actions are rather optimal in any context.