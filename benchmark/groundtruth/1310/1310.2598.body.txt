Statistical mechanics of inference

One is sometimes confronted with the challenge of estimating probabilities from partial information. For example, given a stochastic system that transitions between a very large number of distinct states, the sampling time required to directly obtain a statistically significant estimate - through binning, say - to the occupation probability of some particular state may be prohibitively long. This is often the case in neuroscience experiments, because the number of distinct states that a neural network can access grows exponentially with network size [\cite=Shl-06] [\cite=Sch-06] [\cite=coc-09]. Although rigorous distribution identification is not possible in such situations, inference strategies that intelligently make use of available data can provide good estimates [\cite=Pre-13]. Here, I consider probabilistic inference in the uniform ensemble, where all distributions consistent with a given set of constraints are supposed equally likely. Using methods of statistical mechanics, I obtain a simple approximation to the centroid of the solution set, defined by equations ([\ref=saddle_conds])-([\ref=centroid_est]), below. This, in turn, leads to useful results characterizing the full solution set's geometry, and it also allows for comparison to the maximum entropy solution. I find that the centroid is sometimes expected to be substantially more accurate.

I consider here the following general scenario: It is given that a desired, underlying distribution [formula] on N states, with p*i∈[0,1] the probability of state i, satisfies a set of C  ≪  N linear constraints of the form

[formula]

with the real-valued [formula] specified. The distribution is normalized to

[formula]

I refer to the set S of distributions [formula] satisfying ([\ref=constraints]) and ([\ref=normalization]) as the solution set. We are to select from S one distribution that is optimal: Here, I consider the case where the selected distribution is supposed to be a good approximation to the unknown *. I take as a measure of error in estimate [formula] the quantity

[formula]

the squared distance between the underlying and the estimated distributions. I stress that ([\ref=error]) is not necessarily the only appropriate measure of error in an inference problem. However, it does represent an objective measure that is both familiar and useful to consider.

In certain situations, it may be appropriate to consider certain members of S more likely to be * than others. For example, if we know that p* was generated by a process more likely to generate sparse distributions, sparse members of S should be weighted more heavily [\cite=alb-12]. However, in the absence of such information, an axiom of equal probability is appropriate: Every state consistent with ([\ref=constraints]) and ([\ref=normalization]) should be considered equally likely to be the underlying distribution . I work under this axiom here. In this case, the expected error in an estimate [formula] is obtained by averaging ([\ref=error]) over *,

[formula]

The solution c that minimizes the expected error is obtained by setting the derivative of ([\ref=av_error]), with respect to pi, to zero. This gives,

[formula]

the centroid of the solution set . This represents the formal solution to a particular, well-defined inference problem. Namely, this returns the distribution in S minimizing ([\ref=av_error]). Unfortunately, a simple, general formula for c does not exist [\cite=rad-07]. However, in the following, I obtain an estimate to c that is easy to evaluate. Comparison to this c estimate then provides a simple method for testing the expected performance of other solutions: For [formula] close to c, the expected error ([\ref=av_error]) is nearly minimized. On the other hand, the expected error ([\ref=av_error]) is relatively large for [formula] far from c.

In order to characterize the solution set, I consider the configuration partition sum associated with a free particle, with position [formula], moving through S. This is

[formula]

where I have generalized slightly the constraint equations ([\ref=constraints]) and ([\ref=normalization]), now requiring the sum over probabilities to be equal to s and the dot product of [formula] along j to be tj. As defined, Z is simply equal to the volume of the solution set S. The Laplace transform of Z is

[formula]

Here, I have used ([\ref=partition]) to obtain the second line. The integrals over the {pi} are now decoupled, and [formula] can be evaluated in closed form as

[formula]

The solution set volume is given formally by the inverse Laplace transform of this quantity,

[formula]

where the indicated contours are parallel to the imaginary axis [\cite=Jon-43].

In order to evaluate the integral ([\ref=partition3]), I now assume that N, the number of accessible states, or components of [formula], is large. In this case, the integrand in ([\ref=partition3]) will be highly peaked, and an asymptotic series for log Z can be obtained, the first term being the saddle point value [\cite=Jon-43]. Setting s and the {tj} to their common, physical value, one, we have

[formula]

where the saddle point m* and {λ*j} values are those that leave the derivative of log Z stationary. That is, they satisfy the following equations, obtained by setting the derivatives of the exponent in ([\ref=saddle1]), with respect to m and the {λj}, individually to zero:

[formula]

We can solve directly for one unknown: Summing over the second line above, multiplied by λ*j, and adding to this m* times the first line, gives

[formula]

a simple relationship. The remaining C unknowns, the [formula], must be solved for using ([\ref=constraints]), or, equivalently, the latter conditions of ([\ref=saddle_conds]).

Notice that if we define

[formula]

the saddle point conditions ([\ref=saddle_conds]) imply that the distribution c,1 satisfies both ([\ref=constraints]) and ([\ref=normalization]). In fact, c,1 is the first-order, saddle point estimate to the centroid of S. This is most easily proven by introducing a field hi in ([\ref=partition]) coupled to pi. Following steps similar to those shown above, this gives

[formula]

From the first line above, we obtain

[formula]

an exact identity. Applying the saddle point approximation to ([\ref=partition4]) gives the analog of ([\ref=saddle1]), with the hi field included. Plugging in to ([\ref=gen_cent]) then gives c  ~  c,1, the value in ([\ref=centroid_est]). More accurate estimates are obtained through expansion about the saddle point. For example, writing m  =  m*  +  δm and λj  =  λ*j  +  δλj, evaluation of the Gaussian fluctuations about the saddle point gives

[formula]

where M is the (C + 1)  ×  (C + 1) matrix with components

[formula]

Here, I have written [formula] and [formula], in order to briefly simplify notation. Combining ([\ref=gen_cent]) and ([\ref=centroid_est_2]) gives the second order centroid estimate c,2, a refinement to c,1. In order to carry out the implied variation of ([\ref=centroid_est_2]) with respect to hi here, the field dependences of the [formula] are needed within M . Differentiating the saddle point equations, [formula], gives the matrix equation

[formula]

which can be inverted to solve for the necessary derivatives. Carrying out this procedure is useful for small N. However, for [formula], c,1 already provides an accurate approximation to c.

Once c,1 has been evaluated, one can immediately characterize, approximately, the solution set's geometry. For example, from ([\ref=partition4]), the variance of pi is

[formula]

Plugging in the saddle point estimate for Z gives

[formula]

That is, the width of the solution set in the i direction is approximately equal to pci, the [formula] component of the solution set's centroid. Higher-order cumulant averages also immediately follow. Further, at the saddle point level, from ([\ref=saddle1]), ([\ref=m_sol]), and ([\ref=centroid_est]),

[formula]

We see that the solution set volume is proportional to the product of the centroid's components. This provides a simple, qualitative means for determining whether a given set of constraints ([\ref=constraints]) is strong or weak: By the arithmetic-geometric mean inequality, we have

[formula]

where I have made use of the normalization condition ([\ref=normalization]) to obtain the second line. Equality holds here if and only if each of the [formula] are equal to [formula], which is the case only in the absence of constraints. If constraints are applied, and the {pc,1i} are substantially different in magnitude, the upper bound in ([\ref=a-gmeaninq]) is far from strict. In this limit, the solution set volume is significantly diminished, and the constraints can be considered strong. On the other hand, if the {pc,1i} are all similar in magnitude, log Z is only slightly diminished, and the constraints can be considered weak.

We are now in a position to compare the centroid solution c, which minimizes the expected error ([\ref=av_error]), to the maximum entropy solution ME, which maximizes

[formula]

the Shannon entropy [\cite=Sha-48]. In a sense, ME is the member of S having the smoothest distribution. Objective criteria for its success are of great value, as maximum entropy inference is applied in many contexts. Using Lagrange multipliers, it is easy to show that ME is given formally by [\cite=Jay-57] [\cite=Pre-13]

[formula]

where m and the [formula] must now be chosen so that ([\ref=max_ent]) satisfies the constraints ([\ref=constraints]) and ([\ref=normalization]). As in the c,1 analysis, the normalization condition provides a simple solution for one of the unknowns:

[formula]

The [formula] must again be solved for using ([\ref=constraints]).

The distance between ME and the exact c can be estimated analytically by comparing ([\ref=max_ent]) to ([\ref=centroid_est]), which take a very similar form. Assuming the {fji} are Gaussian distributed, with [formula], expanding either c,1 or ME to first order in the {fji} results in the following solution:

[formula]

where α and the {βj} are given by

[formula]

the values needed for ([\ref=leading_order]) to satisfy ([\ref=constraints]) and ([\ref=normalization]) to leading order in N. The leading form ([\ref=leading_order]), ([\ref=cons_values]) is common to both c,1 and ME because they both take the form of functions having arguments linear in the {fji}. If [formula], the condition formally defining the weak constraint limit for Gaussian-distributed {fji} , the term proportional to [formula] dominates βj in ([\ref=cons_values]), and the second term in ([\ref=leading_order]) is of order [formula]. This is smaller than the leading α contribution in ([\ref=leading_order]), which is O(N- 1). In this case, the distance between ME and c,1 can be estimated by considering expansion up to second order in the {fji}, where the two solutions have differing Taylor series coefficients: [formula], while [formula]. This gives

[formula]

much smaller than the width of the solution space, which, from ([\ref=width]) and ([\ref=leading_order]), is given by [formula]. The maximum entropy and centroid solutions are very close in the large N, weak constraint limit.

In the strong constraint limit, σ  ≪  N1 / 2, the first term in ([\ref=cons_values]) dominates βj, and the second term in ([\ref=leading_order]) is [formula]. As σ  →  O(1), this is no longer smaller than α, signaling the breakdown of the asymptotic expansion. Empirically, I find that in this case

[formula]

That is, in the strong constraint limit, the two solutions are distant, with component separations comparable to the solution space widths. A typical example illustrating this is shown in Fig. [\ref=fig:snub]. Here, as expected, c,1 is much closer to the exact c (obtained via averaging over a random walk through S) than is ME. The discrepancy between the two is largest when pci (which sets the width σpi) is large. Further, pMEi  <  pci for all components taking relatively large or relatively small values, whereas pMEi  >  pci for all i taking intermediate values. This qualitative observation appears to hold quite generally, with entropy maximization occurring at a point whose intermediate weight components are substantially bolstered relative to those of the centroid, while all other components are relatively diminished.

In summary, then, I have shown that the centroid c of S provides the formal solution to ([\ref=constraints]) and ([\ref=normalization]) that minimizes ([\ref=av_error]). Although other variational score functions could be employed - e.g., the entropy - ([\ref=av_error]) represents a useful one to consider, in that it provides an objective measure for the expected error. By comparing the popular maximum entropy solution ME to the centroid's saddle point approximation - c,1, given by equations ([\ref=saddle_conds])-([\ref=centroid_est]), I have shown that ME actually performs quite well, in general, in the weak constraint limit. This is a very useful result, as most prior tests of the maximum entropy principle have relied upon particular, testable examples. In the strong constraint limit, the centroid and maximum entropy solutions are distant, and ME is expected to perform poorly, by measure ([\ref=av_error]). In this limit, centroid inference is typically much more accurate.

Like maximum entropy inference, centroid inference has the benefit of being free from any bias associated with fitting to a particular, model form. In practice, the centroid estimate can be obtained through averaging over a random walk through S. However, the walk time required increases relatively quickly with N. Alternatively, successive analytic approximations to c can be obtained using the method I outline here. The saddle point approximation c,1 provides a simple, first estimate, very similar in form to ME, that is accurate in the large N limit. Evaluation of c,1 provides substantial value, even when not working within the uniform ensemble, as it immediately provides much information relating to the solution set's geometry, as well as to the strength of the applied constraints.