Efficient and Robust Allocation Algorithms in Clouds under Memory Constraints

Keywords: Cloud; reliability; failure; service; allocation; bin packing; linear program; memory; CPU; column generation; large scale; probability estimate; replication; resilience;

Introduction

Recently, there has been a dramatic change in both the platforms and the applications used in parallel processing. On the one hand, there has been a dramatic scale change, that is expected to continue both in data centers and in exascale machines. On the other hand, a dramatic simplification change has also occurred in the application models and scheduling algorithms. On the application side, many large scale applications are expressed as (sequences of) independent tasks, such as MapReduce [\cite=shih2010performance] [\cite=dean2008mapreduce] [\cite=zaharia2008improving] applications or even run as independent services handling requests.

In fact, the main reason behind this paradigm shift is not related to scale but rather to unpredictability. First of all, estimating the duration of a task or the time of a data transfer is extremely difficult, because of NUMA effects, shared platforms, complicating network topologies and the number of concurrent computations/transfers. Moreover, given the number of involved resources, failures are expected to happen at a frequency such that robustness to failures is a crucial issue for large scale applications running on Cloud platforms. In this context, the cost of purely runtime solutions, agnostic to the application and based either on checkpointing strategies [\cite=bougeret2011checkpointing] [\cite=cappello2010checkpointing] [\cite=bouteiller2013multi] or application replication [\cite=wang2009improving] [\cite=ferreira2011evaluating] is expected to be large, and there is a clear interest for application-level solutions that take the inner structure of the application to enforce fault-tolerance.

In this paper, we will consider reliability issues in a very simple context, although representative of many Cloud applications. More specifically, we will consider the problems that arise when allocating independent services running as Virtual Machines (VMs) onto Physical Machines (PMs) in a Cloud Computing platform [\cite=zhang2010cloud] [\cite=armbrust2009above]. The platforms that we target have a few crucial properties. First, we assume that the platform itself is very large, in terms of number of physical machines. Secondly, we assume that the set of services running on this platform is relatively small, and that each service requires a large number of resources. Therefore, our assumptions corresponds well to a datacenter or a large private Cloud such as those presented in [\cite=CirneInvited], but not at all to a Cloud such as Amazon EC2 [\cite=amazon] running a huge number of small applications.

In the static case, mapping VMs with heterogeneous computing demands onto PMs with capacities is amenable to a multi-dimensional bin-packing problem (each dimension corresponding to a different kind of resource, memory, CPU, disk, bandwidth,[formula]). Indeed, in this context, on the Cloud administrator side, each physical machine comes with its computing capacity (i.e. the number of flops it can process during one time-unit), its disk capacity (i.e. the number of bytes it can read/write during one time-unit), its network capacity (i.e. the number of bytes it can send/receive during one time-unit), its memory capacity (given that each VM comes with its complete software stack) and its failure rate (i.e. the probability that the machine will fail during the next time period). On the client side, each service comes with its requirement along the same dimensions (memory, CPU, disk and network footprints) and a reliability demand that has been negociated through an SLA [\cite=CirneInvited].

In order to deal with resource allocation problems in Clouds, several sophisticated techniques have been developed in order to optimally allocate VMs onto PMs, either to achieve good load-balancing [\cite=van2009sla] [\cite=calheiros2009heuristic] [\cite=christopherTPDS] or to minimize energy consumption [\cite=berl2010energy] [\cite=beloglazov2010energy]. Most of the works in this domain have therefore focused on designing offline [\cite=GareyJohnson] and online [\cite=EpsteinvanStee07binpackresaumg] [\cite=Hochbaum97] solutions of Bin Packing variants.

In this paper, we propose to reformulate these heterogeneous resource allocation problems in a form that takes advantage on the assumptions we made on the platform and the characteristics of VMs. This set of assumption is crucial for the algorithms we propose. In this perspective, since we assume that the number of processing resources is very large, so that we will focus on resource allocation algorithms whose complexity is low (in practice logarithmic) in . We also assume that each VM comes with its full software stack, so that the number of different services Kmax that can actually run on the platform is very small, and will be treated as a small constant. We also assume that the number of services is also small. Typically, we will propose algorithms that will rely on the partial enumeration of the possible configurations of a set of PMs (i.e. the set of applications they run) and more precisely on column generation techniques [\cite=barnhart1998branch] [\cite=desrosiers2005primer] in order to solve efficiently the optimization problems. In Section [\ref=sec.simus], we will provide a detailed analysis of the resource allocation algorithm that we propose in this paper for a wide set of practical parameters. The cost of the algorithm will be analyzed both in terms of their processing time to find the allocation and in terms of the quality of the computed allocation (i.e. required number of resources).

Reliability constraints have received much less attention in the context of Cloud computing, as underlined by Cirne et al. [\cite=CirneInvited]. Nevertheless, reliability issues have been addressed in more distributed and less reliable systems such as Peer-to-Peer networks. In such systems, efficient data sharing is complicated by erratic node failure, unreliable network connectivity and limited bandwidth. In this case, data replication can be used to improve both availability and response time and the question is to determine where to replicate data in order to meet simultaneously performance and availability requirements in large-scale systems [\cite=replication_availability] [\cite=replication_cirne_1] [\cite=replication_datagrid] [\cite=replication_DB] [\cite=replication_cirne_2]. Reliability issues have also been addressed by High Performance Computing community. Indeed, the exascale community [\cite=dongarra2009international] [\cite=eesi] underlines the importance of fault tolerance issues [\cite=cappello2009fault] and proposed solutions based either on replication strategies [\cite=ferreira2011evaluating] [\cite=wang2009improving] or rollback recovery relying on checkpointing protocols [\cite=bougeret2011checkpointing] [\cite=cappello2010checkpointing] [\cite=bouteiller2013multi].

This work is a follow-up of [\cite=beaumont:hal-00743524], where the question of how to evaluate the reliability of an allocation has been addressed. One of the main results of [\cite=beaumont:hal-00743524] was that estimating the reliability of a given allocation was already a #P-complete problem [\cite=valiant1979complexity] [\cite=provan1983complexity] [\cite=bodlaender2004note]. In this paper, we prove that rare event detection techniques [\cite=algo-rare] developed by the Applied Probability community are in fact extremely efficient in practice to circumvent this complexity result. This paper is also a follow-up of [\cite=beaumont2013hipc], where asymptotically approximation algorithms for energy minimization have been proposed in the context where services were defined by their processing requirement only (and not their memory requirement). In [\cite=beaumont2013hipc], approximation techniques to estimate the reliability of an allocation were based on the use of Chernoff [\cite=chernoff] and Hoeffding [\cite=hoeffding] bounds. In the present paper, we propose different techniques based on the approximation of the binomial distribution by the Gaussian distribution.

The paper is organized as follows. In Section [\ref=sec.frame], we present the notations that will be used throughout this paper and we define the characteristics of both the platform and the services that are suitable for the techniques we propose. In Section [\ref=sec.resolution], we propose an algorithm for solving the resource allocation problem under reliability constraints. It relies on a pre-processing phase that is used to decompose the problem into a reliability problem and a packing problem. In Section [\ref=proba.est], we propose a new technique based on rare event detection techniques to estimate the reliability of an allocation, and we prove that this technique is very efficient in our context. At last, we present in Section [\ref=sec.simus] a set of detailed simulation results that enable to analyze the performance of the algorithm proposed in Section [\ref=sec.resolution] both in terms of the quality of returned allocations and processing time. Concluding remarks are presented in Section [\ref=sec.conclusion].

Framework

Platform and services description

In this paper, we assume the following model. On the one hand, the platform is composed of homogeneous machines [formula], that have the same CPU capacity and the same memory capacity . On the other hand, we aim at running services [formula], that come with their CPU and memory requirements. In this context, our goal is to minimize the number of used machines, and at to find an allocation of the services onto the machines such that all packing constraints are fulfilled. Nevertheless, this problem is not equivalent to a classical multi-dimensional packing problem. Indeed, the two requirements are of a different nature.

On the one hand, services are heterogeneous from a CPU perspective, hence each service expects that it will be provided a total computation power of (called demand) among all the machines. In addition, CPU sharing is modeled in a fluid manner: on a given machine, the fraction of the total CPU dedicated to a given service can take any (rational) value. This expresses the fact that the sharing between services which are running on a given machine is done through time multiplexing, whose grain is very fine.

On the other hand, memory requirements are homogeneous among all services, and memory requirements cannot be partially allocated: running a service on a machine occupies one unit of memory of this machine, regardless of the amount of computation power allocated to this service. This assumption models the fact that most of the memory used by virtual machines comes from the complete software stack image that needs to be deployed. On the other hand, the homogeneous assumption is not a strong requirement, and our algorithms could be modified to account for heterogeneous memory requirements (at the price of more complex notations). Furthermore, since the complete software stack image is needed, we assume that the memory capacity of the machines is not very high, i.e. each machine can hold at most 10 services.

Failure model

In this paper, we envision large-scale platforms, which means that machine failures are not uncommon and need to be taken into account. Two techniques are usually set up to face those machine failures: migration and replication. The response time of migrations may be too high to ensure continuity of the services. Therefore, we concentrate in this paper on a phase that occurs between two migration and reallocation operations, that are scheduled every x hours. The migration and reallocation strategy is out of the scope of this paper and we rather concentrate on the use of replication in order to provide the resilience between two migration phases. The SLA defines the robustness properties that the allocation should have.

More specifically, we assume that machine failures are independent, and that machines are homogeneous also with regard to failures. We denote the probability that a given machine fails during the time period between two migration phases. Because of those failures, we cannot ensure that a service will have enough computational power at its disposal during the whole time period. The probability that all machines in the platform fail is indeed positive. Therefore, in our model each service is also described with its reliability requirement , which expresses a constraint: the probability that the service has not enough computational power (less than its demand ) at the end of the time period must be lower than .

In this context, replicating a given service of many machines whose failure are independent, it will be possible to achieve any reliability requirement. Our goal in this paper is to do it for all services simultaneously, i.e. to enforce that capacity constraints, reliability requirements and service demands will be satisfied, while minimizing the number of required machines.

subsectionProblem description

We are now ready to state precisely the problem. Let be the CPU allocated to service on machine , for all [formula] and [formula]. For all [formula], we denote the random variable which is equal to 1 if machine is alive at the end of the time period, and 0 otherwise. We can then define, for all [formula], the total CPU amount that is available to service at the end of the time period: [formula]. The problem of the minimization of the number of used machines can be written as:

Equations [\eqref=eq.init.mem] and [\eqref=eq.init.cpu] depict the packing constraints, while Equation [\eqref=eq.init.rel] deals with reliability requirements.

We will use in this paper two approaches for the estimation of the reliability requirements. In the No-Approx model, the reliability constraint is actually written [formula].

However, as previously stated, given an allocation of one service onto the machines, deciding whether this allocation fulfills the reliability constraint or not is a #P-complete problem [\cite=beaumont:hal-00743524]; this shows that estimating this reliability constraint is a hard task. In [\cite=nous-resilience], it has been observed that, based on the approximation of a binomial distribution by a Gaussian distribution, [formula] is approximately equivalent to

[formula]

[formula] is a characteristic of normal distributions, and only depends on [formula], therefore it can be tabulated beforehand. In the following, we will denote this model by the Normal-Approx model.

Both packing and fulfilling the reliability constraints are hard problems on their own, and it is even harder to deal with those two issues simultaneously. In the next section, we describe the way we solve the global problem, by decomposing it into two sub-problems that are easier to tackle.

Problem resolution

We approach the problem through a two-step heuristic. The first step focuses mainly on reliability issues. The general idea about reliability is that, for a given service, in order to keep the replication factor low (and thus reduce the total number of machines used), the service has to be divided into small slices and distributed among sufficiently many machines. However, using too many small slices for each service would break the memory constraints (remember that the memory requirement associated to a service is the same whatever the size of slice, as soon as it is larger than 0). The goal of the first step, described in Section [\ref=sec.homo], is thus to find reasonable slice sizes for each service, by using a relaxed packing formulation which can be solved optimally.

In a second step, described in Sections [\ref=sec.cg], we compute the actual packing of those service slices onto the machines. Since the number of different services allocated to each machine is expected to be low (because of the memory constraints), we rely on a formulation of the problem based on the partial enumeration of the possible configurations of machines, and we use column generation techniques [\cite=barnhart1998branch] [\cite=desrosiers2005primer] to limit the number of different configurations.

Focus on reliability

In this section, we describe the first step of our approach: how to compute allocations that optimize the compromise between reliability and packing constraints, under both No-Approx and Normal-Approx models. This is done by considering a simpler, relaxed formulation of the problem, that can be solved optimally. We start with the Normal-Approx model.

Normal-Approx model

As stated before, in this first phase, we relax the problem by considering global capacities instead of capacities per machine. Thus we dispose of a total budget [formula] for memory requirements and [formula] for CPU needs, and use the following formulation:

In the following, we prove that this formulation can be solved optimally. We define a class of solutions, namely homogeneous allocations, in which each service is allocated on a set of machines, with the same CPU requirement. Formally, an allocation is homogeneous if for all [formula], there exists [formula] such that for all [formula], either [formula] or [formula].

On the relaxed problem, homogeneous allocations is a dominant class of solutions.

Let us assume that there exist i, j1 and j2  ≠  j1, such that [formula]. By setting [formula], we increase the left-hand side of Equation [\eqref=eq.pr.homo.r], and leave unchanged the left-hand sides of Equations [\eqref=eq.pr.homo.m] and [\eqref=eq.pr.homo.c]. From any solution of the problem, we can build another homogeneous solution, which does not use a larger number of machines.

An homogeneous allocation is defined by , the number of machines hosting service , and , the common CPU consumption of service on each machine it is allocated to. The problem of finding an optimal homogeneous allocation can be written as:

which can be simplified into

[formula]

In the following, we search for a fractional solution to this problem: both , the 's and the 's are assumed to be rational numbers. We begin by formulating two remarks to help solving this problem.

We can restrict to solutions which satisfy the following constraints:

For all i, [formula] is a non-increasing function of , thus given a solution of Problem [\ref=pb.init], we can build another solution such that [formula]. Indeed, let [formula] be an optimal solution of Problem [\ref=pb.init], and let [formula] for all [formula]. Now if we set [formula], we have on the one hand [formula], and on the other hand [formula], since [formula].

In the following, we only consider such solutions: let [formula] be a solution of Problem [\ref=pb.init] with machines, and satisfying [formula]. Let us now further assume that, in this solution, [formula]. We show that such a solution is not optimal by exhibiting a valid solution [formula], which uses [formula] machines. We set, for all [formula], [formula], and we define [formula] such that:

[formula]

We have firstly [formula], since [formula]. Furthermore, we prove now that [formula]. On the one hand, again from [formula], we obtain [formula]. On the other hand, from [formula], we have

[formula]

Finally [formula], hence [formula].

The second term of the maximum ensures that:

[formula]

All together, [formula] satisfies which implies that [formula] is not an optimal solution.

Let us now define f1 and f2 by [formula] and [formula].

Necessarily, at a solution with minimal , we have:

[formula]

For a given solution [formula], let us assume that there exist i and j such that [formula]. Without loss of generality, we can assume that i < j. At the first order,

[formula]

Then there exists [formula] such that [formula] and [formula]. Moreover, in the same way as in Remark [\ref=rem.eq], we can show that there also exists [formula] such that [formula] and [formula]. By remarking that [formula], we show that [formula] is not an optimal solution.

Both remarks show that at an optimal solution point, there exists such that

[formula]

By denoting [formula], let us consider the following third-order equation [formula]. The derivative is null at [formula] and [formula], and the function tends to +    ∞   when xi  →    +    ∞  . Since we search for [formula], we deduce that for any [formula], this equation has an unique solution. Let us denote gi(X) the unique value of [formula] such that [formula] is a solution to this equation. As [formula], we know that [formula]. We can thus compute gi(X) with a binary search inside [formula], since [formula] is an increasing function in this interval. Incidentally, we note that for all i, gi is an increasing function of X.

According to remark [\ref=rem.drond], for any optimal solution there exists X such that

[formula]

Since the left-hand side is increasing with , and the right-hand side is decreasing with , this equation has an unique solution [formula] which can be computed by a binary search on . Once [formula] is known, we can compute the [formula] and we are able to derive the [formula]'s. The solution S* computed this way is the unique optimal solution: for any optimal solution S', there exists [formula] which satisfies the previous equation. Since this equation has only one solution, [formula] and S'  =  S*.

We now show how to compute upper and lower bounds for the binary search on X. As shown previously, we have an obvious upper bound: [formula]. We express now a lower bound. Let be defined, for all i, by

[formula]

Then [formula] is a solution of a second-order equation, [formula], and since [formula], we can compute:

[formula]

Now let

[formula]

For all i, [formula]. Since gi is increasing with X, we have [formula]. Since [formula] is non-increasing, this implies

[formula]

From the definition of , we can conclude

[formula]

With the same line of reasoning, we can refine the upper bound into [formula], where [formula], by showing

[formula]

No-Approx model

In the previous section, we showed how to compute an optimal solution to the relaxed problem under the Normal-Approx model, but we have no guarantee that this solution will meet the reliability constraints under the No-Approx model.

Given an homogeneous allocation for a given service , the amount of alive CPU of follows a binomial law: [formula]. We can then rewrite the reliability constraint, under the No-Approx model, as [formula]. This constraint describes the actual distribution, but since the values [formula] have been obtained via an approximation, there is no guarantee that they will satisfy this constraint. However, since the cumulative distribution function of a binomial law can be computed with a good precision very efficiently [\cite=lib-binomial], we can compute [formula], the first integer which meets the constraint. We can the use equation [\eqref=eq.homo.r] to refine the value of [formula]: we compute [formula] so that equation [\eqref=eq.homo.r] with [formula] and [formula] is an equality, so that the approximation of the Normal-Approx model is closer to the actual distribution for these given values of [formula] and [formula].

We compute new 's for all services, and iterate on the resolution of the previous problem, until we reach a convergence point where the values of the do not change. In our simulations (see Section [\ref=sec.simus]), this iterative process converges in at most 10 iterations.

Focus on packing

In the previous section, we have described how to obtain an optimal solution to the relaxed problem [\ref=pb.init], in which homogeneous solutions are dominant. In the original problem, packing constraints are expressed for each machine individually, and the flexibility of non-homogeneous allocations may make them more efficient. Indeed, an interesting property of equation [\eqref=eq.pr.homo.r] is that "splitting" a service (i.e., dividing an allocated CPU consumption on several machines instead of one) is always beneficial to the reliability constraint (because splitting keeps the total sum constant, and decreases the sum of squares). In this Section, we thus consider the packing part of the problem, and the reliability issues are handled by the following constraints: the allocation of service on any machine j should not exceed , and the total CPU allocated to should be at least [formula]. Since the [formula] values are such that the homogeneous allocation satisfies the reliability constraint, the splitting property stated above ensures that any solution of this packing problem satisfies the reliability constraint as well.

The other idea in this Section is to make use of the fact that the number [formula] of services which can be hosted on any machine is low. This implies that the number of different machine configurations (defined as the set of services allocated to a machine) is not too high, even if it is of the order of [formula]. We thus formulate the problem in terms of configurations instead of specifying the allocation on each individual machines. However, exhaustively considering all possible configurations is only feasible with extremely low values of (at most 4 or 5). In order to address a larger variety of cases, we use in this section a standard column generation method [\cite=barnhart1998branch] [\cite=desrosiers2005primer] for bin packing problems.

In this formulation, a configuration is defined by the fraction of the maximum capacity devoted to service . According to the constraints stated above, configuration is valid if and only if [formula], [formula], and [formula]. Furthermore, we only consider almost full configurations, defined as the configurations in which all services are assigned a capacity either 0 or 1, except at most one. Formally, we restrict to the set of valid configurations such that [formula].

We now consider the following linear program , in which there is one variable for each valid and almost full configuration:

[formula]

Despite the high number of variables in this formulation, its simple structure (and especially the low number of constraints) allows to use column generation techniques to solve it. The idea is to generate variables only from a small subset [formula] of configurations and solve the problem on this restricted set of variables. This results in a sub-optimal solution, because there might exist a configuration in [formula] whose addition would improve the solution. Such a variable can be found by writing the dual of (the variables in this dual are denoted pi):

The sub-optimal solution to provides a (possibly infeasible) solution p*i to this dual problem. Finding an improving configuration is equivalent to finding a violated constraint, i.e. a valid configuration such that [formula]. We can thus look for the configuration which maximizes [formula]. This sub-problem is a knapsack problem, in which at most one item can be split.

Let us denote this knapsack sub-problem as Split-Knapsack. It can be formulated as follows: given a set of item sizes si, item profits pi, a maximum capacity C and a maximum number of elements M, find a subset J of items with weights xi such that [formula], and [formula] which maximizes the profit [formula]. We first remark that solutions with at most one split item are dominant for Split-Knapsack (which justifies that we only consider almost full valid configurations, i.e. configurations with at most one split item). Then, we prove that this problem is NP-complete, and we propose a pseudo-polynomial dynamic programming algorithm to solve it. This algorithm can thus be used to find which configuration to add to a partial solution of to improve it. However, for comparison purposes, we also use a Mixed Integer Programming formulation of this problem which is used in the experimental evaluation in Section [\ref=sec.simus].

For any instance of Split-Knapsack, there exists an optimal solution with at most one split item (i.e., at most one i∈J for which 0  <  xi  <  1). Furthermore, this split item, if there is one, has the smallest [formula] ratio.

This is a simple exchange argument: let us consider any solution J with weights xi, and assume by renumbering that [formula] and that items are sorted by non-increasing [formula] ratios. We can construct the following greedy solution: assign weight x'i  =  1 to the first item, then to the second, until the first value k such that [formula], and assign weight [formula] to item k. It is straightforward to see that this greedy solution is valid, splits at most one item, and has profit not smaller than the original solution.

The decision version of Split-Knapsack is NP-complete.

We first notice that checking if a solution to Split-Knapsack can be done in polynomial time, so this problem belongs in NP.

We prove the NP-hardness by reduction to equal-sized 2-Partition: given 2n integers ai, does there exist a set J such that [formula] and [formula] ? From an instance I of this problem, we build the following instance I' of Split-Knapsack: pi  =  1 + ai and si  =  ai, with M  =  n and [formula]. We claim that I has a solution if and only if I' has a solution of profit at least n  +  C. Indeed, if I has a solution J, then J is a valid solution for I' with profit n + C.

Reciprocally, if I' has a solution J with weights xi and profit p  ≥  n + C, then

[formula]

We get [formula], hence [formula]. Since [formula] and xi  ≤  1, this implies that all xi for i∈J are equal to 1 and that [formula]. Furthermore, [formula] verifies S  ≤  B because J is a valid solution for I', and S  ≥  B because S  =  p  -  n. Hence J is thus a solution for I.

An optimal solution to Split-Knapsack can be found in time O(nCM) with a dynamic programming algorithm.

We first assume that the items are sorted by non-increasing [formula] ratios. For any value 0  ≤  u  ≤  C, 0  ≤  l  ≤  M and 0  ≤  i  ≤  n, let us define P(u,l,i) to be the maximum profit that can be reached with a capacity u, with at most l items, and by using only items numbered from 1 to i, without splitting. We can easily derive that

[formula]

We can thus recursively compute P(u,l,i) in O(nCM) time.

Using Remark [\ref=rem.split.dominant], we can use P to compute P'(i), defined as the maximum profit that can be reached in a solution where i is split:

[formula]

Computing P' takes O(nC) time. The optimal profit is then the maximum value between P(C,M,n) (in which case no item is split) and max 1  ≤  i  ≤  nP'(i) (in this case item i is split).

In this section, we have proposed a two-step algorithm to solve the allocation problem under reliability constraints. The complete algorithm is summarized in Algorithm [\ref=algo.packing]. The execution time of the first loop is linear in [formula], and in practice it is executed at most 10 times, so the first step is linear (and in practice very fast). The execution time of the second loop is also polynomial: solving a linear program on rational numbers is very efficient, and the dynamic program has complexity [formula]. Furthermore, in practice the number of generated configurations is very low, of the order of , whereas the total number of possible configurations is [formula].

In the following (especially in Section [\ref=sec.simus]), we will evaluate the performance of this algorithm on several randomly-generated scenarios, in terms of running time and number of required machines. However, since in our approach the reliability constraints are taken into account in an approximate way, we are also interested in evaluating the resulting reliability of generated allocations. This is done in the next Section.

Reliability estimation

From previous results [\cite=beaumont:hal-00743524], we know that computing exactly the reliability of an allocation is a difficult problem: it is actually a #P-complete problem. A pseudo-polynomial dynamic programming algorithm was proposed to solve this problem. However, this algorithm assumes integral allocations and its running time is linear in the number of machines. This makes it not feasible to use it in the context of our paper, with large platforms and several hundreds of services to estimate.

In this section, we explore another way to estimate the reliability value of a given configuration. The algorithm presented here is adapted from Algorithm 2.2 in [\cite=algo-rare]. The objective is to compute a good approximation of the probability that a given service fails, i.e. [formula]. A straightforward approach for this kind of estimation is to generate a large sample of scenarios for , and compute the proportion of scenarios in which [formula]. However, this strategy fails if the events that we aim at detecting are very rare (as reliability requirements violations are in our context since is expected to be of the order of , which could be or order 10- 6 or lower). Indeed, it would require to generate a very large number of samples (more than 108 for the estimate of [formula]). A more sensible approach, as described in [\cite=algo-rare], is to decompose the computation of into a product of conditional probabilities, whose values are reasonably not too small (typically around 10- 1), and hence can be estimated with smaller sampling sizes. With this idea, estimating a reliability of 10- 6 would require 6 iterations, each of which uses a sampling of size 103, which dramatically reduces the 108 sampling size required by the direct approach.

Formal description

Since computing the reliability of each service can be done independently, we consider here a given service , and for the ease of notations, we omit the index i until the end of this section.

We rewrite in the following way:

[formula]

where the 's are thresholds such that

[formula]

We will see in the next subsection that those thresholds can be chosen on-the-fly.

In order to express the probability distribution of , we use the configuration description as in the previous section. We recall that is the CPU allocated to service in configuration and is the number of machines that follows this configuration. Then we have

[formula]

After a straightforward renumbering, and by denoting the number of different configurations in which the service appears, we obtain [formula], such that for all [formula], [formula].

A value of the random variable is thus fully described by the value of the random vector variable [formula]. In addition, when [formula] is a value of this random vector, we define [formula].

Full algorithm

The idea of the algorithm (described in details in Algorithm [\ref=algo.all]) is to maintain a sample of random vectors [formula], distributed according to the original distribution, conditional to [formula] at each step k. Obtaining the sample for step k + 1 is done in three steps. First, the value of [formula] is computed so that a 10% fraction of the current sample satisfy [formula] (line [\ref=algo.all.thres]). Then, in the Bootstrap step, we keep only the values that satisfies [formula], and draw uniformly at random [formula] vectors from this set, with replacement. Finally, in the Resample step, we modify each vector Ys of this set, one coordinate after the other, by generating a new value [formula] according to the distribution [formula] conditional to [formula]. This ensures that the new sample is distributed according to the required conditional distribution. At each step, an unbiased estimate of is the number of vectors which satisfy [formula], divided by the total sampling size .

The Resample step is described in more details in Algorithm [\ref=algo.resample]. In order to generate a new value [formula] conditional to [formula], it is sufficient to compute the total CPU allocated in the other configurations [formula]: then, the condition is equivalent to [formula], and this amounts to generating according to a truncated binomial distribution.

Simulations

In this section, we perform a large set of simulations with two main objectives. On the one hand, we assess the performance of Algorithm [\ref=algo.packing] and we observe the number of machines used, the execution time and the compliance with the reliability requests. On the other hand, we study the behavior of the reliability estimation algorithm in Section [\ref=sec.simus.estimate].

Simulations were conducted using a node based on two quad-core Nehalem Intel Xeon X5550, and the source code of all heuristics and simulations is publicly available on the Web [\cite=simus-liopaul].

Resource Allocation Algorithms

In order to give a point of comparison to describe the contribution of our algorithm, we have designed an additional simple greedy heuristic. This heuristic is based on an exclusivity principle: two different services are not allowed to share the same machine. Each service is thus allocated the whole CPU power of some number of machines. The appropriate number of machines for a given service is the minimum number of machines that have to be dedicated to this service, so that the reliability constraint is met. This can be easily computed using the cumulative distribution function of a binomial distribution [\cite=lib-binomial] and a binary search. We greedily assign the necessary number of machines to each service and obtain an allocation that fulfills all reliability constraints. This heuristic is named no_sharing.

In the algorithms based on column generation, the linear program for Eq([\ref=pb.colgen]) is solved in rational numbers, because its integer version is too costly to solve optimally. An integer solution is obtained by rounding up all values of a given configuration, so as to ensure that the reliability constraints are still fulfilled. This increases the number of used machines, so we also keep the rational solution as a lower bound. The ceiled variant that uses a linear program to solve the Split-Knapsack problem is called colgen, while colgen_pd runs the dynamic programming algorithm. Finally, colgen_float denotes the lower bound. The legend that applies for all the graphs in this Section is given in Figure [\ref=fig.key].

Simulation settings

We explore two different kinds of scenarios in our experiments. In the first scenario, called Uniform, we envision a cloud where all services have close demands. The CPU demand of a service is drawn uniformly between the equivalent CPU capacity of 5 and 50 machines, and we vary the number of services from 20 to 300. In this last case, the overall required number of machines is more than 8000 on average. In the Bivalued scenario, two classes of services request for resources. Three big services set their demand between 900 and 1100 machines, while the 298 other clients need from 5 to 15 machines, so that the machines are fairly shared between the two classes of services. This leads to more than 6000 machines in average.

In both cases, the reliability request for each service is taken to be 10- X, where X is drawn uniformly between 2 and 8. On the platform side, we vary the memory capacity of the machines from 5 to 10, and the failure probability of a machine is set to 0.01.

Number of required machines

The number of required machines by each heuristic is depicted in Figure [\ref=fig.nm].The first observation is that the rounding step increases the number of machines used by at most 2.5%. This can be explained by noting that the number of different configurations that are actually used in a solution of colgen is less than the number of services, hence each configuration is used a relatively large number of times.

Another observation is that the no_sharing heuristic is sensitive to the memory capacity, and, as expected, its quality decreases compared to the column generation heuristics (by construction, given a set of services, no_sharing will return the same solution whatever the memory capacity) and becomes 12.5% worse than the lower bound in the Uniform scenario. Services are indeed distributed on more machines in the column generation heuristics, and hence need less replication to fulfill the reliability constraints.

Finally, concerning the final number of machines, colgen and colgen_pd are very close, which shows that the discretization required for the dynamic programming algorithm does not induce a noticeable loss of quality.

Execution time

We represent in Figure [\ref=fig.time] the execution time of all heuristics, in both scenarios. There appears a difference between colgen and colgen_pd. In the Uniform scenario with low memory capacity, the execution time of colgen is at the same time very high and very unstable (it may reach ), while colgen_pd remains under . With higher memory capacity, the execution time of colgen improves while colgen_pd gets slower; they become similar when 10 services are allowed on the same machine.

On the other hand, in the Bivalued case, the execution time of both variants increase when the memory capacity increases, and colgen_pd is always better than colgen.

Finally in all cases no_sharing confirms that it is a very cheap heuristic, and its execution time never exceeds .

Reliability

We represent in Figure [\ref=fig.rel] the compliance of the services with their reliability constraint. This constraint is met if the point is below the black straight line. Because of the rounding step, the column generation heuristics fulfill easily the reliability bounds in average. Moreover, we observed (results are not displayed here due to lack of space) that the worse cases are very close to the line but remain below it, which is intended by construction of the heuristics.

In the Bivalued scenario, the column generation heuristics produce allocations that are even more reliable than in the Uniform scenario. This come from the fact that the big services are assigned to a large number of different configurations, and hence gain even more CPU after the rounding step.

There is a clear double advantage to the column generation-based algorithm, and especially colgen_pd, against no_sharing: it leads to more reliable allocations with a noticeably smaller number of machines.

Probability estimation algorithm

In Figure [\ref=fig.at] we plot the execution time of the probability estimation algorithm of the reliability of a service as a function of the actual failure probability of the allocation. Obviously, when the failure probability decreases, the execution time increases, since the event that we try to capture is rarer. This estimate algorithm is efficient, since it can estimate an event of probability 10- 17 in about . In our particular case, it is possible to further lower this execution time if we focus only on checking whether the failure probability requirement is not exceeded. Indeed, the requirements are lower than 10- 9. By stopping the algorithm early, it is possible to determine whether the allocation is valid in at most .

We can remark that the estimate of failure probabilities in a solution returned by no_sharing is not expensive, since each service is allocated to exactly one configuration. Hence, at each step of the algorithm, we have only one draw for one binomial distribution, which is not the case with solutions that are provided by the other heuristics.

Conclusion

The evolution of large computing platforms makes fault-tolerance issues crucial. With this respect, this paper considers a simple setting, with a set of services handling requests on an homogeneous cloud platform. To deal with fault tolerance issues, we assume that each service comes with a global demand and a reliability constraint. Our contribution follows two directions. First, with borrow and adapt from Applied Probability literature sophisticated techniques for estimating the failure probability of an allocation, that remains efficient even if the considered probability is very low (10- 10 for instance). Second, we borrow and adapt from the Mathematical Programming and Operations Research literature the use of Column Generation techniques, that enable to solve efficiently some classes of linear programs. The use of both techniques enables to solve in an efficient manner the resource allocation problem that we consider, under a realistic settings (both in terms of size of the problem and characteristics of the applications, for instance discrete unsplittable memory constraints) and we believe that it can be extended to many other fault-tolerant allocation problems.