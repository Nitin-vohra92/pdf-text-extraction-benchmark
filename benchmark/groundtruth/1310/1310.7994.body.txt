Necessary and Sufficient Conditions for Novel Word Detection in Separable Topic Models

Introduction

A series of powerful practical algorithms for probabilistic topic modeling have emerged in the past decade since the seminal work on Latent Dirichlet Allocation (LDA) [\cite=LDA:ref]. This has propelled topic modeling into a popular tool for learning latent semantic structures in large datasets. Formally, topic models consider a collection of M documents, each modeled as being generated by N iid drawings of words from an unknown W  ×  1 document word-distribution vector over a vocabulary of size W. By positing K latent topics as distribution vectors over the vocabulary, each document word-distribution vector arises as a probabilistic mixture of the K topic vectors. The topic proportions for documents are assumed to be sampled in an iid manner from some prior distribution such as the Dirichlet distribution in LDA.

For future reference, let [formula] denote the unknown W  ×  K topic matrix whose columns are the K latent topics, [formula] the K  ×  M probabilistic topic-weight matrix whose columns are topic proportions of the M documents, and let [formula] denote the W  ×  M empirical word-by-document matrix whose columns are word-frequency vectors of the M documents. Typically, W  ≫  K.

While the prevailing approach is to find a maximum likelihood fit of [formula] to the generative model through approximations or heuristics, a recent trend has been to develop topic estimation algorithms with provable guarantees under suitable additional conditions [\cite=ARORA:ref] [\cite=Arora2:ref] [\cite=DDP:ref] [\cite=Anan13:ref]. Chief among them is the so-called topic separability condition [\cite=Donhunique:ref] [\cite=ARORA:ref] [\cite=DDP:ref]: The words that are unique to each topic, referred to as "novel words", are key to the recovery of topics. However, as implicitly suggested in [\cite=Donhunique:ref], and illustrated in Fig. [\ref=counterexample], separability alone does not guarantee the uniqueness of recovery. Therefore to develop algorithms with provable (asymptotic) consistency guarantees (N fixed, M  →    ∞  ), a number of recent papers have imposed additional conditions on the prior distribution of the columns of [formula]. This is summarized in Table [\ref=relatedworks] where [formula] and [formula] are, respectively, the expectation and correlation matrix of the prior on the columns of [formula] and [formula] is the "normalized" correlation matrix. Without loss of generality we can assume that each component of [formula] is strictly positive. Among these additional conditions, the simplicial condition (cf. Sec. [\ref=sec:sim]) on [formula] is the weakest sufficient condition for consistent topic recovery that is available in the literature. However, the existing approaches either lack statistical guarantees or are computationally impractical. Algorithms with both statistical and computational merits have been developed by imposing stronger conditions as in [\cite=Arora2:ref] [\cite=DDP:ref]. Hence the natural questions that arise in this context are:

What are the necessary and sufficient conditions for separable topic recovery?

Do there exist algorithms that are consistent, statistically efficient, and computationally practical under these conditions?

In this paper, we first show that the simplicial condition on the normalized correlation matrix [formula] is an algorithm-independent, information-theoretic necessary condition for consistently detecting novel words in separable topic models. The key insight behind this result is that if [formula] is non-simplicial, we can construct two distinct separable topic models with different sets of novel words which induce the same distribution on the observations [formula]. In Sec. [\ref=sec:rp], we answer the second question in the affirmative by outlining a random projection based algorithm and providing its statistical and computational complexity. Due to space limitations, the details of this novel algorithm and the proofs of the claimed sample and computational complexity will appear elsewhere.

Simplicial Condition

Similar to [\cite=ARORA:ref], we have

A matrix [formula] is simplicial if [formula] such that each row of [formula] is at a Euclidean distance of at least γ from the convex hull of the remaining rows.

For estimating a separable topic matrix [formula], the simplicial condition is imposed on the normalized second order moment [formula]. More precisely: Algorithms with provable performance guarantees that exploit the separability condition typically consist of two steps: (i) novel word detection and (ii) topic matrix estimation. We will only focus on the detection of all novel words since the detection problem is in itself important in many applications, e.g., endmember detection in hyperspectral, genetic, and metabolic datasets, and also because the second estimation step is relatively easier once novel words are correctly identified. Our first main result is contained in the following lemma:

(Simplicial condition is necessary) Let [formula] be separable and W  >  K. If there exists an algorithm that can consistently identify all the novel words of all the topics, then its normalized second order moment [formula] is simplicial.

The proof is by contradiction. We will show that if [formula] is non-simplicial, we can construct two topic matrices [formula] and [formula] whose sets of novel words are not identical and yet [formula] has the same distribution under both models. This will imply the impossibility of consistent novel word detection.

Suppose [formula] is non-simplicial. Then we can assume, without loss of generality, that its first row is within the convex hull of the remaining rows, i.e., [formula], where [formula] denotes the j-th row of [formula], and [formula], [formula] are convex weights. Compactly, [formula] where [formula]. Recalling that [formula], where [formula] is a positive vector and [formula] with [formula] denoting any column of [formula], we have

[formula]

which implies that [formula]. From this it follows that if we define two non-negative row vectors [formula] and [formula], where b  >  0,0  <  α  <  1 are constants, then [formula].

Now we construct two separable topic matrices [formula] and [formula] as follows. Let [formula] be the first row and [formula] be the second in [formula]. Let [formula] be the first row and [formula] the second in [formula]. Let [formula] be a valid separable topic matrix. Set the remaining (W - 2) rows of both [formula] and [formula] to be [formula]. We can choose b to be small enough to ensure that each element of [formula] is strictly less than 1. This will ensure that [formula] and [formula] are column-stochastic and therefore valid separable topic matrices. Observe that [formula] has at lease two non-zero components. Thus, word 1 is novel for [formula] but non-novel for [formula].

By construction, [formula], i.e., the distribution of [formula] conditioned on [formula] is the same for both models. Marginalizing over [formula], the distribution of [formula] under each topic matrix is the same. Thus no algorithm can distinguish between [formula] and [formula] based on [formula].

Our second key result is the sufficiency of the simplicial condition for novel word detection:

Assume that topic matrix [formula] is separable. If [formula] is simplicial, then there exists an algorithm whose running time is at most quadratic in W,M,K,N, that only makes use of empirical word co-occurrences and consistently recovers the set of all novel words for K topics as M  →    ∞  .

This is a consequence of Lemma [\ref=thm:rp] in Sec. [\ref=sec:rp] where an algorithm based on random projections is described that can attain the claimed performance.

We conclude this section with two conditions that each imply the simplical condition.

Let [formula] be the normalized topic correlation matrix matrix. Then, (i) [formula] is diagonal dominant, i.e., [formula], [formula] [formula] [formula] is simplicial. (ii) [formula] is full rank [formula] [formula] is simplicial. Furthermore, the reverse implications in (i) and (ii) do not hold in general.

The proof of the above proposition is omitted due to space limitations but is straightforward. This demonstrates that the diagonal dominant condition in [\cite=DDP:ref] and the full-rank condition in [\cite=Arora2:ref] are both stronger than the simplicial condition.

Random Projection Algorithm

The pseudo-code of an algorithm that can achieve the performance claimed in Lemma [\ref=lem:sufficient] is provided below (cf. Algorithm [\ref=Alg:RP]). Due to space limitations, we only explain the high-level intuition which is geometric. Let [formula] and [formula] be obtained by first splitting each document into two independent copies and then scaling the rows to make them row-stochastic. The key idea is that if [formula] is simplicial, then as M  →    ∞  , with high probability, the rows of [formula] corresponding to novel words will be extreme points of the convex hull of all rows. This suggests finding novel words by projecting the rows onto an isotropically distributed random direction, P times, and then selecting the K rows which maximize the projection value most frequently. We summarize the statistical and computational properties of this algorithm in Lemma [\ref=thm:rp]:

Let topic matrix [formula] be separable and [formula] be simplicial. Then Algorithm [\ref=Alg:RP] will output all novel words of all K topics consistently as M  →    ∞   and P  →    ∞  . Furthermore, [formula], for Algorithm [\ref=Alg:RP] fails with probability at most δ, where c1 to c3 are some absolute constants and d,φ,η, and [formula] are constants that depend on model parameters [formula], [formula], and [formula]. Moreover, the running time of Algorithm [\ref=Alg:RP] is O(MNP  +  WP + K2).

As summarized in Lemma [\ref=thm:rp], the computational complexity of Algorithm [\ref=Alg:RP] is linear in terms of M,N,W and quadratic in terms of K, which typically stays fixed. This is more efficient than the best known provable algorithms in [\cite=ARORA:ref] [\cite=Arora2:ref] [\cite=DDP:ref]. The sample complexities for M and P are both polynomial in terms of W, log (δ), and other model parameters, which are comparable to the current state-of-the-art approaches. It turns out that Algorithm [\ref=Alg:RP] is also amenable to distributed implementation since it only involves aggregating the counts of rows that maximize projection values.

Discussion

The necessity of the simplicial condition we proved in this paper is information-theoretic and algorithm-independent. It is also a sufficient condition. Although widely used priors, e.g., Dirichlet, satisfy the stronger full-rank and diagonal dominant conditions, in certain types of datasets, e.g., Hyperspectral Imaging, these may not hold [\cite=DDP:ref]. This paper only focused on detecting distinct novel words of all topics. In general, the simplicial condition is not sufficient for consistently estimating the topic matrix. It can be shown that the full-rank condition is sufficient but not necessary.