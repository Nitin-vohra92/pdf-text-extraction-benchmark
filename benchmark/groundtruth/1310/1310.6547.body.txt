Sparse Predictive Structure of Deconvolved Functional Brain Networks

[formula], M. Cristoforetti[formula], C. Furlanello[formula], and G. Jurman[formula]

Introduction

Brain Networks [\cite=bullmore2009] (BNs) are high-dimensional objects that represent the pairwise statistical dependence of measured brain signals. In this framework, sensors or regions of interests are represented as nodes and their interactions as links with an associated weight. One of the most critical drawback of this approach is that the construction of a BN results into a fully-connected graph where spurious and non spurious associations cannot be distinguished. Furthermore, it is typically of interest to study the statistical pattern of associations underlying a specific Cognitive State, but functional patterns found in the data are typically confoundable with those representing default brain functioning (RIF). Methods for the estimation of sparse graphs, i.e. networks where only a subset of the links have a non-zero weight, have been implemented by either using l1 penalized regularization techniques [\cite=meinshausen2006high] [\cite=friedman2008sparse] or by thresholding the network. However, the amount of sparsity depends on a regularization (or thresholding) parameter whose choice is completely arbitrary unless the network estimation is integrated in a classification framework [\cite=zanin2012optimizing]. Recently machine learning techniques have been extended to the classification of f-MRI state-dependent Brain Networks [\cite=richiardi2011decoding] [\cite=shirer2012decoding] as well as for the classification of patients [\cite=bassett2011altered] from resting state data, revealing an intrinsic relationship between the BN structure and both transient and permanent cognitive states (see [\cite=Richiardi2013] for an extensive review). Given the high dimensionality of BN data, PCA-based dimensionality reduction techniques [\cite=leonardi2013principal] [\cite=schluep2013principal] and recursive feature selection methods based on node summary statistics [\cite=fekete2013] or topological features [\cite=jietopological] [\cite=jie2012structural] have been proposed. In this paper we adopt the predictive classification framework and consider a sparsified approach (the l1l2 improvement of the elastic network) in integration with a recent network deconvolution method [\cite=feizi2013network] for detecting direct effects from an observed correlation matrix containing both direct and indirect effects. The algorithm removes the effect of all indirect paths of arbitrary length in a closed-form solution by exploiting eigen-decomposition. We present the pipeline in Section [\ref=sec:meth]. In Section [\ref=sec:apps], we compare such an off-the-shelf approach with published methods as well as with SVM and Random Forest predictors on the MEG Biomag 2010 competition 1 dataset and discuss results in Section 4. We show that the new procedure can be used to produce sparse BNs that are predictively associated with the experimental conditions of interest, with an advantage in terms of predictive accuracy as well as of interpretability.

Methods

Notation Let E = {X,Y} be the experiment of interest composed by N trials, each associated with a label yn∈{1, - 1}, occurring N1 and N- 1 times respectively, with N1 + N- 1 n∈N. Brain signals are recorded from p loci with a temporal sampling of t data points per trial, resulting in the p  ×  t data matrix Xn.

The goal of our pipeline is that of approximating the frequency specific BNs φfr(X) and the function f:φfr(X)  ↦  Y. Let φfr(Xn) be the p  ×  p matrix whose (i,j) - th element φfr(Xn)ij corresponds to the spectral coherence cfrij at frequency fr between the i - th and j - th row of the matrix Xn. Since the matrix φfr(X) is symmetric, [formula] is defined over a [formula] vectorial space, where abusing notations φfr(Xn) is used for both vectors in such a space and the p  ×  p matrix representation of the same vectors. Let dφfr(Xn) indicate the deconvolved network and D the deconvolution filter. For the case of interest let the approximated function be linear in the form   =  φfr(X)β  +  μ such that [formula], and let βfrij be the weight of the linear functional βfr associated to element φfr(X)ij.

Data Analysis Pipeline

The experiment E = {X,Y} is randomly split in two parts S times keeping the classes balanced: the development Edevs  =  {Ydevs,Xdevs} and test Etes  =  {Ytes,Xtes}.

Network Construction: Each data input matrix Xn is transformed into φfr(Xn) by estimating the spectral coherence between each combination of rows in Xn at frequency fr.

Network Deconvolution: Each network φfr(Xn) is deconvolved using a non-linear spectral filter D:φfr(X)  ↦  dφfr(X). Following [\cite=feizi2013network] networks are deconvolved in three steps: first the network φfr(X) is linearly scaled such that all its eigenvalues are between -1 and 1, then the rescaled network is decomposed with SVD and finally its eigenvalues are transformed such that [formula], with λdi is the i - th eigenvalue of the deconvolved network and λi the i - th eigenvalue of the rescaled network. As shown in [\cite=feizi2013network], under the assumption that the observed links are formed by an infinite sum of indirect dependencies from nodes at increasing distances, such deconvolution procedure leads to the optimal solution.

Two Steps Elastic Net: the Elastic Net function ffrs is estimated for each split at each frequency. The Elastic Net considered here is a linear regression with mixed l1l2 norm, introduced by [\cite=zou2005] as a solution for the Lasso [\cite=tibshirani1996regression] instability problems in p  ≫  n environment with highly correlated variables. Since the l1 norm introduces a shrinkage effect biasing the estimated coefficients, we followed [\cite=DeMol2009] and considered an elastic net(EN) estimation in two phases by first minimizing using the proximal algorithm of [\cite=mosci2010solving] the function

[formula]

followed by a debiasing step where a ridge regression is estimated using only the variables selected (indicated by φfr(Xi)βfr  ≠  0 )in the previous step by the minimization of:

[formula]

The parameters τ* and λ* are tuned using a k-cv over Eds while μ is chosen to be small and constant across tuning. Only the model at the optimal frequency ffr* associated with the highest k-cv accuracy is selected.

Test Accuracy Estimation Each ffr*s is tested on their correspondent test set Etes.

Sparse Deconvolved Predictive Network Results are aggregated in the predictive network Bfr with entries [formula].

Application to MEG Single Trial Classification

We apply our method to the MEG Biomag 2010 competition 1 dataset [\cite=van2009attention], which being among the first publically available benchmarks for MEG decoding has been extensively considered for testing novel algorithms and pipelines [\cite=bahramisharif2010covert] [\cite=signoretto2012classification] [\cite=kia2013discrete], sometimes stumbling in selection bias and severely over-estimating the classification accuracy [\cite=olivetti2010]. The experiment consists in monitoring the brain activity of the subjects in a MEG scanner under two different conditions: attention had to be covertly modulated either on the right visual field or on the left one. Subjects had to fix a cross at the center of the screen and at regular intervals a cue indicated which direction they had to covertly attend the designated visual field during the next 2500ms. The competition data consists in the MEG measurements of four subjects from 500ms before the cue offset to 2500ms after from 274 DC SQUID axial gradiometers sensors downsampled at 300Hz. The goal of the competition is to classify on what visual field is the subject modulating his attention based on the MEG data. For our application we restrict our attention to the first subject and use a total of 126 trials per condition, for a total of 252 trials to guarantee balanced stratification between conditions.

Preprocessing, Network Construction and Deconvolution The raw signals of each trial are independently decomposed with a multitaper frequency transformation in the 5-40 Hz interval with 2 Hz bin width. The results of the frequency transforms are used to construct a coherence network for each trial, which is successively rescaled such that its eigenvalues are between +1 and -1. After rescaling, the eigenvalues of the network are filtered as explained in Section [\ref=sec:meth] [\cite=feizi2013network].

Elastic Net Model Estimation The dataset is randomly divided 10 times in a class-balanced development (168 trials) and test (84 trials) splits. The reported classification results are the average prediction accuracy on the test set of the models estimated on the correspondent development set. Inside each development split, the optimal parameters τ*,λ* of the Elastic Net are tuned through grid search selecting those with higher prediction accuracy. Finally the βfr weights are computed with the optimal parameters.

Sparse Predictive Network The optimal results of each development-test split are aggregated by averaging the βfr weights. This procedure leads to the construction of a Sparse Predictive Network which can be either directly analyzed or used to filter the average networks of each class for results interpretation.

Competing Methods In order to test the quality of the proposed method we used other two well established learning algorithms for comparison purposes: Random Forest (RF) [\cite=breiman2001random], based on a growing a set of decision trees, and Support Vector Machine (SVM) [\cite=cortes1995support], which is instead looking for a geometric separation surface. Moreover, we tried three different versions of the SVM: the classical linear approach (L-SVM), a kernel based on the Hamming distance [\cite=hamming1950error] (H-SVM) and the novel IM graph kernel (IM-SVM), based on the Ipsen-Mikhailov network metric [\cite=ipsen2002evolutionary] [\cite=jurman2011biological] [\cite=jurman2012]. As a major difference, both RF and SVM Linear and SVM Hamming work on the vectorized nets, so they are making no use of the network structure, which instead is a key feature when using the IM kernel, which is the Radial Basis Function matrix stemming from the distance matrix computed between all pairs of involved nets [\cite=cortes2003positive]. We refer to [\cite=kia2013discrete] to compare our results to a pipeline using the Elastic Net directly on summary statistics of spatio-temporal activation patterns and on their Discrete Cosine Transformations.

Results and Conclusions

Non Deconvolved Networks Results Given the 4-cv, consistently with previous findings[\cite=van2009attention] on this dataset, we found that alpha (8-13 Hz) is the optimal interval. As non-deconvolved Brain Networks at 11 Hz exhibits higher 4-cv accuracy and lower standard errors, we decided therefore to focus our experiments only on those networks. As shown in Table [\ref=table:acc], the performance of all the classifiers based on local properties of raw Brain Networks are comparable or superior to the 0.67 accuracy of the low level features that [\cite=kia2013discrete] employs as baseline. Non-surprisingly RF and EN reach higher results, probably because of their embedded feature selection. Instead, the IM-SVM kernel only reaches near chance-level results, implying that the topological features of the graph are not useful for classification in this task. We speculate that this may be caused by the symmetric nature of the task.

Deconvolved Networks Results As an effect of network Deconvolution all the local methods except for RF increase their predictive power 1) strongly bridging the gap between methods with and without embedded feature selection 2) outperforming the 0.67 benchmark and all reaching comparable results with 2-D DCT basis. Furthermore the combination of both the network based Elastic Net and the SVM with Hamming Kernel with Network Deconvolution reaches an accuracy of 0.74 slightly outperforming the 2-D DCT Elastic Net method.

Conclusions We provide interesting results on the possibility to efficiently use Deconvolved Brain Networks to represent high frequency neural time series from MEG data in decoding problems. Despite the recent increase of application of graph Kernels based on the graph isomorphism paradigm [\cite=vishwanathan2010graph] [\cite=jietopological] [\cite=jie2012structural] [\cite=mokhtari2012decoding] [\cite=vega2013brain], our comparison of different classification methods suggests that in a partially symmetric system like the brain it is fundamental to consider both local and global characteristics as it may be the case that topological properties do not convey useful informations. Future improvements of our pipeline will be devoted to its adaptation to temporal and multiplex networks whose mathematical formalism has been recently extended in [\cite=zhou2010time] [\cite=holme2012temporal] [\cite=de2013mathematical] [\cite=sole2013spectral] [\cite=kivela2013multilayer]. Finally we notice how the graphical form of the solutions might allow better interpretability and understanding of the brain multivariate structure with the application of common complex network tools, like node level metrics [\cite=rubinov2010] or higher level properties like communities [\cite=girvan2002community] [\cite=newman2004finding] [\cite=porter2009communities] or core-peripheries [\cite=borgatti2000models] [\cite=holme2005core] [\cite=rombach2012core] [\cite=bassett2013task] to cross-validated predictive sub-networks.