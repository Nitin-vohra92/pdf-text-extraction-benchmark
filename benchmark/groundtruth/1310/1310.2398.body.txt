Lemma Corollary Claim

Stochastic stability of Lyapunov exponents and Oseledets splittings for semi-invertible matrix cocycles

Introduction

The landmark Oseledets Multiplicative Ergodic Theorem (MET) plays a central role in modern dynamical systems, providing a basis for the study of non-uniformly hyperbolic dynamical systems. Oseledets' theorem has been extended in many ways beyond the original context of products of finite-dimensional matrices, for instance to certain classes of operators on Banach spaces and more abstractly to non-expanding maps of non-positively curved spaces.

The original Oseledets theorem [\cite=Oseledets] was formulated in both an invertible version (both the base dynamics and the matrices are assumed to be invertible) and a non-invertible version (neither the base dynamics nor the matrices are assumed to be invertible). The conclusion in the non-invertible case is much weaker than in the invertible case: in the invertible version, the theorem gives a splitting (that is, a direct sum decomposition) of [formula] into equivariant subspaces, each with a characteristic exponent that is used to order the splitting components from largest to smallest expansion rate; whereas in the non-invertible version, the theorem gives an equivariant filtration (that is, a decreasing nested sequence of subspaces) of [formula].

In various combinations, the current authors and collaborators have been working on extensions of the MET to what we have called the semi-invertible setting [\cite=FLQ1] [\cite=FLQ2] [\cite=GTQuas]. This refers to the assumption that one has an invertible underlying base dynamical system (also known as driving or forcing), but that the matrices or operators that are composed may fail to be invertible. In this setting, our theorems yield an equivariant splitting as in the invertible case of the MET, rather than the equivariant filtration that the previous theorems would have given.

We are interested in applications where the operators are Perron-Frobenius operators of dynamical systems acting on suitable Banach spaces. Here, the 'suitable' Banach spaces are spaces that are mapped into themselves by the Perron-Frobenius operator, and on which the Perron-Frobenius operator is quasi-compact. These Banach spaces have been widely studied in the case of a single dynamical system.

An Ansatz that first appeared in a paper of Dellnitz, Froyland and Sertl [\cite=DellnitzFroylandSertl] in the context of Perron-Frobenius operators of a single dynamical system is the following:

In a series of papers, they take this idea further by showing that level sets of eigenfunctions with eigenvalues peripheral to the essential spectral radius can be used to locate almost-invariant sets in the dynamical system [\cite=DellnitzJunge] [\cite=FroylandDellnitz] [\cite=Froyland2005] [\cite=GTHuntWright]. Figure [\ref=fig1] gives a schematic illustration of such a system: The left and right halves are almost-invariant under the dynamics, but the bottleneck joining them allows small but non-negligible interaction between them.

While this Ansatz was initially made in the context of a single dynamical system, it seems to apply equally in the case of random dynamical systems [\cite=FroylandLloydSantitissadeekorn] [\cite=FroylandSantiMonahan] [\cite=Froyland-AnalyticFramework], and this is the central motivation for our research in this area. It is well known that Perron-Frobenius operators of non-invertible maps are essentially never invertible, but it is often reasonable to assume that the base dynamics are invertible. Indeed, even if the driving system is non-invertible, one can make use of canonical mathematical techniques to extend it to an invertible one. Hence, we naturally find ourselves in the semi-invertible category. The principal object that we are interested in understanding is the second Oseledets subspace (or more generally the first few Oseledets subspaces).

The significance of our extensions to the MET is that the second subspace that we obtain is low-dimensional (typically one-dimensional) instead of (d - 1)-dimensional, which is what would come from the standard non-invertible MET. In numerical applications, where d may be 105 or greater, one cannot expect to say anything reasonable about level sets of functions belonging to a high-dimensional subspace, whereas using the semi-invertible version of the theorem, we are once again in a position to make sense of the level sets.

In practice, of course, one cannot numerically study the action of Perron-Frobenius operators on infinite-dimensional Banach spaces. Nor can one find a finite-dimensional subspace preserved by the operators. A remarkably fruitful approach is the so-called Ulam method. Here, the state space is cut into small reasonably regular pieces and a single dynamical system is treated as a Markov chain, by applying the dynamical system and then randomizing over the cell in which the point lands. This also makes sense for random dynamical systems.

In [\cite=FGTQ], we showed that applying the Ulam method to certain random dynamical systems, the top Oseledets space of the truncated system converges in probability to the true top Oseledets space of the random dynamical system as the size of the partition is shrunk to 0. The top Oseledets space is known to correspond to the random absolutely continuous invariant measure of the system. It is natural to ask whether the subsequent Oseledets spaces for the truncated systems converge to the corresponding Oseledets spaces for the full system. We are not yet able to answer this, although the current paper represents a substantial step in this direction.

In [\cite=FGTQ], we viewed the Ulam projections of the Perron-Frobenius operator as perturbations of the original operator, and showed that the top Oseledets space was robust to the kind of projections that were being considered. In this paper, we prove convergence the of subsequent Oseledets spaces under certain perturbations, but do this in the context of matrices instead of infinite-dimensional operators.

In general, Lyapunov exponents and Oseledets subspaces are known to be highly sensitive to perturbations. A mechanism responsible for this is attributed to Mañé; see also [\cite=ArnoldCong-simple]. Ledrappier and Young considered the case of perturbations of random products of uniformly invertible matrices [\cite=LedrappierYoung]. This followed related work of Young in the two-dimensional setting [\cite=Young86]. In view of the sensitivity results, it was necessary to restrict the class of perturbations that they considered, and they dealt with the situation where the distribution of the perturbation of the matrix at time 0 was absolutely continuous (with control on the density) conditioned on all previous perturbations. The simplest instance of this is the case where the matrices to be multiplied are subjected to additive i.i.d. absolutely continuous noise. In this situation, they showed that the perturbed exponents converge almost surely to the true exponents as the noise is shrunk to 0. While they did not directly address the Oseledets subspaces, work of Ochs shows that convergence of the Lyapunov exponents implies convergence in probability of the Oseledets subspaces [\cite=Ochs].

In this paper, we deal with the case of uniform i.i.d. additive noise in the matrices, but make no assumption on invertibility. The conclusions that we obtain are the same as may be obtained in the invertible case. Our argument first demonstrates stability of the Lyapunov exponents, and then shows stability of the Oseledets subspaces. The first part is closely based on Ledrappier and Young's approach, although we need to do some non-trivial extra work to deal with the lack of uniform invertibility (in Ledrappier-Young's argument, in one step, there is an upper bound to the amount of damage that can be done to the exponents, whereas in the non-invertible case there is no such bound). The second part of the argument is completely new. The methods of Ochs cannot be made to work here because they are based on finding the space with the smallest exponent and then using tensor products to move up the ladder. In the case where the smallest exponent is -    ∞  , when one takes tensor products, all products with this subspace have exponent -    ∞   so there is no distinguished second-smallest subspace. To get around this problem, we use the Grassmannian in place of the exterior algebra. We study evolution of subspaces in the Grassmannian, and show that this is controlled by fractional linear transformations. An important role is played by a higher-dimensional analogue of the cross ratio.

We are hopeful that the techniques that we introduce to control evolution of these Oseledets subspaces under the matrices may be applied much more widely.

Statements of the main results

If [formula] is a measure-preserving transformation of a probability space and [formula] is a measurable matrix-valued function, we let [formula] denote the product [formula]. We call the tuple [formula] a matrix cocycle.

Let [formula] (here and throughout the paper, the norm of a matrix is its operator norm). Let [formula]. We write  = (ω,Δ) for a typical element of [formula]. If Ω is equipped with the measure [formula], we equip [formula] with the measure [formula], where λ is the uniform measure on U and [formula] is the product measure. We fix an ε > 0. Then for an element ∈, the corresponding sequence of matrices is [formula]. This paper is concerned with a comparison of the properties of the matrix cocycle [formula] with those of the matrix cocycle (,,,Aε) as ε  →  0.

The main result of this paper is the following.

Let σ be an ergodic measure-preserving transformation of [formula] and let [formula] be a measurable map such that [formula].

Let the Lyapunov exponents of the matrix cocycle be [formula] with multiplicities [formula] and let the corresponding Oseledets decomposition be [formula].

Let D0 = 0, [formula] and let the Lyapunov exponents (with multiplicity) be [formula], so that μi  =  λk if Dk - 1 < i  ≤  Dk.

(Convergence of Lyapunov exponents) Let the Lyapunov exponents of the perturbed matrix cocycle (,,,Aε) (with multiplicity) be [formula]. Then [formula] for each i as ε  →  0.

(Convergence in probability of Oseledets spaces) Let [formula]. Let ε0 be such that [formula] for each i for all ε  ≤  ε0. For ε  <  ε0, let [formula] denote the sum of the Lyapunov subspaces having exponents in the range (λi  -  τ,λi  +  τ). Then [formula] converges in probability to Yi(ω) as ε  →  0.

Outline of the paper

Section [\ref=sec:prelim] introduces terminology, background results and a collection of lemmas that will be used in the proof of the main result. Theorem [\ref=thm:main][\eqref=part:expts] is established in Section [\ref=sec:Exponents], and part [\eqref=part:spaces] is proven in Section [\ref=sec:Spaces].

Preliminaries

For two subspaces, U and V, of [formula] of the same dimension, we define [formula], where dH denotes Hausdorff distance and B is the unit ball. For two subspaces U and W of complementary dimensions, we define [formula], where S denotes the unit sphere. Thus [formula] is a measure of complementarity of subspaces, taking values between 0 and 1, with 0 indicating that the spaces intersect and 1 indicating that the spaces are orthogonal complements. Note that [formula].

Let sj(A) denote the jth singular value of the matrix A and let Ξj(A) denote [formula]. Note that [formula], so that Ξj(AB)  ≤  Ξj(A) + Ξj(B).

The structure of the proof of the main theorem closely follows that of Ledrappier and Young, in which the orbit of ω is divided into blocks of length ≈  | log ε|. These are classified as good if a number of conditions hold (separation of Lyapunov spaces, closeness of averages to integrals etc.) and bad otherwise. The crucial modifications that we make are in estimations for the bad blocks. In the case of [\cite=LedrappierYoung], the matrices (and hence their perturbations) have uniformly bounded inverses, so that for bad blocks one can give uniform lower bounds on the contribution to the singular value. By contrast, here, there is no uniform lower bound. Upper bounds are straightforward, so all of the work is concerned with establishing lower bounds for the exponents. Absent the invertibility, a similar argument would yield (random) bounds of order log ε, which turn out to be too weak to give the lower bounds that we need.

Given a matrix A with the property that sj + 1(A) < sj(A), we define [formula] to be the space spanned by the (j + 1)st to dth singular vectors and [formula] to be the space spanned by the images of the 1st to jth singular vectors under A. If one has a matrix cocycle with base space Ω and matrices Aω, we use the very similar notation [formula] and [formula] to refer to the Oseledets subspaces. The convention will be that if the argument is a matrix, then they refer to the span of the bottom singular vectors or the images of the top singular vectors, while if the argument is a point of the base space, they refer to spaces appearing in the Oseledets theorem. These spaces are obtained simply as limits of spans of singular vectors, as explained in Lemma [\ref=lem:SVs] below, justifying the notation.

Suppose that the unperturbed system has exponents satisfying λj  >  λj + 1.

Then for almost every ω, [formula] and [formula] as n  →    ∞  .

The statement that [formula] follows from the proof of Oseledets' theorem given in [\cite=ArnoldRDSBook]. The singular value decomposition ensures that [formula], where A* denotes the adjoint of A.

On the other hand, a similar statement is true for the spaces [formula] and [formula]. More precisely, we claim that if we let [formula] be the Oseledets spaces for the dual cocycle with base σ- 1 and generator G(ω) = A(σ -  1ω)*, then [formula]. Applying Oseledets' theorem to the dual cocycle, we obtain [formula].

To prove the claim, suppose for a contradiction that there exist [formula] and [formula] of unit length such that 〈f(ω),e*(ω)〉  ≠  0. By invertibility of [formula] as a map from [formula] to [formula] there exist for all n, unit vectors [formula] such that [formula] is a multiple of f(ω). Then,

[formula]

The right hand side grows at a rate slower than λj. The left hand side grows at a rate at least λj, by [\cite=FLQ1]. This yields a contradiction, so [formula]. Since the dimensions agree, they coincide.

Now the statement that [formula] follows directly from the fact that [formula], and continuity of [formula].

For any δ > 0, there exists a K such that if (i) the ratio of the jth to (j + 1)st singular values of a matrix A exceeds K; (ii) the jth singular value exceeds K; and (iii) [formula], then the following hold:

[formula] and [formula] are less than δ / 3;

[formula] for each i  ≤  j;

If V is any subspace of dimension j such that [formula], then [formula];

If V is a subspace of dimension j and [formula], then | det (A|V)|  ≥  (Dδ)j exp Ξj(A), where D is an absolute constant.

Let σ be an ergodic measure-preserving transformation of [formula] and let [formula] be a measurable map such that [formula] is integrable. There exists C such that for all η0 > 0, there exists ε0 such that for all ε  <  ε0, there exists G  ⊆  Ω of measure at least 1 - η0 such that for all ω∈G, and all [formula] where N = ⌊C| log ε|⌋.

Let [formula] and let C > 0 satisfy [formula]. Notice that provided ε < 1 (and using the fact that the perturbations have norm bounded by ε), [formula], and

[formula]

There exists n0 such that for N  ≥  n0, [formula] on a set of measure at least 1 - η0. In particular, provided ⌊C| log ε0|⌋ > n0, taking N = ⌊C| log ε|⌋, the conclusion follows.

Let σ be an ergodic measure-preserving transformation of [formula] and let [formula] be a measurable map such that [formula]. Let the Lyapunov exponents (with multiplicity) be [formula]. Suppose further that μj  >   max (0,μj + 1).

Let η0 > 0 and δ1 > 0 be given. Then there exist n0 > 0, κ > 0 and δ  ≤   min (δ1,κ) such that: for all n  ≥  n0, there exists a set G  ⊆  Ω with [formula] such that for ω∈G, we have

[formula];

[formula];

[formula];

[formula] and [formula], where K(δ) is as given in Lemma [\ref=lem:LY].

From the proof of Oseledets' theorem, we know [formula] is a positive measurable function. Hence there exists κ > 0 such that [\eqref=it:sep] occurs on a set of measure at least 1 - η0 / 4. Let δ  =   min (δ1,κ).

From the proof of Oseledets' theorem, there exists an n1 > 0 such that for all n  ≥  n1, [formula] and [formula] hold on sets of measure at least 1 - η0 / 4. Hence there is a set of measure at least 1 - η0 / 4 where [\eqref=it:Econt] holds. Similarly, using shift-invariance, there is a set of measure at least 1 - η0 / 4 where [\eqref=it:Fcont] holds.

Since [formula] and [formula], [\eqref=it:svsep] holds on a set of measure at least 1 - η0 / 4 for all n  ≥  n2 for some n2 > 0. Now let n  ≥  n0  =   max (n1,n2). Intersecting the above sets gives a set G satisfying the conclusions of the lemma.

Convergence of Lyapunov exponents

Most of the work in this part is concerned with showing the inequality

[formula]

We also prove

[formula]

which is fairly straightforward using sub-additivity. These facts, combined with the fact that the [formula] and μi are decreasing in i are sufficient to establish the claim that [formula] for each i.

To see this, suppose that [\eqref=eq:liminf] and [\eqref=eq:limsup] hold. Let [formula] and let [formula]. By [\eqref=eq:liminf] and [\eqref=eq:limsup], we have [formula]. If λi + 1 =  -   ∞  , we see [formula] for all j > Di from [\eqref=eq:limsup]. Hence we may assume that λi + 1 >  -   ∞  . Since the exponents are arranged in decreasing order, (Hj(ε))dj = 1 is a 'concave' sequence for each ε (that is Hj + 1(ε) - Hj(ε)  ≤  Hj(ε) - Hj - 1(ε) for each j in range), as is (hj)dj = 1. However, hj is an arithmetic progression for j in the range Di to Di + 1 - 1. Since a concave function is bounded below by its secant, we deduce lim inf ε  →  0Hj(ε)  ≥  hj for Di  ≤  j < Di + 1. Hence we see Hj(ε)  →  hj as ε  →  0 for each j, from which the statement follows.

To show [\eqref=eq:limsup], let χ > 0 and let 1  ≤  j  ≤  d. By the sub-additive ergodic theorem, there exists an N > 0 such that [formula]. Now for sufficiently small ε > 0, [formula] for all ∈. In particular, this shows that Hj(ε) < hj  +  χ for all sufficiently small ε as required. Notice that this part of the argument is completely general, whereas the lower bound depends on the particular properties of the matrix perturbations.

We now focus on proving [\eqref=eq:liminf]. Let j = Di, noting that by the above, we may assume that λi >  -   ∞  . By multiplying the entire family of matrices by a positive constant, we may assume that μj  =  λi  ≥  0.

Let χ > 0 be arbitrary. Let D be the absolute constant occurring in the statement of Lemma [\ref=lem:LY], C be as in the statement of Lemma [\ref=lem:Clogeps] and K be the constant occurring in the statement of Lemma [\ref=lem:trans]. Define a constant η > 0 by

[formula]

Let n0, κ and δ be the quantities given by Lemma [\ref=lem:goodblocks] using [formula] and η0  =  η / 2. Since [formula], it follows that [formula].

Let N(ε) = ⌊C| log ε|⌋, where C is as above. The fact that N scales like | log ε| will be of crucial importance later. Let ε0 be the quantity appearing in Lemma [\ref=lem:Clogeps] with η0 taken to be η / 2.

Let ε be sufficiently small that

[formula]

Let G be the intersection of the good set given by Lemma [\ref=lem:Clogeps] with the good set given by Lemma [\ref=lem:goodblocks] with n taken to be N = N(ε), so that [formula]. If ω∈G, we say the matrix product [formula] is a good block.

Now we divide everything into blocks of length N and estimate the sum of the logarithms of the first j singular values of the ε-perturbed cocycle.

We will bound from above the difference between the sum of the logs of the first j singular values in the unperturbed system and this sum in the perturbed version. We informally speak of the costs due to various contributions. That is, estimates of various contributions to an upper bound for the difference (unperturbed)-  (perturbed). These costs are estimated in the following parts.

To deal with the concatenation of good blocks, we give an upper bound for the difference (sum of individual block exponents)  -  (exponent of concatenated block). This is estimated using Lemma [\ref=lem:LY]. Over the whole block there is a cost of at most log (3 / Dδ), so a cost per index of O(1 / | log ε|).

Reduction of singular values within bad blocks. There is an expected cost of at worst 1.28d2j per index in a bad block from [\eqref=eq:intbadblox].

Reduction of singular values at the first and last matrix of a string of bad blocks. Here, there is an upper bound in expected cost of approximately | log ε| per bad block. Here is where it is crucial that the blocks are of length O(| log ε|). The upper bound for the cost averages out at O(1) per index in each bad block. The argument is saved by the fact that most blocks are good blocks.

The sum of the costs is O(η) + O(1 / | log ε|) per index (η being the frequency of bad blocks), which will allow us to derive [\eqref=eq:liminf]. Let us proceed with the details.

Suppose k < l and [formula]. Let [formula] and [formula] We then claim that

[formula]

where D is the absolute constant appearing in Lemma [\ref=lem:LY].

This is proved inductively using Lemma [\ref=lem:LY]. Recall that [formula]. We let [formula] and define Vn + 1 = BnVn and ṽn + 1  =  nṽn.

We claim that the following hold:

[formula] for each n;

[formula] and [formula] for each n.

Item [\eqref=it:ind1] and the first part of [\eqref=it:ind2] hold immediately for the case n = k. The second part of [\eqref=it:ind2] holds because [formula] and [formula] by Lemma [\ref=lem:LY].

Given that [\eqref=it:ind1] and [\eqref=it:ind2] hold for n = m and that Bm is a good block, Lemma [\ref=lem:LY] implies that [formula], [formula] and [formula], so that [formula], yielding [\eqref=it:ind1] for n = m + 1.

Finally, by the induction hypothesis and Lemma [\ref=lem:LY], we have [formula], and by Lemma [\ref=lem:goodblocks], [formula]. Thus, we obtain [\eqref=it:ind2] for n = m + 1.

Hence using Lemma [\ref=lem:LY][\eqref=it:det], we see that [formula].

Since [formula], multiplying the inequalities and taking logarithms gives the result.

Let [formula] be an arbitrary sequence of matrices. We write

[formula]

and prove that (gε)- is integrable in [formula] and that

[formula]

for all [formula].

There exists B  ≈   - 1.28 such that for all [formula], and all l  ≥  0

Let us show there exists a lower bound; its precise value is irrelevant for our purposes. Since for every [formula], we have that [formula], it suffices to show the lemma holds for [formula]. For z = 0 the integral is 0. Let us assume [formula], and let log -x: =  min (0, log x). Then,

[formula]

The function [formula] for z  ≠  0 and g(0): = 0 is continuous on [formula], and hence bounded on

[formula]

Let B be as in Lemma [\ref=lem:linear] and p be a polynomial. Then, for all l  ≥  0,

If p(0) = 0, then the result is clear. Otherwise, we consider the polynomial f(t) = p(t) / p(0) and demonstrate that [formula].

To see this, notice that f(t) may be expressed as [formula], where [formula] is the set of roots of f, and hence p, with multiplicity. Applying Lemma [\ref=lem:linear] then gives the result.

Let B be the constant from the statement of Lemma [\ref=lem:linear]. Let P(t) be a degree j matrix-valued polynomial. That is, P(t) may be expressed as [formula] for some collection of d  ×  d matrices Ak. Then, for all l  ≥  0,

If P(0) is the zero matrix, the result is trivial. Otherwise, there exist unit vectors [formula] and [formula] such that [formula].

If we set [formula], then we have [formula] and [formula], so the result follows from Lemma [\ref=lem:poly].

Let B  ≈   - 1.28 be the constant from the statement of Lemma [\ref=lem:linear]. Let L, M, A and R be arbitrary d  ×  d matrices. Then

Notice that Λj(L(A + tM)R) is a polynomial family of operators on [formula]. Taking the standard orthogonal basis of [formula], let P(t) be the matrix of Λj(L(A + tM)R). The result then follows by applying Lemma [\ref=lem:operator].

We obtain [\eqref=eq:intbadblox] by a telescoping argument:

[formula]

Recall that [formula]. We estimate the integral of the kth term in the sum. Let [formula] and [formula]. Regarding [formula] as fixed, we need to estimate:

where [formula]. We then disintegrate the measure λ radially, so that dλ = d2td2 - 1  dt  ·  d(∂λ)(H) where Δ = tH, H takes values in [formula] and ∂λ is the boundary measure. For a fixed H, the quantity to estimate is

Since this quantity is uniformly bounded above, by Lemma [\ref=lem:magicunifbd], we obtain [\eqref=eq:intbadblox].

Let L, R and A be given matrices. Then Ξj(L(A + εΔ)R) - (Ξj(L) + Ξj(R)) is has integrable negative part as a function of Δ and has integral bounded below by K log ε, where K is independent of L, A and R.

Write L = O1D1O2 where D1 is diagonal with entries arranged in decreasing order and O1 and O2 are orthogonal. Similarly write R = O3D2O4. Let A' = O2AO3 and Δ' = O2ΔO3. Then we have

[formula]

Using the inequality Ξj(AB)  ≤  Ξj(A) + Ξj(B) and setting C to be the diagonal matrix with 1's in the first j elements of the diagonal and 0's elsewhere, we have

[formula]

The equality between the second and third lines arises because the matrices D1C, C(A' + εΔ')C and CD2 and their product have non-zero entries only in the top left j  ×  j submatrix. For such matrices, Ξj(  ·  ) is numerically equal to the logarithm of the absolute value of the determinant of the submatrix. Since the determinant is multiplicative, the equality follows.

Since Lebesgue measure on [formula] is preserved by the operations of pre- and post-multiplying by an orthogonal matrix, it suffices to show that there exists K > 0 such that for any matrix A,

[formula]

Let A'' be the top left j  ×  j submatrix of A' and notice that the measure on the top left j  ×  j submatrix of Δ is absolutely continuous with respect to the measure on j  ×  j matrices with uniform entries in

[formula]

and λ' is the uniform measure on [formula]. One checks, thinking of the columns of U being generated one at a time, that the probability that the ith column lies within a δ-neighbourhood of the span of the previous columns is at most O(δ  /  ε), so the probability that the determinant of A'' + εU is less than δj is O(jδ  /  ε). Hence we obtain Using the estimate for non-negative random variables [formula], we obtain the bound [formula].

From this, we obtain the O(| log ε|) bound as required.

We apply this by grouping each consecutive string of good blocks into a single matrix (and using [\eqref=eq:goodblox]) and also grouping strings of consecutive bad blocks minus the first and last matrices into a single matrix (and using [\eqref=eq:intbadblox]). The first and last matrices of a string of bad blocks are then handled with Lemma [\ref=lem:trans].

More specifically, we condition on ω∈Ω and calculate [formula]. Let [formula]. Let r(ω) = |S| and [formula] be the increasing enumeration of S. Also let b0 =  - 1 and br + 1 = M. Then we factorize [formula] and [formula] as

[formula]

where Gi = A((bi + 1 - bi - 1)N)σ(bi + 1)Nω, i  =  Aε((bi + 1 - bi - 1)N)σ(bi + 1)Nω, [formula] and [formula] (so the Gi are products of consecutive good blocks and Bi are (single) bad blocks). We further factorize Bi and i as [formula] and Bi = Aσ(bi + 1)N - 1ωCiAσbiNω, where i  =  Aε(N - 2)σbiN + 1ω and Ci = A(N - 2)σbiN + 1ω.

Now using Lemma [\ref=lem:trans] (and the constant K from its statement), we have

From [\eqref=eq:goodblox], we have [formula] for all values of the perturbation matrices that occur inside those blocks. From [\eqref=eq:intbadblox], we have for each 1  ≤  i  ≤  r(ω), Letting E(ω) = Mj log (Dδ / 3) - 1.28d2(N - 2)jr(ω) + r(ω)K log ε and combining the inequalities together with subadditivity of Ξj yields

[formula]

By [\eqref=eq:etachoice] and [\eqref=eq:Ncond], we see [formula]. Finally, we have

Combining these inequalities, we obtain

Taking the limit as M  →    ∞  , we deduce [formula]. Since χ > 0 was arbitrary, we deduce [\eqref=eq:liminf].

Convergence of Oseledets spaces

From Theorem [\ref=thm:main][\eqref=part:expts], we have established the existence of an ε0 > 0 such that for ε  <  ε0, in the perturbed matrix cocycle, [formula] for all j satisfying Di - 1 < j  ≤  Di. Recall that [formula] was defined to be the sum of the Oseledets spaces corresponding to exponents in the range (λi  -  τ,λi  +  τ), with Yi(ω) being the corresponding spaces for the unperturbed matrix cocycle. Let [formula] be the fast subspace for the unperturbed matrix cocycle and [formula] be the slow subspace. We similarly introduce notation [formula] and [formula] in the perturbed matrix cocycle. Notice that  = (ω,Δ), and so [formula] and [formula] may be regarded as living on the same probability space [formula].

The proof of Theorem [\ref=thm:main][\eqref=part:spaces] will follow relatively straightforwardly from the following lemma whose proof will occupy this section.

Let 0 < χ < 1. Let [formula] and [formula] be as above, corresponding to the largest Di Lyapunov exponents of the unperturbed and perturbed cocycles, respectively. Then, for every ε sufficiently small,

[formula]

In particular, [formula] converges in probability to [formula] as ε  →  0.

Recalling that [formula], where [formula] denotes the Oseledets space of the cocycle dual to A, Lemma [\ref=lem:convSpaces] immediately implies the following.

Let [formula] and [formula] denote the slow Oseledets subspaces of the unperturbed and perturbed cocycles, respectively, as described above. Then [formula] converges in probability to [formula] as ε  →  0.

Notice that [formula] and [formula], so we want to show that [formula] converges in probability to [formula] as ε  →  0. We also have

[formula]

Now, lemma 6 of [\cite=FLQ2], together with Lemma [\ref=lem:convSpaces] and the separation of [formula] and [formula] guaranteed by Lemma [\ref=lem:goodblocks] do the job.

Strategy and notation

Throughout, we shall let j = Di, so that we are studying evolution of j-dimensional subspaces. In order to show Lemma [\ref=lem:convSpaces], we will assume that all of the perturbations (Δn) are fixed except for the - 1 time coordinate. That is, we compute the probability that the perturbed and unperturbed fast spaces are close conditioned on (Δn)n  ≠   - 1 and ω.

We think of [formula] as a random variable (depending on Δ- 1), then applying the sequence of matrices (all already fixed), [formula], show that the resulting j-dimensional subspace is highly likely to be closely aligned to [formula].

To control the evolution, we successively apply [formula], [formula], where  = (ω,Δ), s denotes the left shift and () = (σω,sΔ). We will assume that the underlying blocks of unperturbed A's are good blocks. The number, n, of steps will be fixed. In fact, n will depend on the difference λj  -  λj + 1 and the quantity, C, appearing in Lemma [\ref=lem:Clogeps]. Hence for small η, it will be very likely that one has n consecutive good blocks.

We shall use the following parameterization of the Grassmannian of j-dimensional subspaces of [formula]. Let [formula] be a basis for F, a j-dimensional subspace, and [formula] a basis for E, a complementary subspace. Now for any j-dimensional vector space V with the property that [formula], each fk can be uniquely expressed in the form [formula] where vk∈V. The parameterization of V with respect to (F,E) chart (or more formally with respect to chart arising from F and E with their chosen bases) is the matrix B = (bik)1  ≤  i  ≤  d - j,1  ≤  k  ≤  j. Conversely, given the matrix B, one can easily recover a basis for V: [formula] and hence the subspace V.

Let F and E be orthogonal complements in [formula] and let (fi)1  ≤  i  ≤  j and (ei)1  ≤  i  ≤  d - j be orthonormal bases. Let V be a j-dimensional subspace of [formula] such that [formula]. Let the parameterization of V be B. Then In particular for any M > 1, [formula] implies [formula].

Let [formula] so that (vk) forms a basis for V. Now let [formula] belong to [formula], so that [formula]. The closest point in [formula] to v is [formula], which is at a square distance [formula] from v. This distance is minimized when c is the multiple of the dominant singular vector of B for which [formula]. That is, [formula] and [formula]. Substituting this, we obtain the claimed formula for [formula].

We will do the iteration using the following steps:

Express [formula] as a matrix B using the [formula] chart. Set i = 0.

Let [formula]. Compute Ci(V) in the [formula] chart; this is straightforward as Ci is diagonal with respect to the pair of bases on the domain and range spaces.

Change bases to the [formula] chart. Update V to Ci(V), increase i and repeat steps 1 and 2 a total of n times.

We will see that above transformations are given by fractional linear transformations on matrices, and use this formalism, together with properties of multivariate normal distributions to establish the fact that the fast Oseledets space for the perturbed matrix cocycle is with high probability close to the fast Oseledets space for the unperturbed matrix cocycle.

Recall that, once the initial cocycle [formula] and j are fixed, the Lyapunov exponents λj,λj + 1 and the constant C appearing in Lemma [\ref=lem:Clogeps], are fixed as well. Fix n satisfying

[formula]

We apply Lemma [\ref=lem:goodblocks] with η0  =  χ / (2n + 2) and δ1  =  χ / 2. Let κ, δ, G and n0 be as in the conclusion of the lemma.

We now fix the range of ε in which we will obtain the required closeness of the top spaces. We shall set N = ⌊C| log ε|⌋, and will require that N be large enough (and hence that ε should be small enough) to simultaneously satisfy a number of conditions:

N  >  n0;

eNτ > 8 / δ2 (and since δ < 2, eNτ > 1 + 2 / δ);

exp (N(n(λj  -  λj + 1 - 6τ) - 1 / C)) > 4(eπ / 2)d2 / 2j3 / 2  /  χ;

[formula];

N >  log (2 / δ) / τ;

eN( - λj  +  λj + 1 + 2τ) < 5δ2 / (1 + δ).

Let [formula]. Then [formula]. Furthermore, we have the following result, whose proof is deferred until §[\ref=sec:pfClaim].

Assume N satisfies conditions [\eqref=cond:goodblocks]-[\eqref=cond:dunno]. Then,

[formula]

With this result at hand, the proof of Lemma [\ref=lem:convSpaces] may be immediately concluded, as follows.

[formula]

Proof of Claim [\ref=claim:conv4good]

Let [formula], which we consider to be a random variable by fixing all matrices except the - 1st, and let Vi + 1 = Ci(Vi). Write Bi for the matrix of Vi with respect to the [formula] basis, as explained in Step [\ref=step:newzero] above; so that in particular, B0 is a random variable with ε-variability.

Let Ri be the matrix describing multiplication by Ci with respect to the [formula] and [formula] bases. This corresponds to Step [\ref=step:newone]. Let Pi (corresponding to Step [\ref=step:newtwo]) be the basis change matrix from the [formula] to the [formula] basis.

Then, Ri is diagonal, say

[formula]

where D1,i is the diagonal matrix with entries [formula] and D2,i is the diagonal matrix with entries [formula], where [formula] are the singular values of Ci. Notice that the ratio between the largest entry of D2,i and the smallest entry of D1,i is at least e(λj  -  λj + 1  -  τ)N.

Notice that Pi is an orthogonal matrix, as it is the change of basis matrix between two orthonormal bases. Let

[formula]

We use a similar argument to that in Lemma [\ref=lem:perpcalc] to estimate [formula]. From the definition of the good set G (after [\eqref=eq:Ncond]), we know [formula]. Let [formula] be spanned by the singular vector images [formula]; [formula] be spanned by the singular vectors [formula] and [formula] be spanned by [formula]. In particular, if [formula] and [formula], then with respect to the ((gk),(hk)) basis, v has coordinates (ζia,βia). The nearest point in the unit sphere of [formula] has coordinates [formula] with respect to the (hk) vectors. The distance squared between the two points is, by the calculation in Lemma [\ref=lem:perpcalc], [formula]. By the goodness property, this exceeds 72δ2, so that [formula] and [formula]. In particular, we deduce

[formula]

Let us also note that the pi, qi, ri and si depend only on the choice of matrices from time 0 onwards and hence have been fixed by the conditioning, whereas B0 is a random quantity whose conditional distribution we will study in §[\ref=sec:DistB0].

Notice that the matrix Qi is characterized by the property that if the coordinates of [formula] with respect to the [formula] basis are given by z, then the coordinates of [formula] are given by Qiz with respect to the [formula] basis.

Set F0 = I, H0 = B0. Let

[formula]

Recall that Bi is the matrix of Vi with respect to the [formula] basis. Then, we have

[formula]

To see this, consider a point x of V expressed in terms of the [formula] basis as z. Then with respect to the [formula] basis, [formula] has coordinates [formula]. [formula] has a basis expressed in coordinates of the [formula] basis given by the columns of [formula]. Post-multiplying by F- 1i gives an alternative basis for [formula] expressed in terms of the [formula] basis, as the columns of [formula] as required.

In view of Lemmas [\ref=lem:LY] and [\ref=lem:perpcalc], it remains to show that Bn = HnF- 1n is controlled for most choices of the perturbation Δ- 1.

Let

[formula]

where the W's are j  ×  j, X's are j  ×  (d - j), Y's are (d - j)  ×  j and Z's are (d - j)  ×  (d - j).

Singular values and invertibility of W(n)

Let us now show that all j singular values of W(n) are bounded below by a quantity close to enNλj. This will immediately imply invertibility of W(n). We start by proving [formula] for all x and k  ≥  0.

By ([\ref=cond:del2]), if c > δ, then

[formula]

Notice that

Suppose [formula]. Then we have

[formula]

where we used [\eqref=eq:zetabd] to obtain the third inequality. Similarly

[formula]

Using the facts that [formula], and [formula] for all x, and assuming that [formula], we see that [formula], where

[formula]

Everything has been set up so that if ck  >  δ, then ck + 1  >  δ. It is easy to check that c0  >  δ (W(0) = I and Y(0) = 0) so we deduce that

[formula]

for all x and k as required. From this, we see that

[formula]

where for the last inequality, we used ([\ref=cond:dunno]).

In particular we deduce inductively that if x  ≠  0, then W(n)x  ≠  0 for all n. Thus, W(n) is invertible and we see

[formula]

Recursion for En

Let En = Z(n) - Y(n)W(n)- 1X(n). This matrix will play a role in bounding Bn. Notice that

[formula]

In fact, En may be defined this way: En is the unique lower submatrix M such that there exists A satisfying

Now we have To finalise the recursion setup, we now seek matrices B and C such that

[formula]

Combining the above, we see so that En + 1 = rnEn + C.

From [\eqref=eq:Ecalcpart2], we see that B =  - W(n  +  1)- 1snEn, so that C = Y(n + 1)B =  - Y(n + 1)W(n  +  1)- 1snEn. In particular, we obtain Substituting x = W(k)- 1z in [\eqref=eq:W>Y], we obtain [formula], so that [formula] is uniformly bounded by 2 / δ. Furthermore, from the definition of Qk, [\eqref=eq:Qi], and the choice of N, [formula], so that [formula]. Hence by ([\ref=cond:8del]), we obtain

[formula]

Finally, using [\eqref=eq:En], we have that

[formula]

Distribution of B0

Recall that we conditioned on ω and (Δn)n  ≠   - 1. This determines the top subspace at time - 1, as well as the [formula] and [formula] spaces for each k  ≥  0.

Let V be a d  ×  j matrix whose columns consist of an orthonormal basis in [formula] for the fast space at time - 1. The fast space at time 0 has a basis given by the columns of (Aσ- 1ω  +  εΔ- 1)V (recall that Δ was assumed to be independent of the other perturbations of [formula] that have already been fixed). For this section, we write A in place of Aσ- 1ω and Δ in place of Δ- 1. The coordinates of (A + εΔ)V in terms of the [formula] basis are given by where F is the matrix whose column vectors are the (orthonormal) basis for [formula] and E is the matrix whose column vectors are the orthonormal basis for E. Specifically, the jth column of this matrix gives the [formula] coordinates of the image of the jth basis vector of V under A + εΔ. The matrix B0 is then given by Z2Z- 11, that is (ET(A + εΔ)V)(FT(A + εΔ)V)- 1.

Bounds on Bn

Substituting the expression for B0 into [\eqref=eq:B_nE_n], we get

[formula]

where U = W(n)FT + X(n)ET.

We make the following definitions:

[formula]

Now we have

[formula]

What remains is to give an upper bound on [formula] and to show that [formula] is small for a large set of Δ's.

Bounds on [formula] using multivariate normal random variables

For a T > 0 to be fixed below, let R be the subset of d  ×  d matrices C with entries in

[formula]

, so that [formula] (where [formula] is the volume of R as a subset of [formula]). Similarly [formula] since the probability density function of d  ×  d matrices taking values in

[formula]

[formula]

Notice that the entries of the matrix D + εUZV have a multivariate normal distribution. Any two such distributions with the same means and covariances are identically distributed. Recall that V is a d  ×  j matrix whose columns are pairwise orthogonal. A consequence of this is that ZV has the same distribution as a d  ×  j matrix of independent standard normal random variables. To see this, we see immediately that the expectation of each entry is 0. We then need to check the covariances, recalling that the columns of V are orthonormal, we get:

[formula]

as required.

Let Z' = ZV. We next observe (by an identical calculation) that FTZ' and ETZ' are distributed as independent j  ×  j and (d - j)  ×  j matrices with independent standard normal entries. Let Z1 = FTZV and Z2 = ETZV. Recall that   =  D + εUZV and U = W(n)FT + X(n)ET. By [\eqref=eq:colsep], we are interested in the columns of  = D + εW(n)Z1  +  εX(n)Z2.

For a fixed i, we compute the probability that the distance of the ith column of [formula] is distant at least [formula] from the span of the other columns. We give a uniform estimate on this probability conditioned on the columns of Z1 other than the ith and the value of Z2. Having fixed all of this data, let [formula] be a unit normal vector to the (j - 1)-dimensional space spanned by the other columns (a constant given the data). We then want to estimate [formula], where the superscript (i) indicates we are considering the ith column.

Let [formula] and [formula] (both are constant given the data on which we conditioned). We are therefore interested in [formula]. This is bounded above by [formula]. More multivariate normal machinery tells us that the distribution of [formula] has the same distribution as [formula] times a standard normal random variable, so we want to estimate [formula], where Z0 is a standard normal random variable. Simple estimates show this is less than [formula], which, in turn, is bounded above by [formula]. Hence, [formula].

Using [\eqref=eq:Nunifcomp], we obtain

[formula]

Final estimates

We showed in [\eqref=eq:sjWnbd] that sj(W(n))  ≥  enN(λj - 2τ). Recall also the expression for Bn given in [\eqref=eq:BnFinal]. From [\eqref=eq:Enbounds] and [\eqref=def:MD], we have the upper bounds: [formula], after recalling that the columns of E and V are orthonormal. Combining this with the results of §[\ref=sec:NormalBounds] and [\eqref=eq:BnFinal] we obtain that

[formula]

Recalling that N = C| log ε| and [formula], and specialising [\eqref=eq:BoundBn] to T = 1 / δ, we get

[formula]

where we used ([\ref=cond:eps]) and ([\ref=cond:prob]) for the final inequality.

Fix ∈ and suppose that ω∈ and [formula]. Then Lemma [\ref=lem:perpcalc] shows that implies [formula].

We extract three conclusions from the fact that σnNω∈G. Recall that δ  ≤  δ1  =  χ / 2. Lemma [\ref=lem:LY][\eqref=it:contr] yields that [formula]. Next, the hypotheses of Lemma [\ref=lem:LY] are satisfied with [formula] and [formula]. Conclusion [\eqref=it:spacecont] tells us that [formula]. Finally we have [formula] from the definition of G. Combining these we get

[formula]

By [\eqref=eq:BoundBnDelta], [formula]. Thus,

[formula]

We have therefore established [\eqref=eq:toprove], and Claim [\ref=claim:conv4good] is proved.

Acknowledgments

The research of GF and CGT is supported by an ARC Future Fellowship and an ARC Discovery Project (DP110100068). AQ acknowledges NSERC, ARC DP110100068 for travel support and UNSW for hospitality during a research visit in 2012.