Decentralized Coded Caching with Distinct Cache Capacities

Introduction

The ever-increasing mobile data traffic has imposed a great challenge on the current network architectures. The growing demand is typically addressed by increasing the achievable data rates; however, moving content to the network edge has recently emerged as a promising alternative solution as it reduces both the bandwidth requirements and the delay. The use of edge caching is further motivated by the continuous drop in the cost of memory. In this paper, we consider an extreme form of edge caching, in which contents are stored directly at user terminals in a proactive manner. Proactive caching of popular content during off-peak traffic periods also helps flattening the high temporal variability of traffic. [\cite=DowdyCaching] [\cite=AlmerothCacing].

Proactive caching is performed in two phases: The placement phase takes place during off-peak traffic hours, when the resources are abundant, and the users' caches are filled by the server without knowing the future user demands. When the user demands are revealed, the delivery phase is performed, in which a common message is transmitted from the server to all the users over the shared communication channel. Each user decodes its requested file by combining the bits received in the delivery phase with the contents of its local cache. The goal is to minimize the delivery rate, which guarantees that all the user demands are satisfied, independent of the demand combination of the users.

Research on caching over the past decade has mainly focused on the placement phase in order to identify the most popular contents to be cached locally at user terminals [\cite=baev2008approximation] [\cite=BorstCaching]. Recently, coded caching scheme was introduced in [\cite=MaddahAliCentralized] for proactive caching, and it is shown that by storing and transmitting coded contents, and designing the placement and delivery phases jointly, it is possible to significantly reduce the delivery rate compared to uncoded caching.

A centralized caching scenario is studied in [\cite=MaddahAliCentralized], in which the number and the identities of the users are known in advance by the server. This allows coordination of the cache contents across the users during both the placement and delivery phases; such that, by carefully placing pieces of contents in user caches, a maximum number of multicasting opportunities are created for tranmission during the delivery phase. Several other recent work has considered centralized coded caching, and the required delivery rate has been further reduced [\cite=ZhiChenXOR] [\cite=MohammadDenizTCom] [\cite=KaiWanUncodedCaching].

In practice, however, the number or the identity of active users that will participate in the delivery phase might not be known in advance during the placement phase. In such a scenario, called decentralized coded caching, coordination across users is not possible during the placement phase. However, Maddah-Ali and Niesen proposed a scheme that randomly caches parts of each content at each user, and can still exploit multicasting opportunities in the delivery phase, albeit limited compared to the centralized setting [\cite=MaddahAliDecentralized]. Decentralized coded caching has been studied in various other settings, e.g., files with different popularities [\cite=NiesenNonuniform] [\cite=JiArXivNonuniform], and distinct lengths [\cite=ZhangDistinctFileSizes], online caching [\cite=PedarsaniOnlineCaching], etc.

Most of the existing literature on coded caching assume identical cache sizes across users. Recently, in [\cite=WangHeterogenous] decentralized caching to users with heterogeneous cache sizes is studied, and by extending the scheme proposed in [\cite=MaddahAliDecentralized] to this scenarios, authors have shown that significant gains can still be obtained compared to uncoded caching. In this paper, we propose a novel decentralized caching algorithm for users with distinct cache capacities. We show that the proposed scheme requires a smaller delivery rate than the one achieved in [\cite=WangHeterogenous]. The simulation results illustrate that the improvement in the delivery rate is more significant when the distribution of the cache capacities across users is more skewed.

The rest of this paper is organized as follows. The system model is introduced in Section [\ref=SystemModel]. In Section [\ref=s:Results], we introduce the proposed coded caching scheme, analyze its performance in terms of the delivery rate. The performance of the proposed caching scheme is compared with the state-of-the-art result, and some numerical results are presented in Section [\ref=s:Comparison]. We conclude the paper in Section [\ref=Conc].

Notations: The set of integers [formula] is denoted by [formula]. Notation [formula] illustrates the bitwise XOR operation. For two sets Q and P, [formula] is a set including the members of Q and excluding the members of P. Notation [formula] represents cardinality of a set or length of a file. We use the notation [formula] to represent the bitwise XOR operation between binary sequences with different lengths. The arguments of [formula] are first zero-padded to have the same length as the longest argument, and then they are bitwise XOR-ed.

System Model

A server with N independent F-bit files, W1,...,WN, is considered, where each file is assumed to be uniformly distributed over [formula]. There are K active users, U1,...,UK, where user Uk is equipped with a cache of capacity MkF bits, with Mk  ≤  N, [formula]. We denote the cache capacities by vector [formula]. Let Zk denote the contents of Uk's cache at the end of the placement phase. Unlike in centralized coded caching [\cite=MaddahAliCentralized], cache contents are independent of the number of users, their identities, or the user requests. User requests are revealed after the placement phase, where [formula] denotes the file requested by user Uk, [formula]. User demands are served simultaneously through an error-free shared link. Let X denote the RF-bit message transmitted over the shared link by the server to enable each user Uk to decode its requested file Wdk, together with its local cache content. Our goal is to characterize the minimum rate R(μ); such that, each user can decode its desired file with arbitrarily small probability of error, independent of the particular demand combination.

Decentralized Coded Caching

We first illustrate our decentralized coded caching scheme on the following example.

Consider a caching system with N  =  2 files W1 and W2, and K  =  4 users. It is assumed that the cache capacity of user Uk is [formula], [formula].

In the placement phase, user Uk caches a random MkF / 2 bits of each file independently. Since there are N = 2 files in the database, a total of MkF bits are cached by user Uk.

When N < K, it can be shown that the worst-case user demands happens when N users with the smallest cache capacities have different requests. For this particular example, we have [formula], and the worst-case happens when users U1 and U2 request distinct files. Hence, we can assume the worst-case demand combination of dk  =  1, if k = 1,3, and dk  =  2, otherwise.

The contents served in the delivery phase are divided into three distinct parts, where Xi is delivered in part i, for i = 1,2,3. Thus, the common message is [formula]. We further divide the message X2 into three pieces X12, X22, and X32. Below, we explain the purpose of each part in detail.

In the first part of the delivery phase, the bits of each requested file which have not been cached by any user are directly delivered by the server. The following contents are delivered in this part. [formula].

The bits of the file requested by a user having been cached by another user are transmitted in the second part of the delivery phase. The server first delivers each user the bits of its requested file which are in the cache of one user with the same request. Then, each user receives the bits of its requested file which are in the cache of a single user with different request. By delivering the following contents, user Uk can obtain the bits of file Wdk having been cached in user Ul, for [formula], such that l  ≠  k. X12 =   [formula] [formula] [formula], [formula] [formula] [formula], X22 =   [formula] [formula] [formula], [formula] [formula] [formula], X32 =   [formula] [formula] [formula].

In the last part, the server delivers the users the bits of their requested files which have been cached by more than one another user. Accordingly, each user Uk, [formula], can obtain all the bits of file Wdk which are in the cache of users in any set [formula], where [formula], after receiving the following contents. X3 =   [formula] [formula] [formula] [formula] [formula], [formula] [formula] [formula] [formula] [formula], [formula] [formula] [formula] [formula] [formula], [formula] [formula] [formula] [formula] [formula], [formula] [formula] [formula] [formula] [formula] [formula] [formula].

After these parts, each user can decode all the missing bits of its desired file. To find the delivery rate, we first note that, by the law of large number, the length of the subfile Wk,V, for any set [formula], is approximately given by

[formula]

For the example under consideration, when M  =  1, i.e., [formula], the delivery rate is 1.758, while the delivery rate of the scheme proposed in [\cite=WangHeterogenous] for this setting is 2.681. Hence, the proposed scheme provides 34.43% reduction in the delivery rate compared to the state-of-the-art result for this example.

Placement Phase

Since the active users are not known in advance in the decentralized setting, cache contents cannot be coordinated among the users. Similarly to the placement phases of the the decentralized coded schemes in the literature [\cite=MaddahAliDecentralized] [\cite=WangHeterogenous], user Uk caches a random MkF / N bits of each file independently, for k = 1,...,K. Since N files are hosted in the database, a total of MkF bits are cached by each user Uk, and hence, the corresponding cache-capacity constraint is satisfied.

For any set [formula], let Wi,V represent the bits of file Wi that have been exclusively cached by the users in set V at the end of the placement phase, i.e., Wi,V  ⊂  Zk, [formula], and [formula], [formula].

Delivery Phase

User demands are revealed at the beginning of the delivery phase. Without loss of generality, we re-label the users such that the first K1 users, referred to as group G1, have the same request W1, the next K2 users, group G2, request file W2, and so on so forth. For notational convenience, we define [formula]. Therefore, the user demands are as follows:

[formula]

where we set S0  =  0. We further order the users within a group according to their cache sizes, and assume, without loss of generality, that [formula], for [formula].

The proposed delivery phase is presented in Algorithm [\ref=DeliveryHeterogenous]. For any general demand combination described above, the delivery phase presented in Algorithm [\ref=DeliveryHeterogenous] contains two procedures CODED DELIVERY and RANDOM DELIVERY, and in each case the server chooses the one with the smaller delivery rate. Below, we explain these two procedures in detail.

The CODED DELIVERY procedure includes three distinct parts, where the content delivered in part i is denoted by Xi, i = 1,2,3, and the common message [formula] is sent to all the users during the delivery phase. The message transmitted in part 2, X2, is further divided into three pieces X12, X22, and X32, i.e., [formula]. Based on the aforementioned placement phase, the main motivation of the CODED DELIVERY procedure is to enable each user to recover the missing bits of its requested file which have been cached by i other users, [formula].

In Part 1 of the this procedure, each user receives the bits of its requested file which have not been cached by any user.

The purpose of Part 2 is to enable each user to obtain all the missing bits of its request that have been cached by another single user. First, consider the message X12. For i = 1,...,N, each user [formula] (i.e., Uk∈Gi), has access to bits Wi,{k} locally in its cache, and with X12 it can decode all the pieces [formula], [formula], i.e., the bits of its demand Wi, which are in the cache of another user in the same group, and no other user. Delivering the messages X22 and X32 together helps the users to decode the bits of their requested files having been cached by a single user in other groups; that is, after receiving [formula], [formula], and [formula] [formula] [formula], the users in both groups Gi and Gj can obtain the missing bits of their requested files that have been cached by a user in another group, for i = 1,...,N - 1 and j = i + 1,...,N (and no other user). Note that, having received X22, the third message [formula] [formula] [formula] is the smallest number of bits (based on the assumption [formula], [formula]) that enable all the users in both groups Gi and Gj to obtain the missing bits of their desired files that are in the cache of users in the other group, for i = 1,...,N - 1 and j = i + 1,...,N.

Part 3 of our algorithm is the same as the delivery phase proposed in [\cite=WangHeterogenous], and it is performed to send the users the missing bits of their requests that have been cached by more than one user.

Finally, in the RANDOM DELIVERY procedure, as in the DELIVERY procedure of [\cite=MaddahAliDecentralized], the server transmits enough random linear combinations of the bits of file Wi to the users in group Gi to make sure they all can decode the file, for [formula].

Delivery Rate Analysis

In the following, we evaluate the delivery rate of the proposed caching scheme for the worst-case user demands. Consider first the case N  ≥  K. It can be argued in this case that the worst-case user demands happens if each file is requested by at most one user. Hence, by re-ordering the users, for the worst-case user demands, we have Ki  =  1, for 1  ≤  i  ≤  N, and Ki  =  0, otherwise. In this case, it can be shown that the CODED DELIVERY procedure requires a lower delivery rate than the RANDOM DELIVERY procedure; hence, the server uses the former. In this case, it is possible to simplify the CODED DELIVERY procedure such that, only message X32 is transmitted in Part 2, when N  ≥  K, i.e., X2  =  X23. The corresponding common message, [formula], transmitted over the CODED DELIVERY procedure, reduced to the delivery phase of [\cite=WangHeterogenous]. Thus, the proposed scheme achieves the same delivery rate as [\cite=WangHeterogenous] when N  ≥  K.

Next, we consider the case N < K. It is possible to show that the worst-case user demands in this case happens when N users with the smallest cache capacities all request different files, i.e., they end up in different groups. The delivery rate of the proposed delivery phase when N < K is presented in the following theorem. The proof of the worst-case demand distribution as well as Theorem [\ref=TheDelRateDecDistCacheSizes] are skipped due to space limitations; however, they can be found in the longer version of the paper in [\cite=Asilomar16].

In a decentralized caching system with N files in the database, each of size F bits, and K users with cache capacities [formula], such that [formula], the following delivery rate-cache capacity trade-off is achievable when N < K:

[formula]

where

Comparison with the State-of-the-Art and Numerical Results

In this section, the proposed caching scheme is compared with the scheme proposed in [\cite=WangHeterogenous] both analytically and numerically. We note that, although the scheme presented in [\cite=WangHeterogenous] is for N  ≥  K, it can also be applied to the case N  <  K, and the same delivery rate as [\cite=WangHeterogenous], denoted here by Rb(μ), can be achieved. Hence, in the following, when we refer to the scheme stated in [\cite=WangHeterogenous] for N  <  K, we consider its generalization to this scenario. When N < K, according to [\cite=WangHeterogenous] and [\eqref=OurDeliveryRateHeterogenous], we have

[formula]

The inequality (a) holds as long as N  <  K. Therefore, when the number of files in the database is smaller than the number of active users in the delivery phase, the proposed coded caching scheme requires a smaller delivery rate than the one presented in [\cite=WangHeterogenous].

For the numerical results, we consider an exponential cache distribution among users, such that the cache capacity of user Uk is given by

[formula]

where 0  ≤  α  ≤  1, for [formula], and M denote the maximum cache capacity in the system. Thus, we have [formula], such that [formula]. The distribution of cache capacities normalized by [formula], i.e., [formula] denoted by [formula], [formula], is demonstrated in Fig. [\ref=CacheCapacitiesDistribution] for different values of α, when K = 50. Observe that, the smaller the value of α, the more skewed the cache capacity distribution across users become. In the special case of α = 1, we obtain the homogeneous cache capacity model studied in [\cite=MaddahAliDecentralized].

In Fig. [\ref=N50K70], the delivery rate of the proposed scheme, Rc(μ), is compared with that of the coded scheme proposed in [\cite=WangHeterogenous], i.e., Rb(μ), when N = 50, K = 70, and α  =  0.97. The delivery rate is plotted in this figure versus the largest cache capacity in the system, M. As expected the performance improves, i.e., the delivery rate reduces as M increases. We also clearly observe that the proposed scheme outperforms the scheme presented in [\cite=WangHeterogenous]. The improvement is particularly significant for lower values of M. The cut-set lower bound for this setting is also included in the figure. Although the delivery rate of the proposed scheme approaches the lower bound for relatively small values of M, there is still a gap for large values of M, which may as well be due to the looseness of the lower bound.

In order to see the effect of skewness of the cache capacities on the delivery rate, in Fig. [\ref=N_75_K_90_AlphaVary], the delivery rate of different schemes are plotted as a function of [formula], for N = 30, K = 45, and the largest cache capacity of M = 2. The delivery rate of the proposed decentralized coded caching scheme is lower than the one presented in [\cite=WangHeterogenous] for the whole range of α values under consideration, while the gain is more pronounced for smaller values of α, i.e., as the distribution of cache capacities becomes more skewed. We also observe the gap to the cut-set lower bound also diminishes in this regime.

Conclusions

In this paper, we have studied coded caching to users with distinct cache capacities, and proposed a novel decentralized coded caching scheme that improves upon the best known delivery rate in the literature. The improvement is achieved by improving the delivery of bits that have been cached by none of the users, or by only a single user. In particular, the proposed scheme exploits the group-based coded caching scheme we have introduced previously for centralized caching in a system with homogeneous cache capacities [\cite=MohammadQianDenizITW]. Our numerical results show that the improvement upon the scheme proposed in [\cite=WangHeterogenous] is even more pronounced when the cache capacities of the users are more skewed.

We are currently aiming to improve the delivery rate for larger values of cache capacities by finding a more efficient coded delivery scheme for the delivery of the missing bits of the requested files that have been cached by more than one user.