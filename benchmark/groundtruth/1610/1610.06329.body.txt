Multivariate Exponential Analysis from the Minimal Number of Samples

Introduction

Multivariate exponential analysis is a classical problem at the basis of many application domains (such as, for instance, [\cite=650355] [\cite=Man2000] [\cite=YILMAZER2006796] [\cite=4244728]) that recently has gained a lot of attention. The problem statement is that of recovering the vectors [formula] and the coefficients [formula] in the d-variate n-sparse sum

[formula]

from (d + 1)n samples of [formula], which is the minimal number of samples because it equals the number of parameters in the problem statement.

When d = 1 then the problem can be solved using a variety of Prony-based algorithms [\cite=Be.Ti:det:88] [\cite=Sc:mul:86] [\cite=Ro.Ka:esp:89] [\cite=Hu.Sa:mat:90], in which the identification of the φj and αj is separated and taken care of in two stages. The frequencies [formula] are obtained from a generalized eigenvalue or polynomial rooting problem, while the linear coefficients [formula] are computed from a Vandermonde system of linear equations. Input to these algorithms are 2n samples of f(x) at some equidistant points [formula]. This number of samples is minimal if n is known. Otherwise at least one more sample is required to identify the sparsity n. For more details on the latter we refer to [\cite=Ka.Le:ear:03] [\cite=Cu.Le:spa:16].

Several computational methods were developed to solve the problem also when d > 1, from straightforward generalizations to more sophisticated approaches, all of them using more than a minimum of (d + 1)n samples though. It should be obvious to the reader that the challenge is not to recover inner products 〈φj,x〉 and the associated coefficients αj for [formula], from 2n equidistant samples in higher-dimensional space. Under modest conditions this can be achieved using the univariate techniques mentioned above. Instead, the challenge is to recover the individual [formula] and the coefficients αj. We describe the state of the art in multivartiate exponential analysis and explain how our approach differs from it.

The one-dimensional matrix pencil method was generalized to the 2-dimensional matrix enhancement and matrix pencil method (MEMP) [\cite=Hu:est:92] and can be extended to higher dimensions in a straightforward manner. It uses a Hankel-block-Hankel matrix to decompose the 2-dimensional problem into two one-dimensional problems reflecting each dimension. This decomposition introduces an additional challenge though, namely that of matching or pairing the information computed from the one-dimensional problems [\cite=Ro.Na:est:01]. Moreover, when constructing a uniform d-dimensional grid of sample points, the amount of information is O(nd).

Solving the problem along some one-dimensional subspace, in other words computing some projection such as in [\cite=Pl.Wi:how:13] [\cite=Po.Ta:par:13] requires only O(n) samples. However, [\cite=Di.Is:par:15] shows that there is no set of finitely many lines for which the bivariate reconstruction problem has a unique solution. A lower bound for the number of samples in the reconstruction is O(n2) when d = 2. In order to solve the unicity problem, [\cite=Di.Is:par:15] reformulates the problem as a non-convex optimizaion problem, which is not computationally feasible for practical purposes.

Rather than projecting on one-dimensional subspaces, a symbolic approach is developed in [\cite=Sa:pro:16] making use of constructive ideal theory and multivariate polynomial interpolation. The largest number of required samples in this setting is estimated to be O((d + 1)n2 log 2d - 2n). In the same corner one finds [\cite=Ku.Pe.ea:mul:16] and [\cite=Pe.Pl.ea:rec:] which obtain the multivariate exponents as common roots of a finite system of d-variate polynomials. Still making use of O(nd) samples however, algebraic geometry theory now guarantees the unicity of the recovery.

The method we propose differs significantly from all of the above, not only in its informational usage which can be as low as (d + 1)n, but also in its approach which only makes use of 1-dimensional Prony techniques combined with some linear systems of equations because the individual φji appear linearly in the 〈φj,x〉.

After this state of the art of the literature, Sections 2 and 3 deal with the ideal case where some mild assumptions are verified and only (d + 1)n evaluations are necessary, thus generalizing Prony's result where 2n samples solve a univariate exponential analysis problem. In Section 4 the most general case is detailed, requiring slightly more samples because the assumptions do not hold. An analysis of the worst case scenario and an algorithm for the detection of n is presented in Section 5. Finally, the new algorithm is illustrated with an example in Section 6.

Multivariate exponential analysis

As surveyed in the introduction, up to now computational methods require more samples than the minimal number, for one or other reason. We now explain how the problem statement can also be solved in the multivariate setting using the minimal number (d + 1)n of samples. The trick to achieve this is to split the set of samples in two subsets, namely 2n equidistant samples and another (d - 1)n samples that may but need not be equidistant in the higher-dimensional space (they cannot be entirely unstructured though). We discuss the use of the 2n equidistant samples in this section and that of the additional (d - 1)n samples in Section 3. For now we assume in the multivariate setting that the value of n is known. How to detect n is further discussed in Section 5.

Let [formula] and [formula] [\cite=Ny:cer:28] [\cite=Sh:com:49]. Let us sample [formula] at the points [formula]:

[formula]

For the time being, we also assume that the sampling direction Δ is such that the values exp (〈φj,Δ〉), [formula] are mutually distinct. How to deal with collisions in these values is described in Section 4.

Following the univariate scheme [\cite=Hi:int:56] the coefficients [formula] of the polynomial

[formula]

can be obtained from the n  ×  n Hankel system of linear equations

[formula]

or the roots [formula] of B(z) can be found as the generalized eigenvalues λ of the problem

[formula]

So we can recover the expressions exp (Φj) where

[formula]

Although we have not yet identified the individual [formula], nothing prevents us from already computing the linear coefficients αj from the n  ×  n Vandermonde system

[formula]

The latter can also be replaced by the 2n  ×  n Vandermonde system involving all samples, which is then solved in the least squares sense, as recommended in the case of real-life and hence noisy data.

Identification shifts

In order to extract the [formula] from the [formula], still under the assumption that the values [formula] are mutually distinct, some additional samples are required. For this purpose we choose a set [formula] of linearly independent vectors spanning the space [formula]. The additional samples are taken along a linear combination of Δ and some [formula]:

[formula]

where the [formula] for fixed i are taken to be mutually distinct. A simple choice for [formula] for all i is [formula]. Then the additional samples are taken equidistantly along independent shifts δi with respect to the original vector Δ, in other words [formula]. At the same time we assume that

[formula]

We call these vectors [formula] identification shifts for reasons that will become apparent: they allow to identify the individual φji from the computed Φj. For this identification we exploit the fact that the φji appear linearly in the Φj and hence we turn our attention to systems of linear equations rather than to multivariate polynomial root solving or structured generalized eigenvalue problems.

Consider for fixed [formula], meaning for a chosen linearly independent shift vector δi, the following Vandermonde-like system of linear equations:

[formula]

Since we know [formula] and have chosen [formula], with i fixed, the Vandermonde-like coefficient matrix can easily be composed. Note that for the choice [formula] the Vandermonde-like coefficient matrix coincides with that of [\eqref=VM] where k = 0. The unknowns [formula] come from a reinterpretation of the samples [formula] as

[formula]

with

[formula]

and

[formula]

The values Aji  /  αj equal

[formula]

which we denote by

[formula]

Here the index i is still fixed. Note that we have no problem to pair the Φji to the [formula] since for each i the Aji are paired to the [formula] through the Vandermonde-like systems [\eqref=VM] and [\eqref=VMlike].

By setting up [\eqref=VMlike] for each [formula] and pairing its solution with Φj in [\eqref=Phi], we obtain for fixed [formula] the linear system of equations

[formula]

Since the vectors Δ and [formula] are linearly independent, the coefficient matrix of [\eqref=IDshift] is regular and so the individual [formula] can be computed, at the expense of 2n evaluations Fs in [\eqref=1stbatch] and (d - 1)n evaluations [formula] in [\eqref=IDsamples].

Before we continue we point out that (as is clear from the semantics of the formulas) we can also denote Δ as δ0, Fs as Fs0 and Φj as Φj0.

Disentangling collisions

We now turn our attention to the situation in which the first batch of samples Fs at multiples of the vector Δ does not reveal all individual terms because some values [formula] collide. Assume that with 0  ≤  s  ≤  2ν - 1,ν  ≤  n the exponential samples break down into

[formula]

because

[formula]

For ease of notation, but without loss of generality, we take the colliding terms to be successive. Since [formula], we actually have

[formula]

The Vandermonde system [\eqref=VM] now becomes

[formula]

Note that at the same time, the degree of the polynomial B(z) in [\eqref=pol] is only ν. How this is detected and how the true n is revealed is discussed in the next section. To proceed we denote

[formula]

To disentangle the collisions in the exponential sum, we need additional evaluations besides the minimal number (d + 1)n.

We start with i = 1 and the identification shift vector δ1. First we point out how the Vandermonde-like system [\eqref=VMlike] of Section 3 looks like in case of such collisions: in the coefficient matrix the value n is replaced by ν and in Φj the index j is replaced by hj. With the collisions in [\eqref=collision], the unknowns [formula] take the form

[formula]

In the sequel we denote from here on the additional evaluations [formula] mentioned in Section 3 by

[formula]

and we add, still with i = 1, the samples

[formula]

The triple index expresses the shift vector multiple in s, the collision into ν piles of the Φj in [formula], and the identification level in i (which is i = 1 here).

Since the values of hj are actually unknown, the addition of samples is done further and interlaced with singularity checks of some Hankel matrices, as we explain now. The checks are performed for each collision or pile hj and later repeated for each i. Collisions in the space spanned by Δ may not be fully disentangled in the space spanned by Δ and δ1, but they are gradually being disentangled as we add independent vectors δi until we span the whole space. At the last stage, when dealing with the full basis [formula], the true n is revealed because in the end all collisions are taken apart, given enough additional samples. For the moment we continue with i = 1. For each s separately, we set up in analogy with [\eqref=VMlike], the Vandermonde-like system

[formula]

where

[formula]

Note that the coefficient matrix is independent of s. Also, the former unknown Aj1 can as well be indexed as A1j1 and so [\eqref=disVMlike] and [\eqref=Adis] remain valid for s = 1, which is important for the sequel. The values Aj from [\eqref=Aj] and Asj1,s  ≥  1 from [\eqref=Adis] are actually equidistant samples of the function

[formula]

taken at x = sδ1,s  ≥  0. For each fixed [formula] we now put together the Hankel matrix

[formula]

Note that in order to enlarge [\eqref=disHan] with one row and column for a particular j, one needs to solve [\eqref=disVMlike] for two additional values of s, thereby obtaining the additional Asj1 for all 1  ≤  j  ≤  ν.

It is known that the rank of any (hj - hj - 1 + t)  ×  (hj - hj - 1 + t) submatrix for finite t  ≥  0 is bounded by hj - hj - 1 [\cite=Ka.Le:ear:03] [\cite=Cu.Le:spa:16] since hj - hj - 1 equals the number of terms in each of the evaluations Aj,Asj1,s  ≥  1. The actual rank rj of the (hj - hj - 1)  ×  (hj - hj - 1) submatrix with Aj in the top left corner tells us how many of the hj - hj - 1 terms in Aj(x) can indeed be separated at the current level (i = 1) where identification shift δ1 is brought into the picture. The value of rj is discovered as one adds samples [formula], solves [\eqref=disVMlike] and enlarges [\eqref=disHan] step by step. This explains why we need to add samples [formula] until s reaches 2 max j(hj - hj - 1) or until for all j the rank rj is known. How do we proceed to extract the coefficients and exponential parameters from [\eqref=Ax] and disentangle the collisions?

For j fixed, rj of the individual terms

[formula]

of Aj1(x) can be deduced from the samples Aj,Asj1,s  ≥  1 of Aj1(x) using one of the Prony-like methods [\cite=Be.Ti:det:88] [\cite=Sc:mul:86] [\cite=Ro.Ka:esp:89] [\cite=Hu.Sa:mat:90] which were already mentioned to solve for [\eqref=Phi] from [\eqref=HA] or [\eqref=EV] and compute the coefficients from [\eqref=VM]. We remark that when rj  <  hj  -  hj - 1 then some collisions in Aj1(x) still remain indistinguishable in the space spanned by Δ and δ1.

For the sake of completeness we explicitly give the generalized eigenvalue problems that lead to the identification of 1  ≤  rj  ≤  hj - hj - 1 terms in Aj(x):

[formula]

After disentangling at i = 1, at least partially, some of the collisions, we can update the number of terms in the exponential model from ν to μ  ≥  ν and reduce the collisions to

[formula]

It is clear that the previous indices [formula] are among the [formula] but remember that we don't know the values hj or gk explicitly. We only know that for some j a collision from index hj - 1 + 1 to hj,1  ≤  j  ≤  ν may have split into separate piles indexed by some gk and gk + 1,1  ≤  k  ≤  μ. At this moment in the procedure, namely at the completion of step i = 1, we have computed

[formula]

Because [formula] we in fact obtained all the values

[formula]

which we need later on in combination with the

[formula]

to identify the individual φji as in [\eqref=IDshift].

We now explain how to move from i to i + 1. The first thing is to find proper locations for the samples involving the next identification shift δ2. Some care needs to be taken with respect to the regularity of the Vandermonde matrices involved. For i = 2 we collect

[formula]

Let us denote

[formula]

Note that the sum is a direct consequence of the choice Δ  +  δ1 in [\eqref=collect], which is briefly discussed below. Similarly to [\eqref=disVMlike] we write down, for each s separately,

[formula]

where

[formula]

From here it is clear how to finalize the i = 2 phase and how to proceed to the next value of i. We point out that instead of the linear combination Δ  +  δ1 in [\eqref=collect], any linear combination cΔ + eδ1 with [formula] that guarantees the regularity of the coefficient matrix in [\eqref=i2] can be used (then the definition of Ωgj also needs to be adapted). This option may be useful as it allows to control the location of the sample points for numeric purposes or so.

To round up this section, we summarize the algorithm that recovers the vectors φj and coefficients αj for [formula] in case of possible collisions of inner products with the chosen directional vectors [formula]. Before we proceed, we further adapt our notation. Let

[formula]

Our first aim is to identify all the inner products [formula], including possible collisions. This is done by making use of successively collected samples, namely

[formula]

where we assume that empty sums equal zero and values in an empty range need not be specified. The samples are collected by fixing the indices from the right to the left: at identification level i, collision or pile [formula] is being sparsely interpolated using the samples collected at shift multiples s. Here νi indicates the number of non-coinciding inner products at identification level i. Remember that s is running up to twice the number of terms in expression [formula] at level i (for i = 1 this is given in [\eqref=Ax] and it is straightforward to imagine how it looks like for general i). We remind the reader that only the evaluation at multiples of δi,i  ≥  0 needs to follow an equidistant scheme. The values [formula] need not be like that. We also mentioned earlier that the sum [formula] can be replaced by another linear combination. The only crucial element is that the δi,i  ≥  0 are linearly independent. The latter will precisely allow us to identify the vector components [formula] from the inner products [formula] as in [\eqref=IDshift].

Detecting the sparsity

The minimal number of (d + 1)n samples only delivers the parameters [formula] if the value of n is somehow known a priori and no collision of values [formula] occurs. In the previous section we described how to deal with eventual collisions. Here we detail how to detect the value of n should it not be given.

While collecting the samples Fs = f(sΔ) and building the Hankel matrices in [\eqref=HA] or [\eqref=EV], the rank of the Hankel matrix reveals the number ν of terms that do not collide when evaluating in the space spanned by the vector Δ. To this end we need at least 2ν + 1 values so that we can compose the (ν + 1)  ×  (ν + 1) Hankel matrix

[formula]

and conclude that it is singular [\cite=He:app:74] [\cite=Ba.Gr:pad:81] [\cite=Ka.Le:ear:03].

From ν and [\eqref=HA] or [\eqref=EV] we proceed to collect the samples [formula] (s = 1) and [formula] (s = 2), another 2ν in total ([formula]). If all 2  ×  2 Hankel matrices of the form [\eqref=disHan] are singular, then any collisions remain indistinguishable (unless the zero determinant was an unfortunate coincidence [\cite=Ka.Le:ear:03]) also in the space spanned by Δ and δ1. However, if for some j the 2  ×  2 matrix [\eqref=disHan] is regular, then we have to proceed to the next values for s (s = 3,4), collect another 2ν values in total, and find out how many terms actually can be revealed in the space spanned by Δ and δ1. We proceed until we find no larger matrices of the form [\eqref=disHan] that are regular. Only after working ourselves through all regular matrices of the form [\eqref=disHan] with δ1 (i = 1) we can update ν to μ  ≥  ν.

And then we bring the next identification shift vector δ2 in the picture. We collect the samples [formula] (s = 1,2) as in [\eqref=collect] and compose matrices similar to [\eqref=disHan] but now with the last index in the Asj1 replaced by i = 2 and with Asj2 defined as in [\eqref=A2sj]. The inspection of the Hankel matrices containing the values computed for Asj2 is identical to the procedure described in the previous paragraph for i = 1. If required, as before, we add more samples for larger values of s.

Finally, by the time we reach i = d - 1 we can update the number of terms to the true value for n. All the above is now illustrated with an example in which we take the reader through the entire process, first collision-free, then including collision disentanglement.

Numerical illustration

We take d = 2, write u: = x1,v: = x2,x = (u,v)t and consider the exponential sum

[formula]

with When outputting numerical results for this small scale example, we round all values to 4 significant digits. The numerical effect of the choice of the vectors Δ and δi throughout the process, and that of the underlying one-dimensional Prony-like method in use, is beyond the scope of this paper and will be the subject of further investigations.

First we show the simple case described in the Sections 2 and 3, where the number of terms n = 4 is known up front and no collisions of the inner products in the samples occur. Of course, the latter is hard to predict in practice.

We take Δ = (0.01,0.01) and δ1 = ( - 0.01,0.01). Using 8 equidistant evaluations at [formula], we obtain from [\eqref=EV] the values of exp (Φj) and can deduce the [formula] because [formula]:

[formula]

We obtain the coefficients [formula] from [\eqref=VM]:

[formula]

From 4 additional evaluations along the identification shift δ1, we obtain the values of exp (Φ11), exp (Φ21), exp (Φ31), exp (Φ41). Their exponents are the projections of the vectors φj along δ1:

[formula]

We finally obtain the values of φj  =  (φj1,φj2)t by solving for each [formula]

[formula]

This leads to the following numerical approximations for the φj:

[formula]

So far we have used 12 samples in total, which indeed equals (d + 1)n. Next we deal with the situation in which neither n is known, nor the assumption of the non-collision holds.

One additional evaluation in the first batch, at x = 8Δ, would ideally (meaning that the numerical rank is easy to detect) and with high probability (meaning that we don't accidentally hit a root of the determinant) have revealed that n = 4, still under the assumption that no collisions occur at the inner products. But let us instead move to other directions Δ and δ1 that get us in trouble because of colliding inner products.

Take Δ = (0.03,0) and δ1 = (0,0.01). The projections of φ2 and φ3 along Δ clearly coincide. After 7 evaluations at [formula] we found that ν0 = 3 and we obtain from [\eqref=VMnu] that (without actually knowing the values of the hj which we list only to help the reader follow the example):

[formula]

We proceed without knowing n and without knowing whether and where some collisions have occurred. But we know, since d = 2, that after adding an independent shift vector δ1, all terms will have revealed themselves.

So we add evaluations [formula] with [formula] and [formula] For simplicity we choose [formula]. With [formula] and s = 1,2 we find that the matrix

[formula]

where the Asji are computed from [\eqref=Adis], has rank 1 and so h1 = 1 = g1. With [formula] and s = 1,2,3,4 we find that the matrix

[formula]

has rank 2. This indicates that there are 2 terms coinciding at Φh2 (hence h2 = 3 and g2 = 2,g3 = 3). Remember that in order to obtain As21,1  ≤  s  ≤  4, we need to solve [\eqref=disVMlike] which involves the samples Fsj1,1  ≤  j  ≤  3. Hence, continuing the sampling for [formula] drags along [formula] at the same time. In other words, we are now spending 3  ×  4 samples for [formula] rather than only 4 samples for [formula].

We now reveal 〈φ2,δ1〉 and 〈φ3,δ1〉 by solving the generalized eigenvalue problem

[formula]

With [formula] and s = 1,2 we find the same conclusion as with [formula], now for

[formula]

and so ν1 = 4 with h3 = 4,g4 = 4.

At the expense of a total of (2  ×  3  +  1) + 3  ×  4  =  19 evaluations, we find that n = 4 and we can identify all φji and αj for [formula] and i = 1,2.

Conclusion

In 1795 the French scientist G. de Prony showed that a univariate linear combination of n exponential terms with unknown real but mutually distinct exponents could be fitted uniquely to 2n data samples. His result solves the d = 1 case of this paper.

The current paper is the first of its kind where this result is generalized for general d > 1: a multivariate linear combination of n exponential terms with unknown inner product exponents can, under mild conditions, be fitted using only (d + 1)n data.