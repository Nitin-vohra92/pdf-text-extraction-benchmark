Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs

Introduction

recognition [\cite=ZhouLXTO14] [\cite=XiaoHEOT10] is a fundamental problem in computer vision, and has received increasing attention in the past few years [\cite=LiPT05] [\cite=WuR11] [\cite=ZhangZS14] [\cite=ZuoWSZYJ14] [\cite=GongWGL14] [\cite=XieWGZT14] [\cite=YangR15] [\cite=ShenLH15] [\cite=ZuoSWLWWC16]. Scene recognition not only provides rich semantic information of global structure [\cite=OlivaT01], but also yields meaningful context that facilitates other related vision tasks, such as object detection [\cite=Torralba03] [\cite=FelzenszwalbGMR10], event recognition [\cite=wang2015object] [\cite=XiongZLT15], and action recognition [\cite=WangSW16] [\cite=WangQT14]. In general, it is assumed that scene is composed of specific objects arranged in a certain layout, so that scene categories are often defined by multi-level information, including local objects, global layout, background environments and possible interactions between them. Cognitive evidence has implied that human vision system is highly sensitive to the global structure and special regions of an image, while puts little attention on local objects and features outside of the attentional regions. Therefore, compared with object categories, the concept of scene is more subjective and complicated, so that there may not exist consensus on how to determine an environment category. These pose main challenge for developing an effective and robust algorithm that is able to compute all these multi-level information from images.

Recently, large-scale scene datasets (e.g., the Places [\cite=ZhouLXTO14] and Places2 [\cite=Places2]) have been introduced to advance the research on scene understanding, allowing to train powerful convolutional neural networks (CNNs) [\cite=lecun-98] for scene classification. These large-scale datasets consist of a rich scene taxonomy, which includes rich categories to cover the diverse visual environments of our daily experience. With these scene category information, scene keywords can be sent to image search engines (e.g., Google Images, Bing Images or Flicker), where millions of images can be downloaded, and then be further sent to Amazon Mechanical Turk for manual annotation. However, as the number of classes is growing rapidly, these visual categories start to overlap with each other, so that there may exist label ambiguity among these scene classes. As shown in Figure [\ref=fig:ex], images in cubicle office and office cubicles categories are easily confused with each other, similar ambiguities happen in baseball field and stadium baseball. Partially due to this reason, even the human top-1 error rate is still relatively high on the SUN397 dataset (around 30%) [\cite=XiaoHEOT10].

Due to inherent uncertainty of scene concepts and increasing overlap among different categories, it is challenging to conduct scene recognition on large-scale datasets containing hundreds of classes and millions of images. Specifically, current large-scale datasets present two major challenges for scene classification, namely visual inconsistence and label ambiguity.

For visual inconsistence, we refer to the fact that there exist large variations among images from the same scene category. Since it is difficult to define scene categories objectively, natural images are annotated according to people subjective experience when a dataset is created. This naturally leads to large diversity on large-scale scene datasets. For instance, images in kitchen category contain significantly diverse context and appearance, ranging from a whole room with many cooking wares to a single people with food, as shown in Figure [\ref=fig:ex].

For label ambiguity, we argue that some scene categories may share similar visual appearance, and could be easily confused with others. As the number of scene classes increases, the inter-category overlaps can become large. For example, as shown in Figure [\ref=fig:ex], the baseball field category is very similar to the stadium baseball, and they both contain identical representative objects, such as track and people.

These challenges motivate us to develop an effective multi-resolution disambiguation model for large-scale scene classification, by making two major contributions: (1) we propose a multi-resolution convolutional architecture to capture multi-level visual cues of different scales; (2) We introduce knowledge guided strategies to effectively disambiguate similar scene categories. First, to deal with the problem of visual inconsistence (i.e. large intra-class variations), we come up with a multi-resolution CNN framework, where CNNs at coarse resolution are able to capture global structure or large-scale objects, while CNNs at fine resolution are capable of describing local detailed information of fine-scale objects. Intuitively, multi-resolution CNNs combine complementary visual cues at multi-level concepts, allowing them to tackle the issue of large intra-class variations efficiently. Second, for the challenge of label ambiguity (i.e. small inter-class variations), we propose to reorganize the semantic scene space to release the difficulty of training CNNs, by exploiting extra knowledge. In particular, we design two methods with the assistance from confusion matrix computed on validation dataset and publicly available CNN models, respectively. In the first method, we investigate the correlation of different classes and progressively merge similar categories into a super category. In the second one, we use the outputs of extra CNN models to generate new labels. These two methods essentially utilize extra knowledge to produce new labels for training images, which are able to make the training of CNN easier. They essentially act as a model regularizer that guides the CNN to a better optimization and reduce the effect of over-fitting.

To verify the effectiveness of our method, we choose the successful BN-Inception architecture [\cite=IoffeS15] as our basic network structure, and demonstrate the advantages of multi-resolution CNNs and knowledge guided disambiguation strategies on a number of benchmarks. More specifically, we first conduct experiments on three large-scale image recognition datasets, including the ImageNet [\cite=DengDSLL009], Places [\cite=ZhouLXTO14], and Places2 [\cite=Places2], where our method obtains highly competitive performance. Then, we further apply the proposed framework on two high-impact scene recognition challenges, namely the Places2 challenge (held in ImangeNet large scale visual recognition challenge [\cite=ILSVRC15]) and the large-scale scene understanding (LSUN) challenge in CVPR 2016. Our team secures the 2nd place at the Places2 challenge 2015 and the 1st place at the LSUN challenge 2016. Furthermore, we evaluate the generalization ability of our learned models by testing them directly on the MIT Indoor67 [\cite=QuattoniT09] and SUN397 [\cite=XiaoHEOT10] benchmarks, with new state-of-the-art performance achieved. Finally, we present several failure cases by our models to highlight existing challenges for scene recognition, and discuss possible research directions in the future.

The rest of the paper is organized as follows. In Section [\ref=sec:rw], we review related works from aspects of scene recognition, deep networks for image recognition, and knowledge transferring. Section [\ref=sec:mr_cnn] introduces the architecture of multi-resolution convolutional neural networks. In Section [\ref=sec:kd], we develop two types of knowledge guided disambiguation strategies to improve the performance of scene recognition. We report experimental results and analyze different aspects of our method in Section [\ref=sec:con]. Finally, we conclude the paper in Section [\ref=sec:con].

Related Works

In this section, we briefly review previous works related to our method, and clarify the difference between them. Specifically, we present related studies from three aspects: (1) scene recognition, (2) deep networks for image recognition, and (3) knowledge transferring.

Scene recognition. The problem of scene recognition has been extensively studied in previous works. For example, Lazebnik et al. [\cite=LazebnikSP06] proposed spatial pyramid matching (SPM) to incorporate spatial layout into bag-of-word (BoW) representation for scene recognition. Partizi et al. [\cite=PariziOF12] designed a reconfigurable version of SPM, which associated different BoW representations with various image regions. The standard deformable part model (DPM) [\cite=FelzenszwalbGMR10] was extended to scene recognition by Pandey et al. [\cite=PandeyL11]. Quattoni et al. [\cite=QuattoniT09] studied the problem of indoor scene recognition by modeling the spatial layout of scene components. Mid-level discriminative patches or parts were discovered and identified for scene recognition in [\cite=SinghGE12] [\cite=JunejaVJZ13]. Recently, deep convolutional networks have been exploited for scene classification by Zhou et al. [\cite=ZhouLXTO14], where they introduced a large-scale Places dataset and advanced the state of the art of scene recognition by a large margin. After this, they introduced a more challenging dataset [\cite=Places2] with increasing categories and images, called as Places2.

Our paper differs from these previous works from two aspects: (1) We tackle scene recognition problem with a much larger scale database, where new problems, such as large visual inconsistent and significant category ambiguity, are raised. These make our problem more challenging than all previous ones on this task. (2) We design a multi-resolution architecture and propose a knowledge guided disambiguation strategy that effectively handle these new problems. Large-scale problem is the fundamental challenge in computer vision, we provide a high-performance model on such a challenging dataset, setting our work apart from all previous ones on scene recognition.

Deep networks for image recognition. Since the remarkable progress made by AlexNet [\cite=KrizhevskySH12] on ILSVRC 2012, great efforts have been devoted to the problem of image recognition with various deep learning techniques [\cite=ZeilerF14] [\cite=HeZR014] [\cite=SimonyanZ14a] [\cite=SzegedyLJSRAEVR15] [\cite=IoffeS15] [\cite=HeZRS15a] [\cite=ShenLH15] [\cite=Szegedy_2016_CVPR] [\cite=HeZRS15]. A majority of these works focused on designing deeper network architectures, such as VGGNet [\cite=SimonyanZ14a], Inception Network [\cite=SzegedyLJSRAEVR15] [\cite=Szegedy_2016_CVPR], and ResNet [\cite=HeZRS15] which finally contains hundreds of layers. Meanwhile, several regularization techniques and data augmentations have been designed to reduce the over-fitting effect of the network, such as dropout [\cite=KrizhevskySH12], smaller convolutional kernel size [\cite=ZeilerF14] [\cite=SimonyanZ14a], and multi-scale cropping [\cite=SimonyanZ14a]. In addition, several optimization techniques have been also proposed to reduce the difficulty of training networks, so as to improve recognition performance, such as Batch Normalization (BN) [\cite=IoffeS15] and Relay Back Propagation [\cite=ShenLH15].

These works focused on general aspect of applying deep networks for image classification, in particular for object recognition, without considering the specifics of scene recognition problem. As discussed, sense categories more complicated than object classes. They are defined by multi-level visual concepts, ranging from local objects, global arrangements, to dynamic interactions between them. Complementary to previous works on object classification, we conduct a dedicated study on the difficulties of scene recognition, and accordingly come up with two new solutions that address the crucial issues existed in large-scale scene recognition. We propose a multi-resolution architecture to capture visual information from multi-visual concepts, and wish to remedy the visual inconsistence problem. In addition, we design a knowledge guided disambiguation mechanism that effectively handles the issue of label ambiguity, which is an another major challenge for this task.

Knowledge transferring. Knowledge distillation or knowledge transferring between different CNN models is becoming an active topic recently [\cite=HintonVD15] [\cite=RomeroBKCGB14] [\cite=GuptaHM15] [\cite=TzengHDS15] [\cite=ZhangWWQW16]. The basic idea of using the outputs of one network as an assaciated supervision signal to train a different model was invented by Bucila et al. [\cite=BucilaCN06]. Recently, Hinton et al. [\cite=HintonVD15] adopted this technique to compress model ensembles into a smaller one for fast deployment. Similarly, Romero et al. [\cite=RomeroBKCGB14] utilized this approach to help train a deeper network in multiple stages. Tzeng et al. [\cite=TzengHDS15] explored this method to the problem of domain adaption for object recognition. Gupta et al. [\cite=GuptaHM15] proposed to distill knowledge across different modalities, and used RGB CNN models to guide the training of CNNs for depth maps or optical flow field. Zhang et al. [\cite=ZhangWWQW16] developed a knowledge transfer technique that exploits soft codes of flow CNNs to assist the training of motion vector CNNs, with a goal of real-time action recognition from videos.

Our utilization of soft codes as an extra supervision signal differs from these methods mainly from two points: (1) we conduct knowledge transfer crossing different visual tasks (e.g., object recognition vs. scene recognition), while previous methods mostly focused on the same task; (2) we exploit these soft codes to circumvent label ambiguity problem existed in large-scale scene recognition.

Multi-Resolution Convolutional Neural Networks

Generally, a visual scene can be defined as a view that objects and other semantic surfaces are arranged in a meaningful way [\cite=Oliva09]. Scenes contain semantic components arranged in a spatial layout which can be observed at a variety of spatial scales, e.g., the up-close view of an office desk or the view of the entire office. Therefore, when building computational models to perform scene recognition, we need to consider this multi-scale property of scene images. Specifically, in this section, we first describe the basic network structure used in our exploration, and then present the framework of multi-resolution CNN.

Basic network structures

Deep convolutional networks have witnessed great successes in image classification and many powerful network architectures have been developed, such as AlexNet [\cite=KrizhevskySH12], GoogLeNet [\cite=SzegedyLJSRAEVR15], VGGNet [\cite=SimonyanZ14a], and ResNet [\cite=HeZRS15]. As the dataset size of Places2 is much larger than that of ImageNet, we need to trade off between recognition performance and computational cost when building our network structure. In our experiments, we employ the inception architecture with batch normalization [\cite=IoffeS15] (bn-inception) as our network structure. In addition to its efficiency, the inception architecture leverages the idea of multi-scale processing in its inception modules, making it naturally suitable for building scene recognition networks.

As shown in Figure 2, the original bn-inception architecture starts with two convolutional layers and max pooling layers which transform a 224  ×  224 input image into 28  ×  28 feature maps. The small size of feature maps allows for fast processing in the subsequent ten inception layers, two of which have stride of 2 and the rest have stride of 1. This results in 7  ×  7 feature maps, and a global average pooling is used to aggregate these activations across spatial dimensions. Batch Normalization (BN) is applied to the activations of convolutional layers, following by the Rectified Linear Unit (ReLU) for non-linearity.

Two-resolution architectures

The proposed Multi-Resolution CNNs are decomposed into fine resolution and coarse resolution components. The coarse resolution CNNs are the same with the normal bn-inception described in previous subsection, while the fine resolution CNNs share a similar but deeper architecture.

Coarse resolution CNNs operate on image regions of size 224  ×  224, and contain totally 13 layers with weights. The network structure of coarse resolution CNNs is referred as normal bn-inception, since it has the same structure as the original one in [\cite=IoffeS15]. It captures visual appearance and structure at a relatively coarse resolution, focusing on describing global arrangements or objects at larger scale. However, the coarse resolution CNNs may discard local details, such as those fine-scale objects, which are important cues to discriminate sense categories. A powerful scene network should be able to describe multi-level visual concepts, so that it requires to capture visual content in a finer resolution where local detail information is enhanced.

Fine resolution CNNs are developed for high-resolution images of 384  ×  384, and process on image regions of 336  ×  336. By taking a larger image as input, the depth of the network can be increased, allowing us to design a new model with increasing capability. By trading off model speed and network capacity, we add three extra convolutional layers on top of the inception layers, as illustrated in Figure [\ref=fig:mrcnn]. For these newly-added convolutional layers, the pad sizes are set as zeros, so as to keep the resulting feature map as the same size of 7  ×  7 before the global average pooling. We refer this network structure of fine resolution CNN as deeper bn-inception, which aims to describe image information and structure at finer scale, allowing it to capture meaningful local details.

Our two-resolution CNNs take different resolution images as input, so that their receptive fields of the corresponding layers describe different-size regions in the original image, as illustrated in Figure [\ref=fig:mrcnn]. They are designed to describe objects at different scales for scene understanding. Therefore, the prediction scores of our two-resolution models are complementary to each other, by computing an arithmetic average of them.

Discussion. Although sharing similar ideas with common multi-scale training strategy [\cite=SimonyanZ14a], the proposed multi-resolution CNNs differ from it distinctly. The input image sizes are different in our two-resolution architectures (224  ×  224 and 336  ×  336), while multi-scale training in [\cite=SimonyanZ14a] only uses a single image scale, 224  ×  224. This allow us to design two distinct network structures (the bn-inception and deeper bn-inception) with enhanced model capability, which are capable of handling different image scales. The conventional multi-scale training simply uses a single network structure. Thanks to these differences, the proposed multi-resolution architecture is more suitable to capture different level visual information for scene understanding. Moreover, the multi-resolution architecture is complementary to multi-scale training, and can be easily combined with it as stated in next paragraph.

Training of multi-resolution CNNs. The training of multi-resolution CNNs are performed for each resolution independently. We train each CNNs according to common setup of [\cite=KrizhevskySH12] [\cite=SimonyanZ14a]. We use the mini-batch stochastic gradient descent algorithm to learn the network weights, where the batch size is set as 256 and momentum set to 0.9. The learning rate is initialized as 0.1 and decreases according to a fixed schedule determined by the dataset size and specified in Section [\ref=sec:exp]. Concerning data augmentation, the training images are resized as N  ×  N, where N is set as 256 for the bn-inception, and 384 for the deeper bn-inception. Then, we randomly crop a w  ×  h region at a set of fixed positions, where the cropped width w and height h are picked from {N,0.825N,0.75N,0.625N,0.5N}. Then these cropped regions are resized as M  ×  M for network training, where M is set as 224 for the normal bn-inception, and 336 for the deeper bn-inception. Meanwhile, these crops undergo a horizontal flipping randomly. Our proposed cropping strategy is an efficient way to implement the scale jittering [\cite=SimonyanZ14a].

Knowledge Guided Disambiguation

As analyzed above, many scene categories may overlap with others in large-scale datasets, such as Places2 [\cite=Places2]. The increasing number of scene categories causes the problem of label ambiguity, which makes the training of multi-resolution CNNs more challenging. In this section, we propose two simple yet effective methods to handle the issue of label ambiguity by exploiting extra knowledge. Specifically, we first introduce the method of utilizing knowledge from confusion matrix. Then we propose the second one which resorts to knowledge from extra networks.

Knowledge from confusion matrix

As the number of scene classes increases, the difference between scene categories becomes smaller, and some scene classes are easily confused with others from visual appearance. A natural way to relieve this problem is to re-organize scene class hierarchy, and merge those highly ambiguous ones into a super category. The key to merge them accurately is to define the similarity between categories. However, it is difficult to define this similarity (ambiguity) and merge them manually, which is highly subjective and extremely time-consuming on such a large-scale problem. Here we propose a simple yet effective approach that automatically merges visually ambiguous scene categories.

Specifically, we first train a deep model on the original training set of the Places2 which contains 401 classes. Then, we use the trained model to predict image categories on the validation set of the Places2. The confusion matrix is computed by using the predicted categories and ground-truth labels. This confusion matrix displays crossing errors between pairs of categories, which implicitly indicates the degree of ambiguity (similarity) between them. Hence, it is principle to employ this confusion matrix for calculating the pairwise similarities of scene classes. Formally, we define the similarity as follows:

[formula]

where [formula] is the confusion matrix, [formula] represents the probability of classifying ith class as jth class, the larger of this value indicates higher ambiguity between two categories. N is the number of scene classes. The equation ensures the similarity measure is a symmetric metric.

This similarity measure computes underline relationships between categories, providing an important cue for constructing consistent super categories. To this end, we propose a bottom-up clustering algorithm that progressively merges ambiguous categories, as shown in Algorithm [\ref=alg:merge]. At each iteration, we pick a pair of categories with the largest similarity, and merge them into a super category. Then we update the similarity matrix [formula] accordingly, by deleting i* th and j* th rows and columns, and at the same time, adding a new row and column defined as [formula], where [formula] denotes the i* th row vector of [formula]. This iteration repeats until there is no similarity value larger than τ. At the end, all ambiguous classes are merged into a smaller number of categories, resulting in a more consistent category structure that greatly facilitates learning a better CNN.

In current implementation, the original 401 scene classes in the Places2 are re-organized into 372 super-categories, which are used to re-train our CNN. In test processing, the re-trained model only predicts 372 category labels. We transfer them to the original 401 categories, by equally dividing the probability of each super category into its sub categories. This simple strategy turns out to be effective in practice.

Knowledge from extra networks

Knowledge disambiguation by confusion matrix involves class-level re-organization, where we simply consider the similarity between whole classes, and merge them directly into a super category. However, this re-labeling (merging) strategy treats all images in a class equally, and ignores intra-class difference appeared in each single image. The confusion matrix defines category-level ambiguity (similarity) by computing the error rates with other classes, which means that only part of images from these visually ambiguous categories are classified incorrectly. It is more principle to involve image-level re-labeling strategy based on visual content of each image. Hence, in this subsection, we propose to exploit knowledge from extra networks to incorporate the visual information of each single image into this relabeling procedure.

However, it is prohibitively difficult to accomplish the work of image-level re-labeling manually in such a large-scale dataset. Furthermore, the category ambiguity may happen again, since it is challenging to define an objective re-labeling criteria. Fortunately, many CNN models trained on a relatively smaller and well-labeled dataset (e.g. the ImageNet [\cite=DengDSLL009] or Places [\cite=ZhouLXTO14]) are publicly available. These pre-trained models encode rich knowledge from different visual concepts, which is greatly helpful to guiding the image-level re-labeling procedure. They are powerful to extract high-level visual semantics from raw images. Therefore, we utilize these pre-trained models as knowledge networks to automatically assign soft labels to each image by directly using their outputs.

Essentially, this soft label is a kind of distributed representation, which describes the scene content of each image with a distribution over the pre-trained class space. e.g., common object classes by using the ImageNet [\cite=DengDSLL009], or a smaller subset of scene categories by using the Places [\cite=ZhouLXTO14]. As shown in Figure [\ref=fig:label], for instance, the content of dinning room could be described by distribution of common objects, where objects such as dinning table and door may dominate this distribution. For another scene category such as office, the objects of screen and desktop computer may have high probability mass. Utilizing this soft label to represent image content exhibit two main advantages: (1) For visually ambiguous classes, they typically share similar visual elements such as objects and background. Hence they may have similar soft labels which encode the correlation of scene categories implicitly. (2) Compared with class-level re-labeling scheme, the soft label depends on single image content, so that it can be varied from different images in a same class. Normally, images from highly ambiguous classes may share similar but not identical soft labels. Hence, such soft labels are able to capture subtle difference between confused images, making them more informative and discriminative than hard labels.

In current implementation, we consider the complementarity between ground-truth hard labels and soft labels from knowledge networks, and design a multi-task learning framework that utilizes both labels to guide CNN training, as shown in Figure [\ref=fig:kd]. Specifically, during the training procedure, our CNNs predict both the original hard labels and the soft labels simultaneously, by minimizing the following objective function:

[formula]

where D denotes the training dataset. [formula] is the ith image, yi is the ground-truth scene label (hard label), and pi is corresponding predicted scene label. fi is its soft code (soft label) produced by extra knowledge network, and qi is the predicted soft code of image [formula]. λ is a parameter balancing these two terms, and is set as 0.5 in our experiments. K1 and K2 are the dimension of hard label and soft label, corresponding to the numbers of classes in main model and the knowledge model, respectively.

This multi-task objective function forces the training procedure to optimize the classification performance of original scene classification, and imitate the knowledge network at the same time. This multi-task learning framework is able to improve generalization ability by exploiting additional knowledge contained in extra networks as an inductive bias, and reduce the effect of over-fitting on the training dataset. For example, object concepts learned by the ImageNet pre-trained model provide important cues for distinguishing scene categories. As we shall see in Section [\ref=sec:exp], this framework further improves the performance of our proposed multi-resolution CNNs.

Experiments

In this section, we describe the experimental settings and report the performance of our proposed method on six scene benchmarks, including the ImageNet [\cite=DengDSLL009], Places [\cite=ZhouLXTO14], Places2 [\cite=Places2], LSUN [\cite=YuZSSX15], MIT Indoor67 [\cite=QuattoniT09], and SUN397 [\cite=XiaoHEOT10] databases. We first describe these datasets and our implementation details. Then, we verify the effectiveness of multi-resolution CNNs by performing extensive experiments on three datasets. After this, we conduct experiments to explore the effect of knowledge guided disambiguation on the Places2. Furthermore, we report the performance of our method on two large-scale scene recognition challenges, namely the Places2 challenge in ImageNet 2015, and the LSUN challenge in CVPR 2016. Meanwhile, we investegate generalization ability of our models, by directly testing the learned representations on the datasets of MIT Indoor67 [\cite=QuattoniT09] and SUN397 [\cite=XiaoHEOT10]. Finally, we present several failure examples by our methods, and discuss possible reasons.

Large-scale datasets and implementation details

We first evaluate our method on three large-scale image classification datasets, namely ImageNet [\cite=DengDSLL009], Places [\cite=ZhouLXTO14], and Places2 [\cite=Places2]. Results are reported and compared on their validation sets, since the ground-truth labels of their test sets are not available.

The ImageNet [\cite=DengDSLL009] is an object-centric dataset, and is the largest benchmark for object recognition and classification. The dataset for ILSVRC 2012 contains 1,000 object categories. The training data contains around 1,300,000 images from these object categories. There are 50,000 images for validation dataset and 100,000 images for testing. The evaluation measure is based on top-5 error, where algorithms will produce a list of at most 5 object categories to match the ground truth.

The Places [\cite=ZhouLXTO14] is a large-scale scene-centric dataset, including 205 common scene categories (referred to as Places205). The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images). The evaluation criteria of the Places is also based on top-5 error.

The Places2 [\cite=Places2] is extended from the Places dataset, and is probably the largest scene recognition dataset currently. In total, the Places2 contains more than 10 million images comprising more than 400 unique scene categories. The dataset includes 5000 to 30,000 training images per class, consistent with real-world frequencies of occurrence. The dataset used in the Places2 challenge 2015 (which is one of four tasks in the ImageNet large-scale visual recognition challenge) contains 401 scene categories. The training dataset of the Places2 has around 8,100,000 images, while the validation set contains 50 images per category, and the testing set has 950 images per category. Due to the much larger scale, scene recognition on the Places2 is more challenging than other datasets.

The training details of our proposed method on these three datasets are similar, as specified in Section [\ref=sec:mr_cnn]. The only difference is on the iteration numbers, due to the different sizes of training data on these datasets. Specifically, on the ImageNet and Places datasets, we decrease learning rate every 200,000 iterations and the whole training procedure stops at 750,000 iterations, while on the Places2 dataset, learning rate is decreased every 350,000 iterations and the whole training process ends at 1,300,000 iterations. We use the multi-GPU extension [\cite=WangX16] of Caffe [\cite=JiaSDKLGGD14] toolbox for our CNN training. For testing our models, we use the common 5 crops (4 corners and 1 center) and their horizontal flipping for each image at a single scale, thus having 10 crops in total for each image.

Evaluation on multi-resolution CNNs

We begin our experimental studies by investigating the effectiveness of multi-resolution CNNs on the validation sets of the ImageNet, Places, and Places2. Specifically, we study three architectures: (1) normal BN-Inception, which is trained from 256  ×  256 images, (2) deeper BN-Inception, which has a deeper structure and is trained from 384  ×  384 images, and (3) multi-resolution CNN, which is the combination of both models, by using equal fusion weights.

The results are summarized in Table [\ref=tbl:mrcnn]. First, from comparison of normal BN-Inception and deeper BN-Inception, we conclude that CNNs trained from fine resolution images (384  ×  384) are able to yield better performance than those trained by coarse resolution images (256  ×  256) on all three datasets. Such superior performance may be ascribed to the fact that fine resolution images contain richer information of visual content and more meaningful local details. In addition, the deeper BN-Inception is able to exhibit higher modeling capacity by using a deeper model, making it more powerful to capture complicated scene content. Second, we take an arithmetic average over the scores of normal BN-Inception and deeper BN-Inception as the results of multi-resolution CNNs. This simple fusion scheme further boosts the recognition performance on three datasets. These improvements indicate that the multi-level information captured by two CNNs trained from different resolution images are strongly complementary to each other. Finally, we further compare our multi-resolution CNNs with other baselines (such as AlexNet and VGGNet-16) on three datasets, and our approach outperforms these baselines by a large margin. It is worth noting that our multi-resolution CNN is a general learning framework that is readily applicable to existing network structures for enhancing their capacity.

Evaluation on knowledge guided disambiguation

We now turn to study the effect of our proposed knowledge guided disambiguation techniques described in Section [\ref=sec:kd]. To handle the issue of category ambiguity in large-scale scene recognition, we proposed two disambiguation techniques, one of which is based on knowledge of confusion matrix computed on the validation dataset, and the other one explore knowledge from extra networks. As the label ambiguity is particularly important for large-scale scene recognition, we perform experimental explorations on the Places2 dataset.

In the first knowledge guided disambiguation technique, according to the confusion matrix, we merge 401 scene categories into 372 super categories. The results are shown in Table [\ref=tbl:kd]. As can be found, by utilizing knowledge from confusion matrix, the performance of normal BN-Inception network is improved slightly. This result is a little bit surprising, as we use less category information, while obtaining higher performance. This result indicates that label ambiguity may leads to the problem of over-fitting on local subtle differences in an effort to distinguish visually closed categories (e.g., baseball field vs. stadium baseball). But these fine-scale differences may not generalize well on unseen images, so as to decrease the recognition performance on testing set. Merging the confused categories results in a stronger category-level discrimination in the training data, which leads to a better optimization on our trained model.

In our second disambiguation approach, we utilize two extra networks: one pre-trained on the ImageNet dataset (referred as object network) and the other one pre-trained on the Places (referred as scene network). We use the outputs of these extra networks as soft labels to guide the training of our CNNs in multi-task framework. The results are reported in Table [\ref=tbl:kd]. For the normal BN-Inception architecture, the scene network guided CNNs obtain a considerable performance gain by improving the original one from 17.4% to 16.7%, while the object network dose not lead to a performance improvement. For the deeper BN-Inception architecture, the performance can be improved by both object and scene network guided CNNs. These results suggest that exploitation of the knowledge from extra networks is a principle approach that regularizes the training of original networks, which turn out to improve the generalization ability. Meanwhile, we notice that the CNNs guided by scene networks consistently outperform those guided by object networks. This may be ascribed to the fact that the scene classes from the Places are more correlated with the categories in the Places2 than those object classes in the ImageNet.

Finally, we perform model fusion between normally trained CNNs and knowledge guided CNNs. From these results, we see that those knowledge guided CNNs are complementary to those normally trained CNNs. For the normal BN-Inception architecture, the best combination of (A0) and (A2) reduces the top-5 error considerably from 17.4% to 16.3%. With the deeper BN-Inception network, the best combination of (B0) and (B2) achieves a top-5 error of 15.8%, compared to the original 16.7%. These excellent fusion results suggest that our proposed knowledge guided disambiguation techniques not only improve the performance of the original models, but also provide reliable complementary models that build stronger model ensemble.

Results at the Places2 challenge 2015

To provide more convincing results, we investigate the performance of our whole model, including both the multi-resolution CNNs and knowledge guided disambiguation, on large-scale scene recognition challenge. Here we report our results on the Places2 challenge 2015, which is the largest scene recognition challenge, and is held in conjuction with the ImageNet large-scale visual recognition challenge (ILSVRC) [\cite=ILSVRC15].

Results of the Places2 challenge 2015 are summarized in Table [\ref=tbl:challenge]. Our team secured the 2nd place. Our approach was outperformed by the winner method [\cite=ShenLH15], with 0.5% reduction in top-5 error in test phase. A significant difference between two methods is that Shen et al. [\cite=ShenLH15] exploited multi-scale cropping strategy which leads to large performance gains, while we just simply used single-scale cropping in all our experiments. In addition, it is worth noting that our submission did not contain our best model architecture B2, due to deadline of the challenge. After the challenge, we finished the training of B2 model, which achieves better performance on the validation dataset. Finally, we achieve the performance of 15.5% top-5 error on the validation set, surpassing the best result of the winner method (15.7%).

Results at LSUN challenge 2016

In this subsection, we further present our results on another important scene recognition challenge, namely Large-Scale Scene Understanding (LSUN) challenge, which aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset [\cite=YuZSSX15] contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000, which is significantly unbalance. The validation data includes 300 images, and the test data has 1000 images for each category. The evaluation of LSUN classification challenge is based on top-1 accuracy.

In order to verify the effectiveness of our proposed multi-resolution CNN and knowledge guided disambiguation strategy, we transfer the learned representations on the Places2 dataset to the classification task of the LSUN challenge. Specifically, to reduce computational cost and balance the training samples from each category, we randomly sample 100,000 images from each category as our training data. Then, we use our learned CNNs on the Places2 dataset as pre-training models, and fine tune them on the LSUN dataset. The learning rate is initialized as 0.1, which is decreased by [formula] every 60,000 iterations. The batch size is set as 256. The whole training process stops at 180,000 iterations. During the test phase, by following previous common cropping techniques, we crop 5 regions with their horizontal flipping, and use 3 different scales for each image. We take an average over these prediction scores of different crops as the final result of an input image.

We report the performance of our fine-tuned CNNs on the validation set of LSUN dataset, building on various Places2 pre-trained models. Results are presented in Table [\ref=tbl:lsun1]. First, by comparing the performance of CNNs at different resolutions, we find that the deeper BN-Inception networks learned on finer resolution images yield better results than the normal BN-Inception networks (89.9% vs. 90.5%). Second, considering the strategy of knowledge guided disambiguation, both object and scene guided CNNs are capable of bringing improvements (around 0.5%) over those non-guided CNNs. Finally, we fuse prediction scores of multiple networks, and obtain the final performance with a top-1 accuracy of 91.8% on the validation set of LSUN dataset.

We further provide the results of our method on the test set of LSUN dataset, by fusing all our models. We compare our result against those of other teams attending this challenge in Table [\ref=tbl:lsun2]. Our SIAT_MMLAB team obtained the performance of 91.6% top-1 accuracy which secures the 1st place at this challenge. This excellent result strongly demonstrates the effectiveness of our proposed solution for large-scale scene recognition. Furthermore, we obtain an improvement of 0.4% top-1 accuracy (evaluated on a same database) over the strong baseline achieved by Google team, who was the winner of last LSUN challenge in 2015. Our advantages are built on the proposed multi-resolution structure and knowledge guided disambiguation strategy, by using a similar Inception architecture.

Generalization analysis

Extensive experimental results have demonstrated the effectiveness of our proposed method on large-scale datasets, by eigher training from scratch (using the Places2), or adaption with fine tuning (on the LSUN). In this subsection, we evaluate the generalization ability of our learned models, by directly applying them on two other databases: the MIT Indoor67 [\cite=QuattoniT09] and SUN397 [\cite=XiaoHEOT10], which have been used as standard benchmarks for scene recognition for many years. Most recent methods reported and compared their results on these two datasets.

The MIT Indoor67 [\cite=QuattoniT09] contains 67 indoor-scene categories and has a total of 15,620 images, with at least 100 images per category. Following the original evaluation protocol, we use 80 images from each category for training, and another 20 images for testing. The SUN397 [\cite=XiaoHEOT10] has a large number of scene categories by including 397 categories and totally 108,754 images. Each category has at least 100 images. We follow the standard evaluation protocol provided in the original paper by using 50 training and 50 test images for each category. The partitions are fixed and publicly available from [\cite=XiaoHEOT10]. Finally, the average classification accuracy of ten different tests is reported.

In this experiment, we directly use the trained B2 models as generic feature extractors, without fine tuning them on the target dataset. Specifically, the test images are first resized as 384  ×  384. We then crop image regions of different scales (384  ×  384, 346  ×  346, and 336  ×  336) from the input images. After this, these image regions are resized as 336  ×  336 and fed into our pre-trained CNNs for feature extraction. We utilize the activation of global pooling as global representation. These representations of different regions are averaged and normalized with [formula]-norm, which is used as final representation of the input image. For classifier, we use the linear SVM with LIBSVM implementation [\cite=ChangL11].

The experimental results are summarized in Table [\ref=tbl:other]. We compare the transfered representations of our model trained on the Places2 dataset against other deep models (e.g., VGGNet [\cite=SimonyanZ14a] and GoogLeNet [\cite=SzegedyLJSRAEVR15]) trained on various datasets (e.g., the Places or ImageNet). As shown in Table [\ref=tbl:other], our transferred model achieves best performance among all methods, demonstrating that our method generalizes better than the others. To the best of our knowledge, the performance of 86.7% on the MIT Indoor67 and 72.0% on the SUN397 are the best results on both datasets, which advance the state of the art substantially. We believe that our excellent performance is valuable to scene recognition community, and future large-scale recognition algorithm can be built on our pre-trained models.

Failure case analysis

Finally, we present a number of failure examples by our method from the Places2 and LSUN. These examples are illustrated in Figure [\ref=fig:ex_error]. From these examples, we notice that some scene classes are easily confused with others. In the Places2 database, the categories of supermarket, pet-store, toyshop look very similar from outdoor appearance. The classes of downtown, building, and skyscraper may co-occur in many images. Thus, scene image often contains complicated visual content which is difficult to be described clearly by a single category label, and multi-label classification can be applied to ambiguous categories. For the dataset of LSUN, the classes of bridge and tower are highly ambiguous in some cases. Similarly, the category of conference room is sometimes confused with the classroom category, due to their closed spatial layout and common objects contained. Overall, from these failure cases, we can see that scene recognition is still a challenging problem, and label ambiguity is a crucial issue in large-scale scene recognition, which still needs to to be further explored in the future.

Conclusions

In this paper, we have studied the problem of scene recognition on large-scale datasets such as the Places, Places2, and LSUN. Large-scale scene recognition suffers from two major problems: visual inconsistence (large intra-class variation) and label ambiguity (small inter-class variation). We developed powerful multi-resolution knowledge guided disambiguation framework that effectively tackle these two crucial issues. We introduced multi-resolution CNNs which are able to capture visual information from different scales. Furthermore, we proposed two knowledge guided disambiguation approaches to exploit extra knowledge, which guide CNNs training toward a better optimization, with improved generalization ability.

We conducted extensive experiments on three large-scale scene databases: the Places2, Places, and SUN, and directly transferred our learned representation to two widely-used standard scene benchmarks: the MIT Indoor67 and SUN397. Our method achieved superior performance on all five benchmarks, advancing the state-of-the-art results substantially. These results convincingly demonstrate the effectiveness of our method. Importantly, our method attended two most domain-influential challenges for large-scale scene recognition. We achieved the 2nd place at the Places2 challenge in ImageNet 2015, and the 1st place at the LSUN challenge in CVPR 2016. These impressive results further confirm the strong capability of our method.