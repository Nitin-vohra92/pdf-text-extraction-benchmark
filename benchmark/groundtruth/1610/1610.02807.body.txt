Robust Bayesian Compressed Sensing

Introduction

Compressed sensing, a new paradigm for data acquisition and reconstruction, has drawn much attention over the past few years [\cite=ChenDonoho98] [\cite=CandesTao05] [\cite=Donoho06]. The main purpose of compressed sensing is to recover a high-dimensional sparse signal from a low-dimensional linear measurement vector. In practice, measurements are inevitably contaminated by noise due to hardware imperfections, quantization errors, or transmission errors. Most existing studies (e.g. [\cite=Candes08] [\cite=Wainwright09] [\cite=WimalajeewaVarshney12]) assume that measurements are corrupted with noise that is evenly distributed across the observations, such as independent and identically distributed (i.i.d.) Gaussian, thermal, or quantization noise. This assumption is valid for many cases. Nevertheless, for some scenarios, measurements may be corrupted by outliers that are significantly different from their nominal values. For example, during the data acquisition process, outliers can be caused by sensor failures or calibration errors [\cite=LaskaDavenport09] [\cite=MitraVeeraraghavan13], and it is usually unknown which measurements have been corrupted. Outliers can also arise as a result of signal clipping/saturation or impulse noise [\cite=CarrilloBarner10] [\cite=StuderKuppinger12]. Conventional compressed sensing techniques may incur severe performance degradation in the presence of outliers. To address this issue, in previous works (e.g. [\cite=LaskaDavenport09] [\cite=MitraVeeraraghavan13] [\cite=CarrilloBarner10] [\cite=StuderKuppinger12]), outliers are modeled as a sparse error vector, and the observed data are expressed as

[formula]

where [formula] is the sampling matrix with M  ≪  N, [formula] denotes an N-dimensional sparse vector with only K nonzero coefficients, [formula] denotes the outlier vector consisting of T  ≪  M nonzero entries with arbitrary amplitudes, and [formula] denotes the additive multivariate Gaussian noise with zero mean and covariance matrix [formula]. The above model can be formulated as a conventional compressed sensing problem as

[formula]

Efficient compressed sensing algorithms can then be employed to estimate the sparse signal as well as the outliers. Recovery guarantees of [formula] and [formula] were also analyzed in [\cite=LaskaDavenport09] [\cite=MitraVeeraraghavan13] [\cite=CarrilloBarner10] [\cite=StuderKuppinger12].

The rationale behind the above approach is to detect and compensate for these outliers simultaneously. Besides the above method, another more direct approach is to identify and exclude the outliers from sparse signal recovery. Although it may seem preferable to compensate rather than simply reject outliers, inaccurate estimation of the compensation (i.e. outlier vector) could result in a destructive effect on sparse signal recovery, particularly when the number of measurements is limited. In this case, identifying and rejecting outliers could be a more sensible strategy. Motivated by this insight, we develop a Bayesian framework for robust compressed sensing, in which a set of binary indicator variables are employed to indicate which observations are outliers. These variables are assigned a beta-Bernoulli hierarchical prior such that their values are confined to be binary. Also, a Gaussian inverse-Gamma prior is placed on the sparse signal to promote sparsity. A variational Bayesian method is developed to find the approximate posterior distributions of the indicators, the sparse signal and other latent variables. Simulation results show that the proposed method achieves a substantial performance improvement over the compensation-based robust compressed sensing method.

Hierarchical Prior Model

We develop a Bayesian framework which employs a set of indicator variables [formula] to indicate which observation is an outlier, i.e. zm = 1 indicates that ym is a normal observation, otherwise ym is an outlier. More precisely, we can write

[formula]

where [formula] denotes the mth row of [formula], em and wm are the mth entry of [formula] and [formula], respectively. The probability of the observed data conditional on these indicator variables can be expressed as

[formula]

in which those "presumed outliers" are automatically disabled when calculating the probability. To infer the indicator variables, a beta-Bernoulli hierarchical prior [\cite=HeCarin09] [\cite=PaisleyCarin09] is placed on [formula], i.e. each component of [formula] is assumed to be drawn from a Bernoulli distribution parameterized by πm

[formula]

and πm follows a beta distribution

[formula]

where e and f are parameters characterizing the beta distribution. Note that the beta-Bernoulli prior assumes the random variables {zm} are mutually independent, and so are the random variables {πm}.

To encourage a sparse solution, a Gaussian-inverse Gamma hierarchical prior, which has been widely used in sparse Bayesian learning (e.g. [\cite=JiXue08] [\cite=ZhangRao13] [\cite=YangXie13] [\cite=FangShen15]), is employed. Specifically, in the first layer, [formula] is assigned a Gaussian prior distribution

[formula]

where p(xn|αn) = N(xn|0,α- 1n), and [formula] are non-negative hyperparameters controlling the sparsity of the signal [formula]. The second layer specifies Gamma distributions as hyperpriors over the precision parameters {αn}, i.e.

[formula]

where the parameters a and b are set to small values (e.g. a = b = 10- 10) in order to provide non-informative (over a logarithmic scale) hyperpriors over {αn}. Also, to estimate the noise variance, we place a Gamma hyperprior over γ, i.e.

[formula]

where the parameters c and d are set to be small, e.g. c = d = 10- 10. The graphical model of the proposed hierarchical prior is shown in Fig. [\ref=fig3].

Variational Bayesian Inference

We now proceed to perform Bayesian inference for the proposed hierarchical model. Let [formula] denote the hidden variables in our hierarchical model. Our objective is to find the posterior distribution [formula], which is usually computationally intractable. To circumvent this difficulty, observe that the marginal probability of the observed data can be decomposed into two terms

[formula]

where

[formula]

and

[formula]

where [formula] is any probability density function, [formula] is the Kullback-Leibler divergence between [formula] and [formula]. Since [formula], it follows that L(q) is a rigorous lower bound on [formula]. Moreover, notice that the left hand side of ([\ref=variational-decomposition]) is independent of [formula]. Therefore maximizing L(q) is equivalent to minimizing [formula], and thus the posterior distribution [formula] can be approximated by [formula] through maximizing L(q). Specifically, we could assume some specific parameterized functional form for [formula] and then maximize L(q) with respect to the parameters of the distribution. A particular form of [formula] that has been widely used with great success is the factorized form over the component variables in [formula] [\cite=TzikasLikas08]. For our case, the factorized form of [formula] can be written as

[formula]

We can compute the posterior distribution approximation by finding [formula] of the factorized form that maximizes the lower bound L(q). The maximization can be conducted in an alternating fashion for each latent variable, which leads to [\cite=TzikasLikas08]

[formula]

where 〈  ·  〉· denotes an expectation with respect to the distributions specified in the subscript. More details of the Bayesian inference are provided below.

1) Update of [formula]: We first consider the calculation of [formula]. Keeping those terms that are dependent on [formula], we have

[formula]

where

[formula]

[formula] and [formula] denote the expectation of [formula] and [formula], respectively. It is easy to show that [formula] follows a Gaussian distribution with its mean and covariance matrix given respectively by

[formula]

2) Update of [formula]: Keeping only the terms that depend on [formula], the variational optimization of [formula] yields

[formula]

The posterior [formula] therefore follows a Gamma distribution

[formula]

in which ã and n are given respectively as

[formula]

3). Update of qγ(γ): The variational approximation of qγ(γ) can be obtained as:

[formula]

Clearly, the posterior qγ(γ) obeys a Gamma distribution

[formula]

where [formula] and d̃ are given respectively as

[formula]

in which

[formula]

4) Update of [formula]: The posterior approximation of [formula] yields

[formula]

Clearly, zm still follows a Bernoulli distribution with its probability given by

[formula]

where C is a normalizing constant such that P(zm = 1) + P(zm = 0) = 1, and

[formula]

The last two equalities can also be found in [\cite=PaisleyCarin09], in which Ψ(  ·  ) represents the digamma function.

5) Update of [formula]: The posterior approximation of [formula] can be calculated as

[formula]

It can be easily verified that [formula] follows a Beta distribution, i.e.

[formula]

In summary, the variational Bayesian inference involves updates of the approximate posterior distributions for hidden variables [formula], [formula], [formula], [formula], and γ in an alternating fashion. Some of the expectations and moments used during the update are summarized as

[formula]

where [formula] denotes the nth diagonal element of [formula].

Simulation Results

We now carry out experiments to illustrate the performance of our proposed method which is referred to as the beta-Bernoulli prior model-based robust Bayesian compressed sensing method (BP-RBCS). As discussed earlier, another robust compressed sensing approach is compensation-based and can be formulated as a conventional compressed sensing problem ([\ref=RCS-compensation]). For comparison, the sparse Bayesian learning method [\cite=Tipping01] [\cite=JiXue08] is employed to solve ([\ref=RCS-compensation]), and this method is referred to as the compensation-based robust Bayesian compressed sensing method (C-RBCS). Also, we consider an "ideal" method which assumes the knowledge of the locations of the outliers. The outliers are then removed and the sparse Bayeisan learning method is employed to recover the sparse signal. This ideal method is referred to as RBCS-ideal, and serves as a benchmark for the performance of the BP-RBCS and C-RBCS. Note that both C-RBCS and RBCS-ideal use the sparse Bayesian learning method for sparse signal recovery. The parameters {a,b,c,d} of the sparse Bayesian learning method are set to a = b = c = d = 10- 10. Our proposed method involves the parameters {a,b,c,d,e,f}. The first four are also set to a = b = c = d = 10- 10. The beta-Bernoulli parameters {e,f} are set to e = 0.7 and f = 1 - e = 0.3 since we expect that the number of outliers is usually small relative to the total number of measurements. Our simulation results suggest that stable recovery is ensured as long as e is set to a value in the range

[formula]

. The signal [formula] contains K nonzero entries that are independently drawn from a unit circle. Suppose that T out of M measurements are corrupted by outliers. For those corrupted measurements {ym}, their values are chosen uniformly from

[formula]

Conclusions

We proposed a new Bayesian method for robust compressed sensing. The rationale behind the proposed method is to identify the outliers and exclude them from sparse signal recovery. To this objective, a set of indicator variables were employed to indicate which observations are outliers. A beta-Bernoulli prior is assigned to these indicator variables. A variational Bayesian inference method was developed to find the approximate posterior distributions of the latent variables. Simulation results show that our proposed method achieves a substantial performance improvement over the compensation-based robust compressed sensing method.