Introduction

Monte Carlo methods are becoming increasingly popular for filtering in large-scale geophysical applications, such as reservoir modeling and numerical weather prediction, where they are often called ensemble methods for data assimilation. The challenging (and interesting) peculiarity of this type of applications is that the state space is extremely high dimensional (both observation y and state x are of the order of 108), while the computational cost of the time integration step limits the sample size to less than a hundred. Because of those particularly severe constraints, the emphasis is on developing approximate but highly efficient methods, typically relying on strong assumptions and exploiting parallel architectures.

The provides a fully general Bayesian solution to filtering [\cite=gordon_novel_1993] [\cite=pitt_filtering_1999] [\cite=doucet_smc_2001], but it is well-known that it suffers from sample degeneracy and cannot be applied to high-dimensional settings [\cite=snyder_obstacles_2008]. The most popular alternative to the in large-scale applications is the [\cite=evensen_sequential_1994] [\cite=evensen_ensemble_2003], a successful but heuristic method, which implicitly assumes that the predictive distribution is Gaussian.

Three main routes for adapting to high-dimensional settings can be identified. The first one is to use an adaptive with a carefully chosen proposal distribution [\cite=pitt_filtering_1999] [\cite=van_leeuwen_nonlinear_2010]. A second approach is to build hybrid methods between the and the , as for example the [\cite=frei_enkpf_2013]. A third route is localization, as it is a key element of the success of the in practice and could avoid the curse of dimensionality [\cite=snyder_obstacles_2008] [\cite=rebeschini_can_2015].

The first approach requires an explicit model for the transition probabilities, which is typically not available in practical applications. Furthermore [\cite=snyder_performance_2015] showed that even with the optimal proposal distribution the suffers from the curse of dimensionality. Therefore in the present paper we focus on the second and third approaches and explore some possible localized algorithms based on the and the . In a simulation study, we extend an example of [\cite=snyder_obstacles_2008] to illustrate how localization seemingly overcomes the curse of dimensionality, but at the same time introduces some harmful discontinuities in the estimated state. In a second experiment we show how local algorithms can be applied effectively to a filtering problem with the Lorenz96 model [\cite=lorenz_optimal_1998]. The results from these numerical experiments highlight key differences between the algorithms and demonstrate that local are promising candidates for large-scale filtering applications.

Ensemble filtering algorithms

Consider a state space model with state process (xt) and observations (yt), where the state process evolves according to some deterministic or stochastic dynamics and the observations are assumed to be independent given the state process, with likelihood [formula]. The goal is to estimate the conditional distribution of xt given [formula], called the filtering distribution and which we denote by πft. In general it is possible to solve this problem recursively by alternating between a prediction step where the filtering distribution at time (t - 1) is propagated into the predictive distribution πpt at time t, and an update step, also called assimilation, where the predictive distribution is updated with the current observation to compute πft. The update step is done by applying Bayes' rule as [formula], where the predictive distribution is the prior and the filtering distribution the posterior to be estimated.

Sequential Monte Carlo methods [\cite=doucet_smc_2001] approximate the predictive and filtering distributions by finite samples, or ensembles of particles, denoted by (xp,it) and (xf,it) respectively, for [formula]. The update step consists in transforming the predictive ensemble (xp,it) into an approximate sample from the filtering distribution πft. We briefly present the and in this context and give an overview of the . Henceforth we consider the update step only and drop the time index t. Additionally for the and we assume that the observations are linear and Gaussian, i.e. [formula], with φ(y; a,b) the Gaussian density with mean a and covariance b evaluated at y.

approximates the filtering distribution as a mixture of point masses at the predictive particles, reweighed by their likelihood. More precisely:

[formula]

A non-weighted sample from this distribution can be obtained by resampling, for example with a balanced sampling scheme [\cite=kunsch_recursive_2005]. The is asymptotically correct (also for non-Gaussian likelihoods), but to avoid sample degeneracy it needs a sample size which increases exponentially with the size of the problem (for more detail see [\cite=snyder_obstacles_2008]).

is a heuristic method which applies a update to each particle with stochastically perturbed observations. More precisely it constructs (xf,i) as a balanced sample from the following Gaussian mixture:

[formula]

where [formula] is the Kalman gain estimated with the sample covariance p:

[formula]

The stochastic perturbations of the observations are added to ensure that the filter ensemble has the correct posterior covariance on expectation. Some variants of the algorithm use a square-root scheme such that the filter ensemble has the exact correct posterior covariance, but such methods are out of the scope of the present paper (see for example [\cite=whitaker_ensemble_2002] [\cite=tippett_ensemble_2003] [\cite=hunt_efficient_2007]).

combines the and the by decomposing the update step into two stages as [formula], following the progressive correction idea of [\cite=musso2001improving]. The first stage, going from πp(x) to [formula] is done with an . The second stage is done with a and goes from πγ(x) to [formula]. The resulting posterior distribution can be derived analytically as the following weighted Gaussian mixture:

[formula]

where the expressions for the parameters of this distribution and more details on the algorithm can be found in [\cite=frei_enkpf_2013]. To produce the filtering sample (xf,i), one first samples the mixture components with probability proportional to the weights αγ,i, using for example a balanced sampling scheme, and then adds an individual noise term with covariance Σγ to each resampled particle. The single parameter γ defines a continuous interpolation between the (γ = 0) and the (γ = 1).

Local algorithms

Localization consists essentially in updating the state vector by ignoring long range dependencies. This is a sensible thing to do in geophysical applications where the state represents discretized spatially correlated fields of physical quantities. By localizing the update step and using local observations only, one introduces a bias, but achieves a considerable gain in terms of variance reduction for finite sample sizes. For local algorithms the error is asymptotically bigger than for a global algorithm, but it is not dependent on the system dimension anymore and therefore avoids the curse of dimensionality. Furthermore local algorithms can be efficiently implemented in parallel and thus take advantage of modern computing architectures.

The consists in applying a separate at each site, but limiting the influence of the observations to sites that are spatially close (there are different ways to accomplish this in practice, see for example [\cite=houtekamer_sequential_2001] [\cite=ott_lenkf_2004] [\cite=hunt_efficient_2007]). Analogously, we define the as a localized version of the , where the update is done at each location independently, considering only observations in a ball of radius [formula]. In order to avoid arbitrary "scrambling" of the particles indices, we use a balanced sampling scheme [\cite=kunsch_recursive_2005], and some basic ad-hoc methods to reduce the number of discontinuities, but we do not solve this problem optimally as it would greatly hinder the efficiency of the algorithm.

For the we define two different local algorithms: the , in which localization is done exactly as for the , and the , in which data are assimilated sequentially but their influence is restricted to a local area. The does not take particular care of the introduced discontinuities beyond what is done for the , but it is straightforward to implement. The , on the other hand, uses conditional resampling in a transition area surrounding the local assimilation window, which ensures that there are no sharp discontinuities, but it involves more overhead computation. For more detail about the local algorithms see [\cite=robert_arxiv_2016].

Simulation studies

We conducted two simulation studies: first a one-step conjugate normal setup where the effect of localization can be closely studied, and second a cycled experiment with the Lorenz96 model, a non-linear dynamical system displaying interesting non-Gaussian features.

Conjugate normal setup

We first consider a simple setup similar to the one in [\cite=snyder_obstacles_2008], with a predictive distribution πp assumed to be a N-dimensional normal with mean zero and covariance Σp. To imitate the kind of smooth fields that we encounter in geophysical applications, we construct the covariance matrix as Σpii = 1 and Σpij = KGC(d(i,j) / r), where KGC is the Gaspari-Cohn kernel [\cite=gaspari_construction_1999], d(i,j) the distance between sites i and j on a one-dimensional spatial domain with periodic boundary conditions, and the radius r in the denominator is chosen such that the covariance has a finite support of 20 grid points. From this process we generate observations of every component of x and standard Gaussian noise:

[formula]

In order to study finite sample properties of the different algorithms, we compute the of the ensemble mean in estimating the value x at each location, which we denote by mse(x). Because the prior is conjugate to the likelihood, we can compute the mse(x) of the posterior mean analytically for known Σp as the trace of the posterior covariance matrix and use this as a reference. For the simulation we use a sample size of k = 100 and average the results over 1000 runs. It should be noted that because the predictive distribution is normal, this setup is favorable to the and , but the should still perform adequately. For the local algorithms the localization radius [formula] was set to 5, resulting in a local window of 11 grid points, which is smaller than the correlation length used to generate the data. Later on we study the effect of [formula] on the performance of the algorithms. For the algorithms the parameter γ was fixed to 0.25, which means a quarter of and three-quarter of . In practice one would rather choose the value of γ adaptively, but the exact value does not influence the qualitative conclusions drawn from the experiments and fixing it in this way makes the comparison easier.

An example of a sample from the filtering distribution produced by different local algorithms is shown in , with each particle represented as a light blue line, the true state in dark and the observations in red. For more clarity the ensemble size is set to 10 and the system dimension to 40. While all algorithms manage to recover more or less the underlying state, it is clear that they vary in terms of quality. The in particular suffers from sample depletion, even when applied locally, and displays strong discontinuities. If one looks closely at the ensemble, discontinuities can also be identified. The and , on the other hand, produce smoother posterior particles. This example is useful to highlight the behavior of the different local algorithms qualitatively, but we now proceed to a more quantitative assessment with a repeated simulations experiment.

In the first row of , the mse(x) is plotted as a function of the system dimension N, for the global algorithms on the left and the local algorithms on the right. The values are normalized by the optimal mse(x) to make them more interpretable. The degenerates rapidly with an mse(x) worse than using the prior mean (second dashed line). The and the suffer as well from the curse of dimensionality, although to a lesser extent. The local algorithms, on the other hand, are immune to the increase of dimensions N and their mse(x) is constant and very close the optimum, which confirms that localization is working as expected. It is not visible because of the scale, but the , and make an error of less than 5% while the is 20% worse than the optimum.

As the old statistical adage goes, there is no free lunch: localization comes at a cost, particularly for type of algorithms. When doing the update locally with the , the filtering samples are relatively smooth fields, because the update applies spatially smoothly varying corrections to the predictive ensemble. However, for the , when different particles are resampled at neighboring sites, arbitrarily large discontinuities can be created. While this might be discarded as harmless, it is not the case when the fields of interest are spatial fields of physical quantities used in numerical solvers of partial differential equations. One way to measure the impact of discontinuities is to look at the in estimating the lag one increments Δx, which we denote as mse(Δx). While the mse(x) is computed for the posterior mean, the mse(Δx) is computed for each particle separately and then averaged. We again compute the expected mse(Δx) under the conjugate posterior distribution and use it as reference.

The plots in the second row of show this quantity for the different algorithms averaged over 1000 simulation runs. The mse(Δx) of the local algorithms is still constant as a function of N, as expected, but in the case of the and the its value is worse than for the respective global algorithms. On the other hand, the and the improve on their global counterparts and have an error less than 5% worse than the optimum.

In the previous experiment we fixed the localization radius [formula] to 5 and looked at what happens in terms of prediction accuracy with the mse(x), and in terms of discontinuities with the mse(Δx). In we look at as a function of [formula], fixing N to 200 and k to 100. For large values of [formula] the mse(Δx) is smallest as discontinuities are avoided, but the mse(x) is not optimal, particularly for the which is even off-chart. As [formula] is reduced the mse(x) decreases for all methods, while mse(Δx) is kept constant for a wide range of [formula] values. At some point, different for each algorithm, the localization is too strong and becomes detrimental, with both mse(x) and mse(Δx) sharply increasing. This behavior illustrates the trade-off at hand when choosing the localization radius: picking a too small value introduces a bias by neglecting useful information and creates too much discontinuities, while choosing a too large value does not improve mse(Δx) but leads to poorer performance in terms of mse(x).

Filtering with the Lorenz96 model

The Lorenz96 model [\cite=lorenz_optimal_1998] is a 40 dimensional non-linear dynamical system which displays a rich behavior and is often used as a benchmark for filtering algorithms. In [\cite=frei_enkpf_2013] it was shown that the outperforms the in some setups of the Lorenz96 model, but the sample size required was of 400. In the present experiment we use the same setup as in [\cite=frei_enkpf_2013] but with much smaller and realistic ensemble sizes. The data are assimilated at time intervals of 0.4, which leads to strong non-linearities and thus highlights better the relative advantages of the . Each experiment is run for 1000 cycles and repeated 20 times, which provides us with stable estimates of the average performance of each algorithm. As in [\cite=frei_enkpf_2013], the parameter γ of the is chosen adaptively such that the equivalent sample size is between 25% and 50%. It should be noted that for local algorithms a different γ can be chosen at each location, which provides added flexibility and allows to adapt to locally non-Gaussian features of the distribution. We consider mse(x) only and denote it simply by . It takes also errors in the estimation of increments into account through integration in time during the propagation steps.

In the left panel of the of the global algorithms is plotted for increasing ensemble size. The is not represented as it diverges for such small values. The is computed relative to the performance of the prior, which is simply taken as the of an ensemble of the same size evolving according to the dynamical system equations but not assimilating any observations. With ensemble sizes smaller than 50, the filtering algorithms are not able to do better than the prior, which means that trying to use the observations actually makes them worse than not using them at all. Only for ensemble sizes of 100 and more do the global algorithms start to become effective. In practice we are interested in situations where the ensemble size is smaller than the system dimension (here 40), and thus the global methods are clearly not applicable.

On the right panel of we show the same plot but for the local algorithms. For sample sizes as small as 20 or 30 the performances are already quite good. The , however, does not work at all, probably because it still suffers from sample depletion and because the introduced discontinuities have a detrimental impact during the prediction step of the algorithm. The clearly outperforms the other algorithms, particularly for smaller sample sizes, which seems to indicate that it can localize efficiently the update without harming the fields with discontinuities which would have a bad influence on the prediction step.

In order to better highlight the trade-off of localization, we plot similar curves but as a function of the localization radius [formula] in . One can see that for small k (left panel), the error is increasing with [formula], which shows that localization is absolutely necessary for the algorithm to work. For k = 40 (right panel), the first decreases and then increases, with an optimal [formula]. Experiments with larger values display curves that get flatter and flatter as k increases, showing that as the ensemble size is larger, the localization strength needed is smaller, as expected.

Conclusion

Localization is an effective tool to fight some of the difficulties associated with high-dimensional filtering in large-scale geophysical applications. Methods such as the can be localized easily and successfully as they vary smoothly in space. At first sight, the seems to overcome the curse of dimensionality; however, looking more carefully, one notices that it introduces harmful discontinuities in the updated fields. The two localized both overcome the curse of dimensionality and handle better the problem of discontinuities.

The simple conjugate example studied in this paper highlighted the potential improvements coming from localization, as well as the pitfalls when applied blindly to the . The trade-off between the bias coming from localization and the gain coming from the reduced variance was illustrated by exploring the behavior of the algorithms as a function of the localization radius [formula]. Experiments with the Lorenz96 model showed that local algorithms can be successfully applied with ensemble size as small as 20 or 30 and highlighted the localization trade-off. In particular the fared remarkably well, outperforming both the and the in this challenging setup. This confirms other results that we obtained with more complex dynamical models mimicking cumulus convection [\cite=robert_arxiv_2016] and encourages us to pursue further research with localized in a large-scale application in collaboration with Meteoswiss.