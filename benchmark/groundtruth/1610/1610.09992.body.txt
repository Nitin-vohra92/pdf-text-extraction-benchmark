|Vibeke.Skytt, Tor.Dokken, Heidi.Dahl |Q.Harpham

Deconfliction and Surface Generation from Bathymetry Data Using LR B-splines

Introduction

Bathymetry data is usually obtained by single or multi beam sonar or bathymetry LIDAR. Sonar systems acquire data points by collecting information from reflected acoustic signals. Single beam sonar is the traditional technique for acquiring bathymetry data and it collects data as discrete point data long the the path of a vessel equipped with single beam acoustic depth sounders. The equipment is easy to attach to the boat and the acquisition cost is lower than for alternative acquisition methods. The obtained data sets, however, have a scan line like pattern, which gives a highly inhomogeneous point cloud as input to a surface generation application.

Acquisition of bathymetric data with Multi Beam Echo Sounder (MBES) is nowadays of common use. A swath MBES system produces multiple acoustic beams from a single transducer in a wide angle. It generates points in a large band around the vessel on which the equipment is installed. The swath width varies from 3 to 7 times the water depth. In shallow areas, the results of a multi beam sonar degenerates to that of the single beam sonar as the sonar angle is reduced due to a short distance to the sea bottom. Multi beam sonar data acquisition is described in some detail in [\cite=outliers4].

LIDAR (light detection and ranging) measures elevation or depth by analyzing the reflections of pulses of laser light from an object. Near shore, especially in shallow areas or in rough waters that are difficult to reach by a sea-born vessel, data acquisition using bathymetry LIDAR is a good alternative to sonar. Bathymetry LIDAR differs from topography LIDAR by the wavelength of the signals that are used. To be able to penetrate the water, a shorter wavelength is required, so green light is used instead of red. This change reduces the effect of the power used by the laser, and bathymetry LIDAR becomes more costly than the topography equivalent.

Our aim is to represent a specified region with a seamless surface. Some parts of the region are only covered by one survey, while other areas are covered by numerous surveys obtained by different acquisition methods. Where no survey data exists, even vector data created from navigation charts may be taken as input. Collections of bathymetric surveys are a source of potentially "big data" structured as point clouds. Individual surveys vary both spatially and temporally and can overlap with many other similar surveys. Where depth soundings differ greatly between surveys, a strategy needs to be employed to determine how to create an optimal bathymetric surface based on all of the relevant, available data, i.e., select the best data for surface creation.

The digital elevation model (DEM) is the most common format for representing surfaces in geographical information systems (GIS). DEM uses a raster format for storage. Rasters are rectangular arrays of cells (or pixels), each of which stores a value for the part of the surface it covers. A given cell contains a single value, so the amount of detail that can be represented for the surface is limited by the raster cell resolution. The elevation in a cell is frequently estimated using the height values of nearby points. The estimation methods include, but are not restricted to, the inverse weighted interpolation method, also called Shepard's method [\cite=grid:shepard], natural neighbour interpolation, radial basis functions and kriging  [\cite=rbf1] [\cite=grid:interpolate] [\cite=grid:kriging]. Alternatively, one of the existing points lying within the cell can be selected to represent the cell elevation.

Triangulated irregular network (TIN) is used to some extend in GIS context. Sample data points serve as vertices in the triangulation, which normally is computed as a Delaunay triangulation. A triangulated surface can interpolate all points in the point cloud exactly, but for large data sizes an approximate solution is more appropriate. The triangulation data structure is flexible and an irregular and well-chosen distribution of nodes allows capturing rapid changes in the represented sea bed or terrain.

The purpose of trend surfaces is not representation of terrains, but data analytics. These surfaces are described by polynomials of low degree globally approximating the data. Trend surface analysis is used to identify general trends in the data and the input data can be separated into two components: the trend corresponding to the concept of regional features and the residual corresponding to local features. Very often, however, the global, polynomial surface becomes too simplistic compared to the data.

In GIS context, splines are almost entirely understood as regularized splines or splines in tension in the context of radial basis functions. Only in rare instances splines are used for terrain modeling. However, Sulebak et. al., [\cite=Sulebak], use multi-resolution splines in geomorphology. We aim at using polynomial spline surfaces to represent our final result. Moreover, in the process of selecting data surveys for the surface generation, we use spline surfaces as extended trend surfaces. Spline surfaces are able to compactly represent smooth shapes, but our bathymetry data are not likely to describe a globally smooth seabed. Thus, we turn our attention towards locally refineable splines in the form of LR B-spline surfaces.

Section [\ref=LRsplines] gives a brief overview of the concept of LR B-splines. In Section [\ref=surfgen], we will present the construction of LR B-spline surfaces and collections of such surfaces approximating point clouds from bathymetry data. The topic of Section [\ref=deconfliction] is the deconfliction process discussed in the context of outliers detection, both for Geo-spatial data and in a more general setting. Finally, we will present a conclusion including plans for further work in Section [\ref=conclusion].

LR B-splines

LR B-spline surfaces are spline surfaces defined on a box partition as visualized in Figure [\ref=fig:box_partition], see [\cite=lr:lrsplines] for a detailed description of the theory.

In contrast to the well-known tensor-product spline surfaces, LR B-spline spline surfaces posses the property of local refineability. New knot lines, not covering the entire domain of the surface, can be added to the surface description. The new knot line must, however, cover the support of at least one B-spline. The local refinement property implies that models with varying degree of detail can be represented without the drastic increase in model size that would arise in the tensor-product representation. Other approaches addressing the problem of lack of local refimenent methods in the tensor-product construction are hierarchical splines [\cite=approx:hierarchical] and T-splines [\cite=lr:tsplines].

An LR-B spline surface F is expressed with respect to parameters u and v as

[formula]

where Pi are the surface coefficients, Ni are the associated B-splines and si are scaling factors that ensure partition of unity. The B-splines are constructed by taking the tensor-products of univariate B-splines, and are thus defined on a set of knots in both parameter directions. They have polynomial degree d1 and d2 in the first and second parameter direction, respectively.

LR B-spline surfaces possess most of the properties of tensor-product spline surfaces, such as non-negative B-spline functions, limited support of B-splines and partition of unity, which ensure numerical stability and modelling accuracy. Linear independence of the B-spline functions is not guaranteed by default. For LR B-spline surfaces of degree two and three and knot insertion restricted to the middle of knot intervals, no cases of linear dependency are known, but the mathematocal proof is still not completed. Actual occurrences of linear dependence can be detected by the peeling algorithm,  [\cite=peeling], and it can be resolved by a strategy of carefully chosen knot insertions.

Surface Generation

We assume the input to be one point cloud where the initial bathymetry data is translated to points represented by their x, y, and z-coordinates. The points can be obtained from one data survey or collected from several surveys. No further preprocessing of the points is performed.

To exploit the local refineability of the LR B-spline surfaces and to optimize the positioning of the degrees of freedom in the surface, we apply an adaptive surface generation approach using two different approximation methods over given spline spaces.

Due to the acquisition methods, bathymetry data is normally projective onto their x and y-coordinates. Thus, it is possible to parameterize the points by these coordinates and approximate the height values (z-coordinates) by a function. In steep areas, however, a parametric surface would be more appropriate. This issue is discussed in [\cite=IQmulusbook]. In this paper, we will concentrate on approximation of height values.

The description of the surface generation method in the remainder of this section is partly fetched from [\cite=LRapprox] and [\cite=IQmulusbook].

An Iterative Framework for Approximation with LR-spline Surfaces

The aim of the approximation is to fit an LR-spline surface to a given point cloud within a certain threshold or tolerance. Normally this is achieved for the majority of points in the cloud, and any remaining points that are not within the tolerance after a certain number of iterations can be subject to further investigation. Algorithm [\ref=alg:framework] outlines the framework of the adaptive surface approximation method.

The polynomial bi-degree of the generated LR B-spline surface can be of any degree higher than one, however, in most cases a quadratic (degree two) surface will suffice. Quadratic surfaces ensure C1-continuity across knot lines with multiplicity one, and as terrains often exhibits rapid variations higher order smoothness may be too restrictive.

The algorithm is initiated by creating a coarse tensor-product spline space. An initial LR B-spline surface is constructed by approximating the point cloud in this spline space. A tensor-product spline space can always be represented by an LR B-spline surface while an LR B-spline surface can be turned into a tensor-product spline surface by extending all knot lines to become global in the parameter domain of the surface.

In each iteration step, a surface approximation is performed. Two approximation methods are used for this purpose, least squares approximation and multi-resolution B-spline approximation (MBA). Both approximation methods are general algorithms applied to parametric surfaces, which have been adapted for use with LR B-splines. Typically least squares approximation is used for the first iterations as it is a global method with very good approximation properties, while we turn to the MBA method when there is a large variety in the size of the polynomial elements of the surface. A comparison of the performance of the two methods can be found in [\cite=LRapprox]. The distances between the points in the point cloud and the surface is computed to produce a distance field. In our setting the surface is parameterized by the xy-plane and the computation can be performed by a vertical projection mainly consisting of a surface evaluation.

Next we identify the regions of the domain that do not meet the tolerance requirements and refine the representation in these areas to provide more degrees of freedom for the approximation. Specifically, we identify B-splines whose support contain data points where the accuracy is not satisfied, in their support are identified and introduce new knot lines, in one or two parameter directions depending on the current distance field configuration. The new knot lines must cover the support of at least one B-spline. In each iteration step, many new knot line segments will be inserted in the surface description, giving rise to the splitting of many B-splines. The splitting of one B-spline may imply that an existing knot line segment partly covering its support will now completely cover the support of one of the new B-splines that, in turn, is split by this knot line.

Least Squares Approximation

Least squares approximation is a global method for surface approximation where the following penalty function is minimized with respect to the coefficients Pi, over the surface domain, Ω:

[formula]

Here [formula], are the input data points. J(F) is a smoothing term, which is added to the functional to improve the surface quality and ensure a solvable system even if some basis functions lack data points in their support. The approximation is weighted (by the scalars α1 and α2) in order to favour either the smoothing term or the least squares approximation, respectively. The smoothing term is given by

[formula]

The expression approximates the minimization of a measure involving surface area, curvature and variation in curvature. Using parameter dependent measures, the minimization of the approximation functional is reduced to solving a linear equation system. In most cases w1 = 0 while w2 = w3. In our case, however, w2 = 1 and w3 = 0 as we utilize 2nd degree polynomials. A number of smoothing terms exist. The one given above is presented in [\cite=spline:smooth1]. Other measures can be found in [\cite=spline:smooth2], and [\cite=approx:greiner] looks into the effect of choosing different smoothing functionals.

In Equation [\ref=eq:Jf], a directional derivative is defined from the first, second and third derivatives of the surface, and in each point (x,y) in the parameter domain, this derivative is integrated radially. The result is integrated over the parameter domain.

Experience shows that the approximation term must be prioritized in order to achieve a good approximation to the data points. This is in conflict with the role of the smoothing term as a guarantee for a solvable equation system. Estimated height values in areas sparsely populated by data points, are thus included to stabilize the computations. Some details on the stability of least squares approximation used in this context can be found in [\cite=LRapprox].

Locally Refined Multilevel B-spline Approximation (LR-MBA)

Multilevel B-spline approximation (MBA) is a local approximation method [\cite=lr:mba]. Surface coefficients are computed with respect to the distances between data points in the support of the B-spline functions corresponding to the coefficients and a current surface. The procedure is explicit and does not require solving an equation system.

A surface approximating the residuals between a point cloud and a current surface is computed as follows. Let [formula], be the data points above the support of a given B-spline and rc the residual corresponding to [formula]. As the initial point cloud is scattered, there is a large variation in the number of points. If the B-spline has no points in its support or if all the points are closer to the surface than a prescribed tolerance, the corresponding coefficient is set to zero. Otherwise, a coefficient Pi is determined by the following expression:

[formula]

where φc is computed for the residual of each data point as

[formula]

The sum in the denominator is taken over all B-splines which contain (xc,yc) in their support.

The algorithm is based on a B-spline approximation technique proposed for image morphing and is explained in [\cite=lr:mba2] for multilevel B-spline approximation. In the original setting a number of difference surfaces approximating the distances between the point cloud and the current surface is computed. The final surface is evaluated by computing the sum of the initial surface and all the difference surfaces. In the LR B-splines setting, the computed difference function is incrementally added to the initial surface at each step giving a unified expression for the surface.

Tiling and Stitching

Very large point clouds are unfit for being approximated by one surface due to memory restrictions and high computation times. During surface generation each data point is accessed a number of times, and a tiling approach allows for efficient parallelization over several nodes. Moreover, a large number of points are potentially able to represent a high level of detail, which gives rise to approximating LR B-spline surfaces with higher data size. The surface size should, however, be restricted as the non-regularity of the polynomial patches penalizes data structure traversals when the surface is large (more than 50 000 polynomial patches).

We apply tiling to improve computational efficiency and limit the size of the produced surface, and select a regular tiling approach to enable easy identification of tiles based on the x -  and y -  coordinates of the points. Figure [\ref=fig:tile] (a) shows a regular tiling based on a dataset with 131 million points, and (b) a set of LR B-spline surfaces approximating the points. The computation is done tile by tile, and applying tiles with small overlaps gives a surface set with overlapping domains. Each surface is then restricted to the corresponding non-overlapping tile yielding very small discontinuities between adjacent surfaces.

To achieve exact C1-continuity between the surfaces, stitching is applied. The surfaces are refined locally along common boundaries to get sufficient degrees of freedom to enforce the wanted continuity. For C0-continuity a common spline space for the boundary curves enables the enforcement of equality of corresponding coefficients. C1-continuity is most easily achieved by refining the surface to get a tensor-product structure locally along the boundary and adapting corresponding pairs of coefficients from two adjacent surfaces along their common boundary to ensure equality of cross boundary derivatives. C1-continuity can always be achieved in the functional setting, for parametric surfaces it may be necessary to relax the continuity requirement to G1.

Examples

Example 1 We will describe the process of creating an LR B-spline surface from a point cloud with 14.6 million points. The points are stored in a 280 MB binary file. We apply Algorithm [\ref=alg:framework] using a combination of the two approximation methods and examine different stages in the process. Figure [\ref=fig:pts] shows the point cloud, thinned with a factor of 32 to be able to distinguish between the points.

The initial surface approximation with a lean tensor-product mesh is shown in Figure [\ref=fig:sf0]. While the point cloud covers a non-rectangular area the LR B-spline surface is defined on a regular domain (b), thus the surface (a) is trimmed with respect to the extent of the point cloud. The last figure (c) shows the points coloured according to the distance to the surface. The surface roughly represents a trend in the point cloud, while the distance field indicates that the points exhibit a wave-like pattern.

Figure [\ref=fig:sf1] (a) shows the approximating surface after one iteration, together with (b) the corresponding element structure and (c) the distance field. We see that the domain is refined in the relevant part of the surface. After 4 iterations, it can be seen from Figure [\ref=fig:sf4] that the surface starts to represent details in the sea floor. We see from the element structure that the surface has been refined more in areas with local detail. The distance field reveals that most of the points are within the 0.5 meter threshold.

After 7 iterations, the surface, Figure [\ref=fig:sf7] (a), represents the shape of the sea floor very well, the corresponding element structure (b) indicates heavy refinement in areas with local details and only a few features in the point cloud fail to be captured by the surface (c). Table [\ref=fig:tab1] shows the evolution of the approximation accuracy throughout the iterative process.

With every iteration, the surfaces size has increased while the average distance between the points and the surface decreased, as did the number of points outside the 0.5 meters threshold. The decrease in the maximum distance, however, stopped after 5 iterations. We also find that 2 points have a distance larger than 4 meters, while 22 have a distance larger than 2 meters. In contrast, the elevation interval is about 50 meters. If we look into the details of the last distance field (Figure [\ref=fig:distfield7]), we find two categories of large distances: details that have been smoothed out (a) and outliers (b). If, in the first case, a very accurate surface representation is required, a triangulated surface should be applied in the critical areas. Outliers, on the other hand, should be removed from the computation. Still, isolated outliers, as in this case, do not have a large impact on the resulting surface.

Example 2 We approximate a point cloud composed from several data surveys taken from an area in the British channel, and look at the result after four and seven iterations. 10 partially overlapping surveys contain a total of 3.2 million points. The accuracy threshold is again taken to be 0.5 meters. After four iterations, the maximum distance is 27.6 meters and the average distance is 0.2 meters. After seven iterations, the numbers are 26.9 meters and 0.08 meters, respectively. The number of points outside the threshold are 367 593 and 38 915, respectively. Although the average approximation error and number of points with a large distance are significantly reduced from the 4th to the 7th iteration, the numbers are clearly poorer than for the previous example. Table [\ref=tab:ex2] gives more detailed information.

Figure [\ref=fig:example2] shows the point cloud assembled from the partially overlapping data surveys. This construction leads to a data set with a very heterogeneous pattern, in some areas there are a lot of data points, while in others quite few points describe the sea floor. The polynomial patches of the surface, (b) and (c), show that the surface has been refined significantly during the last 3 iterations.

Figure [\ref=fig:ex2_sf] shows the approximating surfaces after four and seven iterations. In the first case (a), the surface is not very accurate, as we have seen in Table [\ref=tab:ex2] and the polynomial mesh is also quite lean, as is seen in Figure [\ref=fig:example2] (b). Neither, the second surface is very accurate, but in this case some oscillations can be identified, Figure [\ref=fig:ex2_sf] (b), and the polynomial mesh has become very dense; it is likely that we are attempting to model noise.

Figures [\ref=fig:ex2_detail1] and [\ref=fig:ex2_detail2] zoom into a detail on the surfaces and show the distance fields of two data surveys, number 2 and 4 in Table [\ref=tab:ex2]. Data set 2 is shown as small dots and 4 as large dots. In Figure [\ref=fig:ex2_detail1] (a) and  [\ref=fig:ex2_detail2] (a), points within the 0.5 meters threshold are coloured green while red points and blue points are outside the threshold. Red points lie below the surface and blue points above. We see that points from the two data sets lie on opposite sides of the surface while being geographically close. In Figure [\ref=fig:ex2_detail2] (b) the distance threshold is increased to 2 meters, and there are still occurrences where close points from the two data sets are placed on opposite sides of the surface. Thus, the vertical distance between these points is at least 4 meters. The polynomial elements of the surface included in (b) indicate that a high degree of refinement has taken place in this area. The combined data collection clearly contains inconsistencies, and is a candidate for deconfliction

Deconfliction

Overfitting or fitting to inappropriate data causes oscillations in the surface and poorly reliable results. Processing the data to remove inconsistencies and selecting the appropriate filtering criteria is a non-trivial task. This filtering process is called deconfliction and is related to outlier detection.

Outlier Detection

An outlier is an observation that is inconsistent with the remainder of the data set. Outlier detection is concerned with finding these observations, and as outliers can drastically skew the conclusions drawn from a data set, statistical methods  [\cite=statistical] for detecting these observations have been a topic for a long time.

Consider a data set, measurements of discrete points on the sea bottom. We compare the data points to a trend surface and obtain a set of residuals, and want to test the hyphotesis that a given point belongs to the continuous surface of the real sea floor. Then the corresponding residual should not be unexpectedly large. In statistical terms, the difference surface between the real sea bottom and our trend surface is the population and the residual set is a sample drawn from the population. The sample mean and standard deviation can be used to estimate the population mean. In order to test if a point is an outlier, i.e., not representative of the population, we define a confidence interval. In a perfect world, this interval would relate to the normal distribution having zero mean and a small standard deviation. Other distributions can, however, be more appropriate. For instance, the so called Student's t distribution depends on the number of samples and is intended for small sampling sizes.

The confidence interval depends on a confidence level α, and is given by [formula]. Typically α∈[0.001,0.2] and the probability that the parameter lies in this interval is 100(1  -  α)%. The value zα / 2 denotes the parameter where the integral of a selected distribution to the right of the parameter is equal to α / 2. It can be computed from the distribution, but tabulated values are also available, see for instance [\cite=studentst] for the Student's t distribution. [formula] is the sample mean and S the sample standard deviation while n is the number of points in the sample.

In the deconfliction setting, we want to test whether the residuals from different data sets can be considered to originate from the same sea floor. I.e., we want to compare two distributions, which requires a slightly different test. To test for equal means of two populations, we can apply the Two-Sample t-Test [\cite=studentst2]. To have equal means the value

[formula]

should lie in an appropriate confidence interval. [formula] is the mean of sample k,k = 1,2 and sk is the standard deviation. Nk is the number of points in the sample. If equal standard deviation is assumed the number of degrees of freedom used to define the confidence interval is N1 + N2 - 1, otherwise a more complex formula involving the standard deviations is applied to compute the degrees of freedom. This test has, depending on the number of sample points, a thicker tail than the normal distribution, but does still assume some degree of regularity in the data. For instance, the distribution is symmetric. Thus, we need to investigate to what extent the test is applicable for our type of data.

Bathymetry data may contain outliers. Erroneous soundings can be caused by several factors, including air bubbles, complexities in the sea floor and bad weather conditions. These measurements need to be located and excluded from further processing to guarantee that correct results will be generated from the cleaned data. The distinction between outliers and data points describing real features in the sea floor is a challenge. True features should be kept and there are no firm rules saying when an outlier removal is appropriate.

For multi beam sonars, outlier detection is discussed in a number of papers [\cite=outliers1] [\cite=outliers2] [\cite=outliers3] [\cite=outliers4]. Traditionally outliers are detected manually by visual inspection. However, due to the size of current bathymetry data surveys, automatic cleaning algorithms are required. The user can define a threshold as a multiple of the computed standard deviation and use statistical methods like confidence intervals or more application specific methods developed from the generic ones to detect outliers. For instance, Grubbs method [\cite=outliers2] is based on the Student's t distribution.

Computations of statistics for outlier removals may be based on the depth values themselves, but often residuals with respect to a trend surface are preferred. In the latter case, the trend surface is typically computed for subsets of the data survey. Selecting the cell size for such subsets is non-trivial. Large cells give larger samples for the computation of statistical criteria, but on the other hand, the cells size must be limited for the trend surface to give a sufficiently adequate representation of the sea floor. In [\cite=outliers4] a multi-resolution strategy is applied to get a reasonable level of detail in the model used for outlier detection. The selection of a suitable neighbourhood of interest for an outlier is relevant also for other types of outlier detection algorithms, for instance proximity based techniques as in k-Nearest Neighbour methods [\cite=outliers2]. A problem in trend surface analysis is that the surface tends to be influenced by the outliers. It has been proposed [\cite=outliers5] to minimize this influence by using a minimum maximum exchange algorithm (MMEA) to select the data points for creating the trend surface. In [\cite=outliers3], the so called M-estimator is utilized for the surface generation.

Preparing for Deconfliction

Deconfliction becomes relevant when we have more than one data survey overlapping in a given area. Two questions arise: are the data surveys consistent, and if not, which survey to choose? The first question is answered by comparing statistical properties of the data surveys. The answer to the second is based on properties of each data survey. The data surveys are equipped with metadata information. This includes the acquisition method, date of acquisition, number of points and point density. Usually, the most recent survey will be seen as the most reliable, but this can differ depending on the needs of the application, for instance when historical data is requested. In any case, an automated procedure is applied for prioritizing the data surveys resulting in scores that allow, at any sub-area in the region of interest, a sorting of overlapping surveys. We will not go into details about the prioritization algorithm.

In the first surface generation example, we observed a couple of outliers that could be easily identified by their distance to the surface. Considering outlier data sets, we want to base the identification on residuals to a trend surface, also called reference surface. In [\cite=outliers3] low order polynomials approximating hierarchical data partitions defined through an adaptive procedure were used as trend surfaces. We follow a similar approach by choosing an LR B-spline surface as the trend surface and use the framework described in Section [\ref=sec:adaptive] to define a surface roughly approximating the point cloud generated by assembling all data surveys.

The deconfliction algorithm is applied for each polygonal patch in the surface. This patch will, in the following be called element, and the element size has a significant impact on the result. Too many degrees of freedom compared to the number of data points results in the reference surface modeling the anticipated noise in the data, while too few will lead to a situation where the statistical properties derived from the residuals become less trustworthy. The strategy for adaptive refinement of an LR B-spline surface implies that the surface will be refined in areas where the accuracy is low. Thus, the size of the polynomial elements will vary: in regions where there is a lot of local detail, the element size will be small, while in smooth regions or regions where the point density is too low to represent any detail, the element size is large. Example 1 in Section [\ref=surfgenexample] shows the element mesh for an LR B-spline surface at different iteration levels. Adaptive refinement automatically implies an adaptive size of the surface elements. However, the number of iterations performed in the algorithm must be selected to get a good basis for the decisions, see Section [\ref=sec::deconfexamples], Figure [\ref=fig:sf0] to [\ref=fig:sf7] for an example of the effect of the refinement level of the reference surface.

The Deconfliction Algorithm

Outliers appear to be inconsistent with the general trend of the data. It is in the nature of outlier detection that there is a subjective judgement involved. Our aim is to develop an automatic outlier detection algorithm where the outliers are subsets of data surveys and where the sample pattern is extremely non-uniform.

If more than two surveys overlap in a domain they are tested pairwise with respect to score. The second highest scored survey is first compared to the one with highest score. Every new survey is tested against all previously accepted surveys and needs to be found consistent with all to be accepted.

After applying the deconfliction, the cleaned data surveys are used to update the reference surface to obtain a final surface with better accuracy. This is done by the surface generation algorithm described in Section [\ref=surfgen], but the process is started from the reference surface and not from a lean initial tensor-product spline surface. Thus, fewer iterations are required to obtain a sufficient accuracy.

Suppose two or more data surveys overlap in an identified area. The point cloud assembled from all the surveys is approximated by an LR B-spline surface of low accuracy. The consistency check is performed elementwise and pairwise. For one comparison of two surveys, several aspects must be taken into consideration:

The pattern described by the combined data surveys may be very non-uniform.

The number of points within an element may differ greatly from element to element and from survey pair to survey pair.

The data surveys may cover roughly the same area within an element, they may be completely disjoint or overlap in a tiny area.

The number of points in each survey may differ by an order of magnitude. The data size of a survey is independent of its priority score. The reference surface will favour the survey with many points.

One or both data surveys may contain outliers.

If the two surveys have the same score and overlap barely or not at all, this probably implies that the surveys originate from the same acquisition, but the point set is split at some stage. This is treated as a special case.

Given this survey configuration, can methodologies from statistics or multi beam outlier detection apply? Given subsets from two data surveys, we want to determine if they belong to the same underlying surface. The following properties are taken into account in the algorithm:

The mean of the two samples and the difference between these means.

The range of distances to the reference surface for each sample.

The standard deviation of the signed distances between the sample points and the reference surface for each sample and for the data set obtained by combining the two samples.

Size of overlap between the sample domains relative to the maximum sample domain size.

The Two sample t-Test value and the associated confidence interval.

A sample in this context is a data survey restricted to one surface element. An immediate observation is that the Two sample t-Test is very strict for this kind of data and that the value becomes very large when the standard deviations of the two samples are small. Thus, applying this test directly would be too strict. However, the t-Test value tends to vary consistently with the other properties. When this tendency is contradicted a closer investigation should be initiated. Similarly, if the standard deviation of one or both data surveys is large, this indicates outliers within the data sets or a high degree of detail in the sea bottom. Also in this case more testing can be beneficial and the deconfliction test is applied to sub-domains within the element.

The surveys are considered consistent if the following criteria hold:

The sample means are close relative to the surface generation threshold.

The range of the distances field of the candidate sample does not exceed the range of the high priority sample with more than an amount deduced from this threshold.

Most of the distances computed from points from the candidate sample lies within the range of the prioritized sample.

The standard deviation computed from the combined data set does not exceed the individual standard deviations with more than a small fraction.

If some of the conditions above do not apply, but the overlap between the samples is small, the test is repeated on a sub-domain where there is a significant overlap between the samples. The possibility of consistency checking on reduced domains implies a second level of adaptivity in addition to the adaptivity in creating the reference surface, even though the reference surface is not updated.

Surface generation, even with a careful selection of approximation method, is sensitive to patterns in the data points. Empty regions with significant variation in the height values may lead to unwanted surface artifacts. However, even if one data survey lacks points in an area, another survey may contain information about the area. Thus, the combination of several surveys can give more complete information than one survey alone, as long as the information from the different surveys is consistent. The question is: What should be done with a group of points that are found to be inconsistent or possibly inconsistent with the remainder of the points in the area? Is it more damaging for the final surface to keep them or remove them? The answer depends on the configuration of points. If the candidate outliers are disjoint from the higher prioritized point clouds, and the distance between the point clusters large enough to fit a reasonable surface, the group of candidate outliers should be kept. Otherwise, the points should be removed.

In the following, we will look into a couple of different classes of configurations and discuss them in some detail. The algorithm classifies sub point clouds into consistent, not consistent and indeterminate, based on statistics on the distance field. The indeterminate cases are first investigated in more detail using the sub-element approach mentioned above and if the case is still classified as indeterminate, treated again using knowledge on other elements covered by the same data surveys, to tune the algorithm.

Element Example 1 We look at is a detail in the test case covered in the first example of Section [\ref=sec::deconfexamples]. The element is overlapped by two of the data surveys, and the patterns of the two data surveys are relatively similar as seen in Figure [\ref=fig:el1].

The range of the distance field at 3 iterations, the mean distance standard deviation and domain size for the two surveys are given in Table [\ref=tab:el1_1]. The domain sizes are given as the bounding box of the x -  and y -  coordinates of the points. The overlap between the surveys has size 1802.3, which imply almost full overlap. The standard deviation computed from the combined point clouds is 0.007. The Two sample t-Test value is 20.5 while the limit with α  =  0.025 is 1.96. The range and standard deviation for the low priority data surveys is lower than for the prioritized one. The differences between range extent and mean value for the two surveys are small compared to the threshold of 0.5 and the standard deviation doesn't increase when the two surveys are seen as one unity. Thus, the surveys look quite consistent even if the T-test value is high compared to the confidence interval, and this is indeed the conclusion of the test.

Element Example 2 The next example, see Figure [\ref=fig:el2], is taken from an area with two overlapping surveys of different patterns. The one with highest score consists of scan lines where the points are close within one scan line, but the distances between the scan lines are large. For the other survey, the points are more sparse, but also more regular. In this configuration, we would prefer to keep most of the points between the scan lines, but only as long as they are consistent with the scan line points.

The mean values of the residuals quite similar, see Table [\ref=tab:el2], but the ranges don't overlap well, which indicates a rejection of the survey with the lower score. However, the individual standard deviations are relatively high, in particular for the second survey. Thus, a more detailed investigation is initiated. In sub-domain 1, the combined standard deviation is 4.75, which is way above the standard deviations for the individual sub surveys. However, the sub surveys don't overlap and after looking into the closest situated points in the two surveys, the conclusion is that the surveys are consistent. In sub-domain 2, the combined standard deviation is 0.537 and there is no overlap between the two sub surveys. The conclusion is consistence for the same reason as for the previous sub-domain. In sub-domain 3, the combined standard deviation is 0.85. The single point from Survey 2 is well within the range of Survey 1, but the standard deviation tells a different story. However, after limiting the domain even more to cover just the neighbourhood of the survey 2 point, the characteristic residual numbers can be seen in Table [\ref=tab:el2_2] as sub-domain 3b and the combined standard deviation is 0.003. The survey is accepted also in this domain. In the last sub-domain, Survey 1 has no points and the final conclusion is acceptance.

Deconfliction Examples

Example 1 Our first example is a small region with three overlapping data surveys, Figure [\ref=fig:deconf4pts] a. The red one (survey 1 in Table   [\ref=tab:reflevel]) has priority score 0.675, the green (survey 2) has score 0.65 and the blue (survey 3) 0.097.

The combined data set is approximated by a reference surface using 4 iterations of the adaptive surface generation algorithm. Deconfliction is applied and the surface generation is continued, approximating only the cleaned point set for 3 more iterations. The result can be seen in Figure [\ref=fig:deconf4res]. About half the points are removed by the deconfliction algorithm and almost all the cleaned points are within the prescribed threshold of 0.5 meters of the final surface. The points that have been removed from the computations, are more distant. However, most of them are also close to the surface. In most of the area, the sea floor is quite flat and even if the data surveys are not completely consistent, the threshold is quite large. In the narrow channel at the top of the data set, the shape becomes more steep and the difference between the cleaned and the remaining points becomes larger. Figure [\ref=fig:deconf4detail] shows a detail close to the channel. In Figure [\ref=fig:deconf4detail] (a) two surveys are shown, and the one with large points has highest priority score. For the other one, some points lie outside the 0.5 meters threshold (blue points), and we can see that the corresponding scan line has different behaviour vertically than the nearby completely green scan line of the high priority survey.

Table [\ref=tab:reflevel] shows how the choice of refinement levels for the reference surface influences the accuracy of the final surface, when 3 and 4 iterations for the reference surface is applied. For comparison, the surface approximation is performed also on the combined points set without any deconfliction. The surveys are prioritized according to their number, and the distance range and mean distance to the reference surface is recorded for all computations in addition to the total number of points for each data survey and the number of points in the cleaned survey after deconfliction. All distances are given in meters. In total, for the final surface, the number of iterations is 7 in all cases, but the data size of the final surfaces differ: The surface generated without any deconfliction is of size 329 KB, the surface with deconfliction level 3 is 131 KB large while the deconfliction level 4 surface is of size 147 KB. The distances between the final surface and the cleaned point clouds are slightly larger, and some more points are removed when deconfliction is performed at iteration level 3, but the accuracy weighed against surface size is more in favour of this choice of deconfliction level. The distances when no deconfliction is applied are larger when compared to the numbers for the cleaned point clouds, but smaller when all points are taken into account. This is no surprise as in the other case, only the cleaned point sets were used for the last iterations of the surface generation. The numbers don't clearly favour either deconfliction level 3 or 4. They are roughly comparable, but the reduced surface size for level 3 is preferable.

Example 2 This example is of a different magnitude. 255 data surveys sum up to 1.5 GB. The data set is split into 5  ×  3 tiles and are approximated by surfaces. As we can see in Figure [\ref=fig:deconex2-1], there is limited overlap between the data surveys.

Figure [\ref=fig:deconex2-2] shows overlap zones between three data surveys together with the kept points (a) and the removed points (b). The distances are computed with respect to the reference surface, which is made with deconfliction level 4. The point colours in these zones indicate that the points from different surveys are more than twice the tolerance apart, and consequently the overlap points from the lowest prioritized survey are removed.

Conclusion and Further Work

A good data reduction effect has been obtained by approximating bathymetry point clouds with LR B-spline surfaces. The approach handles inhomogeneous point clouds and can be used also for topography data, but is mostly suitable if the data set is to some extent smooth or if we want to extract the trend of the data. Data sets that mainly represent vegetation are less suitable.

We have developed an algorithm for automated deconfliction given a set of overlapping and possibly inconsistent data surveys. The cleaned point sets lead to surfaces with a much smaller risk of oscillations due to noise in the input data. The results so far are promising, but there is still potential for further improvements. Interesting aspects to investigate include:

Outlier removal in individual data surveys prior to deconfliction.

Investigation of secondary trend surface approximations based on residuals in situations with many points in an element and small overlaps between the data sets, to detect if there is a systematic behaviour in the approximation errors with respect to the current reference surface.

Continued investigation of the effect of refinement of the LR B-spline surface to create a suitable reference surface. Aspects to study are number of iterations and a possibility for downwards limitations regarding element size and number of points in an element.

There is no principal difference between surface modelling and deconfliction in 2.5D and 3D. Still, an investigation regarding which dimensionality to choose in different configurations could be useful.

A data survey can be subject to a systematic difference with respect to another survey due to differences in registration, for instance the vertical datum can differ. Identification and correction of such occurrences are not covered by the current work. Differences in registration is a global feature of the data set. Indications of it can be detected locally for the reference surface elements, but the determination of an occurrence must be made globally.