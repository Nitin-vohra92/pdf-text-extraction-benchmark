Dynamic Stacked Generalization for Node Classification on Networks

Alyson Wilson

Introduction

Network data and its relational structure have garnered tremendous attention in recent years. A network is composed of nodes and edges, where nodes represent interacting units and edges represent their relationships [\cite=Goldenberg:2010:SSN:1734794.1734795].

Node classification or node labeling on a network is a problem where we observe labels on a subset of nodes and aim to predict the labels for the rest [\cite=DBLP:journals/corr/abs-1101-3291]. There are various kinds of labels; for example, demographic labels such as age, gender, location, or social interests; labels such as political parties, research interests, or research affiliations. Collective inference estimates the labels of a set of related nodes simultaneously given a partially observed network by exploiting the relational auto-correlation of connected units [\cite=Jensen02linkageand], and it has been demonstrated effective in reducing classification error for many applications [\cite=Chakrabarti:1998:EHC:276304.276332] [\cite=neville00iterative] [\cite=DBLP:conf/kdd/JensenNG04] [\cite=sen:aimag08]. Common collective inference methods are the Iterative Classification Algorithm (ICA) [\cite=neville00iterative], Gibbs Sampling (Gibbs), and Relaxation Labeling (RL) [\cite=conf/icml/LuG03] [\cite=Macskassy:2007:CND:1248659.1248693].

In many cases, multiple types of relationships can be observed in the same network. For example, in a citation network, an edge can mean two papers have the same author, or they are published in the same journal, or one paper cites another. We may also observe additional node-level information, such as the title and abstract of a paper, which can potentially help increase the label classification accuracy. When multiple relations are present on a network, one can merge all the relations and sum the weights of common links to perform a typical collective classification [\cite=Macskassy03asimple]. An alternative is to combine all of the information through an ensemble framework. Fürnkranz [\cite=Furnkranz01hyperlinkensembles:] introduced hyperlink ensembles for classifying hypertext documents, where he suggests first predicting the label of each hyperlink attached to a document and then combining these individual predictions using ensembles to make a final prediction for the label of the target document. A different approach was proposed by Heß and Kushmerick [\cite=Heb04iterativeensemble], where they suggest training separate classifiers for the local and relational attributes and then combining the local and relational classifiers through voting. A local classifier is trained using only node-level, or local, features; for example, title, abstract, or year of publication. A relational classifier infers a node's label by using relational features; for example, the labels of the connected neighbors. Cataltepe et al. discussed a similar ensemble approach [\cite=conf/mldm/CataltepeSBE11], where they considered different voting methods, such as the weighted average, average, and maximum. Eldardiry and Neville [\cite=Eldardiry11across-modelcollective] discussed an across-models collective classification method that formed ensembles of the estimates from multiple classifiers using a voting idea similar to collective inference to reduce variance.

The above literature focuses on combining multiple classifiers through some type of aggregation. Preisach and Schmidt-Thieme [\cite=Preisach:2008:ERC:1357641.1357645] proposed to use stacking instead of a simple voting as a more robust and powerful generalizing method to combine predictions made by local and relational classifiers. They suggest training each classifier independently and combining the predicted class probabilities from a pool of local and relational classifiers through stacking, which assigns constant weights to each classifier in a supervised fashion.

Stacked generalization (stacking) [\cite=Wolpert92stackedgeneralization] is a technique for combining multiple classifiers, each of which has been individually trained for a specific classification task, to achieve greater overall predictive accuracy. The method first trains individual classifiers using cross-validation on the training data. The original training data is called level-0 data, and the learned models are called level-0 classifiers. The prediction outcomes from the level-0 models are pooled for the second-stage learning, where a meta-classifier is trained. The pooled classification outcomes are called level-1 data and the meta-classifier is called the level-1 generalizer.

Ting and Witten [\cite=Ting97stackedgeneralization:] showed that for the task of classification, the best practice is to use the predicted class probabilities generated by level-0 models to construct level-1 data. Essentially, stacking learns a meta-classifier that assigns a set of weights to the class predictions made by individual classifiers. The traditional stacking model assumes the weight of each classifier is constant from instance to instance, which does not hold in general for many relational classifiers on a network. For example, the weighted-vote relational neighbor (wvRN) classifier [\cite=Macskassy03asimple] infers a node's label by taking a weighted average of the class membership probabilities of its neighbors. One expects that its performance might be dependent on a node's topological characteristics in the graph; for example, the number of connected neighbors. On the other hand, local classifiers that are trained using only a node's local attributes are less dependent on its topological features. Consequently, when we combine local and relational classifiers, it is beneficial to have a set of weight functions instead of constant weights for each classifier. There has been some previous work on dynamically ensemble local and relational models [\cite=conf/icml/McDowellA12] [\cite=DBLP:conf/icdm/XiangN11]. However, they impose parametric models on the weight functions that are not flexible to capture complex weighting functions.

In this paper, we develop a dynamic stacking framework using a generalized varying coefficient model, which allows the weights for each classifier to vary smoothly with a node's topological characteristics in a non-parametric way. We illustrate the benefits of incorporating a node's topological features into stacking. To the best of our knowledge, this is the first work that considers non-parametric functional weight stacking.

Background and Motivation

Network data can be represented by a graph G  =  (V,E,Y) with vertices (nodes) V, edges (connections) E  =  {v1,v2},v1,v2∈V, and labels Y. The graph G is partitioned into two sets of vertices, [formula] and [formula], with [formula] and [formula]. We are given a classification problem with C classes. Class labels, yi, are observed for nodes in the training set [formula], while the labels of the test set [formula] are unknown and need to be estimated. A relational classifier uses the attributes and/or labels from a node's connected neighbors to make predictions. However, unlike a typical classification problem, a node's neighbors may have missing attributes and/or labels, which in turn need to be estimated. Collective inference [\cite=DBLP:conf/kdd/JensenNG04] [\cite=sen:aimag08] has been developed to make joint inference on the test nodes and produce consistent results.

We examinine the Cora [\cite=mccallum00automating] and the PubMed Diabetes [\cite=sen:aimag08] data sets, where we evaluate the collective classification accuracy on nodes with various topological characteristics. We consider the wvRN classifier as the relational classifier [\cite=Macskassy03asimple], defined as follows, and the Iterative Classification Algorithm (ICA) [\cite=conf/icml/LuG03] [\cite=Macskassy:2007:CND:1248659.1248693] for collective inference as defined in Algorithm [\ref=alg:ICA].

Macskassy and Provost showed that the weighted-vote relational neighbor classifier is equivalent to the Gaussian-field model [\cite=Macskassy:2007:CND:1248659.1248693].

The Cora data set is a public academic database composed of papers from Computer Science. It contains a citation graph with attributes/labels of each paper (including authors, title, abstract, book title, and topic labels). We only consider the topics of each paper as its label and ignore the other attributes. We remove papers with no topic labels and construct the data set by keeping the largest connected component in the network. The final data set is an unweighed and non-directional network, with 19,355 nodes and 58,494 edges. Labels are 70 topic categories, and each paper is classified into one of the categories.

We randomly sample 80% of nodes from V into Vtest and set their labels to null. We then make predictions using ICA on the nodes in Vtest and calculate the classification accuracy for different levels of degrees and closeness centrality. We repeat this experiment 100 times and the results are displayed in Figures ([\ref=fig:100_replication_90_testsize_degree_maximum]) and ([\ref=fig:100_replication_90_testsize_closeness_centrality_maximum]). In Figure ([\ref=fig:100_replication_90_testsize_degree_maximum]), the classification accuracy of the wvRN classifier is dependent on the degree of vi. As the count of a node's neighbor increases from 1 to 10, the average classification accuracy jumps from 45% to more than 60%. There are a limited number of nodes with degrees greater than 10, and thus the variance of the average classification accuracy goes up considerably. Closeness centrality is defined as the reciprocal of a node's total distance from all other nodes, which relates to the idea of "being in the middle of things." Unlike degree, closeness centrality is a continuous variable. We binned its range into 100 equal-length intervals and calculated classification accuracy in each bin. From Figure ([\ref=fig:100_replication_90_testsize_closeness_centrality_maximum]), we observe a steady upward trend in the classification accuracy near the center of the spectrum. There are not many nodes near the two ends of the spectrum, and this contributes to the large variation in the classification accuracy.

We performed the same analysis on the Pubmed Diabetes data set. The Pubmed data set is a medical database composed of diabetes-related medical papers derived from the PubMed database. The graph is a citation network with 19,717 papers and 44,338 edges. Each publication is assigned one of three categories as its label and a TF/IDF weighted word vector as an extra attribute. Here we ignore the extra attributes and only consider the topic category labels. We observe results similar to those from the Cora data set in Figures ([\ref=fig:Pubmed_100_replication_90_testsize_degree_maximum]) and ([\ref=fig:Pubmed_100_replication_90_testsize_closeness_centrality_maximum]).

Dynamic Stacking Model

Notation for the Stacked Generalization Model

Stacked generalization (stacking) is a general method for combining multiple lower-level models to improve overall predictive accuracy [\cite=Wolpert92stackedgeneralization] [\cite=Ting97stackedgeneralization:]. Here we follow the notation in [\cite=Ting97stackedgeneralization:]. Given data [formula] for [formula], let [formula] be the feature vector and yi the label of the i-th observation. Here we focus on categorical responses for y, and assume y has C categories. We first randomly partition the data into J roughly equal-sized parts [formula]. Define [formula] and [formula] to be the test and training data sets for the j-th fold of a J-fold cross validation.

Suppose we have K classifiers. We train each of the K classifiers using the training set [formula] with results [formula]. [formula] are called level-0 models. We then apply the K classifiers on the test set [formula] and denote [formula] as the estimated class probability vector from [formula] for [formula]. We repeat this process for [formula] and collect the outputs from the K models to form the level-1 data as follows:

[formula]

[formula] is the predicted class probability vector from [formula] for observation i, and therefore [formula]. We drop the last element zikC from vector [formula] to avoid multicollinearity issues. We then fit a supervised classification model, [formula], using the level-1 data, which is called the level-1 generalizer.

For prediction, a new observation [formula] is input into the K low-level classifiers, [formula]. The estimated class probability vectors, [formula], are then concatenated and input into [formula], which outputs the final class estimate for that observation.

For classification on networks, Preisach and Schmidt-Thieme [\cite=Preisach:2008:ERC:1357641.1357645] adapted the stacking technique and combined a local classifier with a relational classifier using a logistic regression model as the level-1 generalizer. However, in their paper, the coefficients in the logistic regression are constant, meaning the weights of individual component classifier are "static" from node to node. From previous observations in Figures ([\ref=fig:100_replication_90_testsize_degree_maximum]), ([\ref=fig:100_replication_90_testsize_closeness_centrality_maximum]), ([\ref=fig:Pubmed_100_replication_90_testsize_degree_maximum]), and ([\ref=fig:Pubmed_100_replication_90_testsize_closeness_centrality_maximum]), the accuracy of a relational classifier is often dependent on some topological feature of a node. Therefore, it could be beneficial to "dynamically" allocate the weights of individual classifiers based on some other variable. We discuss a dynamic stacking model using a generalized varying coefficient model in the next section to account for this observation. Compared with competing methods that also consider dynamic stacking [\cite=conf/icml/McDowellA12] [\cite=DBLP:conf/icdm/XiangN11], the proposed model is non-parametric, more flexible, and can learn more complex weighting functions.

Generalized Varying Coefficient Model through Smoothing Splines

Here we develop a dynamic stacking model using a generalized varying coefficient model. Instead of having a set of constant coefficients in the regression, we allow the coefficients to vary as smooth functions of other variables. The generalized varying-coefficient model was proposed by Hastie and Tibshirani [\cite=HastieTibshirani1993] and was reviewed in [\cite=VaryingCoefficientModels08Fan].

Similar to traditional stacked generalization, the inputs for the dynamic stacking model are the assembled outputs from multiple level-1 classifiers, along with an extra covariate: [formula]. yi is the true class label of an observation, [formula] is the concatenated predicted class membership vector from a pool of inhomogeneous classifiers with dimension p. Each of the component classifiers could potentially look at a different set of features of an instance and make a prediction from its point of view. ui is an "extra" covariate of a observation, which presumably would affect the prediction accuracy made by at least one classifier. Here we focus on the case where yi is binary and ui is continuous. One can easily extend this method to multi-class classification problems by using a one-vs-all strategy.

The regression function is modeled as:

[formula]

where g(  ·  ) is the logit link function, [formula] is the functional coefficient vector that varies smoothly with an extra scalar covariate, and β0 is a constant intercept. Instead of a constant intercept, one can trivially add a functional intercept by appending 1 to [formula]. However, in this paper, we focus on the constant intercept case. We assume that each functional coefficient [formula] can be approximated by spline functions:

[formula]

where for each βj, [formula] is a set of spline basis functions. Without loss of generality, we use the same set of B-spline basis functions for all [formula]. Henceforth, we denote the set of B-spline basis functions as [formula], where K is the number of basis functions. We can then rewrite equation ([\ref=eq:betaSpline]) as:

[formula]

We substitute equation ([\ref=eq:betaSpline2]) into equation ([\ref=eq:regressionModel]) and rewrite the regression function as

[formula]

Denote [formula]. We can express [formula] as:

[formula]

and express [formula] as:

[formula]

.

We can estimate [formula] by directly minimizing:

[formula]

where [formula] is the log-likelihood function of the logistic regression, [formula] is a smoothness penalty term that controls the total curvature of the fitted βj(  ·  ) for [formula], and λ is a a smoothing parameter that controls the trade-off between model fit and the roughness of the fitted βj(  ·  )s. When λ  →  0, we have a set of wiggly βj(  ·  )s; as λ  →    ∞  , the minimization of ([\ref=eq:objective]) will produce a set of linear βj(  ·  )s.

For the constant intercept case, one can absorb β0 into [formula] as:

[formula]

and append a constant 1 to the beginning of the product, [formula]. We can write the optimization in equation ([\ref=eq:objective]) w.r.t. [formula] as:

[formula]

where [formula] is the assembled penalty matrix:

[formula]

Hj is the smoothness penalty matrix for βj(  ·  ), and [formula] for [formula]. Since we are using the same set of basis functions, [formula]. It can be shown that [formula] is convex w.r.t. [formula]. Also, one can show that [formula] is positive semi-definite, so [formula] is convex w.r.t. [formula] as well. Therefore, there exists a unique [formula] that optimizes equation ([\ref=eq:eta_objective]). Given a specified smoothness penalty parameter λ, to estimate [formula], we employ an iterative Newton-type optimization method by directly calculating the derivatives of the objective function in equation ([\ref=eq:eta_objective]). The smoothness penalty parameter λ can be chosen by cross-validation, where, for a range of λ values, we iteratively leave out a subset of the training data, fit the model using the rest of the data, and compute the prediction error on the held out data set. The best λ is set to the one with the smallest objective function value.

Simulation Study

Here we compare the performance of the dynamic stacking method against standard benchmarks using simulated data sets. [\cite=Preisach:2008:ERC:1357641.1357645] used a standard logistic regression model as the level-1 generalizer to combine a local and a relational classifier. [\cite=RegularizedStacking] suggested that regularization is necessary to reduce over-fitting and increase predictive accuracy, and they considered lasso regression, ridge regression, and elastic net regression. In our simulation study, we use lasso regression, ridge regression, and logistic regression as benchmark level-1 generalizers, and for each of the benchmark generalizers, we experiment with adding an additional covariate and/or interaction terms into the stacking, and compare their performance with the dynamic stacking model.

For the simulation, N  =  2000 observations are generated for [formula]. Z1i and Z2i are the predicted positive class probabilities from two classifiers, [formula] and [formula], and they are generated independently from a uniform distribution on

[formula]

. Finally, the response yi is generated from a Bernoulli distribution with p(yi  =  1) specified as one of the following three cases. In case 1, the classifier weights are not dependent on u, while case 2 has linear dependence, and case 3 has non-linear dependence.

Case 1:

Case 2:

Case 3: For training and evaluation, the N observations are evenly split into training and testing sets. We train the dynamic stacking model and benchmark methods using the training set, where the penalty parameters of the proposed method and benchmarks are selected by 10-fold cross-validation. The fitted models are then applied to predict on the testing set, and the final prediction accuracy on the test set is measured by the Area Under the Curve (AUC) from prediction scores as shown in Table ([\ref=tab:simulationresult]). For methods1 (Logistic1, Lasso1, and Ridge1), the inputs to the level-1 generalizer are {(yi,Z1i,Z2i)}. For methods2, we add the additional covariate u into the input: {(yi,Z1i,Z2i,ui)}. For methods3, in addition to u, we further add its interaction with Z1i,Z2i into the input: {(yi,Z1i,Z2i,ui,Z1iui,Z2iui)}.

From Table ([\ref=tab:simulationresult]), the dynamic stacking model performs no worse than the standard methods under all scenarios. It has better performance than the "static" stacking models when there is an underlying non-linear dependency between a classifier's performance and the extra covariate. In case 1, the dynamic stacking model generalizes to the traditional stacking models and does not over fit the data. In case 2, where a linear dependency exists, the proposed model generalizes to methods3, where interaction terms with the extra covariate are added into the "static" stacking model. In case 3, where a non-linear dependency exists, the dynamic stacking model outperforms all benchmarks.

Real Data Analysis

Here we revisit the Cora data set [\cite=mccallum00automating], where we use paper titles as node attributes and topic classification as labels. We remove nodes with no title or topic classifications, and the final graph contains 11,187 nodes and 33,777 edges. Seventy topic categories are used as labels, and each paper belongs to one of the categories. For simplicity, we convert the classification problem into a binary classification problem by giving a positive label if the topic falls under the |/Artificialntelligence/| category. We then use the closeness centrality of each node in the graph as an additional covariate in stacking.

We split all the nodes on the graph into a 20% training set and an 80% testing set. On the training set, we observe the titles and the topic classification label of each paper, while on the test set, we only observe the titles. We fit a local classifier using the word vector representation of its title only (Naive Bayes), and a relational classifier (ICA + wvRN) using only the labels from a paper's neighbor. We then fit a dynamic stacking model using the output from the two classifiers with their coefficients being smooth functions of the closeness centrality of a node. The smoothness penalty parameter is chosen by 10-fold cross-validation. One set of fitted coefficient curves for the two classifiers are shown in Figure [\ref=fig:one-curve]. It allocates a higher weight on the relational classifier when a node has a high closeness centrality value and relies on the local classifier for nodes with a small closeness centrality value. This mirrors our observations from the previous discussion.

We compared the dynamic stacking model with the traditional stacking model on multiple standard level-1 generalizers (lasso, ridge, and logistic regression), all of which ignore the closeness centrality of a node during stacking. The penalty parameters for lasso and ridge regression are chosen by 10-fold cross-validation. We also implemented ensemble classification methods from [\cite=DBLP:conf/icdm/XiangN11] [\cite=conf/icml/McDowellA12]. In [\cite=DBLP:conf/icdm/XiangN11], Xiang and Neville proposed a parametric weighting scheme to combine a local classifier and a relational classifier where model parameters are chosen by cross-validation. In [\cite=conf/icml/McDowellA12], outputs from local and relational classifier are combined thorough a concept of label regularization and the model they proposed has no additional parameters. We repeat the train-test process 100 times randomly and record the accuracy score for each run.

We also performed model comparison using the PubMed data with the same general setup. Input to the local classifier is the TF/IDF representation of a paper, and we fit a dynamic stacking model using the output from the local classifier and the relational classifier with their coefficients being smooth functions of the degree centrality of a node. The classification accuracy comparison between the proposed method and the benchmarks is shown in Table ([\ref=model-comparison1]) and Table ([\ref=model-comparison2]), where the accuracy is defined as [formula] where [formula].

By assuming the normality of the classification accuracy difference distribution, the dynamic stacking mode outperforms all benchmarks at p-value <  0.01. For the Cora data set, Figure 6 shows the source of the accuracy improvement. For each of the 100 repetitions, we calculate the difference in the absolute number of correctly classified nodes at different closeness centrality levels between the proposed model and benchmarks. The dynamic stacking model outperforms the benchmarks near the two ends of the closeness centrality spectrum where the balance between the local and relational classifier shifts considerably. For the majority of nodes in this data set, their closeness centrality clusters tightly around a specific value, which leaves little room for the dynamic stacking model to improve much beyond its static-weight counterparts in terms of the overall accuracy. However, for the nodes that are near the two extremes of the closeness centrality spectrum, we do see a significant improvement by using the dynamic stacking method.

Discussion

In this paper, by examining two public data sets, Cora and PubMed, we illustrate the motivation for incorporating node topological characteristics into stacked generalization for node classification on networks. We then develop a novel dynamic stacking method with functional weights for component models, each of which can vary smoothly with an extra covariate. Simulation studies show that the proposed method generalizes well to the benchmarks when the data does not present complex patterns, and outperforms all benchmarks otherwise. Real data analysis using Cora and PubMed shows the proposed method has a small yet significant improvement on the classification accuracy compared with traditional stacking models. Further analysis shows that most of the accuracy improvement comes from nodes near the two extremes of the closeness centrality spectrum where the balance between the local and relational classifier shifts the most. The limited number of nodes in that region explains the small improvement on the overall classification accuracy. However, for the nodes near the extremes of closeness centrality, we do see a considerable improvement on the classification accuracy using the dynamic stacking method. Overall, the dynamic stacking model allows the composition of the stacking model to change as we move across the network, and thus it potentially provides a more versatile and accurate stacking model for label prediction on a network.

Future Work

The proposed dynamic stacking model is a direct extension of the traditional stacking model, which uses logistic regression as level-1 generalizer. As discussed in [\cite=RegularizedStacking], this model tends to overfit, especially when combining a large pool of noisy classifiers. To mitigate this problem, one can add a group-lasso penalty over the model coefficients, [formula], into equation ([\ref=eq:eta_objective]). Coefficients can be naturally grouped if they are the basis coefficients for the same βj(  ·  ): {([formula]. By adding a group-lasso penalty, the dynamic stacking model is more robust to the noise in the level-1 generalizing process.

Acknowledgment

This material is based upon work supported in part with funding from the Laboratory for Analytic Sciences (LAS). Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the LAS and/or any agency or entity of the United States Government.