Lemma Proposition Definition

A New Perspective on Randomized Gossip Algorithms

Introduction

The average consensus problem and randomized gossip algorithms for solving it appear in many applications, including distributed data fusion in sensor networks [\cite=xiao2005scheme], load balancing [\cite=cybenko1989dynamic] and clock synchronization [\cite=freris2012fast]. This subject was studied extensively in the last decade; for instance, the seminal 2006 paper of Boyd et al. [\cite=boyd2006randomized] on randomized gossip algorithms motivated a fury of subsequent research and generated more than 1500 citations to date. For a survey of selected relevant work prior to 2010, we refer the reader to the work of Dimakis et al. [\cite=dimakis2010gossip]. For more recent results on randomized gossip algorithms we suggest [\cite=jun2013performance] [\cite=zouzias2015randomized] [\cite=liu2013analysis]. See also [\cite=dimakis2008geographic] [\cite=aysal2009broadcast] [\cite=olshevsky2009convergence].

The average consensus problem

In the average consensus (AC) problem, we are given an undirected connected network G = (V,E) with node set [formula] and edges E. Each node i∈V "knows" a private value ci∈R. The goal of AC is for every node of the network to compute the average of these private values, [formula], in a distributed fashion. That is, the exchange of information can only occur between connected nodes (neighbors).

Contributions

In this paper we revisit, from a fresh perspective, the AC problem. Our starting point is the recent observation of Gower and Richtárik [\cite=gower2015stochastic] that the most basic randomized gossip algorithm ("randomly pick an edge (i,j)∈E and then replace the values stored at vertices i and j by their average") is an instance of the randomized Kaczmarz (RK) method for solving consistent linear systems, applied to a specific linear system encoding the AC problem. The RK method was first analyzed in 2009 by Strohmer and Vershynin [\cite=RK], and since then, there was an explosion of activity in refining, generalizing and extending the results [\cite=RBK] [\cite=zouzias2013randomized] [\cite=eldar2011acceleration] [\cite=liu2016accelerated] [\cite=needell2015randomized]. In this paper, we examine the Stochastic Dual Ascent (SDA) method of Gower and Richtárik [\cite=gower2015stochastic], which includes the RK method as a special case, in the context of AC problem. We show how the complexity result of SDA implies a bound on the ε-averaging time which is well-known in the literature for a more restricted class of randomized gossip algorithms. Further, we explain how SDA uncovers a fundamental but hitherto hidden duality of randomized gossip algorithms, and give a natural interpretation thereof. We then focus on a specific subclass of SDA which is identical to the randomized block Kaczmarz method [\cite=RBK] in the primal space, and which can be interpreted as a randomized Newton method in the dual space. In particular, we show that the method has a certain superlinear speedup property, and explain what this property means. Finally, we perform experiments to justify the last claim.

SDA: Stochastic Dual Ascent

In this section we briefly review those aspects of the work of Gower and Richtárik [\cite=gower2015stochastic] on Stochastic Dual Ascent (SDA) which we will need in the rest of the paper.

Consider an m  ×  n real matrix [formula] (assume it does not contain any zero rows) and vector b∈Rm such that the linear system [formula] is consistent (i.e., has a solution). Since we do not assume the solution is unique, we shall be interested in a particular solution:

[formula]

Above, [formula] is a given vector and [formula] is the standard Euclidean norm. In words, in [\eqref=generalproblem] we are seeking the solution of the system which is closest to c. By x* we denote the solution of [\eqref=generalproblem]. The dual of problem [\eqref=generalproblem] is

[formula]

SDA is a randomized iterative algorithm for solving [\eqref=Dual_Problem], performing the iteration [formula], where [formula] is a matrix chosen in an i.i.d. fashion throughout the iterative process from an arbitrary but fixed distribution (which is a parameter of the method) and λk is a vector chosen afterwards so that [formula] is maximized in λ. In general, the maximizer in λ is not unique. In SDA, we let λk to be the least-norm maximizer, which leads to the iteration

[formula]

where [formula] (this matrix is always symmetric and positive semidefinite). With the sequence of the dual iterates {yk} we associate a sequence of primal iterates {xk} as follows:

[formula]

By combining [\eqref=connection] with [\eqref=alg:dual], we obtain the following algorithm:

[formula]

If [formula] is chosen randomly from the set of unit coordinate/basis vectors in Rm, then the dual method [\eqref=alg:dual] is randomized coordinate descent [\cite=leventhal2010randomized] [\cite=serial], and the corresponding primal method [\eqref=alg:primal] is RK. More generally, if [formula] is a random column submatrix of the m  ×  m identity matrix, the dual method is the randomized Newton method [\cite=qu2015sdna], and the corresponding primal method is a block version of RK [\cite=RBK]. We shall describe the more general case in more detail in Section [\ref=sec:block].

The basic convergence guarantees for both the primal and the dual iterative processes are presented in the following theorem. We set y0 = 0 so that x0  =  c, which corresponds to the vector of initial private values stored at the nodes.

Let y0 = 0 and assume that the matrix [formula] is well defined and nonsingular. Then the dual iterates {yk} of SDA defined in [\eqref=alg:dual] for all k  ≥  0 satisfy

[formula]

Likewise, the corresponding primal iterates, defined in [\eqref=connection] and explicitly written in [\eqref=alg:primal], for all k  ≥  0 satisfy

[formula]

The convergence rate ρ is given by

[formula]

where λ+min(  ·  ) denotes the minimum nonzero eigenvalue.

Randomized Gossip & SDA

We propose that randomized gossip algorithms be viewed as applications of SDA (either in the primal or dual form) to a particular problem of the form [\eqref=generalproblem] (resp. [\eqref=Dual_Problem]). In particular, we let [formula] be the initial values stored at the nodes of G, and choose [formula] and b so that the constraint [formula] is equivalent to the requirement that xi = xj (the value stored at node i is equal to the value stored at node j) for all (i,j)∈E.

We say that [formula] is an "average consensus (AC) system" when [formula] iff xi  =  xj for all (i,j)∈E.

It is easy to see that [formula] is an AC system precisely when b = 0 and the nullspace of [formula] is {t1n:t∈R}, where 1n is the vector of all ones in Rn. Hence, [formula] has rank n - 1. Moreover, it is easy to see that for any AC system, the solution of [\eqref=generalproblem] necessarily is x*  =    ·  1n -- this is why we singled out AC systems. In this sense, any algorithm for solving [\eqref=generalproblem] will "find" the average [formula]. However, in order to obtain a distributed algorithm we need to make sure that only "local" (with respect to G) exchange of information is allowed.

Standard Form and Mass Preservation

Assume that [formula] is an AC system. Then the primal iterative process [\eqref=alg:primal] can be written in the form

[formula]

where [formula]. Eq [\eqref=eq:W_k] is the standard form in which randomized gossip algorithms are written. What is new here is that the iteration matrix [formula] has a specific structure which guarantees convergence to x* under very weak assumption (see Theorem [\ref=maintheorem]). Note that if y0 = 0, then x0 = c, i.e., the starting primal iterate is the vector of private values (as should be expected from any gossip algorithm).

The primal iterates [\eqref=alg:primal] of SDA enjoy a mass preservation property (the proof follows from [\eqref=connection] in view of [formula]):

If [formula] is an AC system, then the primal iterates [\eqref=alg:primal] for all k  ≥  0 satisfy: [formula].

ε-Averaging Time

Let [formula]. The typical measure of convergence speed employed in the randomized gossip literature, called ε-averaging time and here denoted by K(ε), represents the smallest time k for which xk gets within εz0 from x*, with probability greater than 1 - ε, uniformly over all starting values x0 = c. More formally, we define

[formula]

This definition differs slightly from the standard one in that we use z0 instead of [formula].

Inequality [\eqref=eq:conv-primal], together with Markov inequality, can be used to give a bound on K(ε), formalized next:

Assume [formula] is an AC system. Let y0 = 0 and assume [formula] is nonsingular. Then for any 0 < ε  <  1 we have [formula] where ρ is defined in [\eqref=convergencerate].

It can be shown that under the assumptions of the above theorem, [formula] only has a single zero eigenvalue, and hence [formula] is the second smallest eigenvalue of [formula]. Thus, ρ is the second largest eigenvalue of [formula]. The bound on K(ε) appearing in Thm [\ref=thm:complexity_standard] is often written with ρ replaced by [formula] [\cite=boyd2006randomized].

Block gossip algorithms

In the previous section we highlighted some properties of SDA relevant to the randomized gossip literature, but without interpreting SDA as a gossip, or for that matter, distributed algorithm. In this section we remedy this by focusing on a particular AC system and a particular random matrix [formula]. By being specific, we will be able to give a natural interpretation of SDA as a gossip algorithm.

In particular, we choose [formula] to be the |E|  ×  n matrix such that [formula] directly encodes the constraints xi = xj for (i,j)∈E. That is, row e = (i,j)∈E of matrix [formula] contains value 1 in column i, value - 1 in column j (we use an arbitrary but fixed order of nodes defining each edge in order to fix [formula]) and zeros elsewhere.

Next, [formula] is selected in each iteration to be a random column submatrix of the m  ×  m identity matrix corresponding to columns indexed by a random subset of edges Sk  ⊆  E. We shall write [formula]. If Sk  =  {1,2}, for instance, then [formula] consists of the first and second column of [formula]. For simplicity, from now on we will drop the subscript and write S instead of Sk. This choice means that primal SDA is the randomized block Kaczmarz (RBK) method.

Randomized Block Kaczmarz as a Gossip Algorithm

In our setup, the primal iterative process [\eqref=alg:primal] has the form:

[formula]

Algorithm [\eqref=RBKasSDA] can be shown to be equivalent to the following "sketch and project" iteration (see [\cite=gower2015randomized] for additional equivalent viewpoints):

[formula]

which is a (more general) variant of the RBK method of Needell [\cite=RBK]. More specifically, this method works by projecting the last iterate xk onto the solution set of a row subsystem of [formula], where the selected rows correspond to a random subset S  ⊆  E of selected edges.

While [\eqref=RBKasSDA] (resp. [\eqref=RBKaLgorithm]) may seem to be a complicated algebraic (resp. variational) characterization of the method, due to our choice of [formula] we have the following result which gives a natural interpretation of RBK as a gossip algorithm (see also Figure [\ref=fig:RBK]).

Consider the AC problem. Then each iteration of RBK (Algorithm [\eqref=RBKasSDA]) works as follows: 1) Select a random set of edges S  ⊆  E, 2) Form subgraph Gk of G from the selected edges 3) For each connected component of Gk, replace node values with their average.

There is a very closed relationship between RBK and the path averaging algorithm [\cite=benezit2010order]. The latter is a special case of RBK, when S is restricted to correspond to a path of vertices. Notice that in the special case in which S is always a singleton, Algorithm [\eqref=RBKasSDA] reduces to the randomized Kaczmarz method. This means that only a random edge is selected in each iteration and the nodes incident with this edge replace their local values with their average. This is the pairwise gossip algorithm of Boyd [\cite=boyd2006randomized]. Theorem [\ref=TheoremRBK] extends this interpretation to the case of the RBK method.

Randomized Newton as a Dual Gossip Algorithm

In this subsection we bring a new insight into the randomized gossip framework by presenting how the dual iterative process that is associated to RBK method solves AC problem. The dual iterative process [\eqref=alg:dual] takes on the form:

[formula]

This is a randomized variant of the Newton method applied to the problem of maximizing the quadratic function D(y) defined in [\eqref=Dual_Problem]. Indeed, as we have seen before, in each iteration we perform the update [formula], where λk is chosen greedily so that D(yk + 1) is maximized. In doing so, we invert a random principal submatrix of the Hessian of D, whence the name.

Randomized Newton Method (RNM) was first proposed by Qu et al. [\cite=qu2015sdna]. RNM was first analyzed as an algorithm for minimizing smooth strongly convex functions. In [\cite=gower2015stochastic] it was also extended to the case of a smooth but weakly convex quadratics. This method was not previously associated with any gossip algorithm.

The most important distinction of RNM compared to existing gossip algorithms is that it operates with values that are associated to the edges of the network. To the best of our knowledge, it the first randomized dual gossip method. In particular, instead of iterating over values stored at the nodes, RNM uses these values to update "dual weights" yk∈Rm that correspond to the edges E of the network. However, deterministic dual distributed averaging algorithms were proposed before [\cite=rabbat2005generalized] [\cite=ghadimi2014admm].

Natural Interpretation. In iteration k, RNM (Algorithm [\eqref=SDAsimplified]) executes the following steps: 1) Select a random set of edges Sk  ⊆  E, 2) Form a subgraph Gk of G from the selected edges, 3) The values of the edges in each connected component of Gk are updated: their new values are a linear combination of the private values of the nodes belonging to the connected component and of the adjacent edges of their connected components.

Dual Variables as Advice. The weights yk of the edges have a natural interpretation as advice that each selected node receives from the network in order to update its value (to one that will eventually converge to the desired average).

Consider RNM performing the kth iteration and let Vr denote the set of nodes of the selected connected component that node i belongs to. Then, from Theorem  [\ref=TheoremRBK] we know that [formula]. Hence, by using [\eqref=connection], we obtain the following identity:

[formula]

Thus in each step [formula] represents the term (advice) that must be added to the initial value ci of node i in order to update its value to the average of the values of the nodes of the connected component i belongs to.

Importance of the dual perspective

It was shown in [\cite=qu2015sdna] that when RNM (and as a result, RBK) is viewed as a family of methods indexed by the size τ  =  |S| (we choose S of fixed size in the experiments), then τ  →  1 / (1 - ρ), where ρ is defined in [\eqref=convergencerate], decreases superlinearly fast in τ. In [\cite=qu2015sdna], this was only shown for full rank [formula]. In the next result we extend it to AC matrices [formula] (which are necessarily rank-deficient).

RBK enjoys superlinear speedup in τ. That is, as τ increases by some factor, the iteration complexity drops by a factor that is at least as large.

Numerical Experiments

We devote this section to experimentally evaluate the performance of the proposed gossip algorithms: RBK (the primal method) and RNM (the dual method). Recall that these methods solve the same problem, and their iterates are related via a simple affine transform. Hence, all results shown apply to both RBK and RNM.

Through these experiments we demonstrate the theoretical results presented in the previous section. That is, we show that for a connected network G, the complexity improves superlinearly in τ  =  |S|, where S is chosen as a subset of E of size τ, uniformly at random. In comparing the number of iterations for different values of τ, we use the relative error [formula]. We let ci = i for each node i∈V. We run RBK until the relative error becomes smaller than 0.01. The blue solid line in the figures denotes the actual number of iterations (after running the code) needed in order to achieve ε  ≤  10- 2 for different values of τ. The green dotted line represents the function [formula], where [formula] is the number of iterations of RBK with τ = 1 (i.e., the pairwise gossip algorithm). The green line depicts linear speedup; the fact that the blue line (obtained through experiments) is below the green line points to superlinear speedup.

The networks used in our experiments are the ring graph (cycle) with 30 and 100 nodes (Fig [\ref=fig:test3]) and the 4  ×  4 grid graph (Fig [\ref=fig:test4]). When we choose |S| = m (i.e., we choose to update dual variables corresponding to all edges in each iteration), then ρ = 0, and thus the method converges in one step.