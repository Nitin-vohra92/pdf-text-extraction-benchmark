Identifiability Scaling Laws in Bilinear Inverse Problems

Introduction

examine the problem of identifiability in bilinear inverse problems (BIPs),  input signal pair recovery for systems where the output is a bilinear function of two unknown inputs. Important practical examples of BIPs include blind deconvolution [\cite=hopgood2003], blind source separation [\cite=grady2005survey] and dictionary learning [\cite=xing2012dictionary] in signal processing, matrix factorization in machine learning [\cite=donoho2003whendoes], blind equalization in wireless communications [\cite=johnson1998blind], Of particular interest are signal recovery problems from under-determined systems of measurement where additional structure is needed in order to ensure recovery, and the observation model is non-linear in the parametrization of the problem.

Consider a discrete-time blind linear deconvolution problem. Let [formula] and [formula] be respectively m and n dimensional vectors from domains [formula] and [formula], and suppose that the noise free linear convolution of [formula] and [formula] is observed as [formula]. Then the blind linear deconvolution problem can be represented as the following feasibility problem. = , ∈ D, ∈ D. We draw the reader's attention to the observation/measurement model [formula]. Notice that if either [formula] or [formula] was a fixed and known quantity, then we would have an observation model that is linear in the other variable. However, when both [formula] and [formula] are unknown variables, then the linear convolution measurement model [formula] is no longer linear in the variable pair [formula]. Such a structural characteristic is referred to as a bilinear measurement structure (formally defined in  [\ref=sec:model]). The blind linear deconvolution problem [\eqref=prob:find_xy_deconv] is the resulting inverse problem. Such inverse problems arising from a bilinear measurement structure shall be referred to as bilinear inverse problems (formally defined in  [\ref=sec:model]).

A key issue in many under-determined inverse problems is that of identifiability: "Does a unique solution exist that satisfies the given observations?" Identifiability (and signal reconstruction) for linear inverse problems with sparsity and low-rank structures has received considerable attention in the context of compressed sensing and low-rank matrix recovery, respectively, and are now quite well understood [\cite=chandrasekaran2012convex]. In a nutshell, both compressed sensing and low-rank matrix recovery theories guarantee that the unknown sparse/low-rank signal can be unambiguously reconstructed from relatively few properly designed linear measurements using algorithms with runtime growing polynomially in the signal dimension. For non-linear inverse problems (including BIPs), however, characterization of identifiability (and signal reconstruction) still remains largely open. To illustrate that analyzing identifiability is nontrivial, we present a simple example. Consider the blind linear deconvolution problem represented by  [\eqref=prob:find_xy_deconv]. Suppose that we have the observation [formula] with [formula] and [formula]. It is not difficult to verify that both are valid solutions to  [\eqref=prob:find_xy_deconv]. Furthermore, it is not immediately obvious as to what structural constraints would disambiguate between the above two solutions. We have showed identifiability and constructed fast recovery algorithms in a previous work [\cite=choudhary2012sparse] when [formula] (possibly sparse) is in the non-negative orthant (modulo global sign flip), whereas we show negative results for the more general sparse (with respect to the canonical basis) blind deconvolution problem in [\cite=choudhary2014sbdidentifiability] [\cite=choudhary2014identifiabilitylimitsSBD].

Contributions

We cast conic prior constrained BIPs as low-rank matrix recovery problems, establish the validity of the 'lifting' procedure ( [\ref=sec:lifting]) and develop deterministic sufficient conditions for identifiability ( [\ref=sec:deterministic_identifiability]) while bridging the gap to necessary conditions in a special case. Our characterization agrees with the intuition that identifiability subject to priors should depend on the joint geometry of the signal space and the bilinear map. Our results are geared towards bilinear maps that admit a nontrivial rank two null space, as is the case with many important BIPs like blind deconvolution.

We develop trade-offs between probability of identifiability of a random instance and the complexity of the rank two null space of the lifted bilinear map under three classes of signal ensembles,  dependent but uncorrelated, independent Gaussian, and independent Bernoulli ( [\ref=sec:random_identifiability]). Specifically, we demonstrate that instance identifiability can be characterized by the complexity of restricted rank two null space, measured by the covering number of the set [formula], where [formula] and [formula] denote, respectively, the column and row spaces of the matrix [formula] and [formula] denotes the rank two null space of the lifted bilinear map [formula] restricted by the prior on the signal set to M. To the best of our knowledge, this gives new structural results solely based on the bilinear measurement model and is thus applicable to general BIPs.

We demonstrate that the rank two null space of the lifted bilinear map can be partly characterized in at least one important case (blind deconvolution), and conjecture that the same should be possible for other bilinear maps of interest (dictionary learning, blind source separation, ). Based on this characterization, we present numerical simulations for selected variations on the blind deconvolution problem to demonstrate the tightness of our scaling laws ( [\ref=sec:numerical_results]).

Related Work

Our treatment of BIPs draws on several different ideas. We employ 'lifting' from optimization [\cite=balas2005projection] which enables the creation of good relaxations for intractable optimization problems. This can come at the expense of an increase in the ambient dimension of the optimization variables. Lifting was used in [\cite=candes2011phaselift] for analyzing the phase retrieval problem and in [\cite=ahmed2012blind] for the analysis of blind circular deconvolution. We employ lifting in the same spirit as [\cite=candes2011phaselift] [\cite=ahmed2012blind] but our goals are different. Firstly, we deal with general BIPs which include the linear convolution model of [\cite=asif2009random], the circular convolution model of [\cite=hegde2011sampling] [\cite=ahmed2012blind] and the compressed bilinear observation model of [\cite=walk2012compressed] as special cases. Secondly, we focus solely on identifiability (as opposed to recoverability by convex optimization [\cite=candes2011phaselift] [\cite=ahmed2012blind]) enabling far milder assumptions on the distribution of the input signals.

After lifting, we have a rank one matrix recovery problem, subject to inherited conic constraints. While encouraging results have been shown for low-rank matrix recovery using the nuclear norm heuristic [\cite=gross2011recovering], quite stringent incoherence assumptions are needed between the sampling operator and the true matrix. Furthermore, the results do not generalize to an analysis of identifiability when the sampling operator admits rank two matrices in its null space. We are able to relax the incoherence assumptions in special cases for analyzing identifiability and also consider sampling operators with a non-trivial rank two null space. Since the works [\cite=recht2011nullspace] [\cite=candes2011tight] [\cite=lee2013nearoptimalcompressed] can be interpreted as solving BIPs with the lifted map drawn from a Gaussian random ensemble, thus leading to a trivial rank two null space with high probability, the results therein are not directly comparable to our results.

In [\cite=ahmed2012blind], a recoverability analysis for the blind circular deconvolution problem is undertaken, but the knowledge of the sparsity pattern of one input signal is needed. Taking our  [\eqref=prob:find_xy_deconv] as an example, [\cite=ahmed2012blind] assumes [formula] and [formula] for some tall deterministic matrix [formula] and a tall Gaussian random matrix [formula], where for any matrix [formula], [formula] denotes the column space of [formula]. In contrast, we shall make the less stringent assumption on [formula] and [formula] and show that identifiability holds with high probability in the presence of rank two matrices in the null space of the lifted linear operator (sampling operator).

A closely related (but different) problem is that of retrieving the phase of a signal from the magnitude of its Fourier coefficients (the Fourier phase retrieval problem). This is equivalent to recovering the signal given its auto correlation function [\cite=jaganathan2013phase]. In terms of our example blind deconvolution problem [\eqref=prob:find_xy_deconv], phase retrieval is equivalent to having the additional constraints [formula] and [formula] being the time reversed version of [formula]. While the Fourier phase retrieval problem may seem superficially similar to the blind deconvolution problem, there are major differences between the two, so much as to ensure identifiability and efficient recoverability for the sparsity regularized (in the canonical basis) version of the former [\cite=jaganathan2013sparsephaseretrieval], while one can explicitly show unidentifiability for the sparsity regularized (in the canonical basis) version of the latter [\cite=choudhary2014sbdidentifiability] [\cite=choudhary2014identifiabilitylimitsSBD] (even with oracle knowledge of the supports of both signals). The difference arises because the Fourier phase retrieval problem is a (non-convex) quadratic inverse problem rather than a BIP, and it satisfies additional properties (constant trace of the lifted variable) which make it better conditioned for efficient recovery algorithms [\cite=beck2009matrixQP].

For the dictionary learning problem, an identifiability analysis is developed in [\cite=kammoun2010robustness] leveraging results from [\cite=gribonval2010dictionary] on matrix factorization for sparse dictionary learning using the [formula] norm and [formula] quasi-norm for 0  <  p  <  1. More recently, exact recoverability of over-complete dictionaries from training samples (only polynomially large in the dimensions of the dictionary) has been proved in [\cite=agarwal2013exactrecoveryof] assuming sparse (but unknown) coefficient matrix. While every BIP can be recast as a dictionary learning problem in principle, such a transformation would result in additional structural constraints on the dictionary that may or may not be trivial to incorporate in the existing analyses. This is especially true for bilinear maps over vector pairs. In contrast, we develop our methods to specifically target bilinear maps over vector pairs ( convolution map) and thus obtain definitive results where the dictionary learning based formulations would most likely fail.

Some identifiability results for blind deconvolution are summarized in [\cite=meraim1997blind], but the treatment therein is inflexible to the inclusion of side information about the input signals. Identifiability for non-negative matrix factorization was examined in [\cite=donoho2003whendoes] exploiting geometric properties of the non-negative orthant. Although our results can be easily visualized in terms of geometry, they can also be stated purely in terms of linear algebra ( [\ref=thm:suff_ident]). Identifiability results for low-rank matrix completion [\cite=candes2009exact] [\cite=candes2010thepower] are provided in [\cite=kiraly2012combinatorial] via algebraic and combinatorial conditions using graph theoretic tools, but there is no straightforward way to extend these results to more general lifted linear operators like the convolution map. Overall, to the best of our knowledge, a unified flexible treatment of identifiability in BIPs has not been developed till date. In this paper, we present such a framework incorporating conic constraints on the input signals (which includes sparse signals in particular).

Organization, Reading Guide and Notation

The remainder of the paper is organized as follows. The first half of  [\ref=sec:model] formally introduces BIPs and a working definition of identifiability.  [\ref=sec:lifting] describes the lifting technique to reformulate BIPs as rank one matrix recovery problems, and characterizes the validity of the technique.  [\ref=sec:results] states our main results on both deterministic and random instance identifiability.  [\ref=sec:discussion] elaborates on the intuitions, ideas, assumptions and subtle implications associated with the results of  [\ref=sec:results].  [\ref=sec:numerical_results] is devoted to results of numerical verification and  [\ref=sec:conclusion] concludes the paper. Detailed proofs of all the results in the paper appear in the Appendices [\ref=sec:equivalence_theorem_proof]-[\ref=sec:rank-2_null_space_proposition_proof].

In order to maintain linearity of exposition to the greatest extent possible, we chose to create a separate section ( [\ref=sec:discussion]) for elaborating on intuitions, ideas, assumptions and implications associated with the important results of the paper. Thus, with the exception of  [\ref=sec:discussion], rest of the paper can be read in a linear fashion. However, we recommend the reader to switch between  [\ref=sec:results] and [\ref=sec:discussion] as necessary, to better interpret the results presented in  [\ref=sec:results].

We state the notational conventions used throughout rest of the paper. All vectors are assumed to be column vectors unless stated otherwise. We shall use lowercase boldface alphabets to denote column vectors ( [formula]) and uppercase boldface alphabets to denote matrices ( [formula]). The all zero (respectively all one) vector/matrix shall be denoted by [formula] (respectively [formula]) and the identity matrix by [formula]. The canonical base matrices for the space of m  ×  n real matrices will be denoted by [formula] for 1  ≤  i  ≤  m, 1  ≤  j  ≤  n and is defined (element-wise) as

[formula]

For vectors and/or matrices, [formula], [formula] and [formula] respectively denote the transpose, trace and rank of their argument, whenever applicable. Special sets are denoted by uppercase blackboard bold font ( [formula] for real numbers). Other sets are denoted by uppercase calligraphic font ( S). Linear operators on matrices are denoted by uppercase script font ( [formula]). The set of all matrices of rank at most k in the null space of a linear operator [formula] will be denoted by [formula], defined as

[formula]

and referred to as the 'rank k null space'. For any matrix [formula], we denote the row and column spaces by [formula] and [formula] respectively. The projection matrix onto the column space (respectively row space) of [formula] shall be denoted by [formula] (respectively [formula]). For any rank one matrix [formula], an expression of the form [formula] would denote the singular value decomposition of [formula] with vectors [formula] and [formula] each admitting unit . The standard Euclidean inner product on a vector space will be denoted by [formula] and the underlying vector space will be clear from the usage context. All logarithms are with respect to () base e unless specified otherwise. We shall use the [formula], [formula] and [formula] notation to denote order of growth of any function [formula] of [formula]  its argument. We have,

System Model

This section introduces the bilinear observation model and the associated bilinear inverse problem in  [\ref=sec:bilinear_maps] and our working definition of identifiability in  [\ref=sec:identifiability_defn].  [\ref=sec:lifting] describes the equivalent linear inverse problem obtained by lifting and conditions under which the equivalence holds. This equivalence is used to establish all of our identifiability results in  [\ref=sec:results].

Bilinear Maps and Bilinear Inverse Problems (BIPs)

A mapping [formula] is called a bilinear map if [formula] is a linear map [formula] and [formula] is a linear map [formula].

We shall consider the generic bilinear system/measurement model introduced in [\cite=choudhary2012onidentifiability],

[formula]

where [formula] is the vector of observations, [formula] is a given bilinear map, and [formula] denotes the pair of unknown signals with a given domain restriction [formula]. We are interested in solving for vectors [formula] and [formula] from the noiseless observation [formula] as given by [\eqref=eqn:model]. The BIP corresponding to the observation model [\eqref=eqn:model] is represented by the following feasibility problem. = , ∈ K. The non-negative matrix factorization problem [\cite=donoho2003whendoes] serves as an illustrative example of such a problem. Let [formula] and [formula] be two element-wise non-negative, unknown matrices and suppose that we observe the matrix product [formula] which clearly has a bilinear structure. The non-negative matrix factorization problem is represented by the feasibility problem = , ≥ , ≥ . where the expressions [formula] and [formula] constrain the matrices [formula] and [formula] to be elementwise non-negative. The elementwise non-negativity constraints [formula] form a domain restriction in  [\eqref=prob:NMF_XY], in the same way as the constraint [formula] serves to restrict the feasible set in  [\eqref=prob:find_xy].

Identifiability Definition

Notice that every BIP has an inherent scaling ambiguity due to the identity

[formula]

where [formula] represents the bilinear map. Thus, a meaningful definition of identifiability, in the context of BIPs, must disregard this type of scaling ambiguity. This leads us to the following definition of identifiability.

A vector pair [formula] is identifiable  the bilinear map [formula] if [formula] satisfying [formula], [formula] such that [formula].

It is straightforward to see that our definition of identifiability in turn defines an equivalence class of solutions. Thus, we seek to identify the equivalence class induced by the observation [formula] in [\eqref=eqn:model]. Later, in  [\ref=sec:lifting], we shall 'lift'  [\eqref=prob:find_xy] to  [\eqref=prob:rank] where, every equivalence class in the domain [formula] of the former problem maps to a single point in the domain [formula] of the latter problem.

The scaling ambiguity represented by [\eqref=eqn:scaling_ambiguity] is common to all BIPs and our definition of identifiability ( [\ref=defn:identifiability]) only allows for this kind of ambiguity. There may be other types of ambiguities depending on the specific BIP. For example, the forward system model associated with  [\eqref=prob:NMF_XY] is given by the matrix product operation [formula] which shows the following matrix multiplication ambiguity.

[formula]

where [formula] is the right inverse of [formula]. It is possible to define weaker notions of identifiability to allow for this kind of ambiguity. In this paper, we shall not address this question any further and limit ourselves to the stricter notion of identifiability as given by  [\ref=defn:identifiability].

Lifting

While  [\eqref=prob:find_xy] is an accurate representation of the class of BIPs, the formulation does not easily lend itself to an identifiability analysis. We next rewrite  [\eqref=prob:find_xy] to facilitate analysis, subject to some technical conditions (see  [\ref=thm:equivalence] and  [\ref=cor:equivalence]). The equivalent problem is a matrix rank minimization problem subject to linear equality constraints = , ∈ K', where [formula] is any set satisfying

[formula]

and [formula] is a linear operator that can be deterministically constructed from the bilinear map [formula] with the optimization variable [formula] in  [\eqref=prob:rank] being related to the optimization variable pair [formula] in  [\eqref=prob:find_xy] by the relation [formula]. The transformation of  [\eqref=prob:find_xy] to  [\eqref=prob:rank] is an example of 'lifting' and we shall refer to [formula] as the 'lifted linear operator'  the bilinear map [formula]. Other examples on lifting can be found in [\cite=candes2011phaselift] [\cite=ahmed2012blind]. Before stating the equivalence results between  [\eqref=prob:find_xy] and [\eqref=prob:rank] we describe the construction of [formula] from [formula].

Let [formula] be the [formula] coordinate projection operator of q dimensional vectors to scalars,  if [formula] then [formula]. Clearly, φj is a linear operator and hence the composition [formula] is a bilinear map. As [formula] is a finite dimensional operator, it is a bounded operator, hence by the Riesz Representation Theorem [\cite=rudin1987realandcomplex], [formula] such that [formula] is the unique linear operator satisfying

[formula]

where [formula] denotes an inner product operation in [formula]. Using [\eqref=eqn:Sj], we can convert the bilinear equality constraint in  [\eqref=prob:find_xy] into a set of q linear equality constraints as follows:

[formula]

for each 1  ≤  j  ≤  q, where the last inner product in [\eqref=eqn:coordinate_proj] is the trace inner product in the space [formula] and zj denotes the [formula] coordinate of the observation vector [formula]. Setting [formula] in [\eqref=eqn:coordinate_proj], the q linear equality constraints in [\eqref=eqn:coordinate_proj] can be compactly represented, using operator notation, by the vector equality constraint [formula], where [formula] is a linear operator acting on [formula]. This derivation uniquely specifies [formula] using the matrices [formula], 1  ≤  j  ≤  q, and we have the identity

[formula]

For the sake of completeness, we state the definitions of equivalence and feasibility in the context of optimization problems ( [\ref=defn:equivalence] and [\ref=defn:feasibility]). Thereafter, the connection between  [\eqref=prob:find_xy] and [\eqref=prob:rank] is described via the statements of  [\ref=thm:equivalence] and  [\ref=cor:equivalence].

Two optimization problems P and Q are said to be equivalent if every solution to P gives a solution to Q and every solution to Q gives a solution to P.

An optimization problem is said to be feasible, if the domain of the optimization variable is non-empty.

Let  [\eqref=prob:find_xy] be feasible and let [formula] and [formula] denote the set of solutions to  [\eqref=prob:find_xy] and [\eqref=prob:rank], respectively. Then the following are true.

[\eqref=prob:rank] is feasible with solution(s) of rank at most one.

[formula].

[formula] if and only if [formula] does not hold.

Notice that [formula] and [formula] in  [\ref=thm:equivalence] depend on the observation vector [formula], so that the statements of  [\ref=thm:equivalence] have a hidden dependence on [formula]. Since the observation vector [formula] is a function of the input signal pair [formula] it is desirable to have statements analogous to  [\ref=thm:equivalence] that do not depend on the observation vector [formula]. This is the purpose of  [\ref=cor:equivalence] below which makes use of [formula], the rank one null space of the lifted operator [formula] (see [\eqref=eqn:rank-k_null_space]).

Let  [\eqref=prob:find_xy] be feasible and let [formula] and [formula] respectively denote the set of optimal solutions to  [\eqref=prob:find_xy] and [\eqref=prob:rank] for a given observation vector [formula].  [\eqref=prob:find_xy] and [\eqref=prob:rank] are equivalent,  [formula], for every [formula] if and only if [formula] does not hold.

The statements of  [\ref=thm:equivalence] and  [\ref=cor:equivalence] are needed to establish the validity of lifting for general BIPs with [formula]. In case [formula] ( blind deconvolution),  [\ref=cor:equivalence] immediately implies that lifting is valid.

Notice that lifting  [\eqref=prob:find_xy] to  [\eqref=prob:rank] allows us some freedom in the choice of the set K'. Also, we have the additional side information that the optimal solution to  [\eqref=prob:rank] is a rank one matrix. These factors could be potentially helpful to develop tight and tractable relaxations to  [\eqref=prob:rank], that work better than the simple nuclear norm heuristic [\cite=recht2007guaranteed] ( see [\cite=agarwal2013exactrecoveryof]). We do not pursue this question here.

This transformation from  [\eqref=prob:find_xy] to  [\eqref=prob:rank] gives us several advantages,

[\eqref=prob:rank] has linear equality constraints as opposed to the bilinear equality constraints of  [\eqref=prob:find_xy]. The former is much easier to handle from an optimization as well as algorithmic perspective than the latter.

Convex relaxation for the nonconvex rank constraint in  [\eqref=prob:rank] is well known [\cite=recht2007guaranteed], which is an important requirement from an algorithmic perspective. In contrast, convex relaxation for a generic bilinear constraint is not known.

The bilinear map is completely determined by the set of matrices [formula] and is separated from the variable [formula] in  [\eqref=prob:rank]. Thus,  [\eqref=prob:rank] can be used to study generic BIPs.  [\ref=fig:lifting_example] illustrates a toy example involving the linear convolution map.

For every BIP there is an inherent scaling ambiguity (see [\eqref=eqn:scaling_ambiguity]) associated with the bilinear constraint. However, in  [\eqref=prob:rank], this scaling ambiguity has been taken care of implicitly when [formula] is the variable to be determined. Clearly, [formula] is unaffected by the type of scaling ambiguity described in [\eqref=eqn:scaling_ambiguity]. Norm constraints on [formula] or [formula] can be used to recover [formula] and [formula] from [formula] but these constraints do not affect  [\eqref=prob:rank].

If [formula] and/or [formula] are sparse in some known dictionary (possibly over-complete) then they can be absorbed into the mapping matrices [formula] without altering the structure of  [\eqref=prob:rank]. Indeed, if [formula] and [formula] are dictionaries such that [formula] and [formula] then we have

[formula]

for each 1  ≤  j  ≤  q. It is clear that  [\eqref=prob:rank] can be rewritten with [formula] as the optimization variable (with a corresponding modification to K'), and comparing [\eqref=eqn:sparse_basis_transform] and [\eqref=eqn:coordinate_proj] we see that the matrix [formula] can be designated to play the same role in the rewritten  [\eqref=prob:rank] as [formula] played in the original  [\eqref=prob:rank]. Thus, without loss of generality, we can consider  [\eqref=prob:rank] to be our lifted problem that retains all available prior information from  [\eqref=prob:find_xy] (assuming that the equivalence conditions in  [\ref=cor:equivalence] are satisfied).

Identifiability Results

We state our main results in this section starting with deterministic characterizations of identifiability in  [\ref=sec:universal_identifiability] and [\ref=sec:deterministic_identifiability] that are simple to state but computationally hard to check for a given BIP. Subsequently, in  [\ref=sec:random_identifiability] we investigate whether identifiability holds for most inputs if the input is drawn from some distribution over the domain.

Since we have some freedom of choice in the selection of the set K' according to  [\ref=rem:relax], we will work with an arbitrary K' satisfying [\eqref=eqn:set_change]. The extreme cases of [formula] and [formula] will sometimes be used for examples and to build intuition. Also, for some of the results, we have converse statements only for one of the extreme cases. We shall use the set M to denote the difference K'  -  K', defined as

[formula]

Universal Identifiability

As a straightforward consequence of lifting, we have the following necessary and sufficient condition for  [\eqref=prob:rank] to succeed for all values of the observation [formula].

Let [formula]. The solution to  [\eqref=prob:rank] will be correct for every observation [formula] if and only if [formula].

Notice that the "only if" part of  [\ref=prop:ident_easy] requires uniqueness of an observation [formula] that is valid for  [\eqref=prob:find_xy] as well and not just for  [\eqref=prob:rank]. The latter could have observations that arise because of the freedom in the choice of K', but those may not be valid for the former. As a result, the conclusion of the "only if" part of  [\ref=prop:ident_easy] is somewhat weaker in that it does not imply [formula].

When [formula], M represents the set of all rank two matrices in [formula] so that  [\ref=prop:ident_easy] reduces to the more familiar result: [formula] is necessary and sufficient for the action of the linear operator [formula] to be invertible on the set of all rank one matrices, where the inversion of the action of [formula] is achieved as the solution to  [\eqref=prob:rank].

While the characterization of [formula] for arbitrary linear operators [formula] is challenging, it has been shown that if [formula] is picked as a realization from some desirable distribution then [formula] (implies [formula]) is satisfied with high probability. As an example, [\cite=recht2011nullspace] [\cite=candes2011tight] show that if [formula] is picked from a Gaussian random ensemble, then [formula] is satisfied with high probability for [formula].

Deterministic Instance Identifiability

When [formula] is sampled from less desirable distributions, as for matrix completion [\cite=candes2009exact] [\cite=candes2010thepower] or matrix recovery for a specific given basis [\cite=gross2011recovering], one does not have [formula] with high probability. To guarantee identifiability (and unique reconstruction) for such realizations of [formula], significant domain restrictions via the set K (or K') are usually needed, so that [formula] and  [\ref=prop:ident_easy] comes into effect. Unfortunately, for many important BIPs (blind deconvolution, blind source separation, matrix factorization, ) the lifted linear operator [formula] does have a non-trivial [formula] set. This makes identifiability an important issue in practice. Fortunately, we still have [formula] in many of these cases so that  [\ref=cor:equivalence] implies that lifting is valid. For such maps, we have the following deterministic sufficient condition ( [\ref=thm:suff_ident]) for a rank one matrix [formula] to be identifiable as a solution of  [\eqref=prob:rank].  [\ref=thm:suff_ident] is heavily used for the results in the sequel.

Let [formula] and [formula] be a rank one matrix in [formula]. Suppose that for every [formula] either [formula] or [formula] is true, then given the observation [formula], [formula] can be successfully recovered by solving  [\eqref=prob:rank].

[\ref=thm:suff_ident] is only a sufficient condition for identifiability. We bridge the gap to the necessary conditions under a special case in  [\ref=cor:equal_singular_values] below. We use the notation [formula] to denote the set [formula].

Let [formula] and [formula] be a rank one matrix in [formula]. Suppose that every matrix [formula] admits a singular value decomposition with [formula]. Let us denote such a decomposition as [formula], and let [formula] and [formula] for some [formula] with α21  +  α22  =  α23  +  α24  =  1. Given the observation [formula],  [\eqref=prob:rank] successfully recovers [formula] if and only if for every [formula], α1α3  +  α2α4  ≤  0.

Intuitively,  [\ref=cor:equal_singular_values] exploits the fact that all nonzero singular values of a matrix are of the same sign. Indeed, [formula] (respectively [formula]) is an element of the two dimensional space of representation coefficients of [formula]  [formula] (respectively [formula]  [formula]) with a fixed representation basis.  [\ref=cor:equal_singular_values] says that identifiability of [formula] holds if and only if the vectors [formula] and [formula] do not form an acute angle between them. The assumption of [formula] has been made in  [\ref=cor:equal_singular_values] for ease of intuition. Although we do not state it here, an analogous result holds for [formula] with the condition on the inner product [formula] replaced by the same condition on a weighted inner product, where the weights depend on the ratio of [formula] to [formula].

For arbitrary lifted linear operators [formula], checking  [\ref=thm:suff_ident] for a given rank one matrix [formula] is usually hard, unless a simple characterization of [formula] or [formula] has been provided. It is reasonable to ask "How many rank one matrices [formula] are identifiable?", given any particular lifted linear operator [formula] and assuming that the rank one matrices [formula] are drawn at random from some distribution. It is highly desirable if most rank one matrices [formula] are identifiable. Before we can show such a result we need to define a random model for the rank one matrix [formula].

A Random Rank One Model

We consider [formula] as a random rank one matrix drawn from an ensemble with the following properties:

[formula] (and [formula]) is a zero mean random vector with an identity covariance matrix.

[formula] and [formula] are mutually independent.

As a practical motivation for this random model, we consider a blind channel estimation problem where the transmitted signal [formula] passes through an unknown linear time invariant channel impulse response [formula]. In the absence of measurement noise, the observed signal at the receiver would be the linear convolution [formula], which is a bilinear map. A practical modeling choice puts the channel realization [formula] statistically independent of the transmitted signal [formula]. Furthermore, if channel phase is rapidly varying, then the sign of each entry for [formula] is equally likely to be positive or negative with resultant mean as zero. The transmitted signal [formula] can be assumed to be zero mean with independent and identically distributed entries (and thus identical variance per entry) under Binary-Phase-Shift-Keying and other balanced Phase-Shift-Keying modulation schemes. The assumption of equal variance per tap is somewhat idealistic for channel [formula], but strictly speaking, this requirement is not absolutely necessary for our identifiability results.

Dependent Entries

First, we consider the case when the elements of [formula] (respectively [formula]) are not independent. We shall be interested in the following two possible properties of [formula] and [formula]:

The distribution of [formula] (respectively, [formula]) factors into a product of marginal distributions of [formula] and [formula] (respectively, [formula] and [formula]).

[formula] such that [formula] (respectively [formula]) a.s.

We state the following technical lemmas that will be needed in the proofs of  [\ref=thm:whp_suff_ident] and  [\ref=cor:whp_suff_ident].  [\ref=lem:Markov_estimate_special] is mainly useful when the assumption  cannot be satisfied but one needs bounds that closely resemble that of  [\ref=lem:Markov_estimate]. These lemmas allow us to upper bound the probability that [formula] (respectively [formula]) is close to one of the key subspaces in  [\ref=thm:suff_ident],  [formula] (respectively [formula]) where [formula] is in the appropriately constrained subset of [formula].

Given any m  ×  n real matrix [formula] and a constant [formula], a rank one random matrix [formula] satisfying assumptions - also satisfies,

Given any m  ×  n real matrix [formula] and a constant [formula], a rank one random matrix [formula] satisfying assumptions -, with [formula] (respectively [formula]) satisfying  for a constant [formula] (respectively [formula]), also satisfies,

[\ref=lem:Markov_estimate_special] will give non-trivial bounds if [formula] (respectively [formula]) go to ∞   fast enough as m (respectively n) goes to ∞  , and this growth rate could be slower than [formula] (respectively [formula]).

An example where  [\ref=lem:Markov_estimate_special] is applicable but  [\ref=lem:Markov_estimate] is not, can be constructed as follows. As before, let [formula] represent a channel impulse response independent of [formula], so that  is satisfied. Let [formula] represent a coded data stream under Pulse-Amplitude-Modulation such that [formula] with equal probability, [formula] and xm is coded as a function of [formula] yielding the following conditional correlation matrices: where [formula] is the matrix with elements given by

[formula]

for every 1  ≤  i,j  ≤  m. The expressions in [\eqref=eqn:mag_azimuth_depend] clearly imply that [formula] and [formula] are dependent so that  does not hold. Nonetheless, by construction, we have

[formula]

so that [\eqref=eqn:mag_azimuth_depend] implies [formula], thus satisfying . Also, [formula] a.s. so that  is satisfied. Thus,  [\ref=lem:Markov_estimate_special] is applicable with [formula].

Independent Entries

While  [\ref=lem:Markov_estimate] provides useful bounds, it does not suffice for many problems where [formula] is large. We can get much stronger bounds than  [\ref=lem:Markov_estimate] if the elements of vector [formula] (respectively [formula]) come from independent distributions, by utilizing the concentration of measure phenomenon [\cite=ledoux2011probability]. We shall consider the standard Gaussian and the symmetric Bernoulli distributions, and sharpen the bounds of  [\ref=lem:Markov_estimate] in the two technical lemmas to follow. Note that a zero mean independent and identically distributed assumption on the elements of [formula] and [formula] already implies the assumptions -. The bounds of  [\ref=lem:Bernoulli_Chernoff_estimate] and [\ref=lem:Gaussian_Chernoff_estimate] have an interpretation similar to the restricted isometry property [\cite=candes2006nearoptimal] and are used in the proofs for  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian], respectively. We retain the assumption [formula] from  [\ref=thm:suff_ident] and follow the convention that a random variable Z has a symmetric Bernoulli distribution if [formula].

Let [formula]. Given any m  ×  n real matrix [formula] and a constant [formula], a random vector [formula] with each element drawn independently from a standard normal distribution satisfies

[formula]

[formula]

Let [formula]. Given any m  ×  n real matrix [formula] and a constant [formula], a random vector [formula] with each element drawn independently from a symmetric Bernoulli distribution satisfies

[formula]

[formula]

[\ref=rem:constant_factor_loss] provides additional remarks on  [\ref=lem:Bernoulli_Chernoff_estimate].

Random Instance Identifiability

We first consider the special case where the size of the set [formula] is small  mn, in  [\ref=sec:finite_cardinality]. We use the same intuition in  [\ref=sec:infinite_cardinality] to appropriately partition the set [formula] when its size is large (possibly infinite) with respect to m + n.

Small Complexity of

It is intuitive to expect that the number of rank one matrices [formula] that are identifiable as optimal solutions to  [\eqref=prob:rank] should depend inversely on the size/complexity of [formula]. Below, we shall make this notion precise. We shall do so by lower bounding the probability of satisfaction of the sufficient conditions in  [\ref=thm:suff_ident].

Let [formula] and [formula] be a rank one random matrix satisfying assumptions -. Suppose that the set [formula] is finite with cardinality [formula]. For any constant [formula], the sufficient conditions of  [\ref=thm:suff_ident] are satisfied with probability greater than [formula].

[\ref=rem:measuring_row_column_space_pairs], [\ref=rem:role_of_conic_prior] and [\ref=rem:role_of_delta] provide additional remarks on  [\ref=thm:whp_suff_ident].

In  [\ref=thm:whp_suff_ident], we can drive the probability of identifiability [formula] arbitrarily close to one by increasing m and/or n provided that [formula] grows as [formula]. For many important BIPs (blind deconvolution, blind source separation, matrix factorization, ) this growth rate requirement on [formula] is too pessimistic. Tighter versions of  [\ref=thm:whp_suff_ident], with more optimistic growth rate requirements on [formula], are possible if the assumptions of  [\ref=lem:Gaussian_Chernoff_estimate] or [\ref=lem:Bernoulli_Chernoff_estimate] are satisfied. This is the content of  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian] described in  [\ref=sec:infinite_cardinality].

We provide a corollary to  [\ref=thm:whp_suff_ident] when assumption  does not hold so that  [\ref=lem:Markov_estimate] is inapplicable. The result uses  [\ref=lem:Markov_estimate_special] in place of  [\ref=lem:Markov_estimate] for the proof. The bound is asymptotically useful if [formula] grows as [formula].

Let [formula] and [formula] be a rank one random matrix satisfying assumptions - with [formula] (respectively [formula]) satisfying  for a constant [formula] (respectively [formula]). Suppose that the set [formula] is finite with cardinality [formula]. For any constant [formula], the sufficient conditions of  [\ref=thm:suff_ident] are satisfied with probability greater than [formula].

Large/Infinite Complexity of

When the complexity of [formula] is infinite or exponentially large in m  +  n, the bounds of  [\ref=sec:finite_cardinality] become trivially true for large enough m or n. We investigate an alternative bounding technique for this situation using covering numbers. Intuitively speaking, covering numbers measure the size of discretized versions of uncountable sets. The advantage of using such an approach is that the results are not contingent upon the exact geometry of [formula]. Thus, like  [\ref=thm:whp_suff_ident], the technique and subsequent results are applicable to every bilinear map. We shall see that to arrive at any sensible results, we will need to use the tighter estimates given by  [\ref=lem:Gaussian_Chernoff_estimate] and [\ref=lem:Bernoulli_Chernoff_estimate] that are only possible when our signals [formula] and [formula] are component-wise independent.

For any two sets [formula], the minimum number of translates of B needed to cover D is called the of D  B and is denoted by [formula]. The quantity [formula] is known as the of D  B.

It is known that if [formula] is a bounded convex body that is symmetric about the origin, and we let [formula] for some 0  <  ε  <  1, then the covering number [formula] obeys [\cite=vershynin2009lecturesingeometric]

[formula]

We can equivalently say that the metric entropy [formula] equals [formula]. We shall use this notation for specifying metric entropies of key sets in the theorems to follow.

We state a technical lemma needed to prove  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian]. The lemma bounds the difference between norms of topologically close projection operators as a function of the covering resolution, thus providing a characterization of the sets used to cover over the space of interest.

Let [formula], [formula] and 0  <  ε  <  1. There exists a covering of [formula] with metric entropy [formula]  [formula] such that for any [formula] satisfying [formula] we have

[formula]

for all [formula].

[\ref=rem:meaning_of_covering_lemma] provides additional remarks on  [\ref=lem:metric_entropy].

We are now ready to extend  [\ref=thm:whp_suff_ident] to the case where the complexity of [formula] is large (possibly infinite). We shall do so for Bernoulli and Gaussian priors (as illustrative distributions) in  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian] respectively. The proofs for both these theorems follow on the same lines as that of  [\ref=thm:whp_suff_ident], except that the probability bounds of  [\ref=lem:Markov_estimate] are replaced by those of  [\ref=lem:Bernoulli_Chernoff_estimate] and [\ref=lem:Gaussian_Chernoff_estimate] for Bernoulli and Gaussian priors, respectively.

Let [formula], the sets [formula] and [formula] be defined according to  [\ref=lem:metric_entropy], and [formula] be a rank one random matrix with components of [formula] (respectively [formula]) drawn independently from a symmetric Bernoulli distribution with K' chosen as

[formula]

and M  =  K'  -  K'. Let [formula] (respectively [formula]) denote the metric entropy of the set [formula]  [formula] (respectively [formula]  [formula]) for any 1  >  ε  ≥  ε0  >  0 and let p  =  pc  +  pr. For any constant [formula], the sufficient conditions of  [\ref=thm:suff_ident] are satisfied with probability greater than [formula] with [formula].

Let [formula], the sets [formula] and [formula] be defined according to  [\ref=lem:metric_entropy], and [formula] be a rank one random matrix with components of [formula] (respectively [formula]) drawn independently from a symmetric Bernoulli distribution with K' chosen as

[formula]

and M  =  K'  -  K'. Let [formula] denote the metric entropy of the set [formula]  [formula], [formula] denote the metric entropy of the set [formula]  [formula], for any 1  >  ε  ≥  ε0  >  0 and let p  =  pc  +  pr. For any constant [formula], the sufficient conditions of  [\ref=thm:suff_ident] are satisfied with probability greater than [formula] with [formula].

Let [formula], the sets [formula] and [formula] be defined according to  [\ref=lem:metric_entropy], and [formula] be a rank one random matrix with components of [formula] (respectively [formula]) drawn independently from a standard Gaussian distribution. Let [formula] (respectively [formula]) denote the metric entropy of the set [formula]  [formula] (respectively [formula]  [formula]) for any 0  <  ε  <  1 and let p  =  pc  +  pr. For any constant [formula], the sufficient conditions of  [\ref=thm:suff_ident] are satisfied with probability greater than [formula] where

[formula]

and [formula].

Let [formula], the sets [formula] and [formula] be defined according to  [\ref=lem:metric_entropy], and [formula] be a rank one random matrix with components of [formula] (respectively [formula]) drawn independently from a standard Gaussian distribution. Let [formula] denote the metric entropy of the set [formula]  [formula], [formula] denote the metric entropy of the set [formula]  [formula] for any 0  <  ε  <  1 and let p  =  pc  +  pr. For any constant [formula], the sufficient conditions of  [\ref=thm:suff_ident] are satisfied with probability greater than [formula] where

[formula]

and [formula].

[\ref=rem:bernoulli_example] and [\ref=rem:range_of_epsilon] provide additional remarks on  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian].

A non-trivial illustration of the theoretical scaling law bound of  [\ref=thm:whp_infinite_Gaussian] is provided in  [\ref=fig:theory_bounds], with [formula] as the lifted linear convolution map. Since the bound is parametrized by [formula], we choose ε  =  0.1 and δ  =  10- 4 for the illustration. Quite surprisingly (and fortunately), the metric entropy p in  [\ref=thm:whp_infinite_Gaussian] can be exactly characterized when [formula] represents the lifted linear convolution map. Specifically, we have p  =  m + n - 3. We refer the reader to  [\ref=prop:rank-2_nullspace] in  [\ref=sec:null_space_of_linear_convolution] for details.

We can obtain results analogous to  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian] when [formula] and [formula] are drawn from non-identical distributions,  [formula] is component-wise  symmetric Bernoulli and [formula] is component-wise  standard Gaussian. The argument is a straightforward modification of the proof.

Discussion

In this section, we elaborate on the intuitions, ideas, assumptions and subtle implications associated with the main results of this paper that were presented in  [\ref=sec:results].

A Measure of Geometric Complexity

For the purpose of measuring the size/complexity of [formula] in  [\ref=thm:whp_suff_ident], we used the cardinality [formula] of the set [formula] as a surrogate. This set essentially lists the distinct pairs of row and column spaces in the rank two null space of the lifted linear operator [formula] that are not excluded by the domain restriction [formula]. We note that the cardinality of the set [formula] could be infinite while its complexity could be finite in the sense just described. The same measure of complexity is used for the extensions of  [\ref=thm:whp_suff_ident] in  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian]. Throughout rest of the paper, any reference to the complexity of a set of matrices [formula] is in the sense just described,  through the cardinality of the set [formula].

The Role of Conic Prior

There are three distinct aspects to the prior knowledge in terms of the conic constraint [formula] on the unknown signal.

Probability Bounds: A key advantage of prior knowledge about the signal is apparent from the union bounding step in the proof of  [\ref=thm:whp_suff_ident]. Union bounding over the set [formula] always gives better bounds than union bounding over the superset [formula], the quantitative difference being the number [formula] in the bound of  [\ref=thm:whp_suff_ident]. In general, the difference could be exponentially large in m or n (see  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian]). We also note that K' does not need to be a cone in order to exploit this approach to improve the probability bounds.

Computational Trade-offs: Recalling  [\ref=rem:relax], the size of K' also trades off the ease of computation and the identifiability bounds of  [\ref=thm:whp_suff_ident]. If the size of K' needs to be increased to ease computation, an effort must be made to not suffer a substantial increase in the size/complexity of the set [formula]. For high dimensional problems (m or n is large), non-convex conic priors like the sparse cone in compressed sensing [\cite=donoho2006compressed] and the low-rank cone in matrix completion [\cite=candes2009exact] have been shown to admit good computationally tractable relaxations.

Geometric Complexity Measure: The measure of geometric complexity described in  [\ref=rem:measuring_row_column_space_pairs] followed naturally from  [\ref=thm:suff_ident] in an effort to describe the identifiability of a BIP in terms of quantities like row and column spaces familiar from linear algebra. This measure of complexity is invariant  conic extensions in the following way. Let [formula] be any set of matrices and let M'' denote its conic extension, defined as

[formula]

Then, we have

[formula]

[formula]

Qualitatively speaking, the flavor of results in this paper could also be derived for non-conic priors but the measure of geometric complexity that is used is implicitly based on conic extensions. Thus, there is no significant loss of generality in restricting ourselves to conic priors.

Although the parameter [formula] appears in  [\ref=thm:whp_suff_ident] as an artifact of our proof strategy, it has an important practical consequence. It represents a tolerance parameter for approximate versus exact prior information on the input signals. Specifically,  [\ref=thm:whp_suff_ident] is a statement about identifiability up to a δ-neighborhood around the true signal [formula]. The same holds true for  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian] describing the large/infinite complexity case.

Interpretation of  [\ref=lem:metric_entropy]

[\ref=lem:metric_entropy] can be informally restated as follows. Keeping [\eqref=eqn:projection_bound] satisfied, [formula] can always be covered by [formula] with metric entropy [formula]. In  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian] below, we are interested in covering the subset [formula] by [formula] and suppose that the resulting metric entropy is [formula]. In a sense,  [\ref=lem:metric_entropy] represents the worst case scenario that pc is upper bounded by 2m and no better upper bound is known. In the worst case, the aforementioned subset of [formula] has nearly the same complexity as [formula] and this happens when the set [formula] does not represent a large enough structural restriction on the set of rank two matrices in [formula]. For large m, to guarantee identifiability for most inputs, we would (realistically) want pc to be less than m by at least a constant factor. This is implied by  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian]. Informally, smaller or more structured [formula] implies a smaller value of pc which in turn implies identifiability for a greater fraction of the input ensemble.

The Gaussian and Bernoulli Special Cases

A standard Gaussian prior on the elements of [formula] and [formula] gives an example of the set [formula] with infinite complexity, provided that [formula] is complex enough. In this case, [formula] in  [\eqref=prob:find_xy] implying that [formula] from [\eqref=eqn:set_change]. Thus, [formula] and hence [formula]. Since M is superfluous in this case,  [\ref=thm:whp_infinite_Gaussian] omits all references to it. If the row or column spaces of matrices in [formula] are parametrized by one or more real parameters (see  [\ref=sec:null_space_of_linear_convolution] for an example involving the linear convolution operator), then [formula] has infinite complexity.

The scenario of a Bernoulli prior on elements of [formula] and [formula] gives an example of the set [formula] with finite (but exponentially large in m  +  n) complexity, provided that [formula] is complex enough. The precise statement requires a little more care than the Gaussian case described above. The motivation behind considering Bernoulli priors is to restrict the unit vectors [formula] and [formula] to take values from a large but finite set while adhering to the requirement of a conic prior on [formula] according to  [\eqref=prob:find_xy]. Thus, in this case we have [formula]. Let us select K' according to [\eqref=eqn:set_change], but without any relaxation, as

[formula]

Clearly, matrices in K' can account for at most 2m - 1 distinct column spaces and 2n - 1 distinct row spaces, thus implying that matrices in M  =  K'  -  K' are generated by at most [formula] distinct column spaces and at most [formula] distinct row spaces. Thus, [formula] is of finite complexity. It is clear that the complexity of M is [formula] so that if [formula] is small enough then the complexity of [formula] is exponentially large in m + n.

Distinctions between  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian]

Assumptions on

We prevent an arbitrarily small ε for  [\ref=thm:whp_infinite] by imposing a strictly positive lower bound ε0  >  0. This is necessary for Bernoulli priors on [formula] and [formula] since [formula] has a finite complexity, implying that the covering numbers of [formula]  [formula] and [formula]  [formula] have an absolute upper bound independent of ε. Thus, the logarithmic dependence (of the key metric entropies) on 1 / ε cannot hold unless ε is lower bounded away from zero.  [\ref=thm:whp_infinite_Gaussian], in contrast, allows for arbitrarily small ε since [formula] has infinite complexity, for Gaussian priors on [formula] and [formula]. Despite this distinction between  [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian], we choose to present our results in the stated form to emphasize similarity in the theorem statements and proofs.

A constant factor loss

We loose a constant factor of approximately 2 in the exponent on the r.h.s. of [\eqref=eqn:Bernoulli_Chernoff] as compared to [\eqref=eqn:Gaussian_Chernoff] for a fixed [formula] (compared using first order approximation of log δ). While this seems to be an artifact of the proof strategy, it is unclear whether a better constant can be obtained for the symmetric Bernoulli distribution (or more generally, for subgaussian distributions [\cite=baraniuk2011introduction]). Indeed, for the proof of  [\ref=lem:Gaussian_Chernoff_estimate] in  [\ref=sec:Gaussian_Chernoff_estimate_proof], we have used the rotational invariance property of the multivariate standard normal distribution. This property does not carry over to general subgaussian distributions.

Numerical Results on Blind Deconvolution

We observe that if [formula] then [formula] and  [\ref=thm:whp_suff_ident] correctly predicts that the input signals are identifiable with probability one (in agreement with  [\ref=prop:ident_easy]). Below, we consider example bilinear maps and input distributions with [formula] and numerically examine the scaling behavior suggested by  [\ref=lem:Markov_estimate], [\ref=lem:Bernoulli_Chernoff_estimate] and [\ref=lem:Gaussian_Chernoff_estimate] and  [\ref=thm:whp_suff_ident], [\ref=thm:whp_infinite] and [\ref=thm:whp_infinite_Gaussian]. Since  [\ref=lem:Markov_estimate] and  [\ref=thm:whp_suff_ident] impose only broad constraints on the input distribution, for the purpose of numerical simulations, we construct a specific input distribution that satisfies assumptions - in  [\ref=sec:dependant_but_uncorrelated]. Since this research was motivated by our interest to understand the cone constrained blind deconvolution problem [\eqref=prob:find_xy_deconv], our selection of example bilinear maps are closely related to the linear convolution map. We provide a partial description of the rank two null space for the linear convolution map in  [\ref=sec:null_space_of_linear_convolution].

Bi-orthogonally Supported Uniform Distributions

A bi-orthogonal set of vectors is a collection of orthonormal vectors and their additive inverses. It is widely used for signal representation in image processing and as a modulation scheme in communication systems. We can construct a uniform distribution over a bi-orthogonal set and it would satisfy assumptions - as shown below.

Let [formula] be an orthonormal basis for [formula] and the random unit vector [formula] be drawn according to the law

[formula]

where [formula] has the same meaning as in  [\ref=lem:Markov_estimate] and  [\ref=thm:whp_suff_ident]. Let [formula] be drawn from a distribution (independent of [formula]) supported on the non-negative real axis with [formula]. Then, by construction, [formula] satisfies assumption  and it also satisfies assumption  if [formula] and [formula] are drawn analogously but independent of [formula] and [formula]. Using [\eqref=eqn:biorthogonal_uniform_distribution], we further observe that

[formula]

and,

[formula]

[formula]

where the last equality in [\eqref=eqn:orthonormal_sum] is true since [formula] is an orthonormal basis for [formula]. By independence of [formula] from [formula] we have

[formula]

from [\eqref=eqn:mean_zero_unit_vector], and

[formula]

from [\eqref=eqn:orthonormal_sum]. Hence, [formula] is a zero mean random vector with an identity covariance matrix and thus satisfies assumption .

Following the same line of reasoning as in  [\ref=rem:bernoulli_example], we can show that a bi-orthogonally supported uniform prior on [formula] and [formula] gives an example of the set [formula] with small complexity in the sense described in  [\ref=rem:measuring_row_column_space_pairs]. Indeed, we have [formula] where [formula] and [formula] respectively form an orthonormal basis for [formula] and [formula]. Let us select K' according to [\eqref=eqn:set_change], but without any relaxation, as [formula]. It is clear that matrices in K' can account for at most m distinct column spaces and n distinct row spaces, thus implying that matrices in M  =  K'  -  K' are generated by at most [formula] distinct column spaces and by at most [formula] distinct row spaces. Thus, [formula] is of small complexity (only polynomially large in m and n). In fact, exhaustive search for  [\eqref=prob:rank] is tractable for any bi-orthogonally supported uniform prior, owing to the small complexity of [formula].

Null Space of Linear Convolution

The following proposition establishes a parametric representation of a subset of [formula] where [formula] denotes the lifted equivalent of the linear convolution map in  [\eqref=prob:find_xy_deconv]. As described by [\eqref=eqn:coordinate_proj] in  [\ref=sec:lifting], let [formula], 1  ≤  k  ≤  m + n - 1 denote a basis for [formula]. For 1  ≤  i  ≤  m, 1  ≤  j  ≤  n and 1  ≤  k  ≤  m + n - 1, we have the description

[formula]

[\ref=fig:lifting_example] illustrates a toy example of the linear convolution map with m = 3 and n = 4.

If [formula] admits a factorization of the form

[formula]

for some [formula] and [formula], then [formula].

Since the set of m  ×  n dimensional rank two matrices has 2(m + n - 2) DoF and [formula] maps [formula] to [formula] with [formula], [formula] has at most (2m + 2n - 4)  -  (m + n - 1)  =  (m + n - 3) DoF. We see that the representation on the r.h.s. of [\eqref=eqn:rank-2_nullspace] also has (m + n - 3) DoF, so that our parametrization is tight up to DoF. The converse of  [\ref=prop:rank-2_nullspace] is false in general [\cite=choudhary2014identifiabilitylimitsSBD].

Verification Methodology

We test identifiability by (approximately) solving the following optimization problem, ≤ μ, = , where [formula] is the true matrix and ε is a tuning parameter. The rationale behind solving  [\eqref=prob:ReWtNucNormIdentifiability] is as follows. If the sufficient conditions of  [\ref=thm:suff_ident] are not satisfied, then [formula] such that both [formula] and [formula] are true. We approximate the event As  [\eqref=prob:ReWtNucNormIdentifiability] is itself NP-hard to solve exactly, we can employ the re-weighted nuclear norm heuristic [\cite=mohan2010reweighted] to solve  [\eqref=prob:ReWtNucNormIdentifiability] approximately. If the resulting solution to  [\eqref=prob:ReWtNucNormIdentifiability] has rank two then we declare that event E2 has happened. Clearly, we have E2  ⊆  E1 so that sufficient conditions for identifiability by  [\ref=thm:suff_ident] fail if event E2 took place.

The examples we consider in  [\ref=sec:small_finite_null_space] to [\ref=sec:infinite_null_space] are, however, motivated from the representation in [\eqref=eqn:rank-2_nullspace] and share the same parametrization structure for [formula]. This enables us to use approximate verification techniques that are faster than the re-weighted nuclear norm heuristic, especially if the search space is discrete and finite. The re-weighted nuclear norm heuristic is still useful if no parametrization structure is available for [formula].

Small Complexity of

Let [formula] and [formula] be drawn from bi-orthogonally supported uniform distributions, as described in  [\ref=sec:dependant_but_uncorrelated], where [formula] and [formula] respectively represent the canonical bases for [formula] and [formula]. We consider a lifted linear operator [formula] with the following description: [formula] consists of [formula] parts and the [formula] part Pij, [formula], [formula] is given by

[formula]

where [formula] and [formula] respectively denote the canonical basis for [formula] and [formula], and [formula] is the floor function. Clearly, the elements of Pij are closely related to the representation in [\eqref=eqn:rank-2_nullspace]. For this lifted linear operator, the bound of  [\ref=thm:whp_suff_ident] is applicable with [formula] implying that the probability of failure to satisfy the sufficient conditions of  [\ref=thm:suff_ident] decreases as [formula]. Since exhaustive search for event E2 is tractable (see  [\ref=sec:dependant_but_uncorrelated]), we employ the same to compute the failure probability. The results are plotted in  [\ref=fig:Example3] on a log-log scale. Note that we have plotted the best linear fit for the simulated parameter values, since the probabilities can be locally discontinuous in log n due to the appearance of [formula] function in the expression of [formula]. We see that the simulated order of growth of the failure probability is [formula] for every fixed value of m (exponent determined by slope of plot in  [\ref=fig:Example3]) almost exactly matches the theoretically predicted order of growth (equals [formula]).

Large Complexity of

Let [formula] and [formula] be drawn component-wise independently from a symmetric Bernoulli distribution (see  [\ref=rem:bernoulli_example]) and let [formula] be a constant. Following our guiding representation [\eqref=eqn:rank-2_nullspace], we consider a lifted linear operator [formula] with the following description: [formula] consists of [formula] parts and the [formula] part Pij, [formula], [formula], is given by

[formula]

where [formula] (respectively [formula]) denotes the binary representation of i (respectively j) of length [formula] bits (respectively [formula] bits) expressed in the alphabet set [formula], and the all one column vectors in [\eqref=eqn:large_finite_null_space] are of appropriate dimensions so that the elements of Pij are matrices in [formula]. The bound in  [\ref=thm:whp_infinite] is applicable to this example. We employ exhaustive search for event E2 for small values of m and n (it is computationally intractable for large m or n). The results are plotted in  [\ref=fig:Example4] on a semilog scale, where we have used τ  =  0.2 and δ'  =  0.3 and δ' is as in the statement of  [\ref=thm:whp_infinite]. As in the case of  [\ref=fig:Example3], we plot the best linear fit for the simulated parameter values to disregard local discontinuities introduced due to the use of the [formula] function.

Since it is hard to analytically compute the metric entropies pc and pr, we shall settle for a numerical verification of the scaling law with problem dimension and an approximate argument as to the validity of predictions made by  [\ref=thm:whp_infinite] for this example. By construction, we have the bounds [formula] and [formula] but the careful reader will note that because of the element-wise constant magnitude property of a symmetric Bernoulli random vector, it does not lie in the column span of any of the matrices in [formula], as described by the generative description in [\eqref=eqn:large_finite_null_space], but can be arbitrarily close to such a span as m increases. We thus expect that pc  =  εcm and pr  =  εrn for some parameters εc and εr close to zero. By choice of parameters, [formula]. With εc  =  εr  =  0 and setting ε  =  0.01 the theoretical prediction on the absolute value of the slope is 0.073 which is quite close to the simulated value of 0.093. We clearly recover the linear scaling behavior of the logarithm of failure probability with the problem dimension n.

Infinite Complexity of

Let [formula] and [formula] be drawn component-wise independently from a standard Normal distribution. We consider the linear convolution operator from  [\eqref=prob:find_xy_deconv], letting [formula] denote the lifted linear convolution map. A representation of [formula] and a description of the rank two null space [formula] has been mentioned in the prequel ( [\ref=sec:null_space_of_linear_convolution]). The bound in  [\ref=thm:whp_infinite_Gaussian] is applicable to this example. However, unlike the examples in  [\ref=sec:small_finite_null_space] and [\ref=sec:large_finite_null_space], we cannot employ exhaustive search over [formula] to test identifiability, since the search space is uncountably infinite by  [\ref=prop:rank-2_nullspace]. We resort to the method described in  [\ref=sec:verification_technique] relying on the re-weighted nuclear norm heuristic. The results are plotted in  [\ref=fig:Example2] on a semilog scale, where we have used μ  =  0.8 to detect the occurrence of the event E2 as described by [\eqref=eqn:event_E2], and [formula] in  [\eqref=prob:ReWtNucNormIdentifiability] is normalized such that [formula]. A relatively high value of μ  =  0.8 is used to ensure that the rare event E2 admits a large enough probability of occurrence. Only data points that satisfy n  ≥  m are plotted since the behavior of the convolution operator is symmetric  the order of its inputs. Since the re-weighted nuclear norm heuristic does not always converge monotonically in a small number of steps, we stopped execution after a finite number of steps, which might explain the small deviation from linearity, observed in  [\ref=fig:Example2], as compared to the respective best linear fits on the same plot. Nonetheless, we approximately recover the theoretically predicted qualitative linear scaling law of the logarithm of the failure probability with the problem dimension n, for fixed values of m. There does not seem to be an easy way of comparing the constants involved in the simulated result to their theoretical counterparts as predicted by  [\ref=thm:whp_infinite_Gaussian].

Conclusions

Bilinear transformations occur in a number of signal processing problems like linear and circular convolution, matrix product, linear mixing of multiple sources, Identifiability and signal reconstruction for the corresponding inverse problems are important in practice and identifiability is a precursor to establishing any form of reconstruction guarantee. In the current work, we determined a series of sufficient conditions for identifiability in conic prior constrained Bilinear Inverse Problems (BIPs) and investigated the probability of achieving those conditions under three classes of random input signal ensembles,  dependent but uncorrelated, independent Gaussian, and independent Bernoulli. The theory is unified in the sense that it is applicable to all BIPs, and is specifically developed for bilinear maps over vector pairs with non-trivial rank two null space. Universal identifiability is absent for many interesting and important BIPs owing to the non-triviality of the rank two null space, but a deterministic characterization of the input instance identifiability is still possible (may be hard to check). Our probabilistic results were formulated as scaling laws that trade-off probability of identifiability with the complexity of the restricted rank two null space of the bilinear map in question, and results were derived for three different levels of complexity,  small (polynomial in the signal dimension), large (exponential in the signal dimension) and infinite. In each case, identifiability can hold with high probability depending on the relative geometry of the null space of the bilinear map and the signal space. Overall, most random input instances are identifiable, with the probability of identifiability scaling inversely with the complexity of the rank two null space of the bilinear map. An especially appealing aspect of our approach is that the rank two null space can be partly or fully characterized for many bilinear problems of interest. We demonstrated this by partly characterizing the rank two null space of the linear convolution map, and presented numerical verification of the derived scaling laws on examples that were based on variations of the blind deconvolution problem, exploiting the representation of its rank two null space. Overall, the results in this paper indicate that lifting is a powerful technique for identifiability analysis of general cone constrained BIPs.

Proof of  [\ref=thm:equivalence]

Suppose that [formula] is a solution to  [\eqref=prob:find_xy] for a given observation [formula]. Setting [formula] and using [\eqref=eqn:set_change] we have [formula]. Using [\eqref=eqn:lifted_op] we get [formula] and [formula]. Thus, [formula] is a feasible point of  [\eqref=prob:rank] with rank at most one. As there exists a [formula] matrix [formula] satisfying [formula] and [formula], the solution of  [\eqref=prob:rank] must be of rank one or less.

Any [formula] satisfies [formula] (see proof of first part) and [formula]. Thus, where [\eqref=eqn:condense_rank] is due to [\eqref=eqn:set_change], [\eqref=eqn:condense_constraint] is due to [\eqref=eqn:lifted_op] and [\eqref=eqn:K_opt] is true because  [\eqref=prob:find_xy] is a feasibility problem.

The feasible set for  [\eqref=prob:rank] is [formula]. From the proof of second part, we know that

[formula]

[formula]

Thus, clearly

[formula]

We shall prove the contrapositive statement in each direction. First assume that [formula]. By [\eqref=eqn:K_opt_feasible_set], [formula] is a feasible point for  [\eqref=prob:rank] and thus [formula] is the optimal value for this problem. Since every [formula] has a rank strictly greater than zero we conclude that [formula]. Conversely, suppose that [formula]. Since [formula] (see proof of second part), [formula] such that [formula]. By [\eqref=eqn:K_opt_feasible_set], [formula] is a feasible point for  [\eqref=prob:rank] and hence the optimal value of this problem is strictly less than [formula]. The only way for this to be possible is to have [formula] and the optimal value of  [\eqref=prob:rank] as zero. Since the only matrix of rank zero is the all zero matrix, we conclude that [formula].

Proof of  [\ref=cor:equivalence]

From [\eqref=eqn:rank-k_null_space], [\eqref=eqn:feasible_set] and [\eqref=eqn:K_opt] we have

[formula]

We shall prove the contrapositive statements. First assume that [formula]. Using [\eqref=eqn:rank-1_null_space_and_feasible_set], we have [formula] and the last part of  [\ref=thm:equivalence] implies that [formula]. Since [formula] is nonempty, [formula]. Thus,  [\eqref=prob:find_xy] and [\eqref=prob:rank] are not equivalent (equivalence fails for [formula]). Conversely, suppose that [formula] resulting in [formula]. Using last part of  [\ref=thm:equivalence], we have [formula], which is possible only if [formula]. Now using [\eqref=eqn:rank-1_null_space_and_feasible_set] we get [formula].

Proof of  [\ref=prop:ident_easy]

[\eqref=prob:rank] fails if and only if it admits more than one optimal solution.

Let [formula] and for the sake of contradiction suppose that [formula] and [formula] denote two solutions to  [\eqref=prob:rank] for some observation [formula], so that [formula]. Then, [formula] so that [formula] is in the null space of [formula]. But, [formula] so that we have [formula] and  [\eqref=prob:rank] has a unique solution.

Conversely, let  [\eqref=prob:rank] have a unique solution for every observation [formula]. For the sake of contradiction, suppose that there is a matrix [formula] in [formula]. Since [formula], [formula] such that [formula]. Further, [formula] is in the null space of [formula], so that [formula] with [formula] implying that [formula] and [formula] are both valid solutions to  [\eqref=prob:rank] for the observation [formula]. Since [formula], [formula] such that [formula] and [formula], so that [formula] is a valid observation. This violates the unique solution assumption on  [\eqref=prob:rank] for the valid observation [formula]. Hence [formula], completing the proof.

Proof of  [\ref=thm:suff_ident]

Let [formula] be a solution to  [\eqref=prob:rank] such that [formula]. Since [formula] is a valid solution to  [\eqref=prob:rank], we have [formula] and [formula]. If [formula], then [formula] and [formula]. This contradicts the assumption that at least one of [formula] or [formula] is true and completes the proof.

Proof of  [\ref=cor:equal_singular_values]

We start with the "if" part. For [formula] to be identifiable, we need [formula] for every matrix [formula] in the null space of [formula] that also satisfies [formula]. Since [formula], it is sufficient to consider matrices [formula] with [formula]. Thus, for identifiability of [formula], we need [formula], [formula]. Using [formula] and [formula], we have [formula] and [formula] and by assumption, we have [formula]. Let [formula] and [formula], [formula] for some [formula] with α21  +  α22  =  α23  +  α24  =  1. It is easy to check that [formula] has the following equivalent singular value decompositions,

[formula]

Using the representations for [formula] and [formula], we have,

[formula]

As the column vectors [formula] and [formula] on the right hand side of [\eqref=eqn:mstar] are linearly independent, [formula] is possible if and only if every column of [formula] combines [formula] and [formula] in the same ratio. This means that the row vectors on the r.h.s. of [\eqref=eqn:mstar] are scalar multiples of each other. Thus, for [formula] it is necessary that So, α1α3  +  α2α4  ≤  0 implies that [formula]. As [formula] is arbitrary, [formula] is identifiable by  [\eqref=prob:rank].

Next we prove the "only if" part. Let [formula] be identifiable and [formula] so that [formula] is feasible for  [\eqref=prob:rank]. As before, we have [formula], [formula] and [formula]. If [formula], then [formula], [formula] for some [formula] with α21  +  α22  =  α23  +  α24  =  1. It is simple to check that [\eqref=eqn:alt_svd] and [\eqref=eqn:mstar] are valid. We shall now assume ε  =  α1α3  +  α2α4  >  0 and arrive at a contradiction. Since multiplying a matrix by a nonzero scalar does not change its row or column space and scales every nonzero singular value in the same ratio, we can take [formula] without violating any assumptions on [formula]. Thus, [formula] and we have [formula] (the last implication is due to [\eqref=eqn:mstar]) thus contradicting the identifiability of [formula].

Proof of  [\ref=lem:Markov_estimate]

Using assumption , we have

[formula]

Hence, where [\eqref=eqn:using_expect_norm_squared] follows from [\eqref=eqn:expect_norm_squared], [\eqref=eqn:using_independence] and [\eqref=eqn:relating_u_and_x] are true because [formula] and assumption  implies independence of [formula] and [formula], [\eqref=eqn:square_of_projection_matrix] is true since [formula] for any projection matrix [formula], [\eqref=eqn:E_commutes_with_Tr_and_P] is true since expectation operator commutes with trace and projection operators, [\eqref=eqn:using_cov_id] follows from assumption  and, [\eqref=eqn:using_rank-2_X] is true since [formula] is a matrix of rank at most two.

Finally, applying Markov inequality to the non-negative random variable [formula] and using the computed estimate of [formula] from [\eqref=eqn:square_norm_projected_estimate] gives

[formula]

We have thus established [\eqref=eqn:Markov_1]. Using the exact same sequence of steps for the random vector [formula] gives the bound in [\eqref=eqn:Markov_2].

Proof of  [\ref=lem:Markov_estimate_special]

Notice that [\eqref=eqn:square_of_projection_matrix]-[\eqref=eqn:using_rank-2_X] in the proof of  [\ref=lem:Markov_estimate] in  [\ref=sec:Markov_estimate_lemma_proof] does not use assumption . Hence, reusing the same arguments we get

[formula]

Thus, we have where [\eqref=eqn:relate_u_and_x] is true since [formula], [\eqref=eqn:bound_norm_x] holds because of assumption , [\eqref=eqn:Markov_inequality] follows from applying Markov inequality to the non-negative random variable [formula] and, [\eqref=eqn:bound_norm_projected_x] follows from [\eqref=eqn:expect_projected_norm_square]. Thus, the derivation [\eqref=eqn:derivation] establishes [\eqref=eqn:Markov_1_special]. Using the same sequence of steps for the random vector [formula] gives the bound in [\eqref=eqn:Markov_2_special].

Proof of  [\ref=thm:whp_suff_ident]

For any constant [formula], let [formula] denote the event that [formula] satisfying both [formula] and [formula]. We note that [formula] constitutes a non-decreasing sequence of sets as δ increases. Hence, using continuity of the probability measure from above we have, Note that [formula] denotes the event that [formula] satisfying both [formula] and [formula] which is a "hard" event. The event [formula] corresponds precisely to the sufficient conditions of  [\ref=thm:suff_ident]. Hence, it is sufficient to obtain an appropriate lower bound for [formula] to make our desired statement. Drawing inspiration from [\eqref=eqn:inequality] and [\eqref=eqn:limit_equality], we shall upper bound [formula] by [formula].

For any given [formula] we have, where [\eqref=eqn:orthogonal_decomposition] is true because [formula] (respectively [formula]) is the orthogonal projection matrix onto the orthogonal complement space of [formula] (respectively [formula]), [\eqref=eqn:unit_norm_vectors] is true because we have [formula], [\eqref=eqn:by_independence] is true by independence of [formula] and [formula], and [\eqref=eqn:from_lemma] comes from applying  [\ref=lem:Markov_estimate].

Next we employ union bounding over all [formula] representing distinct pairs of column and row subspaces [formula] to upper bound [formula]. We denote the number of these distinct pairs of [formula] over [formula] by [formula].

Finally, using [\eqref=eqn:multiplicative_estimate] we have where [\eqref=eqn:union_bound_finite] is an union bounding step. Hence, where [\eqref=eqn:relaxation] is from [\eqref=eqn:inequality], [\eqref=eqn:relaxation_value] is from [\eqref=eqn:union_bound] and [formula].

Proof of  [\ref=cor:whp_suff_ident]

The proof is essentially to that of  [\ref=thm:whp_suff_ident] in  [\ref=sec:whp_suff_ident_theorem_proof] with one important difference: we use  [\ref=lem:Markov_estimate_special] instead of  [\ref=lem:Markov_estimate] when bounding the right hand side of [\eqref=eqn:by_independence]. This gives us the bound

[formula]

[formula]

which leads to the bound

[formula]

in place of [\eqref=eqn:lemma_1_result]. Finally, where [\eqref=eqn:relaxation_special] is from [\eqref=eqn:inequality], [\eqref=eqn:relaxation_value_special] is from [\eqref=eqn:union_bound_special] and [formula].

Proof of  [\ref=lem:Gaussian_Chernoff_estimate]

This is a Chernoff-type bound. We set

[formula]

and compute the bound

[formula]

that holds for all values of the parameter t for which the right hand side of [\eqref=eqn:chernoff] exists. Using properties of Gaussian random vectors under linear transforms, we have [formula] and [formula] as statistically independent Gaussian random vectors implying

[formula]

[formula]

with,

[formula]

Since [formula], both [formula] and [formula] are two dimensional spaces. On rotating coordinates to the basis given by [formula], it can be seen that Z1 is the sum of squares of two  standard Gaussian random variables and hence has a χ2 distribution with two DoF. By the same argument, Z2 is a χ2 distributed random variable with (m  -  2) DoF. Recall that the moment generating function of a χ2 distributed random variable Z with k DoF is given by

[formula]

Using [\eqref=eqn:ortho_split], [\eqref=eqn:chernoff], [\eqref=eqn:independent_split] and [\eqref=eqn:chi_square_mgf] we have the bound

[formula]

[formula]

which can be optimized over t. It can be verified by differentiation that the best bound is obtained for

[formula]

Plugging this value of t into [\eqref=eqn:parametrized_exponent] and using [\eqref=eqn:ortho_split] we get the desired result.

Proof of  [\ref=lem:Bernoulli_Chernoff_estimate]

This is also a Chernoff-type bound. Although the final results of  [\ref=lem:Gaussian_Chernoff_estimate] and [\ref=lem:Bernoulli_Chernoff_estimate] look quite similar, we cannot reuse the manipulations in  [\ref=sec:Gaussian_Chernoff_estimate_proof] for this proof and proceed by a slightly different route (also applicable to other subgaussian distributions) since the symmetric Bernoulli distribution does not share the rotational invariance property of the multivariate standard normal distribution. Let [formula] denote an orthonormal basis for [formula] and set

[formula]

Notice that [formula], so we have where [\eqref=eqn:union_bound_1] and [\eqref=eqn:union_bound_2] utilize elementary union bounds, [formula] is a generic unit vector, [\eqref=eqn:symmetric_distribution] uses the symmetry of the distribution of [formula] about the origin, and [\eqref=eqn:chernoff_bound] is the Chernoff bounding step that utilizes the following computation: where [\eqref=eqn:independence] uses independence of elements of [formula], [\eqref=eqn:symmetric_Bernoulli] is true because each element of [formula] has a symmetric Bernoulli distribution, [\eqref=eqn:series_expansion] uses the series expansion of the exponential function, [\eqref=eqn:unit_norm] follows from [formula] and [\eqref=eqn:factorial_bound] is due to

[formula]

The bound in [\eqref=eqn:chernoff_bound] can be optimized over t with the optimum being achieved at

[formula]

Plugging this value of t into [\eqref=eqn:chernoff_bound] gives the desired result.

Proof of  [\ref=lem:metric_entropy]

Consider the norm [formula] on [formula] defined as

[formula]

for all [formula]. It is clear that [formula] is the unit ball [formula] of this norm, which is a convex body symmetric about the origin. Hence, using [\eqref=eqn:covering_number] we have the metric entropy of [formula]  [formula] as [formula]. It is clear that [formula], implying that metric entropy of [formula]  [formula] is [formula].

Let [formula] and [formula] be two elements from [formula] such that [formula], and let [formula] be arbitrary. Then, where [\eqref=eqn:abs_val_inside] is due to [formula], [\eqref=eqn:epsilon_ball] is due to the Cauchy-Schwartz inequality and the bound [formula] as [formula], and [\eqref=eqn:triangle_inequality] is due to the triangle inequality. Since [formula] and [formula] are interchangeable in the derivation of [\eqref=eqn:triangle_inequality] and [formula] is arbitrary, we immediately arrive at [\eqref=eqn:projection_bound].

Proof of  [\ref=thm:whp_infinite]

We follow a proof strategy similar to that of  [\ref=thm:whp_suff_ident]. For any constant [formula], let [formula] (respectively [formula]) denote the event that [formula] satisfying, [formula] (respectively [formula]), and let [formula] denote the event that [formula] satisfying both [formula] and [formula]. We note that [formula] constitutes a non-decreasing sequence of sets as δ increases. Hence, using continuity of the probability measure from above we have, Note that [formula] denotes the event that [formula] satisfying both [formula] and [formula] which is a "hard" event. The event [formula] corresponds precisely to the sufficient conditions of  [\ref=thm:suff_ident]. Hence, it is sufficient to obtain an appropriate lower bound for [formula], or alternatively, upper bound [formula] using [\eqref=eqn:inequality_copy]. It is straightforward to see that where [\eqref=eqn:decompose_probabilities_a] is because [formula] happens only when [formula] and [formula] are caused by the same matrix [formula], and [\eqref=eqn:decompose_probabilities] is due to mutual independence between [formula] and [formula].

We have [formula] and [formula] drawn component-wise  from a symmetric Bernoulli distribution. For any given [formula] we have a bound on [formula] from  [\ref=lem:Bernoulli_Chernoff_estimate]. We focus on the union bounding step to compute [formula]. The proof of  [\ref=lem:metric_entropy] assures us that as long as [formula] are close enough,  within the same [formula] ball for some 1  >  ε  ≥  ε0  >  0, we are guaranteed tight control over [formula] for any arbitrary [formula]. In fact, using [\eqref=eqn:control_proj_norm_diff] we have the bound

[formula]

[formula]

Letting [formula] denote the center of the [formula] [formula] ball we have k ranging from 1 to [formula]. We thus have [formula] upper bounded by where [\eqref=eqn:union_bounding] is from an elementary union bound, [\eqref=eqn:pointwise_bound] is from [\eqref=eqn:bound_from_lemma_4], [\eqref=eqn:intermediate_bound] uses

[formula]

with [formula] being generic, and [\eqref=eqn:final_bound] is true due to  [\ref=lem:Bernoulli_Chernoff_estimate].

Replicating a similar sequence of steps to bound [formula], one readily obtains the bound

[formula]

with δ' given by [\eqref=eqn:delta_prime]. Hence, combining [\eqref=eqn:inequality_copy], [\eqref=eqn:decompose_probabilities], [\eqref=eqn:final_bound] and [\eqref=eqn:final_bound_rows] we get

[formula]

which yields the desired bound for [formula] when p  =  pc  +  pr.

Proof of  [\ref=thm:whp_infinite_Gaussian]

We have [formula] and [formula] drawn component-wise  from a standard Gaussian distribution. The proof is essentially similar to that of  [\ref=thm:whp_infinite] with one important difference (beside replacing all occurrences of [formula] by [formula] and ε assuming values in [formula]): we use the bound given by  [\ref=lem:Gaussian_Chernoff_estimate] instead of  [\ref=lem:Bernoulli_Chernoff_estimate] when evaluating [formula] in [\eqref=eqn:intermediate_bound]. This gives us the bounds where

[formula]

[formula]

As in [\eqref=eqn:advertised_bound_1], we have

[formula]

[formula]

which gives the desired bound, since p  =  pc  +  pr and

[formula]

Proof of  [\ref=prop:rank-2_nullspace]

Let [formula] admit a factorization as in [\eqref=eqn:rank-2_nullspace]. Then,

[formula]

and we see that the matrix [formula] is obtained by shifting down the elements of the matrix [formula] by one unit along the anti diagonals, and then flipping the sign of each element. Since the convolution operator [formula] sums elements along the anti diagonals (see  [\ref=fig:lifting_example] for illustration), the representation of [formula] as in [\eqref=eqn:anti-diagonal_representation] immediately implies that [formula]. Since [\eqref=eqn:rank-2_nullspace] implies that [formula] so we have [formula].