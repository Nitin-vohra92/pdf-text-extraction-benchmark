=1

Asynchronous Transmission over Single-User State-Dependent Channels

Introduction

State dependent channels with side information known at the encoder were first introduced by Shannon. In [\cite=shannon], Shannon established a single-letter expression for the capacity of state dependent channels with side information known causally at the encoder and unknown to the decoder. Subsequently, Kusnetsov and Tsybakov [\cite=Kusnetsov] introduced channels with i.i.d. side information which is known non-causally at the encoder and Gel'fand and Pinsker derived the formula for the capacity of these channels using random binning encoding methods [\cite=GP].

The introduction of state dependent channels with side information at the transmitter was originally aimed at analyzing coding techniques for computer memory with defect whose locations are known to the encoder only [\cite=Heegard]. With the development of communication systems and the Internet, other relevant applications have emerged. Amongst them are cognitive radio [\cite=Devroye] [\cite=GoldsmithJafar2009], watermarking [\cite=Moulin], multiple-input multiple-output broadcast channels [\cite=Caire], multiple-access channels with channel side information [\cite=Anelia2008], etc. The common underlying assumption in the analysis of these channels, is that the side information signal is synchronized with the signal produced by the encoder. However, in practical situations this assumption does not necessarily hold and the side information signal may be a delayed version of the channel states sequence. When the assumption of the synchronization does not hold, the known results of the aforementioned channels are not necessarily valid, new models that encompass the unknown delay of the state sequence at the transmitter need to be addressed.

Other models that may suffer from asynchronism are multi-user channels, in which users are assumed to be synchronized with one another. The discrete memoryless multiple access channel (MAC) with independent sources was the first channel from this family that was considered in an asynchronous setup [\cite=CoverMcEliece1981] [\cite=HuiHumblet1985] [\cite=Verdu1989]. It was shown by Cover et. al. [\cite=CoverMcEliece1981] that if the delay is finite or grows sufficiently slowly relatively to the block length, then the asynchronism does not change the capacity region. However, Hui and Humblet [\cite=HuiHumblet1985] showed that the capacity region may be reduced if the delay is of the same order of the block length, since time sharing cannot be used.

In this paper, we address the question of whether an asynchronous side information is useful when the delay is bounded. By lower bounding the achievable rates using time sharing between all possible delays, we prove that the asynchronous side information can still be of value in the asynchronous Gel'fand-Pinsker channel [\cite=Eilat_us]. We improve the lower bound for the asynchronous Gel'fand-Pinsker channel by studying two of its counterparts: the multicast channel [\cite=Khisti], and the compound channel [\cite=Piantanida] [\cite=Nair], and by taking into account the specific characteristics of our setup. In addition, we observe that if feedback is present, the capacity of the asynchronous Gel'fand-Pinsker channel is equal to the capacity of the synchronous Gel'fand-Pinsker channel. We additionally consider state dependent channels with state information available asynchronously and causally at the transmitter. Contrary to the non-causal and asynchronous state information, in the causal setup there are cases in which the side information does not improve the reliably transmitted rates. We distinguish between two cases of possible delay values: If the maximal delay is positive, i.e., the encoder may observe at each time instant a past actual state, then the side information can be ignored without loss of optimality. Otherwise, a scheme which is based on the limited lookahead scheme of [\cite=WeissmanElGamal] is presented. We additionally consider asynchronous channels with noncausal state information at both transmitter and receiver, whose causal and non-causal counterparts were analyzed in [\cite=Wolfowitz1978] [\cite=HeegardElGamal1983] [\cite=Salehi1992] [\cite=GoldsmithVaraiya1997] [\cite=Rosenzweig2005]. We note that the results of this paper were partially presented in [\cite=Eilat_us].

In recent years a new technology coined as "Cognitive Radio" [\cite=Mitola] [\cite=Haykin] [\cite=GoldsmithJafar2009] has emerged. The term "cognitive radio networks" encompasses several models and definitions, however, generally speaking, the common assumption for these networks is the existence of cognitive users that can sense their surroundings and are able to change their configurations accordingly. The presence of such users in a network can drastically improve spectrum utilization and even help the non-cognitive users. In some models of cognitive radio networks, the cognitive users possess a knowledge of the codewords that licensed users transmit. Consequently, the Gel'fand-Pinsker channel, channels with side information at the transmitter and receiver, and the cognitive MAC are among the building blocks of cognitive radio networks [\cite=GoldsmithJafar2009] [\cite=Devroye]. The capacities of some of these synchronous channel models are known. Nevertheless, practical communication systems are not always synchronized. Examples for practical setups in which asynchronism in state information may arise:

Multicast communication systems, in which the same message is to be transmitted to several destinations where the state sequence suffers different delays.

A communication system with no feedback, in which a cognitive transmitter obtains information about the interfering signal but does not know the time offset by which it is received since the delay towards the receiver is unknown.

A MAC with no feedback in which a cognitive user knows in advance the message which the other user (the non-cognitive user) is about to send, however, the two users may not be fully synchronized for example due to clock synchronization limitation or unknown delay in the channel.

Cellular networks in which a helping interferer helps the base-stations to conceal their messages. In this setup, which is depicted in Fig. [\ref=Eavesdroppers_fig], several base-stations serve mobile users in the network while information leaks to the passive eavesdroppers. The helping interferer is linked to the base-stations by optical fiber channels and periodically informs them of the interfering signals it is about to transmit. Alternatively, the helping interferer and the base-stations can agree on a list of signals which the interferer will transmit in a particular order. It is also assumed that the base-stations can acquire information on the locations of users. However, synchronization issues between the helping interferer and the base-stations, the mobility of users, and unprecise users' location at the base-stations can cause the interfering signal and a base-station's transmitted signal to be out of sync. A partial list of relevant papers for the synchronous setup is [\cite=Mitrpant2006] [\cite=ChenVinck2008] [\cite=SimeoneYener2009] [\cite=KhistiDiggavi2011] [\cite=Xu2014] where one can treat the side information in some of these papers as the interferer's signal.

Cellular networks in which coordinated multipoint (CoMP) techniques are used (see for example [\cite=Karakayali2006] [\cite=Irmer2011]). There are several CoMP methods for the downlink which involve different schemes for cooperation and coordination of base-stations. Base stations cooperation may also occur in the uplink, for example several base-stations can jointly decode received signals. As discussed in [\cite=Irmer2011], there can be synchronization issues in these cooperative schemes. A detailed example of an asynchronous CoMP is discussed in [\cite=LiuLi2013].

We note that the results of this paper were extended to multiuser setups in [\cite=us_ISIT_2014] [\cite=us_paper2].

The rest of this paper is organized as follows. In Section [\ref=sec:Channel_Models] we present channel models which are analyzed and define several notations that are used throughout this paper. Subsequently, in Section [\ref=sec:AGP_Channel] we discuss the asynchronous Gel'fand-Pinsker channel and state lower bounds on its capacity. Section [\ref=sec:Achievable_ACSI] is devoted to channels with asynchronous causal state information at the transmitter. We then present in Section [\ref=sec:Achievable_ACSITR] the capacity of channels with asynchronous channel state information at both the transmitter and receiver. Finally, Section [\ref=sec:Conclusion] contains concluding remarks.

Channel Models and Definitions

We use the following notations and definitions: A vector [formula] is denoted by an, whereas the vector [formula] is denoted by aji. If an is a sequence of vectors, then the notation ai,j is used to address the j entry of the vector ai. The probability law of a random variable X is denoted by PX while P(X) denotes the set of distributions on the alphabet X. The set of all n vectors xn that are ε-strongly typical [\cite=CT] with respect to PX∈P(X) is denoted by Tnε(X). Additionally, we denote by Tnε(X|yn) the set of all n vectors xn that are ε-strongly jointly typical with the vector yn with respect to a probability mass function (p.m.f.) PX,Y. Further, [formula] denotes the indicator function, i.e., [formula] equals 1 if the statement A holds and 0 otherwise.

In addition, D is a set of integers, and D = |D| denotes its cardinality. Further, let P be a conditional p.m.f. from X to Y. For xD∈XD denote by {Pd(y|xD1)} a set of conditional p.m.f.'s from XD to Y, that depend on the value of d, where d∈D. We use the notation Tnd,ε(X,Y) to make the underlying p.m.f. Pd(xD1,y) explicit where d∈D. Similarly, we use the notation Tnp,ε(X) to make the underlying p.m.f. p explicit.

We next describe the channel models of the aforementioned channels.

The Asynchronous Gel'fand-Pinsker Channel

The asynchronous Gel'fand-Pinsker channel (AGP channel), which is depicted in Fig. [\ref=AGP_fig], is a discrete memoryless stationary and state-dependent channel. It is defined by the channel transition probabilities [formula], the channel input alphabet X, the channel output alphabet Y, the state symbol alphabet S, and the state sequence distribution, which is assumed to be i.i.d. PS. The transmitter observes non-causally a possibly delayed version of the states sequence [formula]. In other words, before the beginning of transmission, the transmitter observes a sequence [formula] of state symbols according to: where d∈D, and [formula] are i.i.d. with Zi  ~  PS independent of [formula]. Since An is a possibly delayed version of the sequence Sn, it follows that A  =  S and An∈Sn.

Let xn∈X and sn∈Sn be the codeword and the state-sequence, respectively, and let yn∈Yn be the output of the channel. The conditional distribution of Yn given (Xn,Sn) is given by

Let [formula], and assume that the massage M is a random variable uniformly distributed over the set M. A (2nR,n)-code for the AGP channel consists of an encoding function and a decoding function Define the average probability of error for d∈D as where [formula] and [formula].

A (2nR,n)-code for the AGP channel is said to be a (2nR,n,ε)-code if e,d  ≤  ε for all d∈D. A rate R is said to be achievable for the AGP channel, if there exists a sequence of [formula]-codes with εn  →  0 as n  →    ∞  . The capacity of the AGP channel, CAGP, is the supremum of all achievable rates.

The Causal Case

We next introduce a state dependent channel with asynchronous causal state information (ACSI) at the transmitter. We refer to this channel as the ACSI channel.

The definitions for the ACSI channel are similar to those of the AGP channel, with the following modifications:

In this setup, before transmitting Xi, the encoder observes [formula] which are defined in ([\ref=equation1]) (rather than [formula]).

As before, it is assumed that the messages are equiprobable over M. A (2nR,n)-code for the ACSI channel consists of the encoding functions [formula] where

[formula]

and a decoding function

The average probability of error is given by, where Pd(sn,an) is defined in ([\ref=P_dsa]). The definitions of the achievable rate and the capacity are similar to those of the AGP channel.

Asynchronous Channels with States Available Non-Causally Both at the Transmitter and Receiver

An asynchronous channel with channel states non-causally known at both the transmitter and receiver (see Fig. [\ref=Delay_trans_receiv_fig]) is a stationary discrete memoryless state-dependent channel, defined by {P(y|s,x)},X,Y,S, and PS as before. Both the transmitter and the receiver observe non-causally the sequence [formula], and in addition the link between the state source and the channel may suffer a delay d where d∈D.

Let the random message M be defined as before, i.e., distributed equiprobably over M. A (2nR,n)-code for the asynchronous channel with channel states non-causally known both at the transmitter and receiver, consists of an encoding function and a decoding function Define the average probability of error for d∈D as [formula] and where for all [formula] such that [formula], si - d are arbitrary.

A (2nR,n)-code is said to be a (2nR,n,ε)-code if e,d  ≤  ε for all d∈D. A rate R is said to be achievable for the asynchronous channel with channel states non-causally known both at the transmitter and receiver, if there exists a sequence of [formula]-codes with εn  →  0 as n  →    ∞  .

The capacity of the asynchronous channel with channel states non-causally known both at the transmitter and receiver, CACSITR, is the supremum of all achievable rates.

The Set of Possible Delays

For simplicity of the presentation, throughout this paper, we assume that the set of possible delays in the aforementioned channels is [formula], where 0  ≤  dmin,dmax, it follows that D = dmax + dmin + 1. Additionally, throughout this paper we assume that all transmitters and receivers know a-priori the (finite) values dmin and dmax. We note that the results which are derived in this paper can be easily generalized to arbitrary finite sets of delays, and hold in the general case in which the delay is randomly distributed over a finite set.

Known Delay at the Receiver

In all of the above channel models, i.e., the AGP, ACSI and the asynchronous channel with channel states non-causally known at both the transmitter and receiver, we assume that the decoder does not know the actual delay in the channel before decoding the message. However, since the set of delays D is finite, by sending predefined training sequences in the first o(n) bits, the decoder can deduce the delay with probability of error that vanishes as n tends to infinity. Therefore, we can assume hereafter that the decoder knows the delay d prior to the decoding stage. We will however include transmission of the training sequence in our coding schemes.

The AGP Channel

In this section we derive lower bounds for the capacity of the AGP channel, when the alphabets X,S,Y, and the delay set, D are finite. In addition, we state the capacity of the AGP channel with feedback for finite delays.

An Achievable Rate for the AGP Channel

The single-letter formula for the capacity of the synchronous Gel'fand-Pinsker (GP) channel PY|X,S is given by [\cite=GP]

[formula]

where U - (S,X) - Y is a Markov chain, and |U|  ≤  |X|  ·  |S|.

We next present an achievable rate for the AGP channel. In Section [\ref=sec:Example] we prove that this lower bound is tight for the binary symmetric AGP channel with crossover probability of 0.5 and D  =  {0,1}.

The rate

[formula]

where,

[formula]

is achievable for the AGP channel with channel conditional distribution PY|X,S and a set of delays D.

Note that the rate in ([\ref=AGP_rate1]) converges to the channel capacity with no side information, as D, the size of the set of all possible delays, tends to infinity. In addition, if dmin = dmax = 0, that is, there is no actual delay in the channel, the channel degenerates to the Gelf'and-Pinsker channel, as the formula ([\ref=AGP_rate1]) indicates.

The coding scheme that is used in the proof employs binning and "segment time sharing". In segment time sharing, the codeword is partitioned to several segments. In each segment the encoder chooses a different encoding function (similarly to the ordinary time sharing). To decode the message, the decoder which knows the identity of the segments jointly decodes the segments. That is, unlike the ordinary time sharing, the decoder in segment time sharing jointly decodes all the segments: The main idea of the proof is that the encoder uses the GP coding scheme for each possible delay by dividing the codeword into equal length segments. In each of these segments the encoder assumes a different delay (out of the set D). The decoder knows for each segment the assumed delay which was decided by the encoder. Additionally, as mentioned before, we can assume that the decoder knows the actual delay of the side information (for example by sending a training sequence). Knowing this delay the decoder looks for a codeword such that each of its segments is typical with its corresponding output according to the p.m.f which is induced by the channel transition probability and the assumed delay of the segment in the encoding stage. In the AGP setup segment time sharing yields better results than ordinary time sharing since the redundancy in one segment can help in decoding another segment. For the detailed proof see Appendix [\ref=AGP_A1].

An Example - The Binary Symmetric AGP Channel

Consider the binary symmetric AGP (BS-AGP) channel defined by the input-output relation, where [formula], and with d∈{0,1}. In the ordinary synchronous GP setup, a capacity achieving scheme is to construct a codebook containing all the possible binary vectors un∈{0,1}n. To transmit the vector un, the transmitter sends [formula]. Consequently, the received ith symbol is yi = ui, and the resulting achievable rate is thus that of the clean channel yi = ui, i.e., 1 bit per channel use. In the asynchronous case, consider the following coding scheme which is a special case of the general scheme presented in Section [\ref=sec:Achievable_AGP]. A codebook containing [formula] binary codewords un with binary [formula] symbols is drawn. Recall that [formula] is the (possibly delayed) observed state sequence and let Let PUY be the product p.m.f., i.e. PU,Y(u,y) = PU(u)PY(y). The decoder looks for a sequence un such that if d = 0, or such that if d = 1. In the case of no delay, d = 0, this results in,

In the case where d = 1, this results similarly in Define the random variables [formula] where [formula], and the random variables [formula] where [formula]. Clearly, this scheme can guarantee reliable decoding of the message for all rates lower than In the general case, i.e. the BS-AGP channel with crossover probability p, a similar coding scheme assures reliable decoding of the message for all rates lower than where h2(x) =  - x log 2(x) - (1 - x) log 2(1 - x).

In Fig. [\ref=BSC_example_fig], we compare the lower bound Rl(p) to the capacity of the binary symmetric Gel'fand-Pinker channel and to the capacity of the binary symmetric channel (BSC) with no side information at the encoder nor the decoder, all channels have crossover probability p.

An important question is whether the rate Rl(p) is the capacity of the BS-AGP channel with crossover probability of p and whether the answer depends on the crossover probability p?

To answer these questions we rely on a relevant setup which is considered in [\cite=Khisti]. The state dependent binary multicast channel that is studied in [\cite=Khisti] is composed of the input sequence Xn, the channel states sequence {Sn(1),Sn(2)} and the output sequences. The state dependent binary multicast channel is defined by the following input-outputs relations, where Sn(1),Sn(2),Xn,Yn(1),Yn(2)∈{0,1}n, and [formula] is a symbol-by-symbol modulo-2 operation. It is known [\cite=Khisti], that for two correlated sequences Sn(1),Sn(2) which are not necessarily i.i.d. processes, the capacity of the binary multicast channel is,

The BS-AGP channel can be identified with the binary multicast channel that appears in [\cite=Khisti], where Si and Si - 1 play the roles of S1,i and S2,i, respectively. Note that the process [formula] is not an i.i.d. process.

Now, knowing the capacity of the channel, we can conclude that the capacity of the BS-AGP channel with crossover probability [formula] is a special case of ([\ref=BSC_capacity]). This is true since the process [formula] is i.i.d. when [formula]. However, if p∉{0,0.5,1}, then [formula] is not a memoryless or constant sequence. Therefore, That is, Rl(p) is not the capacity of the BS-AGP channel when p∉{0,0.5,1}. Another insight from equation ([\ref=BSC_capacity]) is that in some channels there is a gain in using multi-letters coding. This insight will be used later in our generalized scheme.

An additional issue concerns the usefulness of the side information for the BS-AGP. For simplicity we consider the BS-AGP channel with crossover probability 0.5 with different cardinality of the delay set D. As mentioned before for [formula], the sequence [formula] is i.i.d. with p.m.f. [formula] for each d∈D. By generalizing the coding scheme in Eq. ([\ref=coding_scheme_2_delays]) for the set of delays of cardinality D and by Theorem [\ref=theorem_AGP1], all rates which are not greater than [formula] bits/channel use are achievable. Fig. [\ref=BSC_example_several_delays] depicts the lower bound on the capacity of the BS-AGP channel with crossover probability 0.5 with respect to the number of possible delays D.

An Improved Lower bound for the AGP Channel

For dmin,dmax  <    ∞   we can identify the AGP channel with a multicast channel with D = dmax + dmin + 1 users, in which all the users share the same channel transition probabilities, but differ in the fact that the state of channel [formula] at time i is Si - dmin + k - 1.

In addition, similarly to [\cite=Piantanida] [\cite=Nair], the channel in Fig.  [\ref=AGP_multicast_fig] has a compound channel representation depicted in Fig. [\ref=AGP_compound_fig] where P(y|x,v,k) = PY|X,S(y|x,vk), [formula], and [formula] is the vector of all possible channel states at time i.

We note that the D - tuples: [formula] are statistically dependent, additionally where [formula].

We next present improved achievable rates for the AGP channel. The coding schemes that achieve these rates are extensions of Theorems 2.4 and 2.6 in [\cite=Piantanida] and of Theorem 1 in [\cite=Nair].

For the sake of clarity, we first present an achievable rate for the case d∈{0,1}.

Let PY|X,S be the channel conditional distribution of an AGP channel with a set of delay D  =  {0,1}. The rate, is achievable where V = (V1,V2) and PV(v1,v2) = PS(v1)PS(v2), and

This result can be generalized for any 0  ≤  dmin,dmax  <    ∞  .

Let PY|X,S be the channel conditional distribution of an AGP channel with a set of delay [formula]. Denote [formula] where D = dmin + dmax + 1. The rate, is achievable for the AGP channel, where PV is given in ([\ref=Pv1]), and

For the simplicity of the presentation we prove only Theorem [\ref=theorem_AGP2], the proof appears in Appendix [\ref=AGP_A2]. The proof of Theorem [\ref=Theorem_AGP_general_rate] consists of similar steps and therefore is omitted.

In addition, in light of the BS-AGP channel example, a coding scheme which involves multi-letter coding can achieve higher rates. Therefore, we include multi-letter coding in the coding scheme of Theorem [\ref=Theorem_AGP_general_rate]. This yields the following result.

Let PY|X,S be the channel conditional distribution of an AGP channel with a set of delays [formula]. Denote [formula] where D = dmin + dmax + 1. The rate, is achievable for the AGP channel, where [formula], and vi,j denotes the j-th entry in of the vector vi. Additionally,

Finally, we remark that similar results hold for stationary Markov state-source, with the exception that equations ([\ref=Pv1]) and ([\ref=v_memoryless_source1])-([\ref=v_memoryless_source2]) are replaced with the probability law of the Markov source. Furthermore, this is also true for stationary and ergodic state-source, where equations ([\ref=Pv1]) and ([\ref=v_memoryless_source1])-([\ref=v_memoryless_source2]) are changed according to the state-source distribution.

The AGP Channel with Feedback

In this section we consider the AGP channel with feedback. In this setup, in addition to the non-causal knowledge of An, at each time instant i the encoder observes Yi - 1. We assume that dmax,dmin  <    ∞   and, as before, the delay is fixed throughout the transmission of a codeword. Unlike the case of the AGP channel, in this case the encoder can recover the actual delay of the side information by sending a training sequence. We next prove that the capacity of the AGP channel with feedback is equal to the capacity of the GP channel.

The capacity of the AGP channel with feedback is given by

[formula]

, , ,

An Achievable Rate for the ACSI Channel

In this section we address the case of causal state information at the transmitter, i.e., the ACSI channel model, as previously defined Section [\ref=acsi_def]. We next show that, unlike the AGP channel model, if the set D includes positive delays, the encoder can ignore the side information with no loss of optimality in terms of achievable rates. We next formalize and prove the above statement.

If dmax > 0, then the capacity of the ACSI channel is given by

[formula]

where [formula].

We note that if dmax = 0, then the synchronous counterpart of this setup for d < 0 is that of a limited lookahead analyzed in [\cite=WeissmanElGamal] which results in a multi-letter expression for the capacity. The encoding scheme we use for the case dmax = 0 is similar to that of the AGP channel described in Section [\ref=sec:Achievable_AGP], in the sense that the transmitter splits the timeline [formula] into D = dmin + 1 segments. In each of the segments, the coding scheme that corresponds to the appropriate lookahead d [\cite=WeissmanElGamal] is applied (and in the segment corresponding to d = 0 Shannon's causal scheme [\cite=shannon] is applied). The resulting achievable rate does not have a single-letter expression and is omitted for the sake of brevity.

Asynchronous Channels with States Non-Causally Available to the Transmitter and the Receiver

In this section, we derive the capacity formula of asynchronous channels with states non-causally available both at the transmitter and the receiver (see Fig. [\ref=Delay_trans_receiv_fig] and Section [\ref=ACSITR_def]). We note that since the decoder knows the side information (and can deduce the actual delay), both the encoder and the decoder rely on the side information sequence in their enccoding/decoding strategies. This is the fundamental difference from the AGP channel model, in which the decoder can only rely on the statistics of the state-sequence if the coding scheme does not include sending the side information to the decoder. Finally, we show that a coding scheme that considers all possible side information symbols for all possible delays is capacity achieving.

Let D be a set of possible delays and D = |D|. The capacity of the asynchronous channel with states non-causally available at the transmitter and the receiver and a channel conditional distribution PY|X,S is where V∈SD is a random variable distributed according to [formula], and where vdmax - d + 1 is the (dmax - d + 1)th entry in the vector v. Additionally,

The achievability coding scheme consists of a "strategy letters" coding scheme [\cite=Salehi1992]. It is implemented by using the sequence Vn as the state-sequence which the strategy maps. The detailed proof which consists of the achievability part and the converse part is included in Appendix [\ref=ACSITR_Capacity_Proof]. Further, we note that a naive rate-splitting coding scheme which uses the sequence Vn as a time-sharing sequence may lead to suboptimal results since each sub-message is separately reconstructed under all possible delays. This degradation follows from the independence between sub-messages in rate-splitting coding scheme which prohibits us from using redundancy in one sub-message to help us decoding another sub-message.

We remark that in contrast to the synchronous models, in which causal and non-causal knowledge of the state-sequence at both the encoder and decoder yield the same channel capacity, in the asynchronous setup the capacities of these models do not necessarily coincide.

In addition, one can consider a different setup in which the delay d symbolizes the presence of a jitter. The jitter is modeled by a delay that randomly changes every sub-block of a sufficiently large size that allows the decoder to find the delay in the sub-block with an error probability that decays with the block length. It can be shown that in this setup, if the delays are i.i.d. random variables distributed over the set D, the minimization over the delay d in ([\ref=capacity_ACSITR_eq]) can be replaced with an expectation over the delay d.

Finally, the result generalizes straightforwardly to state dependent compound channels with state information at the transmitter and receiver. Specifically, let Θ be a finite set of channels from X  ×  S to Y, and let Pθ(y|x,s) denote the transition probability of channel θ, as before X,S,Y denote the channel input, channel state, and channel output, respectively.

The capacity of the state dependent compound channel is given by where

[formula]

Conclusion

In this paper we presented several asynchronous channel models that include side information at the transmitter and/or receiver. We derived an achievable rate for the AGP channel using an encoding scheme which combines binning and time sharing. We then generalized this lower bound by representing the AGP channel as a compound channel. Further, we proved that although the side information is known asynchronously, it is still of value and can be exploited. We further discussed the ACSI channel in which the side information is available asynchronously and causally at the transmitter. We proved that if the delay can take positive values then the side information does not increase the capacity of the ACSI channel. Finally, we established a single-letter expression for the capacity of asynchronous channels with side information at both the transmitter and receiver.

Acknowledgement

This work was partially supported by Israel Science Foundation (ISF) grant 2013/919. The authors would like to thank the anonymous reviewers of the Transactions on Information Theory for their helpful and constructive comments which helped improve the content of this paper.