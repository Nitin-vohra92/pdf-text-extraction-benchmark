Fast matrix computations for functional additive models

Introduction

One of the most frequent scenarios in functional data analysis is that one has a group of related functions (for example, learning curves, growth curves, EEG traces, etc.) and the goal is to describe what these functions have in common and how they vary [\citep=RamsaySilverman:FunctionalDataAnalysis] [\citep=Behseta:HierarchModelsVariabFunctions] [\citep=Cheng:BayesianRegistFunctionsCurves]. In that context functional additive models are very natural, and the simplest is the following two-level model:

[formula]

where functions [formula] are m individual functions (e.g. learning curves for m different individuals), [formula] is a mean function and [formula] describe individual differences.

The two-level model appears on its own, or as an essential building block in more complex models, which may include several hierarchical levels (as in functional ANOVA, [\citealp=RamsaySilverman:FunctionalDataAnalysis] [\citealp=KaufmanSain:BayesianFuncANOVA] [\citealp=Sain:fANOVAandRegClimateExperiments]), time shifts [\citep=Cheng:BayesianRegistFunctionsCurves] [\citep=KneipRamsay:CombiningRegistrationFitting] [\citep=TelescaInoue:BayesHierarchCurveReg], or non-Gaussian observations [\citep=Behseta:HierarchModelsVariabFunctions]. Functional PCA [\citep=RamsaySilverman:FunctionalDataAnalysis], which seeks to characterise the distribution of the difference functions di, is a closely related variant . In a Bayesian framework the two-level model can be expressed as a particular kind of Gaussian process prior [\citep=KaufmanSain:BayesianFuncANOVA]. The goal of this paper is to show that the covariance matrices that arise in the two-level model have a form that lends itself to very efficient computation. We call these matrices restricted quasi-Kronecker, after [\citet=HeersinkFurrer:MoorePenroseInversesQuasiKronecker].

The paper is structured as follows. We first give some background material on Gaussian processes and latent Gaussian models [\citep=Rue:INLA], and describe the particular covariance matrices that obtain in functional additive models. These matrices are quasi-Kronecker (QK) or restricted quasi-Kronecker (rQK), and in the next section we prove some theoretical results that follow from the block-rotated form of rQK matrices. In particular, rQK matrices have very efficient factorisations, and we detail a range of useful computations based on these factorisations. In the following section we apply our results to Gaussian data (joint smoothing), and show that marginal likelihoods and their derivatives are highly tractable. The other application we highlight concerns the modelling of spike trains, which leads to a latent Gaussian model with Poisson likelihood. We show that the Hessian matrices of the log-posterior in the two-level model are quasi-Kronecker, which makes the Laplace approximation and its derivative tractable. This leads to efficient approximate inference methods for large-scale functional additive models.

Notation

The Kronecker product of [formula] and [formula] is denoted [formula], the all-one vector of length n [formula] or [formula] if obvious from the context. Throughout m is used for the number of individual functions in the functional additive model, and n for the number of grid points.

In what follows we will use the following two properties of the Kronecker product and vec operators [\citep=PetersenPedersen:MatrixCookbook]:

[formula]

for compatible matrices, and

[formula]

where the [formula] operator stacks the columns of [formula] vertically.

Gaussian process and latent Gaussian models

We give here a brief description of Gaussian processes (GPs), a much more detailed treatment is available in [\citet=RasmussenWilliamsGP]. A GP with mean 0 and covariance function k(x,x') is a distribution on the space of functions of some input space X into [formula], such that for every set of sampling points [formula], the sampled values [formula] follow a multivariate Gaussian distribution

[formula]

As a shorthand for the above, we will use the notation [formula] in what follows. The covariance function usually expresses the idea that we expect the values of f at xi and xj to be close if xi and xj are themselves close. Often, covariance functions only depend on the distance between two points, so that [formula], with [formula] some distance function (usually Euclidean).

In most actual cases the covariance function is not known and involves two or more hyperparameters, which we will call [formula]. In machine learning the squared-exponential covariance function is especially popular:

[formula]

where θ1 is a (log) length-scale parameter and θ2 controls the marginal variance of the process. For reasons of numerical stability we prefer to use a Matern 5/2 covariance function, which generates rougher functions but leads to covariance matrices that are better behaved. The Matern 5/2 covariance function has the following form:

[formula]

given in [\citet=RasmussenWilliamsGP], page 85. The parameters θ1 and θ2 play similar roles in this formulation, controlling length-scale and marginal variance.

Gaussian processes can be used to formulate priors for Latent Gaussian Models [\citep=Rue:INLA]. The term "Latent Gaussian Model" describes a very large class of statistical models, one that encompasses for example all Generalised Linear Models (with Gaussian priors on the coefficients), Generalised Mixed Models, and Generalised Additive Models. The two main components are (a) a latent Gaussian field, [formula], with prior [formula] and (b) independently distributed data yi  ~  f(y|zi), which depend on the corresponding value of the latent field. In regression models f(y|zi) is Gaussian, but other common distributions include a logit model (for binary y) and exponential-Poisson (as in section [\ref=sub:Spike-train-data]). The results we give here apply to all Latent Gaussian Models where the latent field has the structure of a two-level functional additive model, which we detail next.

Quasi-Kronecker structures in functional additive models

In a Gaussian process context the two-level functional additive model translates into the following model:

[formula]

where individual gi's are conditionally independent given f (they are however marginally dependent).

Fig. [\ref=fig:illus-functional-additive-model] gives an example of a draw from such a model. Eq. [\ref=eq:two-level-model-GP] expresses the model as a prior over functions, but if we have a set of sampling locations [formula] the Gaussian process assumption implies a multivariate Gaussian model for the latent field at the sample locations. We assume throughout that there are n  ×  m observations, corresponding to m realisations of a latent process observed on a grid of size n, with grid points [formula]. The grid may be irregular (i.e. xt + 1 - xt needs not equal a constant), but we need the grid to be constant over individual functions, since otherwise the covariance matrix does not have the requisite structure. Denote by [formula] the vector of sampled values from gi(x): [formula]. The latent field [formula] is formed of the concatenation of [formula], and we may think of it either as a n  ×  m matrix [formula] or a single vector [formula]. Following the LGM framework, we assume that the data are (possibly non-linear, non-Gaussian) observations based on the latent values, so that yij  ~  f(y|gij).

Under a particular grid, sampled values from the mean function [formula] have distribution [formula], and [formula] has conditional distribution [formula]. Draws from the latent field [formula] can therefore be obtained by the following transformation:

[formula]

where [formula] and [formula]. We can use eq. [\ref=eq:generating-latent-field] to find the marginal distribution of [formula] (unconditional on [formula]). Using standard properties of Gaussian random variables we find:

[formula]

[formula] is a dense matrix, whose dimensions (nm  ×  nm) would seem to preclude direct inference, since the costs of factorising such a matrix would be too high with numbers as low as 100 grid points and 20 observed functions. [\citet=HeersinkFurrer:MoorePenroseInversesQuasiKronecker] have shown that the matrix inverse of [formula] is actually surprising tractable, and in the following section we summarise and extend their results.

Some properties of restricted quasi-Kronecker matrices

Quasi-Kronecker (QK) matrices are introduced in [\citet=HeersinkFurrer:MoorePenroseInversesQuasiKronecker] and have the following form:

[formula]

We focus mostly on the restricted case (rQK), i.e. matrices of the form

[formula]

which arise in the functional additive model described just above (eq. [\ref=eq:marginal-g]).

QK and especially rQK matrices have a number of properties that make them much more tractable than dense, unstructured matrices of the same size.

The general (unrestricted) case

The cost of matrix-vector products with QK matrices is [formula], as compared to [formula] in the general case. This follows directly from the definition (eq. [\ref=eq:definition-QK]), as we need only perform [formula] operations of complexity [formula].

[\citet=HeersinkFurrer:MoorePenroseInversesQuasiKronecker] showed that the inverse and pseudo-inverse of QK matrices is also tractable. For the inverse the following formula applies:

[formula]

where [formula]. The result can be proved using the Sherman-Woodbury-Morrison formula, by writing [formula], which follows from eq. ([\ref=eq:kron-prod]). It implies that a mn  ×  mn QK matrix can be inverted in [formula] operations, as opposed to [formula] in the general case. A similar formula holds for the determinant:

[formula]

where [formula] is as in equation [\ref=eq:QK-inverse]. The result implies that determinants of QK matrices can be computed in [formula] operations ([formula] in the general case). We show in section [\ref=sub:Spike-train-data] that it can be used to speed up Laplace approximations in latent Gaussian models [\citep=Rue:INLA].

The restricted case

rQK matrices have some additional properties not enjoyed by general QK matrices. For example, the product of two rQK matrices is another rQK matrix:

[formula]

We use this property below to compute the gradient of the marginal likelihood in Gaussian models (section [\ref=sub:gaussian-marginal-likelihood]). In addition, several other properties follow from the block-rotated form of rQK matrices: for example, the inverse of a rQK matrix is another rQK matrix, and the eigenvalue decomposition has a very structure. We prove these results next.

Block-rotated form of rQK matrices

In this section we first establish that rQK matrices can be block-rotated into a block-diagonal form. From this result the eigendecomposition follows immediately, and so does a Cholesky-based square root. These decompositions need not be formed explicitly, and we will see that their cost scales as [formula] in storage and [formula] in time. The latter is linear in m, compared to cubic for na�ve algorithms.

We begin by defining block rotations, which are straightforward extensions of block permutations. A block permutation can be written using the Kronecker product [formula] of a m  ×  m permutation matrix [formula] and the n  ×  n identity matrix. Applied to a matrix [formula], [formula] will permute blocks of size n  ×  n. Block rotations are defined in a similar way, through the Kronecker product [formula] of a m  ×  m rotation matrix [formula] and the n  ×  n identity matrix. A block rotation matrix is an orthogonal matrix, since:

[formula]

where the second line follows from eq. ([\ref=eq:kron-prod]).

Our main result is the following:

There exists an orthogonal matrix [formula] such that for all n  ×  n matrices [formula], the matrix [formula] can be expressed as:

[formula]

Since [formula] is an orthogonal matrix, and the inner matrix is block-diagonal, the inverse, eigendecomposition, and a Cholesky-based square root of [formula] all follow easily.

We use the following ansatz: the m  ×  m matrix [formula] is an orthogonal matrix whose first row is a scaled version of the all-ones vector

[formula]

and [formula] is chosen such that [formula], so that the rows of L will be orthogonal to [formula], and [formula]. Note that L is not unique, but a matrix that verifies the condition can always be found by the Gram-Schmidt process [\citep=GolubVanLoan:MatrixComputations] (although we will see below that a better option is available).

As noted above [formula]. We left-multiply by [formula] and right-multiply by [formula]:

[formula]

Left-multiplying by [formula] and right-multiplying by [formula] completes the proof.

Inverse, factorised form, and eigenvalue decomposition

A number of interesting properties follow directly from the block-rotated form given by Theorem 1.

The inverse of an rQK matrix is also an rQK matrix. Specifically,

[formula]

This result can also be derived from the Furrer-Heersink formula (eq. [\ref=eq:QK-inverse]) but follows naturally from Theorem 1. Assuming that [formula] and [formula] have full rank, then the inverse of [formula] exists and equals:

[formula]

For [formula] to be an rQK matrix we need to find matrices [formula] and [formula] such that [formula], and [formula]. This implies

[formula]

which requires the existence of [formula] and [formula], both conditions being verified if [formula] is invertible.

An important consequence of this result is that Hessian matrices in latent Gaussian models with prior covariance [formula] turn out to be quasi-Kronecker (section [\ref=sub:Spike-train-data]), so that Laplace approximations can be computed in [formula] time.

The eigenvalue decomposition of [formula] is given by

[formula]

where [formula] is as in theorem 1, [formula] and [formula].

Since [formula] is block-diagonal its eigendecomposition into [formula] is straightforward. The matrix [formula] is orthogonal: [formula], and since [formula] with diagonal [formula] we have its eigenvalue decomposition.

A set of square root factorisations of [formula] is given by [formula], with

[formula]

where [formula] and [formula].

This corollary is an entirely straightforward consequence of the theorem, but note that [formula] has special form, as the product of a block-diagonal matrix and a block-permutation matrix. In the next section we outline an algorithm that takes advantage of these properties to compute the factorisation with initial cost [formula] and further cost [formula] per MVP.

A fast algorithm for square roots of [formula]

Our fast algorithm is based on Corollary 4. Given a matrix square root [formula], the typical operations used in statistical computation are the following:

Correlating transform: given [formula] the vector [formula] is distributed according to [formula]. The correlating transform (MVP with [formula]) takes a IID vector and gives it the right correlation structure.

"Whitening" transform: given [formula] the vector [formula] is distributed according to [formula]. This is the inverse operation of the correlating transform and produces white noise from a correlated vector.

Evaluation of [formula] . This can be done using the whitening transform but in addition the log-determinant of [formula] is needed. We give a formula below.

Our algorithm is based on the square roots [formula] and [formula], which can be obtained from the Cholesky or eigendecompositions. In most cases the Cholesky version is faster but the eigendecomposition has advantages in certain contexts. We outline a generic algorithm here, the only practical difference being in solving the linear systems [formula] and [formula], which should be done via forward or back substitution if [formula] and [formula] are Cholesky factors [\citep=GolubVanLoan:MatrixComputations].

Computing the correlating and whitening transforms

Corrolary 4 tells us that the square root of [formula], [formula] equals:

[formula]

We first show how to compute the correlating transform. Let [formula]. Using eq. [\ref=eq:vec-kron], we can rewrite [formula] as:

[formula]

Computing the correlating transform entails m MVP products with the square roots[formula] ([formula] in the general case) and m MVP products with [formula]. The latter m products can be done in [formula] time, as we show below, meaning that the whole operation has total cost [formula].

The whitening transform can be computed in a similar way:

[formula]

so that for [formula]

[formula]

and since matrix solves of the form [formula] and [formula] have [formula] cost for Cholesky or eigenfactors, the entire operation has the same cost as the correlating transform ([formula]).

Fast rotations

The whitening and correlating transforms above involve MVPs with a m  ×  m orthogonal matrix [formula], where [formula] is such that [formula] is orthonormal. Construction of [formula] via the Gram-Schmidt process would have initial cost [formula] and MVPs with [formula] [formula]. There is however a particular choice for [formula] that brings these costs down to [formula] and [formula]:

The matrix [formula], with [formula] and [formula] is an orthonormal basis for the set of zero-mean vectors [formula].

The proof can be found in Appendix [\ref=sub:An-orthogonal-basis-zero-mean]. With this choice of [formula] the matrix [formula] equals

[formula]

and it is easy to check that [formula], [formula], and that in addition [formula], so that [formula]. Due to the specific structure, a single MVP with [formula] has cost [formula] (which is the cost of multiplying the entries by a, by b and computing the sum).

Computing Gaussian densities and the determinant

In applications we will need to compute Gaussian densities of the following form: given [formula]

[formula]

where [formula] is obtained via whitening. In addition, the log-determinant of [formula] is needed. Similar matrices have the same determinant, so that [formula]. Further, for block-diagonal matrices, [formula], which together with Theorem 1 implies that

[formula]

These two log-determinants can be computed at [formula] cost from the Cholesky or eigendecompositions of [formula] and [formula], which are needed anyway for the computation of the square root of [formula].

Summary

The cost of computing matrix square roots for quasi-Kronecker matrices is dominated by the initial cost of computing two [formula] decompositions. The resulting factors have [formula] storage cost and no further storage is required. Further operations (whitening, correlating, computing the determinant) come at much lower cost ([formula] for whitening and correlating, [formula] for the determinant).

Applications

We first describe how to use our techniques in the linear-Gaussian setting. Under the assumption of Gaussian observations the marginal likelihood of the hyperparameters can be computed efficiently, and we show in addition how to compute the gradient of the marginal likelihood in [formula] cost. The resulting algorithm scales very well with m.

When the observations are not Gaussian, the marginal likelihood is unavailable in closed form. There are many ways to implement inference in this case, including variational inference [\citep=OpperArchambeau:VariationalGaussianApproxRev], nested Laplace approximations [\citep=Rue:INLA], expectation propagation [\citep=Minka:EP], various form of Markov Chain Monte Carlo [\citep=Neal:MonteCarloImplGaussProcModelsBayesRegClass] [\citep=RueHeld:GMRFTheoryApplications] [\citep=Murray:EllSliceSampling], or some combination of methods. We study in detail an example from neuroscience, smoothing of repeated spike train data, where the observations are Poisson distributed. We use a simple Laplace approximation, but our methods can be applied in a similar way to more sophisticated approximate inference or for MCMC sampling.

As stated in the introduction, we assume throughout that there are n  ×  m observations, corresponding to m realisations of a latent process observed on a grid of size n, with grid points [formula]. The grid may be irregular but it is essential that it be constant across realisations, otherwise the covariance matrix does not have the requisite form. The observations may be represented as a n  ×  m matrix [formula], or equivalently as a vector [formula].

Gaussian observations

In the Gaussian case the conditional distribution of the observations is assumed to be iid, with [formula]. To simplify the notation we will sometimes absorb the noise variance σ2 into the hyperparameters [formula].

Depending on the scenario the goal of the analysis varies, and we might seek to estimate the latent functions [formula], the latent mean function [formula], or to make predictions for an unobserved function [formula], etc. How we treat the hyperparameters may vary as well. In a "mixed modelling" spirit we might just want to compute their maximum likelihood estimate, or instead we may want to perform a full Bayesian analysis and produce samples from the posterior over hyperparameters, [formula].

In all of these cases the main quantities of interest are:

The (log-) marginal likelihood [formula] and optionally its derivatives with respect to the hyperparameters

The conditional regressions [formula] and [formula]

Most of the other quantities are simply variants of the above and we will not go explicitly through all the calculations.

Computing the marginal likelihood and its derivatives

The marginal likelihood can be computed easily by noting that [formula], with [formula], so that the marginal distribution of [formula] is

[formula]

and that if [formula] has rQK structure, so does [formula]. Specifically, [formula], and we may use formula ([\ref=eq:Gaussian-density]) directly to compute [formula].

For maximum likelihood estimation of the hyperparameters (and for Hamiltonian Monte Carlo) it is useful to compute the gradient of [formula] with respect to the hyperparameters. To simplify notation we momentarily absorb the noise variance σ2 into the hyperparameters [formula] and note [formula] the covariance matrix of [formula]

An expression for the derivative with respect to a single hyperparameter θj is given by [\citet=RasmussenWilliamsGP] (eq. 5.9):

[formula]

It is straightforward to verify that if [formula] is rQK, so is its derivative [formula], which implies that [formula] is also a rQK matrix (see section [\ref=sub:Block-rotated-form]). Using that property along with theorem 1, some further algebra shows:

[formula]

Computing [formula] can be done using the fast matrix square root, and [formula] is a rQK matrix, so that the dot product [formula] in eq. [\ref=eq:grad-marginal-lik] is tractable as well (with complexity [formula]). Given that factorisations of [formula] and [formula] are needed to compute the marginal likelihood anyway, the extra cost in computing derivatives is relatively small (the factorisations have cost [formula], the rest is [formula]).

Conditional regressions

The conditional regressions are given by the posterior distributions [formula] and [formula]. Both distributions are Gaussian, and their mean and covariance can be found through the usual Gaussian conditioning formulas:

[formula]

which involves [formula] computations, and:

[formula]

where [formula] is rQK and hence all computations are also [formula]. Simultaneous confidence bands can be obtained from the diagonal elements of [formula] and [formula].

Fast updates

"Fast" hyperparameter updates are often available in Gaussian process models [\citep=Neal:MCMCEnsembleOfStates], in the sense that one may quickly recompute the marginal likelihood [formula] from the current value [formula]. Usually a fast update means being able to skip a [formula] factorisation, and this may be done for rQK matrices as well. For example, the matrices [formula], [formula] and [formula] (with [formula]) all have the same eigenvectors, which means one can update certain hyperparameters without the need to recompute the square root of [formula] from scratch. Although this strategy may result in speed-ups, it requires painstaking implementation and we have not explored it further (we note however that fast updates are also linked to fast gradient computations, since eq. [\ref=eq:grad-marginal-lik] often simplifies). It may be worthwhile when used in combination with Neal's [\citeyearpar=Neal:MCMCEnsembleOfStates] strategy of separating fast and slow variables.

Results

Specialised vs. generic factorisation methods

We first checked that implementing our specialised matrix factorisations is indeed worth the trouble (for reasonable problem sizes). Generic matrix factorisation techniques have been greatly optimised over the years and are often faster than an inefficient implementation of a specialised routine.

We implemented our matrix factorisation formulae in R and measured computing times for Gaussian densities [formula]. The non-specialised routine needs to first form the matrix [formula] explicitly, and uses Cholesky factorisation to evaluate the density. Our specialised implementation comes under two variants, one based on Cholesky factors, and the other based on eigendecompositions, as explained in section [\ref=sub:A-fast-algorithm-square-roots]. The algorithms were implemented in R and all benchmarks were executed on a standard desktop PC running R 3.0.1 under Ubuntu.

Figure [\ref=fig:Relative-efficiency] and [\ref=fig:Relative-eff-loglog] summarise the results. We varied m, the number of functions, for different values of n, the grid size. General-purpose factorisation scales with [formula], compared to our method, which scales with [formula]. What we expect, and what Fig. X verifies, is that our method should scale much better with m, as indeed it does: it is faster for all but the smallest problem sizes (e.g. with n = 10 and m = 4 our method is not worth the bother). With n = 100 we do better even with m as low as 2. What the results also underscore is the relative inefficiency of eigenvalue decompositions, relative to Cholesky factorisations. The eigenvalue decomposition remains of interest for regular grids, since in this case the Fourier decomposition can be used to factorise the covariance matrix in [formula] operations [\citep=Paciorek:BayesSmoothGPUsingFourierBasisFunctions], or if iterative methods are used in order to get an approximate factorisation [\citep=Saad:IterMethodsSparseLinearSystems].

Illustration in a joint smoothing problem

We illustrate the application of our method in a joint smoothing problem. We generated data according to the following model:

[formula]

meaning that the individual functions were smooth perturbations around a fixed mean function f(x), as shown on Fig. [\ref=fig:Gaussian-data].

We use a regular grid of points xi∈(0,1).

We fitted a generic additive Gaussian process model, specifically:

[formula]

The covariance functions κK and κA are Matern covariance functions, with fixed smoothness parameter [formula]. The hyperparameters to estimate are the two length-scale parameters, the two Matern variance parameters, and the noise variance log σ2. We note [formula] the vector of hyperparameters. The most straightforward estimation strategy is to maximise [formula] (maximum likelihood, ML), or alternatively [formula] (maximum a posteriori, MAP). MCMC can naturally also be used to sample from [formula], and we compared both methods.

For MAP/ML we found that a quasi-Newton method (limited memory BFGS, [\citealp=LiuNocedal:OnTheLimMemBFGSLargeScaleOpt]) works very well (we used the fast analytical gradient described above), converging typically in ~ 30 iterations. The standard Gaussian approximation of [formula] at the mode [formula] can be computed by finite differences using the analytical gradient. For MCMC we used standard Metropolis-Hastings, with a proposal distribution corresponding to the Gaussian approximation of the posterior. When the data are informative enough the posterior is well-behaved and no further tuning is necessary, but problems can arise if e.g. the noise is very high and there are few measurements.

MAP inference is extremely fast (see fig. [\ref=fig:Runtime-of-MAP]), and remains tractable with extremely large datasets: with a grid size of n = 1,000 and m = 100 latent functions, MAP inference takes a little over two minutes on our machine. MCMC is an order of magnitude slower but still rather tractable (it is likely that large speed-ups could be obtained using a modern Hamiltonian Monte Carlo method). Below we compare the results of MCMC and MAP inference for the data shown on fig. X.

Given a MAP estimate of the hyperparameters, inference for f and the gi's proceeds using the conditional posteriors [formula] and [formula]. This tends to underestimate the uncertainty in the latent processes, but the underestimation is not dramatic in this example. The simultaneous confidence bands for MAP and full MCMC inference are compared on fig. [\ref=fig:Fitted-mean-function] (appendix [\ref=sub:Computing-simultaneous-confidence-bands] explains how the confidence bands were estimated).

Non-Gaussian observations: application to spike count data

In this section we show how to apply our technique to LGMs with non-Gaussian likelihoods, using an example where the data are Poisson counts. The Poisson LGM is one of the most popular kinds, especially in spatial statistics applications [\citep=Illian:StatAnalysisSpatPointPatterns] [\citep=Barthelme:ModelingFixationLocationsSpatialPointProc]. Here we focus on a application to neuroscience, specifically spike count data.

Neurons communicate by sending electrical signals (action potentials, also called spikes) to one another. Each spike is a discrete event, and the occurrence of spikes may be recorded by placing an electrode in the vicinity of a cell. Neurons respond to stimuli by modulating how much they spike. An excited neuron increases its spike rate, an inhibited neuron decreases it. In experiments where spikes are recorded a typical task is to determine how the spike rate changes over time in response to a stimulus.

We use data provided by [\citet=PouzatChaffiol:AutomaticSpikeTrainAnalysis], which consist in external recordings of a cell in the antennal lobe of a locust, an area that responds to olfactory stimulation. Part of the data is shown on Fig. [\ref=fig:Spike-train-data]. The animal was stimulated by an odorant (terpineol), causing a change in the spike rate of the cell. Stimulation was repeated over the course of 20 successive trials. We look at spiking activity occurring between -2 and +2 sec. relative to stimulus onset.

The data are spike counts: we simply count the number of spikes that occurred in a given time bin. Neurons are noisy , and statistical models for spike trains are generally variations on the Poisson model , which assumes that the spike count at time t follows a Poisson distribution with rate [formula] [\citep=Paninski:StatModelsNeuralEncodingDecoding]. The goal is to infer the underlying rate [formula], which may vary spontaneously, drift from one trial to the next, and change in response to the stimulus.

We set up the following hierarchical model:

[formula]

where δ is the time bin and [formula] is the expected spike count in the j-th bin on the i-th trial.

Computing the Laplace approximation

Contrary to the Gaussian case, for generic LGMs the posterior marginals over the hyperparameters ([formula]) cannot be easily computed. The Laplace approximation usually gives sensible results [\citep=Rue:INLA], although Expectation Propagation provides a superior if more expensive alternative [\citep=NickishRasmussen:ApproxGaussianProcClass].

The Laplace approximation is given by:

[formula]

where

[formula]

is the unnormalised log-posterior evaluated at its conditional mode [formula] , and [formula] is the Hessian of [formula] with respect to x evaluated at [formula].

We therefore need to (a) find the conditional mode [formula] for a given value of [formula] and (b) compute the log-determinant of the Hessian at the mode. The latter is possible because the Hessian turns out to be a QK matrix, and therefore its determinant can be computed in [formula] using equation [\ref=eq:determinant-QK].

In LGMs the Hessian matrix of the log-posterior over the latent field is a quasi-Kronecker matrix.

The second derivative of [formula] (eq. [\ref=eq:logpost]) with respect to [formula] is given by:

[formula]

where the Hessian of the log-likelihood [formula] is diagonal (since each yi depends only on xi) and Σ- 1 is rQK by Corrolary 2. The sum of an rQK and a diagonal matrix is a QK matrix.

However, before we can do anything with the Hessian at the mode, we need to find the mode. Proposition 6 implies that one may use Newton's method [\citep=NocedalWright:NumericalOptim] to do so, since [formula] can be computed using equation [\ref=eq:QK-inverse]. However, at each Newton step we need to solve m linear systems of size n  ×  n, making it relatively expensive. We found that quasi-Newton methods [\citep=NocedalWright:NumericalOptim], which do not use the exact Hessian, may be more efficient provided that one chooses a smart parametrisation.

Contrary to Newton's method, which is invariant to linear transformations of the parameters, the performance of quasi-Newton methods depends partly on the conditioning of the Hessian. It is therefore worthwhile finding a transformation [formula] so that [formula] is well-scaled. One option is to use the whitened parametrisation, i.e. [formula] (eq. [\ref=eq:whitening-transform]), in which case

[formula]

Since one of the problems with Gaussian process priors is that [formula] can have very poor conditioning, we might hope that the above transformation would help. We found that it does, but what is generally even more effective is to take into account the diagonal values of [formula] as well (preconditioning the problem). Further discussion of the issue can be found in Appendix [\ref=sub:Preconditioning-for-quasi-Newton]. The Quasi-Newton method we use is limited memory BFGS in the standard R implementation (optim).

Maximising the Laplace approximation

Once we have a way of approximating [formula], similar strategies apply in the general LGM case as do in the linear-Gaussian case. We may just require an approximate MAP estimate, or we may wish to approximately integrate out the uncertainty in [formula] using INLA [\citep=Rue:INLA] or exact MCMC in a pseudo-marginal sampler [\citep=FilliponeGirolami:ExactApproxBayesInferenceGP]. In any case the first step is to find the maximum of the Laplace approximation [formula]. Most optimisation methods will work, but it helps if the gradient [formula] can be computed. It turns out that this is possible albeit rather more expensive than in the Gaussian case, and the derivation is given in Appendix [\ref=sub:Derivative-of-the-Laplace-approx].

Results

We first used the same point process models as in section [\ref=sub:Gaussian-observations] (Matern 5/2 for both [formula] and [formula], with four hyperparameters). We used time bins of 2ms, giving a total of 200 time bins per spike train and 4,000 latent variables in total. We implemented the complete algorithm in R, and maximising the Laplace approximation takes a very reasonable 40 sec. The fitted intensity functions [formula] are shown in fig. [\ref=fig:Fitted-intensity-functions-stationary]. There is a sharp increase in rate following stimulus onset, but little variability across trials. Stationary Gaussian processes such as the Matern process are not very well adapted to the estimation of functions that jump about a lot, and we feared that the ripples visible in the fits (e.g. prior to stimulus onset) could be artefactual. In other words, in order not to oversmooth around the jump, the model undersmooths in other regions.

As an alternative, we formulated a model that allows nonstationarity in the mean function f(t), assuming:

[formula]

where a(t) and b(t) are two independent Matern processes, and m(t) is a mask that limits the effect of b(t) to the time around stimulus onset. The net effect is that extra variability is allowed around the time of the jump (see [\citealp=Bornn:ModelNonstatProcessesDimensionExpansion] for a more sophisticated approach to nonstationarity). We set t0 = 0.3 and st = 0.2 by eye. Adding hyperparameters for b(t) brings the total number of hyperparameters to 6, and fitting the nonstationary model takes about 2 minutes on our machine. The results, shown on fig. [\ref=fig:Fits-nonstationary], indicate that pre-onset ripples are indeed most likely artefactual.

Conclusion

We have shown how restricted quasi-Kronecker matrices can be block rotated and factorised, and how this enables efficient inference in the two-level functional additive model. A similar closed-form factorisation for generic quasi-Kronecker matrices eludes us despite repeated attempts. That would certainly make an interesting topic for future work although perhaps not our own.

Although the algorithms we describe here scale very well with the number of functions in the dataset, they do not scale very well with grid size - the dreaded [formula] remains. For regular grids circulant embeddings (Fourier) approaches can be used [\citep=Paciorek:BayesSmoothGPUsingFourierBasisFunctions], although unfortunately the Laplace approximation becomes hard to compute in that setting. Iterative solvers [\citep=Saad:IterMethodsSparseLinearSystems] [\citep=Stein:StochApproxScoreFunctionsGPs] could overcome the problem, but we leave that for future research.

Quasi-Kronecker and restricted quasi-Kronecker matrices display a number of appealing properties, and should probably join their block-diagonal, circulant, and Toeplitz peers among the set of computation-friendly positive definite matrices.

Appendix

An orthogonal basis for zero-mean vectors

We prove below proposition 5, which we restate:

For [formula] to be an orthogonal basis for A, all that is required is that [formula] for all j and that [formula]. To lighten the notation, define r = m - 1. The first condition requires

[formula]

and the second condition

[formula]

implying that

[formula]

injecting [\ref=eq:condition-ortho-basis-1] into [\ref=eq:condition-ortho-basis2], we get:

[formula]

This is a 2nd order polynomial in b, and it has real roots if:

[formula]

which is true since r = m - 1 is non-negative.

Solving the quadratic equations yields [formula] and substituting into [\ref=eq:condition-ortho-basis-1] yields [formula].

We can therefore always find values a,b such that [formula] is an orthogonal basis for the linear subset of zero-mean vectors. Moreover, we can show that the columns of N have unit norm:

[formula]

where the last line is from [\ref=eq:condition-ortho-basis2]. The result implies that we can compute an orthonormal basis [formula] for A such that matrix-vector products with [formula] are [formula].

Derivative of the Laplace approximation

Using the implicit function theorem an analytical expression for the gradient of the Laplace approximation can be found (see [\citealp=RasmussenWilliamsGP], for a similar derivation). The Laplace approximation is given by:

[formula]

where

[formula]

is the unnormalised log-posterior evaluated at its conditional mode [formula], and [formula] is the Hessian of [formula] with respect to x evaluated at [formula]. Since [formula] is a maximum it satisfies the gradient equation:

[formula]

Assuming that [formula] is twice-differentiable and concave in [formula], we can define an implicit function [formula] such that [formula] for all [formula], with derivative

[formula]

To simplify the notation we will note [formula] the matrix [formula]. Note that the same [formula] matrix appears in the gradient of the Gaussian likelihood with respect to the hyperparameters (eq. [\ref=eq:grad-marginal-lik]). To compute the gradient of [formula], we need the derivatives of [formula] with respect to[formula]:

[formula]

where the first part is 0 since the gradient of f is 0 at [formula], and

[formula]

This is the gradient of a log-Gaussian density, and we can therefore reuse formula [\ref=eq:grad-marginal-lik] for that part.

The gradient of [formula] is also needed and is sadly more troublesome. A formula for the derivative of the log-det function is given in [\citet=PetersenPedersen:MatrixCookbook]:

[formula]

The Hessian at the mode is the sum of the Hessian of [formula] (which depends on [formula] through the implicit dependency of [formula] on [formula]), and the Hessian of [formula], which equals the inverse covariance matrix [formula]. Some additional algebra yields:

[formula]

where we have assumed that the Hessian of the log-likelihood is diagonal. Accordingly [formula] is quasi-Kronecker, and so is [formula] (since it equals the sum of a rQK matrix [formula] and a diagonal perturbation [formula]). Computing the trace in eq. ([\ref=eq:gradient-logdet]) is therefore tractable if slightly painful, and can be done by plugging in eq. [\ref=eq:QK-inverse] and summing the diagonal elements.

Computing simultaneous confidence bands

In this section we outline a simple method for obtaining a Rao-Blackwellised confidence band from posterior samples of [formula]. A simultaneous confidence band around a latent function f(x) is a vector function [formula], such that, for all x:

[formula]

In latent Gaussian models the posterior over latent functions is a mixture over conditional posteriors:

[formula]

The conditional posteriors [formula] are either Gaussian or approximated by a Gaussian. If we have obtained samples from [formula] (as in section [\ref=sub:Gaussian-observations]) we can approximate ([\ref=eq:conditional-post-appendix]) using a mixture of Gaussians, which has an analytic cumulative density function. The c.d.f. can be inverted numerically to obtain values [formula] and [formula].

Preconditioning for quasi-Newton optimisation

In the whitened parametrisation the Hessian matrix has the following form (eq [\ref=eq:making-prior-diagonal])

[formula]

Although the whitened parametrisation gives ideal conditioning for the prior term, the conditioning with respect to the likelihood term ([formula]) may be degraded. Preconditioning the problem can help tremendously: given a guess [formula], one precomputes the diagonal values of [formula] and uses [formula] as a preconditioner. To compute [formula] the following identity is useful:

[formula]

where [formula] is the element-wise product. In LGMs [formula] is diagonal and so:

[formula]

We inject [\ref=eq:whitening-transform] into the above and some rather tedious algebra shows that [formula] can be computed in the following way. Form the matrix [formula] by stacking successive subsets of length n from [formula]. Rotate using the matrix [formula] (section [\ref=sub:Fast-rotations]) to form another matrix:

[formula]

then compute [formula] from:

[formula]

where we have used Matlab notation subsets of columns of [formula], and [formula] and [formula] are as in Corrolary 4.

Acknowledgements

The author wishes to thank Reinhard Furrer for pointing him to [\citet=HeersinkFurrer:MoorePenroseInversesQuasiKronecker], and Alexandre Pouget for support.