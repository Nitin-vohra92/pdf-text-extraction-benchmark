An Achievable Rate-Distortion Region for the Multiple Descriptions Problem

Introduction

The multiple-descriptions (MD) source coding set-up describes a communications system consisting of a centralized encoder and several decoders. The encoder transmits data through a number of noiseless links. Each decoder is connected to the encoder via a subset of these links. The goal is for the encoder to compress an information source and transmit it to the decoders such that the source reconstruction at each decoder meets a specific fidelity criterion. There has been an extensive amount of effort to determine the optimal rate-distortion (RD) region for the general MD set-up, however, even in the case of two-descriptions the optimal region is not known. The best known achievable RD region for the two-descriptions set-up is due to Zhang and Berger [\cite=1]. In [\cite=1], the encoder utilizes a base layer which is decoded by all receivers and a refinement layer which is decoded by individual receivers. The VKG scheme proposed in [\cite=2] generalizes the base layer idea in [\cite=1] to cases with more than two-descriptions. The combinatorial-message-sharing (CMS) strategy [\cite=3] expands the method in [\cite=2] by considering a combinatorial number of base layer codebooks which are decoded in subsets of receivers. In [\cite=4], a random binning scheme was introduced which results in gains over previous known coding strategies. The method in [\cite=4] is only applicable to symmetric sources. Finally, in [\cite=6] the ideas in [\cite=3] and [\cite=4] were combined to form CMS with binning. It was shown that CMS with binning gives gains over previous coding strategies and strictly contains them. All of these coding schemes use random codes to construct codebooks; in this paper we propose using linear codes instead.

Using structured codes in communications problems has traditionally been of interest due to their practicality in comparison with randomly generated codes. Korner and Marton [\cite=7] observed that in some set-ups, application of structured codes may also yield gains in terms of achievable rate-distortions. Specifically they show that in a particular 3-user distributed source coding problem, involving reconstruction of a sum of two BSSs, using linear codes results in a larger achievable RD region. The phenomenon was also observed in channel coding problems. It was shown in the three user interference channel [\cite=8] and the three user broadcast channel [\cite=9], that employing linear codes results in gains. Intuitively, the main idea behind all of these linear coding schemes is that because of their structure, linear codes can compress and transmit sums of binary RVs more efficiently than random codes. Based on these observations it is expected that utilizing linear codes is also advantageous in the MD problems when more than 2 descriptions are transmitted. This turns out to be indeed the case as we illustrate in the next chapters.

The rest of the paper is organized as follows: Section II is allocated to explaining the CMS with binning scheme. In section III, we prove linear codes give gains over previous schemes in two different examples. Section IV contains a proof that the CMS with binning scheme can be improved using random codes. In section V, we provide an achievable RD region for the MD problem. Section VI concludes the paper.

CMS with binning

Here we explain the CMS with binning scheme presented in [\cite=6] for the L-descriptions problem.

Base Layer Construction: For each subset A of

[formula]

Linear Coding Examples

In this section we present two examples showing that linear codes attain points outside of previous known achievable RD regions.

A Three User Example

Figure 2 depicts the three-descriptions problem. Here X and Z are independent BSSs. Distortion is measured at individual decoders (i.e. decoders 1,2 and 3) using Hamming distortion. We choose the distortion functions for decoders 12,13 and 23 such that in a PtP setting they achieve optimal rate-distortion by receiving two independent quantizations of X and Z, where the independent quantizations are done using binary symmetric test channels with cross over probability δ∈[0,0.5]. To construct such a distortion function we use the method in [\cite=10]. Let PXZX̄ be the joint probability distribution of the source along with two quantizations X̂ = X + Nδ and [formula] where Nδ and [formula] are Be(δ) (i.e. with P(Nδ = 1) = δ) and independent of all other RVs. Then the distortion function between the source (X,Z) and the reconstruction (X̂,Ẑ) is defined as

[formula]

Here d0 is chosen such that [formula]. Also c is an arbitrary positive constant. With this distortion function, in a PtP setting if we construct a test channel using PXZX̂Ẑ, it achieves rate-distortion at D = EPXZX̂Ẑ{dXZ((X,Z),(X̂,Ẑ))} and R = I(X̄,;X,Z) = 2(1 - hb(δ)). In the above MD problem, linear codes can achieve the following rate-distortions:

[formula]

One may observe the main idea in the proof is that due to the linearity of the code, Un1 + Un2 is in the codebook, hence it can be sent on the third description with the same rate as other descriptions. Now we prove that CMS with binning cannot achieve the above rate-distortions. We do this by assuming such a rate-distortion vector is achievable and then arriving at a contradiction. The rate-distortions in theorem 1 are not achievable using the CMS with binning scheme.

A Four-Descriptions Example

So far we proved linear codes outperform previous random coding schemes in the three-descriptions problem. The gains are only presenting themselves due to the fact that linear codes can compress sums of binary RVs more efficiently, these are the same gains as the ones in other three-terminal communications problems. Now we proceed to explain our second example. The example involves a four-descriptions problem. We believe the gains in this example point out to a new phenomenon which arises when using linear codes. The set-up is depicted in figure 4. Here X and Z are BSSs which are related to each other through a BSC(δ) (i.e. X = Z + Nδ where Nδ is Be(δ) and independent of X and Z). We are interested in the operating point where decoder 1 reconstructs X with Hamming distortion δ, decoder 4 reconstructs Z with the same distortion, the rest of the reconstructions are lossless as shown in the figure. For the above distortions, linear codes achieve the following rates: Note the linearity of the codebook, along with it being a good channel code and a good source code are crucial for achieving this RD vector. Now we prove that CMS with binning does not achieve the rates and distortions in the previous theorem. CMS with binning does not achieve the RD vector in theorem 3.

Random Coding Improvements

In this section we illustrate that CMS with binning can be improved by including additional randomly generated codebooks. For example the scheme does not include a codebook which is decoded when either description 1 or both descriptions 2 and 3 are received. In the situation depicted in figure 5, the addition of such a codebook results in a larger achievable RD region. Here decoders 1, 23 and 123 have Hamming distortion constraints. The distortion constraint in decoder 2 will be defined later. If decoder 2 is omitted, the example would become equivalent to the two descriptions problem discussed in [\cite=1] by combining descriptions 2 and 3 into one description. In that paper, it was proved that the presence of a codebook decoded at all decoders would result in gains in achievable RD. Let P  =  {PX0,X1,X2,X} be the set of optimizing distributions in the Zhang-Berger RD region in [\cite=1], for a given R, D and D0. X0 is the RV relating to the common codebook in that problem. Define P = argmin(I(X;X0)), where the minimum is taken over all PX0,X1,X2,X∈P. Let X̂0 be an RV such that:

Linear Coding Achievable Region

In this section we provide an inner bound to the achievable RD region using linear codes. RD vectors satisfying the following bounds are achievable using linear codes. Let V = {V(A,k),(A,k)∈A} and U = {U(k,n),(k,n)∈K}

[formula]

Here q is the maximum of the cardinality of all RVs involved in the optimization. Also [formula], [formula], [formula], A1, A2, K1 and K2 are defined in previous sections.

Furthermore if the encoder wants to transmit the sum of two random variables Y,Z∈{UA,j,Vk,n}, the following covering bound must hold:

[formula]

If decoder [formula] is to reconstruct Y + Z, then we have three cases: Case 1: Decoder [formula] reconstructs both Y and Z. In this case, in the packing bound corresponding to this decoder, ρY is replaced with ρY + tρY + Z and ρZ is replaced with ρZ + (1 - t)ρY + Z, where t∈[0,1] and ρY + Z is the rate with which the codebook for Y + Z is binned. Case 2: The decoder only reconstructs Y (or Z), in which case reconstructing Y + Z is the same as reconstructing (Y,Z). The packing bounds are written as if Z was sent to the decoder with binning rate ρY + Z. Case 3: The decoder does not reconstruct Y or Z. In this case the packing bound is deduced by replacing UA with (Y + Z,UA). If Y + Z is taken to be trivial, the above bound reduces to the CMS with binning achievable region. q, [formula] and [formula] are eliminated after the Fourier-Motzkin elimination and do not play a role in determining the achievable region. The above rate region can be improved upon by adding the extra codebooks mentioned in the last section, and also by allowing reconstruction of multi-variate summations of the random variables.

The proof of the theorem follows from the proof of CMS with binning and simple linear coding arguments.

Conclusion

A new coding scheme for the general MD problem was proposed. It was shown that the scheme outperforms previous known random coding schemes. An example was given illustrating that previous random coding schemes can also be improved by including additional randomly generated codebooks.

Consider the set-up in figure 1, assume R1 + R2 = RDd12(D12) where d12 is the distortion function at decoder 12, RDd12 is the PtP rate-distortion function and D12 is distortion at that decoder. Also assume Ri = RDdi(Di). In this situation we have the following lemma: In CMS with binning, with the redundant refinement layer included, at the above rate-distortion vector, we must have [formula] and C12,1  =  φ. Furthermore (U2,1,U2,2,V12,2)  ↔  U1,U2  ↔  X. Let A,B,C and D be RVs such that A  ↔  B,C  ↔  D and A  ↔  B,D  ↔  C, and also assume there is no b∈B for which given B = b there are non-constant functions fb(C) and gb(D) with fb(C) = gb(D) with probability 1. Then A  ↔  B  ↔  C,D. For random variables A,B,C,D, the three short Markov chains A  ↔  B,C  ↔  D, A  ↔  B  ↔  C and B  ↔  C  ↔  D are equivalent to the long Markov chain A  ↔  B  ↔  C  ↔  D.