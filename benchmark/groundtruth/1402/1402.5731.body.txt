Information-Theoretic Bounds for Adaptive Sparse Recovery

Introduction

For many sparse recovery applications, it has been shown that adaptive methods with sequential and flexible measurement designs improve practical performance compared to nonadaptive methods. From a theoretical point of view, adaptive methods should perform at least as well as nonadaptive methods asymptotically, as the latter is a special case of the former. However, it is an interesting problem to determine whether they can perform strictly better for different recovery problems and problem conditions. While such methods have been theoretically analyzed for specific problems of interest, it is not clear at a high level what properties of sparse problems allow adaptive methods to have strictly better recovery performance compared to nonadaptive ones.

In this work we consider a high-level unifying framework and obtain a lower bound on the sample complexity of adaptive sparse recovery problems. Our framework characterizes the problems of interest with the following sparsity assumption: Let [formula] be a set of variables and Y be a corresponding observation generated by an observation model P(Y|X) which satisfies the conditional independence property

[formula]

for some set of variables [formula], where |S|  =  K. Then, the aim is to determine the salient set S given T samples of variable/observation pairs, (XT,YT). These pairs are generated sequentially and variables X can be chosen adaptively depending on past variables and observations. This framework encapsulates many sparse problems of interest, e.g. support recovery in compressive sensing (CS) [\cite=donoho], its nonlinear extensions (e.g. quantized CS [\cite=1bit]) and other nonlinear problems such as group testing [\cite=group_testing].

As the result of our analysis, we obtain a mutual information formula for the lower bound, which depends on the observation model P(Y|XS) and the distribution of X, pt(X), at each step t of the sequence. We obtain this bound using a Fano's inequality type argument, inspired by the proof of the upper bound on capacity for channel coding with feedback [\cite=coverbook]. Our result is unifying for all adaptive sparse recovery problems, similar to [\cite=arxiv] [\cite=arxiv_dep] [\cite=ssp] [\cite=strong]. These works consider the nonadaptive case where variables are generated by a distribution p(X) IID over T samples.

We then obtain adaptive lower bounds for linear and nonlinear applications. We look at the highly nonlinear problem of group testing, where we show that T  =  Ω(K log (N / K)) tests are necessary for the adaptive case. This bound can be achieved by nonadaptive methods [\cite=group_testing], implying adaptivity cannot improve performance. Similarly, we consider 1-bit CS as a nonlinear extension of linear CS. We again show T  =  Ω(K log (N / K)) is a lower bound to argue adaptivity cannot help, as it is matched by nonadaptive upper bounds [\cite=gupta] [\cite=arxiv] for sufficiently high SNR. The same phenomenon happens for linear CS with linear sparsity K  =  Θ(N). In this case we show [formula] is necessary and the lower bound T  =  Ω(N) is achieved by nonadaptive methods for this SNR [\cite=shuchin]. However for sublinear sparsity, we show there might be mild gains for T and SNR, consistent with the results of [\cite=malloy] [\cite=haupt].

There is a large body of work on both adaptive recovery methods and lower bounds, however these analyses are fragmented compared to our unifying approach as they only consider specific problems. The linear problem of adaptive CS has been especially well-studied: Lower bounds have been derived for support recovery [\cite=davenport] [\cite=castro] and adaptive algorithms have been analyzed to obtain upper bounds [\cite=malloy] [\cite=haupt]. There is relatively little work on adaptive recovery on nonlinear models. Adaptive group testing has been investigated by [\cite=aldridge] [\cite=aldridge2] where lower bounds are derived and adaptive algorithms are analyzed (see ref.s in [\cite=aldridge2]). Adaptive 1-bit CS algorithms have been proposed [\cite=1bit], however adaptive lower bounds have not been studied to the extent of our knowledge.

The generality of our analysis also allows us to look at the big picture and comment on the gains due to adaptivity and how it is related to the nature of a problem. We conjecture that adaptivity may help only if there are "sum-power"-like constraints on the variables as in linear CS. If the variables are not constrained and the difficulty of the problem only stems from the observation model, adaptivity does not increase asymptotic performance.

An Information-Theoretic Framework for Sparse Recovery

We assume that an observation Y is generated by an observation model for which we assume P(Y|X)  =  P(Y|XS), for [formula] with |S|  =  K and XS  =  {Xk}k∈S. We consider the scenario where a latent observation model parameter βS may exist, with corresponding P(βS) and P(Y|XS,βS). Finally, let ω index all sets of size K among N variables, such that [formula] and the corresponding set is [formula]. In addition to conditional independence, the only other assumption we make is that ω is chosen uniformly at random among [formula] sets.

A simple example of the type of problems we consider is the CS model [\cite=donoho], where the observations are given by Y  =  〈X,β〉  +  W for a K-sparse vector β with support S, support coefficients βS and noise W. Another example is the group testing model [\cite=group_testing], where X is a Boolean test inclusion vector that determines whether an item is included in the test or not and S is the set of defective items. The group testing model assumes that the test outcomes Y are only dependent on the inclusion of defective items, given by XS. Further examples and details can be found in [\cite=arxiv].

We observe a sequence of T variable-observation pairs (XT,YT)  =  (X(t),Y(t))t  =  1:T, where we used a : b to denote the sequence of integers [formula]. A decoder g(XT,YT) outputs an estimate [formula] of index ω and we aim to characterize the error probability Pe that [formula] to obtain conditions on T for successful recovery, in terms of K, N and other problem parameters.

We now present the bounds on sample complexity from [\cite=arxiv]. The following result is a lower bound on the number of samples for recovery, which is the nonadaptive analogue of our main result, Theorem [\ref=thm:aLB].

Let [formula] be generated IID across t  =  1:T according to p(X). Then, a lower bound (or a necessary condition) on the number of samples required for Pe to be asymptotically bounded away from zero is given by

[formula]

where S̃ is a proper subset of Sω and we define

[formula]

An upper bound on the number of samples is also presented in [\cite=arxiv]. For the upper bound, it is further assumed that the variables are generated IID across both samples t = 1:T and variables n = 1:N. The error probability of a Maximum Likelihood decoder is analyzed to obtain a sufficient condition for recovery. The lower bound given in Theorem [\ref=thm:LB] is order-wise tight as it matches the upper bound, when restricted to IID probability distributions on XT, for K not scaling with N and provided that a mild condition on the mutual information is satisfied.

A Lower Bound for Adaptive Recovery

In this section we analyze the adaptive scenario where at each t = 1:T, X(t) is given by a (possibly random) function X(t)  =  ft(X(1:t - 1),Y(1:t - 1)). We state a lower bound on the number of samples in the adaptive case that holds for any distribution of X(1) and functions ft. The bound depends on the distributions pt(X(t)) marginalized with respect to other sequence indices.

Let [formula] be generated such that each X(t) is a (random) function of X(1:t - 1) and Y(1:t - 1). Then, a lower bound on the number of samples required for Pe to be asymptotically bounded away from zero is given by

[formula]

where S̃ is a proper subset and we define

[formula]

as the average mutual information over the sequence t = 1:T. Each term in the maximization above is also a lower bound.

In the nonadaptive case, XT is generated IID across samples t = 1:T, therefore [formula] and the above bound reduces to the bound given in Theorem [\ref=thm:LB]. This also holds for the adaptive case if the variables are chosen such that [formula] is identically distributed across the sequence t  =  1:T.

In order to obtain bounds for specific applications in the following section, we use two simple methods: We upper bound [formula] directly for any p(X) in group testing and 1-bit CS, which leads to a trivial upper bound on [formula]. For linear CS, we bound [formula] individually for t = 1:T, which then gives an upper bound on [formula].

While the proof is inspired by the feedback proof of [\cite=coverbook], it is fundamentally different since we consider extra latent parameters [formula], we have the extra overlap terms [formula] and we explicitly assume variables depend not only on past outputs but also on past inputs.

Applications

In this section we discuss the implications of the adaptive lower bound for some applications. We will first present results for group testing and 1-bit CS; then we will look at the linear CS model.

Group Testing

Group testing is the problem of identifying a set of "defective" items from a larger set, where group tests can be performed which results in a positive outcome if and only if a defective item is included in the test group. Formally, we let XT denote the Boolean test inclusion matrix for N items and T tests and YT denote the binary outcomes of T tests, where each test outcome is given by the formula

[formula]

T  =  Θ(K log (N / K)) is a lower bound on the number of tests to recover the defective set S with an arbitrarily small error probability using adaptive testing.

The same lower bound was shown for nonadaptive group testing in by the authors in [\cite=group_testing] [\cite=arxiv] and for adaptive in [\cite=aldridge] [\cite=aldridge2]. Asymptotically matching upper bounds have also been shown for nonadaptive and adaptive testing, see [\cite=aldridge2]. In fact, the lower bound can be achieved by choosing entries of XT IID ~   Bernoulli(1 / K) for K  =  o(N) [\cite=group_testing]. Therefore we observe that adaptivity cannot improve performance asymptotically in this sparse recovery problem.

Note that versions of the group testing problem with noisy test outcomes can also be considered as in [\cite=group_testing] and our results for adaptive testing can be extended to these models.

1-bit Compressive Sensing

1-bit CS [\cite=gupta] is interesting as the extreme case of quantized CS models which are of practical importance in many real world applications. Mathematically, we have

[formula]

where XT is a T  ×  N sensing matrix with t-th row corresponding to X(t), β is a K-sparse N  ×  1 vector with support S and WT is an IID noise vector. Q(  ·  ) is a 1-bit quantizer which outputs 1 if the input is nonnegative and 0 otherwise, for each element in the input vector.

T  =  Θ(K log (N / K)) is a lower bound on the number of measurements to recover the support S of β using adaptive measurements with an arbitrarily small error probability.

The proof is the same as the proof for group testing since Y(t) are binary measurements. Matching upper bounds for noiseless and noisy variants of the problem (with sufficiently high SNR) have been shown by the authors in [\cite=gupta] and [\cite=arxiv] using IID Gaussian measurement matrices. Therefore we have shown that support recovery performance in 1-bit CS cannot be increased asymptotically using adaptive measurements in those SNR regimes.

Compressive Sensing

We now look at the CS problem with measurement noise. We have the normalized model [\cite=shuchin] [\cite=arxiv],

[formula]

where XT is the T  ×  N sensing matrix, β is a K-sparse vector of length N with support S and YT is the observation vector of length T. WT is an IID Gaussian noise vector with variance [formula]. We assume w.l.o.g. (since we are obtaining a lower bound) that βk∈{ - 1, + 1} with equal probability and are IID for k∈S. We constrain the total power of the entries of XT similar to [\cite=malloy] [\cite=haupt], where we assume [formula] and [formula], to be consistent with [\cite=arxiv] and [\cite=shuchin]. As a special case, row-wise power constraints can be enforced by setting [formula]. Note that this normalized model uses a different convention from the fixed noise variance models used in [\cite=malloy] [\cite=haupt] [\cite=davenport] where bounds on the minimum magnitude of support coefficients are derived instead of [formula].

In order to obtain a valid lower bound for all distributions on X, we optimize [formula] over probability distributions pt(X(t)) satisfying the power constraint, simultaneously for all S̃  ⊂  S. Let [formula] and note that

[formula]

pt(X(t)) that maximizes [formula] subject to power constraints sets [formula] and is jointly Gaussian in X(t)S, with zero mean and covariance matrix Σ(t) with diagonals [formula] and the off-diagonals equal to some scalar ρ.

We do not prove the lemma due to space constraints, however it follows from the fact that βS is IID and symmetric around zero, the maximization is over all subsets S̃  ⊂  S and since an entropy is being maximized subject to power constraints.

Then, removing the conditioning on [formula], [formula] can be upper bounded by

[formula]

due to Jensen's inequality, where [formula] is the submatrix of Σ corresponding to the indices in [formula]. Since [formula] is a circulant matrix, we can write [formula] where F is the unitary DFT matrix and [formula] is its conjugate transpose. Λ is a diagonal matrix with its first element equal to [formula] and other [formula] diagonals equal to [formula]. It is also easy to show that [formula] is also IID and has variances equal to 1. Then it follows that, independent of the value of ρ, we have

[formula]

For [formula] we can then write

[formula]

where the second inequality follows from the fact that the sum is maximized by [formula] for all t subject to the constraint [formula]. Note that this implies distributing equal power to all T rows. Therefore the same bound holds for the case where row-wise power is constrained instead of the total power of all entries of XT.

We then evaluate [\eqref=eq:aLB] with the above bound on [formula] for [formula] pairs to obtain the following theorem. Note that we have reduced the maximization over S̃  ⊂  S in [\eqref=eq:aLB] to a maximization over i  =  K  -  |S̃|.

A necessary condition on a [formula] pair for exact support recovery of β with adaptive measurements and an arbitrarily small error probability is [formula] for all [formula].

Below corollary then follows by noting that the left-hand side is increasing in T and letting T grow to infinity independent of other quantities. Note that the theorem states that the condition should hold for all i and we obtain the following bound for the case i = 1.

[formula] is a necessary condition for exact support recovery with adaptive measurements and an arbitrarily small error probability.

Our result is unique since the lower bound on [formula] and the relationship between different T and [formula] can be explicitly characterized. Our result proves that for the linear sparsity regime K  =  Θ(N), [formula] is necessary and T  =  Θ(N) is necessary for that [formula]. This shows that adaptivity does not help compared to nonadaptive since [formula] is necessary and T  =  Θ(N) is achievable in that case [\cite=shuchin].

Corollary [\ref=thm:SNR] implies that the necessary condition on [formula] for adaptive measurements is possibly more relaxed for K  =  o(N), which is also implied by the best known adaptive lower bound Ω( log K) [\cite=malloy]. While our bound is weaker in the sparser regimes, it is possible that a tighter analysis of the mutual information may lead to a comparable bound. Similarly, Theorem [\ref=thm:CS] implies that it might be possible to recover S with fewer measurements with the same noise levels for K = o(N): An example is T  =  Θ(K) and [formula], compared to T  =  Θ(K log (N / K)) for nonadaptive recovery; or T  =  Θ(K log N) for [formula], which is achievable for K = o(N) [\cite=malloy]. Therefore, in contrast to the group testing and 1-bit CS examples, in linear CS we have shown that there might be room for improvement using adaptive measurements in the sublinear sparsity regime. This result is consistent with previous lower bounds for adaptive CS [\cite=haupt] [\cite=castro].

Finally, note that performance improvements would not be possible if we constrained the power of each element of the sensing matrix individually or constrained the total power of 〈X,β〉, as in both cases it would not be possible to improve measurements by "concentrating" power on XS with the information from previous measurements.

Discussion

Considering Theorem [\ref=thm:aLB] and the applications discussed above, we can argue that sample complexity is minimized in cases where the average information in the sequence [formula] can approach the maximum value of [formula] maximized over p(X). However, in models where the only difficulty is the uncertainty in the observation model [formula] and there are no total-power restrictions on measurements X (even if there are element-wise restrictions), an optimal p(X) can be determined beforehand. This p(X) would maximize [formula] for all possible sets ω simultaneously, eliminating the need for adaptive measurements. Group testing is an example of such a problem where there are no restrictions on the Boolean testing matrix and the only uncertainty is due to the observation model, and hence we see no gains from adaptivity asymptotically.

However, for problems with a total-power constraint (such as the linear CS model), adaptivity can help by obtaining more information on ω and transferring power from less likely candidates for Sω to more likely ones, therefore increasing the "effective SNR" as the measurement sequence progresses. Since nonadaptive measurements do not have any prior information on ω, they attempt to distribute power evenly over all N candidate indices so they cannot achieve the same performance asymptotically, at least for sublinear sparsity.

1-bit CS is an interesting example since [formula] and the total power of measurement vectors are still important but we observe that for sufficiently high [formula] the binary output constrains performance more than the measurement power. Therefore nonadaptive methods can achieve the adaptive lower bound asymptotically. However, it is an open question whether adaptivity can increase asymptotic performance for lower [formula] values similar to linear CS.

Acknowledgements

This work was supported by NSF Grant 0932114 and NSF Grant CCF-1320547.