A New Mechanism for Maintaining Diversity of Pareto Archive in Multiobjective Optimization

Introduction

The field of multiobjective design optimization has evolved very fast during last years, reflecting the need of solving tasks with several conflicting criteria, which is common in practical problems. From the mathematical point of view, this corresponds to minimization/maximization of a vector-valued function, which rarely leads to a single solution. Consequently, a whole hyperplane of trade-off solutions, called Pareto-optimal set, is expected as the result instead of a single optimum.

A number of algorithms have been presented that generate a set of solutions approximating this hyperplane. The quality of the approximation is usually considered from two points of view: (i) the closeness to the exact trade-off surface and (ii) its distribution. The former is related to convergence properties of an algorithm while the latter describes its ability to maintain diversity. An ideal algorithm should produce well converged solutions perfectly distributed along the Pareto front. However, these requirements are conflicting, and many current approaches concentrates on one of them finding reasonable compromise in the other.

In this study, our attention is focused on the second aspect of diversity of the Pareto-optimal set, namely we present a new strategy for maintaining variety of members of a Pareto archive.

The problem of maintaining uniform distribution at an affordable cost has been addressed by many algorithms. It is known that the notion of crowding distance proposed by Deb et al. for algorithm NSGA-II [\cite=Deb-2001-MOO] [\cite=Deb-2000-FEN] is not sufficient to maintain diversity of the evolution for more than two objectives (e.g. [\cite=Deb-2003-FME] [\cite=Deb-2002-SMO]). On the other hand, SPEA2 by Zitzler et al. [\cite=Zitzler-2002-EMD] is usually able to produce well spread solutions even for three or more objectives. The concept of archiving promising design vectors was first introduced for SPEA by Zitzler and Thiele [\cite=Zitzler-1999-MEA]. Knowles and Corne presented the Pareto Archived Evolutionary Strategy (PAES) [\cite=Knowles-1999-PAE] and proposed the adaptive grid algorithm ([\cite=Knowles-2000-ANF]) to maintain diversity. However, it is difficult to keep the efficiency of this approach in cases with more than three objectives.

The new mechanism presented in this paper was implemented in micro-genetic algorithm μARMOGA proposed by Szöllös et al. [\cite=Szollos-2008-AOM], and results for three standard three-objective benchmark problems are presented.

Our second aim is to investigate the effect of population size for small (sometimes called micro) populations on the performance of μARMOGA. It was reported by Krishnakumar [\cite=Krishnakumar-1989-MGA] for single-objective optimization and by Coello and Pulido [\cite=Coello-2001-MGA] for multiple objectives, that very small populations can lead to fast convergence to the Pareto front. In this context, most experiments were performed using populations of 4, 10 and 20 individuals.

Results got by μARMOGA equipped with the new archiving mechanism are compared with those obtained by two leading methods in the field, namely NSGA-II by Deb et al. [\cite=Deb-2000-FEN] and SPEA2 by Zitzler et al. [\cite=Zitzler-2002-EMD], and a recent interesting algorithm IBEA by Zitzler and Künzli [\cite=Zitzler-2004-IBS]. All these are implemented in the Platform and Programming Language Independent Interface for Search Algorithms (PISA) [\cite=Bleuler-2003-PIS]. PISA is an interesting open source package developed by the team of Prof. E. Zitzler at ETH Zürich. The software implements various selection, crossover, and mutation operators and objective function evaluations. An important idea of the project is to separate the selection of promising candidates from objective function evaluation, crossover and mutation and implement these in two separate programs, interchanging information via formatted files. These programs are called selectors and variators in the PISA context. There is an increasing number of ready-to-use variators and selectors that can be downloaded from the web page of the PISA project. Therefore, the system offers a simple way to produce fair comparisons of various selection schemes with the same variator. While the described scheme of splitting an evolutionary algorithm into two separate programs is very useful for some techniques, in our opinion, it does not fit to algorithms with strong coupling between both stages via the use of an archiving procedure. That is the reason why our implementation of μARMOGA was used, instead of integrating the proposed archiving technique into the PISA framework.

Three metrics, measuring both convergence to the exact front and diversity of the approximate set, are used for the comparison. It is observed, that the new algorithm produces very good distribution of individuals outperforming in this respect the other algorithms in many cases. The archiving strategy does not seem to affect its convergence. Moreover, diversity is maintained in an affordable way as suggested by presented numerical experiments.

The rest of the paper is organised as follows. In Section [\ref=sec:armoga], μARMOGA is recalled with an emphasis on its main aspects. Section [\ref=sec:archive] contains the main contribution, which is the proposition of a new archiving mechanism. Tests and comparisons with the other evolutionary techniques can be found in Section [\ref=sec:comparison], where we describe the test problems ([\ref=sec:problems]), metrics used for evaluating the performance ([\ref=sec:metrics]), detailed setting of particular algorithms ([\ref=sec:setting]), and organization of the experiments ([\ref=sec:experiments]), respectively. Our findings are discussed in detail in Section [\ref=sec:discussion], while Section [\ref=sec:conclusion] contains summary of the work and concluding remarks.

μARMOGA - Multiobjective micro-genetic algorithm with range adaptation: A Brief Introduction

To minimize the costly evaluation of individuals, it is straightforward to see that one way to go is to minimize their number. It is well known for evolutionary practitioners, that using smaller populations and applying the evolutionary operators many times is often more favourable than vice versa (e.g. [\cite=Oyama-2000-WDU]). This idea can be brought to an extreme by using a micro-population (e.g. 4, 5, 10 individuals), what we really did when we utilized some ideas of Krishnakumar [\cite=Krishnakumar-1989-MGA] and of Coello and Pulido [\cite=Coello-2001-MGA]. Krishnakumar came with the concept of micro-genetic approach first, and used it for single-objective optimization. His algorithm contained only selection and crossover operators, and no mutation operator. Instead, the author introduced a reinitialization technique, which was invoked once in a few generations to ensure diversity for the evolution. The latter two researchers proposed a micro-genetic algorithm enabling to tackle multi-objective problems. Their concept was similar to that of Krishnakumar, i.e. it contained selection, crossover, and reinitialization operators supplemented by a mutation operator. Both algorithms were verified on various test problems. In both the cases, the micro-genetic variants converged to the optimum (Pareto-front) much faster than their macrogenetic counterparts used for comparison.

In the approach by Szöllös [\cite=Szollos-2008-AOM], microgenetic algorithm is supplemented by range adaptation and "knowledge-based" reinitializion procedure exploiting the Pareto-archive to generate better individuals.

The concept of range adaptation was originally introduced by Arakawa and Hagiwara [\cite=Arakawa-1998-DAR], who used it with binary coding of the design variables. Its essence lies in ability to promote the evolution towards promising regions of the design space via sophisticated manipulation with the population statistics. Due to the coding, it contained some artificial parameters which were hard to guess in general. Oyama [\cite=Oyama-2000-WDU] used range adaptation in real domain and was successful in avoiding this drawback via encoding a design variable to a real number ri∈(0,1) defined by integration of the Gaussian distribution N(0,1)

[formula]

where [formula] is linked to the original design variable pi by

[formula]

We are using this encoding scheme too, with one important difference: Oyama originally calculated the average μi and the standard deviation σi by sampling the upper half of the population, which is justified as long as macro-populations are used (e.g. with more than fifty individuals). But such approach would be too restrictive in the case of microevolution, since the upper half of the micropopulation contains too little information to keep the diversity. Consequently, the evolution quickly ends up in premature convergence. Thus, we calculate both by taking into account the whole population.

"Knowledge based" reinitialization resulted from an attempt to use the members of the Pareto-archive to get new members, superseding the old ones by putting several of them into the reinitialized population. Moreover, only a subset of the archive is considered. For instance, two archive members with extreme values of two different objectives chosen randomly are usually exploited. In this way, it is possible to further improve the whole archive by improving its subsets. The functioning of μARMOGA can be seen in Figure [\ref=fig:miarmoga_scheme]. After initialization of the population by Latin hypercube sampling (LHS) and evaluation depicted as archive update, the evolution goes through selection, mating and mutation to evaluation of the new population. Each n-th generation the population statistics is updated, range-adaptation takes place, followed by knowledge based (elitist-random) reinitialization. A thorough description of the algorithm is to be found in [\cite=Szollos-2008-AOM].

Our approach contains two new system parameters: adaptation factor δ and minimal standard deviation σmin. In short, μARMOGA strives to keep the evolution in a permanently "excited" state via forced modification of the population statistics. It practically means that the standard deviation is not allowed to fall under certain minimal value of σmin for any design variable. This helps to prevent the micro-genetic algorithm from getting stuck in premature convergence. The role of the adaptation factor δ lies in controlling the frequency of range adaptation: if reinitialization is necessary, and the new standard deviation of a design variable is changed by more than δ  ·  σold, where σold is the standard deviation when the last reinitialization took place, then the range of that design variable is adapted.

The archiving algorithm

Pareto archive is a key component of many evolutionary algorithms. It acts as a collector of good individuals during the evolution, and is often used to give the resulting Pareto front approximation at the end of the evolution. After new individuals are evaluated, the archive is improved if these individuals dominate or are non-dominated with respect to the existing individuals of the archive. During reinitialization, the micro-genetic algorithm retrieves information from the archive, using it to explore the promising regions of the search space.

Obviously, in any real setup we must limit the number of individuals stored in an archive. This is necessary not just to keep the amount of information processed feasible, but also to get a good diversity of the resulting approximation. In our strategy, we use a fixed upper limit on the number of individuals stored in the archive. Ideally, we want to end up with a full archive of Pareto-optimal solutions that is "well spread" over the true Pareto front of the problem. Our approach is an archive dealing with a single new individual at a time. This is particularly suitable for micro-evolutionary approaches, where we only have a few new individuals from each generation.

When a new individual arrives, it is first checked for Pareto dominance with all existing members of the archive. Now, we distinguish among three cases:

The new individual is dominated by one or more members of the archive. In this case, the new individual is discarded.

The new individual dominates one or more members of the archive. The dominated ones are removed, and the new individual is added to the archive and the internal information of the archive is updated (see below).

The new individual is non-dominated and non-dominating. If the number of members of the archive has not yet reached the upper limit, the new individual is added as in the previous case. In the opposite case, we need to discard at least one individual (either the newcomer or one from the archive), but we can not decide this by Pareto dominance. In this case, we proceed to the secondary decision procedure described below:

If we arrive at the case that can not be resolved by Pareto dominance, our secondary goal is to maximize the distance between neighbouring individuals, based on some distance-measure in the objective space. In this paper, we use the standard Euclidean distance, which is meaningful for any dimension of the objective space.

First, we consider the minimum pairwise distance, i.e.,

[formula]

where P denotes the set of archived individuals and [formula] stands for the vector of objective values of individual i. We take the pair of individuals that achieves the minimum in the above expression. If there are multiple pairs, we take any of them. Without the loss of generality, we assume that the minimum pair is [formula]. Further, we denote the vector of objective values of the new individual as [formula]. If

[formula]

we can replace [formula] by [formula]. Alternatively, if

[formula]

we can replace [formula] by [formula]. If either of the above conditions is satisfied, the overall minimum pairwise distance will be improved by the substitution or, if there were multiple minimal pairs, it will stay the same but the number of minimal pairs will reduce. We call this as the global improvement check.

If neither of these conditions is satisfied, we consider the closest archived individual to [formula], say, [formula] instead. If

[formula]

we replace [formula] by [formula]. If this condition holds, there is a certain subset of the archived individuals whose pairwise minimum will improve. This is the local improvement check.

If neither check is successful, we discard the new individual.

Searching for the minimum-distance pair of the archive afresh each time an individual is considered would be too costly. To make the procedure efficient, we maintain for each archived individual a pointer to its closest neighbour (or any of them). Therefore, searching for the pairwise minimum in [\eqref=eq:pwmin] requires only one pass through the archive. Similarly, the right-hand side of equation [\eqref=eq:locimp] is simply the distance of [formula] to its closest neighbour. Hence, these two checks only require computing the distances of the new individual to all archived individuals, and computing the minima on left-hand sides of the equations [\eqref=eq:globimp1], [\eqref=eq:globimp2], and [\eqref=eq:locimp]. Thus, deciding whether to add a new individual has linear complexity in terms of number of archived individuals (evaluating mutual pairwise dominance also has linear complexity).

If the new individual is to be added, the existing closest-neighbour links need to be updated. Each resulting archive member is considered in turn. If the link is valid (i.e. the closest neighbour in the archive was not discarded), we simply check if the newcomer is closer, and possibly update the link. This takes only constant time. However, if the link became invalid (the former closest neighbour was discarded), we need to compute the closest neighbour afresh by computing objective distances of the updated individual to all others.

It can be proven by a simple argument based on k-dimensional ball volumes that the maximum number of points in k-dimensional space having a single common closest neighbour is bounded from above by a constant depending on k. Since the Pareto archive consists of mutually non-dominating vectors, which can not be arranged arbitrarily, in our case the constant is even smaller. For instance, for a two-objective optimization, i.e. k = 2, a single archive member can be the closest neighbour to at most two other members at the same time.

Using this argument, it can be easily seen that the complexity of a single archive update has complexity O(N  +  MN), where N is the size of the Pareto archive and M is the number of archive members dominated by the new individual. As was already said, merely deciding whether the newcomer is to be added costs O(N). If the decision is positive, there are two cases: either the newcomer dominates some M existing archive members, or it was added based on the secondary decision procedure. In the former case, M members will be discarded, so at most c  M nearest-neighbour links will need to be updated, c being the upper bound constant discussed in the previous paragraph. In the latter case, one existing member is discarded, so at most c existing links must be updated. Given that updating a single link costs O(N), together we have the cost O(N + c(1 + M)N) which can be simplified to O(N + MN), given that c is a constant independent of M,N.

While in principle M can be as high as N, in practice it drops to M  ≪  N very quickly as the convergence proceeds and new dominating individuals become increasingly rare. It should also be noted that if M is high at one step, the evolution continues with an archive of N - M which will be significantly smaller than N. Numerical experiments confirm that in real evolutionary runs, the average number of invalid links per archive update is very small, even much smaller than the theoretical bounds suggested above. This might be observed from Tables [\ref=tab:updates] and [\ref=tab:asize_updates]. Therefore, we can conclude that the procedure of adding new individual to our archive is essentially of linear complexity.

Comparison of results

The abilities of the new archiving mechanism are first demonstrated on test functions DTLZ1, DTLZ2 and DTLZ4, suggested by Deb et al. [\cite=Deb-2002-SMO]. To examine the influence of population size, our algorithm was run separately with 4, 10 and 20 individuals. Obviously, it is preferable to maintain the number of function evaluations as low as possible, therefore we study the behaviour of the aforementioned approaches for three fixed numbers of evaluations, 4 000, 20 000 and 40 000. For test problem DTLZ1, the number of function evaluations is extended to 100 000 and 200 000, since the algorithms were unable to converge to the global Pareto front with just 40 000 computations.

To further investigate the behaviour of the proposed method, we performed an experiment with test problem WFG1 suggested by Huband et al. [\cite=Huband-2005-SMO]. For this difficult problem, it was necessary to run the evolution to as many as 2 000 000 evaluations to obtain reasonable convergence to the Pareto front.

Test problems

The algorithms are compared on three benchmark problems introduced in [\cite=Deb-2002-SMO]. The following form of them is considered:

DTLZ1

Minimize f1,f2,f3, where

[formula]

subject to 0  ≤  xi  ≤  1, for [formula]. Here [formula] and [formula].

DTLZ2

Minimize f1,f2,f3, where

[formula]

subject to 0  ≤  xi  ≤  1, for [formula]. Here [formula] and [formula].

DTLZ4

Minimize f1,f2,f3, where

[formula]

subject to 0  ≤  xi  ≤  1, for [formula]. Here [formula] and [formula].

Problem WFG1 is a benchmark problems introduced in [\cite=Huband-2005-SMO]. The following form is considered:

WFG1

Minimize f1,f2,f3, where

[formula]

where [formula], and [formula] are auxiliary variables obtained from design variables [formula] by a series of nonlinear transformations (see [\cite=Huband-2005-SMO] for their definition). However, there is a slight difference in our application of transformation b_poly, which we use with exponent 0.2 instead of 0.02 suggested in [\cite=Huband-2005-SMO] due to numerical issues in floating point arithmetic. Design variables have range 0  ≤  xi  ≤  2i, for [formula].

The exact front of this problem is visualized in Figure [\ref=fig:WFG1_front].

Metrics

The results are evaluated according to three measures. The distance of members of the Pareto archive to the true Pareto front is measured using the generational distance (GD) [\cite=Veldhuizen-1998-MEA], which is defined as

[formula]

where n is the number of nondominated solutions found by an algorithm, and di is the Euclidean distance of the i-th solution to the exact front. In order to evaluate the distance accurately, we implemented an approach, that is able to iteratively find the closest point of the exact front for each approximate solution, provided the analytic expression of the exact front is known. This point is then used for measuring the distance. Zero value of GD corresponds to all members of the archive on the exact front.

We evaluate also another measure of convergence, denoted as TOL5. It is defined as the lowest value, such that di  >   holds for at most 5 % of individuals. In statistics, it is called the 95-th percentile with respect to distance. Again, the lower value of TOL5 the better convergence. Zero value indicates, that at least 95% of archive members are on the exact front. This metric is less sensitive to remote individuals than the GD value.

The uniformity of distribution of archive members is measured by spacing defined in [\cite=Coello-2004-HMO]. It is based on the distance to the nearest neighbour for each member of the archive, which is defined as

[formula]

Now spacing is the ratio of standard deviation of values of these squared distances and their average, i.e.

[formula]

where [formula] stands for the mean value

[formula]

Consequently, zero spacing corresponds to uniform distribution of distances to the nearest neighbour. Although this does not assure global uniformity of distribution (e.g. for pairs of individuals), our experience with this metric is satisfactory.

The coverage of the Pareto front is not evaluated by means of a metric, but rather compared qualitatively at presented plots.

Setting of algorithms

All the algorithms from PISA package [\cite=Bleuler-2003-PIS] (in the PISA context called selectors), i.e. NSGA-II, SPEA2, and IBEA, are used with the following setting of variator DTLZ:

individual mutation probability 1,

individual recombination probability 1,

variable mutation probability 0.01 ,

variable swap probability 0.5,

variable recombination probability 1,

η mutation 20,

η recombination 15,

use symmetric recombination 1,

For variator WFG, these values are the same except the value of variable mutation probability preset to 1 (default).

The simulations with PISA are performed with population of 100 individuals. All of them are selected for mating, producing 100 new individuals in each generation. The tournament of 2 individuals is used in these selectors. Experiments with IBEA are performed using the additive ε-indicator with scaling factor κ equal to 0.05.

The μARMOGA is run with 4, 10 and 20 individuals in population, marked as μARMOGA(4), μARMOGA(10) and μARMOGA(20), respectively (for WFG1, only 4 members of population are considered). The archive size is always set to 100 to produce results comparable with those of PISA algorithms. Simple one-point crossover scheme without mutation is used. It was reported by Oyama [\cite=Oyama-2000-WDU], that this scheme derived for binary coded algorithms [\cite=Goldberg-1989-GAS] works reasonably well also for real-domain. The version with 4 members uses reinitialization in each generation (DTLZ1, DTLZ2, DTLZ4) or once in four generations (WFG1), while for larger populations, the reinitialization is performed once per 3 generations for all problems. After reinitialization, several existing archive members are put into the new population. Their number is 2, 4 and 6 for the population of 4, 10 and 20 members, respectively. Random selection of individuals for mating is then performed with this modified population. The other important parameters of μARMOGA are set to the following values:

adaptation factor δ 1.4,

minimal standard deviation σmin 0.8 (DTLZ1, WFG1), 0.005 (DTLZ2, DTLZ4),

recombination probability 1.

Larger value of σmin helps to attain the global Pareto optimal front of multimodal problems such as DTLZ1 and leads to faster convergence also in the case of WFG1. In general, its large values emphasizes global exploration of the design space while small values lead to refined search.

Experiments

The results for problems DTLZ1, DTLZ2 and DTLZ4 are summarized in Tables [\ref=tab:DTLZ1_4000]-[\ref=tab:DTLZ4_40000], and visualized in Figures [\ref=fig:DTLZ1_4000]-[\ref=fig:DTLZ4_40000]. For problem WFG1, results are summarized in Tables [\ref=tab:WFG1_4000]-[\ref=tab:WFG1_2000000] and Figures [\ref=fig:WFG1_200000]-[\ref=fig:WFG1_2000000]. The values in tables are obtained as averages for 20 different seeds and where it makes sense, the best value is emphasized by bold font. The approximation with the best distribution is selected out of the twenty runs of each algorithm for visualization. The exact Pareto front is marked by grid of small dots in presented figures. However, not all twenty seeds lead to a successful approximation of the Pareto set in some instances, especially for problem DTLZ4. Most of the algorithms suffer from problems with robustness with respect to initial population and produce degenerated fronts for some seeds. We consider a front as degenerated, if all individuals have almost identical value of an objective, and thus cover just a line on the three dimensional surface of the exact front. Numbers of degenerated fronts for all problems and methods are summarized in Table [\ref=tab:degenerated].

The efficiency of the proposed archiving technique is further demonstrated in comparison with the same approach, i.e. μARMOGA, using crowding distance [\cite=Deb-2001-MOO] [\cite=Deb-2000-FEN]. That algorithm was described in [\cite=Szollos-2008-AOM]. Outputs of these experiments are summarized for DTLZ1 in Tables [\ref=tab:DTLZ1_macd] and [\ref=tab:DTLZ1_mana], for DTLZ2 in Tables [\ref=tab:DTLZ2_macd] and [\ref=tab:DTLZ2_mana], for DTLZ4 in Tables [\ref=tab:DTLZ4_macd] and [\ref=tab:DTLZ4_mana], and for WFG1 in Tables [\ref=tab:WFG1_macd] and [\ref=tab:WFG1_mana]. Obtained Pareto fronts are plotted in Figures [\ref=fig:DTLZ1_macd_mana_4000]-[\ref=fig:WFG1_macd_mana_2000000].

Discussion of results

DTLZ1

This problem with three objectives has a linear Pareto optimal front that intersects the axes of the objective space at value 0.5. Apart of the exact front, there exist a number of other parallel planes corresponding to local Pareto fronts. As these also attract an evolution, problem DTLZ1 tests the ability of a genetic algorithm to cope with multi-modality.

As can be seen in Figure [\ref=fig:DTLZ1_4000], none of the algorithms is able to reach the global Pareto front in 4 000 evaluations for any seed, and metrics in Table [\ref=tab:DTLZ1_4000] do not provide much valuable information. However, we can remark that IBEA and μARMOGA(4) provide one order better convergence than the other algorithms and μARMOGA for all sizes of population provides reasonable spacing.

However, all algorithms except NSGA-II are able to reach the global front for some seeds in 20 000 evaluations (Figure [\ref=fig:DTLZ1_20000]). Comparing visually the results in Figure [\ref=fig:DTLZ1_20000], we can conclude that μARMOGA(4) performs best, which is supported by the best values of all three metrics in Table [\ref=tab:DTLZ1_20000]. As individuals for many of the seeds are still away from the global front for all algorithms, metrics in Table [\ref=tab:DTLZ1_20000] do not provide a detailed insight either.

The situation is further improved with 40 000 evaluations, for which all algorithms except NSGA-II are able to reach the global front for most of the seeds (Figure [\ref=fig:DTLZ1_40000]). However, Figure [\ref=fig:DTLZ1_40000] shows that μARMOGA (for all sizes of population) produces the best distribution, which is confirmed by values of spacing in Table [\ref=tab:DTLZ1_40000]. Since for some seeds the individuals still are not in vicinity of the true Pareto front, the averaged metrics in Table [\ref=tab:DTLZ1_40000] are still rather bad. According to Table [\ref=tab:DTLZ1_40000], the best convergence is in average attained by μARMOGA(4) for this case.

For 100 000 and 200 000 evaluations, μARMOGA(4) achieves the global Pareto-optimal front for all seeds. All the other algorithms fail to find the global front for some seeds, which considerably spoils the metrics in Tables [\ref=tab:DTLZ1_100000] and [\ref=tab:DTLZ1_200000]. Since IBEA produces only degenerated fronts in these cases, metrics are not evaluated and are omitted in Tables [\ref=tab:DTLZ1_100000] and [\ref=tab:DTLZ1_200000].

Although the distribution of fronts obtained by μARMOGA for all sizes of population is comparable to SPEA according to Figures [\ref=fig:DTLZ1_100000] and [\ref=fig:DTLZ1_200000], the metrics in Tables [\ref=tab:DTLZ1_100000] and [\ref=tab:DTLZ1_200000] reveal that spacing is, in average, one order better by μARMOGA than by SPEA. The best average convergence metrics are obtained by μARMOGA(4) (Tables [\ref=tab:DTLZ1_100000] and [\ref=tab:DTLZ1_200000]).

DTLZ2

Problem DTLZ2 has three objectives, and the exact front corresponds to the part of a unit sphere when restricted to the octant given by non-negative values of all three objectives. This is the easiest problem for all compared algorithms and tests mainly the speed at which an algorithm is converging to the exact Pareto front.

Already for 4 000 evaluations, the fronts obtained by all the compared methods are reasonably converged and distributed along the exact Pareto front. Figure [\ref=fig:DTLZ2_4000] shows that μARMOGA (regardless of the size of population) and SPEA produce the best distribution of individuals along the exact front, whereas the distribution obtained by IBEA is rather poor. This observation is confirmed by the spacing metric in Table [\ref=tab:DTLZ2_4000]. The best convergence is achieved by IBEA according to GD and TOL5 metrics in Table [\ref=tab:DTLZ2_4000] followed by μARMOGA.

Similar observations can be made from the results for 20 000 evaluations (Table [\ref=tab:DTLZ2_20000] and Figure [\ref=fig:DTLZ2_20000]) and 40 000 evaluations (Table [\ref=tab:DTLZ2_40000] and Figure [\ref=fig:DTLZ2_40000]) - the best spacing is obtained for all sizes of population by μARMOGA and the best convergence is attained by IBEA, although the distribution of individuals along the Pareto front is worse.

DTLZ4

Although the definition of problem DTLZ4 is similar to DTLZ2 (cf. Section [\ref=sec:problems]), the evolution is greatly influenced by the exponential transformation of design variables, which maps most of the space towards the axes in design space. This in turn pushes the evolution to the limits of the objective space. Thus, problem DTLZ4 tests best of the three DTLZ problems the ability of a genetic algorithm to obtain uniform distribution of individuals along the Pareto optimal surface.

For 4 000 evaluations, the best distribution is produced by SPEA2 (Figure [\ref=fig:DTLZ4_4000]). This is confirmed by results of spacing in Table [\ref=tab:DTLZ4_4000]. However, μARMOGA(4) produces the best converged results.

For 20 000 evaluations, the distribution obtained by μARMOGA is already visually comparable with SPEA2 in Figure [\ref=fig:DTLZ4_20000]. Also spacing obtained by μARMOGA is comparable to that of SPEA2 according to Table [\ref=tab:DTLZ4_20000] for 4 and 10 members of population. Algorithms μARMOGA(20), IBEA, and NSGA-II produce in average only slightly worse converged results. The best GD and TOL5 values are attained by μARMOGA(20).

In the case of 40 000 evaluations, the best distribution of individuals is attained by μARMOGA followed by SPEA2 according to Figure [\ref=fig:DTLZ4_40000] and also confirmed by values of spacing in Table [\ref=tab:DTLZ4_40000]. The best convergence is again obtained by μARMOGA(20), followed by μARMOGA(10), μARMOGA(4) and IBEA, respectively (Table [\ref=tab:DTLZ4_40000]).

WFG1

This is a difficult problem and all tested algorithms had problems with convergence to the Pareto front. For this reason, number of evaluations of the objective function was increased to 2 000 000, after which some algorithms were able to attain the exact front.

After 4 000 evaluations, all algorithms produce results rather far from the Pareto optimal set (Table [\ref=tab:WFG1_4000]). Nevertheles, SPEA2 produces the most uniform distribution according to the spacing metric.

After 20 000 evaluations, μARMOGA(4) slightly leads in convergence followed by IBEA (Table [\ref=tab:WFG1_20000]), producing distribution with uniformity between SPEA2 (best spacing) and the rest of the algorithms. The same observations remain valid for 40 000 evaluations (Table [\ref=tab:WFG1_40000].

After 100 000 as well as 200 000 evaluations, μARMOGA(4) dominates in convergence to the exact front (GD and TOL5 metrics in Tables [\ref=tab:WFG1_100000] and [\ref=tab:WFG1_200000]), producing distribution comparable with SPEA2 (best spacing). However, Figure [\ref=fig:WFG1_200000] suggest, that μARMOGA(4) covers the whole Pareto front, unlike SPEA2.

Letting the evolution run to 1 000 000 and 2 000 000 evaluations, μARMOGA(4) dominates both in convergence (one order of magnitude compared to the second IBEA in Tables [\ref=tab:WFG1_100000] and [\ref=tab:WFG1_200000]) and distribution along the exact Pareto front. In spacing metric, μARMOGA(4) is followed by SPEA2. Figures [\ref=fig:WFG1_1000000] and [\ref=fig:WFG1_2000000] show that distribution of individuals by μARMOGA(4) uniformly covers the whole Pareto front, while the other algorithms approaches the region around f1  =  0 only slowly.

Comparison of crowding distance with the new archiving mechanism

A set of experiments was run to compare μARMOGA with crowding distance and μARMOGA with the new archiving mechanism. The population of four individuals was selected for the comparison. Results for problems DTLZ1, DTLZ2, DTLZ4, and WFG1 are summarized in Tables [\ref=tab:DTLZ1_macd]-[\ref=tab:WFG1_mana] and Figures [\ref=fig:DTLZ1_macd_mana_4000]-[\ref=fig:WFG1_macd_mana_2000000].

According to these experiments, the new archiving approach outperforms crowding distance in diversity as is clear from Figs. [\ref=fig:DTLZ1_macd_mana_4000]-[\ref=fig:WFG1_macd_mana_2000000] and spacing metric in Tabs. [\ref=tab:DTLZ1_macd]-[\ref=tab:WFG1_mana]. While it also has very positive effect on the convergence of μARMOGA to the exact Pareto front for problems DTLZ1, DTLZ2, and DTLZ4 (Tabs. [\ref=tab:DTLZ1_macd]-[\ref=tab:DTLZ4_mana]), both algorithms exhibit similar convergence for problem WFG1 (Tabs. [\ref=tab:WFG1_macd] and [\ref=tab:WFG1_mana]).

Summary

As can be seen from above, μARMOGA outperforms the other methods in distribution of individuals along the Pareto front, and in many cases achieves the best convergence as well. However, it should be noted that the default settings of the algorithms from PISA package is used, which may not be optimal for the test problems considered.

While IBEA offers exceptional convergence in some cases, the distribution of individuals along the exact Pareto front is usually rather poor, with many individuals attached to limits of the objective space. Our study confirms that the mechanism of crowding distance does not lead to uniform distribution of individuals along the Pareto front for more than two objectives. The same result might be observed from the comparison of μARMOGA using the two archiving mechanisms - crowding distance and the new proposed technique (Tables [\ref=tab:DTLZ1_macd]-[\ref=tab:WFG1_mana], and Figures [\ref=fig:DTLZ1_macd_mana_4000]-[\ref=fig:WFG1_macd_mana_2000000]). On the other hand, SPEA2 produces very uniform distribution of individuals comparable with μARMOGA in some instances.

Concerning the number of evaluations, DTLZ2 is the only problem, for which only 4 000 evaluations are sufficient to achieve reasonable convergence and distribution of individuals on the Pareto front by all algorithms. On the other hand, for DTLZ1, even 40 000 evaluations do not suffice to reach the true Pareto front for all seeds by any approach, and results for 100 000 and 200 000 evaluations are added for a reasonable comparison. Even this large number of evaluations was not sufficient to reach the proximity of exact front in the case of problem WFG1, and results for 1 000 000 and 2 000 000 evaluations are added. For test functions DTLZ4 and WFG1, the new archiving mechanism is able to drive the evolution to regions, where the coverage of the Pareto front by individuals is sparse, and recover nice distribution of individuals along the Pareto set even for poorly chosen initial population.

To investigate the optimal distribution of the number of function evaluations between population size and number of generations for micro-evolution, μARMOGA is run with 4, 10 and 20 individuals for DTLZ1, DTLZ2, and DTLZ4. According to our experiments, the performance of the algorithm is similar for all configurations with respect to spacing and convergence history and no strong dependence is revealed. However, for problem DTLZ4, the method tends to produce more degenerated fronts with larger population (see Table [\ref=tab:degenerated]). Additionally, population of 4 individuals leads to the best convergence metrics for problem DTLZ1, and population of 20 individuals to the best converged front for problem DTLZ4. Thus, using small populations and larger number of generations seems as the preferable approach.

Conclusion

The goal of our study is twofold: (a) to develop a new approach for selecting individuals to the Pareto archive; (b) to explore the potential of using small population in evolutionary algorithms.

The main contribution of the paper is the presentation of a new archiving mechanism. Although its basic idea is rather simple and straightforward, the technique produces very promising results on all tested problems. We are aware of the fact that the theoretical time complexity of the mechanism might be rather large (quadratic in the worst case). However, our tests justify its usage, since the experimentally found complexity is much more favourable (approximately linear). Moreover, it is intended to be used in combination with small population, for which such more elaborate selection mechanism is usually affordable.

The proposed selection mechanism was combined with μARMOGA and is compared to other three state-of-the-art algorithms (NSGA-II, SPEA2, and IBEA) on four test problems. We can conclude that μARMOGA presents Pareto sets with the same or better distribution as SPEA2, but usually with much better convergence to the exact front that is comparable with IBEA, thus the best combining requirements on both convergence and distribution of individuals. A considerable improvement is attained, using the new mechanism, in comparison with the version of μARMOGA that uses crowding distance. Clearly, μARMOGA equipped with the new diversity mechanism is very promising and may be competitive with respect to other recent approaches.

Our experiments further support using small populations (up to 10 individuals), since runs with four individuals usually produces the best results. It is well known that such small population can lead to rapid convergence. However, in combination with the proposed archiving mechanism, it also seems to be more robust with respect to an initial population.

Regarding the history of convergence to the Pareto front, in some cases as few as 4 000 evaluations of objective function could be sufficient for some problems (DTLZ2), while for other problems (multi-modal problem DTLZ1 or difficult WFG1), even 40 000 evaluations may not be sufficient to approximate the true Pareto front, and as many as 1 000 000 evaluations are needed for reasonable outcome.

Acknowledgement

This research has been supported by the Development of Applied External Aerodynamics Program (Ministry of Education, Youth and Sports of the Czech Republic Grant MSM0001066901), by research project AV0Z10190503 (Academy of Sciences of the Czech Republic), and by grant IAA100760702 (Grant Agency of the Academy of Sciences of the Czech Republic).