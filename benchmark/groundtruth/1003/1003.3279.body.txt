A New Heuristic for Feature Selection by Consistent Biclustering

Introduction

Data mining techniques are nowadays much studied, because of the growing amount of data which is available and that needs to be analyzed. In particular, clustering techniques aim at finding suitable partitions of a set of samples in clusters, where data are grouped by following different criteria. The focus of this paper is biclustering, where samples and features in a given set of data are partitioned simultaneously.

Given a set of samples, each sample in the set can be represented by a sequence of features, which are supposed to be relevant for the samples. If a set of data contains n samples which are represented by m features, then the whole set can be represented by an m  ×  n matrix A, where the samples are organized column by column, and the features are organized row by row. A bicluster is a submatrix of A, which can be equivalently defined as a pair of subsets (Sr,Fr), where Sr is a cluster of samples, and Fr is a cluster of features. A biclustering is then a partition of A in k biclusters: such that the following conditions are satisfied:

[formula]

[formula]

where k  ≤   min (n,m) is the number of biclusters [\cite=Busygin05] [\cite=Hartigan]. Note that the conditions ([\ref=equ:clS]) ensures that [formula] is a partition of the samples in disjoint clusters, while the conditions ([\ref=equ:clF]) ensures that [formula] is a partition of the features in disjoint clusters.

We focus on the problem of finding biclusterings of the set of samples and of the set of features. When such biclusterings can be found, not only clusters of samples are obtained (as in standard clustering), but, in addition, the features causing the partition of samples in these clusters are also identified. This information is very interesting in many real-life applications. In particular, biclustering techniques are widely applied for analyzing gene expression data, where samples represent particular conditions (for example, the presence or absence of a disease), and each sample is represented by a sequence of gene expressions. In this case, finding out which features (genes) are related to the samples can help in discovering information about diseases [\cite=Busygin09] [\cite=Madeira].

The concept of consistent biclustering is very important in this domain [\cite=Busygin05]. Let us consider a set of samples, and let us suppose that a certain classification is assigned to such samples. In other words, we know a partition in clusters of these samples: [formula]. A classification for the corresponding features, i.e. for the features used for representing these samples, can be obtained from BS (see Section [\ref=sec:biclust] for details). Let us refer to this partition of the features with [formula]. Then, the procedure can be inverted, and from the obtained classification BF of the features, another classification for the samples can be computed: [formula]. In general, BS and B̂S differ. In the event in which they instead coincide, the biclustering [formula] is referred to as a consistent biclustering.

Consistent biclusterings can be used for classification purposes. Let us suppose that a training set is available for a certain classification problem. In other words, we suppose that a set of samples, whose classification is known, is available. From the classification of the samples, a classification of the features can be found, and then a certain biclustering, as explained above. If this biclustering is consistent, then the original classification of the samples in the training set can be reconstructed from the classification of its features. Therefore, the classification of these features can also be exploited for finding a classification for other samples, which originally have no known classification.

Unfortunately, sets of data allowing for consistent biclusterings are quite rare. There are usually features that are not relevant for the classification of the samples, which can easily bring to misclassifications. Because of experimental errors or noise, these features could be assigned to a bicluster or another, and this uncertainty causes errors in the classifications. For avoiding this, all the features that are not relevant must be removed. Therefore, we are interested in selecting a certain subset of features for which a consistent biclustering can be found. Since it is preferable to keep the loss of information as low as possible, the number of features to be selected has to be the maximum possible.

The feature selection problem related to consistent biclustering is NP-hard [\cite=Kundakcioglu]. It can be formulated as a 0-1 linear fractional optimization problem, which can be very difficult to solve. In particular, for large (real-life) sets of data, the corresponding optimization problem is also large, and therefore there are no examples in the literature in which deterministic techniques have been employed. In [\cite=Busygin05] [\cite=Nahapatyan], two heuristic algorithms have been proposed for solving the 0-1 linear fractional optimization problem arising in the context of feature selection by biclustering.

In this paper, we propose a new heuristic algorithm for solving this feature selection problem. We reformulate the optimization problem as a bilevel optimization problem, in which the inner problem is linear. Therefore, we use a deterministic algorithm for solving the inner problem, which is nested into a general framework where a heuristic strategy is employed. Our computational experiments show that the proposed heuristic algorithm is able to find subsets of features allowing for consistent biclusterings. The obtained results are compared to the ones reported in other publications [\cite=Busygin05] [\cite=Nahapatyan]: in general, the heuristic algorithm that we propose is able to find consistent biclusterings in which the number of selected features is larger.

The remaining of the paper is organized as follows. In Section [\ref=sec:biclust], we develop the concept of consistent biclustering in more details, and we present the corresponding feature selection problem. In Section [\ref=sec:heuristic], we reformulate this feature selection problem as a bilevel optimization problem and we introduce a heuristic algorithm for an efficient solution of the problem. Computational experiments on real-life sets of data are presented in Section [\ref=sec:experiments], as well as a comparison to another heuristic algorithm. Conclusions are given in Section [\ref=sec:conclusions].

Consistent biclustering

Let A be an m  ×  n matrix related to a certain set of data, where samples are organized column by column, and their features are organized row by row. If a classification of the samples is known, then the centroids of each cluster, computed as the mean among all the members of the same cluster, can be computed. Let CS be the matrix containing all these centroids, organized column by column, where its generic element cSir refers to the ith feature of the centroid of the rth cluster of samples. Analogously, a matrix CF containing the centroids of the clusters related to a known classification of the features can be defined. The generic element cFjr of the matrix CF refers to the jth sample related to the centroid of the rth cluster of features. Finally, the symbol ai refers to the ith row of the matrix A, i.e. to a feature, and the symbol aj refers to the jth column of A, i.e. to a sample. In the following discussion, k represents the number of biclusters (known a priori), and [formula] refers to the generic bicluster. The symbols r̂ and ξ are used for referring to biclusters having particular properties.

Let us suppose that a classification for the samples in A is known. In other words, the following partition in k clusters is available: Starting from this classification, the matrix CS of centroids can be computed. Given a feature ai, we can check the value of cSir for all the clusters. If, for a certain cluster Sr̂, the element cSir̂ is the largest for any possible r, then Sr̂ is the cluster in which the feature ai is mostly expressed. Therefore, it is reasonable to give to this feature the same classification as the samples in Sr̂. Formally, it is imposed that:

[formula]

Note that a complete classification of all the features can be obtained by imposing the equivalence ([\ref=equ:biClustCond1]) for all ai.

Let be the computed classification of the features. Starting from this classification, the matrix CF can be computed. In a similar way, a classification of the samples can be obtained by imposing the following equivalence:

[formula]

Let be the computed classification of the samples. In general, the two classifications BS and B̂S are different from each other. If they coincide, then the partition in biclusters is, by definition, a consistent biclustering. As already remarked in the Introduction, the classification of the features obtained from consistent biclusterings can be exploited for classifying samples with an unknown classification [\cite=Busygin05].

If a consistent biclustering exists for a certain set of data, then it is said to be biclustering-admitting. However, sets of data admitting consistent biclusterings are very rare. Therefore, features must be removed from the set of data for making it become biclustering-admitting [\cite=Busygin05]. During this process, it is very important to remove the least possible number of features, in order to preserve the information in the set of data. In practice, a maximal subset of good features must be extracted from the initial set. The problem of finding the maximal consistent biclustering can be seen as a feature selection problem.

Let fir be a binary parameter which indicates if the generic feature ai belongs to the generic cluster Fr (fir  =  1) or not (fir  =  0). Let [formula] be a binary vector of variables, where xi is 1 if the feature ai is selected, and it is 0 otherwise. The problem of finding a consistent biclustering considering the maximum possible number of features can be formulated as follows:

[formula]

subject, [formula], to:

[formula]

The generic constraint ([\ref=equ:BiConstr]) ensures that the r̂-th feature is the mostly expressed if it belongs to the cluster (Sr̂,Fr̂). Note that the two fractions are used for computing the centroids of the clusters of features, and that the sums (at the numerators and at the denominators) only consider the selected features (each unselected feature is automatically discarded because xi  =  0). The reader is referred to [\cite=Busygin05] for additional details.

In this context, other two optimization problems have also been introduced [\cite=Nahapatyan]. They are extensions of the problem ([\ref=equ:biclustObjFun])-([\ref=equ:BiConstr]), which have been proposed in order to overcome some problems related to data affected by noise. If a partition in clusters for the samples is available, then we can find a partition in clusters for the features. Each feature is therefore assigned to the cluster Fr̂ if cSir̂ is the centroid with the largest value. Let us suppose that the following condition holds for a certain feature ai: where ε is a small positive real number. If this is the case, small changes (i.e.: noise) in the data can bring to different partitions of the features, because the margin between cSir̂ and other centroids is very small.

In order to overcome this problem, the concepts of α-consistent biclustering and β-consistent biclustering have been introduced in [\cite=Nahapatyan]. They bring to the formulation of the following two optimization problems. The problem of finding an α-consistent biclustering with a maximal number of features is equivalent to solving the optimization problem:

[formula]

subject, [formula], to:

[formula]

where each αj  >  0. Similarly, the problem of finding a β-consistent biclustering with a maximal number of features is equivalent to solving the optimization problem:

[formula]

subject, [formula], to:

[formula]

where each βj  >  1. All the presented optimization problems are NP-hard [\cite=Kundakcioglu]. The reader who is interested in more information on the formulation of these optimization problems can refer to [\cite=Busygin05] [\cite=Nahapatyan] [\cite=Pardalos]. For a simple and ampler discussion on biclustering, refer to [\cite=BOOK].

The three optimization problems ([\ref=equ:biclustObjFun])-([\ref=equ:BiConstr]), ([\ref=equ:biclustObjFunAlpha])-([\ref=equ:BiConstrAlpha]) and ([\ref=equ:biclustObjFunBeta])-([\ref=equ:BiConstrBeta]) are linear fractional 0-1 optimization problems. In [\cite=Busygin05], a possible linearization of the problem has been studied. However, the authors noted that currently available solvers for mixed integer programming are not able to solve the considered linearization, due to the large number of variables which are usually involved when dealing with real-life data. Therefore, they presented a heuristic algorithm for the solution of these problems, which is based on the solution of a sequence of linear 0-1 (non-fractional) optimization problems. Successively, in [\cite=Nahapatyan], another heuristic algorithm has been proposed, where a sequence of continuous linear optimization problems needs to be solved. The heuristic algorithm we propose is able to provide better solutions with respect to the ones provided by these two.

An improved heuristic

In the following discussion, only the optimization problem ([\ref=equ:biclustObjFun])-([\ref=equ:BiConstr]) will be considered, because similar observations can be made for the other two problems. The computational experiments reported in Section [\ref=sec:experiments], however, will be related to all three optimization problems.

We propose a reformulation of the problem ([\ref=equ:biclustObjFun])-([\ref=equ:BiConstr]) as a bilevel optimization problem. To this aim, we substitute the denominators in the constraints ([\ref=equ:BiConstr]) with new variables yr, [formula], where each yr is related to the generic bicluster. Then, we can rewrite the constraints ([\ref=equ:BiConstr]) as follows:

[formula]

The constraints ([\ref=equ:BiConstrNew]) must be satisfied for all [formula] and for all j∈Sr̂.

Let us consider a set of values r of yr, and also another proportional set of values [formula], with δ  >  0. It is easy to see that, given certain values for the variables xi, with [formula], the constraints ([\ref=equ:BiConstrNew]) are satisfied with r if and only if they are satisfied with [formula]. As an example, if k = 3 and there is a consistent biclustering in which 20, 30 and 50 features are selected in the k biclusters, then the constraints ([\ref=equ:BiConstrNew]) are also satisfied if 0.20, 0.30 and 0.50, respectively, replace the actual number of features (in this example, the proportional factor δ is 0.01). For this reason, the variables yr can be used for representing the proportions among the cardinalities of the clusters of features. In the previous example, 20% of the selected features are in the first bicluster, 30% of the features in the second one, and 50% in the last one. The variables yr can be bound in the real interval

[formula]

;

let r'' = random in [formula] such that r'  ≠  r'';

set yr'' so that [formula];

At the beginning, the variables xi are all set to 1, and the variables yr are set so that they represent the distribution of all the m features among the k clusters. Therefore, if the biclustering is already consistent, then the function g is 0 with this choice for the variables, and all the features can be selected. In this case, the condition in the while loop is not satisfied and the algorithm ends.

At each step of the algorithm, the inner optimization problem is solved. It is a linear 0-1 optimization problem, and we consider its continuous relaxation, i.e. we allow the variables x to take any real value in the interval

[formula]

for which the constraint ([\ref=equ:newConstr]) can be satisfied. In order to overcome this issue, too large values for range are avoided.

For its nature, the proposed heuristic algorithm can provide different solutions if it is executed more than once (with different seeds for the generator of random numbers). Therefore, the algorithm can be executed a given number of times and the best obtained solution can be taken into consideration.

Computational experiments

We implemented the presented heuristic algorithm for feature selection in AMPL [\cite=AMPL], from which the ILOG CPLEX11 solver is invoked for the solution of the inner optimization problem. Experiments are carried out on an Intel Core 2 CPU 6400 2.13 GHz with 4GB RAM, running Linux.

The first set of data that we consider is a set of gene expressions related to human tissues from healthy and sick (affected by cancer) patients [\cite=Notterman]. This set of data is available on the web site of the Princeton University (see the paper for the web link). It contains 36 samples classified as normal or cancer, and each sample is specified through 7457 features. We applied our heuristic algorithm for finding a consistent biclustering for the samples and the features contained in this set of data.

Table [\ref=tab:carcinoma] shows some computational experiments. We found α-consistent biclusterings and β-consistent biclusterings, with different values for α or β. Note that, even though for each sample a different αj or βj can be considered, we use one unique value for α and β in each experiment. In the table, the number of selected features f(x) is given in correspondence with each experiment.

When α  =  0 or β  =  1 (consistent biclustering), after 4 iterations only (41 seconds of CPU time), our heuristic algorithm is able to provide the list of selected features, and thus to identify the (few) features to be removed in order to have a consistent biclustering. In particular, 7 features on 7457 need to be removed (and therefore 7450 features are selected).

The bilevel optimization problem to be solved gets harder in the case of α-consistent and β-consistent biclustering. As expected, less features are selected when larger α or β values are chosen, because the constraints ([\ref=equ:BiConstrNew]) are more difficult to be satisfied. However, using larger values for α and β allows for identifying the features that are actually important for the classification of the samples. The computational cost of our heuristic algorithm increases when larger α or β values are used: some of the presented experiments need some minutes of CPU time to be performed.

The second real-life set of data we consider consists of samples from patients diagnosed with acute lymphoblastic leukemia (ALL) or acute myeloid leukemia (AML) diseases [\cite=Golub] (to download the set of data, follow the link given in the reference). This set of data is divided in a training set, which we use for finding consistent biclusterings, and a validation set, which can be used for checking the quality of the classifications performed by using the features previously selected. The training set contains 38 samples: 27 ALL samples and 11 AML samples. The validation set contains 34 samples: 20 ALL samples and 14 AML samples. The total number of features in both sets of data is 7129. Since, in this case, a validation set is also available, we are able to validate the quality of the obtained biclusterings in correspondence with different values for the chosen parameter α or β.

The results of our experiments are in Table [\ref=tab:ALLAML]. The total number of features that are selected in each experiment is reported, together with the number err of misclassifications that occur when the samples of the validation set are classified accordingly with the classification of the features in the α-consistent or β-consistent biclusterings. When α  =  0 or β  =  1, our heuristic algorithm is able to find a consistent biclustering, but the selected features are not able to provide a correct classification for all the samples of the validation set (err  =  2). This is due to the fact that the used data are probably noisy, because they have been obtained from an experimental technique. However, the number err of misclassifications decreases when α or β increase. For example, for α  ≥  50, there is only one misclassification for the samples of the validation set.

In Table [\ref=tab:ALLAML], we also compare the obtained results to the ones reported in [\cite=Nahapatyan]. Our heuristic algorithm is able to provide better-quality solutions in the majority of the cases. In particular, for given choices of α or β, our heuristic algorithm is able to find biclusterings in which the total number of selected features is larger, except for only one experiment (α  =  70). These biclusterings allow to perform good-quality classifications (err  =   1 or 2), while a larger number of features in the set of data are preserved.

Conclusions

We proposed a reformulation for the linear fractional 0-1 optimization problem for feature selection by consistent biclustering. Our reformulation transforms the problem into a bilevel optimization problem, in which the inner problem is linear. We presented a heuristic algorithm for the solution of the reformulated problem, where the continuous relaxation of the inner problem is solved exactly at each iteration of the algorithm. Computational experiments showed that the proposed algorithm can solve feature selection problems by finding consistent, α-consistent and β-consistent biclusterings of a given set of data. The results also showed that this algorithm is able to find better solutions with respect to the ones obtained by previously proposed heuristic algorithms. Future works will be devoted to suitable strategies for improving the efficiency of the proposed algorithm.