The Power of Confidence Intervals

It is well known that the most important property of Frequentist Confidence Intervals is coverage: a 100(1 - α)% Confidence Interval belong to a set of intervals that cover the true value of the measured quantity μ with Frequentist probability 1 - α. Neyman's method obtains Confidence Intervals with correct coverage through the construction for each possible value of μ of an acceptance interval with probability 1 - α for an estimator [formula] of μ. The union of all acceptance intervals in the [formula]-μ plane is called the Confidence Belt. The Confidence Interval for μ resulting from a measurement [formula] of the estimator is the set of all values of μ whose acceptance interval for [formula] include [formula].

Coverage is not the only property of Confidence Intervals, because many methods for the construction of a Confidence Belt with exact coverage are available (see Refs. [\cite=Kendall-2A] [\cite=Feldman-Cousins-98] [\cite=Giunti-bo-99] [\cite=Ciampolillo-98] [\cite=Mandelkern-Schultz-99]). These methods differ by power [\cite=Giunti:2000kc], a quantity which is obtained considering the construction of acceptance intervals as hypothesis testing. Coverage and power are connected, respectively, with the so-called Type I and Type II errors in testing a simple statistical hypothesis H0 against a simple alternative hypothesis H1 (see Ref. [\cite=Kendall-2A], section 20.9):

Unfortunately, the power associated with a confidence belt is not easy to evaluate, because for each possible value μ0 of μ considered as a null hypothesis there is no simple alternative hypothesis that allows to calculate the probability β of a Type II error. Instead, we have the alternative hypothesis H1: μ1  ≠  μ0, which is composite. For each value of μ1  ≠  μ0 one can calculate the probability βμ0(μ1) of a Type II error associated with a given acceptance interval corresponding to μ0. A method that gives an acceptance region for μ0 which has the largest possible power πμ0(μ1) = 1 - βμ0(μ1) is Most Powerful with respect to the alternative μ1. Clearly, it would be desirable to find a Uniformly Most Powerful test, i.e. a test that gives an acceptance region for μ0 which has the largest possible power πμ0(μ1) for any value of μ1. Unfortunately, the Neyman-Pearson lemma implies that in general a Uniformly Most Powerful test does not exist if the alternative hypothesis is two-sided, i.e. both μ1  <  μ0 and μ1  >  μ0 are possible, and the derivative of the Likelihood with respect to μ is continuous in μ0 (see Ref. [\cite=Kendall-2A], section 20.18). Nevertheless, it is possible to find a Uniformly Most Powerful test if the class of tests is restricted in appropriate ways. A class of tests that has some merit is that of unbiased tests, such that the power πμ0(μ1) for any value of μ1 is larger or equal to the size α of the test,

[formula]

In other words, the probability of rejecting μ0 when it is false is at least as large as the probability of rejecting μ0 when it is true. The equal-tail test used in the Central Intervals method is unbiased and Uniformly Most Powerful Unbiased for distributions belonging to the exponential family, such as, for example, the Gaussian and Poisson distributions (see Ref. [\cite=Kendall-2A], section 21.31).

Therefore, the Central Intervals method is widely used because it corresponds to a Uniformly Most Powerful Unbiased test. Other methods based on asymmetric tests unavoidably introduce some bias.

Figure [\ref=ci]A illustrates the power π in the Central Intervals method for an estimator [formula] of μ that has a Gaussian distribution. The Gaussian distribution of [formula] for μ  =  μ0 is depicted qualitatively above the horizontal line for μ  =  μ0. The 100(1 - α)% acceptance interval corresponding to the null hypothesis μ0 is limited by the two vertical lines. The area of the two dark-shaded tails of the distribution is equal to α.

Let us consider for example the alternative hypothesis μ+1  >  μ0 (similar considerations apply to the alternative hypothesis μ-1  <  μ0). The Gaussian distribution of [formula] for μ  =  μ+1 is depicted qualitatively above the horizontal line for μ  =  μ+1 in Fig. [\ref=ci]A. The probability β+ of a Type II error in testing μ0 against μ+1 is given by the integral of the distribution of [formula] for μ  =  μ+1 in the interval between the two horizontal lines. The corresponding area is shown dark-shaded in Fig. [\ref=ci]A. The power to test the null hypothesis μ0 against the larger alternative μ+1  >  μ0, is given by the integral of the distribution of [formula] for μ  =  μ+1 in the two semi-infinite intervals of [formula] external to the two horizontal lines. The corresponding areas are shown light-shaded in Fig. [\ref=ci]A (only the one on the right is large enough to be visible).

From Fig. [\ref=ci]A one can see that the power corresponding to alternative hypotheses μ-1 and μ+1, respectively smaller and larger than the null hypothesis μ0, is equal. The Central Intervals method produces the most reliable results in the case of an unbounded μ, because the power is perfectly balanced. Problems arise if one considers the measurement of a bounded quantity μ. As illustrated in Fig. [\ref=ci]B for the case of a bounded [formula], the balanced power in the Central Intervals method is not appropriate. Indeed, a high power to test μ0 against μ-1  <  μ0 when μ0 is near the boundary is not needed, because the alternatives μ-1  <  μ0 are limited. As a result, the Central Intervals method produces in this case clearly unreliable Confidence Intervals if the value of [formula] lies on the left-hand side of Fig. [\ref=ci]B. Sometimes the Confidence Interval can be empty, giving no information. Sometimes one can get a very stringent upper limit, much smaller than the exclusion potential of the experiment [\cite=Feldman-Cousins-98] [\cite=Giunti:2000cd]. This possibility is very dangerous, because it can lead to wrong conclusions if interpreted in inappropriate ways. In any case it gives no useful information on the value of μ.

In the past the Upper Limits method was rather popular. Figures [\ref=ul]A and [\ref=ul]B show that the Upper Limits method is actually worse than the Central Intervals method because it is biased in the wrong direction. As a consequence, it produces limits that are practically always unreliable, except maybe when by chance [formula].

The method biased in the right direction that has been proposed first is the Unified Approach of Feldman and Cousins [\cite=Feldman-Cousins-98], which, as illustrated in Fig. [\ref=ua]A, gives more power to test μ0 against μ+1  >  μ0 than to test μ0 against μ-1  <  μ0 when μ0 is near the boundary. However, the bias is still insufficient to produce reliable results if [formula]: from Fig. [\ref=ua]B one can see that when [formula] the Confidence Interval gives an upper limit for μ that is unphysically too small [\cite=Giunti-bo-99] [\cite=Roe:1998zi] [\cite=Mandelkern-Schultz-99] [\cite=Giunti-CERN-2000-005] [\cite=Astone-CERN-2000-005], much smaller than the exclusion potential of the experiment [\cite=Feldman-Cousins-98] [\cite=Giunti:2000cd].

Figure [\ref=ml]A illustrates the calculation of the power in the Maximum Likelihood Estimator method proposed independently by Ciampolillo in Ref. [\cite=Ciampolillo-98] and Mandelkern and Schultz in Ref. [\cite=Mandelkern-Schultz-99]. In this method the estimator of μ is not [formula], but the maximum likelihood value μ* of μ. Since the range of μ* is equal to the range of μ, the estimate [formula] always lies in the physical range of μ. In the case of a Gaussian distribution for [formula] illustrated in Fig. [\ref=ml]A, μ*  =   for   ≥  0 and μ* = 0 for   ≤  0. Therefore, as shown in Fig. [\ref=ml]A, the upper limit for μ obtained for any [formula] is equal to the upper limit obtained for [formula].

As one can see from Fig. [\ref=ml]A, the Maximum Likelihood Estimator method has optimal bias. As a consequence, this method produces reliable results for any value of [formula], as shown in Fig. [\ref=ml]B.

Let us emphasize that the bias is needed near the boundary and both the Maximum Likelihood Estimator method and the Unified Approach produce Confidence Intervals that practically coincide with those obtained with the Central Intervals method when [formula].

In conclusion, we have shown that the Maximum Likelihood Estimator method [\cite=Ciampolillo-98] [\cite=Mandelkern-Schultz-99] have optimal power in the case of measurement of a bounded quantity and produces always reliable Confidence Intervals. For these reasons, it should be preferred over the Unified Approach [\cite=Feldman-Cousins-98], which is however better than the Central Intervals method. Worse of all is the method of Upper Limits.