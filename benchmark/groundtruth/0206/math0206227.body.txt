Lemma Proposition Corollary Conjecture Definition Example

Proof

Convergence of the Poincaré constant

Introduction and results

Poincaré (or spectral gap) inequalities provide a relationship between L2 norms on functions and their derivatives.

Given a random variable Y, define the Poincaré constant RY:

[formula]

where H1(Y) is the space of absolutely continuous functions on the real line such that   g(Y) > 0 and [formula].

RY will not in general be finite, however it will be finite for the normal and other strongly unimodal distributions (see for example Klaasen (1985), Chernoff (1981), Chen (1982), Cacoullos (1982), Nash (1958), Borovkov and Utev (1984)).

We will exploit various relationships between the Poincaré constant and Fisher information:

For a random variable Y with smooth density p, define the score function ρY(y)  =  p'(y) / p(y), and Fisher information [formula].

Notice that for a given Y, if g is a local maximum of [formula] then for all functions h and small t:

[formula]

so multiplying out, [formula], which can only hold in an interval around zero if:

[formula]

Integration by parts implies therefore that g  =    -  Rg(ρYg'  +  g''), so local maxima correspond to eigenfunctions of the Laplacian DYg = (ρYg'  +  g''), and the global maximum to the least strictly negative eigenvalue (hence the alternative name of spectral gap inequality).

The Poincaré constant can be infinite. For example, consider the discrete random variable, where [formula]. Then, we can choose g such that g'( - 1)  =  g'(1) = ε, but g( - 1)  =   - 1, g(1) = 1, so that   g(X) = 1, but [formula]. This argument will work for any discrete random variable, indeed any random variable whose support is not an interval.

However, our first main result shows that discrete random variables perturbed by small normals have a finite Poincaré constant:

Consider X, a random variable with variance σ2 taking a finite number of values with probabilities [formula] respectively, and Zτ an indepedent normal with mean zero variance τ. Then Yτ  =  X  +  Zτ satisfies a Poincaré inequality with constant

[formula]

See Section [\ref=sec:proofmix].

Note that part 8 of Theorem 1.1 of Utev (1992) also shows that RYτ is finite. However, our bound has an explicit dependence on σ2, and so has independent interest.

In a paper by Johnson and Barron (2002), we show that finiteness of the Poincaré constant gives an explicit rate of convergence of relative entropy distance in the Central Limit Theorem. This is a strong result, and implies convergence in L1.

Theorems 2, 3 and 4 of Borovkov and Utev provide the following results:

For the constant RX defined above:

RaX + b  =  a2RX

If X,Y are independent, then RX + Y  ≤  RX  +  RY

RX  ≥    (X), with equality if and only if X is normal

If RX is finite then [formula], so X has moments of all orders.

If RXn  /    (Xn)  →  1, then [formula] ,where Z is normal, for any continuous w with |w(t)|  <   exp (c|t|), for sufficiently small c.

The first three properties are reminiscent of those of Fisher Information - a subadditive relation holds and the minimising case characterises the normal distribution. In analogy with the approach to the Central Limit Theorem developed by Brown (1982) and Barron (1986), we add an extra term into the subadditive relation RX + Y  ≤  RX  +  RY, which is sandwiched as convergence occurs. This gives us an answer to the question posed by Chen and Lou (1990), of identifying the limit of the Poincaré constant in the Central Limit Theorem. This was also answered by Utev (1992), though without the explicit rate of convergence that we provide.

Consider [formula] IID, with R = RXi and I = I(X) finite. Defining [formula], then there exists a constant C, depending only on I and R, such that RUn  -  1  ≤  C / n.

See Section [\ref=sec:proofconv].

We can argue that this is the best possible rate, up to the choice of the constant. Considering g(x)  =  x2 - 1, we know that [formula]. Since [formula], if RUn  -  1  =  f(n) / n then [formula], so [formula] and hence does not tend to zero.

Since we can perturb random variables by adding small normals to ensure that the Fisher information is finite, we use this to prove a strong form of the Central Limit Theorem.

If [formula] are IID random variables with mean 0, variance σ2 and finite RX = R, then [formula] has the property that:

[formula]

where Z is standard normal, for any continuous w such that w(t)  ≤   exp c|t|, where [formula].

Given a random variable U, define Uτ  ~  U  +  Zτ. Now, since RUτn  ≤  RUn  +  τ  ≤  R  +  τ, by Lemma [\ref=lem:borbas].4, taking τ0 such that [formula]:

[formula]

for all n, if τ  ≤  τ0,c  <  c0.

Now, since Uτn have uniformly bounded Fisher information I(Uτn)  ≤  I(Zτ)  =  1 / τ, and RUτn  ≤  R + τ, Theorem [\ref=thm:rconv] implies that [formula].

Hence, since:

[formula]

we need only show that given ε, [formula] for τ small enough. This follows by uniform integrability arguments (see Theorem 25.12 of Billingsley), since [formula], for some small p, and since w(Uτ) converges weakly to w(U).

Finiteness of R for mixtures of normals

Without loss of generality, consider X taking a finite number of values [formula] with probabilities [formula] respectively, where [formula].

We introduce the 'squared span' [formula], and write p for min sps.

By Theorem 1 of Borovkov and Utev, we need to check that for some R and all x, the density fτ of Y satisfies:

[formula]

Since [formula], the LHS of Equation ([\ref=eq:bornec]) becomes (defining [formula] and an + 1  =    -    ∞  ):

[formula]

since un = 0, so for each interval Ij  =  (x - aj,x - aj + 1) we need to consider bounds on min y∈Ijy2.

We write r for the index such that ar  ≤  x  <  ar - 1.

First, we consider x  ≥  0, where we can distinguish 3 cases: for j < r; x - aj + 1  <  0, so for y∈Ij:

[formula]

For j = r; y∈Ij means that: y2  ≥  0  ≥  (x  -  aj)2  -  (aj - 1  -  aj)2  ≥  (x  -  aj)2  -  M.

For j  >  r; x  -  aj  >  0, so for y∈Ij: y2  ≥  (x  -  aj)2.

Hence for all j, min y∈Ijy2  ≥  (x - aj)2  -  M, so:

[formula]

In Lemma [\ref=lem:techn], we prove two technical results, that uj(aj  -  aj + 1)  ≤  σ2, and that Mp  ≤  2σ2. This allows us to deduce that for x  ≥  0:

[formula]

as required.

Similarly, for x  ≤  0, we deduce that for y∈Ij,  min y∈Ijy2  ≥  (x - aj + 1)2 - M, and thus:

[formula]

Using the notation above: uj(aj  -  aj + 1)  ≤  σ2, and Mp  ≤  2σ2.

Note that [formula].

For aj + 1  ≥  0: [formula]

For aj  ≤  0: [formula]

For aj + 1  ≤  0  ≤  aj: [formula]

For the second part, we consider two cases, firstly where M  =  a2s  -  a2s  ±  1. In this case:

[formula]

Alternatively, if M  =  (as  -  as + 1)2 then:

[formula]

Convergence of the Poincaré constant

We establish an explicit rate of convergence of the Poincaré constant, using projection inequalities similar to those in Johnson and Barron (2002)

Given independent random variables X,Y with Poincaré constants RX,RY, for any function g:

[formula]

and hence RX + Y  ≤  RX  +  RY.

Without loss of generality, we can consider g such that [formula], and define [formula], which thus also has mean zero. Now:

[formula]

To consider the second term, we use the score function ρY and define:

[formula]

where by the Stein equation, [formula]. Further, by Cauchy-Schwarz:

[formula]

so that:

[formula]

and writing [formula], we obtain:

[formula]

which, rearranging, leads to:

[formula]

Next we need a Lemma which again uses the idea that if g' is nearly constant, then g is close to linear. We'd like to apply it to the optimal g, which achieves the maximum in Definition [\ref=def:borov]. However, rather than use compactness arguments to show such a function exists, we can instead use a 'good' g instead.

For any random variable W with mean zero, and any function g such that [formula] has a local maximum at t = 0:

[formula]

Without loss assume that [formula], and write [formula] and [formula] implies:

[formula]

since [formula], and since by Lemma [\ref=lem:borbas].3,   (W)  ≤  RW.

Note, we can come up with tighter bounds: for example taking h(W) = W in Equation ([\ref=eq:intpart]), [formula]. Hence [formula]. This implies that:

[formula]

However, Lemma [\ref=lem:gbd] is sufficient for our purposes.

We consider convergence along the 'powers of 2' subsequence Sk  =  U2k, which implies convergence for the whole sequence by subadditivity.

For all k, 1  ≤  RSk  ≤  R, and ISk  ≤  I. Taking [formula] and [formula] (an identical copy) in Lemma [\ref=lem:rdec] implies (since [formula] and [formula]) that for any g:

[formula]

Now, given W  =  Sk + 1, we can find h such that [formula]. Since [formula] tends to   (W) at ±    ∞  , and has one maximum t0 and one minimum, we can find g(W)  =  h(W) + t0W, which satisfies the conditions of Lemma [\ref=lem:gbd]:

[formula]

where x+  =   max (x,0) and C = 18R2Sk + 1(1 / RSk + I(Sk))  ≤  18R(IR + 1). That is, since ε is arbitrary,

[formula]

Note that since RSk is decreasing and bounded below, successive differences tend to zero, and thus RSk  →  1.

To obtain a rate, write uk  =  (RSk  -  1) / C, Equation ([\ref=eq:dynsys]) gives uk(1  +  uk)  ≤  uk - 1. Since uk are decreasing: [formula], and hence: u2n  ≤  un - 1  -  un, u2n  ≤  un - 2  -  un - 1, u2n  ≤  un - 3  -  un - 2 and so on. Summing, we obtain that for m  ≤  n: (n - m)u2n  ≤  um  -  un  ≤  um. Taking n = 2r, m = 2r - 1 implies that:

[formula]

Repeating this N times, we deduce that (since uk  ≤  u1  ≤  1):

[formula]

so if N  =  r - 2, then u2r  ≤  4 / 2r, and we can 'fill in the gaps' by subadditivity, to show that uk  ≤  16 / k for all k.

Acknowledgements

The author is a Fellow of Christ's College, Cambridge, who helped support a trip to Yale University during which many useful discussions with Andrew Barron took place. Yurii Suhov of Cambridge University and Alexander Holroyd of UCLA provided useful advice, and the anonymous referee provided several extremely useful improvements to the proofs.