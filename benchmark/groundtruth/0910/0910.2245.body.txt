Searching for Minimum Storage Regenerating Codes

Introduction

Erasure codes can be used in storage systems to efficiently store data while protecting against failures much more efficiently than replication. We can divide a file of size M into k pieces, each of size M / k, encode them into n coded pieces using an (n,k) maximum distance separable (MDS) code, and store them at n nodes. Then, the original file can be recovered from any set of k coded pieces. This is optimal in terms of the redundancy-reliability tradeoff because k pieces, each of size M / k, provide the minimum data for recovering the file, which is of size M.

In practical distributed storage systems based on (n,k) MDS codes, we are often faced with the repair problem [\cite=DGWR_Infocom07]: If a node storing a encoded piece fails or leaves the system, in order to maintain the same level of reliability, we need to create a new encoded piece and store it at a new node, but we can only access other encoded blocks. One straightforward way to do so is to let the new node download k encoded pieces from a subset of the surviving nodes, reconstruct the original file, and compute the needed new coded piece. In this process, the new node incurred a total network traffic of γnaive = k  ×  M / k = M.

Recent prior work [\cite=DGWR_Infocom07] showed that it is possible to reduce this repair bandwidth below M and developed information theoretic lower bounds and achievable schemes. At this point, need to distinguish between two different repair problems: In this paper we consider the problem of systematic repair [\cite=Wu09] (also called exact repair [\cite=Rashmi09]) where we require that it is exactly the same block that is reconstructed after a failure. This is in sharp contrast to functional repair i.e. only requiring that the new block is linearly independent and hence forms a good erasure code jointly with the other existing blocks [\cite=DGWR_Infocom07]. Systematic repair is a strictly harder problem, which however is of great practical interest since in most practical storage systems reading parts of the data is the most common operation and it should not require decoding of blocks if no failures have occurred (see also [\cite=ICDCS09] for a practical analysis).

As was shown in [\cite=DGWR_Infocom07], the functional repair problem is equivalent to a multicasting problem on an information flow graph that adds all reconstruction points as virtual data collectors who demand all the data. Using cut-set arguments (which are achievable for multicasting [\cite=Ahlswede00] [\cite=RLNC]) we can determine the minimum repair bandwidth for MDS codes (codes matching this bound are called Minimum Storage Regenerating codes [\cite=DGWR_Infocom07]):

[formula]

if the new node is allowed to connect to d = n - 1 surviving nodes, after one failure. Note that throughout this paper, we are only considering the minimum storage point and we do not address other points in the storage-repair tradeoff curve of [\cite=DGWR_Infocom07].

The systematic repair problem, however, is equivalent to a network coding problem where there are receivers who want all the data (the data collectors) and receivers who want subsets of the data (the nodes who will replace the failed ones are now also sinks with a demand of the lost blocks). This reduction shows exactly why systematic repair is a much harder problem and careful coefficient selections are required. Further, the cut-set bound [\ref=cutset] is no longer necessarily tight and the optimal systematic repair rates are unknown for general (n,k). Recent work [\cite=Wu09] has developed an achievable scheme that is based on aligning the undesired subspaces, similarly to recent ideas for the interference channel (see e.g.  [\cite=CadambeJ:08]) that have an achievable repair rate of

[formula]

achieved by sub-packetizing each packet into q = n - k blocks of size M / kq and communicating a total of (k - 1)q + 1 blocks from d = n - 1 surviving nodes.

It is easy to verify that the achievable rate ([\ref=IR_rate]) is matching the cut-set lower bound ([\ref=cutset]) for k = 2 and k = n - 1 but the other cases remain unknown. In this paper we present a searching approach to find systematic MSR codes that match the information theoretic lower bound ([\ref=cutset]). Our search found some optimal systematic (5,3) MSR codes (the existence of which was previously unknown), the simplest of which is shown in figure [\ref=fig:53repair]. The key property that allows optimality is that when one of the undesired subspaces is aligned (as done in the scheme of [\cite=Wu09]), the other is also aligned because of the selection of coefficients of the code. This remarkable property is only possible if the code coefficients are carefully chosen and is closely linked to the size of the finite field.

To search the space of potential codes in feasible amounts of time, we reduce the search space in several ways. We impose an additional condition that restricts the type of codes that we consider. This allows us to consider only highly symmetric codes that can be more concisely specified. We specify a code in a simple alternative way, using additional recovered coefficients rather than transmission coefficients. The space of codes can be searched more easily and efficiently when codes are specified this way. Finally, we use linear transformations to relate codes to each other and place them into equivalence classes. This allows us to check only one code from each equivalence class.

Definitions and Notation

The storage networks that we are concerned with contain n equivalent storage nodes. We wish to store M bits of data in the network, where M is k times the size of one of the storage nodes. Because of this, we say that the network has k source nodes.

Lower bound on recovery bandwidth

During the recovery process, [formula] bits of data must be transmitted, where d is the number of nodes providing data [\cite=DGWR_Infocom07]. We are interested in the case where d = n - 1, so this bound becomes [formula]. There are n - 1 nodes that each contain [formula], so each node is transmitting [formula] of its contents. Because of this, we store n - k packets of data in each storage node. We break the source data up into packets of the same size and each storage packet will be some linear combination of the k(n - k) packets of source data.

Notation

We use several matrices to represent the data and coefficients used in an MSR code.

The ith storage node contains [formula], the original data multiplied by the storage coefficients for that node.

Independence

The storage nodes of the code are independent if any k nodes can reproduce the original data. That is, for all combinations of k storage nodes, there is a matrix [formula] such that

[formula]

for any value of D. An equivalent condition is that for each combination of k nodes, the matrix of storage coefficients must have full rank, i.e. a nonzero determinant.

Recovery

When node j fails, the ith node transmits [formula]. The code allows the recovery of node j if there is a matrix [formula] that recreates the lost node from the transmitted vectors:

[formula]

for any value of [formula]. Therefore, [formula] drops out of both the independent and recovery conditions, and we can focus on the coding coefficients only. We can also ignore the [formula] matrices because from the recovery condition we can see that in a working code the [formula] matrices are fully specified by the [formula] and [formula] matrices. With these two conditions, we can determine whether a set of [formula] and [formula] matrices form a code.

General Position

A stronger version of the independence condition is also interesting. A collection of n-dimensional vectors is in general condition if every combination of n vectors is full rank. To apply this condition to a code, we consider the rows of the [formula] matrices as vectors. In order to satisfy the independence condition there are [formula] combinations of vectors that must be full rank, while there are [formula] that must be full rank for the vectors of the code to be in general position.

Rotationally Symmetric Codes

To reduce the total number of coefficients, we consider codes whose [formula] matrices are related to each other by a simple transformation. Let [formula] be an k(n - k)  ×  k(n - k) matrix such that

[formula]

and let

[formula]

A discussion of the [formula] matrices themselves can be found in [\cite=DanThesis]. This reduces the number of storage coefficients needed to specify a code by a factor of n, reducing the search space exponentially.

Recovery Condition

This makes the recovery condition

[formula]

We can replace [formula] with [formula], reorder the rows of the transmitted coefficient matrix, and replace [formula] with [formula]. Now there is only one recovery condition.

[formula]

This is an improvement of a factor of n.

Independence Condition

Similarly, when checking independence, we only need to check combinations that include the first node.

[formula]

This reduces the number of conditions from [formula] to [formula]. This is an improvement of a factor of [formula].

Example

[formula]

The [formula] matrices gives us the transmitted vectors.

[formula]

[formula]

[formula]

From these we can complete the code by calculating [formula].

[formula]

Additional Recovered Coefficients

The [formula] matrices cannot be eliminated in a similarly simple manner, but their contribution to the code to be represented in alternative way. During recovery n - 1 vectors are transmitted to the lost node, but the original [formula] matrix has only n - k rows. Thus k - 1 additional vectors of coefficients are recovered. Specifying these vectors allows the [formula] matrices to be determined.

Let [formula] be the k - 1  ×  k(n - k) matrix that contains the additional rows recovered when node j is lost. Let

[formula]

be the n - 1  ×  k(n - k) matrix that contains all of the rows recovered when node j is lost. Then [formula] projects vectors into [formula]. A row vector [formula] is in [formula] if the projection does not change the vector, or

[formula]

This can be rewritten as

[formula]

[formula] gives the difference between the original vector and the projection. This is a projection to the (k - 1)(n - k - 1)-dimensional space [formula]. The only potentially useful vectors to transmit during recovery are those in [formula], so we need to ensure that the transmitted vector [formula] must satisfy

[formula]

Thus the choices for [formula] are the vectors in the nullspace of [formula].

Unrecovered Coefficients

Let [formula] refer to a basis that spans [formula]. Now we can rewrite the projection as [formula]. Now we can say [formula] must satisfy [formula], which reduces to [formula]. Thus the null space of [formula] is the same as the nullspace of [formula]. [formula] is a (n - k)  ×  (k - 1)(n - k - 1) matrix, so its nullity is at least (n - k) - (k - 1)(n - k - 1) or 1 + (n - k - 1)(2 - k). However, if 2 < k < n - 1, this bound does not force the nullity to be positive. This bound does explain why it is so easy to find codes when k = 2.

Example: Obtaining [formula] from [formula]

Now we can see how the [formula] vectors were discovered in the previous example. Let [formula]. Note that [formula] as required. We apply [formula] to the other [formula] matrices and find the [formula] vectors that satisfy [formula].

[formula]

[formula]

[formula]

For the n = 4, k = 2 case, nearly all choices for [formula] produce a working code. This is not the case for larger coefficients.

Transformations of codes and equivalence classes

Row transformations

Suppose we have an invertible (n - k)  ×  (n - k) matrix [formula] and a working code defined by [formula] and [formula] matrices. Then the matrices [formula] and [formula] also define a working code. For recoverability we have

[formula]

and for independence we have

[formula]

The row transformation is applied to the [formula] matrices from the left and the rotation matrix in a rotationally symmetric code is applied from the right. Thus, applying the transformation to a rotationally symmetric code results in another rotationally symmetric code that uses the same rotation matrix. We can define codes to be equivalent if they are related by a row transformation. Testing only one code from each equivalence class reduces the search space by k2 dimensions.

Column transformations

The same technique can be applied to the columns. If we have an invertible k(n - k)  ×  k(n - k) matrix [formula] and a working code defined by [formula] and [formula] matrices, then the matrices [formula] and [formula] also define a working code. For recoverability we have

[formula]

and for independence we have

[formula]

In a rotationally symmetric code, the column transformation and the rotation are both applied from the right, so they interact.

[formula]

So the new code is rotationally symmetric with a different rotation matrix, [formula]. This means that we can use a simple rotation matrix when searching for codes and simultaneously check all rotationally symmetric codes that use similar rotation matrices.

This also makes it possible to put any rotationally symmetric code into systematic form. When a code is in systematic form, the first k storage matrices can be stacked to form an identity matrix.

[formula]

Finding the transformation that puts a code into systematic form is simple. It is simply the inverse of the stack of first k storage matrices.

[formula]

Example: Systematic Form

[formula]

The same [formula] vectors as before will work for recovery.

[formula]

[formula]

[formula]

The same [formula] matrix as before will also work.

[formula]

Search Procedure

When searching for codes of a given n and k over a finite field, this procedure was used. Iterate over [formula] matrices in a way that ensures that exactly one matrix from each row transformation equivalence class is produced. For each [formula] matrix, produce the collection of n [formula] matrices using a single simple column rotation matrix. Then test the independence condition. Test it before the recovery condition because it requires only [formula] matrices. If the independence condition is met, iterate over the space of potential additional recovered coefficients. For each [formula] matrix produced by this process, check the recovery condition. If the condition is met, this is a code.

Search results

n = 4, k = 2

These coefficients are small enough to all several fields to be searched exhaustively. We have searched the prime fields up to GF(13). There are no rotationally symmetric codes in GF(2), but in all larger fields codes are extremely easy to find. In all of these fields, nearly all of the potential codes that satisfy the independence condition also satisfy the recovery condition. As the field size increases, larger and larger fractions of the potential codes satisfy the independence condition. In GF(3), 22% of potential codes satisfy the independence condition, and of these all satisfy the recovery condition. In GF(13), 78% of potential codes satisfy the independence condition and of these 92% also satisfy the recovery condition.

n = 5, k = 3

For these coefficients, codes were not previously known. We have exhaustively searched GF(2), GF(3), GF(4), and GF(5) and randomly searched in larger fields for rotationally symmetric codes. We found codes in GF(3), GF(4), GF(7), and larger fields, but none in GF(2) or GF(5). While the codes we have found in smaller fields are not composed of vectors in general position, we found a code in GF(17) that is. Several of these codes are given in the appendix. The full descriptions can be found in [\cite=DanThesis].

n = 6, k = 3

For these coefficients, We have yet to find any codes. In GF(3), only about 1% of potential codes satisfy the independence condition. In GF(4) this number is about 14% and in GF(5) it is about 30%.

Acknowledgments

I would like to thank my thesis advisor, Professor Tracey Ho, for introducing me to this topic and guiding me throughout the year. I would also like to thank Dr. Georgios Dimakis for helping me to understand his results regarding the problem and for sending me in fruitful directions. Special thanks to Sherwin Doroudi for his help during a variety of discussions throughout the year, to Mason Smith for his help with linear algebra, and to Shengbo Xu for his help during the writing process.

Appendix

Rotation Matrices

The simplest way to construct a matrix [formula] with period n is to have it rotate n of columns of any matrix it is applied to. If the dimension is also n, this matrix is

[formula]

Call this kind of rotation matrix a simple rotation matrix. If the dimension of the matrix is larger than the period, we can build the matrix by stringing multiple basic rotation matrices together along the diagonal.

[formula]

The period of the entire matrix will be the least common multiple of the periods of the basic rotation matrices (a, b, and c).

If a matrix satisfies

[formula]

then it satisfies

[formula]

We are only interested in real matrices so

[formula]

for odd n and

[formula]

for even n. Thus its eigenvalues satisfy

[formula]

Additionally, [formula] which implies that [formula]. Thus all of the eigenvalues must be roots of unity and they must occur in sets that sum to a rational number. This greatly restricts the number of classes of rotation matrices.

The eigenvalues of a simple rotation matrix of period n are the n roots of unity. When a rotation matrix is built from simple rotation matrices, each simple matrix contributes one eigenvalue equal to 1.

Rotation Matrices with too many eigenvalues equal to 1

If more than k of the eigenvalues of the rotation matrix [formula] are equal to 1, it is impossible to use the [formula] to construct a code that satisfies the independence condition. Suppose j of the eigenvalues of [formula] are equal to 1. Decompose [formula] as

[formula]

where the columns of [formula] are the eigenvectors of [formula] and [formula] is a diagonal matrix of the eigenvalues with all of the 1's in the upper left. Thus [formula] has the form

[formula]

Let

[formula]

Now

[formula]

Now let

[formula]

where [formula] contributes the left j columns and [formula] contributes the right k(n - k) - j columns. Thus

[formula]

The matrix obtained by stacking [formula] through [formula] is

[formula]

This matrix must be full rank for the independence condition to be satisfied. [formula] is full rank so we can ignore it. The other part of the matrix can be transformed by row operations into

[formula]

If j > k, the block of zeros in the lower left includes at least one entry on the diagonal. Then further row operations can be performed on the upper left and lower right blocks independently to make the whole matrix upper triangular. At this point, the determinant of the matrix is the product of the values on the diagonal. At least one of these values is 0, so the determinant is 0. This means that the independence condition cannot be satisfied if j > k and no codes can be constructed using such a [formula] matrix.

As a result of this restriction, there are pairs of n and k for which it is impossible to construct a workable [formula] matrix by putting together simple rotation matrices. n = 7 and k = 4 are such a pair. 7 is prime so there are no shorter cycles that can be used in the construction of the matrix. k(n - k) = 12 is the dimension of [formula], do that leaves 5 entries on the diagonal that can only be filled with 1's. Thus any [formula] matrix will will have at least 6 eigenvalues equal to 1.

Unrecovered Coefficients when n = 4, k = 2

Suppose that you have [formula] along with [formula] that recovers [formula] and [formula] that recovers [formula]. Then we know that [formula] and [formula]. If we let [formula] with [formula], then the set of coefficients [formula] recovers [formula]:

[formula]

Thus for a given [formula], the [formula] vectors that create a working code form a vector space.

(5,3) code over GF(3)

[formula]

[formula]

[formula]

[formula]

[formula]

[formula]

(5,3) code over GF(7)

[formula]

[formula]

[formula]

[formula]

[formula]

[formula]

General Position (5,3) code over GF(17)

[formula]

[formula]

[formula]

[formula]

[formula]

[formula]

(5,3) code over GF(3) in systematic form

[formula]

[formula]

[formula]

[formula]

[formula]

(5,3) code over GF(7) in systematic form

[formula]

[formula]

[formula]

[formula]

[formula]