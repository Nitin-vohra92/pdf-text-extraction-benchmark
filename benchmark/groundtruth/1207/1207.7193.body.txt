Canalizing Boolean Functions Maximize the Mutual Information

Introduction

In systems and computational biology Boolean networks are widely used to model various dependencies. One major application of such networks are regulatory networks [\cite=K69] [\cite=CKR04]. In this context Boolean networks in general and consequently Boolean functions have been extensively studied in the past.

One focus of research is the information processing in such networks. It has been stated, that networks, whose dynamical behavior is critical, i.e., at the edge between stable and chaotic behavior, have somehow an optimized information processing ability [\cite=L90] [\cite=SML96] [\cite=LF00]. Further, it has been shown, that biologically motivated networks have critical dynamics [\cite=K69] [\cite=K74]. The dynamics depend on two major characteristics of the network, the topology and the choice of functions. Here, so-called canalizing Boolean functions seem to have a stabilizing effect [\cite=KPS03] [\cite=KPST04]. It has been shown [\cite=SJ08] [\cite=KFG11], that regulatory networks, such as the regulatory network of Escherichia Coli, consist mainly of canalizing functions.

One way to formalize and to quantify information procession abilities is the well-known mutual information based on Shannon's theory [\cite=S48]. The mutual information is a measure for the statistical relation between some input and output of a system, as for example shown by Fano's inequality [\cite=HV94]. It has already been applied in the context of Boolean networks and functions, such as cellular automata [\cite=L90], random Boolean networks [\cite=LF00] [\cite=RKL08] and iterated function systems [\cite=CY90].

In this paper, we use Fourier analysis to investigate Boolean functions. This has first been addressed in [\cite=G59]. Recently a relation between the mutual information and the Fourier spectra has been derived [\cite=HSB]. In particular, it has been shown that the mutual information between one input variable and the output only depends on two coefficients, namely the zero coefficient and the first order coefficient of this variable. Further, a relation between the canalizing property and these coefficients has been derived [\cite=KSB11]. In this paper, we combine two approaches to show that canalizing functions maximize the mutual information for a given expectation value of the functions output.

The remainder of this paper is structured as follows: In the next section we will introduce some basic definitions and notation. In particular we will introduce the concepts of Fourier analysis as far as they are relevant to this work. In Section [\ref=sec:MI] our main results are proven in two steps. First, we will address Boolean function with uniform distributed input variables, secondly, the result is extended to the more general product distributed case. This is followed by a short discussion of our finding (Section [\ref=sec:discussion]), before we will conclude with some final remarks.

Basic Definitions and Notation

Boolean Functions and Fourier Analysis

A Boolean function (BF) f∈Fn  =  {f:Ωn  →  Ω} with Ω  =  { - 1, + 1} maps n-ary input tuples to a binary output. Let us consider [formula] as an instance of a product distributed random vector [formula], i.e., its probability density functions can be written as Furthermore, let μi be the expected value of Xi, i.e., [formula] and let

[formula]

be the standard deviation of Xi. It can be easily seen that It is well known that any BF f can be expressed by the following sum, called Fourier-expansion, where [formula] and

[formula]

For [formula] we define [formula]. The Fourier coefficients (U) can be recovered by

[formula]

If the input variables Xi are uniformly distributed, i.e., μi = 0 and σi = 1, Eq. ([\ref=eq:phi]) reduces to and consequently, as [formula] for all [formula], Eq. ([\ref=eq:fourier]) reduces to

Canalizing Function

A BF is called canalizing in variable i, if there exists a Boolean restrictive value ai∈{ - 1, + 1} such that the function

[formula]

for all x1,...xi - 1,xi + 1....xn, where bi∈{ - 1, + 1} is a constant.

As shown in [\cite=KSB11] the Fourier coefficients of canalizing functions then fulfill the following conditions:

[formula]

Hence, as stated in [\cite=KSB11], a BF is canalizing in i, if and only if [formula] and ({i}) fulfill Eq. ([\ref=eq:cond:1]). Further, it can be easily seen, that in the uniform distributed case

[formula]

where [formula] gives the sign.

Mutual Information of Boolean Functions

The mutual information (MI) between two random variables is defined as:

[formula]

where

[formula]

is Shannon's entropy in bits of some discrete random variable X with its domain X. For the special case that |X| = 2, it reduces to the binary entropy function:

[formula]

with [formula]. Further, H(Y|X) is the conditional entropy between two discrete random variables X∈X and Y∈Y

[formula]

with

[formula]

It has been shown in [\cite=HSB], that the mutual information between one input variable i and the output of a boolean function is given as:

[formula]

The fact that the MI is only dependent on [formula] and ({i}) coincides with our statement in the previous section, that the canalizing property also depend on these two coefficients. Hence, we will only focus on those two Fourier coefficients in the following considerations. The remaining coefficient can be chosen arbitrarily and have no influence on our findings. Also, the number of input variables n does not restrict our investigations, it only determines the possible values [formula] and ({i}), since they are a multiple of 2- n.

Mutual Information under Uniform Distribution

For sake of clarity and ease of comprehension we will first focus on canalizing functions in the uniform distributed case. In the next section we will then generalize this result to product distributed variables.

Let f be a Boolean function with uniform distributed inputs. For a given and fixed [formula], the mutual information (see Eq. ([\ref=eq:MIi])) between one input variable i and the output of f, is maximized by all functions, which are canalizing in variable i.

Since [formula] is constant, the only remaining degree of freedom in Eq. ([\ref=eq:MIi]) is ({i}). First, we show that the mutual information is convex with respect to ({i}). Since the first summand of Eq. ([\ref=eq:MIi]) only depends on [formula] we can consider it as constant. We hence can focus on the second part, which we can write as:

[formula]

The binary entropy function h is concave, and since its argument is an affine mapping, [formula] is convex [\cite=Boyd04]. Finally the expectation is a non-negative weighted sum, which preserves convexity, hence the mutual information is convex.

Obviously for ({i}) = 0, the mutual information is minimized, hence, due to the convexity, the maximum can be found on the boundaries of the domain. The domain is limited by the non-negativity of the arguments of h, i.e.,

[formula]

Hence, the boundaries are given by

[formula]

It can be seen from Definition [\ref=def:canalizing], that all functions, which are canalizing in variable i, are located on the boundary of the domain of the mutual information. These function are constrained with:

[formula]

Since ai,bi∈{ - 1, + 1}, there exists four such types of functions on the boundary. Looking at their mutual information leads us to:

[formula]

which yields in:

[formula]

and hence:

[formula]

For the uniform distributed case we write:

[formula]

Due to [formula] and the symmetry of h, we finally get:

[formula]

Hence, the mutual information is independent from ai and bi, which concludes the proof.

Mutual Information under Product Distribution

The result from the previous section can be extended to product distributed input variables. We will see, that the probability distribution of the canalizing variable plays a key role in maximizing the MI.

Let f be a Boolean function with product distributed inputs. For a given and fixed [formula], the mutual information (see Eq. ([\ref=eq:MIi])) between one input variable i and the output of f, is maximized by all functions, which are canalizing in variable i, where ai and bi are chosen as follows

[formula]

The first part of this proof goes along with the proof of Proposition [\ref=prop:uniform], where we simply replace [formula] by [formula]. Hence, we can again show that the MI is convex and that the boundary consists of the canalizing functions. Starting from Eq. ([\ref=eq:mi_can]), we hence get

[formula]

where

[formula]

Obviously there are four possible sets of ai and bi. However, the for each choice of [formula] there exists two possible choices of ai and bi, of which only one maximizes the MI.

Lets first look at the possible combinations of ai and bi in dependence of [formula]. From Parsevals theorem we know hat

[formula]

and hence

[formula]

Solving that inequation for [formula] leads us to the possible sets of ai and bi:

[formula]

Hence, to maximize the MI, we have to minimize [formula] for each possible choice of [formula]. We can rewrite [formula] for all four combinations of ai and bi as follows:

[formula]

Now, lets assume [formula], i.e., bi =  - 1. Hence, have to compare [formula] and [formula] and search for the correct choice of ai which maximizes the MI. Lemma [\ref=lem:a1] (can be found in the Appendix) shows that this is achieved if choosing [formula].

If [formula], i.e., bi =  + 1 we have again to compare the choices of ai =  + 1 and ai =  - 1. As above ai must be chosen to be [formula] in order to maximize the MI (see Lemma [\ref=lem:a2] in the Appendix).

Now let [formula], hence we need to choose between bi =  - 1 and bi =  + 1. Lemma [\ref=lem:a3] (Appendix) shows that in this case the MI is maximized if [formula], which concludes the proof.

Discussion and Conclusion

To visualize our findings we plotted in Figure [\ref=fig:3d:uni] a 3D diagram of the mutual information of a BF with uniform distributed input variables versus [formula] and ({i}). In Figure [\ref=fig:2d:uni] we present a projection of the surface in the ([formula])-plane. It can be seen from these pictures that the canalizing function form the boundary of the domain of the MI. Further, the symmetry with respect to ai and [formula] can be seen. In addition it becomes visible, that the mutual information also depends of the actual zero coefficient. This is mainly due to the first term of the MI (Eq. [\ref=eq:MIi]), the entropy of the functions output.

In Figures [\ref=fig:3d:prod] and [\ref=fig:2d:prod] the same plots can be found for product distributed input variables, with pi = 0.3. Here, the skew of the mutual information towards the more probable canalizing value and the symmetry with respect to bi can bee seen.

Our findings show the optimality of canalizing functions with respect to information processing abilities. Further, it has been stated in literature [\cite=KPST04], that canalizing functions have a stabilizing influence on the network dynamics. This supports the conjectures [\cite=L90] [\cite=SML96] [\cite=LF00], that these two properties are closely related.

An open problem remains the impact of canalizing function on the mutual information between a set of variables and the function's output. One may presume that based on the results of this paper, that it is maximized by functions, which are somehow canalizing in all that variables.

Acknowledgment

This work was supported by the German research council "Deutsche Forschungsgemeinschaft" (DFG) under Grant Bo 867/25-2.

If [formula], then

[formula]

First, lets recall, that

[formula]

and

[formula]

Lets assume that μi > 0. Due to the concavity and the parabolic form of [formula] and [formula], they can intersect at most two times. Obviously, q( - 1) = r( - 1) = 0, and q( - μi) = 0  <  r( - μi). Hence, if the slope of r at [formula] is larger than the slope of s, then [formula] on the interval [formula].

Building the derivative of [formula] leads us to

[formula]

and similar

[formula]

One can see that

[formula]

which concludes the proof for μi > 0. The proof for μi < 0 goes along the lines as for μi > 0.

If [formula], then

[formula]

First, lets recall that

[formula]

and

[formula]

Now we assume, that μi > 0. Due to the concavity and the parabolic form of [formula] and [formula], they can intersect at most two times. Obviously, s( + 1) = t( + 1) = 0, and s(μi) = 0  <  t(μi). Hence, if the slope of t at [formula] is larger than the slope of s, then [formula] on the interval [formula].

Building the derivative of [formula] leads us to

[formula]

and similar

[formula]

One can see that

[formula]

which concludes the proof for μi > 0. The proof for μi < 0 goes along the lines as for μi > 0.

If μi  >  0 and [formula], then

[formula]

If μi  <  0 and [formula], then

[formula]

Lets first assume μi  >  0. One can easily see, that

[formula]

and

[formula]

Further,

[formula]

which due to concavity of t and r proofs the first part of the Lemma. The proof of the second part goes along the lines.