Relational Data Mining Through Extraction of Representative Exemplars

Abstract:

With the growing interest on Network Analysis, Relational Data Mining is becoming an emphasized domain of Data Mining. This paper addresses the problem of extracting representative elements from a relational dataset. After defining the notion of degree of representativeness, computed using the Borda aggregation procedure, we present the extraction of exemplars which are the representative elements of the dataset. We use these concepts to build a network on the dataset. We expose the main properties of these notions and we propose two typical applications of our framework. The first application consists in resuming and structuring a set of binary images and the second in mining co-authoring relation in a research team.

Keywords:

relational data, data mining, representative, exemplar, clustering, network, Borda.

Introduction

The data mining is interested in discovering knowledge from data. Nowadays finding interesting patterns or structures is a crucial task in the field of data analysis. Thus the paper addresses the problem of the extraction of representative elements from a dataset. This problem presents a significant interest when designing recommendation systems [\cite=Pazzani2007], selecting leaders or specimens [\cite=Brun2010], community detection [\cite=Newman2004], customer Relationship analysis [\cite=Tuzhilin2012] or sub-sampling.

The classical ways to determine representative elements refer to the task of data clustering [\cite=Alfred2010]. The goal is to partition of the dataset. Then the representative elements are the prototypes of the clusters. They can be chosen as the average elements of each cluster or selected after a random initialization step. For instance, when using k - means or k - centers algorithms (see [\cite=Jain1999] for a review of clustering methods including k - means algorithm), the centers of the obtained clusters provide the prototypes of the dataset. The prototypes are first randomly selected and the algorithms iteratively refine the set of prototypes. The final elements are quite sensitive to the initial selection. Moreover k - means algorithm leads to average prototype which are not "real" elements of the initial dataset. this kind of methods is not satisfactory. The lacks of all the approaches based on clustering are multiple. Firstly the partition into clusters is predate to the extraction of representative elements and the clusters have to be validated and interpreted to justify the prototypes. Secondly the representative elements depend on the choice of clustering algorithms and the extraction of the prototypes depends on the implicit assumptions about the shape of clusters and data distributions. Moreover when one cluster contains more than one sub-population, only one prototype is extracted. Finally, in the case of clustering algorithms like k-means, the centers are not elements of the original dataset. They are average computed elements. How make a mean-element meaningful ? Most of the time, providing a non-existing element (virtual element like a mean-element) does not make sense.

In this paper the approach we present consists in extracting elements we called exemplars directly from the whole dataset, without any a priori clustering step (in one pass unlike [\cite=Frey2007]) . The exemplars summarize the dataset and are particular elements of the original dataset. Thus they are real data. These elements are as representative as possible of the whole set without any assumption on the shape or the density of data distribution (unlike in [\cite=Luhr2008]). To achieve the extraction of exemplar, we construct a degree of representativeness on the dataset. The exemplars are finally chosen as local maxima of the degree of representativeness. By fitting the locality parameter (in topological terms the scale factor) we adapt the scale to determine the number of exemplars.

The paper is organized as follows. In the first section we introduce the context and expose our method. We present the formal definitions of scores between data, the notion of standard and the concept of exemplars in the dataset. Then we show how to build a network of exemplars to visualize these notions. For each definition we present some interesting and remarkable properties (robustness, stability etc.) In Section 3, we provide two applications in very different context. Firstly we apply our method on a set of binary images. We compute scores and exemplars and we build the network to structure the dataset. The second application concerns the analysis of co-authoring in a research laboratory. We exhibit a co-authoring network that permits to visualize how researchers are really clustered and how they work together. Section 4 is a brief conclusion that outlines our main contributions and that expose our current and future works.

Method

Let Ω be a set of n elements in a multidimensional space. The n elements are qualitative, quantitative or mixed data. We assume that Ω is a relational data set without any underlying distribution. Let us describe the way we use to extract the exemplars of Ω structuring this set in a network. In this paper, the elements are called objects.

Pairwise Valued Relation

Ω is a relation dataset. Let us specify this relation. Let R be a pairwise valued relation on Ω. R is defined by : The use of a pairwise valued relation is very useful in data processing. A distance is a special case of this kind of relation. But a distance is frequently not available when processing qualitative data. Thus a relation is more widespread than a distance for pairwise comparisons of objects. In this paper, the value R(x,y) is also called the cost from x to y, indicating the generality of the relation. The relation must follow three trivial properties.

The relation must be total. This means that each pair of objects of Ω is valued by R.

The relation must be positive. The cost is a positive value for all pairs.

The cost from x to x is null forall x (i.e. [formula])

Unlike a distance, the relation does not necessarily respect the property of symmetry. R(x,y) may be different from R(y,x). For instance, if the cost from a point x to a point y is the time to go from x to y, then the cost from y to x could differ from the first one because of the slope, wind, flow, etc. Moreover, the relation does not respect the triangle inequality. A dissimilarity index gives a classical example of such a relation which does not respect the triangle inequality. x is dissimilar from y with R(x,y) and y is dissimilar from z with R(y,z) but x could be dissimilar from z with R(x,z)  >  R(x,y)  +  R(y,z). Such a relation can lead to a vote to designate exemplars within the dataset. Specifically, we can rank the objets of Ω taking into account the relation to set up votes between the objects themselves. The following subsection describes this procedure.

Score

In this paper, we select an exemplar object from Ω according to the Borda voting method [\cite=deBorda1781]. But firstly, we transform values of the relation into ranks [\cite=Barnett1976][\cite=Conover1981][\cite=David2003]. Let x be an object of Ω. All objects can be sorted by the ascending order of their costs relative to x. Let us note Rkx(y) the rank of y relative to x. The rank is obtained when sorting the set {R(x,z) / z∈Ω}. Using Borda method [\cite=deBorda1781][\cite=vanErp2000], the object x assigns a relative score to all objects of Ω. The score Scx relative to x is defined by: where n is the number of objects of Ω. Thereby the relative score is an integer and it lies between 0 and n - 1. The lower the cost from x to y, the higher the score of y relative to x. Computing all relative scores, each object x receives n relative scores corresponding to the votes of all objects of Ω (i.e. the n values Scy(x) with y∈Ω). Then the relative scores are aggregated to choose the winner of the voting procedure. The aggregate score is defined by: In this paper, the aggregation function is the mean function. Let us observe the aggregated scores in a relational dataset. Figure [\ref=dataSetScores] displays an example of a dataset with 120 two dimensional random samples (A). Euclidean distance is used as the pairwise valued relation between samples. The respective aggregated scores (B) confirm that the score increases when the sample approaches the center of the dataset, i.e. in the midst of this one.

Standard

The object with the highest aggregated score is the standard we propose. Let us observe some properties of the standard. Figure [\ref=StandardExemples] displays three datasets A, B, and C. Each dataset has 100 random samples (n  =  100). The aggregated scores are computed using Euclidean distance as pairwise valued relation. The maxima of aggregated score are respectively 68.75, 70.55, and 68.77 for A, B and C. Filled circles indicate the three respective standards with the highest aggregated scores. Figure [\ref=StandardExemples] confirms that each standard lies in the midst of its dataset.

When resampling the dataset using the bootstrap technique [\cite=Thomas2000], the standard could change. If it does not change, the extraction of this standard is robust against the resampling. Using many bootstraps, the highest frequency of the extracted standards indicates the stability of the standard when resampling. Our experiments using simulated data and real data show that the standard depends very weakly in the resampling. Figure [\ref=StandardFrequencies] displays the standards obtained when resampling the datasets (A), (B), and (C) of Figure [\ref=StandardExemples]. The initial datasets have 100 elements displayed with crosses. Stems with filled circles show the frequencies of the standards obtained with 200 bootstraps. The extracted standards remain in the center of respectively A, B, and C. The frequencies of the most frequent standards when resampling the 100 initial samples are respectively equal to 40%, 32%, 36%. These frequencies assess the stability of the standard with respect to the samples. Respectively 90%, 88%, and 90% of the dataset elements are never extracted as standards when resampling. Thus we assume that a standard gives a clue on the center of the dataset. Because the standard is a real element, it avoids the nonsense that the classical averages could produce with a virtual out-of-scope element outside of the data distribution. Note that the stability of the standard (i.e. the frequency of the most frequent standard) increases when the number of objects increases.

Let us examine the stability of the standard when outliers are feared. We simulate outliers that we append to an initial dataset. We consider that the standard extraction is robust against outliers when the extracted standard remains one of three most frequent standards of the initial dataset. In this paper we describe the study of robustness (see [\cite=Rousseeuw2003] for more details about the concept of robustness) using the datasets A, B and C of Figure [\ref=StandardExemples]. The outliers are random elements out of the range of the initial data domain. In this section, the domain is defined by elements of coordinates (x,y) where - 10  ≤  x  ≤  40 and - 15  ≤  y  ≤  15. Outliers are simulated in a larger domain defined by - 10000  ≤  x  ≤  40000 and - 15000  ≤  y  ≤  15000 (the initial limits are multiplied by 1000) excluding the elements that are too close from the initial domain by keeping the elements (x,y) where x  ≤   - 1000 or 4000  ≤  x and y  ≤   - 1500 or 1500  ≤  y (the limits of initial domain are multiplied by 100). We add such random outliers to an initial dataset until the extracted standards changes (i.e. until the extracted standard from the new dataset with outliers will not be one of the three most frequent standards of the initial dataset). When outliers are randomly generated in a such very large domain, the percentage of outliers could be higher than 200% without changing the initial standard. Then the standard is robust when the outliers are spread in a large domain. But the standard remains also robust when outliers are concentrated into only one duplicate object. When only one outlier is randomly generated in the very large domain, we could add up to 20% of out-of-range elements using this single outlier without changing the initial standard. Then we assume that the standard is particularly robust against outliers.

Exemplars and Networks

The standard is the only exemplar extracted from a dataset. But the dataset may be complex and it could require more than one exemplar to represent the whole set. This section describes how the dataset can be structured to retrieve these exemplars from the set. The first step consists in defining the neighborhood of each object within Ω. Let x be one of the n objects of Ω. Let k be a value between 0 and n. The k-nearest neighbors of x are defined using the ranks relative to x. Then the k - neighborhood of x in Ω is defined by: Thus Nk(x) is the set of k nearest objects of x. In a second step, each object x is associated with the neighbor having the highest aggregated score. Thus we define a link from x to its preferred neighbor. Each object x is linked to an object y. The links are defined by: In this definition, x is linked to y and y is generally different from x when Sc(y) > Sc(x). If Sc(x) is maximal inside Nk(x), then y = x and x is linked to x itself. These self-linked objects are simply called exemplars of Ω. Using the links, the dataset becomes a network where the nodes are the objects. The exemplars becomes the terminal nodes of this network (i.e. the roots of the trees forming the network). The exemplars depend on the value of k which influences the network configuration. In this paper, k is the size of the neighborhood we use. This parameter is called scale factor. Figure [\ref=networkScaleFactor] displays four networks obtained from the simulated dataset of Figure [\ref=dataSetScores] (A). The dataset has the 120 samples (n = 120). The four networks are configured using the scale factors 5, 10, 20, and 40. The exemplars are displayed with a filled circle, they are the terminal nodes of the networks. The numbers of extracted exemplars are respectively equal to 8, 4, 2 and 1. Distinctly the number of exemplars depends on the scale factor k. The following describes the influence of the scale factor.

Exemplars and Scale Factor

The higher the scale factor, the lower the number of exemplars. Moreover, when the scale factor increases from one to n, the number of exemplars decreases from n to one. Let us explain this property. When k = 1, N1(x) is the singleton equal to x. Therefore each object x is itself an exemplar of Ω (i.e. x is linked to x). Then the set of exemplars is Ω and the number of exemplars is equal to n. When k = n, Nn(x) is equal to Ω. Each object x is linked to the standard which has the highest aggregated score within Ω. Then the number of exemplars is equal to 1 the network becomes only one tree and the standard is its root. At the scale k, an exemplar x has the highest aggregated score within the neighborhood Nk(x) (i.e. within the k nearest neighbors of x). If k1  ≤  k2, then Nk1(x)  ⊆  Nk2(x). If x is an exemplar at the scale k2, then it is an exemplar at the scale k1. Therefore the number of exemplars necessarily decreases when the scale factor increases. Increasing the scale factor, some exemplars could disappear among those who were extracted. But an object never appears as an exemplar if it was not extracted at lower scale factor. Figure [\ref=duration] displays the duration of each exemplar when increasing the scale factor. The exemplars are extracted from Figure [\ref=dataSetScores] dataset (n = 120). When the scale factor is equal to 1, all the objects are exemplars. When the scale factor increases, some exemplars disappear and their duration is shortened. Only the standard is kept from scale 1 to the scale n. It has the longest duration equal to n.

At the scale k, we assume that the numbers of exemplars is smaller than n  -  (k - 1) where k is the scale factor and n is the number of objects of the dataset. At each scale k, we want to reduce the number of exemplars. When this number is equal to n - k + 1, we consider that the extraction of exemplars is suboptimal. This case is observed when k = 1 or k = n. In this paper, the scale factor becomes optimal when the difference between n - k + 1 and the number of extracted exemplars is maximum. Let koptimum be this optimal value of the scale factor we propose in this paper. Figure [\ref=functionScaleFactor] displays the numbers of exemplars according to the scale factor k. It uses the dataset of Figure [\ref=dataSetScores] (A) (n = 120). The scale factor increases from 1 to 120 and the number of exemplars decreases from 120 to 1. The numbers of exemplars is smaller than 121 - k. The difference between 121 - k and the number of exemplars is maximum when k = 9. The black filled circle shows this optimum value. Then four exemplars are extracted using k = 9.

Figure [\ref=optimumExemplars] displays the exemplars obtained with optimal scale factor from the datasets ((A), (B), and (C) on the Figure [\ref=StandardExemples]). The random datasets have 100 samples (n = 100). koptimum is respectively equal to 9, 7 and 10. The filled circles display the exemplars and a larger filled circle shows the standard.

Applications

This section presents applications of our method in two typical and very different contexts. The first application consists in extracting exemplars from a binary image database and building the graph of exemplars of this database. The second application present an analysis of the co-authoring in a research team by extracting exemplar authors and exhibiting the implicit structure.

Extraction of exemplars from a set of binary images

In this first application we consider a set of binary images contained in a database. The goal is to extract exemplar images from this database. The interest could be providing a set of resuming images or distinguishing subsets of images according to their content. The database is presented in the Table [\ref=Tab:imagettesScores]. In a first we construct the relation matrix by using the Asymetric Haussdorff Distance. Classical methods of clustering have to work with symmetric distance. They are inapplicable when distance from an image A to image B is not equal to distance from image B to image A. As we wrote at the beginning of this paper, the symmetry property is not required in our method.

Firstly, we compute the score of each image of the database. The table [\ref=Tab:imagettesScores] represents the images sorting by decreasing order of scores. Secondly we build the associated directed graph presented in Figure [\ref=Fig:IM_1] and representing the exemplars network (with a scale factor of 4). This graph show how the dataset is structured. We can observe that the connected components of this graph are grouping image according to the object they represent. The three images of the Table [\ref=Tab:imagettesExemplars] are the exemplars of this dataset and provide a good summary of the whole dataset.

Exploration of co-authoring network

The second application of our concept deals with publication data inside a laboratory, a research team or any other group of researchers. Co-authoring informations can be considered as relational data ([\cite=Mcgovern2003], [\cite=Neville2003]). In this work, we consider that the value of the relation from a researcher named Alice to a researcher named Bob is computed as the sum for each common publication of the product of the number of coauthor on the publication and the number of publication of Alice. This relation is not symmetric. In fact, generally, Alice can be the "preferred" co-author of Bob, but Bob is not necessarily the "preferred" co-author of Alice. This valued relation characterizes the "quality" of links between the members and takes account of their publication activity.

The dataset we used is the set of publications of the CReSTIC Laboratory (University of Reims, France) [\cite=Benassarou2010]. This informations are extracted from the web site of the laboratory and have been anonymized.

The graph of the Figure [\ref=Fig:CO1] represents this dataset. Each node is a lab member and each edge between two members represents one common publication. Different colors are used to represents the different teams that compose the laboratory (but this information is not used in the computation of the exemplars). Therefore the scale factor is not used in this application because the size of the neighborhood is implicitly fixed in the dataset (according to the number of co-author of each member of the team).

After computing the scores, we built the exemplars graph represented on the Figure [\ref=Fig:CO2]. The size of the node is proportional to its score. This graph is displayed using the same position for the nodes. In the Figure [\ref=Fig:CO3], the nodes are rearranged to propose a clearer visualization.

The graphs presented in the Figures [\ref=Fig:CO1], [\ref=Fig:CO2] and [\ref=Fig:CO3] show several interests of our method. The first interests is the simplification of the graph of the Figure [\ref=Fig:CO1]. When the numbers of vertices and edges are growing the graph becomes more unreadable. For big data, resuming and simplifying is a necessary task. The second interest is to exhibit such a sub-structure of the team (this task is called community detection in a network [\cite=citeulike:3070598]). The Figures [\ref=Fig:CO2] and [\ref=Fig:CO3] show how groups are connected, and which members are the most representative. The exemplars members are connecting the others and can be viewed as natural leaders (or natural mentors) according to their publications and their co-authors. It emphasizes the important (critical) position of some members in a research team. Incidentally, we can observe that the resulting clustering obtained by partitioning the graph in connected components is a little bit different of the real partitioning in sub-groups (represented by the different colors)

Conclusion

In the framework of data mining, this paper describes a new way for extracting exemplars from a relational dataset. The method we propose is based on a pairwise comparison assuming a coarse relation on the dataset. This approach is particularly adapted when no distance is available or meaningful in the data domain. Moreover the coarse relation between data does not need symmetry or transitivity properties. Thus the method is useful for any kinds of relational data. An aggregated score is defined from these pairwise comparisons. The paper defines the standard which is the sample with the highest score. Simulations show the robustness of the standard against outliers and the stability of the standard when resampling the dataset. Thus these results confirm the standard as a robust location estimator. Moreover the aggregated score is used to extract exemplars which are real objects. Then our approach of location estimator avoids the drawbacks of average objects which are meaningless when processing qualitative data. Using a score based on the pairwise comparison, we define the k nearest neighbors of each datum. This approach permits us to extract exemplars depending on this k value. We state that the number of local exemplars decreases from n to 1 (n is the number of data samples) when k value increases from 1 to n. Thus k is considered as a scale factor. The method we propose allows us to explore the dataset through different scales. We can adjust the k value for extracting a reduced number of exemplars. An automated approach is proposed to determine an optimal number of exemplars. On top of the extraction of exemplars, the method proposes to design a network. The paper shows that the network is reconfigured when the scale factor changes. The network eases the explanation of the exemplar roles in the dataset. When the scale factor increases, some exemplars could disappear keeping the most important ones (i.e. the exemplars which are important nodes for connecting some data).

In future works we propose to use the fuzzy set theory as in [\cite=Blanchard2010] to generalize our framework in the case of fuzzy relation, when ranking data is not easy. The major way we would to explore is the area of Social Network Analysis. We are convinced that our concept of exemplar could be a significant tool for extracting leaders or mentors in social network and improve recommendation systems. Our concept of degree of representativeness should be compared to the different definitions of centrality in a network [\cite=Pfeiffer2010].