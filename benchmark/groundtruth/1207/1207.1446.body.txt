Numerical Computation of [formula]-values with myFitter

Introduction

Even though the LHC experiments have, so far, not found any clear signs for physics beyond the Standard Model (SM) they already put strong constraints on the favourite SM extensions of many theorists. For example, the SM with a (perturbative) sequential fourth generation of fermions (SM4) has recently been excluded at the 5σ level by a combination of Higgs and electroweak precision data [\cite=Eberhardt:2012gv] (see also [\cite=Eberhardt:2012sb] [\cite=Eberhardt:2012ck]). Other models with additional fermions or even some constrained versions of Supersymmetry may follow soon.

In this situation some thoughts should be spent on the methods and criteria by which we decide if a certain model is ruled out. A well-established technique in (frequentist) statistical analyses is the method of likelihood ratio tests. (For an introduction see e.g. [\cite=CasellaBerger] or the statistics chapter of [\cite=PDG].) In this method two models are compared with a test statistic constructed from the ratio of their likelihood functions. Wilks' theorem states that under certain assumptions the test statistic is distributed according to the well-known χ2-distribution [\cite=Wilks]. In this case the relation between the likelihood values at the best fit points and the statistical significance (p-value) of the corresponding hypothesis test is described by the normalised lower incomplete gamma function.

There are, however, also many realistic scenarios where Wilks' theorem does not hold and the probability density function of the test statistic is not known analytically. One example is the case of likelihood ratio tests where the two models to be compared are not nested, meaning that one model can not be obtained from the other by fixing some of its parameters. This problem was encountered in the above-mentioned analyses of the Standard Model (SM) with a fourth generation of fermions [\cite=Eberhardt:2012gv] [\cite=Eberhardt:2012sb] [\cite=Eberhardt:2012ck]. In these analyses it is not possible to regard the SM with three fermion generations as a limiting case of the SM with four generations due to non-decoupling contributions of chiral fermions in electroweak precision observables and Higgs production and decay rates. Another case where analytical formulae for p-values are not reliable is the situation where some of the parameters of a model are bounded, in the sense that they are only allowed to float within a certain range. Most notably, this applies to analyses where systematic errors are treated within the RFit scheme [\cite=Hocker:2001xe], i.e. by introducing so-called nuisance parameters with a limited range.

When analytic formulae fail one has to resort to numerical methods, and the computation of p-values is no exception. The brute-force method is to generate a large sample of random toy measurements distributed according to the prediction of the null hypothesis. For each toy measurement the value of the test statistic is computed and compared to the value obtained from the actual data. With a large enough sample we can then estimate the probability that the value of the test statistic is larger than a certain number, usually chosen to be the value of the test statistic obtained from the observed data. This probability is called statistical significance or p-value of the test. Unfortunately, the computational cost of the required numerical simulations can be rather high, especially when the p-value is small. In this paper I discuss some methods for improving the efficiency of numerical computations of p-values. These methods have been applied in [\cite=Eberhardt:2012ck] [\cite=Eberhardt:2012gv], where, based on the constraints from Higgs searches and electroweak precision observables, likelihood ratio tests comparing the SM with three and four fermion generations were performed. The methods are also implemented in a publicly available code called myFitter, which I present in this paper.

The paper is organised as follows: in Sec. [\ref=sec:setup] I describe the general mathematical setup and the definitions of the test statistics for nested and non-nested models. In Sec. [\ref=sec:ana] I sketch the derivation of Wilks' theorem and discuss its range of applicability. In Sec. [\ref=sec:num] I explain the strategy for improving the efficiency of numerical computations of p-values. The myFitter framework and the implementation of the methods from Sec. [\ref=sec:num] are discussed in Sec. [\ref=sec:myFitter]. Performance tests of the myFitter code are presented in Sec. [\ref=sec:performance]. I conclude in Sec. [\ref=sec:concl].

General Setup

Let [formula] be a set of experimental observables. In frequentist statistics we regard observables as random variables distributed according to some probability density function (PDF). A statistical model with free parameters [formula] is therefore described by a function [formula], which must be a PDF for any fixed value of [formula] and considered as a function of [formula] only:

[formula]

The problem of statistical inference is to draw conclusions about the parameters [formula] from a given set of measurements [formula] of the observables [formula].

In global analyses like [\cite=Eberhardt:2012gv] [\cite=Eberhardt:2012sb] [\cite=Eberhardt:2012ck] the observables come from many different collider experiments and the parameters [formula] to be determined are the fundamental parameters of some theory of particle physics (the SM or extensions thereof). In this situation the function f usually does not depend on the parameters [formula] directly. For example, a cross section σ is measured by counting the number N of events that pass certain cuts and dividing by the integrated luminosity L and the selection efficiency ε. If the theory is realised with parameters [formula] we denote the predicted value of the cross section as [formula]. The integrated luminosity and selection efficiency are usually constants which, to a good approximation, do not depend on the theory parameters. Since N follows a Poisson distribution with mean value [formula] the distribution of the measured value of the cross section, σ = N / (Lε), depends on [formula] only trough the predicted cross section [formula].

These considerations motivate us to write the function f in the following way:

[formula]

where [formula] are the "predicted" values of the observables and D is a function which we shall call the input function. The goal of this re-writing is to cleanly separate information about the theoretical model from information about the experimental uncertainties: the theory is described by the function [formula], which maps parameters to observables, and the experimental uncertainties are described by the function [formula], which is - 2 times the logarithm of the probability density for measuring values [formula] if the "true" values are [formula]. For example, in many situations the experimental errors are Gaussian, independent of the true values [formula] and described by a covariance matrix V. In this case we have

[formula]

which, if substituted in [\eqref=eq:f], gives the usual expression for a correlated Gaussian probability density with central value [formula]. Up to a constant term, [formula] is simply the χ2-value associated with the parameters [formula], input data [formula] and covariance matrix V.

For the methods presented in this paper, the exact definition of the functions [formula] and D is not important, as long as D has the following properties:

For any given [formula] the input function D must satisfy the normalisation condition

[formula]

For any given [formula], and considered as a function of [formula], the input function [formula] must be bounded from below and have its unique absolute minimum at [formula].

For any given [formula], and considered as a function of [formula], the input function [formula] must be bounded from below and have its unique absolute minimum at [formula].

The first property guarantees that [\eqref=eq:fnorm] is satisfied. The second property guarantees that, if parameters [formula] exist with [formula], the maximum likelihood estimate of the parameters is indeed [formula]. The third property can be regarded as a definition of the term "predicted value": if the theory is realised with some parameters [formula], the most likely outcome of a measurement of the observables [formula] should be [formula].

Note that, without any modifications to the model, the third property does not hold in the presence of systematic errors, since a systematic error is an offset between the true value of an observable and its most likely measured value. This offset is the same each time the measurement is performed and does therefore not average out when the measurement is repeated many times. This results in a difference between [formula] and the maximum of the distribution of the random variables [formula]. The central idea of the RFit method [\cite=Hocker:2001xe] is that systematic errors should not be treated as errors at all, but as unknown theory parameters, so-called nuisance parameters, that may vary within a certain range. In a way, the presence of a systematic error means that theorists and experimentalists are simply not talking about the same quantity. Since the difference between the two quantities can neither be modeled nor measured it has to be treated as an additional model parameter, but with a limited range of possible values. Thus, the third assumption does hold if systematic errors are treated within the RFit scheme, i.e. by introducing a nuisance parameter for each source of systematic errors.

The results of hypothesis tests for a certain theoretical model should not depend on the way we parametrise the model. To make this parametrisation-independence manifest it is convenient to define the theory manifold as the image of the function [formula]:

[formula]

where [formula] is the parameter space (i.e. the set of allowed parameter values) of the model. Different parametrisations of the same model are represented by different functions [formula] and parameter spaces Ω, but always have the same theory manifold.

The general procedure for a likelihood ratio test (LRT) with nested models may now be described as follows: given certain experimental data [formula], we first maximise the PDF [formula] with respect to the parameters [formula]. This is equivalent to minimising the function [formula] with respect to [formula] on the theory manifold M:

[formula]

Next, we consider a constrained version of the model, which is usually obtained from the original model by fixing some of its parameters. However, with the notion of theory manifolds at hand, we can be more general and simply require that the theory manifold Mc of the constrained model is a subset of M:

[formula]

Maximising the likelihood for the constrained model we get

[formula]

Now we construct a test statistic S from the ratio of the two maximum likelihood values:

[formula]

To perform the actual test, we choose a certain realisation of the constrained model as null hypothesis. Let [formula] be the corresponding parameters and [formula]. The statistical significance, or p-value, of the test is obtained by considering an ensemble of toy measurements [formula] distributed according to the PDF [formula] and computing the probability that [formula] is larger than some threshold value S0:

[formula]

where θ denotes the Heavyside step-function. If, in the real experiments, the data [formula] was measured, one typically takes the maximum likelihood estimates for [formula] in the constrained model as null hypothesis, i.e. one chooses [formula] so that 0∈Mc and [formula]. Then one performs the test with [formula].

Note that the definition [\eqref=eq:pvalue] is manifestly independent of the parametrisation of the models: the factor [formula] is fixed by the null hypothesis and the test statistic is defined in terms of the functions [formula] and [formula] whose definitions [\eqref=eq:Dmin] and [\eqref=eq:Dcmin] depend on the manifolds M and Mc, but not on their parametrisation. This parametrisation-independent language allows us to easily generalise the definition [\eqref=eq:pvalue] for the case of (a large class of) non-nested models. Consider two models with theory manifolds M1 and M2 such that [formula] and [formula]. However, we assume that the relation between the PDFs of the two models and their respective theory manifolds is still given by [\eqref=eq:f] with the same input function D. This usually holds for global fits in particle physics, where a model imposes certain relations between the predicted observables, but the random distribution of the measured quantities is fixed by the predicted values, irrespective of the model under consideration. In this case we can simply combine the two theories into one by joining their theory manifolds,

[formula]

and do a LRT as described above, with M as the full theory and either M1 or M2 as the constrained theory. Let

[formula]

Then the test statistic for testing M2 against M is

[formula]

and the test statistic for testing M1 against M is obtained by exchanging [formula] and [formula]. Assume without restriction that for the measured data [formula] we have

[formula]

Then [formula] is zero and the LRT for M1 (using [formula] as threshold value for the test) has a p-value of 1. So, only the LRT for the model which describes the data less well (i.e. gives a bigger value for [formula]) can have a p-value smaller than one.

Analytical Formulae for -values

In many cases the computation of p-values in LRTs is trivial due to a theorem by Wilks [\cite=Wilks]. It states that the test statistic S from [\eqref=eq:S] follows a χ2 distribution with dim (M) -  dim (Mc) degrees of freedom if the models are nested and the maximum likelihood estimates [formula] of the parameters [formula] follow a Gaussian distribution. In this case the p-value [\eqref=eq:pvalue] is given by

[formula]

where ν  =   dim (M) -  dim (Mc) is the difference of dimensions of the theory manifolds (usually equal to the number of parameters that were fixed) and P denotes the normalised lower incomplete Gamma function.

In global analyses in particle physics Wilks' theorem is commonly used, but the validity of its underlying assumptions are rarely discussed. For PDFs of the form [\eqref=eq:f] the requirements for Wilks' theorem translate to certain assumptions about the function D and the theory manifolds M and Mc. These assumptions are:

The first assumption, combined with the properties of D discussed in Sec. [\ref=sec:setup], implies that D is of the form [\eqref=eq:gaussian], i.e. that the experimental errors are Gaussian. The second assumption is invalid if experimental errors are large, so that the curvature of the theory manifolds can not be neglected. It also fails if some parameters of the model have upper or lower bounds, so that the corresponding manifold does not extend to infinity. The last assumption is invalid if none of the two models to be compared can be considered as a special case of the other model.

The derivation of Wilks' theorem from the assumptions above will be instructive for our discussion of numerical methods in the next section, so I will briefly sketch it here. The first step is to perform an affine-linear coordinate transformation in the space of observables, which maps 0 (the predicted observables under the null hypothesis) to the origin and changes the PDF to an n-dimensional normal distribution. In other words, we introduce new coordinates [formula], so that [formula] and

[formula]

The linear part of this transformation is easily constructed by diagonalising the matrix V from [\eqref=eq:gaussian] and then scaling the new coordinates appropriately. We see that, up to a constant term, the function D is simply the squared euclidean norm of the vector [formula] (denoted as [formula]). Let M' and Mc' denote the images of M and Mc under the coordinate transformation [formula]. Since M and Mc both contain 0, the hyperplanes M' and Mc' both contain the origin and are therefore linear subspaces. Consequently, the functions [formula] and [formula] (see Eq. [\ref=eq:Dmin] and [\ref=eq:Dcmin]) are simply the squared euclidean length of the component of [formula] perpendicular to M' and Mc', respectively. For n = 3, dim (M) = 2 and dim (Mc) = 1 the situation is depicted in Fig. [\ref=fig:nested]. We may write the vector [formula] as a sum of three orthogonal vectors [formula], [formula] and [formula] with [formula] and [formula]. The test statistic S is then:

[formula]

In terms of the coordinates [formula] the integral from [\eqref=eq:pvalue] becomes

[formula]

In other words, the p-value is the integral of an n-dimensional normal distribution in the region outside an (infinitely long) "hyper-cylinder" defined by [formula]. In Fig. [\ref=fig:nested], this "cylinder" is the region between the planes indicated by dashed lines. The integral in [\eqref=eq:pvalue-Wilks-1] can easily be computed. The integrals over the components [formula] and [formula] are just Gaussian integrals and give an overall factor of (2π)(n - ν) / 2. Introducing spherical coordinates in the ν-dimensional subspace corresponding to the component [formula] and exploiting rotational symmetry immediately leads to [\eqref=eq:pvalue-Wilks].

Numerical Calculation of -values

In the last section we have seen how Wilks' theorem emerges from geometric arguments if the models under consideration satisfy three assumptions, which we called gaussianity, linearity and nestedness. In practice, these assumptions are rarely satisfied exactly. Usually, they are only more or less valid approximations. If we do not want to rely on these approximations we have to resort to numerical integration methods to calculate p-values. To make these methods efficient it is a good idea to take some or all of the approximations as a starting point and optimise the numerical integration for the case where they are valid. In the following, we will use Monte Carlo integration with importance sampling to compute the integral [\eqref=eq:pvalue], and construct sampling densities which are optimal for models that satisfy gaussianity and linearity.

To compute the integral [\eqref=eq:pvalue] with the importance sampling method, we generate a large number N of sample points [formula] according to some sampling distribution ρ. The integral [\eqref=eq:pvalue] is then estimated as

[formula]

To reduce the statistical error of this estimate one has to choose the function ρ as similar as possible to the integrand, so that the terms in the sum are (ideally) all of the same size. A common approach (especially if [formula] is a Gaussian distribution) is to choose [formula] and let the numerics take care of the theta function in the integrand. For large p-values this is a viable option, but if p is small most sample points give a contribution of zero to the integrand and the numerical integration becomes very inefficient. (Remember that each evaluation of the test statistic [formula] requires the computation of [formula] and [formula] which, in general, has to be done by numerical minimisation.) For small p-values, the efficiency of the integration can be significantly improved by choosing a sampling density ρ which avoids the region where the theta function is zero (i.e. the region between the dashed planes in Fig. [\ref=fig:nested]). Knowledge about the geometric properties of the theory manifolds can be used to construct such a sampling density. In the numerical methods proposed in this paper, we assume gaussianity and linearity for the purpose of constructing the sampling density, but make no approximations when computing the p-value.

To see how this works, let us start with the case where the nestedness assumption is still valid. For definiteness, we choose a parametrisation so that

[formula]

Let [formula] denote again the parameters under the null hypothesis. Now we define the hyperplanes H and Hc as tangent planes on M and Mc at the point [formula]:

[formula]

By construction the function [formula], considered as a function of [formula], has a minimum at [formula] (see Sec. [\ref=sec:setup]). Consequently, we define the matrix V- 1 as the Hessian matrix at that minimum:

[formula]

As in the derivation of Wilks' theorem, we now perform an affine-linear coordinate transformation which maps 0 to zero and transforms V- 1 to a unit matrix. To this end, we define

[formula]

where O is an orthogonal matrix chosen so that [formula] with positive eigenvalues [formula]. Let H' and H'c denote the images of the hyperplanes H and Hc, respectively, under this coordinate transformation. Since H and Hc contain 0, H' and H'c must contain the origin and are therefore linear subspaces of [formula]. Any vector [formula] may thus be decomposed into three orthogonal components [formula], [formula] and [formula] with [formula] and [formula] (see Fig. [\ref=fig:nested]). If the assumptions of gaussianity and linearity were satisfied exactly, the theta function in [\eqref=eq:pvalueMC] would vanish if and only if [formula] and we should not waste sample points on this region. If gaussianity and linearity are only approximations, we should be more careful and use a sampling density ρ which is small, but non-vanishing for [formula]. A choice which can still be sampled efficiently is

[formula]

where [formula] is the Jacobian of the coordinate transformation. The parameters and [formula] may be tuned to improve the efficiency of the numerical integration (subject, of course, to the constraint that the PDF ρ is properly normalised).

We see that the sampling density ρ factorises into three terms which only depend on the components [formula], [formula] and [formula], respectively. The task of generating points distributed according to ρ thus reduces to the task of generating components [formula], [formula] and [formula] distributed according to the respective factors. For [formula] and [formula] these factors are Gaussian, so generating the components [formula] and [formula] is trivial. The generation of the component [formula] requires special care.

Before we address this problem let us talk about the case of non-nested models. Assume that we have two theory functions [formula] and [formula] with parameter spaces Ω1 and Ω2, respectively, of arbitrary and possibly different dimension. Our null hypothesis is that theory 2 is realised with parameters [formula]. We therefore approximate the theory manifold M2 by its tangent hyperplane H2 at [formula]. Since M2 is no subset of M1 we have to approximate M1 by its tangent hyperplane at some other parameters [formula]. If [formula] is the maximum likelihood estimate of some measured data [formula], i.e. [formula], an obvious choice for [formula] would be the maximum likelihood estimate of [formula] in theory 1, i.e. [formula]. In any case we define hyperplanes H1 and H2 analogous to [\eqref=eq:nested_hyperplanes]:

[formula]

with [formula] and [formula]. We define the matrix V- 1 as in [\eqref=eq:nested_hessian] and construct coordinates [formula] according to [\eqref=eq:y], but with 0 replaced by 20. Let H'1 and H'2 be the images of H1 and H2, respectively, under the coordinate transformation [formula] and let [formula]. The image of 20 under [formula] is the origin. Since H2 contains 20 the hyperplane H'2 contains the origin and is thus a linear subspace of [formula]. H'1, on the other hand, contains 10 but not necessarily the origin, so it is not a linear subspace. For n = 3, a two-dimensional H'2 and a one-dimensional H'1, this situation is depicted in Fig. [\ref=fig:nonnested].

We see that for non-nested models the boundaries of the region with [formula] are curved, even if a linear approximation is used for the theory manifolds. This makes it harder to construct a sampling distribution which avoids this region. We shall try it anyway: let H'21  ⊂  H'2 be the subspace obtained by shifting H1 by -  10 (so that it contains the origin) and projecting it onto H'2. Furthermore, let H'22 be the orthogonal complement of H'21 in H2. Finally, let [formula] be the orthogonal complement of H'2 in [formula]. The projection of H'1 onto H'22 is then a single point [formula], which can be obtained by projecting 10 onto H'22. Any vector [formula] can now be written as the sum of three orthogonal components [formula], [formula] and [formula] with [formula], [formula] and [formula]. (See Fig. [\ref=fig:nonnested].) The distance between [formula] and H'1 is larger than [formula] since projecting any vector on a lower-dimensional subspace reduces its length and the projection of any vector pointing from [formula] to H'1 onto the subspace H'22 is [formula].

Now recall the test statistic S2 from [\eqref=eq:S2], which we constructed to test theory 2 against the "union" of theories 1 and 2. In the approximation where the theory manifolds M1 and M2 are equal to the hyperplanes H1 and H2 the functions [formula] and [formula] satisfy

[formula]

Thus, [formula] holds if

[formula]

Note, however, that this condition is sufficient but not necessary. [formula] must be smaller than S0 in the region defined by [\eqref=eq:nonnested_cond], but it may also be smaller than S0 outside this region. Nonetheless, a good choice for the sampling density ρ will be one which avoids the region defined by [\eqref=eq:nonnested_cond]. Analogous to [\eqref=eq:rho_nested], this density can be constructed as follows:

[formula]

where [formula] is again the Jacobian of the coordinate transformation [formula]. After requiring that ρ is properly normalised, there are still two free parameters which can be tuned to improve the efficiency of the numerical integration. As in [\eqref=eq:rho_nested], the density ρ factorises into three terms. The first two are Gaussian and only depend on the components [formula] and [formula], respectively. Generating components [formula] and [formula] with the correct statistical distribution is therefore trivial. A new complication in [\eqref=eq:rho_nonnested] is that the last factor, i.e. the distribution of [formula], now depends on [formula]. This simply means that we have to generate a value for [formula] before we generate [formula].

The remaining problem is to generate a random vector [formula] of some dimension m distributed according to the PDF

[formula]

with some Δ  >  0. (In [\eqref=eq:rho_nested] we have [formula], m = ν and Δ2 = S0 while in [\eqref=eq:rho_nonnested] we have [formula], [formula] and [formula].) We first note that the PDF ρ' is rotationally invariant. The length [formula] of the vector [formula] is then distributed according to a PDF

[formula]

We may write this as

[formula]

where f∈[0,1] is a free parameter and

[formula]

are PDFs normalised to 1. Here, Pm / 2 is the normalised lower incomplete Gamma function. Since '< and '> are normalised, the parameter f is just the fraction of sample points that will be put in the "inner region" with r < Δ. For a given f, the corresponding values of a and b are

[formula]

By integrating '<, '> and ' from 0 to r we obtain the cumulative distribution functions (CDFs) To generate random variables r distributed according to ' we need the inverse of [formula]: where P- 1m / 2 is the inverse of the normalised lower incomplete Gamma function. Random vectors [formula] distributed according to ρ may now be generated in the following way: First generate a vector [formula] according to a m-dimensional normal distribution. Then pick a uniformly distributed random variable q∈[0,1] and set [formula]. The variable r is then distributed according to '. The vector [formula] with the correct random distribution is [formula].

Introducing myFitter

The ideas for the numerical computation of p-values outlined in the last section have been implemented in a publicly available code called myFitter. The source code is available at Hepforge [\cite=myFitter]. Detailed documentation is included in the source distribution. Here I just want to provide a brief description of the user interface and discuss some details of the implementation.

myFitter is a C++ class library and makes extensive use of inheritance and polymorphism to separate the tasks of fitting a model to experimental data and computing p-values from the tasks of implementing the observables (as functions of the model's parameters) or the input function D (as a function of the observables). The main classes the user will have to deal with are:

Both, for the case of nested and non-nested models, the efficiency of the integration can be improved further by adaptive integration techniques, where the shape of the sampling density ρ is tuned during the actual integration. For the adaptation, the implementation in myFitter uses the OmniComp/Dvegas package [\cite=DVEGAS] by Nikolas Kauer, which implements the VEGAS algorithm [\cite=VEGAS] and was developed in the context of [\cite=Kauer:2001sp] [\cite=Kauer:2002sn]. Thanks to OmniComp, parallelised integration is fully supported.

To maximise the likelihood function, myFitter uses a custom implementation of the BFGS method for numerical optimisation [\cite=Broyden] [\cite=Fletcher] [\cite=Goldfarb] [\cite=Shanno]. The optimisation terminates successfully when the length of the gradient of the likelihood function drops below a certain value configurable by the user. Other optimization algorithms can be implemented by subclassing the Minimizer class and assigning an instance of this class to the Fitter object via the Fitter::minimizer() method. The problem of minimising a function of bounded parameters (i.e. of parameters that have an upper or lower limit) is solved in the usual way by smoothly and invertably mapping the real axis [formula] to the allowed range of the parameter. Internally, myFitter does this with the function

[formula]

Performance Tests

The performance of the myFitter method for the numerical integration of Eq. [\ref=eq:pvalue] was compared with more generic methods in three tests using simple toy models. All alternative methods use the coordinate transformation [\eqref=eq:y] to transform the PDF f to a normal distribution. The simplest method, referred to as no adaptation in the following, just uses importance sampling with a normal distribution as sampling density. The other two methods map the integration volume (i.e. the [formula]) to the unit hypercube n by using

[formula]

as integration variables and use the VEGAS algorithm [\cite=VEGAS] to perform the integration over the variables ti. The VEGAS algorithm is most efficient when the features of the integrand are aligned with the coordinate axes. In one variant, called aligned VEGAS in the following, we perform a rotation which aligns the tangent hyperplanes of the theory manifolds with the coordinate axes before mapping to the unit cube. This usually leads to an integrand whose features are aligned with the coordinate axes. In a second variant, which we call misaligned VEGAS, we choose the rotation so that the theory manifolds are not aligned with the coordinate axes. The misaligned VEGAS method is the best possible method when no information about the theory manifolds can be used.

In the first test we study the performance of the four integration methods in the context of a model with a curved theory manifold. To this end we consider a model with seven observables [formula] and four parameters [formula]. The theory function [formula] is given by

[formula]

where λ is a fixed number which controls the curvature of the theory manifold. As input function we use the expression [\eqref=eq:gaussian] for Gaussian errors with a unit covariance matrix:

[formula]

The constrained version of this model is defined by fixing ξ2 to zero and ξ1 to some other value r. The test statistic S is then defined according to [\eqref=eq:S]. We take [formula] as the actually measured data and perform the test with [formula]. Different choices for r lead to different values of S0 and thus to different p-values.

For two values of λ, the value of r was varied to obtain p-values roughly corresponding to 2, 3, 4 and 5 standard deviations. The p-value was then computed numerically with myFitter and the three alternative methods to a relative precision of 1%. The number of integrand evaluations needed by each method are summarised in Tab. [\ref=tab:curvature]. The p-values obtained by applying Wilks theorem are also shown. We see that adaptive methods always lead to a significant speedup and that the myFitter method performs best in all cases. For three standard deviations or less the two VEGAS methods still compete rather well with myFitter. At four standard deviations myFitter is faster than the VEGAS methods by a factor of 3 for large curvature and a factor of 10 for small curvature. At five standard deviations only myFitter is able to compute the p-value with a reasonable number of evaluations. The main reason for the poor performance of the VEGAS methods at small p-values is the fact that they require a large number of initial evaluations to find any points in the integration region which give a nonzero contribution to the integrand. The myFitter method converges faster because it "knows", to a certain approximation, where the integrand is nonzero.

The second test compares the performance of the four integration methods in the case of models with bounded parameters. To this end, we use the theory function [\eqref=eq:test_theory] with λ = 0 and the input function [\eqref=eq:test_input]. In the constrained version of the model, the parameters ξ1 and ξ2 are still fixed to r and 0, respectively. We assume again that [formula] is the actually measured data, perform the fit with [formula] and vary r to change the p-value. However, in the full model the parameter ξ2 is now restricted to the interval

[formula]

Conclusions

Likelihood ratio tests are a popular tool in global analyses of models in particle physics. For a correct statistical interpretation of the data, reliable methods for the computation of p-values in likelihood ratio tests are needed. There are many realistic situations where Wilks' theorem does not apply and the distribution of the test statistic is not known analytically. These include likelihood ratio tests of non-nested models or models with parameters that are only allowed to float in a finite range. Real-world examples of the former case are the global analyses [\cite=Eberhardt:2012ck] [\cite=Eberhardt:2012gv] of the Standard Model with a fourth generation of fermions where the models being compared are not nested due to the non-decoupling nature of the additional fermions. The latter case includes models where systematic errors are treated within the RFit scheme. In these situations one has to resort to numerical methods. Monte Carlo integration can be used to compute p-values numerically, but the integration usually becomes very inefficient for small p-values.

In this paper I presented an efficient approach to the numerical computation of p-values which is based on importance sampling and applies to a broad class of statistical models. In global analyses in particle physics, the predictions of a theoretical model can be described by a manifold in the space of observables. The PDF of the statistical model is then obtained by "smearing out" the theory manifold in a way determined by the experimental uncertainties. The proposed methods use geometric information about the theory manifolds to construct suitable sampling densities for the Monte Carlo integration and substantially improve the performance of the numerical integration for small p-values. These methods are implemented in a publicly available C++ framework for likelihood ratio tests called myFitter.

Acknowledgements

I would like to thank Jérôme Charles for fruitful discussions about likelihood ratio tests for non-nested models. I also thank Otto Eberhardt for checking fit results with CKMfitter and Ulrich Nierste for thorough proof reading.