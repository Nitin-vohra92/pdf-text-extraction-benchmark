Exact testing with random permutations

Corollary Lemma Proposition

Introduction

Permutation tests are nonparametric tests that can be used when under the null hypothesis the distribution of the data is invariant under certain transformations. Permutation tests are frequently used in for example omics. Not only permutations can be used, but other groups of transformations as well. Examples are rotations [\citep=langsrud2005rotation] and, when under the null the data distribution is symmetric, multiplication of part of the data by - 1. We will often use the term 'permutation test' when we consider tests based on other transformations as well. Traditionally, [\citeauthor=fisher1935]'s Lady Tasting Tea experiment from 1935 is mentioned as the first permutation test [\citep=hoeffding1952large] [\citep=ernst2004permutation] [\citep=phipson2010permutation]. As we will argue, however, the first permutation test was described in 1936.

Apart from the permutation test for a single hypothesis, there are permutation-based methods for familywise error rate (FWER) and false discovery proportion (FDP) control in multiple testing contexts. Generally, permutations of the data are used to simulate realizations of the null distribution. This has been done by [\cite=pawitan2006estimation] to estimate the FDP in certain contexts. Well-known FWE-controlling methods that use permutations are the minP and maxT methods by [\cite=westfall1993resampling]. Another, more recent method, which gives uniform upper bounds for the FDP, has been published by [\cite=meinshausen2006false]. This method is closely related to the basic permutation test for one hypothesis, as is explained in this paper.

As we will illustrate, it is important that the set of transformations used is a group. To our knowledge, the first author who explicitly assumed a group structure is [\cite=hoeffding1952large]. The importance of the group structure has only recently been emphasized, by e.g. [\cite=southworth2009properties] and [\cite=goeman2010sequential]. The latter note that in particular the set of balanced permutations cannot be used, since it does not form a group.

Often it is computationally infeasible to use the whole group of permutations, hence random permutations are used. The set of randomly drawn permutations is usually not a group. Using random permutations was first proposed by [\cite=dwass1957] and is often done in the various permutation-based methods. The question how and why random permutations can be used, has until now not fully been answered. [\cite=dwass1957] gives a proof, but bases it on an incorrect assumption. Consequently his test is anti-conservative.

Corresponding to the test that Dwass defined, permutation p-values are often calculated incorrectly and are consequently too small. In particular, p-values can become zero, as e.g. [\cite=knijnenburg2009fewer] and [\cite=phipson2010permutation] note. The latter illustrate that in combination with Bonferroni's method, this can lead to excessive rejection probabilities.

The permutation-based multiple testing methods that we will consider, are also anti-conservative when random permutations are employed in the usual way (although these methods are not based on permutation p-values). For the methods of Westfall and Young and Meinshausen, the anti-conservativeness is limited if α and the number of permutations are not too small. However, some methods can become highly anti-conservative, as we will illustrate with a new global test.

[\cite=phipson2010permutation] have noted and solved part of the problem of anti-conservativeness. For the usual single-hypothesis permutation test, they give an elegant, exact formula for the permutation p-values, when the random permutations are drawn without replacement. They do this under the assumption that the drawn permutations give distinct test statistics. From this formula we can see that - under this assumption - random permutations, drawn without replacement, do give an exact permutation test, as long as the identity permutation is added to the collection of randomly drawn permutations.

Until now is has not been clear whether a similar result holds when permutations are drawn with replacement. This is, however, what is usually done in practice. Phipson and Smyth prove a formula for the exact p-value when permutations are drawn with replacement (under the assumption that under the permutation distribution, all possible test statistics are equally likely). This formula however is quite involved and may result in increased computation times, compared to the simpler solution that we will provide. Also, the approach op Phipson and Smyth does not easily generalize to permutation-based multiple testing methods.

The main purpose of this paper is to show in a general way how random transformations, either drawn with or without replacement, can be used in a permutation test and generalizations thereof, such as [\citeauthor=meinshausen2006false]'s ([\citeyear=meinshausen2006false]) method and Westfall and Young's maxT method to obtain exact tests. The main message is that in all these methods the identity transformation should be added to the set of randomly drawn transformations, both when drawing with and without replacement. When drawn without replacement, the random transformations are not allowed to be the identity. We do not need the assumptions on the test statistics that [\cite=phipson2010permutation] use. Some authors, e.g. [\cite=ge2003resampling] and [\cite=knijnenburg2009fewer], already added the identity, for intuitive reasons.

This paper is built up as follows. We will first discuss the importance of the group structure of the set of transformations used. We will then formulate how random permutations can be used. As examples of methods that use random permutations, we will consider a new global test, Westfall and Young's maxT-method and Meinshausen's method for FDP control. We will use simulations to show that these methods are anti-conservative, when the identity permutation is not added.

The role of the group structure in the permutation test

In this section we discuss the importance of the group structure of the set of transformations used for a permutation test.

In the Lady Tasting Tea experiment no group structure is needed

Many authors [\citep=hoeffding1952large] [\citep=ernst2004permutation] [\citep=phipson2010permutation] mention Fisher's famous Lady Tasting Tea experiment, from his 1935 book The Design of Experiments, as the first permutation test in the literature. The Lady Tasting Tea experiment is designed to determine whether a lady can distinguish two types of cups of tea with milk: cups in which the milk was poured first, and cups in which the tea was poured first. While this experiment can indeed be formulated as a permutation test, there is an important difference with the usual permutation test as it has been described by [\cite=fisher1936coefficient], [\cite=pitman1937significance] and many authors since.

The setup of the experiment is as follows. The lady is given eight cups - there are four of both kinds, which the lady is told beforehand - and she has to label the cups correctly as being of the first or the second kind. There are [formula] ways to label the cups. It is obvious that under the hypothesis that the lady cannot distinguish between the two types of cups, the probability that she guesses correct is [formula]. Note that the lady could have a preference for picking certain patterns, but that this probability would still be [formula], as long as the researcher picks each possible pattern of cups with probability [formula].

There are 8! = 40320 permutations of eight cups, and each pattern corresponds to [formula] of these permutations. It is not essential to use all 70 patterns, i.e. the whole permutation group; the researcher could also use a smaller set of n < 70 patterns. As long as he would tell the lady from which set of patterns he randomly picked one, the lady would guess correctly with probability [formula]. Thus, in the Lady Tasting Tea experiment, it is not important whether the transformations form a group. This is due to the randomization inherent in the experimental design.

For the usual permutation test a group structure is needed

For a permutation test as it is usually defined, it is less immediately obvious than for the Lady Tasting Tea experiment, that the test has the stated level: we need to use the group structure of the set of transformations. As an example of the usual permutation test, we consider a hypothetical experiment that Fisher described in 1936, which we consider to be the first permutation test. This was only a thought-experiment, since it would have been computationally infeasible at that time. The null hypothesis to be tested is that the statures of Frenchmen and Englishmen are distributed equally. To test the hypothesis, a hundred Frenchmen and a hundred Englishmen are sampled. Write the data as X = (X1,...,X200), where X1,...,X100 are the statures of the Frenchmen. The test statistic is

[formula]

For all permutations of X, we compute the corresponding test statistics. Then to have an α-level test, we should reject when T(X) is among the highest α  ·  100% of the computed test statistics. (This simple test can be conservative when α is not chosen suitably, or due to tied values.) Pitman descibes a mathematically analogous experiment in 1937 (p. 122). The data which he uses in his example is a vector of only eight numbers, so that he is able to actually execute the test. What these authors do not note, is that it is less obvious than in the Lady Tasting Tea Experiment, that the desired level is indeed obtained.

To prove that the usual permutation test has the desired level, the explicit use of the group structure of the set of permutation maps G is needed. Indeed, the group structure guarantees that Gg = G for all g∈G and consequently

[formula]

where T(1)  ≤  ...  ≤  T(M) are the ordered test statistics for the M: =   #  G permutated versions of the data. Due to the above property, it holds that

[formula]

for all g∈G, where k = ⌈(1 - α)M⌉. Under the null hypothesis that [formula] for all g∈G it follows that

[formula]

[formula]

[formula]

The proof we have given is essentially the same as the one that [\cite=hoeffding1952large] and [\cite=lehmann2005testing] give, but spells out the role of the group structure more explicitly.

A sidenote is, that to guarantee a rejection probability of exactly α, it suffices to reject with a suitable probability a in the boundary case that T(X) = T(k)(X), as [\cite=hoeffding1952large] shows. a is given by

[formula]

where

[formula]

[formula]

When the set of transformations G is not a group, the permutation test usually doesn't attain the desired level. Indeed, the ratio of the desired level α and the real level, can be arbitrarily large or small. This we prove with the following examples.

Take n  ≥  3 and let X = (X1,...,X2n) be a any random vector in [formula], where X1,...,X2n are continuous. Let G be the group of all permutation maps [formula] of the form (x1,...,x2n)  ↦  (xi1,...,xi2n), where (i1,...,i2n) is a permuted version of (1,...,2n). Let H0 be that the Xi are i.i.d.. Define the test statistic by [formula]. Let π̂∈G be the permutation that interchanges the first and the (n + 1)-th element of its argument. Take [formula], where

[formula]

Note that #  U = n!n! + 1. Moreover, xn + 1 < x1 implies that T(ux) < T(x) for all [formula] If we take [formula], then for the usual permutation test, using only the permutations in U instead of all of G, under H0, [formula]

[formula]

[formula]

Thus, under H0, as n  →    ∞  , [formula].

We now give a counterexample where [formula]. Take X and H0 as above, take as the set of permutations

[formula]

where πi is the permutation map that interchanges the i-th and the (i + n)-th element of its argument, and let [formula]. Then, under H0, [formula], so [formula]. In these examples we only added the identity to U to show that we get a completely wrong rejection probability, even if we add the identity. We conclude that the relative difference between the desired level and the actual level of the test, can be arbitrarily small or large if we do not require the set of transformations to be a group.

Another example of a set of permutations that is not a group is the set of balanced permutations, which [\cite=southworth2009properties] discuss. These permutations have been used in various papers since they can have an intuitive appeal. However, they tend to give rejection probabilities larger than α under H0. Had more emphasis been put on the importance of the group structure, then the use of balanced permutations might have been prevented.

The role of the group structure has not been given the attention it deserves, since the first permutation test was formulated. This is possibly due to the confusion caused by viewing the Lady Tasting Tea experiment as a permutation test. The first writing we know of that explicitly assumes that the transformations are a group is by [\cite=hoeffding1952large]. He does not make the importance of the group structure very explicit. [\cite=southworth2009properties] show this role explicitly. [\cite=goeman2011multiple] explicitly show the role of the group structure in the permutation-based maxT method by Westfall and Young.

Exact test with random permutations

It is often computionally infeasible to use the whole group of permutations in a permutation test. A vector of 20 numbers can already be permuted in 20!  ≈  2.4  ·  1018 ways. When the test statistic is given by T(x1,...,x20) = x1 + ... + x10 and we use only one permutation from each class of 10!10! equivalent permutations, then we still use [formula] different permutations.

A possible solution is to use a subgroup of the whole set of transformations. A simple example is to partition the data vector into m subvectors of equal size and to consider the m! permutations of these chunks of data. In practice, researchers choose to use random permutations to limit the computation time. As [\cite=phipson2010permutation] explain, the way in which random permutations have usually been used in permutation tests, leads to anti-conservativeness.

Applying Bonferroni's method to a set of p-values as they are often calculated, can lead to much too large rejection probabilities. In the following sections we will discuss three other permutation-based multiple testing methods that become anti-conservative when random permutations are employed in the usual way.

In this section, we discuss the use of random permutations in the basic permutation test. We first discuss Phipson and Smyth's formulas for exact permutation p-values. Then in Theorem [\ref=mainthmsingle] we formulate a basic test with random permutations which generalizes some of Phipson and Smyth's results. In particular we do not assume that all permutations drawn give distinct test statistics and we allow the permutations to be drawn with replacement. The multiple testing methods considered in the following sections are based on this basic test.

Exact p-values

[\cite=phipson2010permutation] provide a simple formula for exact permutation p-values, for when the permutations are drawn without replacement. The (slightly generalized) setting is as follows. Let X be data with any distribution, T a test statistic and G a finite group of transformations from and to the range of X. Assume that all these transformations in G give distinct test statistics. This is in a sense a large assumption, even when the data are continuous, since there are often equivalent transformations that always give the same test statistic. However, e.g. in case that the test statistic is the difference of the averages of two groups of size m and n, it can be shown that we are allowed to take G such that it contains exactly one element from each of the classes of equivalent permutations. We then essentially take G to be the [formula] different relabellings.

The formula for the permutation p-value that researchers (e.g. Dwass) typically use is

[formula]

where b is the number of random permutations g in G for which T(gX)  ≥  T(X) and m the total number of random permutations. However, as Phipson and Smyth explain, the correct formula is (under the above assumption)

[formula]

where the definition of b remains the same except that the random permutations are drawn from [formula] Note that if we would add the identity transformation to the random permutations, then this would be equivalent to adding + 1 in the numerator and the denominator of [formula]. But this implies that (under the assumption that all transformations in G give distinct test statistics) we can use random transformations, drawn without replacement from [formula], in the basic permutation test, if we add the identity transformation.

As Phipson and Smyth note, the formula for the permutation p-value above is analogous to the formula for the Monte Carlo p-value. This is because in both settings, b has the uniform distribution on {0,...,m}. We should add however that the reason why b is uniform in the permutation context is quite different from why it is uniform in the Monte Carlo setting: that b is uniform in the permutation context is a consequence of the group structure of the set from which the random permutations have been drawn.

Phipson and Smyth also provide a formula for the p-value in the case of drawing with replacement. In this case the random permutations are drawn from G instead of [formula] The assumption that is necessary in this case is that under the permutation distribution there are mt possible distinct values of the test statistic (not counting the original value) and all (including the original value) are equally likely. This is often satisfied, due to equally sized classes of equivalent permutations. The formula is computationally involved though. It is not necessary to use this p-value, due to Theorem [\ref=mainthmsingle]. For this theorem we do not need the assumptions on the test statistics, that Phipson and Smyth use.

The exact test

We show in Theorem [\ref=mainthmsingle] how random transformations can be used in the basic permutation test for a single hypothesis. We first define the vector of random transformations.

Given a finite group of transformations G, let G' be the vector (id,g2,...,gw), where id is the identity in G and g2,...,gw are random elements from G. Write id = :g1. The transformations can be drawn either with or without replacement: the statements in this paper hold for both cases. If we draw g2,...gw without replacement, then we take them to be uniformly distributed on [formula], otherwise uniform on G.

Let X be data with any distribution. Suppose G is a finite group (under composition of maps) of measurable transformations from and to the range of X. Let G' be as in Definition [\ref=defrp].

Let T be a test statistic on the range of X. Let T(1)(X,G')  ≤  ...  ≤  T(w)(X,G') be the ordered test statistics T(gi'X), 1  ≤  i  ≤  w. Let α∈[0,1] and k = ⌈(1 - α)w⌉.

Let H0 be a null hypothesis such that if H0 is true, then the joint distribution of the test statistics T(gX), g∈G, is invariant under all transformations in G of X. This holds in particular if [formula] for all g∈G. Reject H0 when T(X,G') > T(k)(X,G'). Then the rejection probability under H0 is at most α.

From the group structure of G, it follows that for all 1  ≤  j  ≤  w, G'g- 1j and G' have the same distribution, if we disregard the order of the elements. Let j have the uniform distribution on {1,...,w} and write h = gj. Under H0,

[formula]

Since (G'h- 1)(hX) = G'(h- 1hX), the above equals

[formula]

We end this section with some remarks. The test above can be slightly conservative if w and α are not chosen suitably or due to tied values of the test statistics. However, the level will be exactly α if we randomly reject with a suitable probability a = a(X,G') in the boundary case that T(X) = T(k)(X,G'). a is given in [\eqref=eqa], with the adjustment that w now takes the role of M and G' takes the role of G.

It is possible to define a permutation test that needs no group structure whatsoever, by slightly randomizing the rejection rule as follows. Let G be any finite, nonempty set of invertible transformations from and to the range of X. Let h have the uniform distribution on G. Let T, T(k) and H0 be as usual. Reject H0 only if

[formula]

This is a randomized rejection rule, since it depends on h, which is randomly drawn each time the test is executed. The rejection probability is at most α, which follows from an argument analogous to the last four steps of the proof of Theorem [\ref=mainthmsingle]. Note that if G is a group, then Gh- 1 = G and this test coincides with the basic permutation test. Thus it is a generalization thereof.

In case the transformations are drawn with replacement in Theorem [\ref=mainthmsingle], a simple upper bound for the permutation p-value is

[formula]

As discussed, [\cite=phipson2010permutation] give formulas for the exact p-value for both the cases of drawing with and without replacement, under certain additional assumptions. The formula for the p-value for the case of drawing with replacement is rather involved. We can also use an alternative definition of the p-value, which is simpler. Consider the exact variant of the test in Theorem [\ref=mainthmsingle] that rejects with a suitable probability a when T(X) = T(k)(X,G'). Suppose w.l.o.g. that when T(X) = T(k), the test rejects if and only if a > u, where u is uniform on

[formula]

p̃=+u· .

[formula]

. Observe that given u, for all c∈[0,1], P(p̃  ≤  c) = P(φc = 1). Thus, under H0,

[formula]

A result analogous to Theorem [\ref=mainthmsingle] holds for Monte Carlo testing: the original observation should be added to the random draws from the null distribution. Again we do not need to assume that the test statistics are distinct. We can again guarantee exactness of the test by introducing a randomized decision.

Application: a permutation-based global test

In certain permutation-based multiple testing contexts, adding the identity permutation can be of great importance. The anti-conservativeness can become huge when we do not add the identity, even if we use many random permutations. To illustrate this, we consider the following procedure. It is perhaps the most obvious permutation-based global test, next to Westfall and Young's single-step maxT method. In fact, we used this method ourselves, before we found that we needed to add the identity. Therefore this section can serve as a caveat to the research community. It illustrates that a seemingly reasonable method may become wildly anti-conservative if random permutations are used incorrectly.

Consider data X with any distribution, hypotheses H1,...,Hm and corresponding p-values P1(X),...,Pm(X). Let G be a group of transformations from and to the range of X. Suppose that [formula] implies that [formula] for all g∈G. The method that we will define permutes the data w times, each time obtaining a p-value curve. In this manner a permutation distribution is obtained for the p-value curve. Next a critical curve [formula] is constructed with the property that ⌈(1 - α)w⌉ of the w p-value curves lie everywhere above [formula]. Thus, under the permutation distribution, the probability that the p-value curve lies below [formula] somewhere is at most α. Assuming that under H0 the permutation distribution is a good approximation of the true distribution, the probability under H0 that the original p-value curve lies below [formula] somewhere is at most α. Thus rejecting H0 when the original p-value curve lies below [formula] somewhere gives a valid α-level global test. See figure [\ref=figglobal] for a visualization of the method.

The precise definition of the method is the following. Let G' be as defined in Definition [\ref=defrp]. Define a critical curve [formula] as follows. For each 1  ≤  j  ≤  w, consider the corresponding curve of sorted p-values,

[formula]

Pick r∈{1,...,w} such that #  J  ≥  (1 - α)w, where

[formula]

Define [formula] by

[formula]

Note that at least (1 - α)100% of the p-value curves lie everywhere above [formula]. Reject H0 when

[formula]

Suppose we do not add the identity, i.e. we let g1 be random instead of taking g1 = id. As we will see in section [\ref=secsimsglobal], the method is then very anti-conservative. The method is the most anti-conservative when there are many hypotheses and the number of permutations is limited, but even if the number of hypotheses is small (e.g. m = 50) and the number of permutations large (e.g. w = 1000), the anti-conservativess is substantial. We conclude that the permutation distribution is apparently not a good approximation of the true distribution in this context.

If we take g1 te be the identity, then the method becomes correct. In this case the method can actually be rewritten in a simpler form: it becomes a basic permutation test as defined in Theorem [\ref=mainthmsingle]. Indeed, define the test statistic T' on the range of X by [formula]. Let

[formula]

be the sorted test statistics T'(giX), 1  ≤  i  ≤  w. Note that the method rejects H0 if and only if T'(X) > T'(  #  J)(X,G'), i.e. it is a basic permutation test as defined in Theorem [\ref=mainthmsingle]. As noted below that theorem, the test can be easily made exact by introducing a randomized decision.

The reason that the test becomes correct when we add the identity, is not that under H0 the permutation distribution is then a better approximation of the true distribution. Instead, the method becomes correct due to the fact that the original p-value curve is among the w curves and under H0 all p-value curves have the same probability of lying below [formula] somewhere. The latter is in turn a consequence of the group structure.

Application: Westfall and Young's maxT-method

Westfall and Young's maxT-method is a well-known procedure for strong FWER control, which uses no assumptions on the correlations between the p-values. Beside the single-step variant of this method, there is also a less conservative, sequential variant, which has been shown to be asymptotically optimal in certain cases [\citep=meinshausen2011asymptotic]. The sequential method coincides with a closed testing procedure where for each intersection hypotheses the single-step test is used.

In the single-step method, all hypotheses among H1,...,Hm are rejected for which the test statistic is is higher than the (1 - α)-quantile of the global null distribution of max mi = 1Ti, where the Ti are the test statistics corresponding to the Hi. Often though the global null hypothesis doesn't imply a specific distribution of max mi = 1Ti, such that this method can't be used. However if the data corresponding to the true hypotheses are permutation invariant, we can instead use the permutation distribution of max mi = 1Ti.

If the identity map is not added to the random permutations though, the method can become anti-conservative, especially when the number of permutations is limited. In section [\ref=secsims] we use simulations to show this. It follows that the - less conservative - sequential procedure must also become anti-conservative.

Using Theorem [\ref=mainthmsingle], we now show that the single-step method becomes correct if we add the identity to the vector of random transformations.

Let X be data with any distribution and α∈[0,1]. Let G be a finite group of transformations from and to the range of X. Consider hypotheses H1,...,Hm and let N  ⊆  {1,...,m} be the indices of the true hypotheses. Suppose that for each 1  ≤  i  ≤  m a test statistic Ti(X) for Hi is defined.

Assume that the null hypotheses are such that the joint distribution of the test statistics Ti(gX) with i∈N, g∈G, is invariant under all transformations in G of the data X.

Let G' be as in Definition [\ref=defrp]. For each 1  ≤  j  ≤  w, consider μ(gjX): =  max {Ti(gjX):1  ≤  i  ≤  m}. Let

[formula]

be the w sorted maxima μ(gjX), 1  ≤  j  ≤  w.

Let k = ⌈(1 - α)w⌉ and reject the hypotheses Hi with Ti(X) > μ(k)(X,G'). Then the FWER is at most α.

When [formula] there are no false positives, so assume [formula]. For each 1  ≤  j  ≤  w consider

[formula]

Let

[formula]

be the sorted values μN(gjX), 1  ≤  j  ≤  w. Theorem [\ref=mainthmsingle] implies that

[formula]

Note that for each 1  ≤  j  ≤  w, μ(gjX)  ≥  μN(gjX). Consequently,

[formula]

[\eqref=eq:eeen] and [\eqref=eq:tweee] imply that

[formula]

which means that the FWER is at most α.

In the sequential variant of the maxT method, random permutations can be used in the same way. The false rejection probability of the above method is exactly α if all hypotheses are true, with probability one there are no ties among the test statistics and [formula]. If the latter two conditions are not both satisfied, then the rejection probability becomes α if we reject the hypotheses Hi with Ti(X) = μ(k)(X,G') with a suitable probability analogous to ([\ref=eqa]).

Application: Meinshausen's method for uniform FDP control

[\citeauthor=meinshausen2006false]'s ([\citeyear=meinshausen2006false]) procedure is a permutation-based multiple testing method which provides lower bounds for the number of true discoveries. The bounds are uniform over all rejected sets of the form

[formula]

where P1,...,Pm are the p-values. More precisely, the method provides a lower bound (t) for the number of correct rejections S(t), such that with probability at least 1 - α, (t)  ≤  S(t) for all t∈[0,1] simultaneously. Since the lower bound is uniform, post hoc selection of t is allowed. Hence the method is well-suited for exploratory research.

The method makes no assumptions on the correlation structure of the p-values. The procedure is based on the assumption that the distribution of the p-values for the true hypotheses is invariant under a group of transformations. The method transforms the data many times, obtaining a collection of p-value curves. Like the global test of section [\ref=sectieglobal], Meinshausen's method constructs a critical curve, [formula], such that (1 - α)100% of the p-value curves lie everywhere above [formula]. The lower bound (t) depends on the horizontal distance between [formula] and the original p-value curve: see figure [\ref=figmh].

We will state Meinshausen's method as set forth in his paper. We make two adjustments. Firstly, we add the identity transformation to the collection of randomly drawn transformations. In section [\ref=secsims] we show that the method can otherwise be anti-conservative. Secondly, we pick the critical curve Ql(α) from a different set of candidate curves, which do not depend on the data. This adjustment is needed for the following reason. In Meinshausen's proof, Q and consequently the critical curve Q(α) depend not only on the p-values Pi with i∈N, but also on the Pi with [formula]. Even though the distributions of P̃ and Q(α) are invariant under permutation of the data, the distribution of (P̃,Q(α)) is not. Consequently, from the fact that (1 - α)100% of the P̃-curves lie above Q(α) does not follow that one particular curve lies above Q(α) with probability 1 - α.

The lower bound (t) that Meinshausen's method constructs, is an example of a bound that [\cite=goeman2011multiple] derive for a general class of closed testing procedures. We do not explicitly use closed testing in the proof, but we could do this, obtaining the lower bound by using shortcut (7) in section 4.2 of [\cite=goeman2011multiple]. Assumptions Let X be data with any distribution and α∈[0,1]. Let G be a finite group of transformations from and to the range of X. Consider hypotheses H1,...,Hm and corresponding p-values P1,...,Pm. Let N  ⊆  {1,...,m} be the indices of the true hypotheses. Suppose that the null hypotheses are such that the joint distribution of the p-values Pi(gX) with i∈N, g∈G, is invariant under all transformations in G of the data X.

Reject the hypotheses with indices in

[formula]

Let S(t) be the number of correct rejections. The algorithm Under the above assumptions, the following algorithm gives the lower bound (t). Let G' be as in Definition [\ref=defrp]. For each 1  ≤  j  ≤  w, let P(1)(gjX)  ≤  ...  ≤  P(m)(gjX) be the m sorted p-values found after applying transformation gj to the data. Write P(gjX) = (P(1)(gjX),...,P(m)(gjX)).

Let F be subset of m, independent of the data. F is a family of candidate 'curves'. Suppose that it is of the form F = {fγ:γ∈Γ}, where [formula] is bounded and closed and fγ depends continuously on γ. Suppose that γ1  ≤  γ2 implies that fγ1  ≤  fγ2, i.e. fγ1k  ≤  fγ2k for all 1  ≤  k  ≤  m.

For each γ∈Γ define

[formula]

Let

[formula]

the largest γ for which β(γ) is still at least 1 - α. Let [formula]

For all t∈[0,1], let

[formula]

Define the lower bound as

[formula]

Define V(t) =   #  R(t) - S(t) to be the number of false rejections. Let (t) be as above and define [formula]. Under the assumptions above,

[formula]

[formula]

Let PN(X) = (PN(1)(X),...,PN(  #  N)(X)) be the vector containing the sorted p-values Pi(X) with i∈N. Define the test statistic T by

[formula]

Let

[formula]

be the sorted test statistics T(gjX), 1  ≤  j  ≤  w.

Note that

[formula]

It easily follows with Theorem [\ref=mainthmsingle] that

[formula]

Define

[formula]

and let [formula] be the critical curve [formula] we would have found if we had used the p-value curves PN(gjX) instead of the P(gjX) and the set of candidate curves FN instead of F.

Observe that

[formula]

where [formula] means that [formula] for at least one 1  ≤  i  ≤    #  N. Hence [formula], i.e.

[formula]

For all 1  ≤  k  ≤    #  N, PN(k)(gjX)  ≥  P(k)(gjX), and consequently

[formula]

From [\eqref=eq:breuk] and [\eqref=eq:qs] follows that

[formula]

This implies that

[formula]

Hence

[formula]

S(t) is monotonously increasing in t, so

[formula]

Thus

[formula]

as we wanted to show.

Simulations

From the simulation results that follow we will see that the methods of the previous sections can become anti-conservative when the identity isn't added to the random transformations. The simulations were done in R. The goal of this section is not simulate realistic data, but to show in a simple way that the methods considered are incorrect when we do not add the identity map.

Permutation-based global test

As data we used an m  ×  20-matrix (Xij) of independent standard normally distributed variables. For each 1  ≤  i  ≤  m, the hypothesis Hi that the random variables in the i-th row were i.i.d., was tested. Thus all hypotheses were true. For each Hi we calculated the p-value as the probability under Hi of a larger value of [formula] than observed. As the group of transformations we used the 20! maps that shuffle the columns of (Xij).

For different values of α, the number of hypotheses m and the number of permutations w, we tested [formula] using the global test of section [\ref=sectieglobal]. First we used w random permutations and recorded whether the method rejected the global hypothesis. We then substituted the identity map for the first permutation and did the same. We generated (Xij) many times, each time recording whether the method rejected, to obtain an estimate of the rejection probability. The estimated rejection probabilities (divided by α) are shown in Table [\ref=tableglobal]. From this table we see that the rejection probability was much too large when we did not add the identity. The anti-conservativeness seems to increase as w decreases or m increases. When α decreases, the relative anti-conservativeness seems to increase.

Westfall and Young's max method

We simulated data using the same distribution as for the permutation-based global test. We used the same group of transformations. The test statistic we used for Hi was [formula]. In the same way as before we estimated the error rate for different values of w, m and α. The results are shown in Table [\ref=tablewy]. They suggest that the anti-conservativeness increases as w decreases, and that the relative anti-conservativeness increases as α decreases.

The results suggest that the method is only substantially anti-conservative when few permutations are used. Researchers in various fields use few permutations in permutation tests for computational reasons. Examples of recent papers using few permutations are [\cite=byrne2013monozygotic] (100 permutations) and [\cite=schimanski2013tracking] (25 to 100 permutations).

Meinshausen's method

We simulated data using the same distribution as for the permutation-based global test and used the same transformations. Again we calculated the p-value for Hi as the probability of a larger value of [formula] than observed. As the family F of candidate curves, we used many straight lines through the origin. We took t = 0.2. In the same way as before we estimated the error rate (the probability that (t) > 0) for different values of w, m and α. The results are shown in Table [\ref=tablemh]. Again they suggest that the anti-conservativeness increases as w decreases, and the relative anti-conservativeness increases as α decreases.

Discussion

There are various multiple testing methods that are based on the well-known permutation test. We have discussed three such methods: a new global test, Westfall and Young's maxT method for strong FWER control and Meinshausen's method for uniform FDP control. In areas such as omics, the number of hypotheses is often huge. Using the whole group of permutations or other transformations, would then be computationally infeasible. Thus researchers often use random permutations to limit the computation time.

The way in which random permutations have been used however can lead to anti-conservativeness. The anti-conservativeness is minor if a single hypothesis is tested and many permutations are used, but can otherwise be substantial. The anti-conservativeness is limited for Meinshausen's method and Westfall and Young's method when α is not too small and many random permutations are used, but can in other situations be excessive. An example is our permutation-based global test. The cause of the anti-conservativeness is that the permutation distribution is not a good enough approximation of the true distribution.

We have shown that all the permutation-based methods considered become correct when the identity map is added to the random transformations. A fundamental assumption is the group structure of the set from which the random transformations are drawn.

Permutation-based methods can only be used when the null hypotheses imply invariance of the data distribution under transformation. There are many applications where this is the case. Often researchers use random permutations instead of the whole group. This influences the power of the methods, however. Beside using random permutations, a subgroup of the set of all permutations could be used. The question remains open what works best in which situation. As we remarked in section [\ref=singlerandom], we can in fact use any set of invertible transformations - not just groups - if we slightly adapt the test. In particular, when we use permutations we can use any subset of the permutation group. The question how such a subset should be chosen remains an open problem.

In the methods discussed, instead of permutation sampling we could use Monte Carlo sampling. As we remarked in section [\ref=singlerandom], the original observation then needs to be included with the random draws from the null distribution. If the original observation is not included, then we expect problems similar to when permutations are used.

Acknowledgements

We thank Aldo Solari and Vincent van der Noort for their valuable suggestions.

Radboud Institute for Molecular Life Sciences, Radboud University Medical Center, Geert Grooteplein 28, 6525 GA Nijmegen, The Netherlands E-mail: Jesse.Hemerik@radboudumc.nl, Jelle.Goeman@radboudumc.nl