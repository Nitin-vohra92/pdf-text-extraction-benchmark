=

Lemmata Lemma Claim Subclaim Proposition Corollary Fact Conjecture Question Example Definition Remark Observation

C. Seshadhri

Avoiding the Global Sort: A Faster Contour Tree Algorithm

Introduction

Geometric data (potentially massive in size) is often represented as a function [formula]. Typically, a finite representation is given by considering f to be piecewise linear over some triangulated mesh (i.e. simplicial complex) [formula] in [formula]. Contour trees are a specific topological structure used to represent and visualize the function f. For example, consider a rocket engine during a burn. The 3D mesh represents the locations of points in space around the engine, and f represents the temperature at each point. The boundary of the flame is then represented by an isotherm, i.e. a contour.

It is convenient to think of f as a manifold sitting in [formula], with the last coordinate (i.e. height) given by f. Imagine sweeping the hyperplane xd + 1  =  h with h going from +    ∞   to -    ∞  . At every instance, the intersection of this plane with f gives a set of connected components, the contours at height h. As the sweeping proceeds various events occur: new contours are created or destroyed, contours merge into each other or split into new components, contours acquire or lose handles. More generally, these are the events where the topology of the level set changes. The contour tree is a concise representation of all these events.

If f is smooth, all points where the gradient of f is zero are critical points. These points are the "events" where the contour topology changes and which form the vertices of the contour tree. An edge of the contour tree connects two critical points if one event immediately "follows" the other as the sweep plane makes its pass. (We provide formal definitions later.)

and show examples of simplicial complexes, with heights and their contour trees. Think of the contour tree edges as pointing downwards, so we can talk of the "up-degree" and "down-degree" of vertices. Leaves of down-degree 1 are maxima, because a new contour is created at such points. Leaves of up-degree 1 are minima, because a contour is destroyed there. Up-degree 2 vertices are "joins", where contours merge, and down-degree 2 vertices are "splits". When d = 2 (as in ), contours are closed loops. In dimensions d  ≥  3, we may have internal vertices of both up and down-degree 1. For example, these could be events where contours gain or lose handles.

Suppose we have [formula], where [formula] is a triangulated mesh with n vertices, N faces in total, and t  ≤  n critical points. It is standard to assume that f is PL-Morse (a well-behaved input assumption) and that the maximum degree in the contour tree is 3. A fundamental result in this area is the algorithm of Carr, Snoeyink, and Axen to compute contour trees, which runs in O(n log n  +  Nα(N)) time [\cite=csa-cctad-00] (where α(  ·  ) denotes the inverse Ackermann function). In practical applications, N is typically Θ(n) (certainly true for d = 2). The most expensive operation is an initial sort of all the vertex heights. Chiang et al. build on this approach to get a faster algorithm that only sorts the critical vertices, yielding a running time of O(t log t  +  N) [\cite=cllr-sooscctmp-05]. Common applications for contour trees involve turbulent combustion or noisy data, where the number of critical points is likely to be Ω(n). So we are still left with the bottleneck of sorting a large set of numbers. Unfortunately, there is a worst-case lower bound of Ω(t log t) by Chiang et al. [\cite=cllr-sooscctmp-05], based on a construction of Bajaj et al. [\cite=BaKr+98].

But this is only a worst-case bound. All previous algorithms begin by sorting (at least) the critical points, and hence must pay Ω(t log t) for all instances. Can we beat this sorting bound for certain instances, and can we characterize which inputs are hard? Our main result gives an affirmative answer.

For a contour tree T, a leaf path is any path in T containing a leaf, which is also monotonic in the height values of its vertices. Then a path decomposition, P(T), is a partition of the vertices of T into a set of vertex disjoint leaf paths.

Consider a PL-Morse [formula], where the contour tree T has maximum degree 3. There is a deterministic algorithm to compute T whose running time is [formula], where P(T) is a specific path decomposition (constructed implicitly by the algorithm).

The number of comparisons made is [formula].

In particular, any comparisons made are only between ancestors and descendants in the contour tree, while previous algorithms compared everything.

Let us interpret the running time. For any path decomposition P(T), obviously [formula], and so the running time is O(t log t  +  N). But this is tight only when T is tall and thin. A better upper bound is O(t log D  +  tα(t)  +  N), where D is the diameter of T. This is clearly an improvement when the tree is short and fat, but it can be even better. Consider a contour tree that is constant balanced, meaning that the ratio of sizes of sibling subtrees is Θ(1). A calculation yields that for any decomposition of such a contour tree, our bound is O(tα(t)  +  n). Consider the example of extended to a larger binary tree both above and below, so t  =  Ω(n). The running time of the algorithm of is O(nα(n)), whereas all previous algorithms require Ω(n log n) time.

We complement with a lower bound, suggesting that the comparison model complexity of [formula] is inherent for any contour tree algorithm. The exact phrasing of the lower bound requires some care. Consider the d = 2 (terrain) case. Let a path decomposition P be called valid if it is actually output by the algorithm on some input.

Consider a valid path decomposition P. There exists a family [formula] of terrains (i.e. d = 2) with the following properties. Any contour tree algorithm makes [formula] comparisons in the worst case over [formula]. Furthermore, for any terrain in [formula], the algorithm of makes [formula] comparisons.

In other words, for any path decomposition, P, we can design a hard family where any algorithm requires [formula] time in the worst case, and our algorithm matches this bound.

Beyond worst-case analysis.

Our result provides understanding of the complexity of contour tree construction beyond the standard worst-case analysis. Our theorem and lower bound characterize hard instances. Trees that are short and fat are easy to compute, whereas trees with long paths are hard. This is tangentially related to the concept of instance-optimal algorithms, as defined by Afshani et al. [\cite=abc-ioga-09]. Their notion of optimality is far more refined that what we prove, but it shares the flavor of understanding the spectrum of easy to hard instances. In general, our analysis forces one to look closely at the relationship between sorting and the contour tree problem.

Previous Work

Contour trees were first used to study terrain maps by Boyell and Ruston, and Freeman and Morse [\cite=BoRu63] [\cite=FrMo67]. They were used for isoline extraction in geometric data by van Krevald et al. [\cite=kobps-ctsssit-97], who provided the first formal algorithm. Contour trees and related topological structures have been applied in analysis of fluid mixing, combustion simulations, and studying chemical systems [\cite=LaBe+06] [\cite=BrWe+10] [\cite=BeWe+11] [\cite=BrWe+11] [\cite=MaGr+11]. Carr's thesis [\cite=c-tmi-04] gives various applications of contour trees for data visualization and is an excellent reference for contour tree definitions and algorithms.

There are numerous algorithms for computing contour trees. An O(N log N) time algorithm for functions over 2D meshes and an O(N2) algorithm for higher dimensions, was presented by van Kreveld et al. [\cite=kobps-ctsssit-97]. Tarasov and Vyalya [\cite=tv-cct-98] improved the running time to O(N log N) for the 3D case. The influential paper of Carr et al. [\cite=csa-cctad-00] improved the running time for all dimensions to O(n log n  +  Nα(N)). This result forms the basis of subsequent practical and theoretical work in computational topology applications. Pascucci and Cole-McLaughlin [\cite=pc-ectls-02] provided an O(n + t log n) time algorithm for 3-dimensional structured meshes. Chiang et al. [\cite=cllr-sooscctmp-05] provide an unconditional O(N + t log t) algorithm. The latter result crucially uses monotone paths to improve the running time, and our algorithm repeatedly exploits numerous properties of monotone paths. Carr's thesis shows relationships between monotone paths in [formula] and its contour tree [formula].

Contour trees are a special case of Reeb graphs, a general topological representation for real-valued functions on any manifold. We refer the reader to Chapter 6 in [\cite=HaEd10] for more details. Algorithms for computing Reeb graphs is an active topic of research [\cite=sk-crgacs-91] [\cite=cehnp-lrbm-03] [\cite=PaScBr07] [\cite=DoNa09] [\cite=HaWaWe10] [\cite=Pa12], where two results explicitly reduce to computing contour trees [\cite=TiGySi09] [\cite=DoNa13].

Contour tree basics

We detail the basic definitions about contour trees. We follow the terminology of Chapter 6 of Carr's thesis [\cite=c-tmi-04]. All our assumptions and definitions are standard for results in this area, though there is some variability in notation. The input is a continuous piecewise-linear (PL) function [formula], where [formula] is a simply connected and fully triangulated simplicial complex in [formula], except for specially designated boundary facets. So f is defined only on the vertices of [formula], and all other values are obtained by linear interpolation.

We assume that f has distinct values on all vertices, except for boundaries, as expressed in the following constraint, which intuitively requires that all boundaries look like (local) horizontal cuts.

The function f is boundary critical if the following holds. Consider a boundary facet F. All vertices of F have the same function value. Furthermore, all neighbors of vertices in F, which are not also in F itself, either have all function values strictly greater than or all function values strictly less than the function value at F.

This is convenient, as we can now assume that f is defined on [formula]. Any point inside a boundary facet has a well-defined height, including the infinite facet, which is required to be a boundary facet. However, we allow for other boundary facets, to capture the resulting surface pieces after our algorithm make a horizontal cut.

We think of the dimension, d, as constant, and assume that [formula] is represented in a data structure that allows constant-time access to neighboring simplices in [formula] (e.g. [\cite=BoMa12]). (This is analogous to a doubly connected edge list, but for higher dimensions.) Observe that [formula] can be thought of as a d-dimensional simplicial complex living in [formula], where f(x) is the "height" of a point [formula], which is encoded in the representation of [formula]. Specifically, rather than writing our input as [formula], we abuse notation and typically just write [formula] to denote the lifted complex.

The level set at value h is the set {x|f(x)  =  h}. A contour is a maximal connected component of a level set. An h-contour is a contour where f-values are h.

Note that a contour that does not contain a boundary is itself a simplicial complex of one dimension lower, and is represented (in our algorithms) as such. We let δ and ε denote infinitesimals. Let [formula] denote a ball of radius ε around x, and let [formula] be the restriction of f to [formula].

The Morse up-degree of x is the number of (f(x)  +  δ)-contours of [formula] as δ,ε  →  0+. The Morse down-degree is the number of (f(x)  -  δ)-contours of [formula] as δ,ε  →  0+.

We categorize points depending on the local changes in topology.

A point x is categorized as follows. Non-regular points are critical. Morse joins and splits are saddles. Maxima and minima are extrema.

The set of critical points is denoted by V(f). Because f is piecewise-linear, all critical points are vertices in [formula]. A value h is called critical, if f(v)  =  h, for some v∈V(f). A contour is called critical, if it contains a critical point, and it is called regular otherwise.

The critical points are exactly where the topology of level sets change. By assuming that our manifold is boundary critical, the vertices on a given boundary are either collectively all maxima or all minima. We abuse notation and refer to this entire set of vertices as a maximum or minimum.

Two regular contours ψ and ψ' are equivalent if there exists an f-monotone path p connecting a point in ψ to ψ', such that no x∈p belongs to a critical contour.

This equivalence relation gives a set contour classes. Every such class maps to intervals of the form (f(xi),f(xj)), where xi,xj are critical points. Such a class is said to be created at xi and destroyed at xj.

The contour tree is the graph on vertex set V  =  V(f), where edges are formed as follows. For every contour class that is created at vi and destroyed vj, there is an edge (vi,vj). (Conventionally, edges are directed from higher to lower function value.)

We denote the contour tree of [formula] by [formula]. The corresponding node and edge sets are denoted as V(  ·  ) and E(  ·  ). It is not immediately obvious that this graph is a tree, but alternate definitions of the contour tree in [\cite=csa-cctad-00] imply this is a tree. Since this tree has height values associated with the vertices, we can talk about up-degrees and down-degrees in [formula]. We assume there are no "multi-saddles", so up and down-degrees are at most 2, and the total degree is at most 3. This is again a standard assumption in topological algorithms and can be achieved by vertex unfolding (Section 6.3 in [\cite=HaEd10]).

Some technical remarks

Note that if one intersects [formula] with a given ball B, then a single contour in [formula] might be split into more than one contour in the intersection. In particular, two (f(x) + δ)-contours of [formula], given by , might actually be the same contour in [formula]. Alternatively, one can define the up-degree (as opposed to Morse up-degree) as the number of (f(x) + δ)-contours (in the full [formula]) that intersect [formula], a potentially smaller number. This up-degree is exactly the up-degree of x in [formula]. (Analogously, for down-degree.) When the Morse up-degree is 2 but the up-degree is 1, the topology of the level set changes but not by the number of connected components changing. For example, when d = 3 this is equivalent to the contour gaining a handle. When d = 2, this distinction is not necessary, since any point with Morse degree strictly greater than 1 will have degree strictly greater than 1 in [formula].

As Carr points out in Chapter 6 of his thesis, the term contour tree can be used for a family of related structures. Every vertex in [formula] is associated with an edge in [formula], and sometimes the vertex is explicitly placed in [formula] (by subdividing the respective edge). This is referred to as augmenting the contour tree, and it is common to augment [formula] with all vertices. Alternatively, one can smooth out all vertices of up-degree and down-degree 1 to get the unaugmented contour tree. (For d = 2, there are no such vertices in [formula].) The contour tree of is the typical definition in all results on output-sensitive contour trees, and is the smallest tree that contains all the topological changes of level sets. is applicable for any augmentation of [formula] with a predefined set of vertices, though we will not delve into these aspects in this paper.

A tour of the new contour tree algorithm

Our final algorithm is quite technical and has numerous moving parts. However, for the d = 2 case, where the input is just a triangulated terrain, the main ideas of the parts of the algorithm can be explained clearly. Therefore, in the remaining few allotted pages, rather than providing an incomplete subset of the technical content, we provide a high level view of the entire result, which we view as essential for understanding our approach. The technical details can be found in the full version of the paper [\cite=rs-mvrms-14].

In the interest of presentation, the definitions and theorem statements in this section will slightly differ from those in the main bodyof the full version. They may also differ from the original definitions proposed in earlier work.

Do not globally sort:

Arguably, the starting point for this work is . We have two terrains with exactly the same contour tree, but different orderings of (heights of) the critical points. Turning it around, we cannot deduce the full height ordering of critical points from the contour tree. Sorting all critical points is computationally unnecessary for constructing the contour tree. In , the contour tree consists of two balanced binary trees, one of the joins, another of the splits. Again, it is not necessary to know the relative ordering between the mounds on the left (or among the depressions on the right) to compute the contour tree. Yet some ordering information is necessary: on the left, the little valleys are higher than the big central valley, and this is reflected in the contour tree. Leaf paths in the contour tree have points in sorted order, but incomparable points in the tree are unconstrained. How do we sort exactly what is required, without knowing the contour tree in advance?

Breaking [formula] into simpler pieces

Let us begin with the algorithm of Carr, Snoeyink, and Axen [\cite=csa-cctad-00]. The key insight is to build two different trees, called the join and split trees, and then merge them together into the contour tree. Consider sweeping down via the hyperplane xd + 1  =  h and taking the superlevel sets. These are the connected components of the portion of [formula] above height h. For a terrain, the superlevel sets are a collection of "mounds". As we sweep downwards, these mounds keep joining each other, until finally, we end up with all of [formula]. The join tree tracks exactly these events. Formally, let [formula] denote the simplicial complex induced on the subset of vertices which are higher than v.

The critical join tree [formula] is built on the set V of all critical points. The directed edge (u,v) is present when u is the smallest valued vertex in V in a connected component of [formula] and v is connected to this component (in [formula]).

Abusing notation, for now we will just call this the join tree. (In the full version we carefully handle the distinction between the join tree and the critical join tree.) Refer to for the join tree of a terrain. Note that nothing happens at splits, but these are still put as vertices in the join tree. They simply form a long path. The split tree is obtained by simply inverting this procedure, sweeping upwards and tracking sublevel sets.

A major insight of [\cite=csa-cctad-00] is an ingeniously simple linear time procedure to construct the contour tree from the join and split trees. So the bottleneck is computing these trees. Observe in that the split vertices form a long path in the join tree (and vice versa). Therefore, constructing these trees forces a global sort of the splits, an unnecessary computation for the contour tree. Unfortunately, in general (i.e. unlike ) the heights of joins and splits may be interleaved in a complex manner, and hence the final merging of [\cite=csa-cctad-00] to get the contour tree requires having the split vertices in the join tree. Without this, it is not clear how to get a consistent view of both joins and splits, required for the contour tree.

Our aim is to break [formula] into smaller pieces, where this unnecessary computation can be avoided.

Contour surgery:

We first need a divide-and-conquer lemma. Any contour φ can be associated with an edge e of the contour tree. Suppose we "cut" [formula] along this contour. We prove that [formula] is split into two disconnected pieces, such the contour trees of these pieces is obtained by simply cutting e in [formula]. Alternatively, the contour trees of these pieces can be glued together to get [formula]. This is not particularly surprising, and is fairly easy to prove with the right definitions. The idea of loop surgery has been used to reduce Reeb graphs to contour trees [\cite=TiGySi09] [\cite=DoNa13]. Nonetheless, our theorem appears to be new and works for all dimensions.

Cutting [formula] into extremum dominant pieces:

We define a simplicial complex endowed with a height to be minimum dominant if there exists only a single minimum. (Our real definition is more complicated, and involves simplicial complexes that allow additional "trivial" minima.) In such a complex, there exists a non-ascending path from any point to this unique minimum. Analogously, we can define maximum dominant complexes, and both are collectively called extremum dominant.

We will cut [formula] into disjoint extremum dominant pieces, in linear time. One way to think of our procedure is a meteorological analogy. Take an arbitrary maximum x, and imagine torrential rain at the maximum. The water flows down, wetting any point that has a non-ascending path from x. We end up with two portions, the wet part of [formula] and the dry part. This is similar to watershed algorithms used for image segmentation [\cite=RoMe00]. The wet part is obviously connected, while there may be numerous disconnected dry parts. The interface between the dry and wet parts is a set of contours, given by the "water line". The wet part is clearly maximum dominant, since all wet points have a non-descending path to x. So we can simply cut along the interface contours to get the wet maximum dominant piece [formula]. By our contour surgery theorem, we are left with a set of disconnected dry parts, and we can recur this procedure on them.

But here's the catch. Every time we cut [formula] along a contour, we potentially increase the complexity of [formula]. Water flows in the interior of faces, and the interface will naturally cut some faces. Each cut introduces new vertices, and a poor choice of repeated raining leads to a large increase in complexity. Consider the left of . Each raining produces a single wet and dry piece, and each cut introduces many new vertices. If we wanted to partition this terrain into maximum dominant simplicial complexes, the final complexity would be forbiddingly large.

A simple trick saves the day. Unlike reality, we can choose rain to flow solely downwards or solely upwards. Apply the procedure above to get a single wet maximum dominant [formula] and a set of dry pieces. Observe that a single dry piece [formula] is boundary critical with the newly introduced boundary φ (the wet-dry interface) behaving as a minimum. So we can rain upwards from this minimum, and get a minimum dominant portion [formula]. This ensures that the new interface (after applying the procedure on [formula]) does not cut any face previously cut by φ. For each of the new dry pieces, the newly introduced boundary is now a maximum. So we rain downwards from there. More formally, we alternate between raining upwards and downwards as we go down the recursion tree. We can prove that an original face of [formula] is cut at most once, so the final complexity can be bounded. In , this procedure would yield two pieces, one maximum dominant, and one minimum dominant.

Using the contour surgery theorem previously discussed, we can build the contour tree of [formula] from the contour trees of the various pieces created. All in all, we prove the following theorem.

There is an O(N) time procedure that cuts [formula] into extremum dominant simplicial complexes [formula]. Furthermore, given the set of contour trees [formula], [formula] can be constructed in O(N) time.

Extremum dominance simplifies contour trees:

We will focus on minimum dominant simplicial complexes [formula]. By , it suffices to design an algorithm for contour trees on such inputs. For the d = 2 case, it helps to visualize such an input as a terrain with no minima, except at a unique boundary face (think of a large boundary triangle that is the boundary). All the valleys in such a terrain are necessarily joins, and there can be no splits. Look at . The portion on the left is minimum dominant in exactly this fashion. More formally, [formula] is connected for all v, so there are no splits.

We can prove that the split tree is just a path, and the contour tree is exactly the join tree. The formal argument is a little involved, and we employ the merging procedure of [\cite=csa-cctad-00] to get a proof. We demonstrate that the merging procedure will actually just output the join tree, so we do not need to actually compute the split tree. (The real definition of minimum dominant is a little more complicated, so the contour tree is more than just the join tree. But computationally, it suffices to construct the join tree.)

We stress the importance of this step for our approach. Given the algorithm of [\cite=csa-cctad-00], one may think that it suffices to design faster algorithms for join trees. But this cannot give the sort of optimality we hope for. Again, consider . Any algorithm to construct the true join tree must construct the path of splits, which implies sorting all of them. It is absolutely necessary to cut [formula] into pieces where the cost of building the join tree can be related to that of building [formula].

Join trees from painted mountaintops

Arguably, everything up to this point is a preamble for the main result: a faster algorithm for join trees. Our algorithm does not require the input to be extremum dominant. This is only required to relate the join trees to the contour tree of the initial input [formula]. For convenience, we use [formula] to denote the input here. Note that in , the join tree is defined purely combinatorially in terms of the 1-skeleton (the underlying graph) of [formula].

The join tree [formula] is a rooted tree with the dominant minimum at the root, and we direct edges downwards (towards the root). So it makes sense to talk of comparable vs incomparable vertices. We arrive at the main challenge: how to sort only the comparable critical points, without constructing the join tree? The join tree algorithm of [\cite=csa-cctad-00] is a typical event-based computational geometry algorithm. We have to step away from this viewpoint to avoid the global sort.

The key idea is paint spilling. Start with each maximum having a large can of paint, with distinct colors for each maximum. In arbitrary order, we spill paint from each maximum, wait till it flows down, then spill from the next, etc. Paint is viscous, and only flows down edges. It does not paint the interior of higher dimensional faces. That is, this process is restricted to the 1-skeleton of [formula]. Furthermore, our paints do not mix, so each edge receives a unique color, decided by the first paint to reach it. In the following,

[formula]

The running time analysis:

Relating the running time to a path decomposition is the most technical part of the paper. All the non-heap operations can be easily bounded by O(tα(t)  +  N) (the tα(t) is from the union-find data structure for colors). It is not hard to argue that at all times, any heap always contains a subset of a leaf to root path. Unfortunately, this subset is not contiguous, as the right part of shows. Specifically, in this figure the far left saddle (labeled i) is hit by blue paint. However, there is another saddle on the far right (labeled j) which is not hit by blue paint. Since this far right saddle is slightly higher than the far left one, it will merge into the component containing the blue mound (and also the yellow and red mounds) before the far left one. Hence, the vertices initially touched by blue are not contiguous in the contour tree.

Nonetheless, we can get a non-trivial (but non-optimal) bound. Let dv denote the distance to the root for vertex v in the join tree. The total cost (of the heap operations) is at most [formula]. This immediately proves a bound of O(t log D), where D is the maximum distance to the root, an improvement over previous work. But this bound is non-optimal. For a balanced binary tree, this bound is O(t log  log t), whereas the cost of any path decomposition is O(t).

The cost of heap operations depends on heap sizes, which keep changing because of the repeated merging. This causes the analysis to be technically challenging. There are situations where the initial heap sizes are small, but they eventually merge to create larger heaps. We employ a variant of heavy path decompositions, first used by Sleator and Tarjan for analyzing link/cut trees [\cite=st-dsdt-83]. The final analysis charges expensive heap operations to long paths in the decomposition.

The lower bound

Consider a contour tree T and the path decomposition P(T) used to bound the running time. Denoting [formula], we construct a set of [formula] functions on a fixed domain such that each function has a distinct (labeled) contour tree. By a simple entropy argument, any algebraic decision tree that correctly computes the contour tree on all instances requires worst case [formula] time. We prove that our algorithm makes [formula] comparisons on all these instances.

We have a fairly simple construction that works for terrains. In P(T), consider the path p that involves the root. The base of the construction is a conical "tent", and there will be |p| triangular faces that will each have a saddle. The heights of these saddles can be varied arbitrarily, and that will give |p|! different choices. Each of these saddles will be connected to a recursive construction involving other paths in P(T). Effectively, one can think of tiny tents that are sticking out of each face of the main tent. The contour trees of these tiny tents attach to a main branch of length |p|. Working out the details, we get [formula] terrains each with a distinct contour tree.

Acknowledgements.

We thank Hsien-Chih Chang, Jeff Erickson, and Yusu Wang for numerous useful discussions. This work is supported by the Laboratory Directed Research and Development (LDRD) program of Sandia National Laboratories. Sandia National Laboratories is a multi-program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy�s National Nuclear Security Administration under contract DE-AC04-94AL85000.