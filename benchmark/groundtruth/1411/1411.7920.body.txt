Probability Theory without Bayes' Rule

Introduction

Let A and B be random variables taking values [formula] and [formula], respectively, and let P(ai,bj) be the joint probability distribution for A and B, which may be represented as a matrix [formula] of size NA  ×  NB. The conditional probability distribution P(ai|bj) may likewise be represented as an NA  ×  NB matrix, denoted [formula], with the i,jth component given according to the standard Kolmogorov axioms of probability [\cite=Kolmogorov33] by

[formula]

In the Kolmogorov framework, P(ai,bj) is viewed as the measure of the intersection of sets representing ai and bj. It thus follows from the symmetry of the intersection operator that P(ai,bj)  =  P(bj,ai). From the definition of conditional probabilities we can thus derive Bayes' rule, which gives the relationship between P(A|B) and P(B|A):

[formula]

However, Bayes' rule gives rise to some surprising behavior. By unitarity and additivity, it follows that

[formula]

and hence

[formula]

Equation [\ref=eq4] is simply the statement that the vector [formula] with components P(ai) is obtained from the vector [formula] with components P(bj) by the action of [formula]. Likewise for B, we have

[formula]

By substitution, it follows that

[formula]

Given [formula], equation [\ref=InversionFallacy] must hold for all choices of the probability distribution [formula]. One natural conclusion would be that [formula] and [formula] are inverses, i.e.,

[formula]

However, the matrix inverse of [formula] will have negative values and values greater than 1 in general. Instead, given constant [formula], [formula] varies with [formula] according to Bayes' rule, in such a way that [formula] is always an eigenvalue of the matrix [formula] with eigenvalue 1. This situation is particularly dissatisfying when one thinks of [formula] as describing the action of a channel, in which case [formula] would describe the result of running the channel backwards. In this case, Bayes' rule does not allow a simultaneous description of the forward channel and the reverse channel independent of the distribution that goes through it. One imagines that the matrix inverse might be a more appropriate description of the reverse channel than the matrix [formula] obtained via Bayes' rule.

The matrix inverse of [formula] shows up in other ways as well. For example, suppose we are given [formula] and [formula], and wish to infer [formula]. This task is not possible using Bayes' rule, since [formula] depends on [formula]; one requires a prior distribution. Nonetheless, [formula] may always be obtained from [formula] via the inverse of the matrix [formula]:

[formula]

Within the strict framework of Kolmogorov probability theory, one must view this procedure as a coincidence, since the matrix P(A|B)- 1 is not a conditional probability distribution.

As we show here, the application of [formula] in this way may be explained using an alternative axiomatization of probability theory in which Bayes' rule is replaced by the requirement that

[formula]

referred to as the "inversion rule." Moreover, the inversion rule is only one of a continuous set of possible alternative axiomatizations of probability theory, each of which comes with its own rule for statistical inference. In the second section, we provide the general axiomatic framework for probability theory that we will consider in the remainder of the paper. In the third section, we explore the specific axiom that gives rise to equation [\eqref=Inv] as a rule for inference, and show how it may be used alongside Bayes' rule in practice. Finally, in the fourth section, we characterize the set of all possible inference rules. We find that Bayes' rule and the inversion rule form a conjugate pair, and that they can be used to generate all other rules for inference.

Axioms

We seek to formulate a set of axioms that preserves the notion of a probability distribution in its entirety without Bayes' rule. Since Bayes' rule is a consequence of the symmetry of the intersection operator, it is necessary to replace Kolmogorov's axiomatization in terms of set theory by a similar axiomatization in terms of sequences. The primary hurdle will be to establish analogs of the union and intersection operators for sequences, which is necessary to facilitate comparison with the Kolmogorov axioms.

We consider the case of N random variables [formula], where A(i) takes values aij∈Ji, for [formula]. Let S be the set of all permutations of the integers from 1 to N. Given some s∈S, we denote by Es the set of all sequences Q of N elements such that the ith element in Q is in Js(i). Q is then said to have order s. Given sequences Q and Q', Q is said to be a subsequence of Q' if Q may be obtained by removing elements from Q'. We denote by Fs the set of all subsequences of elements of Es. A sequence Q is then said to have order s if it is in Fs. For any sequence Q, we denote by [formula] the set of elements appearing at some position in Q, referred to as the "membership set" of Q. We write Q  ⊆  sQ' if Q and Q' both have order s, and if   ⊆  '.

For any Q∈Fs, we define Rs(Q) to be the set of sequences in Es of which Q is a subsequence, i.e.,

[formula]

We may now evidently define the operator [formula] such that for all Q1,Q2∈Fs,

[formula]

Likewise, we define the operator [formula] such that for all Q1, Q2∈Fs,

[formula]

The axioms are then as follows, and are formulated for the greatest similarity to the original axioms of Kolmogorov:

Crucially, we note that because P is not explicitly a function of s, axiom [\ref=Kol2] implies that for all s,s'∈S and for all [formula],

[formula]

Equation [\ref=CausalityConstraint] is equivalent to the statement that the marginal probability distribution over a subset of variables depends only on the marginal ordering of the variables in the subset.

For any given value of s, the axioms [\ref=Kol1] through [\ref=Kol4] are identical to the standard statement of Kolmogorov's axioms without the requirement that P(Q) be non-negative. Thus, the axioms are consistent, but not complete.

For the sake of concreteness, we will henceforth focus on the case of two variables, A and B, so that there are only two orderings. Then a conditional probability may be easily defined:

[formula]

Note that the ordering of the variables should be read off from right to left, so P(ai,bj) is the probability of observing ai and bj relative to the ordering in which B precedes A. This convention has been chosen to keep with the standard convention for conditional probabilities, according to which the conditioning variables are written on the right, and the conditioned variables are written on the left.

We are interested in our ability to perform statistical inference within this system of axioms. In the problem of statistical inference, we consider an observable variable A and a hidden variable B. The goal is to determine a posterior for the hidden variable, P(bj|ai), in terms of a model, P(ai|bj), and a prior, P(bj). Within the current axiomatic system, the posterior distribution P(bj|ai) is constrained in terms of P(ai|bj) and P(bj) via equation [\ref=CausalityConstraint], which applied to the current system becomes

[formula]

However, this constraint does not specify P(bj|ai) uniquely in terms of P(ai|bj). Thus, in order to perform statistical inference, it is necessary to impose an additional "inference axiom" that provides us with a rule for inferring P(bj|ai) in terms of P(ai|bj) and P(bj). Bayes' rule is one possible rule of inference, and may be recovered from the current axiomatization by requiring that the joint probability over some set of variables is independent of the ordering of those variables, in which case P(ai,bj)  =  P(bj,ai) for all i and j, and hence

[formula]

As we show below, however, there are many other choices of inference axioms that will nonetheless yield correct results.

In all that follows, we will assume that [formula] is invertible, which ensures that the problem of inference will be uniquely solvable.

The Inversion Rule

Before examining the set of all possible inference axioms, it will be useful to examine one particular inference rule, the inversion rule, in depth. The inversion rule is given by equation [\eqref=Inv], and is noteworthy for being the only inference rule in which [formula] is specified exclusively in terms of [formula]. It is thus associated with the following inference axiom:

We prove that this axiom uniquely specifies the inversion rule as follows:

For all non-product distributions P(A,B), the matrices [formula] and [formula] are equal to the identities of size NA and NB respectively.

From the definition of conditional probability, we have

[formula]

and

[formula]

From eq. [\eqref=CausalityConstraint], we have

[formula]

Eq. [\eqref=InvThm1] may be recast in matrix form as

[formula]

Equation [\ref=InvThm2] must hold for all choices of [formula]. However, [formula] is independent of [formula] by axiom [\ref=IndependenceAx]. It follows that [formula] must be the identity. The proof for [formula] is identical.

Equation [\eqref=InvThm2] may in general still be applied even if [formula] does not have full rank by restricting the inversion to the support of [formula]. However, when [formula] is a product distribution, all quantities associated with all other orderings of the variables are undefined. In the case of a product distribution, it is impossible to decouple [formula] from [formula] since P(ai|bj)  =  P(ai) for all j, so axiom [\ref=IndependenceAx] is inconsistent with the other axioms.

The joint distributions for the two different orderings may be related as follows:

Let [formula] be a non-product joint distribution. Then,

[formula]

Combining [\eqref=InvThm2] with the definition of conditional probability, it is clear that

[formula]

and

[formula]

Inverting [\eqref=MainDeriv0] and equating the left-hand sides of the two equations, we obtain

[formula]

from which it follows that

[formula]

Clearly, if all the entries of [formula] are positive then the entries of [formula] will neither be strictly positive nor smaller than 1 in general.

A further interesting consequence of Theorem [\ref=ReversalThm] is that the function P is completely specified by the joint distribution for only a single order, and hence contains the same amount of information as the joint distribution in the Kolmogorov framework. More generally, as long as the chosen inference axiom specifies [formula] completely in terms of [formula] and [formula], [formula] will be completely specified in terms of [formula]. For this reason, we are always free to choose a preferred ordering of the variables, relative to which all joint, conditional, and marginal probabilities agree with the Kolmogorov probabilities:

Let PK be a non-product probability distribution obeying the Kolmogorov axioms of probability. Then there exists a probability distribution PI obeying axioms [\ref=Kol1] through [\ref=Kol4] and [\ref=IndependenceAx] such that for all i and j, PK(ai,bj)  =  PI(ai,bj).

Corollary [\ref=AgCor] allows us to use the inversion rule to perform statistical inference on Kolmogorov probabilities as follows. Given a model [formula] and an empirical estimate P̃K(A) for [formula], we consider the probability distribution [formula] in the inversion framework, defined such that [formula]. Then, the best empirical estimate for P̃I(B) using the inversion rule is given by

[formula]

Furthermore, in the limit as [formula], the estimate P̃I(B) obviously converges to [formula]. Because [formula], we may interpret P̃I(B) as an approximation to P̃K(B):

[formula]

Equation [\ref=KolApprox1] cannot be justified on the basis of the axioms of Kolmogorov probability theory. In the Kolmogorov framework, equation [\ref=InfUsingInv] can only be used to relate the specific distributions [formula] and [formula]. Because [formula] is not a conditional probability distribution, there is no guarantee that P̃K(B) obtained using [\eqref=KolApprox1] is a probability distribution. Nonetheless, using the inversion framework, we have proven the validity of [\eqref=KolApprox1] as a method of performing inference.

One interpretation of the preceding discussion is that it is possible to use the inversion rule to infer a prior from the data. Surprisingly, because the inversion rule and Bayes' rule are independent, one may then apply Bayes' rule to do Bayesian inference using the inferred prior. We have

[formula]

Here, [formula] is the diagonal matrix with the entries of [formula] along the diagonal. The estimate K(B|A) converges to [formula] as [formula], so P̃(B) is guaranteed to be a "good" prior, even if it is not a Kolmogorov probability distribution.

All Possible Inference Axioms

We now examine the set of all possible inference axioms that can be added to axioms [\ref=Kol1] through [\ref=Kol4]. An inference axiom is any way of specifying [formula] in terms of [formula] and [formula]. For an inference axiom to be consistent with axioms [\ref=Kol1] through [\ref=Kol4], the matrix [formula] must satisfy equation [\ref=CausalityConstraint2]. For equation [\ref=CausalityConstraint2] to be satisfied, the columns of [formula] must sum to 1, and the marginal distributions must not depend on the order of the joint distribution over which one chooses to marginalize. From this latter requirement and the definition of conditional probabilities, we derive the requirement that

[formula]

Multiplying both sides by [formula], we obtain

[formula]

Because the right-hand side is equal to [formula], we can multiply both sides by [formula], obtaining

[formula]

where [formula] is the vector of 1s with support equal to the support of [formula]. We define [formula] by

[formula]

Substituting equation [\eqref=Rdef] into [\eqref=Axiomeq], we obtain

[formula]

For any matrix [formula] obeying [\eqref=REigEq] for all choices of [formula] and [formula], there is definition of [formula] that satisfies [\eqref=CausalityConstraint2], given by

[formula]

In retrospect, equation [\ref=QEqn] is obvious. For all choices of [formula] and [formula], [formula] must map [formula] onto [formula]. This requirement is clearly satisfied only if the matrix [formula] given in [\eqref=QEqn] maps [formula] onto [formula]. For the columns of [formula] to sum to 1, we further require

[formula]

From [\eqref=QEqn], this is equivalent to the statement that

[formula]

or equivalently,

[formula]

so [formula] must itself be the transpose of a conditional probability distribution. The challenge is now to enumerate the set of matrices [formula] that satisfy both [\eqref=REigEq] and [\eqref=REigEq2]. We can categorize the choices by the order of their dependency on [formula].

Zeroth-order axioms

It is possible to specify [formula] entirely independently of [formula], while still satisfying both of the requirements of [\eqref=CausalityConstraint2]. One such specification involves setting all of the columns of [formula] to be equal to [formula], or equivalently, setting [formula] to be the matrix in which all of the rows are given by [formula]. However, this axiom is uninteresting from the perspective of inference, since it is impossible to infer [formula] without knowing [formula] to begin with. The corresponding rule says, if [formula] is already known, one can simply ignore all measurements.

First-order axioms

The set of first-order axioms is a one-dimensional convex hull, the two extreme points of which form a conjugate pair. The first extreme point is obtained by setting [formula] equal to (P(A|B)T)- 1. Applying [\eqref=QEqn], we recover Bayes' rule:

[formula]

The matrix [formula] given in [\eqref=BayesRuleAgain] also maps [formula] onto [formula], and also satisfies [\eqref=REigEq2], so it is also a candidate for [formula]. This results in the second axiom:

[formula]

Applying [\eqref=QEqn], we find the inversion rule,

[formula]

The conjugacy of Bayes' rule and the inversion rule is now obvious. Denoting by [formula] the axiom specifying Bayes' rule and by [formula] the axiom specifying the inversion rule, we have

[formula]

and likewise,

[formula]

The set of all first-order axioms may then be obtained by noting that any convex combination of first-order axioms is also a first-order axiom, so the set of first-order axioms is a convex hull. Since there are no other matrix functions first order in P(A|B) that obey [\eqref=REigEq] and [\eqref=REigEq2], the entire set of first-order axioms may be parametrized by

[formula]

for p∈[0,1].

Higher-order Axioms

Just as any convex combination of [formula] matrices is also a [formula] matrix, any product of [formula] matrices and [formula]-matrix inverses is a [formula] matrix as well. To see this, note that if [formula] maps [formula] to [formula], then [formula] maps [formula] to [formula]. Likewise, if [formula] maps [formula] to [formula], then [formula] maps [formula] to [formula]. The set of higher-order axioms may thus be generated by combining different first-order [formula] matrices in the pattern

[formula]

corresponding to the rule

[formula]

where [formula] is the conditional probability associated with [formula]. Evidently, rules only exist at odd orders, and clearly the set of nth-order rules is the convex hull of the nth-order product set of the set of first-order rules. As an example of a rule at higher-order, we have

[formula]

which implies

[formula]

This particular rule has the property that it discounts the evidence in favor of the prior and the model, as is evident from the fact that in this system, [formula] depends only at first order on [formula], while it depends on [formula] at second order and [formula] at third order. Although it is more costly to implement computationally, it also displays some properties that may be useful for machine learning applications. For example, unlike for Bayes' rule or the inversion rule, the columns of [formula] will not in general sum to 1 unless the correct values of [formula] and [formula] are used in the calculation, thus providing a test of convergence.

Conclusion

We have shown that the axiomatization of probability theory in terms of set theory originally formulated by Kolmogorov imposes strong conditions on the form of the reverse conditional probabilities. These conditions ensure desirable properties required for consistency with the frequentist interpretation of probability theory, including positivity and correct behavior when P(A,B) is a product distribution or not invertible. However, they are not necessary for correct statistical inference. Within a more general axiomatization, one can use alternative rules for statistical inference in parallel with Bayes' rule, leading to unexpected new ways of performing inference. As a result, alternative rules of inference may prove more useful than Bayes' rule for some applications.

In addition, we ask whether there may be physical circumstances in which an alternative axiomatization of probability theory would be preferred to the Kolmogorov theory. For example, it is known that although observable probabilities are always positive, the underlying distributions describing quantum mechanical states do not necessarily obey positivity [\cite=Wigner32]. Indeed, the violation of positivity by quantum states has been shown to be a fundamental distinguishing property of quantum mechanics [\cite=Spekkens08]. It may be that for describing these quantum mechanical probability distributions, which are not necessarily subject to a standard frequentist interpretation, an alternative axiomatization of probability is more suitable.