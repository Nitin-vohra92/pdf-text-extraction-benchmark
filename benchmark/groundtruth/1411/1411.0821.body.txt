Corollary Conjecture Lemma Definition Proposition Proposition Remark

NP-hardness of hypercube 2-segmentation

Introduction

We consider the following problem.

Hypercube 2-segmentation (H2S). Given a set of k vectors [formula] in {0,1}d, one needs to select two centers c1,c2 in {0,1}d maximizing

[formula]

where agree(x,y) counts on how many coordinates vectors x and y agree (which is d minus the Hamming distance between x and y).

H2S may also be phrased in the following equivalent way.

H2S - [formula] maximization formulation. Given a set of k vectors [formula] in {1, - 1}d, partition the k vectors into two sets, such that the sum of the [formula] norms of the two corresponding vector sums is maximized.

The equivalence between the two formulations of H2S follows from the fact that for a set of vectors on the hypercube, the location of the center that maximizes agreement is determined by taking the majority value on each coordinate separately. The [formula] norm of the sum shows how much this optimal center gains compared to placing a center at 0d (the center of the {1, - 1}d hypercube).

The following theorem was claimed in [\cite=KPR98] without proof.

The hypercube 2-segmentation problem is NP-hard.

In this manuscript we provide a proof of this theorem.

Related work

Kleinberg, Papadimitriou and Raghavan [\cite=KPR98] undertook a systematic study of the complexity of segmentation problems. Item (4) of Theorem 2.1 in [\cite=KPR98] claims that H2S is NP-hard. The proof sketch of that theorem states that the proof is by reduction "from maximum satisfiability with clauses that are equations modulo two", and gives no further details. That paper also shows that given any set of vectors, at least one of these vectors is a nearly optimal center for the set: its agreement score with the set of vectors is at least a [formula] fraction of that of the optimal center. This easily implies a polynomial time algorithm for approximating H2C within a ratio 0.828.

Alon and Sudakov [\cite=AS99] provide a PTAS for H2S. Specifically, for every choice of ε  >  0 they provide a linear time algorithm (with leading constant that depends on ε) that approximates H2S with a factor no worse than 1  -  ε. Similar results apply to hypergraph k-segmentation for constant k  >  2.

In [\cite=KPR04], the journal version of [\cite=KPR98], Item (3) of Theorem 1.1 claims that H2S is MAXSNP-complete, and cites [\cite=KPR98] as reference, without providing a proof. This claim of MAXSNP-completeness contradicts the fact (proved in [\cite=AS99]) that H2S has a PTAS, and hence is incorrect. (The authors of [\cite=KPR04] do cite [\cite=AS99].)

Wulff, Urner and Ben-David [\cite=WUB13] study a problem that they refer to as monochromatic biclustering (MCBC). They present a PTAS for (the maximization version of) MCBC, and also prove NP-hardness of MCBC in the case that the input instance may contain don't care symbols. This NP-hardness result is based on a reduction from max-cut. In [\cite=WUB13], it is conjectured that NP-hardness holds even without don't care symbols. H2C is a special case of MCBC without don't care symbols, and hence Theorem [\ref=thm:KPR] implies the conjecture of [\cite=WUB13]. (Apparently, the authors of [\cite=WUB13] were unaware of the previous work on H2C cited above. The term biclustering does not appear in [\cite=KPR04], whereas the term segmentation does not appear in [\cite=WUB13].)

Proof of NP-hardness

We start with some background. A Hadamard code HM of dimension M is a collection of M vectors in {1, - 1}M with the property that every two vectors are orthogonal. There are well known recursive constructions of Hadamard codes when M is a power of 2, and hence we shall assume M to be a power of 2.

Recall the notions of [formula] and [formula] norms of a vector. We shall use the following proposition.

Consider an arbitrary set of distinct vectors from an arbitrary Hadamard code HM. Then the [formula] norm of their sum is at most M3 / 2.

The [formula] norm of a code word is [formula]. As the codewords are orthogonal, the [formula] norm of the sum of q distinct vectors is [formula]. The [formula] norm can exceed the [formula] norm by a factor of at most [formula]. As q  ≤  M, the proof follows.

We now prove Theorem [\ref=thm:KPR].

The proof is by reduction from max cut, and uses for H2S the [formula] maximization formulation.

Consider a graph G(V,E) with n vertices and m edges that serves as an input instance for max cut. Orient the edges of G arbitrarily. Our reduction uses an integer parameter M (setting M to be O(n2m2) will suffice). We reduce the oriented G into an instance of H2S with k  =  Mn vectors of dimension d  =  Mm as follows.

The coordinates of vectors are partitioned into m blocks of M coordinates. Each block corresponds to one edge of G. Every vertex vi of G gives rise to M vectors [formula]. In each of these vectors, in every block Be that corresponds to edge e:

If vi is the head of e then all entries of Be are + 1.

If vi is the tail of e then all entries of Be are - 1.

If vi is not incident with e, then the entries of Be in vi,j (for 1  ≤  j  ≤  M) are the jth codeword of the Hadamard code HM.

Yes instances. Let (V1,V2) be the optimal cut for G, and suppose that it cuts c edges (necessarily [formula]). Consider the solution to the H2S instance that partitions the vectors into two clusters X1 and X2 in agreement with the partition (V1,V2).

The value of the solution can be lower bounded as follows. There are Mn vectors and m blocks, each of size M. Consider a block that corresponds to an edge e that is cut. In each of X1 and X2 there is one endpoint of the edge, and this vertex has a monochromatic block that contributes M2 to the [formula] norm. This might be partially offset by the other blocks. But the block of each vertex not incident with e can offset at most M3 / 2 of the [formula] norm, by Proposition [\ref=pro:hadamard]. As there are c edges in the cut, the value of the solution is at least c(2M2  -  (n - 2)M3 / 2). (The value is in fact higher because blocks corresponding to edges not in the cut also contribute to the [formula] norm, but we ignore this further tightening of the parameters.)

No instances. Suppose now that no cut of G cuts c - 1 edges. Consider an arbitrary partition of the vectors of the H2S instance into two parts X1 and X2. This partition corresponds to a fractional partition of G, where the extent xi to which a vector vi is in V1 is equal to the fraction of its vectors that are in X1. Similarly, 1  -  xi is the extent to which vi is in V2. For an edge e  =  (vi,vj), the extent to which it is cut is ye  =  |xi  -  xj| (which of course equals |(1  -  xi)  -  (1  -  xj)|).

For an arbitrary edge e, consider the contribution of the blocks associated with it to the [formula] norm. The combination of two monochromatic blocks that are associated with its end points contribute M2ye to the [formula] norm of each of X1 and X2. The Hadamard blocks associated with vertices that are not end points of e each contributes at most [formula] towards the sum of [formula] norms of X1 and X2. (There is a multiplier of [formula] rather than just 1 because a vertex may be split among both sides of the cut. Proposition [\ref=pro:hadamard] allows one to upper bound the effect of this split by [formula].) Summing up over all edges and all blocks, the value of any solution is at most [formula].

To bound [formula], observe that local search can always change a fractional cut into an integer cut which is at least as large. Hence [formula].

Summary. Subtracting the upper bound for no instances from the lower bound for yes instances, it follows that the yes instance leads to higher value than the no instance if [formula]. Taking M  >  2m2n2 suffices.

Remark: The value of M in the proof of Theorem [\ref=thm:KPR] can be reduced to O(n2) by using the fact that max-cut is APX-hard. By the results of [\cite=hastad01], for no instances we may assume that there is no cut with 0.942c edges.

Acknowledgements

The question of NP-hardness of the hypergraph 2-segmentation problem was communicated to me by Shai Ben-David and Ruth Urner at the Dagstuhl seminar 14372, Analysis of Algorithms Beyond the Worst Case, September 2014, where the work reported here was also performed.

Work supported in part by the Israel Science Foundation (grant No. 621/12) and by the I-CORE Program of the Planning and Budgeting Committee and the Israel Science Foundation (grant No. 4/11).