Special Case

An Improved Hazard Rate Twisting Approach for the Statistic of the Sum of Subexponential Variates (Extended Version)

Introduction

The problem of analyzing the statistic of the sum of RVs is often encountered in many applications of wireless communication systems. The most relevant example is that of estimating the probability that the total interference power [\cite=930627], often modeled as a sum of RVs, exceeds a certain threshold. Addressing this issue can essentially serve to shed the light on the behavior of the outage probability of the signal-to-interference-plus-noise (SINR) ratio, which is among the most important performance metrics in practice. This question is receiving an increasing interest, mostly spurred by the emergence of cognitive radio systems, in which the control of the interference is of paramount importance [\cite=6364048]. However, closed-form expressions for many well-known challenging sum distributions are generally intractable and unknown, which makes this problem far from being trivial. In particular, we are interested in estimating the sum distribution of RVs with subexponential decay which includes for example the Log-normal and the Weibull (with shape parameter less than 1) RVs. The sum distribution of these two RVs has received a lot of interests [\cite=1275712] [\cite=1369233] [\cite=4814351] [\cite=4623761] [\cite=5425708] [\cite=5282371] [\cite=1665128] [\cite=alouini1] [\cite=1388722]. A crude Monte Carlo (MC) simulation is of course the standard technique to estimate the probability of interest. However, it is widely known that this technique becomes computationally expensive when rare events are considered, i.e. events with very small probabilities.

Importance Sampling (IS) is a well-known variance reduction technique which aims to improve the computational efficiency of naive MC simulations [\cite=opac-b1100693]. The idea behind this technique is to consider a suitable change of the underlying sampling distribution in a way to achieve a substantial variance reduction of the IS estimator. Many research efforts were carried out to propose efficient IS simulation approaches. For instance, an interesting hazard rate twisting technique was derived that deals with the sum of independent and identically distributed (i.i.d) subexponential RVs [\cite=Juneja:2002:SHT:566392.566394]. In [\cite=DBLP:journals/corr/RachedBKAT14], this technique was further extended to handle the sum of independent and not necessarily identically distributed subexponential RVs. The idea behind this technique was to twist the hazard rate of each component in the summation by a twisting parameter θ between 0 and 1. In this letter, we propose an improved version of the work in [\cite=DBLP:journals/corr/RachedBKAT14] by twisting only the heaviest components which have the biggest impact on the right-tail of the sum distribution.

Importance Sampling

Let us consider a sequence X1,X2,...,XN of independent and not necessarily identically distributed positive RVs. We denote the Probability Density Function (PDF) of Xi by fi(  ·  ), i = 1,2,...,N. Our goal is to efficiently estimate:

[formula]

for a sufficiently large threshold γth. We focus on the case where the RVs Xi, i = 1,2...,N, belong to the class of heavy-tailed distributions, i.e distributions which decay slower than any exponential distribution. In particular, we are interested in a subclass called subexponential distributions which contains some of the most commonly used heavy-tailed distributions such as the Log-normal distribution and the Weibull distribution with shape parameter less than 1.

Obviously, a naive MC simulation technique can be used to estimate α. In the framework of small threshold values, this naive MC simulation is actually efficient and no further computational improvement is worthy to try. However, it is well known that this approach is computationally expensive when we consider the estimation of rare events, i.e. events with very small probabilities. IS is an alternative approach which can improve the computational efficiency of naive MC simulations [\cite=opac-b1100693]. The idea behind this variance reduction technique is to construct an unbiased estimator IS of α with much smaller variance than the naive MC estimator. In fact, the IS approach is based on performing a change of the sampling distribution as follows:

[formula]

where the expectation [formula] is taken with respect to the new probability measure p* under which the PDF of each Xi is gi(  ·  ), i = 1,2,...,N. The likelihood ratio L is defined as:

[formula]

The IS estimator is then given by:

[formula]

where M is the number of simulation runs and (  ·  ) is the indicator function. The fundamental issue in importance sampling lies in the choice of the new sampling distribution gi(  ·  ), i = 1,2,...,N, that results in substantial computational gains. In fact, the new sampling distribution should emphasize the generation of values that have more impact on the desired probability α (in our setting, important samples are the ones which satisfy SN  >  γth). Thus, by sampling these important values frequently, the estimator's variance can be reduced.

The asymptotic optimality is an interesting criterion that can be used to quantify the pertinence of the probability measure change [\cite=Juneja:2002:SHT:566392.566394]. This criterion is often achieved through a clever choice of the sampling distribution. Let us define the sequence of RVs Tγth as:

[formula]

From the non-negativity of the variance of Tγth, it follows that:

[formula]

for any probability measure p*. We say that the asymptotic optimality criterion is achieved if the previous inequality holds with equality as γth  →    +    ∞  , that is:

[formula]

For instance, the naive MC simulation is not asymptotically optimal since the limit in ([\ref=asymp_opt]) is equal to 1. A lot of research efforts were carried out to propose interesting changes of the sampling distribution. The exponential twisting technique, derived from the large deviation theory, is the most used technique in the setting where the underlying distribution is light-tailed [\cite=54903]. In the heavy-tailed case, the exponential twisting technique is no more feasible and an alternative approach is needed. In [\cite=Juneja:2002:SHT:566392.566394], an interesting hazard rate twisting technique (we call it conventional hazard rate-based IS technique in the rest of this work) was derived to deal with heavy-tailed distributions. In that work, an i.i.d sum of distributions with subexponential decay were considered and the asymptotic optimality of the hazard rate twisting approach was verified. In [\cite=DBLP:journals/corr/RachedBKAT14], an extension of [\cite=Juneja:2002:SHT:566392.566394] to the case of independent and not necessarily identically distributed sum of subexponential variates is developed and the asymptotic optimality criterion was again shown to be satisfied. Let us define the hazard rate of Xi, i = 1,2,...,N, as

[formula]

where Fi(  ·  ) is the cumulative distribution function of Xi, i = 1,2,...,N. We also define the hazard function as

[formula]

The PDF of Xi, i = 1,2,...,N, is related to the hazard rate and the hazard function as follows

[formula]

The conventional hazard rate twisting technique [\cite=DBLP:journals/corr/RachedBKAT14] considers a new sampling distribution which is obtained by twisting the hazard rate of the underlying distribution ([\ref=under]) of each component in the summation SN by the same quantity 0  ≤  θ < 1

[formula]

Improved Approach

Proposed Approach

Similar to [\cite=DBLP:journals/corr/RachedBKAT14], we consider a sequence X1,X2,...,XN of independent and not necessarily identically distributed subexponential positive RVs, belonging to the same family of distribution, i.e., for example a sum of independent Log-normal variates with different means and variances. From this sequence, we extract a sub-sequence containing the RVs with the heaviest right-tail which are naturally i.i.d RVs. Let us denote by s the number of RVs contained in this sub-sequence. It is important to note that the particular i.i.d case occurs when s = N. For instance, for the particular Weibull distributions with scale parameters βi and shape parameters ki, i = 1,2,...,N, the number s is defined as:

[formula]

where #   denotes the cardinality of the set, kmin  =   min iki and βmax  =   max i;ki = kminβi. For Log-normal RVs with mean μi and standard deviation σi, i = 1,2,...,N, the number s is

[formula]

where σmax  =   max iσi, and μmax  =   max i;σ  =  σmaxμi. The adopted methodology in the present work is to twist only these s heaviest i.i.d RVs and keep the others untwisted. The intuition behind this methodology is that the remaining untwisted RVs have a negligible effect on the right-tail of the sum distribution. Moreover, by only twisting the dominating RVs, the new sum distribution is less heavier than the one obtained by the conventional approach (since the hazard rate twisting of a RV results in a more heavier distribution). Furthermore, our improved technique remains able to generate important samples, i.e realizations that exceed the given threshold. Therefore, one could expect a variance reduction using our improved approach. In order to validate our expectation, we represent in Fig. [\ref=fig1] the second moment of the RV Tγth, given by the conventional and the improved IS approaches, as function of the twisting parameter θ for the sum of four independent but not identically distributed Log-normal RVs and for a fixed threshold γth. It is clear from this figure that the idea of considering only the dominating RVs, instead of treating all the components similarly, reduces the second moment [formula] for all values of θ and hence decreases the variance of Tγth. In the following subsection, we will describe a procedure to determine the optimal (in a sense that will be explained later) twisting parameter which ensures the largest amount of variance reduction. Without loss of generality, we assume that the s heaviest i.i.d RVs which will be twisted are X1,X2,...,Xs. From [\eqref=like] and [\eqref=eq:twist], the likelihood ratio of the improved approach is then:

[formula]

where Λ1(  ·  ) is the hazard function of the RVs X1,X2,...,Xs.

Determination of the Twisting Parameter

The determination of the parameter θ is performed following the steps of the minmax approach derived in [\cite=DBLP:journals/corr/RachedBKAT14]. It is important to note that applying the same technique in the present setting is not attractive since it results in a zero twisting parameter. Hence, a slight modification is required. In fact, the second moment of Tγth can be decomposed into two terms as follows:

[formula]

Instead of applying the minmax approach to [formula] as in [\cite=DBLP:journals/corr/RachedBKAT14], we propose to determine the parameter θ through considering only the dominant term [formula]. In the first step, an upper bound of this term is derived through the resolution of the following maximization problem (P)

[formula]

Let X*1,X*2,...,X*s be the solution of (P). From ([\ref=Like_ratio]), it follows that

[formula]

The second step in the minmax approach is to minimize the previous upper bound with respect to θ. Equivalently, we minimize the function [formula]. Equating The first derivative of log (f(θ)) with respect to θ to zero yields

[formula]

This leads to the minmax optimal parameter given by

[formula]

Through a simple computation, we prove that the function [formula] is actually convex and hence θ* is a minimizer.

Asymptotic Optimality

The asymptotic optimality criterion ([\ref=asymp_opt]) of our proposed IS technique is stated in the the following theorem:

The second moment of the RV Tγth could be written using ([\ref=Like_ratio]) as follows

[formula]

Let us now consider the following minimization problem (P')

[formula]

Let us denote by X* '1,X* '2,...,X* 'N the solution of (P') and [formula]. From ([\ref=secon_mom]), we have

[formula]

By replacing θ by θ* in ([\ref=theta_opti]), it follows that

[formula]

where [formula]. By applying the Logarithm function on both sides, we get

[formula]

In the other hand, since {X1  >  γth}  ⊂  {SN  >  γth} (this follows from the positivity of X1,X2,...,XN), we get by applying the Logarithm function that

[formula]

The last step of the proof is to investigate the asymptotic behavior of the optimization problems (P) and (P'). Under a concavity assumption (which is satisfied by all commonly used subexponential distributions such as the Log-normal RV and the Weibull RV with shape parameter less than 1), an equivalent optimization problem was studied in details in [\cite=DBLP:journals/corr/RachedBKAT14]. Applying the results of [\cite=DBLP:journals/corr/RachedBKAT14] to (P) yields

[formula]

Moreover, the assumption [formula], i = s + 1,...,N, as γth  →    +    ∞   implies that 2Λ1(γth)  -  Λi(γth)  →    -    ∞  , i = s + 1,...,N, as γth  →    +    ∞  . Thus, the results in [\cite=DBLP:journals/corr/RachedBKAT14] applied to (P') results in

[formula]

From ([\ref=beh1]), ([\ref=beh2]) and the fact that Λ1(γth)  →    +    ∞   as γth  →    +    ∞  , we deduce that the left hand side of ([\ref=secon_log]) is negative for a sufficiently large γth. Hence, since [formula], it follows that

[formula]

Finally, using ([\ref=beh1]) and ([\ref=beh2]), we deduce that

[formula]

Using ([\ref=res1]), the asymptotic optimality criterion ([\ref=asymp_opt]) is satisfied and the proof is concluded.

The assumption that [formula], i = s + 1,...,N, as γth  →    +    ∞   does not introduce in most of the cases a strong limitation. For instance, this assumption holds for the Weibull distribution when the source of heaviness is due to the shape parameter , that is k1  <  ki, i = s + 1,...,N. Moreover, it is also satisfied in the Log-normal setting provided that [formula], i = s + 1,...,N.

Through extensive simulation results, we believe that Theorem 1 holds also when the assumption [formula], i = s + 1,...,N, is not satisfied.

Simulation Results and Concluding Remarks

In this section, we will present some selected simulation results to show the computational gain achieved by our new proposed approach compared to the naive Monte Carlo simulation technique and the conventional hazard rate-based technique [\cite=DBLP:journals/corr/RachedBKAT14] where all the components in the summation are twisted. In Fig. [\ref=fig2], our objective is to quantify the amount of variance reduction reached by our proposed IS technique. For this purpose, we plot in this figure the second moment of Tγth given by both the improved and the conventional techniques as function of the threshold. The minmax optimal parameter ([\ref=theta_opti]) is used for the improved IS technique while for the conventional IS technique the corresponding minmax parameter is given in [\cite=DBLP:journals/corr/RachedBKAT14]. We deduce from Fig. [\ref=fig2] that, for both cases N = 2 and N = 4, the variance is reduced for all the range of considered thresholds. Moreover, it is worthy to point out that the variance reduction of the improved method is much more important when N = 4 than when N = 2. This result is expected since in the case of N = 4, the two additional components are lighter than the dominating RV and hence they do not affect considerably the right-tail of the sum distribution. Consequently, twisting these two RVs by the conventional IS approach will obviously worsen the second moment of Tγth. We also deduce from Fig. [\ref=fig2] that for N = 4, the second moment obtained by the improved IS approach remains approximately the same as for N = 2. The argument is that the variance given by the improved IS technique depends strongly on the number of dominating RVs. Hence, adding the two non-dominating RVs, which are not twisted, to the sum does not affect considerably the variance of the proposed IS method.

In a second step, we evaluate the computational gain of the proposed method. For that, we define, for a fixed accuracy requirement, the efficiency measure ξ1 between the improved IS technique and the naive MC simulation approach as:

[formula]

Similarly, we define the efficiency ξ2 between the conventional IS technique and the naive MC simulation technique as:

[formula]

where MMC, MI, MC, are the number of simulation runs for respectively, the naive MC simulation technique, the improved IS and the conventional IS techniques, whereas [formula], [formula] and [formula] refer to their corresponding variances. The efficiency ξ1 (respectively ξ2) measures the gain achieved by the improved IS technique (respectively the conventional IS technique) over the naive MC simulation technique in terms of necessary number of simulation runs to meet a fixed accuracy requirement. In Fig. [\ref=fig3], the two efficiency measures ξ1 and ξ2 are plotted as function of the threshold for both cases N = 2 and N = 4. We note from this figure that in both cases ξ1 and ξ2 are much more bigger than 1 and hence the improved and the conventional IS techniques are more efficient than the naive MC simulation technique. Moreover, it is important to point out that the larger is γth, the more efficient are the two IS techniques compared to the naive MC simulation. For instance, for a fixed requirement, the naive MC simulation needs approximately MMC = 105  ×  MC = 107  ×  MI simulations runs when γth = 32 dB and N = 4, (see Fig. 3). Furthermore, the most interesting result is that our improved IS technique is more efficient than the conventional one for both scenarios N = 2 and N = 4. For instance, for N = 4 and γth=32 dB, the conventional IS approach requires approximately [formula] simulation runs to achieve the same variance (accuracy) given by our improved IS technique. In addition, the efficiency of our improved approach compared to the conventional one [formula] is clearly higher in the case N = 4 than the one obtained when N = 2. This is actually expected from the analysis of the variance in Fig. 2.