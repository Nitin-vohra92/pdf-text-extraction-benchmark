Observed Range Maximum Likelihood Estimation

Summary of previous work

Let [formula] be i.i.d. real valued random variables with distribution function F and corresponding realized values [formula]. In the remainder of the paper we assume that [formula]. The realized value xn of the random variable Xn is either an exact observation or censored into an interval (tn,t2n]. We allow for the possibility that t2n  =    ∞   and adopt the convention that (tn,t2n] is to be interpreted as (tn,  ∞  ) in that special case.

For a given element τ∈(F) we define dτ as the number of sample values observed to be less than or equal to τ and aτ as the number of sample values observed to be greater than τ. The count uτ represents the number of censored sample values with censoring intervals that capture τ. For example, a censored value xn is included in the count dτ iff t2n  ≤  τ and in the count aτ iff tn  ≤  τ. From these definitions immediately follows that for any τ∈(F) we have that dτ + aτ + uτ  =  N.

Let kτ be the actual number of sample values not exceeding τ. Due to the presence of the censoring mechanism the value of kτ is only observed to satisfy dτ  ≤  kτ  ≤  dτ + uτ; we label the latter event as E. The likelihood of E is given by

[formula]

Let us define the function :(F)  →  [0,1] as the value of p that maximizes

[formula]

subject to the constraint 0  ≤  p  ≤  1. The value of (τ) has been derived to be

[formula]

Furthermore, the function [formula] can be used as an estimator for F since it is a non-decreasing function over (F).

Multivariate extension

In this section the definition of the estimator [formula] has been extended to the general case of a sample of M-variate observations. Let [formula] be i.i.d. M-vectors with distribution function F and the matrix [formula] be defined as

[formula]

For the rest of the paper we have assumed that all observations Xnm are censored into corresponding intervals (Lnm,Rnm] since the treatment of a dataset [formula] containing exact in addition to censored observations does not provide any new mathematical insight.

We also adopt the convention that unless explicitly stated otherwise, an index represented by a small letter ranges between 1 and the value of the corresponding capital letter inclusive. For example, [formula]. Furthermore, a random quantity will be always designated by a capital letter and the corresponding small letter will be reserved for its realization. For example, n is the realization of the random vector n.

Let (m)im be the value of the im-th biggest element, [formula], of the set

[formula]

and the set G(m) be defined as

[formula]

Consequently, the elements of G(m) are all distinct and such that

[formula]

Let us also define the grid G as [formula]. Our goal will be to estimate F over G.

Let [formula] and [formula]. We will write   <  ' iff xm  <  x'm. The expressions   >  ',   ≤  ' and   ≥  ' are defined analogously. Let [formula] and [formula]. By analogy with the 1-dimensional case we define d() as the count of observations n such that n  ≤   and u() as the count of observations satisfying n  <    <  n. It is important to point out that the count a()  =  N  -  d()  -  u() is not the number of observations such that n  <  . Finally, let k() be the realized value of the actual count of observations such that n  ≤   and E designate the event d()  ≤  k()  ≤  d() + u().

Now we can estimate F() by the value of the variable p that maximizes the function

[formula]

subject to the constraint 0  ≤  p  ≤  1. Consequently the estimator [formula] of the unknown distribution function F is given by

[formula]

We briefly consider once again a sample of univariate observations [formula] with Xn censored into an interval (Ln,Rn] and assume that the random vectors (Xn,  Ln,  Rn) are all i.i.d. according to some cdf FXLR. The latter function provides a quantitative descsription of the censoring mechanism at play. By setting n  =  (Xn,  Ln,  Rn) and employing the estimation procedure just described we can construct an estimator XLR for the unknown function FXLR allowing us to estimate how the censoring mechanism operates.

Kernel density estimation in 1 and 2 dimensions

Consider a univariate random sample [formula] from some unknown pdf fZ and suppose that the corresponding observations are all exact. The kernel density estimate Z of fZ is defined as

[formula]

where h is an appropriately chosen parameter. The rationale for such a construction is to place a "bump" of size 1 / N centered over each one of the sample values zn. The general shape of each bump is determined by the choice of the kernel function K while its spread is controlled by the parameter h. All the bumps are set to be of equal size 1 / N due to the i.i.d. nature of the observations. The size of the bump over zn can be also interpreted as the amount of probability assigned over the interval (zn - 1,  zn] by the empirical cdf z and is thus equal to Z(zn) - Z(zn - 1).

We apply the reasoning from above to the case of a univariate random sample [formula] from some unknown density function fX such that each Xn is censored into some interval (Ln,  Rn]. The set G is reduced to the set [formula] of unique element values of the set

[formula]

listed in increasing order. We proceed to define the function w:G  →  [0,1] by w(1) = (1) and w(i) = (i)  -  (i - 1) if 2  ≤  i  ≤  I. Now we define the smoothed density estimator X as

[formula]

Next we generalize the latter construction to the case of a random sample {(Xn,Yn)} of censored 2-dimensional random vectors with unknown p.d.f fXY. The set G is given by G  =  G(x)  ×  G(y) where

[formula]

The definition of the function w:G  →  [0,1] is extended as follows: w(i,j) = 0 if i = 1 or j = 1. In all other cases w(i,j) equals the cummulative probability assigned by [formula] over the interior of the rectangle in R2 defined by the points (i - 1,j - 1), (i,j - 1), (i,j) and (i - 1,j) along with the line segments connecting (i,j - 1) with (i,j) and (i - 1,j) with (i,j). Consequently, the function value w(i,j), 2  ≤  i  ≤  I, 2  ≤  j  ≤  J, is given by

[formula]

Pseudocode employing the recursive relationship from above to compute the weights w(i,j) is provided next: j = 1:J w(1,j)  =  0 j i = 1:I w(i,1)  =  0 i j = 2:J i = 2:I w(i,j)  =  (i,j)  -  (i,j - 1)  -  (i - 1,j)  +  (i - 1,j - 1) i j

Having developed a method for computing the weights w(i,j) we are ready to present the expression for the smoothed density estimator XY(x,y):

[formula]

Kernel method in M dimensions

We will use [formula] to designate an arbitrary element [formula] of the grid G. Furthermore, given any vector ∈G such that im  ≥  2 for [formula] we will define the vector

[formula]

Let Ω() be the set of all hyperplanes passing through [formula] and parallel to the coordinate planes. For example, Ω() is the set of all hyperplanes passing through [formula] and parallel to the coordinate planes. Define the function w:G  →  [0,1] as follows: w()  =  ()  =  0 if there exists a component im of [formula] such that im = 1. In the case when im  ≥  2 for [formula] the value of w() is given by the cummulative probability assigned by [formula] over the hyperrectangle in RM bounded by the hyperplanes in Ω(') and Ω() but excluding the points lying on the hyperplanes in Ω(').

Let [formula]. The smoothed function estimator is given by

[formula]

where

[formula]

A loss function for computing the optimal bandwidth

Consider once again a univariate random sample [formula] from some unknown pdf fZ and the kernel density estimator

[formula]

The bandwidth h will be treated as a variable for the remainder of the section. Also, to simplify notation whenever no ambiguity arises we will distinguish density functions by their argument only and drop the subscripting random variable. For example, f(z) will represent fZ(z). In addition, we will use a subscript "- n" to indicate that a quantity has been derived based on the subset of the original random sample obtained after removing the n-th observation. For example, - n(z) is the kernel density estimator for f(z) calculated after removing Zn from the original sample.

The integrated square error is defined as

[formula]

and the value of h minimizing the risk function R(h) given by

[formula]

is generally viewed as the optimal choice for the value of h in Z(z;h). The term [formula] is independent of h and as a result we need to minimize

[formula]

The latter goal, however, is unachievable since the density fZ is unknown.

In reality we seek to minimize the score function

[formula]

for two reasons. It is straightforward to demonstrate that

[formula]

which immediately implies that

[formula]

In addition, as stated by Silverman [\cite=r2] "Assuming that the minimizer of M0(h) is close to the minimizer of E{M0(h)} indicates why we might hope that minimizing M0 gives a good choice of smoothing parameter."

Now we move on to motivate and introduce a score function 0(h) that mimics the form of M0(h) and can be used in the presence of censoring. We begin by defining the random variables

[formula]

If we make the assumption that the probability distribution functions g of Vn and f of Xn are approximately equal, i.e g(v)  ≈  f(v), then we have that

[formula]

Since the expected values [formula] and [formula] converge asymptotically we can conclude that for large samples

[formula]

Consequently, we define 0(h) as

[formula]

In M  ≥  2 dimensions we define the random variables

[formula]

and the random vector [formula]. Under the assumption that the probability distribution functions g of n and f of n are approximately equal, i.e g()  ≈  f(), and based on identical reasoning we generalize the definition of 0() as follows:

[formula]

Nadaraya Watson regression with censored data

In regression analysis the goal is to estimate the expected value [formula] based on a random sample [formula] from some unknown p.d.f. f where n is an M-dimensional vector of explanatory variables. Nadaraya and Watson [\cite=r3] [\cite=r4] have proposed a non-parametric estimator for [formula] derived from the kernel density estimator for f in the case when all sample observations are exact. We employ the newly developed censoring kernel density estimator

[formula]

and an identical pattern of reasoning to adapt the Nadaraya-Watson estimator for use with censored data.

In 1 + 1 dimensions the censoring kernel density estimator can be written as

[formula]

Consequently

[formula]

and

[formula]

Now we define the estimator [formula] as follows:

[formula]

In (M + 1) dimensions the same reasoning leads us to define the estimator [formula] as

[formula]

Parameter estimation for a multinomial distribution in the presence of censoring

Let c1 and c2 be the respective observed numbers of outcomes of type 1 and type 2 in a binomial experiment with N trials, u  =  N - c1 - c2  ≥  1 number of trials with unknown outcomes and probability π of a single trial being of type 1. Let N1 and N2 designate the actual counts of type 1 and type 2. Consequently N1 and N2 are censored such that (N1,  N2)∈S2 where the set S2 is defined by

[formula]

If E designates the event (N1,  N2)∈S then the likelihood of observing E is given by

[formula]

As already derived, the value π̂ of p that maximizes the function

[formula]

subject to the constraint 0  ≤  p  ≤  1 is given by

[formula]

The treatment of an multinomial experiment with N trials, M possible outcome types and probabilities [formula] of each outcome type is based on the same reasoning. We use [formula] to designate the observed counts of each type and [formula] to designate the actual and possibly censored outcome counts. Suppose [formula] and define the vectors

[formula]

The definition of the set S2 generalizes to

[formula]

and accordingly E is redefined to be the event [formula]. The likelihood of E as a function of [formula] is given by

[formula]

An approximate solution to the resulting estimation problem can be constructed as follows. If p̂m is the value of the variable pm that maximizes the function

[formula]

then we could employ

[formula]

as an estimator for the unknown probability πm.

Next we consider a trinomial (M = 3) experiment such that u12 trials are of type 1 or type 2 and u23 are of type 2 or type 3 and define

[formula]

Let p̂m be the value of the variable pm that maximizes the function

[formula]

and

[formula]

The quantities π̂*1, π̂*2 and π̂*3 can be used to estimate the unknown probabilities π1, π2 and π3. Generalizing to the case of M possible outcomes in the presence of partial censoring is straightforward. Let um be the maximum possible number of censored outcomes of type m and assume that [formula] are all possible counts for the number of unobserved outcomes of type m. Consequently π̂*m is a potential estimator for πm.

So far we have been constructing likelihood functions without making assumptions or having the benefit of prior knowledge about the nature of the censoring mechanism. Let qm be the conditional probability of observing an outcome ot type m and [formula]. For example, let us consider a binomial (M = 2) experiment with known parameters q1 and q2. The probability of not being able to observe the outcome of a single trial Xn is given by (1 - q1)p1 + (1 - q2)p2  =  (p1 + p2) - p1q1 - p2q2. Consequently the likelihood of observing c1 outcomes of type 1, c2 outcomes of type 2 and u  =  N - c1 - c2 outcomes of unknown type is

[formula]

Generalizing is trivial:

[formula]

where [formula].

Finally we turn our attention to a binomial experiment such that q1 remains unknown but q2 is known. The outcome xn of a single trial Xn can be classified in exactly one of the following four categories: observed of type 1, observed of type 2, unobserved of type 1 and unobserved of type 2. Let Ñ1 designate the number of censored outcomes of type 1, Ñ2 designate the number of censored outcomes of type 2 and the set S̃2 be defined as

[formula]

The likelihood of the event Ẽ  =    "(Ñ1,  N2)∈S̃2" is given by

[formula]

where the summation index (ñ1,ñ2) spans the set S̃2. Consequently we seek to maximize the function

[formula]

subject to the constraints p1 + p2 = 1 and 0  ≤  q2'  ≤  1.

Analysis of contingency tables with incomplete counts

Since each cell in an I  ×  J contingency table can be uniquely associated with an ordered pair (i,j) the set of ordered pairs {(i,j)} constitutes the space of possible outcomes for a sample random variable Xn. Define the probabilities πij, qij and αij as

[formula]

Furthermore, let cij and Nij be the respective observed and actual counts in cell (i,j). We can quantify the effect of the censoring mechanism by observing that the ratio [formula] constitutes an MLE for the joint probability αij and using the plug-in principle within the equation αij  =  πijqij to obtain the estimator [formula] for the unknown probability qij.

The actual count Nij may be unknown due to the censoring mechanism. From the definitions follows that cij  =  Nij if outcomes of type (i,j) are not subject to censoring and cij  ≤  Nij otherwise. Finally, let us use [formula] to designate the j-th column total and in the case when Nj is known let [formula] designate the number of sample outcomes censored into the j-th column.

Consider the special case of a 2  ×  2 (I = 2,J = 2) contingency table and the null hypothesis

[formula]

which can be rewritten as

[formula]

Assuming H0 in an estimation problem amounts to introducing the constraint

[formula]

where pij is the variable associated with the unknown cell probability πij. In the special case of predetermined column totals N1 and N2 we have that π11  +  π12 = 1 as well as π12  +  π22 = 1. Consequently, the null hypothesis is reduced to H0:  π11  =  π12  =  π and accordingly the null constraint becomes p11 = p22 = p.

Before turning our attention to three examples of censored 2  ×  2 contingency tables we introduce some additional notation:

[formula]

In each example we construct the likelihood necessary to derive a set of estimators {π̂ij} for the elements of {πij}. A superscript "(0)" will be used to label quantities derived under H0. For example, π̂(0)ij is the null esimator for πij.

Example 1

Suppose that N1 and N2 are predetermined by the experimenter, the counts N11 and N21 are exact implying u1 = 0 while the counts N12 are N22 are censored implying u2  ≥  1. Let E1 designate the event "∈S and N11 + N21 = N1 and N12 + N22 = N2". The likelihood of observing E1 is given by

[formula]

where

[formula]

Since the column totals N1 and N2 are fixed and known in advance, under H0 the likelihood function needs to be modified by setting p  =  p11  =  p12:

[formula]

Let   =  (N̄11,  12,  22,  ū2) be a particular vector of counts for the contingency table. Then the probability of observing [formula] is given by

[formula]

where

[formula]

We can estimate {} by using π̂11,  π̂21,  12 and 22 for the unknown probabilities π11,  π21,  α12 and α22. Under H0 we estimate {} by employing the appropriate null estimators π̂(0)11 and π̂(0)21 as opposed to π̂11 and π̂21.

Example 2

Suppose N1 and N2 are predetermined by the experimenter and the counts N11,N21,N12,N22 are all unobserved. We use E2 designate the event "∈S and N11 + N21 = N1 and N12 + N22 = N2". The likelihood of E2 is given by

[formula]

where

[formula]

The two factors L1(p11;E) and L2(p12;E) can be maximized independently if no further assumptions are made. Under the null constraint p  =  p11  =  p12 the likelihood L(;E2) is modified as follows:

[formula]

where (n11,  n12)∈S and n11 + n21 = N1 and n12 + n22 = N2.

The probability of a particular contingency table configuration is given by

[formula]

with

[formula]

The estimators for αij remain unchanged under H0 unless additional assumptions are made regarding the nature of the censoring mechanism.

Example 3

Suppose that N11,N21,N12,N22 as well as the column totals N1 and N2 are all unknown. Let u  =  N  -  (c11 + c21 + c12 + c22) and E3 designate the event "∈S". The estimators π̂ij maximize the likelihood

[formula]

By enforcing the null constraint the above likelihood is reduced to

[formula]

The probability of a particular contingency table configuration is given by

[formula]

where

[formula]

Assuming H0 does not modify the estimate for {,  ū}.

Extending the ideas presented in this section to the construction of appropriate likelihood functions for contingency tables with I  ≥  2 rows and J  ≥  2 columns in the presence of a censoring mechanism should be trivial in most cases. Solving the resulting optimizataion problems, however, may be far from straightforward.