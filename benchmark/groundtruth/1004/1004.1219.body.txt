Verifiable nonlinear quantum evolution implies failure of density matrices to represent proper mixtures

In a recent Letter, Bennett and coworkers [\citep=Bennett2009] argue that proofs of exotic quantum effects using closed timelike curves (CTC's) based on the work of Deutsch [\citep=Deutsch1991], or other nonlinear quantum dynamics, suffer from a fallacy that they call the "linearity trap," by which one cannot assume that a classical mixture of input states will lead to the same mixture of corresponding output states. We will show that this prescription is inconsistent with the assumption that one can verify the postulated nonlinear evolution. Specifically, if some agent can empirically verify the deterministic nonlinear action of a physical device for some set of pure-state inputs, then one cannot generally use density matrices to represent the input for a second agent who is ignorant of the actual pure state prepared. Our argument is epistemological and makes no appeal to the detailed physics of CTC's.

The work of [\citep=Bennett2009] is motivated by [\citep=Brun2009]. As acknowledged in [\citep=Bennett2009], "there it was shown that for any pair of pure states, [formula] and [formula], there is a CTC-assisted circuit that maps these to orthogonal states [formula] and [formula], respectively," which we call statement S. The two papers agree on S, but the authors of [\citep=Bennett2009] disagree that it can be used to distinguish nonorthogonal states [\cite=Brun2009].

If S is to be of any empirical consequence, an agent must be able to verify it. If Rob purchases a reusable device D (or many such devices, all purported to be the same), which is said to perform the map [formula], j∈{0,1}, then Rob will first perform repeated trials to test this. Rob has two devices Pj that prepare [formula], respectively. During each trial n, he uses Pfn to prepare state [formula] and send it into D, where fn is an n-indexed binary sequence; he measures the output in [formula]. Let C label the procedure by which fn is chosen. Note that many C's may produce the same fn (e.g., both flipping a fair coin (C1) and choosing to alternate between 0 and 1 (C2) might result in [formula]). Details of C are irrelevant as long as one such C exists that would lead Rob to believe D deterministically performs M when fed [formula]. In practice, C must seem random enough to Rob (e.g., flipping coins, using pseudorandom numbers, choosing by fiat, etc.) to convince him of the deterministic, Markovian nature of D. If no such C exists, then Rob concludes that D does not perform M as advertised. Asserting S without the possibility of independent verification of M for some D is of no empirical consequence, so we assume that such a C exists for now. Alice has been watching Rob's verification procedure. They both note that every time Pj is chosen, measuring D's output in [formula] reveals [formula]. Sometimes Alice watches the entire procedure; other times she looks away, but all trials succeed, as assumed by verifiability. Rob now asks her to look away while he chooses the device for the next run through the same process C. Alice opens her eyes to see the system entering D. Alice has looked away before, and her ignorance of the input did not affect the trials' success. By verifiability, Alice's ignorance in this instance similarly cannot influence D. By finding [formula] as the output, Alice knows that [formula] was the input.

Surprisingly, [\citep=Bennett2009] claims that Alice would get no information and that she should have calculated the output of D not by looking at M but at how it maps the density matrix [formula] that representing Alice's partial knowledge of the input; she estimates p from the lab record and other knowledge of C (e.g., coin weighting, etc.). Since ρA is independent of fn, D's output would be insensitive to which Pj was actually chosen. But this would remove determinism even for Rob, who knew what state he prepared, thus violating verifiability. The mistake in [\citep=Bennett2009] was to hold too fast to the description of mixtures by density matrices, which was developed for linear quantum theory. Decompositions of a density matrix are indistinguishable in ordinary quantum theory, but the proof requires linearity. With nonlinear evolution, they may be distinguishable, as shown in [\cite=Brun2009] and revisited in [\cite=Ralph2010]. The authors claim (without justification) that these approaches do not reduce to ordinary quantum theory far from any CTC [\cite=Bennett2009]. This claim is too strong, since only empirical consistency is required. Dynamical collapse models are not ruled out by experiments to date and would produce preferred ensembles; collapse-free resolutions may also be possible.

Another possibility is that S cannot be verified. But then no rational agent would believe that any device performs M. In summary, empirical verifiability of the deterministic, nonlinear action of some physical device for some set of pure-state inputs generally precludes using density matrices to represent proper mixtures of such inputs. Proper mixtures are based on ignorance of the state actually prepared and are a distinct empirical concept from improper mixtures (partial trace of an entangled state). In a nonlinear theory, their distinguishability remains an open empirical question.

We thank Jacques Pienaar, Tim Ralph, and Steve Flammia for assistance. This work was partly funded by an ARC Discovery grant and Postdoctoral Research Fellowship. Research at Perimeter Institute is supported by the Government of Canada through Industry Canada and by the Province of Ontario through the Ministry of Research & Innovation.

Eric G. Cavalcanti[formula] and Nicolas C. Menicucci[formula]

1. Centre for Quantum Dynamics, Griffith University, Brisbane QLD 4111, Australia

2. Perimeter Institute for Theoretical Physics, Waterloo, Ontario N2L 2Y5, Canada