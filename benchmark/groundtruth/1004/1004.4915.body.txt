Lemma Claim Corollary Definition Remark Conjecture Proposition

Algorithm

Proof of :[formula]

Graph Sparsification via Refinement Sampling

Michael Kapralov

Sanjeev Khanna

Introduction

The notion of graph sparsification was introduced in [\cite=benczurkarger96], where the authors gave a near linear time procedure that takes as input an undirected graph G on n vertices and constructs a weighted subgraph H of G with O(n log n / ε2) edges such that the value of every cut in H is within a 1  ±  ε factor of the value of the corresponding cut in G. This algorithm has subsequently been used to speed up algorithms for finding approximately minimum or sparsest cuts in graphs ([\cite=benczurkarger96] [\cite=krv06]), as well as in a host of other applications (e.g. [\cite=kl02]). A more general class of spectral sparsifiers was recently introduced by Spielman and Srivastava in [\cite=ss:sample2008]. The algorithms developed in [\cite=benczurkarger96] and [\cite=ss:sample2008] take near-linear time in the size of the graph and produce very high quality sparsifiers, but require random access to the edges of the input graph G, which is often prohibitively expensive in applications to modern massive data sets. The streaming model of computation, which restricts algorithms to use a small number of passes over the input and space polylogarithmic in the size of the input, has been studied extensively in various application domains (e.g. [\cite=b:streaming]), but has proven too restrictive for even the simplest graph algorithms (even testing s - t connectivity requires Ω(n) space). The less restrictive semi-streaming model, in which the algorithm is restricted to use Õ(n) space, is more suited for graph algorithms [\cite=fkmsz05]. The problem of constructing graph sparsifiers in the semi-streaming model was recently posed by Anh and Guha [\cite=anh-guha], who gave a one-pass algorithm for finding Benczúr-Karger type sparsifiers with a slightly larger number of edges than the original Benczúr-Karger algorithm, i.e. [formula] as opposed to O(n log n / ε2). Their algorithm requires only one pass over the data, and their analysis is quite non-trivial. However, its time complexity is Ω(mn(n)), making it impractical for applications where both time and space are important constraints

Apart from the issue of random access vs disk, the semi-streaming model is also important for scenarios where edges of the graph are revealed one at a time by an external process. For example, this application maps well to online social networks where edges arrive one by one, but efficient network computations may be required at any time, making it particularly useful to have a dynamically maintained sparsifier.

Our results:

We introduce the concept of refinement sampling. At a high level, the basic idea is to sample edges at geometrically decreasing rates, using the sampled edges at each rate to refine the connected components from the previous rate. The sampling rate at which the two endpoints of an edge get separated into different connected components is used as an approximate measure of the "strength" of that edge. We use refinement sampling to obtain two algorithms for computing Benczúr-Karger type sparsifiers of undirected graphs in the semi-streaming model efficiently. The first algorithm requires O( log n) passes, O( log n) space per node, O( log n log  log n) work per edge and produces sparsifiers with O(n log 2n / ε2) edges. The second algorithm requires one pass over the edges of the graph, O( log 2n) space per node, O( log  log n) work per edge and produces sparsifiers with O(n log 3n / ε2) edges. Several properties of these results are worth noting:

In the incremental model, the amortized running time per edge arrival is O( log  log n), which is quite practical and much better than the previously best known running time of Ω(n).

The sample size can be improved for both algorithms by running the original Bencúr-Karger algorithm on the sampled graph without violating the restrictions of the semi-streaming model, yielding [formula] and [formula] amortized work per edge respectively.

Somewhat surprisingly, this two-stage (but still semi-streaming) algorithm improves upon the runtime of the original sparsification scheme when m = ω(n log 2n) for the O( log n)-pass version and m = ω(n log 3n) for the one-pass version.

As a by-product of our analysis, we show that refinement sampling can be regarded as a one-pass algorithm for producing a sparse connectivity certificate of a weighted undirected graph (see Corollary [\ref=cor:sparse-cert]). Thus we obtaining a streaming analog of the Nagamochi-Ibaraki result [\cite=nagamochi-ibaraki] for producing sparse certificates, which is in turn used in the Bencúr-Karger sampling.

Finally, in Section [\ref=sec:two-pass] we give an algorithm for constructing O(n log n / ε2)-size sparsifiers in O(m) time using two passes over the input when m = Ω(n1 + δ).

Related Work:

In [\cite=anh-guha] the authors give an algorithm for sparsification in the semi-streaming model based on the observation that one can use the constructed sparsification of the currently received part of the graph to estimate of the strong connectivity of a newly received edge. A brief outline of the algorithm is as follows. Denote the edges of G in their order in the stream by [formula]. Set [formula]. For every t > 0 compute the strength st of et in Ht - 1, and with probability pet  =   min {ρ / st,1} set [formula], giving et weight 1 / pet in Ht and Ht = Ht - 1 otherwise. For every t the graph Ht is an ε-sparsification of the subgraph received by time t. The authors show that this algorithm yields an ε-sparsifier with [formula] edges. However, it is unclear how one can calculate the strengths st efficiently. A naive implementation would take Ω(n) time for each t, resulting in Ω(mn) time overall. One could conceivably use the fact that Ht - 1 is always a subgraph of Ht, but to the best of our knowledge there are no results on efficiently calculating or approximating strong connectivities in the incremental model.

It is important to emphasize that our techniques for obtaining an efficient one-pass sparsification algorithm are very different from the approach of [\cite=anh-guha]. In particular, the structure of dependencies in the sampling process is quite different. In the algorithm of [\cite=anh-guha] edges are not sampled independently since the probability with which an edge is sampled depends on the the coin tosses for edges that came earlier in the stream. Our approach, on the other hand, decouples the process of estimating edge strengths from the process of producing the output sample, thus simplifying analysis and making a direct invocation of the Benczúr-Karger sampling theorem possible.

Organization:

Section [\ref=sec:prelim] introduces some notation as well as reviews the Benczúr-Karger sampling algorithm. We then introduce in Section [\ref=sec:refinement] our refinement sampling scheme, and show how it can be used to obtain a sparsification algorithm requiring O( log n) passes and O( log n log  log n) work per edge. The size of the sampled graph is O(n log 2n / ε2), i.e. at most O( log n) times larger than that produced by Benczúr-Karger sampling. Finally, in Section [\ref=sec:onepass] we build on the ideas of Section [\ref=sec:refinement] to obtain a one-pass algorithm with O( log  log n) work per edge at the expense of increasing the size of the sample to O(n log 3n / ε2).

Preliminaries

We will denote by G(V,E) the input undirected graph with vertex set V and edge set E with |V| = n and |E| = m. For any ε  >  0, we say that a weighted graph G'(V,E') is an ε-sparsification of G if every (weighted) cut in G' is within (1  ±  ε) of the corresponding cut in G. Given any two collections of sets that partition V, say S1 and S2, we say that S2 is a refinement of S1 if for any X∈S1 and Y∈S2, either [formula] or Y  ⊂  X. In other words, [formula] form a laminar set system.

Benczúr-Karger Sampling Scheme

We say that a graph is k-connected if the value of each cut in G is at least k. The Benczúr-Karger sampling scheme uses a more strict notion of connectivity, referred to as strong connectivity, defined as follows:

[\cite=benczurkarger96] A k-strong component is a maximal k-connected vertex-induced subgraph. The strong connectivity of an edge e, denoted by se, is the largest k such that a k-strong component contains e.

Note that the set of k-strong components form a partition of the vertex set of G, and the set of k + 1-strong components forms a refinement this partition. We say e is k-strong if its strong connectivity is k or more, and k-weak otherwise. The following simple lemma will be useful in our analysis.

[\cite=benczurkarger96] The number of k-weak edges in a graph on n vertices is bounded by k(n - 1).

The sampling algorithm relies on the following result:

[\cite=benczurkarger96] Let G' be obtained by sampling edges of G with probability [formula], where ρ = 16(d + 2) ln n, and giving each sampled edge weight 1 / pe. Then G' is an ε-sparsification of G with probability at least 1 - n- d. Moreover, expected number of edges in G' is O(n log n).

It follows easily from the proof of theorem [\ref=thm:bk-sampling] in [\cite=benczurkarger96] that if we sample using an underestimate of edge strengths, the resulting graph is still an ε-sparsification.

Let G' be obtained by sampling each edge of G with probability p̃e  ≥  pe and and give every sampled edge e weight 1 / p̃e. Then G' is an ε-sparsification of G with probability at least 1 - n- d.

In [\cite=benczurkarger96] the authors give an O(m log 2n) time algorithm for calculating estimates of strong connectivities that are sufficient for sampling. The algorithm, however, requires random access to the edges of the graph, which is disallowed in the semi-streaming model. More precisely, the procedure for estimating edge strengths given in [\cite=benczurkarger96] relies on the Nagamochi-Ibaraki algorithm for obtaining sparse certificates for edge-connectivity in O(m) time ([\cite=nagamochi-ibaraki]). The algorithm of [\cite=nagamochi-ibaraki] relies on random access to edges of the graph and to the best of our knowledge no streaming implementation is known. In fact we show in Corollary [\ref=cor:sparse-cert] that refinement sampling yields a streaming algorithm for producing sparse certificates for edge-connectivity in one pass over the data.

In what follows we will consider unweighted graphs to simplify notation. The results obtained can be easily extended to the polynomially weighted case as outlined in Remark [\ref=rmk:weights] at the end of Section [\ref=sec:onepass].

Refinement Sampling

We start by introducing the idea of refinement sampling that gives a simple algorithm for efficiently computing a BK-sample, and serves as a building block for our streaming algorithms.

To motivate refinement sampling, let us consider the simpler problem of identifying all edges of strength at least k in the input graph G(V,E). A natural idea to do so is as follows: (a) generate a graph G' by sampling edges of G with probability Õ(1 / k), (b) find connected components of G', and (c) output all edges (u,v)∈E as such that u and v are in the same connected component in G'. The sampling rate of Õ(1 / k) suggests that if an edge (u,v) has strong connectivity below k, the vertices u and v would end up in different components in G', and conversely, if the strong connectivity of (u,v) is above k, they are likely to stay connected and hence output in step (c). While this process indeed filters out most k-weak edges, it is easy to construct examples where the output will contain many edges of strength 1 even though k is polynomially large (a star graph, for instance). The idea of refinement sampling is to get around this by successively refining the sample obtained in the final step (c) above.

In designing our algorithm, we will repeatedly invoke the subroutine (S,p) that essentially implements the simple idea described above.

It is easy to see that  can be implemented using O(n) space, a total of n   operations with O(n log n) overall cost and m   operations with O(1) cost per operation, for an overall running time of O(n log n  +  m)(see, e.g. [\cite=b:clr]). Also,  can be implemented using a single pass over the set of edges. A scheme of refinement relations between Sl,k is given in Fig. [\ref=fig:f1].

The refinement sampling algorithm computes partitions Sl,j for [formula] and [formula]. Here L =  log (2n) is the number of strength levels (the factor of 2 is chosen for convenience to ensure that SL,K consists of isolated vertices whp), K is a parameter which we call the strengthening parameter. Also, we choose a parameter φ  >  0, which we will refer to as the oversampling parameter. For a partition S, let X(S) denote all the edges in E which have endpoints in two different sets in S. The partitions are computed as follows:

The following two lemmas relate the probabilities z(e) to the sampling probabilities used in the Benczúr-Karger sampling scheme.

For any K > 0, with probability at least 1 - Kn- d every edge e satisfies z(e)  ≤  4φρ / (ε2se).

Consider an edge e with strong connectivity se, and let C denote the se-strongly connected component containing e. By Theorem [\ref=thm:bk-sampling], sampling with probability min {4ρ / se,1} preserves all cuts up to [formula] in C with probability at least 1 - n- d. Hence, all se-strongly connected components stay connected after K passes of  for all l > 0 such that 2- l  ≥  4ρ / se, yielding the lemma.

If K  >   log 4 / 3n, then 2- L(e) + 1  ≥  1 / (2se) for every e∈E(G) with probability at least 1 - Ke- (n - 1) / 100.

Consider a level l such that p = 2- l  <  1 / (2se). Let H be the graph obtained by contracting all (se + 1)-strong components in G into supernodes. Since H contains only (se + 1)-weak edges, the number of edges is at most se(n - 1) by Lemma [\ref=lm:k-weak]. As the expected number of (se + 1)-weak edges in the sample is at most (n - 1) / 2, by Chernoff bounds, the probability that the number of (se + 1)-weak edges in the sample exceeds 3(n - 1) / 4 is at most (e1 / 4(5 / 4)- 5 / 4)- (n - 1) / 2 < e- (n - 1) / 100. Thus at least one quarter of the supernodes get isolated in each iteration. Hence, no (se + 1)-weak edge survives after K =  log 4 / 3n rounds of refinement sampling with probability at least 1 - Ke- (n - 1) / 100. Since L(e) was defined as the least l such that e∈X(Sl,K), the endpoints of e were connected in SL(e) - 1,K, so 2- L(e) + 1  ≥  1 / (2se).

Let G' be the graph obtained by running Algorithm 1 with φ: = 4ρ. Then G' has O(n log 2n / ε2) edges in expectation, and is an ε-sparsification of G with probability at least 1 - n- d + 1.

We have from lemma [\ref=lm:lower] and the choice of φ that the sampling probabilities dominate those used in Benczúr-Karger sampling with probability at least 1 - Ke- (n - 1) / 100. Hence, by corollary [\ref=cor:oversampling] we have that every cut in G' is within 1  ±  ε of its value in G with probability at least 1 - Ke- (n - 1) / 100 - n- d. The expected size of the sample is O(n log 2n / ε2) by lemma [\ref=lm:upper] together with the fact that ρ = O( log n). The probability of failure of the estimate in lemma [\ref=lm:lower] is at most Kn- d, so all bounds hold with probability at least 1 - Kn- d + Ke- (n - 1) / 100 - n- d > 1 - n- d + 1 for sufficiently large n. The high probability bound on the number of edges follows by an application of the Chernoff bound.

The next lemma follows from the discussion above:

For any ε  >  0, an ε-sparsification of G with O(n log 2n / ε2) edges can be constructed in O( log n) passes of  using O( log n) space per node and O( log 2n) time per edge.

We now note that one log n factor in the running time comes from the fact that during each pass k Algorithm 1 flips a coin at every level l to decide whether or not to include e into Sl,k when e∈Sl,k - 1. If we could guarantee that Sl,k is a refinement of Sl',k for all l' < l and for all k, we would be able to use binary search to find the largest l such that e∈Sl,k in O( log  log n) time. Algorithm 2 given below uses iterative sampling to ensure a scheme of refinement relations given in Fig. [\ref=fig:f2]. For each edge e, 1  ≤  k  ≤  K, and [formula], we define for convenience independent Bernoulli random variables Al,k,e such [formula], even though the algorithm will not always need to flip all these O( log 2n) coins. Also define [formula]. The algorithm uses connectivity data structures Dl,k, 1  ≤  l  ≤  L,1  ≤  k  ≤  K. Adding an edge e to Dl,k merges the components that the endpoints of e belong to in Dl,k.

For any ε  >  0, there exists an O( log n)-pass streaming algorithm that produces an ε-sparsification G' of a graph G with at most O(n log 2n / ε2) edges using O((n / m) log n  +   log n log  log n) time per edge.

The correctness of Algorithm 2 follows in the same way as for Algorithm 1 above, so it remains to determine its runtime. An O((n / m) log n + 1) term per edge comes from amortized O(n log n + m) complexity of UNION-FIND operations. The log n factor in the runtime comes from the log n passes, and we now show that step 3 can be implemented in O( log  log n) time. First note that since Sl',k' is a refinement of Sl,k whenever l'  ≥  l and k'  ≥  k, one can use binary search to determine the largest l0 such that ut and vt are connected in Dl0 - 1,k - 1. One then keeps flipping a fair coin and adding e to connectivity data structures Dl,k for successive l  ≥  l0 as long as the coin keeps coming up heads. Since 2 such steps are performed on average, it takes O(K) = O( log n) amortized time per edge by the Chernoff bound. Putting these estimates together, we obtain the claimed time complexity.

The scheme of refinement relations between Sl,k is depicted in Fig. [\ref=fig:f2].

For any ε  >  0, there is an O( log n)-pass algorithm that produces an ε-sparsification G' of an input graph G with at most O(n log n / ε2) edges using O( log 2n) space per node, and performing O( log n log  log n + (n / m) log 4n) amortized work per edge.

One can obtain a sparsification G' with O(n log 2n / ε2) edges by running Algorithm 2 on the input graph G, and then run the Benczúr-Karger algorithm on G' without violating the restrictions of the semi-streaming model. Note that even though G' is a weighted graph, this will have overhead O( log 2n) per edge of G' since the weights are polynomial. Since G' has O(n log 2n) edges, the amortized work per edge of G is O( log n log  log n + (n / m) log 4n). The Benczúr-Karger algorithm can be implemented using space proportional to the size of the graph, which yields O( log 2n) space per node.

The algorithm improves upon the runtime of the Benczúr-Karger sparsification scheme when m = ω(n log 2n).

A One-pass Õ(n + m)-Time Algorithm for Graph Sparsification

In this section we convert Algorithm 2 obtained in the previous section to a one-pass algorithm. We will design a one-pass algorithm that produces an ε-sparsifier with O(n log 3n / ε2) edges using only O( log  log n) amortized work per edge. A simple post-processing step at the end of the algorithm will allow us to reduce the size to O(n log n / ε2) edges with a slightly increased space and time complexity. The main difficulty is that in going from O( log n) passes to a one-pass algorithm, we need to introduce and analyze new dependencies in the sampling process.

As before, the algorithm maintains connectivity data structures Dl,k, where 1  ≤  l  ≤  L and 1  ≤  k  ≤  K. In addition to indexing Dl,k by pairs (l,k) we shall also write DJ for Dl,k, where J = K(l - 1) + k, so that 1  ≤  J  ≤  LK. This induces a natural ordering on Dl,k, illustrated in Fig. [\ref=fig:f3], that corresponds to the structure of refinement relations. We will assume for simplicity of presentation that D0 = D1,0 is a connectivity data structure in which all vertices are connected. For each edge e, [formula], and 1  ≤  k  ≤  K, we define an independent Bernoulli random variable A'l,k,e with [formula]. The algorithm is as follows:

Informally, Algorithm 3 underestimates strength of some edges until the data structures Dl,k become properly connected but proceeds similarly to Algorithms 1 and 2 after that. Our main goal in the rest of the section is to show that this underestimation of strengths does not lead to a large increase in the size of the sample.

Note that not all LK = Θ( log 2n) coin tosses A'l,k,e per edge are necessary for an implementation of Algorithm 3 (in particular, we will show that Algorithm 3 can be implemented with O( log  log n) = o(LK) work per edge). However, the random variables A'l,k,e are useful for analysis purposes. We now show that Algorithm 3 outputs a sparsification G' of G with O(n log 3n / ε2) edges whp.

For any ε  >  0, w.h.p. the graph G' is an ε-sparsification of G.

We can couple behaviors of Algorithms 1 and 3 using the coin tosses A'l,k,e to show that L(e)  ≥  L'(e) for every edge e, i.e. z'(e)  ≥  z(e). Hence G' is a sparsification of G by Corollary [\ref=cor:oversampling].

It remains to upper bound the size of the sample. The following lemma is crucial to our analysis; its proof is deferred to the Appendix [\ref=app:xd] due to space limitations.

Let G(V,E) be an undirected graph. Consider the execution of Algorithm 3, and for 1  ≤  J  ≤  LK where J = (l,k), let XJ denote the set of edges e = (u,v) such that u and v are connected in DJ - 1 when e arrives. Then [formula] with high probability.

The number of edges in G' is O(n log 3n / ε2) with high probability.

Recall that Algorithm 3 samples an edge et = (ut,vt) with probability [formula], where L'(et) is the minimum l such that ut and vt are not connected in Dl,K. As before, for J = (l,k), we denote by XJ the set of edges e = (u,v) such that u and v are connected in DJ - 1 when e arrives. Note that w.h.p. [formula] w.h.p. by our choice of L =  log (2n). For each 1  ≤  l  ≤  L, let [formula]. We have by Lemma [\ref=lm:xd] that [formula] w.h.p. Also note that edges in Yl are sampled with probability at most [formula]. Hence, we get that the expected number of edges in the sample is at most

The high probability bound now follows by standard concentration inequalities.

Finally, we have the following theorem.

For any ε  >  0 and d  >  0, there exists a one-pass algorithm that given the edges of an undirected graph G streamed in adversarial order, produces an ε-sparsifier G' with O(n log 3n / ε2) edges with probability at least 1 - n- d. The algorithm takes O( log  log n) amortized time per edge and uses O( log 2n) space per node.

Lemma [\ref=lm:sparsification] and Lemma [\ref=lm:bound] together establish that G' is an ε-sparsifier G' with O(n log 3n / ε2) edges. It remains to prove the stated runtime bounds.

Note that when an edge et = (ut,vt) is processed in step 3 of Algorithm 3, it is not necessary to add et to any data structure DJ in which ut and vt are already connected. Also, since DJ is a refinement of DJ' whenever J'  ≤  J, for every edge et there exists J* such that ut and vt are connected in DJ for any J  ≤  J* and not connected for any J  ≥  J*. The value of J* can be found in O( log  log n) time by binary search. Now we need to keep adding et to DJ, for each J  ≥  J* such that Ul,k,et = 1. However, we have that [formula]. Amortizing over all edges, we get O(1) per edge using standard concentration inequalities.

For any ε  >  0 and d  >  0, there exists a one-pass algorithm that given the edges of an undirected graph G streamed in adversarial order, produces an ε-sparsifier G' with O(n log n / ε2) edges with probability at least 1 - n- d. The algorithm takes amortized O( log  log n + (n / m) log 5n) time per edge and uses O( log 3n) space per node.

One can obtain a sparsification of G' with O(n log 3n / ε2) edges by running Algorithm 3 on the input graph G, and then run the Benczúr-Karger algorithm on G' without violating the restrictions of the semi-streaming model. Note that even though G' is a weighted graph, this will have overhead O( log 2n) per edge of G' since the weights are polynomial. Since G' has O(n log 3n) edges, the amortized work per edge of G is O( log n log  log n + (n / m) log 5n). The Benczúr-Karger algorithm can be implemented using space proportional to the size of the graph, which yields O( log 3n) space per node.

The algorithm avove improves upon the runtime of the Benczúr-Karger sparsification scheme when m = ω(n log 3n).

Sparse k-connectivity Certificates: Our analysis of the performance of refinement sampling is along broadly similar lines to the analysis of the strength estimation routine in [\cite=benczurkarger96]. To make this analogy more precise, we note that refinement sampling as used in Algorithm 3 in fact produces a sparse connectivity certificate of G, similarly to the algorithm of Nagamochi-Ibaraki[\cite=nagamochi-ibaraki], although with slightly weaker guarantees on size.

[\cite=benczurkarger96] A sparse k-connectivity certificate, or simply a k-certificate, for an n-vertex graph G is a subgraph H of G such that

H has k(n - 1) and

H contains all edges crossing cuts of value k or less.

A k-connectivity certificate, or simply a k-certificate, for an n-vertex graph G is a subgraph H of G such that contains all edges crossing cuts of size k or less in G. Such a certificate always exists with O(kn) edges, and moreover, there are graphs where Ω(kn) edges are necessary. The algorithm of [\cite=nagamochi-ibaraki] depends on random access to edges of G to produce a k-certificate with O(kn) edges in O(m) time. We now show that refinement sampling gives a one-pass algorithm to produce a k-certificate with O(kn log 2n) edges in time O(m log  log n + n log n). The result is summarized in the following corollary:

Whp for each l  ≥  1 the set X(Dl,K) is a 2l-certificate of G with O( log 2n)2ln edges.

Whp X(Dl,K) contains all 2l-weak edges, in particular those that cross cuts of size at most 2l. The bound on the size follows by Lemma [\ref=lm:xd].

Algorithms 1-3 can be easily extended to graphs with polynomially bounded integer weights on edges. If we denote by W the largest edge weight, then it is sufficient to set the number of levels L to log (2nW) instead of log (2n) and the number of passes to log 4 / 3nW instead of log 4 / 3n. A weighted edge is then viewed as several parallel edges, and sampling can be performed efficiently for such edges by sampling directly from the corresponding binomial distribution.

A Linear-time Algorithm for O(n log n / ε2)-size Sparsifiers

We now present an algorithm for computing an ε-sparsification with O(n log n / ε2) edges in [formula] expected time for any δ > 0. Thus, the algorithm runs in linear-time whenever m = Ω(n1  +  Ω(1)). We note that no (randomized) algorithm can output an ε-sparsification in sub-linear time even if there is no restriction on the size of the sparsifier. This is easily seen by considering the family of graphs formed by disjoint union of two n-vertex graphs G1 and G2 with m edges each, and a single edge e connecting the two graphs. The cut that separates G1 from G2 has a single edge e, and hence any ε-sparsifier must include e. On the other hand, it is easy to see that Ω(m) probes are needed in expectation to discover the edge e.

Our algorithm can in fact be viewed as a two-pass streaming algorithm, and we present is as such below. As before, let G = (V,E) be an undirected unweighted graph. We will use Algorithm 3 as a building block of our construction. We now describe each of the passes.

Note that the first pass takes O(m) expected time since Algorithm 3 has an overhead O( log  log n) time per edge and the expected size of |E'| is |E| /  log n.

Recall that the partitions D*l are used in Algorithm 3 to estimate strength of edges e∈E'. We now show that these partitions can also be used to estimate strength of edges in E. The following lemma establishes a relationship between the edge strengths in G' and G. For every edge e∈E, let s'e denote the strength of edge e in the graph [formula].

Whp s'e  ≤  se  ≤  2se' log n  +  ρ log n for all e∈E, where ρ = 16(d + 2) ln n is the oversampling parameter in Karger sampling.

The first inequality is trivially true since G'e is a subgraph of G. For the second one, let us first consider any edge e∈E with se  >  ρ log n. Let C be the se-strong component in G that contains the edge e. By Karger's theorem, whp the capacity of any cut defined by a partition of vertices in C decreases by a factor of at most 2 log n after sampling edges of G with probability p = 4 /  log n = ρ / ((1 / 2)2ρ log n), i.e. in going from G to G'. So any cut in C, restricted to edges in E' has size at least se / (2 log n), implying that s'e  ≥  se / (2 log n). Finally, for any edge e with se  ≤  ρ log n, s'e is at least 1, and the inequality thus follows.

We now discuss the second pass over the data. Recall that in order to estimate the strength s'e of an edge e∈E', Algorithm 3 finds the minimum L(e) such that the endpoints of e are not connected in D*l by doing a binary search over the range

[formula]

Proof of Lemma [\ref=lm:xd]

We denote the edges of G in their order in the stream by [formula]. In what follows we shall treat edge sets as ordered sets, and for any E1  ⊆  E write [formula] to denote the result of removing edges of E1 from E while preserving the order of the remaining edges. For a stream of edges E we shall write Et to denote the set of the first t edges in the stream.

For a κ-connected component C of a graph G we will write |C| to denote the number of vertices in C. Also, we will denote the result of sampling the edges of C uniformly at random with probability p by C'. The following simple lemma will be useful in our analysis:

Let C be a κ-connected component of G for some positive integer κ. Denote the graph obtained by sampling edges of C with probability p  ≥  λ  /  κ by C'. Then the number of connected components in C' is at most γ|C| with probability at least 1 - e-  η|C|, where γ = (7 / 8 + e-  λ / 2 / 8) and η = 1 - e-  λ / 2.

Choose A,B  ⊂  V(C) so that [formula], |A|  ≥  |V(C)| / 2 and for every v∈A at least half of its edges that go to vertices in C go to B. Note that such a partition always exists: starting from any arbitrary partition of vertices of C, we can repeatedly move a vertex from one side to the other if it increases the number of edges going across the partition, and upon termination, the larger side corresponds to the set A. Denote by Y the number of vertices of A that belong to components of size at least 2. Note that Y can be expressed as sum of |A| independent 0 / 1 Bernoulli random variables. Let [formula]; we have that μ  ≥  |A|(1 - (1 - λ  /  κ)κ / 2)  ≥  |A|(1 - e-  λ / 2). We get by the Chernoff bound that [formula]. Hence, at least a (1 - e-  λ / 2) / 4 fraction of the vertices of C are in components of size at least 2. Hence, the number of connected components is at most a 1 - (1 - e-  λ / 2) / 8 = 7 / 8 + e-  λ / 2 / 8 = γ fraction of the number of vertices of C.

The proof is by induction on J. We prove that w.h.p. for every J = (l,k) one has [formula] for a constant c1 > 0.

It is important to note that at each step s we only flip coins Rs that correspond to edges in E(Cs), and delete only those edges from Es. While there may be edges going across partitions Ps for which we do not perform a coin flip, there number is bounded by O(2ln) since these edges do not contain a 2l-connected component.

Note that for any s > 0 the number of connected components in Ps is at most

We now show that it is very unlikely that [formula] is more than a constant factor smaller than [formula], thus showing that the number of connected components cannot be more than 1 when [formula] for an appropriate constant c > 0.

For any constant d > 0 define I+  =  {i  ≥  0:|Ci| > ((d + 2) / η) log n} and I-  =  {i  ≥  0:|Ci|  ≤  ((d + 2) / η) log n}. Also define [formula].

First note that one has [formula] for any j∈I+ by lemma [\ref=lm:components-weak]. Hence, it follows by taking the union bound that i  ≤  n2 one has [formula].

We now consider Z-i. Note that Z-i's define a martingale sequence with respect to [formula]: [formula]. Also, |Z-i - Z-i - 1|  ≤  ((d + 2) / η) log n for all i. Hence, by Azuma's inequality (see, e.g. [\cite=b:alonspencer]) one has Now consider the smallest value τ such that [formula]. Note that τ < n / (2(1 - e- 2η)(1 - γ)) since |Ci|  ≥  2. If [formula], then we have that Z+τ = S+ > 2n / (1 - γ) with probability at least 1 - n- d. Thus, Otherwise [formula] and by Azuma's inequality we have Since |Ci|  ≥  2, we have |Ci|(1 - e-  η|Ci|)  ≥  |Ci|(1 - e- 2η) and thus we get

We have shown that there exists a constant c' > 0 such that with probability at least 1 - n- d after c'2ln edges are sampled by the algorithm at level J all subsequent edges will have their endpoints connected in DJ. Note that we never flipped coins for those edges that did not contain a 2l-connected component. Setting c1 = c' + 1, we have that w.h.p. [formula]. By the inductive hypothesis we have that [formula], which together with the previous estimate gives us the desired result.

It now follows that [formula] w.h.p., finishing the proof of the lemma.